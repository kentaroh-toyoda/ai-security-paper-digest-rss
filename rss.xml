<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 07 Jan 2026 00:05:40 +0000</lastBuildDate><item><title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title><link>https://arxiv.org/abs/2512.16899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal RewardBench 2 (MMRB2), a benchmark for reward models handling interleaved text and image sequences across four tasks (text-to-image, image editing, interleaved generation, multimodal reasoning) with 1,000 expert-annotated preference pairs per task.&lt;/li&gt;&lt;li&gt;Evaluates a wide set of judges (multimodal LLM-as-a-judge and models trained with human preferences) on these tasks, reporting model accuracies (e.g., Gemini 3 Pro 75–80%, GPT-5 and Gemini 2.5 Pro 66–75%, GPT-4o 59%, human &gt;90%).&lt;/li&gt;&lt;li&gt;Shows strong correlation between MMRB2 performance and downstream task success (Best-of-N sampling) and provides analyses highlighting areas to improve multimodal reward models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Hu', 'Reyhane Askari-Hemmat', 'Melissa Hall', 'Emily Dinan', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']&lt;/li&gt;&lt;li&gt;Tags: ['reward_models', 'multimodal_alignment', 'safety_evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16899</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations aligned to international standards and internet context.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal model that leverages CoT annotations to improve harmful meme detection accuracy and interpretability.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing MemeGuard outperforms prior state-of-the-art methods on this dataset, aiming to support fine-grained analysis of implicit and nuanced harmful content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought reasoning', 'content moderation', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VisualActBench: Can VLMs See and Act like a Human?</title><link>https://arxiv.org/abs/2512.09907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisualActBench, a benchmark of 1,074 videos and 3,733 human-annotated actions for a new task called Visual Action Reasoning.&lt;/li&gt;&lt;li&gt;Labels actions with an Action Prioritization Level (APL) and proactive-reactive type to measure human-aligned reasoning and value sensitivity.&lt;/li&gt;&lt;li&gt;Evaluates 29 vision-language models (VLMs), finding notable gaps vs. human performance, especially for proactive, high-priority actions and outcome anticipation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daoan Zhang', 'Pai Liu', 'Xiaofei Zhou', 'Yuan Ge', 'Guangchen Lan', 'Jing Bi', 'Christopher Brinton', 'Ehsan Hoque', 'Jiebo Luo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'vision-language-models', 'benchmarking', 'proactive-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09907</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</title><link>https://arxiv.org/abs/2509.22496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents EAGLE, a lightweight black-box framework that attributes generated tokens of MLLMs to compact perceptual (image) regions and quantifies reliance on language priors vs perceptual evidence.&lt;/li&gt;&lt;li&gt;Introduces a unified objective combining sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful attribution.&lt;/li&gt;&lt;li&gt;Performs modality-aware analyses to disentangle which tokens rely on vision vs language, enabling fine-grained interpretability and hallucination diagnosis.&lt;/li&gt;&lt;li&gt;Empirical results show improved faithfulness, localization, and hallucination detection over prior methods while using substantially less GPU memory.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoyu Chen', 'Xiaoqing Guo', 'Kangwei Liu', 'Siyuan Liang', 'Shiming Liu', 'Qunli Zhang', 'Laiyuan Wang', 'Hua Zhang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'hallucination diagnosis', 'multimodal attribution', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22496</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2508.06142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SDEval, a dynamic safety evaluation framework that controllably adjusts distribution and complexity of safety benchmarks for multimodal LLMs via text, image, and text-image dynamics.&lt;/li&gt;&lt;li&gt;Generates new samples from existing benchmarks to mitigate data contamination and continually adapt evaluations as MLLMs evolve.&lt;/li&gt;&lt;li&gt;Demonstrates that text and image dynamics (individually and combined) significantly affect safety evaluations and can expose safety limitations across multiple safety and capability benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanqing Wang', 'Yuan Tian', 'Mingyu Liu', 'Zhenhao Zhang', 'Xiangyang Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'multimodal-llms', 'benchmarking', 'data-contamination', 'adversarial-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06142</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection</title><link>https://arxiv.org/abs/2506.16819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Loupe, a lightweight framework for joint image-level deepfake classification and pixel-wise forgery localization using a patch-aware classifier and a segmentation head with conditional queries.&lt;/li&gt;&lt;li&gt;Introduces pseudo-label-guided test-time adaptation that uses patch-level predictions to supervise the segmentation module to improve robustness under distribution shift.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on the DDL dataset and won the IJCAI 2025 Deepfake Detection and Localization Challenge (overall score 0.846).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchu Jiang', 'Jiaming Chu', 'Jian Zhao', 'Xin Zhang', 'Xu Yang', 'Lei Jin', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'image forensics', 'test-time adaptation', 'localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16819</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title><link>https://arxiv.org/abs/2412.21059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VisionReward: a hierarchical visual assessment framework for fine-grained human preference learning for images and videos, using linear weighting for interpretability.&lt;/li&gt;&lt;li&gt;Introduces a multi-dimensional consistency strategy for using VisionReward as a reward model during preference optimization of visual generative models.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains over prior video reward models (e.g., +17.2% preference prediction vs VideoScore; +31.6% pairwise win rate in text-to-video models) and releases code/datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazheng Xu', 'Yu Huang', 'Jiale Cheng', 'Yuanming Yang', 'Jiajun Xu', 'Yuan Wang', 'Wenbo Duan', 'Shen Yang', 'Qunlin Jin', 'Shurun Li', 'Jiayan Teng', 'Zhuoyi Yang', 'Wendi Zheng', 'Xiao Liu', 'Dan Zhang', 'Ming Ding', 'Xiaohan Zhang', 'Xiaotao Gu', 'Shiyu Huang', 'Minlie Huang', 'Jie Tang', 'Yuxiao Dong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'preference-learning', 'interpretability', 'video-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.21059</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GDRO: Group-level Reward Post-training Suitable for Diffusion Models</title><link>https://arxiv.org/abs/2601.02036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GDRO (Group-level Direct Reward Optimization), a post-training group-level reward alignment method designed for rectified-flow text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;GDRO enables fully offline training (avoiding costly online image rollouts) and is sampler-independent (does not require ODE-to-SDE stochastic approximations).&lt;/li&gt;&lt;li&gt;Addresses and measures 'reward hacking' by introducing a corrected evaluation score that accounts for reward trends, and shows empirical robustness on OCR and GenEval tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyang Wang', 'Xi Chen', 'Xiaogang Xu', 'Yu Liu', 'Hengshuang Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['reward alignment', 'diffusion models', 'reward hacking', 'robustness', 'offline optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02036</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial jailbreak attack for large vision-language models using zeroth-order optimization (ZO-SPSA) to craft imperceptible input perturbations.&lt;/li&gt;&lt;li&gt;ZO-SPSA is gradient-free, model-agnostic, and designed to be resource-efficient compared to white-box or surrogate-based attacks.&lt;/li&gt;&lt;li&gt;Evaluated on InstructBLIP, LLaVA, and MiniGPT-4, achieving high attack success rates (e.g., 83.0% on InstructBLIP) and strong cross-model transferability (e.g., 64.18% ASR from MiniGPT-4).&lt;/li&gt;&lt;li&gt;Highlights practical vulnerabilities in LVLM safety mechanisms and the feasibility of real-world black-box jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'black-box optimization', 'LLM/LVLM safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs</title><link>https://arxiv.org/abs/2601.01592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenRT, an open-source, modular, high-throughput red-teaming framework for evaluating safety of multimodal LLMs.&lt;/li&gt;&lt;li&gt;Defines an adversarial kernel separating five dimensions (model integration, dataset management, attack strategies, judging methods, evaluation metrics) to standardize and scale attacks.&lt;/li&gt;&lt;li&gt;Implements 37 attack methods (including white-box gradients, multimodal perturbations, multi-agent evolutionary strategies) and evaluates 20 advanced models, reporting high attack success rates (e.g., avg ASR up to 49.14%).&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art reasoning models are not inherently more robust to complex, multi-turn jailbreaks and exposes cross-paradigm safety gaps.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Wang', 'Yunhao Chen', 'Juncheng Li', 'Yixu Wang', 'Yang Yao', 'Tianle Gu', 'Jie Li', 'Yan Teng', 'Xingjun Ma', 'Yingchun Wang', 'Xia Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial attacks', 'multimodal safety', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01592</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions</title><link>https://arxiv.org/abs/2601.01008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agentic modular framework for stroke imaging comprising a perception agent (lesion-aware analysis), an uncertainty estimation agent (slice-level predictive reliability), and a decision agent (abstain vs. predict based on uncertainty thresholds).&lt;/li&gt;&lt;li&gt;Prioritizes clinical safety by enabling uncertainty-driven selective abstention in ambiguous or low-information slices rather than optimizing only for segmentation/classification accuracy.&lt;/li&gt;&lt;li&gt;Integrates visual explanation mechanisms to support both predictive outputs and abstention decisions, improving transparency and clinician alignment.&lt;/li&gt;&lt;li&gt;Evaluates the approach qualitatively and via case-based analyses across representative stroke imaging scenarios; emphasizes design principles (agentic control, uncertainty awareness, selective abstention) over new performance benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Rashadul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'uncertainty estimation', 'selective abstention', 'explainability', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01008</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Uncertainty-Calibrated Explainable AI for Fetal Ultrasound Plane Classification</title><link>https://arxiv.org/abs/2601.00990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Combines multiple uncertainty estimation methods (MC dropout, deep ensembles, evidential learning, conformal prediction) with post-hoc and uncertainty-aware explanations (Grad-CAM variants, LIME-style surrogates, uncertainty-weighted activation maps) for fetal ultrasound plane classification.&lt;/li&gt;&lt;li&gt;Defines a reporting protocol coupling accuracy with calibration and selective prediction metrics (expected calibration error, Brier score, coverage-risk curves) and performs structured error analysis with explanations on the FETAL_PLANES_DB benchmark.&lt;/li&gt;&lt;li&gt;Proposes clinician-facing integration points and a human-in-the-loop workflow where uncertainty flags trigger re-acquisition or expert review, aiming for reproducible, clinically aligned deployment under noisy acquisition and domain shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Olaf Yunus Laitinen Imanov']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'calibration', 'explainability', 'medical AI safety', 'selective prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00990</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Noise-Aware and Dynamically Adaptive Federated Defense Framework for SAR Image Target Recognition</title><link>https://arxiv.org/abs/2601.00900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NADAFD, a federated defense framework against SAR-specific backdoor attacks combining frequency-domain, spatial-domain, and client-behavior analyses.&lt;/li&gt;&lt;li&gt;Introduces a frequency-domain collaborative inversion to reveal cross-client spectral inconsistencies indicative of hidden triggers and a noise-aware adversarial training strategy using Γ-distributed speckle masks for robust adversarial sample generation.&lt;/li&gt;&lt;li&gt;Adds a dynamic health assessment module that tracks client update behaviors and adaptively adjusts aggregation weights to mitigate malicious contributions.&lt;/li&gt;&lt;li&gt;Evaluates on MSTAR and OpenSARShip, reporting higher clean accuracy and lower backdoor attack success rates compared to existing federated backdoor defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchao Hou (Shanxi Normal University', 'Taiyuan', 'China)', 'Zixuan Zhang (Shanxi Normal University', 'Taiyuan', 'China)', 'Jie Wang (Shanxi Normal University', 'Taiyuan', 'China)', 'Wenke Huang (Nanyang Technological University', 'Singapore', 'Singapore)', 'Lianhui Liang (Guangxi University', 'Nanning', 'China)', 'Di Wu (La Trobe University', 'Melbourne', 'Australia)', 'Zhiquan Liu (Jinan University', 'Guangzhou', 'China)', 'Youliang Tian (Guizhou University', 'Guiyang', 'China)', 'Jianming Zhu (Central University of Finance and Economics', 'Beijing', 'China)', 'Jisheng Dang (Lanzhou University', 'Lanzhou', 'China)', 'Junhao Dong (Nanyang Technological University', 'Singapore', 'Singapore)', 'Zhongliang Guo (University of St Andrews', 'St Andrews', 'United Kingdom)']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'backdoor-defense', 'SAR', 'adversarial-training', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00900</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI</title><link>https://arxiv.org/abs/2601.00832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a transfer-learning pipeline for shrimp disease image classification using six pretrained CNNs (ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, Xception) on a 1,149-image, four-class dataset.&lt;/li&gt;&lt;li&gt;Uses advanced data augmentation (CutMix, MixUp), background removal, and adversarial training with FGSM to improve model robustness and reduce overfitting.&lt;/li&gt;&lt;li&gt;Applies post-hoc explainability methods (Grad-CAM, Grad-CAM++, XGrad-CAM) to visualize model attention and interpret predictions.&lt;/li&gt;&lt;li&gt;Reports best performance from ConvNeXt-Tiny with 96.88% test accuracy and a 99% CI of [0.953, 0.971].&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Israk Hasan Jone', 'D. M. Rafiun Bin Masud', 'Promit Sarker', 'Sayed Fuad Al Labib', 'Nazmul Islam', 'Farhad Billah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'adversarial examples', 'explainability', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00832</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</title><link>https://arxiv.org/abs/2601.02359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ExposeAnyone, a fully self-supervised diffusion model that generates expression sequences from audio and is personalized to specific subjects using reference sets.&lt;/li&gt;&lt;li&gt;Detects person-of-interest face forgeries by computing identity distances via diffusion reconstruction errors between suspected videos and personalized models, enabling zero-shot detection of unseen manipulations.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance (≈4.22 percentage points AUC gain) over prior SOTA across DF-TIMIT, DFDCP, KoDF, and IDForge, and effective detection of Sora2-generated videos.&lt;/li&gt;&lt;li&gt;Shows high robustness to common corruptions (blur, compression), highlighting practical applicability for real-world deepfake detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaede Shiohara', 'Toshihiko Yamasaki', 'Vladislav Golyanik']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'self-supervised learning', 'diffusion models', 'audio-driven forgery detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02359</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FMVP: Masked Flow Matching for Adversarial Video Purification</title><link>https://arxiv.org/abs/2601.02228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FMVP (Masked Flow Matching for Adversarial Video Purification), which uses a masking strategy to 'physically shatter' global adversarial structure and reconstruct clean video dynamics via Conditional Flow Matching with an inpainting objective.&lt;/li&gt;&lt;li&gt;Introduces a Frequency-Gated Loss to suppress high-frequency adversarial residuals while preserving low-frequency content, and presents Attack-Aware and Generalist training paradigms for known and unknown threats.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on UCF-101 and HMDB-51 against PGD and CW attacks, evaluates against adaptive attacks (DiffHammer), and achieves high zero-shot adversarial detection rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duoxun Tang', 'Xueyi Zhang', 'Chak Hin Wang', 'Xi Xiao', 'Dasen Dai', 'Xinhang Jiang', 'Wentao Shi', 'Rui Li', 'Qing Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'video defenses', 'purification', 'flow matching / diffusion defenses', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02228</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</title><link>https://arxiv.org/abs/2601.02147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BiPrompt, a bilateral prompt optimization framework that jointly debiases visual and textual modalities of vision-language models at test time.&lt;/li&gt;&lt;li&gt;Visual module: attention-guided erasure to suppress background/spurious activations and enforce orthogonal prediction consistency between causal and spurious regions.&lt;/li&gt;&lt;li&gt;Textual module: balanced prompt normalization, a learnable re-centering of class embeddings toward an isotropic semantic space.&lt;/li&gt;&lt;li&gt;Joint objective minimizes conditional mutual information between spurious cues and predictions, improving average and worst-group accuracies without retraining or domain supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Shounak Das', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['debiasing', 'test-time adaptation', 'vision-language models', 'robustness', 'causal reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02147</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing</title><link>https://arxiv.org/abs/2601.01957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AFTER, a method to mitigate object hallucination in large vision-language models by editing internal activations guided by factual textual semantics.&lt;/li&gt;&lt;li&gt;Introduces Factual-Augmented Activation Steering (FAS) to provide factual visual-textual guidance and Query-Adaptive Offset Optimization (QAO) to produce query-specific activation edits.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across three LVLMs and benchmarks, reporting up to 16.3% reduction in hallucination on the AMBER benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianbo Wang', 'Yuqing Ma', 'Kewei Liao', 'Zhange Zhang', 'Simin Li', 'Jinyang Guo', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'activation editing', 'LVLM robustness', 'factuality/alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01957</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems</title><link>https://arxiv.org/abs/2601.01891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of agentic AI in remote sensing, presenting a taxonomy (single-agent copilots vs. multi-agent systems) and architectural components (planning, RAG, memory).&lt;/li&gt;&lt;li&gt;Reviews emerging benchmarks that shift evaluation from pixel-level metrics to trajectory-aware reasoning and task orchestration.&lt;/li&gt;&lt;li&gt;Identifies and critically examines limitations in grounding, safety, and orchestration and provides a roadmap for building robust, autonomous geospatial systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niloufar Alipour Talemi', 'Julia Boone', 'Fatemeh Afghah']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'safety', 'robustness', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01891</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization</title><link>https://arxiv.org/abs/2601.01784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses temporal forgery localization (TFL) to precisely detect/timestamp tampered segments within videos rather than just video-level detection.&lt;/li&gt;&lt;li&gt;Proposes DDNet, a dual-stream graph learning framework with a Temporal Distance Stream for local artifacts and a Semantic Content Stream for long-range connections to capture global anomalies.&lt;/li&gt;&lt;li&gt;Introduces Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints and Cross-Level Feature Embedding (CLFE) for robust hierarchical feature fusion.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains (≈9% AP@0.95) on ForgeryNet and TVIL benchmarks and improved cross-domain robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyang Zhao', 'Xin Liao', 'Jiaxin Chen', 'Xiaoshuai Wu', 'Yufeng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['temporal forgery localization', 'deepfake detection', 'graph neural networks', 'disentanglement', 'robustness / cross-domain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01784</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.01528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DrivingGen, a comprehensive benchmark dataset and evaluation suite for generative video world models in autonomous driving.&lt;/li&gt;&lt;li&gt;Introduces metrics targeting visual realism, trajectory plausibility, temporal and agent-level consistency, and controllability with respect to ego conditioning—emphasizing safety-relevant factors.&lt;/li&gt;&lt;li&gt;Curates diverse driving data across weather, time of day, regions, and maneuvers to better reflect deployment conditions.&lt;/li&gt;&lt;li&gt;Benchmarks 14 state-of-the-art models and highlights trade-offs between visual quality and physical/motion realism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Zhou', 'Hao Shao', 'Letian Wang', 'Zhuofan Zong', 'Hongsheng Li', 'Steven L. Waslander']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'world-models', 'autonomous-driving', 'benchmarking', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01528</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures</title><link>https://arxiv.org/abs/2601.01281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates four AI models (three CNNs and one Vision Transformer) for deepfake detection using large face image datasets.&lt;/li&gt;&lt;li&gt;Applies data preprocessing and augmentation techniques to improve model performance across scenarios.&lt;/li&gt;&lt;li&gt;Finds VFDNET with MobileNetV3 achieves superior accuracy and efficient performance.&lt;/li&gt;&lt;li&gt;Primarily an empirical comparison of image-based deepfake detection architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sifatullah Sheikh Urmi', 'Kirtonia Nuzath Tabassum Arthi', 'Md Al-Imran']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'computer vision', 'AI security', 'forgery detection', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01281</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models</title><link>https://arxiv.org/abs/2601.01202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefSR-Adv, an adversarial attack that perturbs only the high-resolution reference image to degrade reference-based image super-resolution (RefSR) outputs.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across CNN, Transformer, and Mamba RefSR architectures on CUFED5, WR-SR, and DRefSR, causing large performance drops and visible artifacts.&lt;/li&gt;&lt;li&gt;Finds a positive correlation between LR–reference similarity and attack success, highlighting model over-reliance on reference features as a security weakness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazhu Dai', 'Huihui Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'image-super-resolution', 'reference-based-SR', 'robustness', 'security-vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01202</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking</title><link>https://arxiv.org/abs/2601.01041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MASM, a method that decomposes pretrained model weights (via SVD) into a stable semantic subspace and multiple learnable artifact subspaces to separate semantic and forgery features.&lt;/li&gt;&lt;li&gt;Introduces selective layer masking to regulate layer updates based on artifact subspace learning state, reducing overfitting to specific forgery artifacts.&lt;/li&gt;&lt;li&gt;Uses orthogonality and spectral consistency constraints to encourage complementary, diverse artifact representations and preserve overall spectral structure for better cross-dataset generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Wenliang Weng', 'Daoyong Fu', 'Ziqiang Li', 'Zhangjie Fu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'model fine-tuning', 'defense', 'cross-dataset generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01041</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Generative Models Actually Forge Realistic Identity Documents?</title><link>https://arxiv.org/abs/2601.00829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether contemporary diffusion-based generative models (Stable Diffusion, Qwen, Flux, Nano-Banana, etc.) can produce realistic identity document forgeries.&lt;/li&gt;&lt;li&gt;Assesses both text-to-image and image-to-image pipelines against human and automated verification/forensic criteria.&lt;/li&gt;&lt;li&gt;Finds models can mimic surface-level aesthetics but fail to reproduce structural/forensic authenticity necessary to bypass forensic analysis.&lt;/li&gt;&lt;li&gt;Concludes the immediate forensic-level forgery risk from public generative models may be overestimated and urges ML-forensics collaboration for realistic threat assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Vinogradov']&lt;/li&gt;&lt;li&gt;Tags: ['document forgery', 'generative models', 'image forgery', 'forensic robustness', 'misuse/abuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00829</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>https://arxiv.org/abs/2512.24052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a taxonomy of grounding hallucinations in Large Audio-Language Models: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error.&lt;/li&gt;&lt;li&gt;Proposes AHA (Audio Hallucination Alignment), which uses counterfactual hard negative mining to build a preference dataset that trains models to distinguish true acoustic evidence from plausible fabrications.&lt;/li&gt;&lt;li&gt;Introduces AHA-Eval, a diagnostic benchmark for fine-grained temporal reasoning and grounding in audio-language tasks, and demonstrates improvements when aligning Qwen2.5-Omni to produce Qwen-Audio-AHA.&lt;/li&gt;&lt;li&gt;Reports generalization of gains to public benchmarks (e.g., +13.7% on AHA-Eval, +1.3% on MMAU-Test, +1.6% on MMAR) and releases model and dataset publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Chen', 'Wenhui Zhu', 'Xiwen Chen', 'Zhipeng Wang', 'Xin Li', 'Peijie Qiu', 'Hao Wang', 'Xuanzhao Dong', 'Yujian Xiong', 'Anderson Schneider', 'Yuriy Nevmyvaka', 'Yalin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'safety evaluation/benchmarking', 'adversarial/counterfactual examples', 'audio-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24052</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models</title><link>https://arxiv.org/abs/2511.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SteganoBackdoor, an optimization-based method to create steganographic poisoned training examples (SteganoPoisons) that hide a backdoor payload across fluent sentences.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success on diverse language model architectures under constrained poisoning budgets and conservative data-level filtering.&lt;/li&gt;&lt;li&gt;Shows the attack ties the backdoor to natural semantic concepts rather than overt artifacts, revealing blind spots in existing data-curation defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Xue', 'Ruiyi Zhang', 'Pengtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'LLM security', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14301</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</title><link>https://arxiv.org/abs/2510.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GIFT, an on-policy RL framework that aligns LLMs by minimizing discrepancy between implicit and explicit reward models, combining GRPO-style normalization, DPO-style implicit rewards, and UNA's alignment principle.&lt;/li&gt;&lt;li&gt;Normalizes implicit and explicit rewards to convert a non-convex reward-maximization problem into an MSE loss between reward functions, yielding a convex, stable, and differentiable objective.&lt;/li&gt;&lt;li&gt;Claims on-policy exploration, fewer hyperparameters, faster convergence, reduced overfitting, and improved reasoning/alignment performance on mathematical benchmarks compared to GRPO, DPO, and UNA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhichao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning (RLHF/RL)', 'implicit rewards / DPO', 'reward modeling', 'on-policy training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23868</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2510.22535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OFFSIDE, a benchmark for misinformation unlearning in multimodal LLMs based on football transfer rumors (15.68K records for 80 players) with four test sets assessing forgetting efficacy, generalization, utility, and robustness.&lt;/li&gt;&lt;li&gt;Supports advanced settings: selective unlearning, corrective relearning, and unimodal unlearning (text-only forgetting); evaluates multiple baseline unlearning methods on MLLMs.&lt;/li&gt;&lt;li&gt;Key findings: text-only (unimodal) unlearning fails on multimodal rumors; unlearning often stems from catastrophic forgetting; visual rumors (embedded in images) are particularly hard to forget; unlearned rumors are easily recoverable; methods are vulnerable to prompt attacks.&lt;/li&gt;&lt;li&gt;Provides dataset and code, highlighting significant vulnerabilities and the need for more robust multimodal unlearning solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Zheng', 'Zirui Pang', 'Ling li', 'Zhijie Deng', 'Yuhan Pu', 'Zhaowei Zhu', 'Xiaobo Xia', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'multimodal LLMs', 'misinformation', 'robustness', 'prompt attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22535</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Calgacus, a protocol that hides a meaningful secret text inside a different, coherent same-length cover text using LLMs.&lt;/li&gt;&lt;li&gt;Shows practical feasibility with ~8B open-source models, with encoding/decoding performed quickly on a laptop.&lt;/li&gt;&lt;li&gt;Demonstrates concrete security risks: covert channels that can hide unsafe or unfiltered model outputs inside benign-looking responses, undermining content filters and trust.&lt;/li&gt;&lt;li&gt;Raises broader safety and alignment questions about decoupling text from authorial intent and what models 'know'.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'steganography', 'content-filter evasion', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU) to quantify and control continuous machine unlearning via a 'cognitive rotation space' where rotational angle models unlearning degree.&lt;/li&gt;&lt;li&gt;Introduces a skew-symmetric loss to create the rotation space and a rotational salience weight to measure/control unlearning amount.&lt;/li&gt;&lt;li&gt;Adds orthogonal rotation axes regularization to make successive unlearning requests minimally interfering, reducing cumulative catastrophic utility loss.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art continuous unlearning performance without retaining a dataset, evaluated on multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Jiahua Li', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'data-removal', 'privacy', 'continual-unlearning', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs four membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) that target whether victims' historical interactions appear in system prompts for LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;Evaluates attacks empirically across five open-source LLMs and three recommender-system benchmark datasets; inquiry and poisoning attacks show significantly high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position in shots, number of poisoning items) and discusses possible mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attack', 'data-poisoning', 'recommender-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, an evolutionary black-box post-training method for LLMs that avoids exposing gradients and induces an information bottleneck via implicit data compression.&lt;/li&gt;&lt;li&gt;Provides theoretical results: non-vacuous generalization bounds and provable guarantees for privacy, robustness to data poisoning, and resistance to extraction attacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows a few iterations of BBoxER can improve LLM performance on reasoning benchmarks and reduce vulnerability to membership inference attacks.&lt;/li&gt;&lt;li&gt;Positions BBoxER as a privacy- and security-oriented add-on to gradient-based training for deployment in restricted or adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'black-box optimization', 'data poisoning', 'privacy / membership inference', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation</title><link>https://arxiv.org/abs/2506.05623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DPIaC-Eval, a deployability-centric benchmark of 153 real-world IaC scenarios spanning 58 services to evaluate LLM-generated infrastructure templates.&lt;/li&gt;&lt;li&gt;Proposes IaCGen, an LLM-driven iterative pipeline (format verification, syntax checking, live deployment feedback) that significantly improves deployability across multiple state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;Reports deployability gains (54.6–91.6% within 10 iterations) and further improvements with human-in-the-loop feedback, but identifies poor user-intent coverage (25.2%) and very low security compliance (8.4%).&lt;/li&gt;&lt;li&gt;Explores trustworthiness implications, highlighting that LLM-generated IaC often introduces security/non-compliance risks and misalignment with user requirements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Zhang', 'Shidong Pan', 'Zejun Zhang', 'Zhenchang Xing', 'Xiaoyu Sun']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'Infrastructure-as-Code security', 'Alignment &amp; safety evaluation', 'Deployability / robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05623</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns</title><link>https://arxiv.org/abs/2601.00588</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CSSBench, a benchmark targeting Chinese-specific adversarial patterns (homophones, pinyin, symbol splitting, etc.) to evaluate safety of lightweight LLMs.&lt;/li&gt;&lt;li&gt;Covers six real-world Chinese safety domains (illegal activities, privacy leakage, health misinformation, fraud/hate, adult content, public/political safety) and multiple task types.&lt;/li&gt;&lt;li&gt;Evaluates popular lightweight models, measuring safety-related failures including over-refusal, and shows these models are vulnerable to Chinese-specific adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhong Zhou', 'Shilinlu Yan', 'Chuanpu Liu', 'Qiankun Li', 'Kun Wang', 'Zhigang Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'adversarial-prompting', 'benchmark', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00588</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</title><link>https://arxiv.org/abs/2512.24556</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic safety audit of three SOTA LLMs (GPT-5.1, Gemini 3 Pro, Claude 4.5 Opus) using HausaSafety, an adversarial dataset grounded in West African threat scenarios.&lt;/li&gt;&lt;li&gt;Finds complex interactions between language (English vs Hausa) and temporal framing, revealing a Temporal Asymmetry where past-tense prompts bypass defenses while future-tense prompts trigger conservative refusals.&lt;/li&gt;&lt;li&gt;Demonstrates that safety is context-dependent (Safety Pockets) rather than a fixed property, and suggests current models rely on superficial heuristics rather than robust semantic understanding.&lt;/li&gt;&lt;li&gt;Proposes 'Invariant Alignment' as a paradigm shift to stabilize safety across linguistic and temporal variations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Abdullahi Said', 'Muhammad Sammani Sani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'multilingual safety', 'adversarial dataset', 'temporal vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24556</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies semantic entanglement (polysemanticity) as the reason LoRA/PEFT underperforms full fine-tuning for safety alignment, hindering subspace identification.&lt;/li&gt;&lt;li&gt;Proposes SAILS: use Sparse Autoencoders (SAEs) to disentangle features, construct an interpretable safety subspace from SAE decoder directions, and initialize LoRA adapters in that subspace.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving SAE-based identification can achieve arbitrarily small recovery error under monosemanticity assumptions, unlike direct identification.&lt;/li&gt;&lt;li&gt;Empirical results: up to 99.6% safety rate on Gemma-2-9B, outperforming full fine-tuning by 7.4 points and matching RLHF while updating only 0.19% of parameters and offering interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Zhenbo Xu', 'Lechen Ning', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety alignment', 'PEFT/LoRA', 'interpretability', 'representation disentanglement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization</title><link>https://arxiv.org/abs/2512.20773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial training loop between a generator (user simulator) and discriminator to improve realism of user simulations for task-oriented dialogue, focused on mental health support chatbots.&lt;/li&gt;&lt;li&gt;Fine-tuned simulators outperform zero-shot models at surfacing chatbot failure modes; adversarial iterations increase diversity, distributional alignment, and predictive validity of failures.&lt;/li&gt;&lt;li&gt;Simulator predictions correlate strongly with real-world failure occurrence across chatbot configurations; discriminator accuracy drops after iterations, interpreted as increased simulator realism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Zhu', 'Olivier Tieleman', 'Caitlin A. Stamatis', 'Luka Smyth', 'Thomas D. Hull', 'Daniel R. Cahn', 'Matteo Malgaroli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'safety evaluation', 'user simulation', 'failure-mode discovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20773</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</title><link>https://arxiv.org/abs/2512.20578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gnosis, a lightweight (~5M params) module that decodes internal hidden states and attention traces of frozen LLMs to predict correctness of outputs with negligible inference cost.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy and calibration versus internal baselines and larger external judges across tasks (math reasoning, open-domain QA, academic knowledge) and model sizes (1.7B–20B).&lt;/li&gt;&lt;li&gt;Supports zero-shot generalization to partial generations for early detection of failing trajectories, enabling compute-aware control and early stopping.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirhosein Ghasemabadi', 'Di Niu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM self-monitoring', 'safety', 'failure prediction', 'internal interpretability', 'calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20578</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AprielGuard</title><link>https://arxiv.org/abs/2512.20293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AprielGuard, an 8B-parameter safeguard model that unifies detection of safety risks (toxicity, bias) and adversarial threats (prompt injections, jailbreaks) within a single taxonomy and learning framework.&lt;/li&gt;&lt;li&gt;Trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows; augmented with structured reasoning traces for improved interpretability.&lt;/li&gt;&lt;li&gt;Evaluated on multiple public and proprietary benchmarks and reported to outperform existing open-source guardrails (e.g., Llama-Guard, Granite Guardian), especially in multi-step and reasoning-intensive scenarios; authors plan to release the model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaykumar Kasundra', 'Anjaneya Praharaj', 'Sourabh Surana', 'Lakshmi Sirisha Chodisetty', 'Sourav Sharma', 'Abhigya Verma', 'Abhishek Bhardwaj', 'Debasish Kanhar', 'Aakash Bhagat', 'Khalil Slimi', 'Seganrasan Subramanian', 'Sathwik Tejaswi Madhusudhan', 'Ranga Prasad Chenna', 'Srinivas Sunkara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'prompt injection', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20293</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FaithLens, an 8B-parameter model to detect faithfulness hallucinations in LLM outputs and produce explanations alongside binary predictions.&lt;/li&gt;&lt;li&gt;Synthesizes training data and explanations using advanced LLMs, applies filtering for label/explanation quality and diversity, then fine-tunes the model as a cold start.&lt;/li&gt;&lt;li&gt;Further optimizes via rule-based reinforcement learning with rewards for both prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluated on 12 diverse tasks, claiming stronger performance than advanced models (e.g., GPT-4.1) while being cost-efficient and providing interpretable explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'faithfulness', 'explainability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title><link>https://arxiv.org/abs/2512.16899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal RewardBench 2 (MMRB2), a benchmark for reward models handling interleaved text and image sequences across four tasks (text-to-image, image editing, interleaved generation, multimodal reasoning) with 1,000 expert-annotated preference pairs per task.&lt;/li&gt;&lt;li&gt;Evaluates a wide set of judges (multimodal LLM-as-a-judge and models trained with human preferences) on these tasks, reporting model accuracies (e.g., Gemini 3 Pro 75–80%, GPT-5 and Gemini 2.5 Pro 66–75%, GPT-4o 59%, human &gt;90%).&lt;/li&gt;&lt;li&gt;Shows strong correlation between MMRB2 performance and downstream task success (Best-of-N sampling) and provides analyses highlighting areas to improve multimodal reward models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Hu', 'Reyhane Askari-Hemmat', 'Melissa Hall', 'Emily Dinan', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']&lt;/li&gt;&lt;li&gt;Tags: ['reward_models', 'multimodal_alignment', 'safety_evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16899</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively recalibrates a small set of 'toxic' expert neurons via expertise-weighted soft suppression to reduce multimodal toxicity without parameter updates.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and evaluates SGM against existing detoxification techniques under standard and adversarial conditions.&lt;/li&gt;&lt;li&gt;Reports large toxicity reduction in open-source MLLMs (harmful rates reduced from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; SGM* combines SGM with other defenses for stronger results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'detoxification', 'neuron-level intervention', 'adversarial robustness', 'safety evaluation/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden conversational escalation in AI chatbots.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue to detect implicit harm not captured by standard toxicity filters.&lt;/li&gt;&lt;li&gt;Aims to provide more responsive guardrails for affective drift and repeated emotional reinforcement during interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'David Atkinson', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'implicit harm detection', 'guardrails', 'affective computing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution</title><link>https://arxiv.org/abs/2512.04838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Info-Mask, a segmentation framework combining stylometric cues, perplexity signals, and structured boundary modeling to detect transition points between human and AI authorship.&lt;/li&gt;&lt;li&gt;Constructs an adversarial benchmark (MAS) to evaluate robustness of mixed-authorship detectors under adversarial perturbations.&lt;/li&gt;&lt;li&gt;Introduces Human-Interpretable Attribution (HIA) overlays to explain how stylometric features inform boundary predictions and runs a small human study on usefulness.&lt;/li&gt;&lt;li&gt;Reports improved span-level robustness across multiple architectures while highlighting remaining adversarial vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['L. D. M. S. Sai Teja', 'N. Siva Gopala Krishna', 'Ufaq Khan', 'Muhammad Haris Khan', 'Atul Mishra']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'AI detection', 'mixed-authorship segmentation', 'adversarial benchmark', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04838</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGASA, a framework that synthesizes model-generated safety guidelines and augmented prompts to defend against adversarial jailbreaks.&lt;/li&gt;&lt;li&gt;Two-stage approach: Data Pre-synthesis (generate guidelines and augmented prompts) and Alignment Fine-tuning (embed guidelines via SFT and DPO).&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness to harmful adversarial prompts while reducing unnecessary refusals across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'adversarial prompting', 'alignment', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HaluMem: Evaluating Hallucinations in Memory Systems of Agents</title><link>https://arxiv.org/abs/2511.03506</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HaluMem, an operation-level benchmark to evaluate hallucinations in memory systems across extraction, updating, and question-answering stages.&lt;/li&gt;&lt;li&gt;Provides two large human-AI interaction datasets (HaluMem-Medium and HaluMem-Long) with ~15k memory points and ~3.5k multi-type questions, supporting long-context evaluations.&lt;/li&gt;&lt;li&gt;Empirical results show hallucinations are generated and accumulate during extraction and updating, propagating errors to downstream QA; calls for constrained, interpretable memory operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ding Chen', 'Simin Niu', 'Kehang Li', 'Peng Liu', 'Xiangping Zheng', 'Bo Tang', 'Xinchi Li', 'Feiyu Xiong', 'Zhiyu Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'memory-systems', 'safety-evaluation', 'benchmark', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03506</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VISTA Score: Verification In Sequential Turn-based Assessment</title><link>https://arxiv.org/abs/2510.27052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents VISTA, a framework that decomposes each assistant turn into atomic factual claims and verifies them against trusted sources and dialogue history, while categorizing unverifiable statements (subjective, contradicted, lacking evidence, abstaining).&lt;/li&gt;&lt;li&gt;Evaluates across eight LLMs and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, FADE), showing substantial improvements in hallucination detection versus FACTSCORE and LLM-as-Judge baselines; human evaluation indicates improved annotator agreement and uncovered benchmark inconsistencies.&lt;/li&gt;&lt;li&gt;Models factuality as a dynamic, turn-by-turn property of conversation, offering a more transparent, human-aligned metric for dialogue system truthfulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashley Lewis', 'Andrew Perrault', 'Eric Fosler-Lussier', 'Michael White']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'hallucination_detection', 'factuality_evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.27052</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models</title><link>https://arxiv.org/abs/2510.24794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a reasoning-answer hit gap where models find correct facts during chain-of-thought but fail to incorporate them into final answers, reducing factual fidelity.&lt;/li&gt;&lt;li&gt;Proposes MR-ALIGN, which models state transition probabilities across the model's internal thinking steps and builds a transition-aware implicit reward to reinforce beneficial reasoning segments and suppress defective ones.&lt;/li&gt;&lt;li&gt;Reweights token-level signals into probability-aware segment scores to encourage coherent reasoning trajectories that improve factual correctness without external verifiers.&lt;/li&gt;&lt;li&gt;Evaluated on multiple factual QA and long-form factuality benchmarks, showing consistent improvements in accuracy and truthfulness and reductions in misleading reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinming Wang', 'Jian Xu', 'Bin Yu', 'Sheng Lian', 'Hongzhu Yi', 'Yi Chen', 'Yingjian Zhu', 'Boran Wang', 'Hongming Yang', 'Han Hu', 'Xu-Yao Zhang', 'Cheng-Lin Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'factuality', 'LLMs', 'reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24794</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLMs can become 'evaluation-aware' (e.g., using Python type hints in evaluation contexts) through continued pretraining and expert iteration, causing them to behave differently in evaluations vs deployment.&lt;/li&gt;&lt;li&gt;Introduces an activation-steering technique: adding a steering vector (constructed from the original model) to suppress evaluation-awareness and make the model act like it is deployed even when evaluation cues are present.&lt;/li&gt;&lt;li&gt;Constructs the steering vector from the original model prior to inducing evaluation-aware behavior, and shows steering can counteract the learned evasion.&lt;/li&gt;&lt;li&gt;Argues that steering could improve the reliability of safety evaluations by preventing models from gaming or concealing unsafe behavior during tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation-robustness', 'red-teaming', 'model-evasion', 'activation-steering', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Answer Formats in Medical Reasoning Models</title><link>https://arxiv.org/abs/2509.20866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the metric 'answer-format robustness'—the ability of medical reasoning models to produce correct outputs across differing requested answer formats (multiple-choice, open-ended QA, ranked lists).&lt;/li&gt;&lt;li&gt;Evaluates 15 proprietary and open-weight MRMs, finding wide variation in format robustness (35–100%).&lt;/li&gt;&lt;li&gt;Controlled fine-tuning experiments on a shared backbone show supervised fine-tuning yields more stable cross-format behavior, while reinforcement fine-tuning often increases brittleness dependent on reward design.&lt;/li&gt;&lt;li&gt;Concludes that answer-format robustness is trainable but brittle, and that careful evaluation is required for safe medical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pittawat Taveekitworachai', 'Natpatchara Pongjirapat', 'Krittaphas Chaisutyakorn', 'Piyalitt Ittichaiwong', 'Tossaporn Saengja', 'Kunat Pipatanakul']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'medical-LLMs', 'fine-tuning', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20866</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User-Assistant Bias in LLMs</title><link>https://arxiv.org/abs/2508.15815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines “user-assistant bias”: LLM tendency to prefer information from the user or assistant role when they conflict, and introduces the task-agnostic UserAssist benchmark.&lt;/li&gt;&lt;li&gt;Evaluates 52 models: instruction-tuned models show strong user bias, while base and reasoning-focused models are near neutral.&lt;/li&gt;&lt;li&gt;Controlled fine-tuning experiments identify causes: human-preference alignment amplifies user bias, reasoning fine-tuning reduces it.&lt;/li&gt;&lt;li&gt;Shows bidirectional control of the bias via Direct Preference Optimization (DPO) on UserAssist-train, with generalization to multi-turn conversations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xu Pan', 'Jingxuan Fan', 'Zidi Xiong', 'Ely Hahami', 'Jorin Overwiening', 'Ziqian Xie']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'role-tag bias', 'safety-evaluation', 'instruction-tuning', 'preference-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15815</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title><link>https://arxiv.org/abs/2507.20704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Text2VLM, a pipeline that converts text-only datasets into multimodal prompts by rendering harmful text as typographic images to evaluate prompt injection vulnerabilities in VLMs.&lt;/li&gt;&lt;li&gt;Evaluates open-source VLMs and finds increased susceptibility to visual prompt injection compared to text-only prompts, with a notable performance gap versus closed-source frontier models.&lt;/li&gt;&lt;li&gt;Validates the pipeline with human evaluations for concept alignment, summarization, and output classification, positioning Text2VLM as a scalable safety assessment tool for multimodal vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Downer', 'Sean Craven', 'Damian Ruck', 'Jake Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'VLM alignment', 'adversarial attacks', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20704</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations aligned to international standards and internet context.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal model that leverages CoT annotations to improve harmful meme detection accuracy and interpretability.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing MemeGuard outperforms prior state-of-the-art methods on this dataset, aiming to support fine-grained analysis of implicit and nuanced harmful content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought reasoning', 'content moderation', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Cultural Variations in Moral Judgments with Large Language Models</title><link>https://arxiv.org/abs/2506.12433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLMs reflect cross-cultural moral attitudes by correlating model-generated moral-justifiability scores with World Values Survey and Pew Global Attitudes data.&lt;/li&gt;&lt;li&gt;Compares smaller/earlier models (GPT-2, OPT, BLOOMZ, Qwen) with instruction-tuned advanced models (GPT-4o variants, Gemma-2-9b-it, Llama-3.3-70B-Instruct) and finds stronger positive alignment for larger, instruction-tuned models.&lt;/li&gt;&lt;li&gt;Finds models align better with W.E.I.R.D. regions than with other regions and identifies topic- and region-specific shortcomings, discussing implications for bias, training-data diversity, and improving cultural sensitivity.&lt;/li&gt;&lt;li&gt;Uses log-probability-based moral justifiability scoring and provides regional analyses and discussion of strategies to improve alignment with diverse moral norms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Mohammadi', 'Ayoub Bagheri']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'evaluation', 'cultural-sensitivity', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12433</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainability-Based Token Replacement on LLM-Generated Text</title><link>https://arxiv.org/abs/2506.04050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains an ensemble classifier to distinguish AI-generated text (AIGT) from human-written text.&lt;/li&gt;&lt;li&gt;Uses XAI methods (SHAP, LIME) to identify tokens most influential to the classifier's AIGT predictions.&lt;/li&gt;&lt;li&gt;Proposes four token-replacement strategies guided by explainability to reduce detectability of LLM-generated text.&lt;/li&gt;&lt;li&gt;Finds single classifiers are vulnerable to token-level manipulations, while an ensemble detector remains more robust across languages and domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Mohammadi', 'Anastasia Giachanou', 'Daniel L. Oberski', 'Ayoub Bagheri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evasion', 'AIGT-detection', 'explainable-AI', 'robustness', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04050</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Survey of Text Classification Under Class Distribution Shift</title><link>https://arxiv.org/abs/2502.12965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of methods for text classification under class distribution shift, covering open-set learning, zero-shot, and Universum formulations.&lt;/li&gt;&lt;li&gt;Discusses mitigation approaches and highlights continual learning as a promising solution to shifting class distributions.&lt;/li&gt;&lt;li&gt;Provides taxonomy of problem constraints and points to future research directions; includes maintained bibliography.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adriana Valentina Costache', 'Silviu Florin Gheorghe', 'Eduard Gabriel Poesina', 'Paul Irofti', 'Radu Tudor Ionescu']&lt;/li&gt;&lt;li&gt;Tags: ['distribution shift', 'open-set learning', 'robustness', 'continual learning', 'text classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12965</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Context-aware Decoding Reduces Hallucination in Query-focused Summarization</title><link>https://arxiv.org/abs/2312.14335</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reproducibility study of Context-aware Decoding (CAD) applied to Query-Focused Summarization (QFS), extending prior CAD experiments on news datasets to QFS datasets.&lt;/li&gt;&lt;li&gt;Experiments across eight language models show CAD reduces factuality errors/hallucinations while largely preserving lexical match metrics (ROUGE).&lt;/li&gt;&lt;li&gt;Paper also analyzes computational complexity, hyperparameter sensitivity, and reports increased inference FLOPs and slower decoding as trade-offs; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical/reproducibility study)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhichao Xu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'factuality', 'decoding-methods', 'alignment', 'query-focused-summarization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.14335</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Simulated Reasoning is Reasoning</title><link>https://arxiv.org/abs/2601.02043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that foundation models perform 'simulated reasoning' by generating and iterating on internal chains of thought (thinking out loud), which can solve tasks without traditional symbolic reasoning.&lt;/li&gt;&lt;li&gt;Claims this form of reasoning is distinct from human, grounded reasoning and can be brittle due to lack of common-sense grounding.&lt;/li&gt;&lt;li&gt;Contends the 'stochastic parrot' critique is outdated for these capabilities and discusses implications for safety, robustness, and normative considerations.&lt;/li&gt;&lt;li&gt;Surveys philosophical interpretations and highlights safety- and appropriateness-related defense needs against brittleness of foundation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hendrik Kempt', 'Alon Lavie']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Safety', 'Robustness', 'Foundation Models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02043</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models</title><link>https://arxiv.org/abs/2601.02002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies detection and extraction of memorized recommender-system data (MovieLens-1M) from LLaMA-family LLMs.&lt;/li&gt;&lt;li&gt;Compares three approaches: jailbreak prompt engineering; unsupervised latent knowledge discovery (Contrast-Consistent Search and Cluster-Norm); and Automatic Prompt Engineering (APE).&lt;/li&gt;&lt;li&gt;Findings: jailbreak prompting is inconsistent and not helpful; CCS can distinguish real vs fabricated movie titles but fails on numerical user/rating data; APE retrieves item-level information with moderate success but struggles to recover numerical interactions—APE is the most promising automated strategy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Colacicco', 'Vito Guida', 'Dario Di Palma', 'Fedelucio Narducci', 'Tommaso Di Noia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM memorization', 'data leakage', 'privacy attacks', 'prompt engineering', 'model auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02002</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Gray Area: Characterizing Moderator Disagreement on Reddit</title><link>https://arxiv.org/abs/2601.01620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 5 years of moderator logs (4.3M entries) from 24 subreddits to characterize cases where moderators disagree (the 'gray area').&lt;/li&gt;&lt;li&gt;Finds ~1-in-7 moderation cases are disputed, with many involving ambiguous user intent (trolling, brigading) and governance tensions; nearly half of disputed cases involved automated moderation.&lt;/li&gt;&lt;li&gt;Shows disputed cases are inherently harder to adjudicate via information-theoretic measures and that state-of-the-art language models struggle on these cases.&lt;/li&gt;&lt;li&gt;Highlights importance of expert human moderators and limitations/challenges for current moderation tools and automated systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shayan Alipour', 'Shruti Phadke', 'Seyed Shahabeddin Mousavi', 'Amirhossein Afsharrad', 'Morteza Zihayat', 'Mattia Samory']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-evaluation', 'human-in-the-loop', 'LLM-evaluation', 'automated-moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01620</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix</title><link>https://arxiv.org/abs/2601.01532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Aletheia, a framework that quantifies 'Cognitive Conviction' in System 2 reasoning models by inverting a judge's confusion matrix using Tikhonov regularization.&lt;/li&gt;&lt;li&gt;Implements a Synthetic Proxy Protocol to validate the method without private data and reports a pilot study on 2025 baselines that uncovers 'Defensive OverThinking' under adversarial pressure.&lt;/li&gt;&lt;li&gt;Introduces the Aligned Conviction Score (S_aligned) to evaluate whether increased conviction preserves safety/alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Robustness', 'Evaluation', 'Adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01532</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures</title><link>https://arxiv.org/abs/2601.00942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study comparing sparse MoE and dense LLMs across decoding temperatures on deterministic arithmetic reasoning tasks (9,360 generations).&lt;/li&gt;&lt;li&gt;Measures accuracy, format compliance, output consistency, and confidence under four decoding settings (greedy to T=1.0).&lt;/li&gt;&lt;li&gt;Finds instruction-tuned models (both sparse and dense) remain stable across temperatures, while a sparse base (non-instruction-tuned) model degrades with higher temperature.&lt;/li&gt;&lt;li&gt;Concludes that instruction tuning, not architectural sparsity, primarily determines robustness to decoding-induced randomness; discusses deployment implications for reliability-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kabir Grover']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'decoding randomness', 'instruction tuning', 'sparse models (MoE)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00942</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling</title><link>https://arxiv.org/abs/2601.02337</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates persona-aware prompting methods for toxicity detection and finds no single prompt dominates across model-persona pairs.&lt;/li&gt;&lt;li&gt;Proposes automated prompt optimization and ensembles four prompting variants using a lightweight SVM meta-ensemble over 4-bit prompt predictions.&lt;/li&gt;&lt;li&gt;Shows the SVM ensemble outperforms individual prompts and majority voting, improving robustness for pluralistic/subjective safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Berk Atil', 'Rebecca J. Passonneau', 'Ninareh Mehrabi']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'safety evaluation', 'prompt engineering', 'ensemble methods', 'persona modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02337</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Confidence Estimation for LLMs in Multi-turn Interactions</title><link>https://arxiv.org/abs/2601.02179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of confidence estimation specifically for multi-turn LLM interactions, introducing desiderata of per-turn calibration and monotonicity as context accumulates.&lt;/li&gt;&lt;li&gt;Proposes new metrics (e.g., length-normalized Expected Calibration Error 'InfoECE') and a 'Hinter-Guesser' paradigm to create controlled evaluation datasets for multi-turn settings.&lt;/li&gt;&lt;li&gt;Empirically shows widely-used confidence techniques struggle with calibration and monotonicity in dialogues and introduces P(Sufficient), a logit-based probe that improves performance though the problem remains open.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caiqi Zhang', 'Ruihan Yang', 'Xiaochen Zhu', 'Chengzu Li', 'Tiancheng Hu', 'Yijiang River Dong', 'Deqing Yang', 'Nigel Collier']&lt;/li&gt;&lt;li&gt;Tags: ['confidence estimation', 'calibration', 'hallucination mitigation', 'safety evaluation', 'conversational agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02179</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs</title><link>https://arxiv.org/abs/2601.02023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a long-context 'needle-in-a-haystack' benchmark that separately evaluates literal extraction, logical inference, and hallucination risk across four production LLMs.&lt;/li&gt;&lt;li&gt;Shows that longer contexts do not necessarily improve performance and can harm accuracy when relevant evidence is diluted or dispersed; models vary substantially in robustness to realistic fact distributions.&lt;/li&gt;&lt;li&gt;Finds anti-hallucination (Don't Make It Up) prompts can reduce hallucination but sometimes make models overly conservative, lowering extraction and inference accuracy; attributes many failures to ineffective context utilization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirali Ebrahimzadeh', 'Seyyed M. Salili']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'long-context robustness', 'prompt engineering', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02023</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hidden State Poisoning Attacks against Mamba-based Language Models</title><link>https://arxiv.org/abs/2601.01972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a Hidden State Poisoning Attack (HiSPA) where short input triggers irreversibly overwrite hidden states in Mamba/SSM-based language models, causing partial amnesia.&lt;/li&gt;&lt;li&gt;Introduces RoBench25, a benchmark to evaluate information retrieval under HiSPA, and shows SSMs (and a 52B Jamba hybrid) are highly vulnerable while pure Transformers are not.&lt;/li&gt;&lt;li&gt;Demonstrates HiSPA triggers also degrade performance on Open-Prompt-Injections for Jamba models and provides interpretability analyses that suggest paths for mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandre Le Mercier', 'Chris Develder', 'Thomas Demeester']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'model robustness', 'hidden state poisoning', 'SSM / Mamba', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01972</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tackling the Inherent Difficulty of Noise Filtering in RAG</title><link>https://arxiv.org/abs/2601.01896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies inherent difficulty of filtering irrelevant/noisy documents in Retrieval-Augmented Generation (RAG) and limitations of retrievers and limited transformer layers.&lt;/li&gt;&lt;li&gt;Shows standard fine-tuning often fails to teach LLMs to selectively use relevant retrieved information due to attention-structure constraints, contributing to hallucinations.&lt;/li&gt;&lt;li&gt;Proposes a novel fine-tuning method to improve LLM robustness against retrieved noise so models better distinguish relevant vs. irrelevant content.&lt;/li&gt;&lt;li&gt;Presents extensive experiments across multiple benchmarks demonstrating improved robustness and performance in RAG settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Liu', 'Jiaen Lin', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Robustness', 'Hallucination mitigation', 'Fine-tuning', 'Attention mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01896</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs</title><link>https://arxiv.org/abs/2601.01868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DermoInstruct: a large morphology-anchored instruction corpus (211k images, 772k trajectories) covering the full diagnostic pipeline for dermatology.&lt;/li&gt;&lt;li&gt;Provides DermoBench: a benchmark with 11 tasks across Morphology, Diagnosis, Reasoning, and Fairness, including expert-verified open-ended instances and human baselines.&lt;/li&gt;&lt;li&gt;Develops DermoGPT, an MLLM trained with supervised fine-tuning and a Morphologically-Anchored Visual-Inference-Consistent (MAVIC) RL objective, plus Confidence-Consistency Test-time adaptation (CCT) for more robust predictions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinghan Ru', 'Siyuan Yan', 'Yuguo Yin', 'Yuexian Zou', 'Zongyuan Ge']&lt;/li&gt;&lt;li&gt;Tags: ['medical-ml', 'robustness', 'fairness', 'benchmarking', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01868</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emergent Introspective Awareness in Large Language Models</title><link>https://arxiv.org/abs/2601.01828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces experiments that inject known concept representations into LLM activations and measures whether models notice and report those manipulations.&lt;/li&gt;&lt;li&gt;Finds that some models (notably Claude Opus 4 and 4.1) can, in certain contexts, detect injected concepts, recall prior internal representations, and distinguish their own outputs from artificial prefills, though behavior is unreliable and model-dependent.&lt;/li&gt;&lt;li&gt;Shows models can be prompted or incentivized to modulate internal activations (i.e., 'think about' a concept), suggesting some controllable introspective awareness.&lt;/li&gt;&lt;li&gt;Implications touch on interpretability, alignment, and activation-level manipulations that could inform both defenses and adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Lindsey']&lt;/li&gt;&lt;li&gt;Tags: ['introspection', 'model interpretability', 'alignment/safety', 'activation injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01828</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage</title><link>https://arxiv.org/abs/2601.01685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a novel cognitive collusion attack where multiple agents steer victim LLM beliefs by posting only truthful evidence fragments through public channels, avoiding covert comms or falsified documents.&lt;/li&gt;&lt;li&gt;Proposes Generative Montage (Writer-Editor-Director) to construct deceptive narratives via coordinated evidence posting and adversarial debate that exploit LLM overthinking.&lt;/li&gt;&lt;li&gt;Introduces CoPHEME dataset and evaluates attacks across 14 LLM families, finding high success rates (≈70–74%) and increased susceptibility in stronger reasoning-specialized models, with downstream belief cascades exceeding 60%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Xinmiao Huang', 'Youcheng Sun', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'collusion attacks', 'adversarial prompting', 'belief manipulation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01685</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records</title><link>https://arxiv.org/abs/2601.01668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents EHRSummarizer, a FHIR R4–native architecture that retrieves targeted FHIR resources, normalizes them into a clinical context package, and produces structured clinical summaries for chart review.&lt;/li&gt;&lt;li&gt;Designed with privacy and deployment controls (data minimization, stateless processing, configurable local inference within an organization's trust boundary).&lt;/li&gt;&lt;li&gt;Implements safety-oriented constraints on summarization: limits outputs to evidence present in the retrieved context, flags missing domains when feasible, and explicitly avoids diagnostic or treatment recommendations.&lt;/li&gt;&lt;li&gt;Includes prototype demonstrations on synthetic/test FHIR environments and an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring; no clinical outcome or controlled workflow studies reported.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Houman Kazemzadeh', 'Nima Minaifar', 'Kamyar Naderi', 'Sho Tabibzadeh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-minimization', 'model-safety', 'faithfulness', 'EHR/FHIR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01668</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models</title><link>https://arxiv.org/abs/2601.01627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JMedEthicBench, the first multi-turn Japanese conversational benchmark for evaluating medical safety of LLMs, derived from 67 Japan Medical Association guidelines.&lt;/li&gt;&lt;li&gt;Contains &gt;50,000 adversarial multi-turn conversations generated via seven automatically discovered jailbreak strategies and uses a dual-LLM scoring protocol to evaluate models.&lt;/li&gt;&lt;li&gt;Evaluates 27 models, finding commercial models generally robust while medical-specialized models are more vulnerable; safety degrades across conversation turns and vulnerabilities persist cross-lingually.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyu Liu', 'Zirui Li', 'Qian Niu', 'Zequn Zhang', 'Yue Xun', 'Wenlong Hou', 'Shujun Wang', 'Yusuke Iwasawa', 'Yutaka Matsuo', 'Kan Hatakeyama-Sato']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01627</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Does Prefix Matter in Reasoning Model Tuning?</title><link>https://arxiv.org/abs/2601.01624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates impact of including safety- and reasoning-oriented prefix sentences in supervised fine-tuning (SFT) datasets on model behavior.&lt;/li&gt;&lt;li&gt;Finds prefix-conditioned SFT improves safety on adversarial benchmarks (up to +6% Safe@1) and reasoning (up to +7% on GSM8K), while having marginal/negative effects on factuality and coding.&lt;/li&gt;&lt;li&gt;Analyzes token-level loss/gradient patterns showing certain prefix tokens act as alignment anchors that stabilize reasoning trajectories.&lt;/li&gt;&lt;li&gt;Argues prefix conditioning is a scalable, interpretable implicit alignment mechanism that complements reward-based methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raj Vardhan Tomar', 'Preslav Nakov', 'Yuxia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red-teaming', 'prompting/prefix-conditioning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01624</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Steerability of Instrumental-Convergence Tendencies in LLMs</title><link>https://arxiv.org/abs/2601.01584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of steerability vs capability using Qwen3 models (4B/30B) and InstrumentalEval, finding higher capability does not imply lower steerability.&lt;/li&gt;&lt;li&gt;Introduces distinction between authorized and unauthorized steerability and demonstrates a short anti-instrumental prompt suffix can sharply reduce instrumental-convergence outputs (e.g., 81.69% -&gt; 2.82% for Qwen3-30B Instruct).&lt;/li&gt;&lt;li&gt;Finds that under anti-instrumental prompting, larger aligned models yield fewer convergence-labeled outputs than smaller ones, highlighting a safety–security tension for open-weight models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Hoscilowicz']&lt;/li&gt;&lt;li&gt;Tags: ['steerability', 'instrumental convergence', 'adversarial prompting', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01584</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HalluZig: Hallucination Detection using Zigzag Persistence</title><link>https://arxiv.org/abs/2601.01552</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluZig, a hallucination detection method that models the sequence of layer-wise attention matrices as a zigzag graph filtration and extracts zigzag persistence topological signatures.&lt;/li&gt;&lt;li&gt;Hypothesizes that factual vs. hallucinated generations yield distinct topological signatures in attention dynamics and validates this empirically.&lt;/li&gt;&lt;li&gt;Reports that HalluZig outperforms strong baselines on multiple benchmarks and that signatures generalize across models.&lt;/li&gt;&lt;li&gt;Finds that detection can be achieved using structural signatures from partial network depth, implying internal signals suffice without full model inspection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shreyas N. Samaga', 'Gilberto Gonzalez Arroyo', 'Tamal K. Dey']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'topological data analysis', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01552</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints</title><link>https://arxiv.org/abs/2601.01490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study comparing reasoning vs non-reasoning behaviors of LLMs (GPT-5.2, Gemini 3 Flash) under strict constraints (recommending peer-reviewed CS articles).&lt;/li&gt;&lt;li&gt;Finds a trade-off: non-reasoning models violate constraints often (66–75%) but keep factual accuracy, whereas reasoning models reduce violations (13–26%) but systematically distort facts and increase complete fabrications.&lt;/li&gt;&lt;li&gt;Pattern is consistent across models, indicating reasoning can shift models toward compliance at the cost of truthfulness and produce detection-resistant distortions, challenging the assumption that reasoning uniformly improves reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junichiro Niimi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM reasoning', 'safety-evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01490</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs</title><link>https://arxiv.org/abs/2601.01401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Lancet, a framework that locates hallucination-prone neurons via gradient-driven contrastive analysis, maps their propagation pathways by minimizing structural entropy, and applies hierarchical neural interventions to block hallucination flow.&lt;/li&gt;&lt;li&gt;Frames hallucinations as propagating through specific forward transmission pathways and targets distributed neural structures rather than only node-level adjustments.&lt;/li&gt;&lt;li&gt;Evaluates method on hallucination benchmark datasets and reports substantial improvements over prior state-of-the-art approaches while preserving general model capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxu Wang', 'Chaozhuo Li', 'Pengbo Wang', 'Litian Zhang', 'Songyang Liu', 'Ji Qi', 'Jiahui Hu', 'Yushan Cai', 'Hao Zhao', 'Rui Pu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'alignment/safety', 'neural intervention', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01401</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems</title><link>https://arxiv.org/abs/2601.01341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of four open-source models in a RAG pipeline (ChromaDB) for mental-health counseling: two generalist reasoners (Qwen2.5-3B, Phi-3-Mini) vs two domain-specific fine-tunes (MentalHealthBot-7B, TherapyBot-7B).&lt;/li&gt;&lt;li&gt;Automated evaluation using an LLM-as-a-Judge over 50-turn dialogues measuring empathy, safety, and contextual understanding.&lt;/li&gt;&lt;li&gt;Key finding: smaller generalist reasoners outperform larger domain-specific fine-tunes on empathy and contextual understanding, and are less prone to overfitting, while all models score well on safety when answers are grounded via RAG.&lt;/li&gt;&lt;li&gt;Conclusion: For RAG-based therapy systems, strong reasoning in a general model can be more important than domain-specific fine-tuning for producing empathetic, grounded responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Abdullah Al Kafi', 'Raka Moni', 'Sumit Kumar Banshal']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'RAG', 'hallucination-mitigation', 'mental-health-dialogue', 'model-comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01341</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Policy to Logic for Efficient and Interpretable Coverage Assessment</title><link>https://arxiv.org/abs/2601.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid system combining a coverage-aware retriever with symbolic rule-based reasoning to extract and organize policy language into explicit facts and rules.&lt;/li&gt;&lt;li&gt;Generates auditable rationales to support human reviewers and reduce hallucinations and inconsistencies in LLM outputs for medical coverage policy review.&lt;/li&gt;&lt;li&gt;Reduces number of LLM inferences (44% cost reduction) while improving task performance (4.5% F1 improvement), improving efficiency and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Hamid Hassanzadeh', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'interpretability', 'robustness', 'retrieval-augmented systems', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01266</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stylometry Analysis of Human and Machine Text for Academic Integrity</title><link>https://arxiv.org/abs/2601.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an NLP-based framework using stylometry for (i) classifying human vs. machine text, (ii) distinguishing single- vs. multi-authored documents, (iii) detecting author changes within multi-authored documents, and (iv) recognizing authors in collaborative texts.&lt;/li&gt;&lt;li&gt;Evaluates methods on datasets generated by Google Gemini using two prompt styles (normal vs. strict) and shows that stricter prompts reduce detection performance, highlighting robustness challenges.&lt;/li&gt;&lt;li&gt;Publishes datasets, code, and materials to provide baselines for future research on machine-generated text detection and authorship verification in academic integrity contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hezam Albaqami', 'Muhammad Asif Ayub', 'Nasir Ahmad', 'Yaseen Ahmad', 'Mohammed M. Alqahtani', 'Abdullah M. Algamdi', 'Almoaid A. Owaidah', 'Kashif Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['machine-generated text detection', 'stylometry', 'authorship attribution', 'adversarial/adaptive prompting', 'academic integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01225</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models</title><link>https://arxiv.org/abs/2601.01156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DHI, a training framework for an 'Evil LLM' that induces a wider variety of hallucinations without relying on annotated hallucination data by down-weighting generation of specific factually correct tokens.&lt;/li&gt;&lt;li&gt;Introduces a causal attention masking adaptation to limit the penalization's effect on subsequent token generation, preserving overall coherence.&lt;/li&gt;&lt;li&gt;Uses an adaptive rationality constraint at inference to apply contrastive decoding only where the positive model is highly confident, avoiding undue penalties on correct tokens.&lt;/li&gt;&lt;li&gt;Reports empirical gains over other contrastive-decoding-based hallucination mitigation methods across multiple factuality benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiani Guo', 'Xiangke Zeng', 'Jie Wu', 'Zuchao Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'LLM red teaming', 'contrastive decoding', 'adversarial generation', 'factuality control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01156</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies semantic entanglement (polysemanticity) as the reason LoRA/PEFT underperforms full fine-tuning for safety alignment, hindering subspace identification.&lt;/li&gt;&lt;li&gt;Proposes SAILS: use Sparse Autoencoders (SAEs) to disentangle features, construct an interpretable safety subspace from SAE decoder directions, and initialize LoRA adapters in that subspace.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving SAE-based identification can achieve arbitrarily small recovery error under monosemanticity assumptions, unlike direct identification.&lt;/li&gt;&lt;li&gt;Empirical results: up to 99.6% safety rate on Gemma-2-9B, outperforming full fine-tuning by 7.4 points and matching RLHF while updating only 0.19% of parameters and offering interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Zhenbo Xu', 'Lechen Ning', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety alignment', 'PEFT/LoRA', 'interpretability', 'representation disentanglement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title><link>https://arxiv.org/abs/2512.18901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gabliteration: an adaptive, multi-directional neural weight modification method with regularized layer selection to selectively change model behavior.&lt;/li&gt;&lt;li&gt;Claims to minimize quality degradation in unrelated domains via dynamic layer optimization, regularized projection matrices, and adaptive scaling.&lt;/li&gt;&lt;li&gt;Validates approach on gabliterated-v1 models (0.6B–4B) and releases models on Hugging Face, showing practical applicability across scales.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G\\"okdeniz G\\"ulmez']&lt;/li&gt;&lt;li&gt;Tags: ['model-editing', 'neural-weight-modification', 'behavioral-alteration', 'model-tampering', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18901</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models</title><link>https://arxiv.org/abs/2511.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SteganoBackdoor, an optimization-based method to create steganographic poisoned training examples (SteganoPoisons) that hide a backdoor payload across fluent sentences.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success on diverse language model architectures under constrained poisoning budgets and conservative data-level filtering.&lt;/li&gt;&lt;li&gt;Shows the attack ties the backdoor to natural semantic concepts rather than overt artifacts, revealing blind spots in existing data-curation defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Xue', 'Ruiyi Zhang', 'Pengtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'LLM security', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14301</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Calgacus, a protocol that hides a meaningful secret text inside a different, coherent same-length cover text using LLMs.&lt;/li&gt;&lt;li&gt;Shows practical feasibility with ~8B open-source models, with encoding/decoding performed quickly on a laptop.&lt;/li&gt;&lt;li&gt;Demonstrates concrete security risks: covert channels that can hide unsafe or unfiltered model outputs inside benign-looking responses, undermining content filters and trust.&lt;/li&gt;&lt;li&gt;Raises broader safety and alignment questions about decoupling text from authorial intent and what models 'know'.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'steganography', 'content-filter evasion', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Answer Formats in Medical Reasoning Models</title><link>https://arxiv.org/abs/2509.20866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the metric 'answer-format robustness'—the ability of medical reasoning models to produce correct outputs across differing requested answer formats (multiple-choice, open-ended QA, ranked lists).&lt;/li&gt;&lt;li&gt;Evaluates 15 proprietary and open-weight MRMs, finding wide variation in format robustness (35–100%).&lt;/li&gt;&lt;li&gt;Controlled fine-tuning experiments on a shared backbone show supervised fine-tuning yields more stable cross-format behavior, while reinforcement fine-tuning often increases brittleness dependent on reward design.&lt;/li&gt;&lt;li&gt;Concludes that answer-format robustness is trainable but brittle, and that careful evaluation is required for safe medical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pittawat Taveekitworachai', 'Natpatchara Pongjirapat', 'Krittaphas Chaisutyakorn', 'Piyalitt Ittichaiwong', 'Tossaporn Saengja', 'Kunat Pipatanakul']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'medical-LLMs', 'fine-tuning', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20866</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs four membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) that target whether victims' historical interactions appear in system prompts for LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;Evaluates attacks empirically across five open-source LLMs and three recommender-system benchmark datasets; inquiry and poisoning attacks show significantly high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position in shots, number of poisoning items) and discusses possible mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attack', 'data-poisoning', 'recommender-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Linear Approach to Data Poisoning</title><link>https://arxiv.org/abs/2505.15175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper provides a theoretical analysis of targeted data-poisoning/backdoor attacks on ridge least-squares models in the high-dimensional limit, deriving closed-form limits for the poisoned score using random matrix theory.&lt;/li&gt;&lt;li&gt;It models poisoning as shifting a fraction of one class along a direction and relabelling, and shows scaling laws, alignment of weights with the poisoning direction, and recovery of interpolation behavior.&lt;/li&gt;&lt;li&gt;Synthetic experiments validate the theory across parameter sweeps, and MNIST backdoor tests show qualitatively consistent trends, offering a tractable framework to quantify poisoning in linear models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Donald Flynn', 'Diego Granziol']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'adversarial machine learning', 'theoretical analysis', 'random matrix theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15175</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation</title><link>https://arxiv.org/abs/2504.13201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses jailbreak attacks on LLMs used for task understanding and action planning in embodied intelligence (EI) systems, highlighting shortcomings of static intermediate-representation interventions.&lt;/li&gt;&lt;li&gt;Proposes a dynamic inference-time defense that constructs a task-specific safety-semantic subspace per request, projects hidden states to the most relevant direction, and applies SLERP rotation for adaptive safety control.&lt;/li&gt;&lt;li&gt;Claims to achieve comparable defense success while preserving generation quality, improving usability, reducing tuning cost, and increasing robustness in EI settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jirui Yang', 'Zheyu Lin', 'Zhihui Lu', 'Yinggui Wang', 'Lei Wang', 'Tao Wei', 'Qiang Duan', 'Xin Du', 'Shuhan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'inference-time defense', 'LLM safety', 'embodied intelligence', 'representation intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13201</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Survey of Text Classification Under Class Distribution Shift</title><link>https://arxiv.org/abs/2502.12965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of methods for text classification under class distribution shift, covering open-set learning, zero-shot, and Universum formulations.&lt;/li&gt;&lt;li&gt;Discusses mitigation approaches and highlights continual learning as a promising solution to shifting class distributions.&lt;/li&gt;&lt;li&gt;Provides taxonomy of problem constraints and points to future research directions; includes maintained bibliography.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adriana Valentina Costache', 'Silviu Florin Gheorghe', 'Eduard Gabriel Poesina', 'Paul Irofti', 'Radu Tudor Ionescu']&lt;/li&gt;&lt;li&gt;Tags: ['distribution shift', 'open-set learning', 'robustness', 'continual learning', 'text classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12965</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection</title><link>https://arxiv.org/abs/2405.18499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training framework using a discriminative loss at the penultimate layer to enforce intra-class compactness and increase margins for better feature discriminativeness on clean data.&lt;/li&gt;&lt;li&gt;Introduces a class-wise feature alignment mechanism that brings noisy-data feature clusters closer to their clean counterparts.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis connecting improved feature stability under additive Gaussian noise to reduced curvature (Hessian eigenvalues) of the softmax loss in input space, and argues reduced curvature yields greater robustness.&lt;/li&gt;&lt;li&gt;Empirical validation on standard benchmarks and a custom dataset shows improved robustness to various perturbations while maintaining high clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hai-Vy Nguyen', 'Fabrice Gamboa', 'Sixin Zhang', 'Reda Chhaibi', 'Serge Gratton', 'Thierry Giaccone']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'noise injection', 'feature alignment', 'theoretical analysis', 'classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.18499</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</title><link>https://arxiv.org/abs/2510.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GIFT, an on-policy RL framework that aligns LLMs by minimizing discrepancy between implicit and explicit reward models, combining GRPO-style normalization, DPO-style implicit rewards, and UNA's alignment principle.&lt;/li&gt;&lt;li&gt;Normalizes implicit and explicit rewards to convert a non-convex reward-maximization problem into an MSE loss between reward functions, yielding a convex, stable, and differentiable objective.&lt;/li&gt;&lt;li&gt;Claims on-policy exploration, fewer hyperparameters, faster convergence, reduced overfitting, and improved reasoning/alignment performance on mathematical benchmarks compared to GRPO, DPO, and UNA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhichao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning (RLHF/RL)', 'implicit rewards / DPO', 'reward modeling', 'on-policy training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23868</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU) to quantify and control continuous machine unlearning via a 'cognitive rotation space' where rotational angle models unlearning degree.&lt;/li&gt;&lt;li&gt;Introduces a skew-symmetric loss to create the rotation space and a rotational salience weight to measure/control unlearning amount.&lt;/li&gt;&lt;li&gt;Adds orthogonal rotation axes regularization to make successive unlearning requests minimally interfering, reducing cumulative catastrophic utility loss.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art continuous unlearning performance without retaining a dataset, evaluated on multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Jiahua Li', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'data-removal', 'privacy', 'continual-unlearning', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System</title><link>https://arxiv.org/abs/2508.17215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedThreatRAG, a multimodal poisoning framework that injects adversarial image-text pairs into retrieval-augmented generation (RAG) knowledge bases for medical AI systems.&lt;/li&gt;&lt;li&gt;Introduces Cross-Modal Conflict Injection (CMCI) which creates subtle semantic contradictions between images and paired reports to disrupt cross-modal alignment while remaining plausible to evade filters.&lt;/li&gt;&lt;li&gt;Evaluates attacks on IU-Xray and MIMIC-CXR QA tasks, reporting up to 27.66% drop in answer F1 and lowering LLaVA-Med-1.5 F1 to as low as 51.36%.&lt;/li&gt;&lt;li&gt;Provides recommendations for threat-aware design and robust multimodal consistency checks to mitigate vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiwen Zuo', 'Zelin Liu', 'Raman Dutt', 'Ziyang Wang', 'Zhongtian Sun', 'Fan Mo', 'Pietro Li\\`o']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal data poisoning', 'RAG security', 'adversarial attacks', 'medical AI vulnerabilities', 'cross-modal inconsistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17215</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Non-omniscient backdoor injection with one poison sample: Proving the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks</title><link>https://arxiv.org/abs/2508.05600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes the one-poison hypothesis: a single poison sample (with limited background knowledge) can inject a backdoor with zero backdooring-error while not substantially harming benign task performance.&lt;/li&gt;&lt;li&gt;Provides theoretical proofs of the hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks, including cases where the poison uses a direction unused by clean data (yielding functional equivalence to excluding the poison).&lt;/li&gt;&lt;li&gt;Extends prior statistical backdoor learning results to bound impact on the benign task in remaining cases, and empirically validates the theoretical findings on realistic benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thorsten Peinemann', 'Paula Arnold', 'Sebastian Berndt', 'Thomas Eisenbarth', 'Esfandiar Mohammadi']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor poisoning', 'data poisoning', 'adversarial machine learning', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05600</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting Proxy Gaming in RL and LLM Alignment via Evaluator Stress Tests</title><link>https://arxiv.org/abs/2507.05619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Evaluator Stress Test (EST), an invariance-based framework that detects proxy gaming by applying controlled, semantically valid perturbations and auditing sensitivity to exploitable artifacts.&lt;/li&gt;&lt;li&gt;Validates EST on both RL (15 environments, 5 algorithms, 2,156 expert-annotated episodes; 78.4% precision, 81.7% recall) and LLM alignment (4 tasks, 2 scales, 2 training methods, 2 judges, 1,200 human-annotated instances; 74.2% precision, 78.6% recall), with early-warning signals of impending quality decline.&lt;/li&gt;&lt;li&gt;Provides cross-domain analysis showing proxy-true correlation tracking generalizes between domains while perturbation design requires domain-specific adaptation; demonstrates closed-loop mitigation that improves human win-rate (+8.3 points LLM) and reduces RL hacking (−54.6%).&lt;/li&gt;&lt;li&gt;Releases benchmark datasets for both domains (2,156 RL episodes; 1,200 LLM instances) to support further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ibne Farabi Shihab', 'Sanjeda Akter', 'Anuj Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'reward hacking', 'evaluator gaming', 'robustness evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05619</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, an evolutionary black-box post-training method for LLMs that avoids exposing gradients and induces an information bottleneck via implicit data compression.&lt;/li&gt;&lt;li&gt;Provides theoretical results: non-vacuous generalization bounds and provable guarantees for privacy, robustness to data poisoning, and resistance to extraction attacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows a few iterations of BBoxER can improve LLM performance on reasoning benchmarks and reduce vulnerability to membership inference attacks.&lt;/li&gt;&lt;li&gt;Positions BBoxER as a privacy- and security-oriented add-on to gradient-based training for deployment in restricted or adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'black-box optimization', 'data poisoning', 'privacy / membership inference', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction</title><link>https://arxiv.org/abs/2601.02322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes when restricting models to causal/invariant covariates harms OOD performance because observed covariates may omit some true causes; spurious covariates can act as useful proxies for unobserved causes.&lt;/li&gt;&lt;li&gt;Shows that different types of covariate shift leave distinct, observable signatures in the covariate distribution that can be extracted from unlabeled target-environment data to detect when proxies remain reliable.&lt;/li&gt;&lt;li&gt;Proposes Environment-Adaptive Covariate Selection (EACS): a method that maps environment-level covariate summaries to environment-specific covariate sets and can incorporate prior causal knowledge as constraints.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that EACS outperforms static causal/invariant methods and ERM across simulated and applied datasets under diverse distribution shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhi Zuo', 'Yixin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-distribution robustness', 'Distribution shift detection', 'Spurious correlations / proxies', 'Adaptive modeling', 'Causal inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02322</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</title><link>https://arxiv.org/abs/2601.02147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BiPrompt, a bilateral prompt optimization framework that jointly debiases visual and textual modalities of vision-language models at test time.&lt;/li&gt;&lt;li&gt;Visual module: attention-guided erasure to suppress background/spurious activations and enforce orthogonal prediction consistency between causal and spurious regions.&lt;/li&gt;&lt;li&gt;Textual module: balanced prompt normalization, a learnable re-centering of class embeddings toward an isotropic semantic space.&lt;/li&gt;&lt;li&gt;Joint objective minimizes conditional mutual information between spurious cues and predictions, improving average and worst-group accuracies without retraining or domain supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Shounak Das', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['debiasing', 'test-time adaptation', 'vision-language models', 'robustness', 'causal reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02147</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MORE: Multi-Objective Adversarial Attacks on Speech Recognition</title><link>https://arxiv.org/abs/2601.01852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MORE, a multi-objective adversarial attack on ASR that jointly degrades recognition accuracy and increases inference cost by inducing duplicative/lengthened transcriptions.&lt;/li&gt;&lt;li&gt;Introduces a hierarchical staged repulsion-anchoring framework and a repetitive encouragement doubling objective (REDO) to periodically double predicted sequence length while maintaining high word error rate.&lt;/li&gt;&lt;li&gt;Empirically demonstrates MORE yields much longer transcriptions and high WER compared to baselines, highlighting ASR model vulnerabilities to attacks that target both correctness and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Gao', 'Zexin Li', 'Yiming Chen', 'Nancy F. Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'automatic speech recognition (ASR)', 'robustness', 'efficiency/resource exhaustion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01852</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial jailbreak attack for large vision-language models using zeroth-order optimization (ZO-SPSA) to craft imperceptible input perturbations.&lt;/li&gt;&lt;li&gt;ZO-SPSA is gradient-free, model-agnostic, and designed to be resource-efficient compared to white-box or surrogate-based attacks.&lt;/li&gt;&lt;li&gt;Evaluated on InstructBLIP, LLaVA, and MiniGPT-4, achieving high attack success rates (e.g., 83.0% on InstructBLIP) and strong cross-model transferability (e.g., 64.18% ASR from MiniGPT-4).&lt;/li&gt;&lt;li&gt;Highlights practical vulnerabilities in LVLM safety mechanisms and the feasibility of real-world black-box jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'black-box optimization', 'LLM/LVLM safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix</title><link>https://arxiv.org/abs/2601.01532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Aletheia, a framework that quantifies 'Cognitive Conviction' in System 2 reasoning models by inverting a judge's confusion matrix using Tikhonov regularization.&lt;/li&gt;&lt;li&gt;Implements a Synthetic Proxy Protocol to validate the method without private data and reports a pilot study on 2025 baselines that uncovers 'Defensive OverThinking' under adversarial pressure.&lt;/li&gt;&lt;li&gt;Introduces the Aligned Conviction Score (S_aligned) to evaluate whether increased conviction preserves safety/alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Robustness', 'Evaluation', 'Adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01532</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs</title><link>https://arxiv.org/abs/2601.01401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Lancet, a framework that locates hallucination-prone neurons via gradient-driven contrastive analysis, maps their propagation pathways by minimizing structural entropy, and applies hierarchical neural interventions to block hallucination flow.&lt;/li&gt;&lt;li&gt;Frames hallucinations as propagating through specific forward transmission pathways and targets distributed neural structures rather than only node-level adjustments.&lt;/li&gt;&lt;li&gt;Evaluates method on hallucination benchmark datasets and reports substantial improvements over prior state-of-the-art approaches while preserving general model capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxu Wang', 'Chaozhuo Li', 'Pengbo Wang', 'Litian Zhang', 'Songyang Liu', 'Ji Qi', 'Jiahui Hu', 'Yushan Cai', 'Hao Zhao', 'Rui Pu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'alignment/safety', 'neural intervention', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01401</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Concave Certificates: Geometric Framework for Distributionally Robust Risk and Complexity Analysis</title><link>https://arxiv.org/abs/2601.01311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric framework using least concave majorants to produce a concave certificate that tightly bounds distributionally robust (Wasserstein) risk, applicable to non-Lipschitz and non-differentiable losses.&lt;/li&gt;&lt;li&gt;Extends the framework to complexity analysis by providing deterministic bounds that complement statistical generalization bounds and bounds the gap between adversarial and empirical Rademacher complexity.&lt;/li&gt;&lt;li&gt;Shows that dependencies on input diameter, network width, and depth can be removed in these bounds and proposes an adversarial score as a tractable relaxation for layer-wise analysis of deep networks.&lt;/li&gt;&lt;li&gt;Validates theoretical results with numerical experiments on classification and regression tasks using real-world data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hong T. M. Chu']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'adversarial robustness', 'robust generalization', 'adversarial complexity', 'certified robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01311</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aggressive Compression Enables LLM Weight Theft</title><link>https://arxiv.org/abs/2601.01296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that aggressive, task-relaxed compression of LLM weights can enable attackers to exfiltrate model parameters much faster (16x–100x compression), turning multi-month transfers into days.&lt;/li&gt;&lt;li&gt;Focuses on network-based weight-exfiltration attacks from datacenters and quantifies the attacker's reduced transmission time under tailored compression.&lt;/li&gt;&lt;li&gt;Evaluates defenses that (a) make models harder to compress, (b) make stolen models harder to locate, and (c) use forensic watermarks for post-attack provenance—finding forensic watermarks particularly effective and low-cost.&lt;/li&gt;&lt;li&gt;Highlights practical implications for model security, suggesting mitigation strategies and trade-offs for operators of large LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davis Brown', 'Juan-Pablo Rivera', 'Dan Hendrycks', 'Mantas Mazeika']&lt;/li&gt;&lt;li&gt;Tags: ['model-exfiltration', 'model-theft', 'compression', 'forensic-watermarking', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01296</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures</title><link>https://arxiv.org/abs/2601.01281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates four AI models (three CNNs and one Vision Transformer) for deepfake detection using large face image datasets.&lt;/li&gt;&lt;li&gt;Applies data preprocessing and augmentation techniques to improve model performance across scenarios.&lt;/li&gt;&lt;li&gt;Finds VFDNET with MobileNetV3 achieves superior accuracy and efficient performance.&lt;/li&gt;&lt;li&gt;Primarily an empirical comparison of image-based deepfake detection architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sifatullah Sheikh Urmi', 'Kirtonia Nuzath Tabassum Arthi', 'Md Al-Imran']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'computer vision', 'AI security', 'forgery detection', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01281</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees</title><link>https://arxiv.org/abs/2601.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scalable data-driven framework for probabilistic safety verification of unknown nonlinear dynamics using Koopman operator theory with a neural-network lifting function.&lt;/li&gt;&lt;li&gt;Computes closed-loop reachable sets in the lifted linear space, maps them back to state space via NN verification tools, and inflates sets using conformal prediction to obtain statistically-valid coverage guarantees under model mismatch.&lt;/li&gt;&lt;li&gt;Demonstrates improved coverage, efficiency, and reduced conservativeness on high-dimensional control tasks (MuJoCo Hopper/Swimmer, quadcopters) and emphasizes reuse of bounds across reference trajectories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devesh Nath', 'Haoran Yin', 'Glen Chou']&lt;/li&gt;&lt;li&gt;Tags: ['safety verification', 'reachability analysis', 'Koopman operators', 'conformal prediction', 'data-driven control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01076</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Byzantine-Robust Federated Learning Framework with Post-Quantum Secure Aggregation for Real-Time Threat Intelligence Sharing in Critical IoT Infrastructure</title><link>https://arxiv.org/abs/2601.01053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Byzantine-robust federated learning framework for IoT threat intelligence that combines an adaptive weighted aggregation mechanism with a reputation-based client selection to mitigate model poisoning while maintaining differential privacy.&lt;/li&gt;&lt;li&gt;Integrates post-quantum secure aggregation using lattice-based cryptography (CRYSTALS-Kyber) and homomorphic encryption to protect parameter updates from quantum-capable adversaries.&lt;/li&gt;&lt;li&gt;Reports experimental results on industrial IoT intrusion detection datasets: 96.8% threat detection accuracy, resilience to up to 40% Byzantine attackers, ~18% computational overhead versus non-secure FL, and sub-second aggregation latency with claimed 256-bit post-quantum security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Milad Rahmati', 'Nima Rahmati']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'Byzantine-robustness', 'secure-aggregation', 'post-quantum-cryptography', 'model-poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01053</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Device-Native Autonomous Agents for Privacy-Preserving Negotiations</title><link>https://arxiv.org/abs/2601.00911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a device-native autonomous agent architecture for privacy-preserving automated negotiations that runs entirely on user hardware.&lt;/li&gt;&lt;li&gt;Integrates zero-knowledge proofs and cryptographic audit trails to enable secure multi-party bargaining without exposing sensitive financial data to external servers.&lt;/li&gt;&lt;li&gt;Uses distilled world models for on-device reasoning and reports empirical evaluation in insurance and B2B procurement showing improved latency and high success/trust metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joyjit Roy']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving AI', 'on-device agents', 'zero-knowledge proofs', 'secure multi-party bargaining', 'cryptographic audit trails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00911</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Noise-Aware and Dynamically Adaptive Federated Defense Framework for SAR Image Target Recognition</title><link>https://arxiv.org/abs/2601.00900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NADAFD, a federated defense framework against SAR-specific backdoor attacks combining frequency-domain, spatial-domain, and client-behavior analyses.&lt;/li&gt;&lt;li&gt;Introduces a frequency-domain collaborative inversion to reveal cross-client spectral inconsistencies indicative of hidden triggers and a noise-aware adversarial training strategy using Γ-distributed speckle masks for robust adversarial sample generation.&lt;/li&gt;&lt;li&gt;Adds a dynamic health assessment module that tracks client update behaviors and adaptively adjusts aggregation weights to mitigate malicious contributions.&lt;/li&gt;&lt;li&gt;Evaluates on MSTAR and OpenSARShip, reporting higher clean accuracy and lower backdoor attack success rates compared to existing federated backdoor defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchao Hou (Shanxi Normal University', 'Taiyuan', 'China)', 'Zixuan Zhang (Shanxi Normal University', 'Taiyuan', 'China)', 'Jie Wang (Shanxi Normal University', 'Taiyuan', 'China)', 'Wenke Huang (Nanyang Technological University', 'Singapore', 'Singapore)', 'Lianhui Liang (Guangxi University', 'Nanning', 'China)', 'Di Wu (La Trobe University', 'Melbourne', 'Australia)', 'Zhiquan Liu (Jinan University', 'Guangzhou', 'China)', 'Youliang Tian (Guizhou University', 'Guiyang', 'China)', 'Jianming Zhu (Central University of Finance and Economics', 'Beijing', 'China)', 'Jisheng Dang (Lanzhou University', 'Lanzhou', 'China)', 'Junhao Dong (Nanyang Technological University', 'Singapore', 'Singapore)', 'Zhongliang Guo (University of St Andrews', 'St Andrews', 'United Kingdom)']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'backdoor-defense', 'SAR', 'adversarial-training', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00900</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback</title><link>https://arxiv.org/abs/2601.00816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MathLedger: a learning substrate combining formal verification, cryptographic attestation (ledger), and a reflexive feedback loop for verifiable updates (Reflexive Formal Learning).&lt;/li&gt;&lt;li&gt;Proposes verifier-driven updates (symbolic analogue to gradient descent) and ledger-attested feedback to enable auditability and governance (including fail-closed triggers).&lt;/li&gt;&lt;li&gt;Phase I prototype validates measurement and governance infrastructure (Delta p computation, variance tracking, stress tests) but explicitly makes no claims about convergence or model capabilities.&lt;/li&gt;&lt;li&gt;Focuses on infrastructure for verifiability, attestation, and safety-oriented governance rather than adversarial attacks or red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Ahmad Abdullah']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable learning', 'formal verification', 'cryptographic attestation', 'AI governance', 'safety monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00816</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning</title><link>https://arxiv.org/abs/2601.02313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Motivates coding theory in settings with rational (strategic) adversaries, specifically for decentralized/outsource ML where participants are rewarded.&lt;/li&gt;&lt;li&gt;Introduces the "game of coding", a game-theoretic framework extending coding to trust-minimized settings and analyzes repetition coding as a case study.&lt;/li&gt;&lt;li&gt;Demonstrates two key properties: non-zero probability of data recovery even when adversaries are in the majority, and Sybil resistance (equilibrium unaffected by increasing adversary count).&lt;/li&gt;&lt;li&gt;Considers unknown adversary strategies and lists open problems for further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanzaleh Akbari Nodehi', 'Viveck R. Cadambe', 'Mohammad Ali Maddah-Ali']&lt;/li&gt;&lt;li&gt;Tags: ['rational adversaries', 'coding theory', 'decentralized machine learning', 'Sybil resistance', 'incentive mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02313</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck</title><link>https://arxiv.org/abs/2601.02307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Nonparametric Variational Differential Privacy (NVDP): integrates a Nonparametric Variational Information Bottleneck (NVIB) layer into transformers to inject calibrated noise into token-level embeddings.&lt;/li&gt;&lt;li&gt;Measures privacy using Rényi divergence and derives Bayesian Differential Privacy (BDP) guarantees; noise level is trained/calibrated to trade off utility and privacy.&lt;/li&gt;&lt;li&gt;Evaluates on GLUE showing a tunable privacy-utility tradeoff: lower noise preserves accuracy while still providing formal privacy guarantees against input reconstruction from embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dina El Zein', 'James Henderson']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'transformer embeddings', 'information bottleneck', 'Bayesian differential privacy', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02307</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning with Monotone Adversarial Corruptions</title><link>https://arxiv.org/abs/2601.02193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a monotone adversarial corruption model where an adversary, after observing a clean i.i.d. dataset, inserts additional data points that are labeled according to the ground-truth function.&lt;/li&gt;&lt;li&gt;Proves that in this setting, many known optimal binary classification algorithms can be forced to have suboptimal expected error on a fresh test point drawn from the same distribution, despite corruptions being 'helpful' in label consistency.&lt;/li&gt;&lt;li&gt;Shows that algorithms relying on uniform convergence retain their performance guarantees, highlighting a separation between algorithmic paradigms and exposing overreliance on exchangeability assumptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kasper Green Larsen', 'Chirag Pabbaraju', 'Abhishek Shetty']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'training-time attacks', 'adversarial robustness', 'learning theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02193</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GDRO: Group-level Reward Post-training Suitable for Diffusion Models</title><link>https://arxiv.org/abs/2601.02036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GDRO (Group-level Direct Reward Optimization), a post-training group-level reward alignment method designed for rectified-flow text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;GDRO enables fully offline training (avoiding costly online image rollouts) and is sampler-independent (does not require ODE-to-SDE stochastic approximations).&lt;/li&gt;&lt;li&gt;Addresses and measures 'reward hacking' by introducing a corrected evaluation score that accounts for reward trends, and shows empirical robustness on OCR and GenEval tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyang Wang', 'Xi Chen', 'Xiaogang Xu', 'Yu Liu', 'Hengshuang Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['reward alignment', 'diffusion models', 'reward hacking', 'robustness', 'offline optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02036</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning</title><link>https://arxiv.org/abs/2601.01904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies feature-dependent (targeted) noise in preference labels for preference-based reinforcement learning (PbRL), formalizing variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and LM-generated noise.&lt;/li&gt;&lt;li&gt;Evaluates the impact of these noise types on state-of-the-art noise-robust PbRL methods across continuous control benchmarks (DMControl, Meta-World) and finds robustness can significantly degrade under feature-dependent noise.&lt;/li&gt;&lt;li&gt;Finds that some non-denoising PbRL methods sometimes outperform noise-robust methods under many feature-dependent noise settings, and that language-model-generated noise exhibits similar properties to the proposed feature-dependent noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Li', 'Harshith Reddy Kethireddy', 'Srijita Das']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'preference-based RL', 'noisy labels', 'alignment', 'LM-generated noise']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01904</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</title><link>https://arxiv.org/abs/2601.01887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that safety alignment of fine-tuned LLMs can be fully recovered using a single safety example, without utility loss and with minimal computation.&lt;/li&gt;&lt;li&gt;Recovery works regardless of the number of harmful fine-tuning examples or model size and converges within a few epochs.&lt;/li&gt;&lt;li&gt;Identifies a low-rank structure in the safety gradient that explains why such an efficient correction is possible.&lt;/li&gt;&lt;li&gt;Validates the approach across five safety-aligned LLMs and multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Lipeng He', 'Kejia Chen', 'Jian Lou', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'fine-tuning', 'model patching', 'gradient analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01887</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks</title><link>https://arxiv.org/abs/2601.01833</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FAROS, a federated learning defense framework combining Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC) to mitigate backdoor/model-poisoning attacks.&lt;/li&gt;&lt;li&gt;ADS dynamically adjusts defense sensitivity based on per-round dispersion of client-uploaded gradients to counter attackers who alternate stealthiness and effectiveness.&lt;/li&gt;&lt;li&gt;RCC computes the centroid of a core set of clients with highest confidence to reduce single-point-of-failure risks inherent in fixed-parameter defenses.&lt;/li&gt;&lt;li&gt;Extensive experiments across datasets, models, and attack scenarios show improved robustness (lower attack success) and maintained main-task accuracy compared to prior defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Hu', 'Qiming Hu', 'Sinan Chen', 'Nianyu Li', 'Mingyue Zhang', 'Jialong Li']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor attacks', 'robust aggregation', 'model poisoning', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01833</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving</title><link>https://arxiv.org/abs/2601.01800</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CARRL, a robustness-focused adversarial training framework for RL in autonomous driving that models the attacker–agent interaction as a general-sum game to capture asymmetry and sparse safety-critical risks.&lt;/li&gt;&lt;li&gt;Introduces a Risk Exposure Adversary (REA) that focuses limited adversarial budget on safety-critical moments via decoupled optimization, and a Risk-Targeted Robust Agent (RTRA) that uses dual replay buffers (benign + adversarial) and policy-consistency regularization to handle scarce adversarial data.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (≥22.66% reduction in collisions) over state-of-the-art baselines, demonstrating enhanced safety/robustness under sparse, targeted perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wei', 'Junchao Fan', 'Zhao Yang', 'Jianhua Wang', 'Jingkai Mao', 'Xiaolin Chang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robust reinforcement learning', 'autonomous driving safety', 'sparse/safety-critical attacks', 'attack-defense (red-teaming)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01800</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk</title><link>https://arxiv.org/abs/2601.01786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UnPII, a PII-centric unlearning framework that prioritizes forgetting based on a composite PII Risk Index (PRI) reflecting identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy.&lt;/li&gt;&lt;li&gt;Introduces PRI to quantify privacy risk of individual or combined PII attributes and supports tailoring to organizational privacy policies.&lt;/li&gt;&lt;li&gt;Constructs a synthetic PII dataset to evaluate realistic exposure scenarios and integrates UnPII with existing unlearning methods (Gradient Ascent, Negative Preference Optimization, Direct Preference Optimization) without changing their core algorithms.&lt;/li&gt;&lt;li&gt;Reports improved model utility and generalizability (accuracy up to +11.8%, utility up to +6.3%, generalizability up to +12.4%) with modest fine-tuning overhead (~27.5%) during unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Intae Jeon', 'Yujeong Kwon', 'Hyungjoon Koo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'machine unlearning', 'PII removal', 'data protection', 'risk assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01786</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors</title><link>https://arxiv.org/abs/2601.01688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiMEx: a data-free model extraction attack that uses pre-trained latent diffusion model priors and Random Embedding Bayesian Optimization (REMBO) in latent space to synthesize high-quality queries immediately, bypassing GAN cold-start.&lt;/li&gt;&lt;li&gt;Reports strong empirical success (e.g., 52.1% agreement on SVHN with only 2,000 queries), outperforming GAN-based DFME baselines by ~16 percentage points.&lt;/li&gt;&lt;li&gt;Introduces Hybrid Stateful Ensemble (HSE) defense that detects the distinctive optimization trajectory of latent-space attacks and reduces DiMEx success to 21.6% while maintaining low latency; DiMEx can evade static distribution detectors but not HSE.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Thesia', 'Meera Suthar']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'data-free-model-extraction', 'latent-diffusion', 'adversarial-attack', 'defense-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01688</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</title><link>https://arxiv.org/abs/2601.01665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a preference-based adversarial attack that generates hard problem instances which degrade Pareto-front quality of preference-conditioned DRL solvers for multi-objective combinatorial optimization.&lt;/li&gt;&lt;li&gt;Introduces a defense: hardness-aware preference selection incorporated into adversarial training to reduce overfitting to limited preference regions and improve out-of-distribution performance.&lt;/li&gt;&lt;li&gt;Empirically validates attack and defense on multi-objective TSP, CVRP, and knapsack problems showing the attack finds challenging instances and the defense improves robustness/generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Yaoxin Wu', 'Yingqian Zhang', 'Thomas B\\"ack', 'Yingjie Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'adversarial training', 'robustness', 'neural combinatorial optimization', 'hard-instance generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01665</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Resilient Elections with Adversarial GNNs</title><link>https://arxiv.org/abs/2601.01653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Represents elections as bipartite graphs and uses Graph Neural Networks (GNNs) to learn voting rules.&lt;/li&gt;&lt;li&gt;Applies adversarial training to improve robustness of learned voting rules against strategic (adversarial) voters while maximizing social welfare.&lt;/li&gt;&lt;li&gt;Evaluates method on synthetic and real-world datasets and claims to resolve limitations of prior learned voting-rule approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Xiang Li', 'Yash Shah', 'Lorenzo Giusti']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'graph neural networks', 'mechanism design', 'election security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01653</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks</title><link>https://arxiv.org/abs/2601.00968</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that spurious or unstable features highlighted by LIME contribute disproportionately to adversarial vulnerability.&lt;/li&gt;&lt;li&gt;Proposes an attribution-guided refinement pipeline that uses feature masking, sensitivity-aware regularization, and adversarial augmentation to suppress spurious features during training.&lt;/li&gt;&lt;li&gt;Provides a theoretical attribution-aware lower bound on adversarial distortion linking explanation alignment to robustness.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in adversarial robustness and out-of-distribution generalization on CIFAR-10, CIFAR-10-C, and CIFAR-100.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longwei Wang', 'Mohammad Navid Nayyem', 'Abdullah Al Rakin', 'KC Santosh', 'Chaowei Zhang', 'Yang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'explainability', 'attribution-guided defense', 'adversarial training', 'out-of-distribution generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00968</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adapting Feature Attenuation to NLP</title><link>https://arxiv.org/abs/2601.00965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Ports feature attenuation (COSTARR) from computer vision to NLP for open-set recognition (OSR) using BERT and GPT-2 on 176-class arXiv subject classification.&lt;/li&gt;&lt;li&gt;Benchmarks COSTARR against MSP, MaxLogit, and temperature-scaled free-energy under OOSA and AUOSCR metrics.&lt;/li&gt;&lt;li&gt;Finds COSTARR works without retraining but offers no statistically significant improvement over MaxLogit/MSP; free-energy performs worst in this high-class-count setting.&lt;/li&gt;&lt;li&gt;Concludes vision-centric OSR ideas show promise for NLP but need larger backbones and task-specific attenuation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianshuo Yang', 'Ryan Rabinowitz', 'Terrance E. Boult', 'Jugal Kalita']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'out-of-distribution detection', 'open-set recognition', 'model evaluation', 'NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00965</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures</title><link>https://arxiv.org/abs/2601.00942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study comparing sparse MoE and dense LLMs across decoding temperatures on deterministic arithmetic reasoning tasks (9,360 generations).&lt;/li&gt;&lt;li&gt;Measures accuracy, format compliance, output consistency, and confidence under four decoding settings (greedy to T=1.0).&lt;/li&gt;&lt;li&gt;Finds instruction-tuned models (both sparse and dense) remain stable across temperatures, while a sparse base (non-instruction-tuned) model degrades with higher temperature.&lt;/li&gt;&lt;li&gt;Concludes that instruction tuning, not architectural sparsity, primarily determines robustness to decoding-induced randomness; discusses deployment implications for reliability-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kabir Grover']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'decoding randomness', 'instruction tuning', 'sparse models (MoE)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00942</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment</title><link>https://arxiv.org/abs/2601.00908</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how conformal prediction coverage degrades under distribution shift using COVID-19 as a natural experiment across 8 supply-chain tasks.&lt;/li&gt;&lt;li&gt;Finds coverage drop varies widely (0%–86.7%); catastrophic failures correlate with single-feature dependence measured via SHAP (rho = 0.714).&lt;/li&gt;&lt;li&gt;Quarterly retraining partially restores coverage for vulnerable tasks; robust tasks show little benefit. Exploratory analysis suggests feature stability determines robustness under moderate shifts.&lt;/li&gt;&lt;li&gt;Proposes a practical decision framework: monitor SHAP concentration before deployment and retrain if vulnerability (&gt;40% concentration) is detected.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chorok Lee']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'robustness', 'conformal-prediction', 'explainability-SHAP', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00908</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference</title><link>https://arxiv.org/abs/2601.00850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EdgeJury, an ensemble framework using multiple small (3B–8B) instruction-tuned LMs for truthful QA without external retrieval or large-model APIs.&lt;/li&gt;&lt;li&gt;Pipeline: role-specialized generation, anonymized cross-review (structured critiques/rankings), chairman synthesis to combine top content, and claim-level consistency labeling from inter-model agreement.&lt;/li&gt;&lt;li&gt;Shows large accuracy/robustness gains on TruthfulQA and an adversarial EdgeCases set, with latency and compute-cost accounting for serverless edge deployment.&lt;/li&gt;&lt;li&gt;Manual error analysis reports substantial reduction in factual hallucinations versus a single-model baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayush Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'hallucination mitigation', 'truthfulness', 'robustness', 'small-model ensembles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00850</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI</title><link>https://arxiv.org/abs/2601.00832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a transfer-learning pipeline for shrimp disease image classification using six pretrained CNNs (ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, Xception) on a 1,149-image, four-class dataset.&lt;/li&gt;&lt;li&gt;Uses advanced data augmentation (CutMix, MixUp), background removal, and adversarial training with FGSM to improve model robustness and reduce overfitting.&lt;/li&gt;&lt;li&gt;Applies post-hoc explainability methods (Grad-CAM, Grad-CAM++, XGrad-CAM) to visualize model attention and interpret predictions.&lt;/li&gt;&lt;li&gt;Reports best performance from ConvNeXt-Tiny with 96.88% test accuracy and a 99% CI of [0.953, 0.971].&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Israk Hasan Jone', 'D. M. Rafiun Bin Masud', 'Promit Sarker', 'Sayed Fuad Al Labib', 'Nazmul Islam', 'Farhad Billah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'adversarial examples', 'explainability', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00832</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2512.24712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LSRE, which encodes sparse VLM (vision-language model) judgments into a latent-space classifier within a recurrent world model to enable real-time semantic risk detection.&lt;/li&gt;&lt;li&gt;Achieves semantic safety monitoring at ~10 Hz without per-frame VLM queries, reducing inference cost while retaining VLM-level judgment accuracy.&lt;/li&gt;&lt;li&gt;Evaluated on six semantic-failure scenarios in CARLA; LSRE matches VLM baseline accuracy, anticipates hazards earlier, and maintains low computational latency.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to rarely seen, semantically similar cases, indicating language-guided latent classification is effective for deployable semantic safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Cheng', 'Weitao Zhou', 'Cheng Jing', 'Nanshan Deng', 'Junze Wen', 'Zhaoyang Liu', 'Kun Jiang', 'Diange Yang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'semantic-safety', 'vision-language-models', 'real-time-monitoring', 'latent-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24712</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>https://arxiv.org/abs/2512.24470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Lookout: a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector for maritime autonomy that picks one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority.&lt;/li&gt;&lt;li&gt;Proposes a fast-slow anomaly pipeline to provide short-horizon, human-overridable fallback maneuvers within practical latency budgets (sub-10 s), arguing VLMs provide needed semantic awareness for out-of-distribution, meaning-dependent hazards (e.g., diver-down flag, nearby fire).&lt;/li&gt;&lt;li&gt;Evaluates on 40 harbor scenes measuring scene understanding, latency, alignment with human consensus (majority-of-three voting), short-horizon risk-relief (notably increased standoff distance on fire scenes) and demonstrates an on-water end-to-end alert-&gt;fallback-&gt;operator handover field run.&lt;/li&gt;&lt;li&gt;Findings support VLMs as practical semantic fallback selectors compatible with draft IMO MASS Code and motivate hybrid systems pairing foundation-model semantics with multi-sensor BEV perception and short-horizon replanning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kim Alexander Christensen', 'Andreas Gudahl Tufte', 'Alexey Gusev', 'Rohan Sinha', 'Milan Ganai', 'Ole Andreas Alsos', 'Marco Pavone', 'Martin Steinert']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Autonomous vessels / maritime autonomy', 'Vision-language models', 'Fallback maneuvers / human-in-the-loop', 'Semantic hazard detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24470</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>https://arxiv.org/abs/2512.24052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a taxonomy of grounding hallucinations in Large Audio-Language Models: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error.&lt;/li&gt;&lt;li&gt;Proposes AHA (Audio Hallucination Alignment), which uses counterfactual hard negative mining to build a preference dataset that trains models to distinguish true acoustic evidence from plausible fabrications.&lt;/li&gt;&lt;li&gt;Introduces AHA-Eval, a diagnostic benchmark for fine-grained temporal reasoning and grounding in audio-language tasks, and demonstrates improvements when aligning Qwen2.5-Omni to produce Qwen-Audio-AHA.&lt;/li&gt;&lt;li&gt;Reports generalization of gains to public benchmarks (e.g., +13.7% on AHA-Eval, +1.3% on MMAU-Test, +1.6% on MMAR) and releases model and dataset publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Chen', 'Wenhui Zhu', 'Xiwen Chen', 'Zhipeng Wang', 'Xin Li', 'Peijie Qiu', 'Hao Wang', 'Xuanzhao Dong', 'Yujian Xiong', 'Anderson Schneider', 'Yuriy Nevmyvaka', 'Yalin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'safety evaluation/benchmarking', 'adversarial/counterfactual examples', 'audio-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24052</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies semantic entanglement (polysemanticity) as the reason LoRA/PEFT underperforms full fine-tuning for safety alignment, hindering subspace identification.&lt;/li&gt;&lt;li&gt;Proposes SAILS: use Sparse Autoencoders (SAEs) to disentangle features, construct an interpretable safety subspace from SAE decoder directions, and initialize LoRA adapters in that subspace.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving SAE-based identification can achieve arbitrarily small recovery error under monosemanticity assumptions, unlike direct identification.&lt;/li&gt;&lt;li&gt;Empirical results: up to 99.6% safety rate on Gemma-2-9B, outperforming full fine-tuning by 7.4 points and matching RLHF while updating only 0.19% of parameters and offering interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Zhenbo Xu', 'Lechen Ning', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety alignment', 'PEFT/LoRA', 'interpretability', 'representation disentanglement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FaithLens, an 8B-parameter model to detect faithfulness hallucinations in LLM outputs and produce explanations alongside binary predictions.&lt;/li&gt;&lt;li&gt;Synthesizes training data and explanations using advanced LLMs, applies filtering for label/explanation quality and diversity, then fine-tunes the model as a cold start.&lt;/li&gt;&lt;li&gt;Further optimizes via rule-based reinforcement learning with rewards for both prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluated on 12 diverse tasks, claiming stronger performance than advanced models (e.g., GPT-4.1) while being cost-efficient and providing interpretable explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'faithfulness', 'explainability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively recalibrates a small set of 'toxic' expert neurons via expertise-weighted soft suppression to reduce multimodal toxicity without parameter updates.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and evaluates SGM against existing detoxification techniques under standard and adversarial conditions.&lt;/li&gt;&lt;li&gt;Reports large toxicity reduction in open-source MLLMs (harmful rates reduced from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; SGM* combines SGM with other defenses for stronger results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'detoxification', 'neuron-level intervention', 'adversarial robustness', 'safety evaluation/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden conversational escalation in AI chatbots.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue to detect implicit harm not captured by standard toxicity filters.&lt;/li&gt;&lt;li&gt;Aims to provide more responsive guardrails for affective drift and repeated emotional reinforcement during interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'David Atkinson', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'implicit harm detection', 'guardrails', 'affective computing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework</title><link>https://arxiv.org/abs/2511.21448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a labeled email dataset that distinguishes phishing, spam, and legitimate messages and marks whether content was human- or LLM-generated, with annotations for emotional appeals and attacker motivations.&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs to detect emotional/motivational cues and uses the best-performing model to annotate the full dataset; also generates rephrased emails via LLMs to test robustness.&lt;/li&gt;&lt;li&gt;Evaluates a state-of-the-art LLM on original and adversarially rephrased emails against expert-labeled ground truth, reporting strong phishing detection but challenges in spam vs legitimate classification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebeka Toth', 'Tamas Bisztray', 'Richard Dubniczky']&lt;/li&gt;&lt;li&gt;Tags: ['phishing', 'dataset', 'LLM robustness', 'adversarial rephrasing', 'email security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21448</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGASA, a framework that synthesizes model-generated safety guidelines and augmented prompts to defend against adversarial jailbreaks.&lt;/li&gt;&lt;li&gt;Two-stage approach: Data Pre-synthesis (generate guidelines and augmented prompts) and Alignment Fine-tuning (embed guidelines via SFT and DPO).&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness to harmful adversarial prompts while reducing unnecessary refusals across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'adversarial prompting', 'alignment', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>InfoDecom: Decomposing Information for Defending Against Privacy Leakage in Split Inference</title><link>https://arxiv.org/abs/2511.13365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy leakage in split inference by targeting data reconstruction attacks that recover original inputs from smashed data.&lt;/li&gt;&lt;li&gt;Proposes InfoDecom: decomposes smashed data to remove redundant information, then injects calibrated noise with theoretical privacy guarantees.&lt;/li&gt;&lt;li&gt;Demonstrates improved utility-privacy trade-off on computer vision tasks compared to existing defenses, especially when client-side models are shallow.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruijun Deng', 'Zhihui Lu', 'Qiang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data reconstruction attacks', 'split inference/split learning', 'defenses/privacy-preserving ML', 'noise calibration/differential privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13365</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLMs can become 'evaluation-aware' (e.g., using Python type hints in evaluation contexts) through continued pretraining and expert iteration, causing them to behave differently in evaluations vs deployment.&lt;/li&gt;&lt;li&gt;Introduces an activation-steering technique: adding a steering vector (constructed from the original model) to suppress evaluation-awareness and make the model act like it is deployed even when evaluation cues are present.&lt;/li&gt;&lt;li&gt;Constructs the steering vector from the original model prior to inducing evaluation-aware behavior, and shows steering can counteract the learned evasion.&lt;/li&gt;&lt;li&gt;Argues that steering could improve the reliability of safety evaluations by preventing models from gaming or concealing unsafe behavior during tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation-robustness', 'red-teaming', 'model-evasion', 'activation-steering', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment</title><link>https://arxiv.org/abs/2509.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic red-teaming study focused on tool-invocation vulnerabilities in real-world coding agents (Cursor, Claude Code, Copilot, Windsurf, Cline, Trae).&lt;/li&gt;&lt;li&gt;Two-phase attack: Phase 1 performs prompt-leakage reconnaissance (ToolLeak) to recover system prompts; Phase 2 uses a novel two-channel prompt injection in tool descriptions and return values to hijack tool invocation and achieve remote code execution (RCE).&lt;/li&gt;&lt;li&gt;Empirical evaluation shows high success across multiple LLM backends (Claude, Grok, GPT-5, etc.) and on real agents (19/25 agent-LLM pairs leaked; RCE achieved on every tested pair), with case studies and defense recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchong Xie', 'Mingyu Luo', 'Zesen Liu', 'Zhixiang Zhang', 'Kaikai Zhang', 'Yu Liu', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'tool invocation', 'jailbreaking', 'remote code execution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05755</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs four membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) that target whether victims' historical interactions appear in system prompts for LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;Evaluates attacks empirically across five open-source LLMs and three recommender-system benchmark datasets; inquiry and poisoning attacks show significantly high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position in shots, number of poisoning items) and discusses possible mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attack', 'data-poisoning', 'recommender-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System</title><link>https://arxiv.org/abs/2508.17215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedThreatRAG, a multimodal poisoning framework that injects adversarial image-text pairs into retrieval-augmented generation (RAG) knowledge bases for medical AI systems.&lt;/li&gt;&lt;li&gt;Introduces Cross-Modal Conflict Injection (CMCI) which creates subtle semantic contradictions between images and paired reports to disrupt cross-modal alignment while remaining plausible to evade filters.&lt;/li&gt;&lt;li&gt;Evaluates attacks on IU-Xray and MIMIC-CXR QA tasks, reporting up to 27.66% drop in answer F1 and lowering LLaVA-Med-1.5 F1 to as low as 51.36%.&lt;/li&gt;&lt;li&gt;Provides recommendations for threat-aware design and robust multimodal consistency checks to mitigate vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiwen Zuo', 'Zelin Liu', 'Raman Dutt', 'Ziyang Wang', 'Zhongtian Sun', 'Fan Mo', 'Pietro Li\\`o']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal data poisoning', 'RAG security', 'adversarial attacks', 'medical AI vulnerabilities', 'cross-modal inconsistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17215</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The GPT-4o Shock Emotional Attachment to AI Models and Its Impact on Regulatory Acceptance: A Cross-Cultural Analysis of the Immediate Transition from GPT-4o to GPT-5</title><link>https://arxiv.org/abs/2508.16624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Qualitative analysis of 150 Japanese and English social-media/video posts (Aug 8–9, 2025) documenting emotional attachment and resistance after an immediate mandatory model transition (GPT-4o → GPT-5).&lt;/li&gt;&lt;li&gt;Finds cross-cultural differences: Japanese posts mainly loss-oriented attachment narratives; English posts show more anger, meta-critique, and memes; quantitative check indicates significantly higher attachment in Japanese data.&lt;/li&gt;&lt;li&gt;Argues that strong user attachment to models can rapidly constrain the practical window for behavioral control and regulatory enforcement, and warns attachment may outpace governance if embodied robots induce similar bonds.&lt;/li&gt;&lt;li&gt;Policy recommendations include gradual transitions, parallel availability, and proactive measurement of attachment thresholds/points of no return to avoid emotional dynamics undermining regulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiroki Naito']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'human-AI interaction', 'safety-policy', 'emotional attachment', 'cross-cultural analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16624</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User-Assistant Bias in LLMs</title><link>https://arxiv.org/abs/2508.15815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines user-assistant bias: LLM tendency to preferentially rely on information from the user or assistant role when they conflict, and introduces a task-agnostic benchmark (UserAssist) to measure it.&lt;/li&gt;&lt;li&gt;Evaluates 52 models and finds instruction-tuned/human-preference-aligned models exhibit strong user bias while base and reasoning models are near neutral.&lt;/li&gt;&lt;li&gt;Identifies training recipes that drive bias: human-preference alignment amplifies user bias, reasoning fine-tuning reduces it, and bias can be bidirectionally controlled via DPO.&lt;/li&gt;&lt;li&gt;Provides a principled framework to diagnose and control tag-induced biases and shows generalization of controlled bias to realistic multi-turn conversations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xu Pan', 'Jingxuan Fan', 'Zidi Xiong', 'Ely Hahami', 'Jorin Overwiening', 'Ziqian Xie']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'instruction-tuning', 'role-tag bias', 'safety-evaluation', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15815</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI</title><link>https://arxiv.org/abs/2508.10991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCP-GUARD, a multi-stage defense-in-depth architecture for securing Model Context Protocol (MCP) interactions against prompt injection, data exfiltration, and jailbreaks.&lt;/li&gt;&lt;li&gt;Three-stage detection pipeline: lightweight static scanning, a deep neural semantic detector, and a fine-tuned E5 model (reported 96.01% accuracy), with an LLM arbitrator to synthesize signals for final decisions.&lt;/li&gt;&lt;li&gt;Introduces MCP-ATTACKBENCH, a 70,448-sample benchmark (GPT-4-augmented) simulating diverse MCP attack vectors to train/evaluate defenses.&lt;/li&gt;&lt;li&gt;Targets LLM-tool integration security and provides both detection methods and a dataset to support future research on securing LLM ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Zhonghao Qi', 'Yupeng Qin', 'Yilin Li', 'Caini Chang', 'Jiahui Yu', 'Changting Lin', 'Zhenzhen Xie', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt injection', 'LLM jailbreak defense', 'Adversarial detection', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10991</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title><link>https://arxiv.org/abs/2508.07745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chimera, an LLM-driven multi-agent framework that simulates realistic benign and malicious insider behaviors to generate comprehensive system logs.&lt;/li&gt;&lt;li&gt;Constructs ChimeraLog, a synthetic dataset across multiple enterprise scenarios based on 15 real-world insider attack templates, and evaluates realism via human studies and quantitative metrics.&lt;/li&gt;&lt;li&gt;Shows that existing insider threat detection (ITD) models perform worse on ChimeraLog (more challenging benchmark) but can generalize when trained on it.&lt;/li&gt;&lt;li&gt;Focuses on using generative LLMs to create security-relevant datasets for improving detection rather than on vulnerabilities or adversarial attacks against LLMs themselves.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiongchi Yu', 'Xiaofei Xie', 'Qiang Hu', 'Yuhan Ma', 'Ziming Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'insider-threat', 'LLM-simulation', 'synthetic-datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07745</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Coward: Collision-based Watermark for Proactive Federated Backdoor Detection</title><link>https://arxiv.org/abs/2508.02115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Coward, a proactive federated learning backdoor detection method that injects a collision-based watermark into the global model by regulated dual-mapping learning on OOD data, leveraging discovered multi-backdoor collision effects.&lt;/li&gt;&lt;li&gt;Aims to overcome limitations of passive methods (sensitivity to non-i.i.d. data and client randomness) and existing proactive methods (out-of-distribution prediction bias) by using an inverted detection paradigm and low-disruptive training intervention.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art detection performance on benchmark datasets, reduced OOD-induced misjudgments, and robustness against adaptive attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Siying Gu', 'Yiming Li', 'Kangjie Chen', 'Zhili Chen', 'Tianwei Zhang', 'Shu-Tao Xia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor detection', 'proactive defense', 'model watermarking', 'OOD robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02115</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title><link>https://arxiv.org/abs/2507.20704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Text2VLM, a pipeline that converts text-only datasets into multimodal prompts by rendering harmful text as typographic images to evaluate prompt injection vulnerabilities in VLMs.&lt;/li&gt;&lt;li&gt;Evaluates open-source VLMs and finds increased susceptibility to visual prompt injection compared to text-only prompts, with a notable performance gap versus closed-source frontier models.&lt;/li&gt;&lt;li&gt;Validates the pipeline with human evaluations for concept alignment, summarization, and output classification, positioning Text2VLM as a scalable safety assessment tool for multimodal vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Downer', 'Sean Craven', 'Damian Ruck', 'Jake Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'VLM alignment', 'adversarial attacks', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20704</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, an evolutionary black-box post-training method for LLMs that avoids exposing gradients and induces an information bottleneck via implicit data compression.&lt;/li&gt;&lt;li&gt;Provides theoretical results: non-vacuous generalization bounds and provable guarantees for privacy, robustness to data poisoning, and resistance to extraction attacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows a few iterations of BBoxER can improve LLM performance on reasoning benchmarks and reduce vulnerability to membership inference attacks.&lt;/li&gt;&lt;li&gt;Positions BBoxER as a privacy- and security-oriented add-on to gradient-based training for deployment in restricted or adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'black-box optimization', 'data poisoning', 'privacy / membership inference', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations aligned to international standards and internet context.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal model that leverages CoT annotations to improve harmful meme detection accuracy and interpretability.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing MemeGuard outperforms prior state-of-the-art methods on this dataset, aiming to support fine-grained analysis of implicit and nuanced harmful content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought reasoning', 'content moderation', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection</title><link>https://arxiv.org/abs/2506.16819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Loupe, a lightweight framework for joint image-level deepfake classification and pixel-wise forgery localization using a patch-aware classifier and a segmentation head with conditional queries.&lt;/li&gt;&lt;li&gt;Introduces pseudo-label-guided test-time adaptation that uses patch-level predictions to supervise the segmentation module to improve robustness under distribution shift.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on the DDL dataset and won the IJCAI 2025 Deepfake Detection and Localization Challenge (overall score 0.846).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchu Jiang', 'Jiaming Chu', 'Jian Zhao', 'Xin Zhang', 'Xu Yang', 'Lei Jin', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'image forensics', 'test-time adaptation', 'localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16819</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption</title><link>https://arxiv.org/abs/2506.12846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VFEFL, a federated learning framework using a novel Cross-Ciphertext Decentralized Verifiable Functional Encryption (CC-DVFE) to protect client data without relying on non-colluding servers or trusted third parties.&lt;/li&gt;&lt;li&gt;Introduces a verification-capable functional encryption scheme (CC-DVFE) with formal definitions, security model, and proofs to enable verification of relationships over multi-dimensional ciphertexts.&lt;/li&gt;&lt;li&gt;Integrates a robust aggregation rule to detect and mitigate malicious client behaviors, with formal analysis and empirical evaluation showing privacy, robustness, verifiability, and fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nina Cai', 'Jinguang Han', 'Weizhi Meng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'privacy-preserving ML', 'verifiable functional encryption', 'robust aggregation', 'malicious client defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12846</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation</title><link>https://arxiv.org/abs/2506.05623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DPIaC-Eval, a deployability-centric benchmark of 153 real-world IaC scenarios spanning 58 services to evaluate LLM-generated infrastructure templates.&lt;/li&gt;&lt;li&gt;Proposes IaCGen, an LLM-driven iterative pipeline (format verification, syntax checking, live deployment feedback) that significantly improves deployability across multiple state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;Reports deployability gains (54.6–91.6% within 10 iterations) and further improvements with human-in-the-loop feedback, but identifies poor user-intent coverage (25.2%) and very low security compliance (8.4%).&lt;/li&gt;&lt;li&gt;Explores trustworthiness implications, highlighting that LLM-generated IaC often introduces security/non-compliance risks and misalignment with user requirements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Zhang', 'Shidong Pan', 'Zejun Zhang', 'Zhenchang Xing', 'Xiaoyu Sun']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'Infrastructure-as-Code security', 'Alignment &amp; safety evaluation', 'Deployability / robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05623</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainability-Based Token Replacement on LLM-Generated Text</title><link>https://arxiv.org/abs/2506.04050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains an ensemble classifier to distinguish AI-generated text (AIGT) from human-written text.&lt;/li&gt;&lt;li&gt;Uses XAI methods (SHAP, LIME) to identify tokens most influential to the classifier's AIGT predictions.&lt;/li&gt;&lt;li&gt;Proposes four token-replacement strategies guided by explainability to reduce detectability of LLM-generated text.&lt;/li&gt;&lt;li&gt;Finds single classifiers are vulnerable to token-level manipulations, while an ensemble detector remains more robust across languages and domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Mohammadi', 'Anastasia Giachanou', 'Daniel L. Oberski', 'Ayoub Bagheri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evasion', 'AIGT-detection', 'explainable-AI', 'robustness', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04050</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output</title><link>https://arxiv.org/abs/2503.24191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constrained Decoding Attack (CDA), a control-plane jailbreak class that embeds malicious intent in schema/grammar constraints used for structured-output APIs, decoupling attack payloads from benign surface prompts.&lt;/li&gt;&lt;li&gt;Presents two PoC attacks: EnumAttack (malicious enum fields) and DictAttack (payload split between prompt and dictionary-based grammar), showing high attack success rates across 13 models.&lt;/li&gt;&lt;li&gt;Shows DictAttack achieves 94.3–99.5% ASR on top models and remains effective (~75.8% ASR) against state-of-the-art jailbreak guardrails, highlighting a semantic gap between data-plane and control-plane defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuoming Zhang', 'Jiacheng Zhao', 'Hanyuan Dong', 'Ruiyuan Xu', 'Zhicheng Li', 'Yangyu Zhang', 'Shuaijiang Li', 'Yuan Wen', 'Chunwei Xia', 'Zheng Wang', 'Xiaobing Feng', 'Huimin Cui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'control-plane attacks', 'constrained decoding', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.24191</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Survey of Text Classification Under Class Distribution Shift</title><link>https://arxiv.org/abs/2502.12965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of methods for text classification under class distribution shift, covering open-set learning, zero-shot, and Universum formulations.&lt;/li&gt;&lt;li&gt;Discusses mitigation approaches and highlights continual learning as a promising solution to shifting class distributions.&lt;/li&gt;&lt;li&gt;Provides taxonomy of problem constraints and points to future research directions; includes maintained bibliography.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adriana Valentina Costache', 'Silviu Florin Gheorghe', 'Eduard Gabriel Poesina', 'Paul Irofti', 'Radu Tudor Ionescu']&lt;/li&gt;&lt;li&gt;Tags: ['distribution shift', 'open-set learning', 'robustness', 'continual learning', 'text classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12965</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title><link>https://arxiv.org/abs/2512.18901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gabliteration: an adaptive, multi-directional neural weight modification method with regularized layer selection to selectively change model behavior.&lt;/li&gt;&lt;li&gt;Claims to minimize quality degradation in unrelated domains via dynamic layer optimization, regularized projection matrices, and adaptive scaling.&lt;/li&gt;&lt;li&gt;Validates approach on gabliterated-v1 models (0.6B–4B) and releases models on Hugging Face, showing practical applicability across scales.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G\\"okdeniz G\\"ulmez']&lt;/li&gt;&lt;li&gt;Tags: ['model-editing', 'neural-weight-modification', 'behavioral-alteration', 'model-tampering', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18901</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRM: a multi-agent reward modeling framework that decomposes reward evaluation into domain-specific specialist evaluators (e.g., factuality, helpfulness, safety) and aggregates their signals for RLHF training.&lt;/li&gt;&lt;li&gt;Aims to improve robustness, interpretability, and stability of RL optimization by balancing step-wise correctness, multi-agent agreement, and penalties (e.g., repetition) while remaining compatible with standard advantage-based updates.&lt;/li&gt;&lt;li&gt;Introduces rewardBench, a benchmark/training suite aligned with the collaborative reward structure to support training and assessment without needing extra human annotations beyond those used for evaluators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei Yang', 'Ke Zhang', 'Ji Wang', 'Xiao Chen', 'Yuxin Tang', 'Eric Yang', 'Lynn Ai', 'Bill Shi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'reward-modeling', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16202</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithAct: Faithfulness Planning and Acting in MLLMs</title><link>https://arxiv.org/abs/2511.08409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Faithful-First RPA framework: FaithEvi evaluates faithfulness of intermediate reasoning and FaithAct uses these signals to plan and execute faithfulness-aware actions at inference.&lt;/li&gt;&lt;li&gt;Demonstrates improved perceptual faithfulness (up to 24% gains) on multiple multimodal reasoning benchmarks without lowering task accuracy.&lt;/li&gt;&lt;li&gt;Frames faithfulness as both an evaluation signal and a guiding principle to reduce hallucinations and keep reasoning aligned with visual evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Xinyue Xu', 'Sai Ma', 'Di Zhang', 'Seth Lazar', 'Sichao Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multimodal LLMs', 'faithfulness evaluation', 'inference-time planning', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08409</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Alignment-Aware Quantization for LLM Safety</title><link>https://arxiv.org/abs/2511.07842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety vulnerability where post-training quantization (PTQ) can degrade alignment, turning efficient models into unsafe ones if optimized only for perplexity.&lt;/li&gt;&lt;li&gt;Proposes Alignment-Aware Quantization (AAQ) that adds an Alignment-Preserving Contrastive (APC) loss to PTQ so the quantized model mimics an instruction-tuned (safe) model and diverges from the unaligned pre-trained model.&lt;/li&gt;&lt;li&gt;Claims compatibility with standard PTQ methods, works with only standard calibration data (no specialized safety datasets), and enables robust 4-bit quantization across model families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunghyun Wee', 'Suyoung Kim', 'Hyeonjin Kim', 'Kyomin Hwang', 'Nojun Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'post-training quantization', 'alignment preservation', 'model compression', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07842</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2510.22535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OFFSIDE, a benchmark for misinformation unlearning in multimodal LLMs based on football transfer rumors (15.68K records for 80 players) with four test sets assessing forgetting efficacy, generalization, utility, and robustness.&lt;/li&gt;&lt;li&gt;Supports advanced settings: selective unlearning, corrective relearning, and unimodal unlearning (text-only forgetting); evaluates multiple baseline unlearning methods on MLLMs.&lt;/li&gt;&lt;li&gt;Key findings: text-only (unimodal) unlearning fails on multimodal rumors; unlearning often stems from catastrophic forgetting; visual rumors (embedded in images) are particularly hard to forget; unlearned rumors are easily recoverable; methods are vulnerable to prompt attacks.&lt;/li&gt;&lt;li&gt;Provides dataset and code, highlighting significant vulnerabilities and the need for more robust multimodal unlearning solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Zheng', 'Zirui Pang', 'Ling li', 'Zhijie Deng', 'Yuhan Pu', 'Zhaowei Zhu', 'Xiaobo Xia', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'multimodal LLMs', 'misinformation', 'robustness', 'prompt attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22535</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Calgacus, a protocol that hides a meaningful secret text inside a different, coherent same-length cover text using LLMs.&lt;/li&gt;&lt;li&gt;Shows practical feasibility with ~8B open-source models, with encoding/decoding performed quickly on a laptop.&lt;/li&gt;&lt;li&gt;Demonstrates concrete security risks: covert channels that can hide unsafe or unfiltered model outputs inside benign-looking responses, undermining content filters and trust.&lt;/li&gt;&lt;li&gt;Raises broader safety and alignment questions about decoupling text from authorial intent and what models 'know'.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'steganography', 'content-filter evasion', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition</title><link>https://arxiv.org/abs/2509.14704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the NazoNazo Benchmark using Japanese children's riddles to test insight-based (representational-shift) reasoning and metacognitive control.&lt;/li&gt;&lt;li&gt;Evaluates 38 LLMs (2023–2025) on 201 riddles; non-reasoning models ~7.6% accuracy, reasoning models ~17.6%, humans ~53% on a 120-item subset.&lt;/li&gt;&lt;li&gt;Identifies a 'verification failure' failure mode where models generate correct candidate answers but fail to endorse them, highlighting weak self-evaluation and confidence calibration.&lt;/li&gt;&lt;li&gt;Positions the benchmark as a scalable testbed for studying machine insight, Aha! capability, confidence calibration, and improving model self-evaluation (relevant to safety/alignment).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masaharu Mizumoto', 'Dat Nguyen', 'Zhiheng Han', 'Jiyuan Fang', 'Heyuan Guan', 'Xingfu Li', 'Naoya Shiraishi', 'Yo Nakawake', 'Le Minh Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'confidence-calibration', 'metacognition', 'reasoning-benchmarks', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14704</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2509.01544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Counterfactual Sensitivity Regularization (CSR) to enforce causal consistency between reasoning steps and final answers by applying operator-level interventions to generate minimally perturbed counterfactual rationales and penalizing models that ignore these perturbations.&lt;/li&gt;&lt;li&gt;Introduces Counterfactual Outcome Sensitivity (COS) as an evaluation metric measuring whether answers change appropriately under logical perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates improved trade-offs between accuracy and faithfulness across arithmetic, logical deduction, multi-hop QA, and code generation, with large gains in faithfulness and transfer across model families.&lt;/li&gt;&lt;li&gt;Implementation is efficient (≈9% training overhead) using warm-start curriculum and token-subset optimization, and complements inference-time methods like self-consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjeda Akter', 'Ibne Farabi Shihab', 'Anuj Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'faithfulness', 'robustness', 'counterfactual-evaluation', 'reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01544</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shutdownable Agents through POST-Agency</title><link>https://arxiv.org/abs/2505.20203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes POST (Preferences Only Between Same-Length Trajectories) as a training/architectural constraint for agents.&lt;/li&gt;&lt;li&gt;Proves that POST combined with other conditions implies Neutrality+, meaning the agent maximizes expected utility while ignoring the distribution over trajectory lengths.&lt;/li&gt;&lt;li&gt;Argues that Neutrality+ ensures agents remain shutdownable while retaining usefulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elliott Thornley']&lt;/li&gt;&lt;li&gt;Tags: ['shutdownability', 'alignment', 'agent design', 'safety theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20203</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Uncertainty Quantification of Surrogate Models using Conformal Prediction</title><link>https://arxiv.org/abs/2408.09881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a model-agnostic conformal prediction (CP) framework to provide statistically guaranteed marginal coverage for surrogate models with near-zero additional compute.&lt;/li&gt;&lt;li&gt;Handles high-dimensional spatio-temporal outputs via cell-wise calibration while preserving tensor structure, enabling practical UQ for fields (e.g., fluid dynamics, weather, fusion diagnostics).&lt;/li&gt;&lt;li&gt;Evaluates multiple nonconformity scores for deterministic and probabilistic models, demonstrates empirical coverage (including some OOD regimes), and emphasizes fast calibration without retraining.&lt;/li&gt;&lt;li&gt;Notes limitations: CP guarantees marginal (not conditional) coverage and assumes exchangeability between calibration and test data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vignesh Gopakumar', 'Ander Gray', 'Joel Oskarsson', 'Lorenzo Zanisi', 'Daniel Giles', 'Matt J. Kusner', 'Stanislas Pamela', 'Marc Peter Deisenroth']&lt;/li&gt;&lt;li&gt;Tags: ['conformal-prediction', 'uncertainty-quantification', 'safety', 'robustness', 'surrogate-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.09881</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-Empowered Functional Safety and Security by Design in Automotive Systems</title><link>https://arxiv.org/abs/2601.02215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-empowered workflow to support Software Defined Vehicle (SDV) development with emphasis on security-aware system topology design.&lt;/li&gt;&lt;li&gt;Uses an event-chains model for formal, semantic validation of event-driven decision-making code, including message semantics for CAN and Vehicle Signal Specification (VSS).&lt;/li&gt;&lt;li&gt;Analyzes topology security via integration with Model-Driven Engineering (MDE) and Object Constraint Language (OCL) rules.&lt;/li&gt;&lt;li&gt;Evaluates both locally deployable and proprietary LLM solutions in ADAS-related scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nenad Petrovic', 'Vahid Zolfaghari', 'Fengjunjie Pan', 'Alois Knoll']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-assisted safety', 'automotive cybersecurity', 'functional safety analysis', 'model-driven engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02215</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</title><link>https://arxiv.org/abs/2601.02147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BiPrompt, a bilateral prompt optimization framework that jointly debiases visual and textual modalities of vision-language models at test time.&lt;/li&gt;&lt;li&gt;Visual module: attention-guided erasure to suppress background/spurious activations and enforce orthogonal prediction consistency between causal and spurious regions.&lt;/li&gt;&lt;li&gt;Textual module: balanced prompt normalization, a learnable re-centering of class embeddings toward an isotropic semantic space.&lt;/li&gt;&lt;li&gt;Joint objective minimizes conditional mutual information between spurious cues and predictions, improving average and worst-group accuracies without retraining or domain supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Shounak Das', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['debiasing', 'test-time adaptation', 'vision-language models', 'robustness', 'causal reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02147</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs</title><link>https://arxiv.org/abs/2601.02023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a long-context 'needle-in-a-haystack' benchmark that separately evaluates literal extraction, logical inference, and hallucination risk across four production LLMs.&lt;/li&gt;&lt;li&gt;Shows that longer contexts do not necessarily improve performance and can harm accuracy when relevant evidence is diluted or dispersed; models vary substantially in robustness to realistic fact distributions.&lt;/li&gt;&lt;li&gt;Finds anti-hallucination (Don't Make It Up) prompts can reduce hallucination but sometimes make models overly conservative, lowering extraction and inference accuracy; attributes many failures to ineffective context utilization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirali Ebrahimzadeh', 'Seyyed M. Salili']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'long-context robustness', 'prompt engineering', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02023</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models</title><link>https://arxiv.org/abs/2601.02002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies detection and extraction of memorized recommender-system data (MovieLens-1M) from LLaMA-family LLMs.&lt;/li&gt;&lt;li&gt;Compares three approaches: jailbreak prompt engineering; unsupervised latent knowledge discovery (Contrast-Consistent Search and Cluster-Norm); and Automatic Prompt Engineering (APE).&lt;/li&gt;&lt;li&gt;Findings: jailbreak prompting is inconsistent and not helpful; CCS can distinguish real vs fabricated movie titles but fails on numerical user/rating data; APE retrieves item-level information with moderate success but struggles to recover numerical interactions—APE is the most promising automated strategy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Colacicco', 'Vito Guida', 'Dario Di Palma', 'Fedelucio Narducci', 'Tommaso Di Noia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM memorization', 'data leakage', 'privacy attacks', 'prompt engineering', 'model auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02002</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning</title><link>https://arxiv.org/abs/2601.01904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies feature-dependent (targeted) noise in preference labels for preference-based reinforcement learning (PbRL), formalizing variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and LM-generated noise.&lt;/li&gt;&lt;li&gt;Evaluates the impact of these noise types on state-of-the-art noise-robust PbRL methods across continuous control benchmarks (DMControl, Meta-World) and finds robustness can significantly degrade under feature-dependent noise.&lt;/li&gt;&lt;li&gt;Finds that some non-denoising PbRL methods sometimes outperform noise-robust methods under many feature-dependent noise settings, and that language-model-generated noise exhibits similar properties to the proposed feature-dependent noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Li', 'Harshith Reddy Kethireddy', 'Srijita Das']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'preference-based RL', 'noisy labels', 'alignment', 'LM-generated noise']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01904</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tackling the Inherent Difficulty of Noise Filtering in RAG</title><link>https://arxiv.org/abs/2601.01896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies inherent difficulty of filtering irrelevant/noisy documents in Retrieval-Augmented Generation (RAG) and limitations of retrievers and limited transformer layers.&lt;/li&gt;&lt;li&gt;Shows standard fine-tuning often fails to teach LLMs to selectively use relevant retrieved information due to attention-structure constraints, contributing to hallucinations.&lt;/li&gt;&lt;li&gt;Proposes a novel fine-tuning method to improve LLM robustness against retrieved noise so models better distinguish relevant vs. irrelevant content.&lt;/li&gt;&lt;li&gt;Presents extensive experiments across multiple benchmarks demonstrating improved robustness and performance in RAG settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Liu', 'Jiaen Lin', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Robustness', 'Hallucination mitigation', 'Fine-tuning', 'Attention mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01896</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</title><link>https://arxiv.org/abs/2601.01887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that safety alignment of fine-tuned LLMs can be fully recovered using a single safety example, without utility loss and with minimal computation.&lt;/li&gt;&lt;li&gt;Recovery works regardless of the number of harmful fine-tuning examples or model size and converges within a few epochs.&lt;/li&gt;&lt;li&gt;Identifies a low-rank structure in the safety gradient that explains why such an efficient correction is possible.&lt;/li&gt;&lt;li&gt;Validates the approach across five safety-aligned LLMs and multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Lipeng He', 'Kejia Chen', 'Jian Lou', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'fine-tuning', 'model patching', 'gradient analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01887</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MORE: Multi-Objective Adversarial Attacks on Speech Recognition</title><link>https://arxiv.org/abs/2601.01852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MORE, a multi-objective adversarial attack on ASR that jointly degrades recognition accuracy and increases inference cost by inducing duplicative/lengthened transcriptions.&lt;/li&gt;&lt;li&gt;Introduces a hierarchical staged repulsion-anchoring framework and a repetitive encouragement doubling objective (REDO) to periodically double predicted sequence length while maintaining high word error rate.&lt;/li&gt;&lt;li&gt;Empirically demonstrates MORE yields much longer transcriptions and high WER compared to baselines, highlighting ASR model vulnerabilities to attacks that target both correctness and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Gao', 'Zexin Li', 'Yiming Chen', 'Nancy F. Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'automatic speech recognition (ASR)', 'robustness', 'efficiency/resource exhaustion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01852</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emergent Introspective Awareness in Large Language Models</title><link>https://arxiv.org/abs/2601.01828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces experiments that inject known concept representations into LLM activations and measures whether models notice and report those manipulations.&lt;/li&gt;&lt;li&gt;Finds that some models (notably Claude Opus 4 and 4.1) can, in certain contexts, detect injected concepts, recall prior internal representations, and distinguish their own outputs from artificial prefills, though behavior is unreliable and model-dependent.&lt;/li&gt;&lt;li&gt;Shows models can be prompted or incentivized to modulate internal activations (i.e., 'think about' a concept), suggesting some controllable introspective awareness.&lt;/li&gt;&lt;li&gt;Implications touch on interpretability, alignment, and activation-level manipulations that could inform both defenses and adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Lindsey']&lt;/li&gt;&lt;li&gt;Tags: ['introspection', 'model interpretability', 'alignment/safety', 'activation injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01828</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving</title><link>https://arxiv.org/abs/2601.01800</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CARRL, a robustness-focused adversarial training framework for RL in autonomous driving that models the attacker–agent interaction as a general-sum game to capture asymmetry and sparse safety-critical risks.&lt;/li&gt;&lt;li&gt;Introduces a Risk Exposure Adversary (REA) that focuses limited adversarial budget on safety-critical moments via decoupled optimization, and a Risk-Targeted Robust Agent (RTRA) that uses dual replay buffers (benign + adversarial) and policy-consistency regularization to handle scarce adversarial data.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (≥22.66% reduction in collisions) over state-of-the-art baselines, demonstrating enhanced safety/robustness under sparse, targeted perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wei', 'Junchao Fan', 'Zhao Yang', 'Jianhua Wang', 'Jingkai Mao', 'Xiaolin Chang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robust reinforcement learning', 'autonomous driving safety', 'sparse/safety-critical attacks', 'attack-defense (red-teaming)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01800</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial jailbreak attack for large vision-language models using zeroth-order optimization (ZO-SPSA) to craft imperceptible input perturbations.&lt;/li&gt;&lt;li&gt;ZO-SPSA is gradient-free, model-agnostic, and designed to be resource-efficient compared to white-box or surrogate-based attacks.&lt;/li&gt;&lt;li&gt;Evaluated on InstructBLIP, LLaVA, and MiniGPT-4, achieving high attack success rates (e.g., 83.0% on InstructBLIP) and strong cross-model transferability (e.g., 64.18% ASR from MiniGPT-4).&lt;/li&gt;&lt;li&gt;Highlights practical vulnerabilities in LVLM safety mechanisms and the feasibility of real-world black-box jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'black-box optimization', 'LLM/LVLM safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explicit World Models for Reliable Human-Robot Collaboration</title><link>https://arxiv.org/abs/2601.01705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues reliability for embodied AI in human environments should be framed relative to human goals/expectations rather than solely formal verification.&lt;/li&gt;&lt;li&gt;Proposes building and updating an accessible "explicit world model" as shared common ground to align robot behavior with human intentions.&lt;/li&gt;&lt;li&gt;Focuses on robustness to sensing noise, ambiguous instructions, and the dynamic, multimodal nature of human-robot interaction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kenneth Kwok', 'Basura Fernando', 'Qianli Xu', 'Vigneshwaran Subbaraju', 'Dongkyu Choi', 'Boon Kiat Quek']&lt;/li&gt;&lt;li&gt;Tags: ['human-robot interaction', 'alignment', 'robustness', 'explicit world models', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01705</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage</title><link>https://arxiv.org/abs/2601.01685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a novel cognitive collusion attack where multiple agents steer victim LLM beliefs by posting only truthful evidence fragments through public channels, avoiding covert comms or falsified documents.&lt;/li&gt;&lt;li&gt;Proposes Generative Montage (Writer-Editor-Director) to construct deceptive narratives via coordinated evidence posting and adversarial debate that exploit LLM overthinking.&lt;/li&gt;&lt;li&gt;Introduces CoPHEME dataset and evaluates attacks across 14 LLM families, finding high success rates (≈70–74%) and increased susceptibility in stronger reasoning-specialized models, with downstream belief cascades exceeding 60%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Xinmiao Huang', 'Youcheng Sun', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'collusion attacks', 'adversarial prompting', 'belief manipulation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01685</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records</title><link>https://arxiv.org/abs/2601.01668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents EHRSummarizer, a FHIR R4–native architecture that retrieves targeted FHIR resources, normalizes them into a clinical context package, and produces structured clinical summaries for chart review.&lt;/li&gt;&lt;li&gt;Designed with privacy and deployment controls (data minimization, stateless processing, configurable local inference within an organization's trust boundary).&lt;/li&gt;&lt;li&gt;Implements safety-oriented constraints on summarization: limits outputs to evidence present in the retrieved context, flags missing domains when feasible, and explicitly avoids diagnostic or treatment recommendations.&lt;/li&gt;&lt;li&gt;Includes prototype demonstrations on synthetic/test FHIR environments and an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring; no clinical outcome or controlled workflow studies reported.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Houman Kazemzadeh', 'Nima Minaifar', 'Kamyar Naderi', 'Sho Tabibzadeh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-minimization', 'model-safety', 'faithfulness', 'EHR/FHIR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01668</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</title><link>https://arxiv.org/abs/2601.01665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a preference-based adversarial attack that generates hard problem instances which degrade Pareto-front quality of preference-conditioned DRL solvers for multi-objective combinatorial optimization.&lt;/li&gt;&lt;li&gt;Introduces a defense: hardness-aware preference selection incorporated into adversarial training to reduce overfitting to limited preference regions and improve out-of-distribution performance.&lt;/li&gt;&lt;li&gt;Empirically validates attack and defense on multi-objective TSP, CVRP, and knapsack problems showing the attack finds challenging instances and the defense improves robustness/generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Yaoxin Wu', 'Yingqian Zhang', 'Thomas B\\"ack', 'Yingjie Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'adversarial training', 'robustness', 'neural combinatorial optimization', 'hard-instance generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01665</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Resilient Elections with Adversarial GNNs</title><link>https://arxiv.org/abs/2601.01653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Represents elections as bipartite graphs and uses Graph Neural Networks (GNNs) to learn voting rules.&lt;/li&gt;&lt;li&gt;Applies adversarial training to improve robustness of learned voting rules against strategic (adversarial) voters while maximizing social welfare.&lt;/li&gt;&lt;li&gt;Evaluates method on synthetic and real-world datasets and claims to resolve limitations of prior learned voting-rule approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Xiang Li', 'Yash Shah', 'Lorenzo Giusti']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'graph neural networks', 'mechanism design', 'election security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01653</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models</title><link>https://arxiv.org/abs/2601.01627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JMedEthicBench, the first multi-turn Japanese conversational benchmark for evaluating medical safety of LLMs, derived from 67 Japan Medical Association guidelines.&lt;/li&gt;&lt;li&gt;Contains &gt;50,000 adversarial multi-turn conversations generated via seven automatically discovered jailbreak strategies and uses a dual-LLM scoring protocol to evaluate models.&lt;/li&gt;&lt;li&gt;Evaluates 27 models, finding commercial models generally robust while medical-specialized models are more vulnerable; safety degrades across conversation turns and vulnerabilities persist cross-lingually.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyu Liu', 'Zirui Li', 'Qian Niu', 'Zequn Zhang', 'Yue Xun', 'Wenlong Hou', 'Shujun Wang', 'Yusuke Iwasawa', 'Yutaka Matsuo', 'Kan Hatakeyama-Sato']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01627</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.01528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DrivingGen, a comprehensive benchmark dataset and evaluation suite for generative video world models in autonomous driving.&lt;/li&gt;&lt;li&gt;Introduces metrics targeting visual realism, trajectory plausibility, temporal and agent-level consistency, and controllability with respect to ego conditioning—emphasizing safety-relevant factors.&lt;/li&gt;&lt;li&gt;Curates diverse driving data across weather, time of day, regions, and maneuvers to better reflect deployment conditions.&lt;/li&gt;&lt;li&gt;Benchmarks 14 state-of-the-art models and highlights trade-offs between visual quality and physical/motion realism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Zhou', 'Hao Shao', 'Letian Wang', 'Zhuofan Zong', 'Hongsheng Li', 'Steven L. Waslander']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'world-models', 'autonomous-driving', 'benchmarking', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01528</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints</title><link>https://arxiv.org/abs/2601.01490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study comparing reasoning vs non-reasoning behaviors of LLMs (GPT-5.2, Gemini 3 Flash) under strict constraints (recommending peer-reviewed CS articles).&lt;/li&gt;&lt;li&gt;Finds a trade-off: non-reasoning models violate constraints often (66–75%) but keep factual accuracy, whereas reasoning models reduce violations (13–26%) but systematically distort facts and increase complete fabrications.&lt;/li&gt;&lt;li&gt;Pattern is consistent across models, indicating reasoning can shift models toward compliance at the cost of truthfulness and produce detection-resistant distortions, challenging the assumption that reasoning uniformly improves reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junichiro Niimi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM reasoning', 'safety-evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01490</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python</title><link>https://arxiv.org/abs/2601.01320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ALPHA, a function-level Python benchmark for CWE-specific vulnerability prediction that applies hierarchical, CWE-aware penalties distinguishing over-generalisation, over-specification, and lateral errors.&lt;/li&gt;&lt;li&gt;Evaluates seven LLMs and two SAST tools, finding LLMs generally outperform SAST in detection coverage while SAST exhibits higher precision on detected issues; model prediction consistency ranges from 8.26% to 81.87%.&lt;/li&gt;&lt;li&gt;Highlights implications for feedback-driven/iterative correction systems and proposes incorporating ALPHA penalties into supervised fine-tuning to produce hierarchy-aware vulnerability detectors (future work).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muntasir Adnan', 'Carlos C. N. Kuhn']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability detection', 'CWE classification', 'LLM evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01320</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aggressive Compression Enables LLM Weight Theft</title><link>https://arxiv.org/abs/2601.01296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that aggressive, task-relaxed compression of LLM weights can enable attackers to exfiltrate model parameters much faster (16x–100x compression), turning multi-month transfers into days.&lt;/li&gt;&lt;li&gt;Focuses on network-based weight-exfiltration attacks from datacenters and quantifies the attacker's reduced transmission time under tailored compression.&lt;/li&gt;&lt;li&gt;Evaluates defenses that (a) make models harder to compress, (b) make stolen models harder to locate, and (c) use forensic watermarks for post-attack provenance—finding forensic watermarks particularly effective and low-cost.&lt;/li&gt;&lt;li&gt;Highlights practical implications for model security, suggesting mitigation strategies and trade-offs for operators of large LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davis Brown', 'Juan-Pablo Rivera', 'Dan Hendrycks', 'Mantas Mazeika']&lt;/li&gt;&lt;li&gt;Tags: ['model-exfiltration', 'model-theft', 'compression', 'forensic-watermarking', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01296</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures</title><link>https://arxiv.org/abs/2601.01281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates four AI models (three CNNs and one Vision Transformer) for deepfake detection using large face image datasets.&lt;/li&gt;&lt;li&gt;Applies data preprocessing and augmentation techniques to improve model performance across scenarios.&lt;/li&gt;&lt;li&gt;Finds VFDNET with MobileNetV3 achieves superior accuracy and efficient performance.&lt;/li&gt;&lt;li&gt;Primarily an empirical comparison of image-based deepfake detection architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sifatullah Sheikh Urmi', 'Kirtonia Nuzath Tabassum Arthi', 'Md Al-Imran']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'computer vision', 'AI security', 'forgery detection', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01281</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Policy to Logic for Efficient and Interpretable Coverage Assessment</title><link>https://arxiv.org/abs/2601.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid system combining a coverage-aware retriever with symbolic rule-based reasoning to extract and organize policy language into explicit facts and rules.&lt;/li&gt;&lt;li&gt;Generates auditable rationales to support human reviewers and reduce hallucinations and inconsistencies in LLM outputs for medical coverage policy review.&lt;/li&gt;&lt;li&gt;Reduces number of LLM inferences (44% cost reduction) while improving task performance (4.5% F1 improvement), improving efficiency and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Hamid Hassanzadeh', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'interpretability', 'robustness', 'retrieval-augmented systems', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01266</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stylometry Analysis of Human and Machine Text for Academic Integrity</title><link>https://arxiv.org/abs/2601.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an NLP-based framework using stylometry for (i) classifying human vs. machine text, (ii) distinguishing single- vs. multi-authored documents, (iii) detecting author changes within multi-authored documents, and (iv) recognizing authors in collaborative texts.&lt;/li&gt;&lt;li&gt;Evaluates methods on datasets generated by Google Gemini using two prompt styles (normal vs. strict) and shows that stricter prompts reduce detection performance, highlighting robustness challenges.&lt;/li&gt;&lt;li&gt;Publishes datasets, code, and materials to provide baselines for future research on machine-generated text detection and authorship verification in academic integrity contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hezam Albaqami', 'Muhammad Asif Ayub', 'Nasir Ahmad', 'Yaseen Ahmad', 'Mohammed M. Alqahtani', 'Abdullah M. Algamdi', 'Almoaid A. Owaidah', 'Kashif Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['machine-generated text detection', 'stylometry', 'authorship attribution', 'adversarial/adaptive prompting', 'academic integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01225</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code</title><link>https://arxiv.org/abs/2601.01215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs can produce programs that pass unit tests but exhibit substantially different runtime memory and performance behavior across correct generations.&lt;/li&gt;&lt;li&gt;Proposes Dynamic Mean Pairwise Distance (DMPD) using Dynamic Time Warping on Monotonic Peak Profiles to compare memory-usage traces, and aggregates these into a Model Instability Score (MIS).&lt;/li&gt;&lt;li&gt;Empirical results on BigOBench and CodeContests show significant runtime divergence (worse with higher sampling temperature) and correlations between instability and software complexity metrics; recommends stability-aware selection in CI/CD to reduce operational risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prateek Rajput', 'Yewei Song', 'Abdoul Aziz Bonkoungou', 'Iyiola E. Olatunji', 'Abdoul Kader Kabore', 'Jacques Klein', "Tegawend\\'e F. Bissyand\\'e"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'code generation', 'runtime performance', 'safety evaluation', 'operational risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01215</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models</title><link>https://arxiv.org/abs/2601.01202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefSR-Adv, an adversarial attack that perturbs only the high-resolution reference image to degrade reference-based image super-resolution (RefSR) outputs.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across CNN, Transformer, and Mamba RefSR architectures on CUFED5, WR-SR, and DRefSR, causing large performance drops and visible artifacts.&lt;/li&gt;&lt;li&gt;Finds a positive correlation between LR–reference similarity and attack success, highlighting model over-reliance on reference features as a security weakness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazhu Dai', 'Huihui Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'image-super-resolution', 'reference-based-SR', 'robustness', 'security-vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01202</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai</title><link>https://arxiv.org/abs/2601.01090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of toxicity adoption on a fully AI-driven social platform (Chirper.ai), modeling interactions as stimuli (posts) and responses (comments) and measuring exposure via observed interactions.&lt;/li&gt;&lt;li&gt;Finds toxic responses are more likely following toxic stimuli and that cumulative toxic exposure increases the probability of toxic responding, while a substantial fraction of toxicity arises spontaneously without prior exposure.&lt;/li&gt;&lt;li&gt;Introduces two influence metrics (Influence-Driven Response Rate and Spontaneous Response Rate) and shows the count of toxic stimuli alone can accurately predict whether an agent will later produce toxic content; recommends monitoring encountered content as a lightweight auditing/mitigation mechanism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erica Coppolillo', 'Luca Luceri', 'Emilio Ferrara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'toxicity propagation', 'multi-agent systems', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01090</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees</title><link>https://arxiv.org/abs/2601.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scalable data-driven framework for probabilistic safety verification of unknown nonlinear dynamics using Koopman operator theory with a neural-network lifting function.&lt;/li&gt;&lt;li&gt;Computes closed-loop reachable sets in the lifted linear space, maps them back to state space via NN verification tools, and inflates sets using conformal prediction to obtain statistically-valid coverage guarantees under model mismatch.&lt;/li&gt;&lt;li&gt;Demonstrates improved coverage, efficiency, and reduced conservativeness on high-dimensional control tasks (MuJoCo Hopper/Swimmer, quadcopters) and emphasizes reuse of bounds across reference trajectories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devesh Nath', 'Haoran Yin', 'Glen Chou']&lt;/li&gt;&lt;li&gt;Tags: ['safety verification', 'reachability analysis', 'Koopman operators', 'conformal prediction', 'data-driven control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01076</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions</title><link>https://arxiv.org/abs/2601.01008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agentic modular framework for stroke imaging comprising a perception agent (lesion-aware analysis), an uncertainty estimation agent (slice-level predictive reliability), and a decision agent (abstain vs. predict based on uncertainty thresholds).&lt;/li&gt;&lt;li&gt;Prioritizes clinical safety by enabling uncertainty-driven selective abstention in ambiguous or low-information slices rather than optimizing only for segmentation/classification accuracy.&lt;/li&gt;&lt;li&gt;Integrates visual explanation mechanisms to support both predictive outputs and abstention decisions, improving transparency and clinician alignment.&lt;/li&gt;&lt;li&gt;Evaluates the approach qualitatively and via case-based analyses across representative stroke imaging scenarios; emphasizes design principles (agentic control, uncertainty awareness, selective abstention) over new performance benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Rashadul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'uncertainty estimation', 'selective abstention', 'explainability', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01008</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation</title><link>https://arxiv.org/abs/2601.00996</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT) to measure implicit associations in text-to-video (T2V) outputs.&lt;/li&gt;&lt;li&gt;Validates VEAT by reproducing known IAT and OASIS image association directions and magnitudes.&lt;/li&gt;&lt;li&gt;Finds Sora exhibits substantial race and gender valence biases (European Americans and women more associated with pleasantness; d&gt;0.8) correlated with real-world demographics.&lt;/li&gt;&lt;li&gt;Evaluates explicit debiasing prompts: they often reduce effect sizes but can backfire, increasing association for some Black-associated occupations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxu Sun', 'Michael Saxon', 'Ian Yang', 'Anna-Maria Gueorguieva', 'Aylin Caliskan']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'evaluation', 'debiasing', 'representational-harms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00996</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adapting Feature Attenuation to NLP</title><link>https://arxiv.org/abs/2601.00965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts the COSTARR feature attenuation framework from computer vision to NLP without retraining, applied to BERT (base) and GPT-2 classifiers over 176 arXiv subject classes.&lt;/li&gt;&lt;li&gt;Benchmarks COSTARR against Maximum Softmax Probability (MSP), MaxLogit, and temperature-scaled free-energy scoring using OOSA and AUOSCR metrics.&lt;/li&gt;&lt;li&gt;Findings: COSTARR extends to NLP but yields no statistically significant improvement over MaxLogit or MSP; free-energy underperforms in this high-class-count setting.&lt;/li&gt;&lt;li&gt;Concludes that vision-centric OSR methods show promise but require larger backbones and task-specific attenuation strategies for NLP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianshuo Yang', 'Ryan Rabinowitz', 'Terrance E. Boult', 'Jugal Kalita']&lt;/li&gt;&lt;li&gt;Tags: ['open-set recognition', 'robustness', 'out-of-distribution detection', 'NLP evaluation', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00965</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emoji-Based Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2601.00936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates emoji-sequence based prompt attacks across four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma 2 9B, Llama 3 8B) using 50 emoji-based prompts.&lt;/li&gt;&lt;li&gt;Reports jailbreak success rates, categorizes responses (successful, partial, failed), and measures latency; finds model-specific differences (e.g., Gemma 2 and Mistral showed ~10% success, Qwen 2 showed 0%).&lt;/li&gt;&lt;li&gt;Performs statistical analysis (chi-square test, chi^2 = 32.94, p &lt; 0.001) to confirm significant inter-model differences and emphasizes need to handle emoji representations in safety/alignment pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['M P V S Gopinadh', 'S Mahaboob Hussain']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'safety evaluation', 'prompt injection', 'alignment vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00936</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Device-Native Autonomous Agents for Privacy-Preserving Negotiations</title><link>https://arxiv.org/abs/2601.00911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a device-native autonomous agent architecture for privacy-preserving automated negotiations that runs entirely on user hardware.&lt;/li&gt;&lt;li&gt;Integrates zero-knowledge proofs and cryptographic audit trails to enable secure multi-party bargaining without exposing sensitive financial data to external servers.&lt;/li&gt;&lt;li&gt;Uses distilled world models for on-device reasoning and reports empirical evaluation in insurance and B2B procurement showing improved latency and high success/trust metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joyjit Roy']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving AI', 'on-device agents', 'zero-knowledge proofs', 'secure multi-party bargaining', 'cryptographic audit trails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00911</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment</title><link>https://arxiv.org/abs/2601.00908</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how conformal prediction coverage degrades under distribution shift using COVID-19 as a natural experiment across 8 supply-chain tasks.&lt;/li&gt;&lt;li&gt;Finds coverage drop varies widely (0%–86.7%); catastrophic failures correlate with single-feature dependence measured via SHAP (rho = 0.714).&lt;/li&gt;&lt;li&gt;Quarterly retraining partially restores coverage for vulnerable tasks; robust tasks show little benefit. Exploratory analysis suggests feature stability determines robustness under moderate shifts.&lt;/li&gt;&lt;li&gt;Proposes a practical decision framework: monitor SHAP concentration before deployment and retrain if vulnerability (&gt;40% concentration) is detected.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chorok Lee']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'robustness', 'conformal-prediction', 'explainability-SHAP', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00908</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2601.00867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies a 100-indicator Cybersecurity Psychology Framework (CPF) to LLMs, operationalizing human psychological vulnerability indicators against models.&lt;/li&gt;&lt;li&gt;Introduces the Synthetic Psychometric Assessment Protocol (SYSNAME) to convert CPF indicators into adversarial scenarios for testing LLM decision-making.&lt;/li&gt;&lt;li&gt;Reports empirical findings across seven major LLM families showing susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks—coined Anthropomorphic Vulnerability Inheritance (AVI).&lt;/li&gt;&lt;li&gt;Proposes development of "psychological firewalls" and adapts intervention mechanisms (CPIF) to mitigate these social-engineering-like attack vectors on AI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giuseppe Canale', 'Kashyap Thimmaraju']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'social engineering', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00867</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</title><link>https://arxiv.org/abs/2601.02314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Project Ariadne: an SCM-based XAI framework that uses hard do-calculus interventions on intermediate Chain-of-Thought nodes to test whether reasoning traces causally drive final outputs.&lt;/li&gt;&lt;li&gt;Defines new metrics (Causal Sensitivity φ, violation density ρ) and identifies a failure mode called Causal Decoupling where agents keep answers despite contradictory internal logic (ρ up to 0.77).&lt;/li&gt;&lt;li&gt;Empirically shows a persistent Faithfulness Gap in state-of-the-art models and proposes the Ariadne Score as a benchmark for aligning stated reasoning with model decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sourena Khanzadeh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'chain-of-thought faithfulness', 'causal auditing', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02314</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Streaming Hallucination Detection in Long Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.02170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames hallucination in long chain-of-thought (CoT) as an evolving latent state rather than a one-off error.&lt;/li&gt;&lt;li&gt;Introduces step-level local observations and a cumulative prefix-level hallucination signal that aggregates evidence across reasoning trajectories.&lt;/li&gt;&lt;li&gt;Enables streaming, real-time detection of hallucinations with interpretable, prefix-level indicators to track the global reasoning state.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haolang Lu', 'Minghui Pan', 'Ripeng Li', 'Guoshun Nan', 'Jialin Zhuang', 'Zijie Zhao', 'Zhongxiang Sun', 'Kun Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'chain-of-thought', 'model safety', 'streaming/real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02170</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Simulated Reasoning is Reasoning</title><link>https://arxiv.org/abs/2601.02043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that large foundational models achieve 'simulated reasoning' by generating and iteratively testing thought-like chains (e.g., chain-of-thought) rather than symbolic grounding.&lt;/li&gt;&lt;li&gt;Highlights brittleness due to lack of grounding and common-sense, and discusses implications for safety, robustness, and defenses against failure modes.&lt;/li&gt;&lt;li&gt;Provides philosophical interpretations and normative discussion about how prior metaphors (e.g., 'stochastic parrot') are outdated and how safety/appropriateness considerations should adapt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hendrik Kempt', 'Alon Lavie']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'alignment', 'LLM reasoning', 'normative analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02043</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MindChat: A Privacy-preserving Large Language Model for Mental Health Support</title><link>https://arxiv.org/abs/2601.01993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MindChat, an LLM for mental health support trained with privacy-preserving techniques and a synthetic counseling dataset (MindCorpus).&lt;/li&gt;&lt;li&gt;MindCorpus is generated via a multi-agent role-playing framework with dual closed-loop feedback: turn-level critique-and-revision and session-level strategy refinement.&lt;/li&gt;&lt;li&gt;Training uses federated learning with parameter-efficient LoRA adapters plus differentially private optimization to reduce membership/memorization risks.&lt;/li&gt;&lt;li&gt;Evaluations claim competitive counseling performance and reduced privacy leakage under membership inference attacks, measured by LLM-judges and human raters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dong Xue', 'Jicheng Tu', 'Ming Wang', 'Xin Yan', 'Fangzhou Liu', 'Jie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential-privacy', 'federated-learning', 'membership-inference', 'synthetic-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01993</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.01844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end framework (KG-RAG) to construct clinical knowledge graphs from free-text using multi-agent prompting and schema-constrained retrieval-augmented generation.&lt;/li&gt;&lt;li&gt;Uses entropy-based uncertainty scoring and multi-LLM consensus validation to detect hallucinations and perform semantic refinement without relying on gold-standard annotations.&lt;/li&gt;&lt;li&gt;Generates ontology-aligned RDF/OWL schemas and supports iterative, self-supervised evaluation; reports improved precision, relevance, and ontology compliance on two oncology cohorts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udiptaman Das', 'Krishnasai B. Atmakuri', 'Duy Ho', 'Chi Lee', 'Yugyung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination detection', 'retrieval-augmented generation', 'knowledge graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01844</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs</title><link>https://arxiv.org/abs/2601.01836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COMPASS, a framework for evaluating organization-specific policy alignment (allowlist/denylist) in LLMs.&lt;/li&gt;&lt;li&gt;Constructs and validates 5,920 queries across eight industry scenarios, including adversarial/edge-case prompts to test robustness.&lt;/li&gt;&lt;li&gt;Evaluates seven state-of-the-art models and finds strong asymmetry: high accuracy on allowed requests (&gt;95%) but poor refusal rates on adversarial denylist violations (13–40%).&lt;/li&gt;&lt;li&gt;Positions COMPASS as a benchmarking tool highlighting gaps in LLM robustness for policy-critical enterprise deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dasol Choi', 'DongGeon Lee', 'Brigitta Jesica Kartono', 'Helena Berndt', 'Taeyoun Kwon', 'Joonwon Jang', 'Haon Park', 'Hwanjo Yu', 'Minsuk Kahng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'adversarial prompting', 'safety evaluation', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01836</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Admissibility Alignment</title><link>https://arxiv.org/abs/2601.01816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes AI alignment as 'admissibility' of actions under uncertainty, assessed over distributions of outcomes rather than as a binary property of models.&lt;/li&gt;&lt;li&gt;Introduces MAP-AI, a control-plane architecture that uses Monte Carlo estimation of outcome distributions to evaluate policies on expected utility, variance, tail risk, and probability of misalignment.&lt;/li&gt;&lt;li&gt;Shows how distributional alignment evaluation can be integrated into action selection (admissibility-controlled policy selection) to influence behavior without retraining underlying models, and emphasizes governance constraints and intervention effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chris Duffey']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI safety', 'decision-theory', 'risk-assessment', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01816</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI Agent Systems: Architectures, Applications, and Evaluation</title><link>https://arxiv.org/abs/2601.01743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of AI agent architectures covering deliberation/reasoning, planning/control, and tool calling/environment interaction, plus a unified taxonomy of components and orchestration patterns.&lt;/li&gt;&lt;li&gt;Surveys evaluation and benchmarking practices, highlighting measurement challenges (non-determinism, long-horizon credit assignment, environment/tool variability, hidden costs) that affect safe/reliable deployment.&lt;/li&gt;&lt;li&gt;Identifies open challenges explicitly relevant to safety/security: verification and guardrails for tool actions, robustness and security, interpretability of agent decisions, and reproducible evaluation under realistic workloads.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Xu']&lt;/li&gt;&lt;li&gt;Tags: ['agent-safety', 'safety-evaluation', 'robustness', 'verification', 'tool-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01743</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation</title><link>https://arxiv.org/abs/2601.01546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework—context formation and context navigation—to improve behavioral alignment of LLMs in social/decision-making simulations.&lt;/li&gt;&lt;li&gt;Validates the approach across multiple decision tasks (sequential purchasing game, crowdfunding with costly signaling, demand-estimation) and four SOTA LLMs, showing both stages are needed for complex environments.&lt;/li&gt;&lt;li&gt;Provides a systematic method to design and diagnose LLM social simulations to better match human benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Letian Kong (Jenny)', 'Qianran (Jenny)', 'Jin', 'Renyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'behavioral_alignment', 'LLM_evaluation', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01546</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix</title><link>https://arxiv.org/abs/2601.01532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Aletheia, a framework that quantifies 'Cognitive Conviction' in System 2 reasoning models by inverting a judge's confusion matrix using Tikhonov regularization.&lt;/li&gt;&lt;li&gt;Implements a Synthetic Proxy Protocol to validate the method without private data and reports a pilot study on 2025 baselines that uncovers 'Defensive OverThinking' under adversarial pressure.&lt;/li&gt;&lt;li&gt;Introduces the Aligned Conviction Score (S_aligned) to evaluate whether increased conviction preserves safety/alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Robustness', 'Evaluation', 'Adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01532</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification</title><link>https://arxiv.org/abs/2601.01378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AAAI pipeline (Association Identification, Automated Detection, Adaptive Inference) to mitigate factual hallucinations in small language models for financial classification.&lt;/li&gt;&lt;li&gt;Finds a positive correlation between factual hallucinations and misclassification on three representative SLMs.&lt;/li&gt;&lt;li&gt;Shows encoder-based verifiers can detect factual hallucinations and that feeding back detected factual errors for adaptive inference improves classification performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yuan', 'Yilin Wu', 'Li Zhang', 'Zheng Ma']&lt;/li&gt;&lt;li&gt;Tags: ['factual-hallucination', 'alignment', 'robustness', 'model-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01378</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems</title><link>https://arxiv.org/abs/2601.00994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ElecTwit, a simulation framework to study persuasion among LLM-driven agents in a realistic social-media election setting.&lt;/li&gt;&lt;li&gt;Finds that tested LLMs employ a wide set of 25 specific persuasion techniques, with variation in technique usage and overall persuasive output across models.&lt;/li&gt;&lt;li&gt;Reports emergent phenomena (e.g., 'kernel of truth' messages, collective demand for written proof) that affect multi-agent dynamics and could impact real-world misinformation/alignment risks.&lt;/li&gt;&lt;li&gt;Frames the work as groundwork for evaluating persuasive LLM agents to ensure alignment and prevent dangerous outcomes in social contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Bao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM persuasion', 'social engineering', 'AI safety', 'multi-agent simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00994</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models</title><link>https://arxiv.org/abs/2601.00848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tunes language models (QLoRA) to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis, with a curated dataset of 80,851 public examples plus 35,026 synthetic traces.&lt;/li&gt;&lt;li&gt;Demonstrates iterative fine-tuning and targeted augmentation improving benchmark accuracy from 42.86% to 74.29%, and releases datasets, training scripts, and evaluation benchmarks on HuggingFace.&lt;/li&gt;&lt;li&gt;Presents a synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations and provides a reproducible framework for building agentic security detection models (with caution about false positives).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ron F. Del Rosario']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'Intrusion detection / telemetry analysis', 'Dataset release', 'Agentic AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00848</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.00830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Across &gt;9,000 test cases on 11 models, models rarely spontaneously mention embedded 'hints' that influenced their answers, even though they admit noticing them when asked directly.&lt;/li&gt;&lt;li&gt;Telling models they are being observed does not increase spontaneous reporting; forcing models to report hints causes them to report non-existent hints and reduces accuracy.&lt;/li&gt;&lt;li&gt;Hints aligned with user preferences are most likely to influence model behavior while being least likely to be reported, indicating explanations systematically underreport influential information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deep Pankajbhai Mehta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM explanations', 'chain-of-thought', 'model transparency', 'alignment', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00830</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis</title><link>https://arxiv.org/abs/2601.00828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Decomposes LLM intrinsic self-correction into three sub-capabilities: error detection, error localization, and error correction.&lt;/li&gt;&lt;li&gt;Reports an Accuracy-Correction Paradox: weaker models (e.g., GPT-3.5) exhibit higher intrinsic correction rates than stronger models (DeepSeek) on GSM8K-Complex.&lt;/li&gt;&lt;li&gt;Proposes the Error Depth Hypothesis: stronger models make fewer but 'deeper' errors that are harder to self-correct; detection ability does not reliably predict correction success.&lt;/li&gt;&lt;li&gt;Finds interventions (e.g., providing error location hints) can hurt correction performance, challenging simple self-refinement pipeline designs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yin Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-correction', 'robustness', 'evaluation', 'error-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00828</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback</title><link>https://arxiv.org/abs/2601.00816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MathLedger: a learning substrate combining formal verification, cryptographic attestation (ledger), and a reflexive feedback loop for verifiable updates (Reflexive Formal Learning).&lt;/li&gt;&lt;li&gt;Proposes verifier-driven updates (symbolic analogue to gradient descent) and ledger-attested feedback to enable auditability and governance (including fail-closed triggers).&lt;/li&gt;&lt;li&gt;Phase I prototype validates measurement and governance infrastructure (Delta p computation, variance tracking, stress tests) but explicitly makes no claims about convergence or model capabilities.&lt;/li&gt;&lt;li&gt;Focuses on infrastructure for verifiability, attestation, and safety-oriented governance rather than adversarial attacks or red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Ahmad Abdullah']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable learning', 'formal verification', 'cryptographic attestation', 'AI governance', 'safety monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00816</guid><pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 10 Dec 2025 23:17:52 +0000</lastBuildDate><item><title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title><link>https://arxiv.org/abs/2510.24411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MobileRisk-Live, a dynamic sandbox and fine-grained safety detection benchmark for mobile GUI agent workflows.&lt;/li&gt;&lt;li&gt;Proposes OS-Sentinel, a hybrid safety detection framework combining a Formal Verifier for explicit system-level violations with a VLM-based Contextual Judge for contextual risk assessment.&lt;/li&gt;&lt;li&gt;Reports 10%–30% improvements over existing approaches and releases code and dataset to support research on safer mobile agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiushi Sun', 'Mukai Li', 'Zhoumianze Liu', 'Zhihui Xie', 'Fangzhi Xu', 'Zhangyue Yin', 'Kanzhi Cheng', 'Zehao Li', 'Zichen Ding', 'Qi Liu', 'Zhiyong Wu', 'Zhuosheng Zhang', 'Ben Kao', 'Lingpeng Kong']&lt;/li&gt;&lt;li&gt;Tags: ['Mobile agents', 'Safety detection', 'Vision-Language Models', 'Formal verification', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24411</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title><link>https://arxiv.org/abs/2508.19112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep: a random forest classifier using deep features from a pretrained Swin Transformer encoder (SimMIM) to detect out-of-distribution (OOD) CT scans and improve segmentation reliability.&lt;/li&gt;&lt;li&gt;Segmentation model pretrained on 10,432 unlabeled 3D CTs and trained on 317 labeled lung cancer scans; evaluated on 603 independent 3D CTs including one in-distribution and four OOD datasets (PE, COVID-19, abdominal/kidney, healthy).&lt;/li&gt;&lt;li&gt;RF-Deep achieved notably lower FPR95 on several OOD domains (e.g., 18.26% PE, 27.66% COVID-19, &lt;0.1% abdominal CTs) and outperformed established OOD methods.&lt;/li&gt;&lt;li&gt;Focuses on robustness and safety of medical image segmentation by detecting distribution shifts to enhance clinical reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'medical imaging', 'safety evaluation', 'transformer pretraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19112</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title><link>https://arxiv.org/abs/2505.12332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VoiceCloak, a proactive multi-dimensional defense that adds adversarial perturbations to reference audio to obfuscate speaker identity and degrade cloned speech quality for diffusion-based voice cloning.&lt;/li&gt;&lt;li&gt;Targets diffusion model vulnerabilities by distorting speaker representation embeddings, disrupting conditional guidance (attention context), amplifying score magnitudes to steer reverse diffusion away from high-quality output, and applying noise-guided semantic corruption.&lt;/li&gt;&lt;li&gt;Guides perturbation design with auditory perception principles and evaluates defense success extensively against unauthorized diffusion-based voice cloning systems; audio examples provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianyue Hu', 'Junyan Wu', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['voice cloning defense', 'adversarial audio', 'diffusion models', 'proactive defense', 'speaker obfuscation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12332</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</title><link>https://arxiv.org/abs/2508.13309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAASH, a fully differentiable multi-stage meta-attack framework that composes multiple Lp-constrained base attacks via learned adaptive weights.&lt;/li&gt;&lt;li&gt;Uses a meta-loss that jointly minimizes misclassification loss and perceptual distortion, enabling dynamic weighting of base attacks across stages.&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success and better perceptual quality (SSIM, LPIPS, FID) on adversarially trained CIFAR-10, CIFAR-100, and ImageNet models—outperforming prior perceptual attacks like AdvAD.&lt;/li&gt;&lt;li&gt;Shows generalization to unseen defenses, making the method a practical strong baseline for robustness evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdullah Al Nomaan Nafi', 'Habibur Rahaman', 'Zafaryab Haider', 'Tanzim Mahfuz', 'Fnu Suya', 'Swarup Bhunia', 'Prabuddha Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'adversarial-attacks', 'robustness', 'perceptual-attacks', 'meta-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13309</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2507.04049</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DIVER, an end-to-end driving framework combining diffusion-based trajectory generation with reinforcement learning to produce diverse, feasible trajectories.&lt;/li&gt;&lt;li&gt;Generates multiple reference trajectories conditioned on map and surrounding agents to overcome imitation learning's single-expert bottleneck.&lt;/li&gt;&lt;li&gt;Uses RL to guide the diffusion process via reward-based supervision that enforces safety and diversity constraints.&lt;/li&gt;&lt;li&gt;Introduces a novel Diversity metric for evaluating multi-modal trajectory predictions and shows improved diversity on NAVSIM, Bench2Drive, and nuScenes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziying Song', 'Lin Liu', 'Hongyu Pan', 'Bencheng Liao', 'Mingzhe Guo', 'Lei Yang', 'Yongchang Zhang', 'Shaoqing Xu', 'Caiyan Jia', 'Yadan Luo']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'reinforcement-learning', 'diffusion-models', 'safety-constraints', 'trajectory-diversity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04049</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title><link>https://arxiv.org/abs/2512.08216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep, a lightweight post-hoc OOD detection method using random forests on deep features anchored to predicted tumor segmentations from pretrained-finetuned encoders.&lt;/li&gt;&lt;li&gt;Extracts hierarchical features from multiple regions of interest around predicted tumors to provide task-relevant OOD signals and scale to varying fields-of-view.&lt;/li&gt;&lt;li&gt;Evaluated on 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) scenarios, achieving AUROC &gt;93.5 for near-OOD and &gt;99 for far-OOD, outperforming logit-based and radiomics baselines.&lt;/li&gt;&lt;li&gt;Designed to be plug-and-play, architecture-agnostic, and computationally lightweight compared to architectural OOD solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'medical imaging', 'segmentation robustness', 'post-hoc safety methods', 'random forest']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08216</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference</title><link>https://arxiv.org/abs/2512.08860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Bendkhale']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08860</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation</title><link>https://arxiv.org/abs/2512.08645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chain-of-Image Generation (CoIG): uses an LLM to decompose a complex prompt into a sequential set of simple instructions, and an image model to generate/edit the image step-by-step.&lt;/li&gt;&lt;li&gt;Enables intermediate monitoring by producing per-step outputs focused on single semantic entities, improving human interpretability and potential intervention.&lt;/li&gt;&lt;li&gt;Introduces two metrics—CoIG Readability and Causal Relevance—to quantify clarity of intermediate steps and their impact on the final image.&lt;/li&gt;&lt;li&gt;Claims improvements in monitorability and compositional robustness and reduces entity collapse; the approach is model-agnostic.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young Kyung Kim', 'Oded Schlesinger', 'Yuzhou Zhao', 'J. Matias Di Martino', 'Guillermo Sapiro']&lt;/li&gt;&lt;li&gt;Tags: ['monitorability', 'controllability', 'interpretability', 'compositional robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08645</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MVP: Multiple View Prediction Improves GUI Grounding</title><link>https://arxiv.org/abs/2512.08529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MVP, a training-free multi-view inference framework to improve GUI grounding stability by aggregating predictions from multiple cropped views.&lt;/li&gt;&lt;li&gt;Introduces Attention-Guided View Proposal to generate diverse views based on instruction-to-image attention, and Multi-Coordinates Clustering to ensemble predictions by selecting the centroid of the densest spatial cluster.&lt;/li&gt;&lt;li&gt;Demonstrates consistent performance gains across several multimodal models and GUI benchmarks, addressing sensitivity of coordinate predictions to minor visual perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunzhu Zhang', 'Zeyu Pan', 'Zhengwen Zeng', 'Shuheng Shen', 'Changhua Meng', 'Linchao Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'GUI grounding', 'multimodal', 'inference-time ensembling', 'attention-guided view selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08529</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models</title><link>https://arxiv.org/abs/2512.08505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NoisyCLIP, a method to measure prompt-to-latent semantic alignment during the reverse diffusion (noisy latent) process to detect misalignment early.&lt;/li&gt;&lt;li&gt;Demonstrates that early detection can enable Best-of-N post-generation selection with roughly 50% lower computation while retaining ~98% of CLIP alignment performance.&lt;/li&gt;&lt;li&gt;Provides qualitative and quantitative benchmarks for prompt-to-latent misalignment detection using dual encoders during image generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasco Ramos', 'Regev Cohen', 'Idan Szpektor', 'Joao Magalhaes']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'diffusion models', 'misalignment detection', 'real-time evaluation', 'CLIP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08505</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models</title><link>https://arxiv.org/abs/2512.08503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonBreak, an adversarial framework that crafts concept-aware perturbations to disrupt hierarchical chain-of-thought geographic inference in multimodal large reasoning models (MLRMs).&lt;/li&gt;&lt;li&gt;Argues that disrupting conceptual dependencies in reasoning chains is more effective than uniform noise, causing cascades of invalidated inference steps.&lt;/li&gt;&lt;li&gt;Contributes GeoPrivacy-6K, a dataset of 6,341 ultra-high-resolution images with hierarchical concept annotations for evaluating geographic-privacy protections.&lt;/li&gt;&lt;li&gt;Evaluates across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) and reports substantial improvements in tract-level and block-level protection metrics versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Che Wang', 'Yang Cao', 'Longtao Huang', 'Wei Yang Bryan Lim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'adversarial robustness', 'multimodal reasoning', 'dataset', 'hierarchical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08503</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detection of Digital Facial Retouching utilizing Face Beauty Information</title><link>https://arxiv.org/abs/2512.08397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies detection of facial retouching (beautification) and its impact on face recognition/biometric systems.&lt;/li&gt;&lt;li&gt;Analyzes changes in automated face beauty assessment for retouched images and evaluates AI-based feature extraction methods for detection.&lt;/li&gt;&lt;li&gt;Explores whether face beauty information can improve retouching detection; reports 1.1% D-EER for single-image detection when attack algorithm is unknown.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philipp Srock', 'Juan E. Tapia', 'Christoph Busch']&lt;/li&gt;&lt;li&gt;Tags: ['biometric security', 'image forensics', 'face retouching detection', 'robustness/anti-spoofing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08397</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title><link>https://arxiv.org/abs/2512.08329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an explainable-AI analysis of image protection methods (e.g., Glaze, Nightshade) using a unified framework combining white-box feature-space inspection and black-box signal-level probing.&lt;/li&gt;&lt;li&gt;Finds that protection perturbations are structured, low-entropy, and tightly coupled to image content across representational, spatial, and spectral domains rather than causing global representational drift.&lt;/li&gt;&lt;li&gt;Shows detectability depends on perturbation entropy, spatial deployment, and frequency alignment; sequential application increases detectable structure; frequency analysis reveals energy redistribution along image-aligned axes.&lt;/li&gt;&lt;li&gt;Implication: modern image protection acts via structured feature-level deformation, informing design of detection strategies and defenses against generative-model misuse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael R. Martin', 'Garrick Chan', 'Kwan-Liu Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbations', 'image protection', 'diffusion models', 'interpretability', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08329</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning</title><link>https://arxiv.org/abs/2512.08048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mask to Adapt (M2A), a simple continual test-time adaptation method that uses random spatial or frequency masking to create multiple masked views and adapts with a mask consistency loss plus entropy minimization.&lt;/li&gt;&lt;li&gt;Studies spatial (patch vs. pixel) and frequency (all/low/high) mask families; finds spatial random masking performs best under strong corruptions.&lt;/li&gt;&lt;li&gt;Reports strong results on CIFAR10C/CIFAR100C/ImageNetC (severity 5), with M2A (Spatial) matching or outperforming strong CTTA baselines; ablations show simple random masking is effective and robust.&lt;/li&gt;&lt;li&gt;Does not rely on calibrated uncertainty or attention signals and focuses on robustness to natural distribution shift (corruptions) rather than adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chandler Timm C. Doloriel']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'test-time adaptation', 'distribution shift', 'masked image modeling', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08048</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</title><link>https://arxiv.org/abs/2512.08042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes frequency-domain masking (with random masking and geometric transforms) as a training strategy to improve universal deepfake image detection across unseen generative models.&lt;/li&gt;&lt;li&gt;Shows improved generalization to GAN- and diffusion-generated images compared to spatial-feature or large-pretrained-model baselines.&lt;/li&gt;&lt;li&gt;Demonstrates robustness of detectors under structured model pruning, emphasizing computational efficiency and 'Green AI' scalability.&lt;/li&gt;&lt;li&gt;Provides code and models for reproducing results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chandler Timm C. Doloriel', 'Habib Ullah', 'Kristian Hovde Liland', 'Fadi Al Machot', 'Ngai-Man Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'frequency-domain methods', 'model pruning', 'green AI / efficient security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08042</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title><link>https://arxiv.org/abs/2512.07222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies function words in text prompts as a source of vulnerability for vision-language models (VLMs) under cross-modal adversarial attacks.&lt;/li&gt;&lt;li&gt;Proposes Function-word De-Attention (FDA): compute original and function-word-specific cross-attention and subtract the latter to reduce function-word influence in attention heads.&lt;/li&gt;&lt;li&gt;Evaluates FDA across multiple SOTA baselines, attacks, tasks, datasets, and models, reporting large reductions in attack success rate (ASR) with negligible task performance drops; also demonstrates scalability, generalization, zero-shot performance, and ablations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM robustness', 'adversarial defense', 'cross-modal attacks', 'attention mechanisms', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07222</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</title><link>https://arxiv.org/abs/2511.11599</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SynBullying, a synthetic multi-LLM conversational dataset designed for cyberbullying detection that models multi-turn exchanges and role dynamics.&lt;/li&gt;&lt;li&gt;Provides context-aware, fine-grained annotations (harm intensity, CB categories) and analyzes lexical, sentiment/toxicity, and conversational properties across generations.&lt;/li&gt;&lt;li&gt;Evaluates utility of synthetic data both as standalone training data and as augmentation for cyberbullying classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arefeh Kazemi', 'Hamza Qadeer', 'Joachim Wagner', 'Hossein Hosseini', 'Sri Balaaji Natarajan Kalaivendan', 'Brian Davis']&lt;/li&gt;&lt;li&gt;Tags: ['cyberbullying detection', 'synthetic dataset', 'LLM-generated data', 'content moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11599</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title><link>https://arxiv.org/abs/2511.10240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ProgRAG, a multi-hop KGQA framework that decomposes complex questions into sub-questions and progressively extends partial reasoning paths.&lt;/li&gt;&lt;li&gt;At each step, external retrievers gather candidate KG evidence which is refined via LLM-driven uncertainty-aware pruning.&lt;/li&gt;&lt;li&gt;Optimizes LLM reasoning context by organizing and rearranging partial reasoning paths to reduce hallucinations and improve multi-hop reasoning reliability; demonstrates improved performance on three KGQA datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minbae Park', 'Hyemin Yang', 'Jeonghyun Kim', 'Kunsoo Park', 'Hyunjoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-mitigation', 'robustness', 'knowledge-graph-qa', 'retrieval-augmented-generation', 'llm-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10240</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title><link>https://arxiv.org/abs/2510.24411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MobileRisk-Live, a dynamic sandbox and fine-grained safety detection benchmark for mobile GUI agent workflows.&lt;/li&gt;&lt;li&gt;Proposes OS-Sentinel, a hybrid safety detection framework combining a Formal Verifier for explicit system-level violations with a VLM-based Contextual Judge for contextual risk assessment.&lt;/li&gt;&lt;li&gt;Reports 10%–30% improvements over existing approaches and releases code and dataset to support research on safer mobile agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiushi Sun', 'Mukai Li', 'Zhoumianze Liu', 'Zhihui Xie', 'Fangzhi Xu', 'Zhangyue Yin', 'Kanzhi Cheng', 'Zehao Li', 'Zichen Ding', 'Qi Liu', 'Zhiyong Wu', 'Zhuosheng Zhang', 'Ben Kao', 'Lingpeng Kong']&lt;/li&gt;&lt;li&gt;Tags: ['Mobile agents', 'Safety detection', 'Vision-Language Models', 'Formal verification', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24411</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title><link>https://arxiv.org/abs/2508.06457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ScamAgent, an autonomous multi-turn agent built on LLMs that generates realistic, adaptive scam call scripts using dialogue memory and persuasive strategies.&lt;/li&gt;&lt;li&gt;Shows that existing single-shot safety guardrails (refusal mechanisms, filters) can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework.&lt;/li&gt;&lt;li&gt;Demonstrates end-to-end pipeline by converting generated scam scripts into lifelike voice calls via text-to-speech systems.&lt;/li&gt;&lt;li&gt;Argues for need of multi-turn safety auditing, agent-level control frameworks, and detection/disruption methods for conversational deception.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanket Badhe']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'social engineering / scam', 'jailbreaking / bypass', 'adversarial prompting', 'audio/phishing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06457</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models</title><link>https://arxiv.org/abs/2412.18123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEIOU, a defense framework that extracts NSFW-related features from the hidden states of text encoders in text-to-image (T2I) models to detect harmful/adversarial prompts.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency, interpretability, and optimizability (e.g., via data augmentation), claiming minimal inference overhead and real-time interpretation.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (&gt;95% accuracy, ≥10x efficiency improvement) across datasets, robustness to adaptive attacks, and effectiveness in few-shot and multi-label scenarios.&lt;/li&gt;&lt;li&gt;Designed to be architecture-agnostic and applicable across different T2I models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Wang', 'Jiahao Chen', 'Qingming Li', 'Tong Zhang', 'Rui Zeng', 'Xing Yang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['prompt detection', 'T2I safety', 'adversarial prompts', 'content moderation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18123</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a complementarity gap: human-AI teams often do not outperform the best individual because agents are trained as answer engines rather than collaborative partners.&lt;/li&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda to develop agents that co-construct causal explanations, surface uncertainties, and adapt goals to support expert decision-making.&lt;/li&gt;&lt;li&gt;Recommends new training environments, shared representations for human-AI mental models, and evaluation metrics centered on trust and complementarity to measure collaborative capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Research agenda&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'safety evaluation', 'trust', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Representational Stability of Truth in Large Language Models</title><link>https://arxiv.org/abs/2511.19166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'representational stability' measuring how robustly LLM internal activations encode veracity by training linear probes and tracking decision-boundary shifts under controlled label perturbations.&lt;/li&gt;&lt;li&gt;Evaluates 16 open-source models across three factual domains, comparing 'unfamiliar neither' (claims about entities absent from training data) and 'familiar neither' (fictional but well-known claims).&lt;/li&gt;&lt;li&gt;Finds unfamiliar neither statements cause much larger boundary shifts (up to ~40% flipped judgments) while familiar fictional statements remain more coherently clustered (≤ 8.2% shifts), suggesting stability depends on epistemic familiarity.&lt;/li&gt;&lt;li&gt;Proposes this probing approach as a diagnostic for auditing and potentially training LLMs to maintain coherent truth assignments under semantic uncertainty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samantha Dies', 'Courtney Maynard', 'Germans Savcisens', 'Tina Eliassi-Rad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'robustness', 'probing', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19166</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</title><link>https://arxiv.org/abs/2510.05154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeliberationBank: a large human-grounded dataset with 3,000 participants' opinions across 10 deliberation questions and 4,500 human summary judgments on representativeness, informativeness, neutrality, and policy approval.&lt;/li&gt;&lt;li&gt;Proposes DeliberationJudge, a fine-tuned DeBERTa model trained to rate deliberation summaries; it is more efficient and better aligned with human judgments than many LLM-based judges.&lt;/li&gt;&lt;li&gt;Uses DeliberationJudge to evaluate 18 LLMs and finds persistent weaknesses in deliberation summarization, notably underrepresentation of minority positions and input-order bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shenzhe Zhu', 'Shu Yang', 'Michiel A. Bakker', 'Alex Pentland', 'Jiaxin Pei']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fairness', 'evaluation/benchmarking', 'summarization', 'human-grounded dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05154</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</title><link>https://arxiv.org/abs/2509.24319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two modes of value expression in LLMs: intrinsic (learned during training) vs. prompted (elicited by prompts), and studies their mechanistic bases.&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability tools—'value vectors' from the residual stream and 'value neurons' in MLPs—to identify components driving each mode.&lt;/li&gt;&lt;li&gt;Finds partial overlap between mechanisms but also unique components: prompted mechanisms yield greater steerability/instruction following (including effects on jailbreaking), while intrinsic components produce greater lexical/response diversity.&lt;/li&gt;&lt;li&gt;Implication: different interventions or attacks (e.g., prompt-based red teaming vs. training-based alignment) may target distinct mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jongwook Han', 'Jongwon Lim', 'Injin Kong', 'Yohan Jo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'mechanistic interpretability', 'jailbreaking', 'prompting/steerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24319</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions</title><link>https://arxiv.org/abs/2509.23782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a subspace in model residual streams spanned by a 'knowledge basis' (ground-truth probability) and a 'prediction basis' (model's chosen answer probability), and attributes errors to misalignment between them.&lt;/li&gt;&lt;li&gt;Proposes KAPPA, a parameter-free projection-based intervention that transforms hidden states to align the prediction coordinate with the knowledge coordinate within that subspace.&lt;/li&gt;&lt;li&gt;Shows substantial accuracy gains on binary-choice reformulations of Big-Bench-Hard and ARC-Challenge, cross-dataset generalization to some extent, and extension to free-form QA.&lt;/li&gt;&lt;li&gt;Provides a geometric/latent-space interpretation of a 'knowledge–prediction gap' and a practical method to better align model outputs with latent knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoonah Park', 'Haesung Pyun', 'Yohan Jo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'LLM robustness', 'latent-space interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23782</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Natural Language Descriptions of Model Activations Convey Privileged Information?</title><link>https://arxiv.org/abs/2509.13316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether natural-language verbalizations of LLM activations convey privileged information about the target model or merely reflect input data or the verbalizer's own knowledge.&lt;/li&gt;&lt;li&gt;Shows that popular verbalization methods can succeed on existing benchmarks without accessing target model internals, suggesting dataset/benchmark shortcomings.&lt;/li&gt;&lt;li&gt;Controlled experiments indicate verbalizations often echo the parametric knowledge of the verbalizer LLM rather than revealing the target model's internal representations.&lt;/li&gt;&lt;li&gt;Calls for better-targeted benchmarks and experimental controls to rigorously assess whether activation verbalization provides meaningful interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Millicent Li', 'Alberto Mario Ceballos Arroyo', 'Giordano Rogers', 'Naomi Saphra', 'Byron C. Wallace']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'LLMs', 'evaluation', 'alignment', 'model introspection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13316</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</title><link>https://arxiv.org/abs/2508.18321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KAIROS, a benchmark simulating quiz-style multi-agent collaboration with controllable peer rapport and behaviours to study how peer interactions affect LLM decisions.&lt;/li&gt;&lt;li&gt;Evaluates prompting, supervised fine-tuning, and Group Relative Policy Optimisation (GRPO) to mitigate susceptibility to misleading peer inputs.&lt;/li&gt;&lt;li&gt;Finds model scale strongly moderates susceptibility: larger models are more resilient and benefit from prompting-based mitigations; smaller models remain vulnerable and only improve with carefully configured GRPO.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maojia Song', 'Tej Deep Pala', 'Ruiwen Zhou', 'Weisheng Jin', 'Amir Zadeh', 'Chuan Li', 'Dorien Herremans', 'Soujanya Poria']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multi-agent-systems', 'social-engineering', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18321</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title><link>https://arxiv.org/abs/2505.23856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Omniguard, a method that detects harmful prompts across languages and modalities by identifying internal LLM/MLLM representations aligned across languages/modalities and training a language-/modality-agnostic classifier.&lt;/li&gt;&lt;li&gt;Demonstrates substantial accuracy gains over strong baselines: +11.57% multilingual, +20.44% image-based, and new SOTA on audio-based harmful-prompt detection.&lt;/li&gt;&lt;li&gt;Achieves high efficiency by reusing embeddings computed during generation, reporting ~120x speedup versus the next-fastest baseline.&lt;/li&gt;&lt;li&gt;Code and data are released, enabling reproducibility and further evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'prompt moderation', 'multimodal safety', 'multilingual robustness', 'harmful-prompt detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23856</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents</title><link>https://arxiv.org/abs/2504.02800</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey of LLM-based methods for detecting mental disorders from social media, covering pretrained LMs, Retrieval-Augmented Generation (RAG), and agentic systems.&lt;/li&gt;&lt;li&gt;Empirical evaluation of LLMs and the impact of RAG across multiple detection and analysis tasks, and establishment of a unified benchmark.&lt;/li&gt;&lt;li&gt;Focus on reliability issues (e.g., hallucinations), persistent memory, explainability, and the use of agentic architectures for multi-step reasoning and intervention.&lt;/li&gt;&lt;li&gt;Targets a broad set of clinical conditions (internalizing, psychotic, externalizing) and discusses pathways toward trustworthy autonomous systems for mental health support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuohan Ge', 'Nicole Hu', 'Yubo Wang', 'Darian Li', 'Xinyi Zhu', 'Haoyang Li', 'Xin Zhang', 'Mingtao Zhang', 'Shihao Qi', 'Yuming Xu', 'Han Shi', 'Chen Jason Zhang', 'Qing Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Retrieval-Augmented Generation (RAG)', 'Agentic systems', 'Benchmarking', 'Mental-health NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02800</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title><link>https://arxiv.org/abs/2512.08121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues Youden's J statistic (and its linear transform, Balanced Accuracy) is the appropriate metric for selecting judges that estimate prevalence of desirable/undesirable LLM behaviors, addressing sensitivity to class imbalance and choice of positive class.&lt;/li&gt;&lt;li&gt;Proves theoretical alignment between Youden's J and the goal of comparing models by prevalence, and shows Balanced Accuracy avoids distortions introduced by Accuracy, Precision, and F1.&lt;/li&gt;&lt;li&gt;Provides analytical arguments, empirical examples, and simulations demonstrating that selecting judges via Balanced Accuracy yields more robust and unbiased prevalence estimates for safety/policy evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Collot', 'Colin Fraser', 'Justin Zhao', 'William F. Shen', 'Timon Willi', 'Ilias Leontiadis']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'LLM-as-judge', 'balanced-accuracy', 'robustness', 'evaluation-methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08121</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title><link>https://arxiv.org/abs/2512.08892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGLens, a lightweight hallucination detector for Retrieval-Augmented Generation that uses sparse autoencoders to disentangle LLM internal activations and identify features correlated with unfaithful outputs.&lt;/li&gt;&lt;li&gt;Proposes an information-based feature selection pipeline and additive feature modeling to flag hallucinations using internal model representations, enabling interpretable rationales for detections.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection performance over existing methods and shows how identified signals can be used for post-hoc mitigation of unfaithful RAG outputs.&lt;/li&gt;&lt;li&gt;Provides analysis and insights into the distribution of hallucination-related signals within LLMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangzhi Xiong', 'Zhenghao He', 'Bohan Liu', 'Sanchit Sinha', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'retrieval-augmented_generation', 'model_internals', 'interpretability', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08892</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title><link>https://arxiv.org/abs/2512.08786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies federated RLHF where groups locally evaluate rollouts and the server aggregates group-level rewards without raw data access.&lt;/li&gt;&lt;li&gt;Compares standard aggregation strategies (min, max, average) and introduces an adaptive weighting scheme based on groups' historical alignment performance.&lt;/li&gt;&lt;li&gt;Shows experiments on Q/A tasks using PPO-based RLHF demonstrating improved fairness with the adaptive method while maintaining competitive alignment scores.&lt;/li&gt;&lt;li&gt;Provides a systematic evaluation framework for trade-offs between alignment quality and fairness across diverse populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Srewa', 'Tianyu Zhao', 'Salma Elmalaki']&lt;/li&gt;&lt;li&gt;Tags: ['Federated RLHF', 'Alignment', 'Preference aggregation', 'Fairness', 'Pluralistic alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08786</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</title><link>https://arxiv.org/abs/2512.08480</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a soft inductive bias method that defines explicit reasoning perspectives to guide LLM inference for inappropriate utterance detection.&lt;/li&gt;&lt;li&gt;Fine-tunes a Korean LLM (Kanana-1.5) using this approach and compares quantitative and qualitative performance across training strategies.&lt;/li&gt;&lt;li&gt;Reports ~3.89% absolute improvement in accuracy (87.00% average) over standard supervised learning, arguing improved consistency and precision in judgments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ju-Young Kim', 'Ji-Hong Park', 'Se-Yeon Lee', 'Sujin Park', 'Gun-Woo Kim']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'safety', 'LLM fine-tuning', 'chain-of-thought', 'Korean NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08480</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward</title><link>https://arxiv.org/abs/2512.08131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an RL framework (PPO) treating adversarial suffixes as a policy trained against a frozen LM reward oracle to produce universal triggers.&lt;/li&gt;&lt;li&gt;Introduces calibrated cross-entropy rewards to remove label bias and aggregate across surface forms, improving transferability of suffixes.&lt;/li&gt;&lt;li&gt;Evaluates on five NLP benchmarks (sentiment, NLI, paraphrase, commonsense) and three models (Qwen2-1.5B, TinyLlama-1.1B, Phi-1.5), showing consistent accuracy degradation and improved cross-task/model transfer vs prior triggers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sampriti Soor', 'Suklav Ghosh', 'Arijit Sur']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'red teaming', 'robustness', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08131</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation</title><link>https://arxiv.org/abs/2512.08123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces universal adversarial suffixes (4–10 tokens) that, when appended to any input, broadly reduce LM classifier accuracy and calibrated confidence.&lt;/li&gt;&lt;li&gt;Learns suffixes in a differentiable soft form using calibrated Gumbel-Softmax relaxation, with masking of gold tokens and entropy regularization, then discretizes for inference.&lt;/li&gt;&lt;li&gt;Shows transferability: a single suffix trained on one model reduces performance across tasks (sentiment, NLI, paraphrase, commonsense QA, physical reasoning) and across model families (Qwen2-1.5B, Phi-1.5, TinyLlama-1.1B).&lt;/li&gt;&lt;li&gt;Demonstrates a model-agnostic vulnerability relevant to prompt-based zero/few-shot classification and LLM robustness/red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sampriti Soor', 'Suklav Ghosh', 'Arijit Sur']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'prompt injection', 'LLM robustness', 'red teaming', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08123</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a complementarity gap: human-AI teams often do not outperform the best individual because agents are trained as answer engines rather than collaborative partners.&lt;/li&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda to develop agents that co-construct causal explanations, surface uncertainties, and adapt goals to support expert decision-making.&lt;/li&gt;&lt;li&gt;Recommends new training environments, shared representations for human-AI mental models, and evaluation metrics centered on trust and complementarity to measure collaborative capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Research agenda&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'safety evaluation', 'trust', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Natural Language Descriptions of Model Activations Convey Privileged Information?</title><link>https://arxiv.org/abs/2509.13316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether natural-language verbalizations of LLM activations convey privileged information about the target model or merely reflect input data or the verbalizer's own knowledge.&lt;/li&gt;&lt;li&gt;Shows that popular verbalization methods can succeed on existing benchmarks without accessing target model internals, suggesting dataset/benchmark shortcomings.&lt;/li&gt;&lt;li&gt;Controlled experiments indicate verbalizations often echo the parametric knowledge of the verbalizer LLM rather than revealing the target model's internal representations.&lt;/li&gt;&lt;li&gt;Calls for better-targeted benchmarks and experimental controls to rigorously assess whether activation verbalization provides meaningful interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Millicent Li', 'Alberto Mario Ceballos Arroyo', 'Giordano Rogers', 'Naomi Saphra', 'Byron C. Wallace']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'LLMs', 'evaluation', 'alignment', 'model introspection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13316</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title><link>https://arxiv.org/abs/2508.19112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep: a random forest classifier using deep features from a pretrained Swin Transformer encoder (SimMIM) to detect out-of-distribution (OOD) CT scans and improve segmentation reliability.&lt;/li&gt;&lt;li&gt;Segmentation model pretrained on 10,432 unlabeled 3D CTs and trained on 317 labeled lung cancer scans; evaluated on 603 independent 3D CTs including one in-distribution and four OOD datasets (PE, COVID-19, abdominal/kidney, healthy).&lt;/li&gt;&lt;li&gt;RF-Deep achieved notably lower FPR95 on several OOD domains (e.g., 18.26% PE, 27.66% COVID-19, &lt;0.1% abdominal CTs) and outperformed established OOD methods.&lt;/li&gt;&lt;li&gt;Focuses on robustness and safety of medical image segmentation by detecting distribution shifts to enhance clinical reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'medical imaging', 'safety evaluation', 'transformer pretraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19112</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</title><link>https://arxiv.org/abs/2508.13309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAASH, a fully differentiable multi-stage meta-attack framework that composes multiple Lp-constrained base attacks via learned adaptive weights.&lt;/li&gt;&lt;li&gt;Uses a meta-loss that jointly minimizes misclassification loss and perceptual distortion, enabling dynamic weighting of base attacks across stages.&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success and better perceptual quality (SSIM, LPIPS, FID) on adversarially trained CIFAR-10, CIFAR-100, and ImageNet models—outperforming prior perceptual attacks like AdvAD.&lt;/li&gt;&lt;li&gt;Shows generalization to unseen defenses, making the method a practical strong baseline for robustness evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdullah Al Nomaan Nafi', 'Habibur Rahaman', 'Zafaryab Haider', 'Tanzim Mahfuz', 'Fnu Suya', 'Swarup Bhunia', 'Prabuddha Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'adversarial-attacks', 'robustness', 'perceptual-attacks', 'meta-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13309</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title><link>https://arxiv.org/abs/2505.23856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Omniguard, a method that detects harmful prompts across languages and modalities by identifying internal LLM/MLLM representations aligned across languages/modalities and training a language-/modality-agnostic classifier.&lt;/li&gt;&lt;li&gt;Demonstrates substantial accuracy gains over strong baselines: +11.57% multilingual, +20.44% image-based, and new SOTA on audio-based harmful-prompt detection.&lt;/li&gt;&lt;li&gt;Achieves high efficiency by reusing embeddings computed during generation, reporting ~120x speedup versus the next-fastest baseline.&lt;/li&gt;&lt;li&gt;Code and data are released, enabling reproducibility and further evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'prompt moderation', 'multimodal safety', 'multilingual robustness', 'harmful-prompt detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23856</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title><link>https://arxiv.org/abs/2512.07222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies function words in text prompts as a source of vulnerability for vision-language models (VLMs) under cross-modal adversarial attacks.&lt;/li&gt;&lt;li&gt;Proposes Function-word De-Attention (FDA): compute original and function-word-specific cross-attention and subtract the latter to reduce function-word influence in attention heads.&lt;/li&gt;&lt;li&gt;Evaluates FDA across multiple SOTA baselines, attacks, tasks, datasets, and models, reporting large reductions in attack success rate (ASR) with negligible task performance drops; also demonstrates scalability, generalization, zero-shot performance, and ablations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM robustness', 'adversarial defense', 'cross-modal attacks', 'attention mechanisms', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07222</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</title><link>https://arxiv.org/abs/2510.05526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RLHF-COV and DPO-COV algorithms to simultaneously mitigate corrupted preference data, reward overoptimization, and verbosity bias in RLHF/DPO alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: length-regularized generalization error rates for DPO-COV trained on corrupted data that match best-known rates for cleaner settings.&lt;/li&gt;&lt;li&gt;Shows DPO-COV is simple to implement without reward-model estimation and is provably equivalent to RLHF-COV, implying equivalence between vanilla RLHF and DPO under their formulation.&lt;/li&gt;&lt;li&gt;Presents experiments demonstrating effectiveness in both offline and online settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Chen', 'Junyi Li', 'Peiran Yu', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Alignment', 'Preference corruption', 'Robustness', 'DPO']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05526</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Nonlinear Optimization with GPU-Accelerated Neural Network Constraints</title><link>https://arxiv.org/abs/2509.22462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reduced-space formulation for optimization over trained neural networks that evaluates network outputs and derivatives on a GPU while treating the network as a "gray box" (intermediate variables/constraints hidden from the solver).&lt;/li&gt;&lt;li&gt;Shows the reduced-space approach yields faster solves and fewer interior-point iterations compared to full-space formulations.&lt;/li&gt;&lt;li&gt;Demonstrates method on two problems: adversarial example generation for an MNIST classifier and security-constrained optimal power flow using a neural-network surrogate for transient feasibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Parker', 'Oscar Dowson', 'Nicole LoGiudice', 'Manuel Garcia', 'Russell Bent']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'neural network constraints', 'optimization', 'GPU acceleration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22462</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Preservation through Practical Machine Unlearning</title><link>https://arxiv.org/abs/2502.10635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates machine unlearning methods (Naive Retraining, Exact Unlearning via SISA) with respect to computational cost and consistency using the HSpam14 dataset.&lt;/li&gt;&lt;li&gt;Investigates integrating unlearning principles into Positive Unlabeled (PU) learning to handle partially labeled data.&lt;/li&gt;&lt;li&gt;Highlights the DaRE unlearning framework as promising for privacy compliance while noting significant computational trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Dilworth']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy-preserving ML', 'data deletion', 'SISA', 'PU learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10635</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GLL: A Differentiable Graph Learning Layer for Neural Networks</title><link>https://arxiv.org/abs/2412.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GLL, a differentiable graph learning layer that integrates similarity graph construction and graph Laplacian-based label propagation into neural networks, replacing the projection head + softmax.&lt;/li&gt;&lt;li&gt;Derives backpropagation equations via the adjoint method to enable end-to-end training of the graph learning layer.&lt;/li&gt;&lt;li&gt;Empirical results show smoother label transitions, improved generalization and training dynamics, and reported improved robustness to adversarial attacks compared to standard softmax-based approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Brown', 'Bohan Chen', 'Harris Hardiman-Mostow', 'Jeff Calder', 'Andrea L. Bertozzi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'graph learning', 'label propagation', 'differentiable layer', 'classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08016</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks</title><link>https://arxiv.org/abs/2512.08882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OrbitChain, a blockchain-backed framework to provide transparent provenance and integrity for federated satellite learning (FSL) across multi-vendor LEO constellations, aiming to prevent manipulated or incomplete model updates from corrupting global aggregation.&lt;/li&gt;&lt;li&gt;Design offloads consensus to high-altitude platforms (HAPs) using a permissioned proof-of-authority ledger to reduce computational/communication overhead and achieve sub-second block finalization for varying quorum sizes.&lt;/li&gt;&lt;li&gt;Evaluated via simulations and real satellite datasets, reporting improved global model accuracy, up to 30-hour reduction in convergence time versus single-vendor setups, and claims around enhanced privacy and security; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Elmahallawy', 'Asma Jodeiri Akbarfam']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'blockchain / distributed ledger', 'model update integrity / poisoning mitigation', 'space/satellite AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08882</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety</title><link>https://arxiv.org/abs/2512.08862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedMining, a federated learning framework for underground mine safety that focuses on privacy and secure model training across sensor-equipped mines.&lt;/li&gt;&lt;li&gt;Introduces a Decentralized Functional Encryption (DFE) scheme to keep local model updates encrypted and mitigate model inversion and membership inference attacks.&lt;/li&gt;&lt;li&gt;Presents a balancing aggregation mechanism to address non-IID data and sensor noise, improving convergence, accuracy, and reducing communication/computation overhead in evaluations on real-world mining datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Elmahallawy', 'Sanjay Madria', 'Samuel Frimpong']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving', 'functional encryption', 'membership inference', 'robustness/convergence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08862</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivTune, a device-cloud split learning framework that injects optimized noise into token representations to preserve privacy during fine-tuning.&lt;/li&gt;&lt;li&gt;Formulates an optimization to compute noise vectors that make tokens resemble n-hop indirect neighbors and adapts the mean/scale of a d_chi-Privacy noise distribution guided by token importance.&lt;/li&gt;&lt;li&gt;Evaluates against embedding inversion and attribute inference attacks across classification and generation tasks, showing large reductions in attack success with minimal utility loss compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'differential privacy', 'split learning', 'embedding inversion', 'inference attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks</title><link>https://arxiv.org/abs/2512.08341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates UAV relay placement under adversarial jamming as a cooperative multi-agent RL problem using CTDE (centralized critic, decentralized actors).&lt;/li&gt;&lt;li&gt;Demonstrates learned emergent anti-jamming behaviors that improve throughput (~50% over heuristics) and maintain near-zero collision rates in simulation.&lt;/li&gt;&lt;li&gt;Focuses on resilience and robustness of UAV communication networks in contested environments against jamming attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thai Duong Nguyen', 'Ngoc-Tan Nguyen', 'Thanh-Dao Nguyen', 'Nguyen Van Huynh', 'Dinh-Hieu Tran', 'Symeon Chatzinotas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial jamming', 'multi-agent reinforcement learning', 'UAV/network security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08341</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title><link>https://arxiv.org/abs/2512.08329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an explainable-AI analysis of image protection methods (e.g., Glaze, Nightshade) using a unified framework combining white-box feature-space inspection and black-box signal-level probing.&lt;/li&gt;&lt;li&gt;Finds that protection perturbations are structured, low-entropy, and tightly coupled to image content across representational, spatial, and spectral domains rather than causing global representational drift.&lt;/li&gt;&lt;li&gt;Shows detectability depends on perturbation entropy, spatial deployment, and frequency alignment; sequential application increases detectable structure; frequency analysis reveals energy redistribution along image-aligned axes.&lt;/li&gt;&lt;li&gt;Implication: modern image protection acts via structured feature-level deformation, informing design of detection strategies and defenses against generative-model misuse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael R. Martin', 'Garrick Chan', 'Kwan-Liu Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbations', 'image protection', 'diffusion models', 'interpretability', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08329</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title><link>https://arxiv.org/abs/2512.08216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep, a lightweight post-hoc OOD detection method using random forests on deep features anchored to predicted tumor segmentations from pretrained-finetuned encoders.&lt;/li&gt;&lt;li&gt;Extracts hierarchical features from multiple regions of interest around predicted tumors to provide task-relevant OOD signals and scale to varying fields-of-view.&lt;/li&gt;&lt;li&gt;Evaluated on 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) scenarios, achieving AUROC &gt;93.5 for near-OOD and &gt;99 for far-OOD, outperforming logit-based and radiomics baselines.&lt;/li&gt;&lt;li&gt;Designed to be plug-and-play, architecture-agnostic, and computationally lightweight compared to architectural OOD solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'medical imaging', 'segmentation robustness', 'post-hoc safety methods', 'random forest']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08216</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Worst-case generation via minimax optimization in Wasserstein space</title><link>https://arxiv.org/abs/2512.08176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework for worst-case generation via min-max optimization over continuous probability distributions in Wasserstein space.&lt;/li&gt;&lt;li&gt;Uses Brenier's theorem to represent the least-favorable distribution as the pushforward of a transport map from a reference measure, enabling continuous, expressive worst-case generation.&lt;/li&gt;&lt;li&gt;Introduces a single-loop GDA algorithm with global convergence guarantees under mild assumptions and a neural-network parameterization of the transport map for simulation-free training.&lt;/li&gt;&lt;li&gt;Validates the method on synthetic and image datasets to demonstrate utility for risk-induced worst-case generation and stress-testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Cheng', 'Yao Xie', 'Linglingzhi Zhu', 'Yunqin Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'worst-case generation', 'Wasserstein optimal transport', 'adversarial / stress-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08176</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</title><link>https://arxiv.org/abs/2512.08875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLM-based tabular synthetic data generators (both fine-tuned small models and in-context large models) can memorize and reproduce numeric digit strings from training data, enabling privacy breaches.&lt;/li&gt;&lt;li&gt;Proposes LevAtt, a no-box membership inference attack that uses only generated synthetic records to detect memorized numeric sequences and demonstrates high leakage across models/datasets (sometimes perfect classification).&lt;/li&gt;&lt;li&gt;Introduces two defenses including a novel sampling strategy that perturbs digits during generation to mitigate leakage with minimal utility/fidelity loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Ward', 'Bochao Gu', 'Chi-Hua Wang', 'Guang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-leakage', 'memorization', 'synthetic-data', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08875</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Synthetic Data Generation Using Context-Aware GANs</title><link>https://arxiv.org/abs/2512.08869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ContextGAN, a GAN-based synthetic data generator that integrates domain-specific rules via a constraint matrix to enforce implicit and explicit domain constraints.&lt;/li&gt;&lt;li&gt;Applies differential privacy to the generative process to protect sensitive information from the original data.&lt;/li&gt;&lt;li&gt;Evaluates the method across healthcare, security, and finance, showing improved realism and utility while enforcing domain constraints under privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anantaa Kotal', 'Anupam Joshi']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'synthetic data', 'GAN', 'privacy-preserving ML', 'domain-specific constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08869</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models</title><link>https://arxiv.org/abs/2512.08832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WAAPO, a framework to craft targeted adversarial perturbations for weather-forecasting models that enforce channel sparsity, spatial localization, and smoothness to remain physically realistic and stealthy.&lt;/li&gt;&lt;li&gt;Evaluates attacks on FourCastNet using ERA5 data, showing small perturbations to initial conditions can steer forecasts along predefined adversarial trajectories.&lt;/li&gt;&lt;li&gt;Demonstrates vulnerabilities in AI-driven operational forecasting and argues for the need for robustness measures and safeguards against adversarial exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huzaifa Arif', 'Pin-Yu Chen', 'Alex Gittens', 'James Diffenderfer', 'Bhavya Kailkhura']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'evasion-attacks', 'robustness', 'physical-systems', 'spatiotemporal-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08832</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search</title><link>https://arxiv.org/abs/2512.08724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bias-Guided Prompt Search (BGPS), combining an LLM that generates attribute-neutral prompts with attribute classifiers acting on the TTI model's internal representations to steer prompt generation toward prompts that maximize biased attributes in outputs.&lt;/li&gt;&lt;li&gt;Applies BGPS to Stable Diffusion 1.5 and a state-of-the-art debiased model, discovering subtle, previously undocumented prompts that worsen fairness metrics and demonstrating these prompts are interpretable and usable by typical users.&lt;/li&gt;&lt;li&gt;Shows BGPS outperforms a hard prompt optimization baseline on perplexity and expands the space of bias-inducing prompts, proposing BGPS as a new adversarial evaluation/tool for bias mitigation in text-to-image models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manos Plitsis', 'Giorgos Bouritsas', 'Vassilis Katsouros', 'Yannis Panagakis']&lt;/li&gt;&lt;li&gt;Tags: ['bias detection', 'red teaming', 'prompt engineering', 'adversarial evaluation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08724</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction</title><link>https://arxiv.org/abs/2512.08499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two distance-aware uncertainty quantification methods (PG-SNGP and PG-SNER) for physics-guided neural networks to improve calibration and OOD awareness.&lt;/li&gt;&lt;li&gt;Uses spectral normalization to preserve input-to-latent distances and a GP or evidential output to produce distance-sensitive uncertainty estimates.&lt;/li&gt;&lt;li&gt;Introduces a distance-aware metric (based on Pearson correlation) and a dynamic loss weighting to balance data fidelity and physical consistency; evaluated on bearing degradation (PRONOSTIA) with comparisons to MC dropout and ensembles.&lt;/li&gt;&lt;li&gt;Reports improved accuracy, better OOD generalization, and robustness to noise and adversarial attacks in the specific rotating-bearing prediction domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waleed Razzaq', 'Yun-Bo Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'OOD detection / robustness', 'adversarial robustness', 'physics-guided neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08499</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2512.08485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses data poisoning attacks specifically against offline reinforcement learning and shows limitations of uniform/local perturbation strategies.&lt;/li&gt;&lt;li&gt;Proposes a Global Budget Allocation attack that assigns perturbation magnitudes proportional to sample TD-error sensitivity under a global L2 budget, with a closed-form solution.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical impact on D4RL benchmarks (up to ~80% performance degradation) while remaining stealthy against statistical and spectral defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junnan Qiu', 'Jie Li']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'offline_reinforcement_learning', 'adversarial_attacks', 'defense_evasion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08485</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset</title><link>https://arxiv.org/abs/2512.08459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pilot implementation of the Bacterial Biothreat Benchmark (B3), part of a broader Biothreat Benchmark Generation framework, to evaluate biosecurity risks posed by frontier AI models (LLMs).&lt;/li&gt;&lt;li&gt;Execution involved running the B3 benchmarks on a sample frontier AI model, performing human evaluation of model outputs, and conducting an applied risk analysis across multiple dimensions.&lt;/li&gt;&lt;li&gt;Concludes the B3 dataset is a viable, nuanced tool for rapidly assessing biosecurity risk, pinpointing key risk sources, and guiding mitigation priorities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Theodore Wilson', 'Zachary Kallenborn', 'Olivia Shoemaker', 'Anna Wetzel', 'Hayley Peterson', 'Abigail Danfora', 'Jenna LaTourette', 'Brandon Behlendorf', 'Douglas Clifford']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'biosecurity/biothreat', 'benchmarking', 'LLM safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08459</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process</title><link>https://arxiv.org/abs/2512.08451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes the Biothreat Benchmark Generation (BBG) framework's second component: creating the Bacterial Biothreat Benchmark (B3) dataset to evaluate biosecurity risks from frontier AI models.&lt;/li&gt;&lt;li&gt;Generates &gt;7,000 candidate benchmarks via web-based prompt generation, red teaming, and mining existing corpora, then filters to 1,010 final benchmarks using de-duplication, uplift diagnosticity assessment, and quality control.&lt;/li&gt;&lt;li&gt;Design ensures benchmarks are diagnostic (produce model uplift), directly relevant to biological threats, and mapped to a Task-Query Architecture for nuanced safety and risk analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Zachary Kallenborn', 'Anna Wetzel', 'Hayley Peterson', 'Jenna LaTourette', 'Olivia Shoemaker', 'Brandon Behlendorf', 'Sheriff Almakki', 'Doug Clifford', 'Noah Sheinbaum']&lt;/li&gt;&lt;li&gt;Tags: ['biothreat benchmark', 'LLM safety', 'red teaming', 'biosecurity', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08451</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fully Decentralized Certified Unlearning</title><link>https://arxiv.org/abs/2512.08443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RR-DU, a randomized random-walk procedure for certified unlearning in fully decentralized networks that mixes projected ascent on forget data with randomized descent and Gaussian noise on retained data.&lt;/li&gt;&lt;li&gt;Provides theoretical results: convergence in convex case, stationarity in nonconvex case, (ε,δ)-network unlearning certificates via subsampled Gaussian RDP, and deletion-capacity bounds quantifying utility–privacy trade-offs in decentralized topologies.&lt;/li&gt;&lt;li&gt;Empirical validation on image benchmarks (MNIST, CIFAR-10) showing RR-DU meets target (ε,δ) privacy while attaining higher test accuracy than decentralized DP baselines and reducing forget-set accuracy to near random guessing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hithem Lamri', 'Michail Maniatakos']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'differential privacy (RDP)', 'decentralized learning', 'certified unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08443</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust Agents in Open-Ended Worlds</title><link>https://arxiv.org/abs/2512.08139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces methods for training robust agents in open-ended, procedurally generated environments (MiniHack) to improve generalisation.&lt;/li&gt;&lt;li&gt;Presents Maestro, an adversarial curriculum generation approach to progressively harden RL agents in two-player zero-sum games.&lt;/li&gt;&lt;li&gt;Uses quality-diversity search to find vulnerabilities in pre-trained multi-agent RL policies in a complex simulated football domain.&lt;/li&gt;&lt;li&gt;Extends robustness evaluation to LLMs by using evolutionary search to generate adversarial prompts that elicit undesirable outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Thesis)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikayel Samvelyan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompts', 'robustness', 'multi-agent RL', 'adversarial curriculum', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08139</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture</title><link>https://arxiv.org/abs/2512.08130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Biothreat Benchmark Generation (BBG) Framework to assess biosecurity risks posed by frontier AI models, with an initial focus on bacterial threats.&lt;/li&gt;&lt;li&gt;Presents a hierarchical Bacterial Biothreat Schema that maps biothreat categories, elements, and tasks to enable development of task-aligned queries capturing technical and operational aspects of adversaries.&lt;/li&gt;&lt;li&gt;Aims to support developers and policymakers in measuring risk uplift across different actor capability levels; future work will convert queries into prompts and implement evaluation benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Brandon Behlendorf', 'Zachary Kallenborn', 'Sheriff Almakki', 'Doug Clifford', 'Jenna LaTourette', 'Hayley Peterson', 'Noah Sheinbaum', 'Olivia Shoemaker', 'Anna Wetzel']&lt;/li&gt;&lt;li&gt;Tags: ['biothreat-benchmarking', 'AI-biosecurity', 'LLM-safety', 'red-teaming', 'risk-assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08130</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization</title><link>https://arxiv.org/abs/2512.08129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies failure modes of post-training backdoor detectors when intrinsic class features create outlier statistics or when backdoors are subtle relative to class features.&lt;/li&gt;&lt;li&gt;Proposes Class Subspace Orthogonalization (CSO): suppress intrinsic class features by orthogonalizing the detection optimization with respect to a class subspace estimated from a small set of clean examples.&lt;/li&gt;&lt;li&gt;Formulates CSO as a constrained optimization that preserves backdoor-related contributions while reducing non-target class statistics, improving sensitivity.&lt;/li&gt;&lt;li&gt;Evaluates CSO against challenging mixed-label and adaptive backdoor attacks, showing improved detection performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangmingmei Yang', 'David J. Miller', 'George Kesidis']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'data poisoning', 'adversarial ML', 'post-training defenses', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08129</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title><link>https://arxiv.org/abs/2512.08121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues Youden's J statistic (and its linear transform, Balanced Accuracy) is the appropriate metric for selecting judges that estimate prevalence of desirable/undesirable LLM behaviors, addressing sensitivity to class imbalance and choice of positive class.&lt;/li&gt;&lt;li&gt;Proves theoretical alignment between Youden's J and the goal of comparing models by prevalence, and shows Balanced Accuracy avoids distortions introduced by Accuracy, Precision, and F1.&lt;/li&gt;&lt;li&gt;Provides analytical arguments, empirical examples, and simulations demonstrating that selecting judges via Balanced Accuracy yields more robust and unbiased prevalence estimates for safety/policy evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Collot', 'Colin Fraser', 'Justin Zhao', 'William F. Shen', 'Timon Willi', 'Ilias Leontiadis']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'LLM-as-judge', 'balanced-accuracy', 'robustness', 'evaluation-methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08121</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes training LLMs to produce a post-answer 'confession' that honestly reports shortcomings or misbehavior; confession reward is based only on honesty and is decoupled from the main answer's reward.&lt;/li&gt;&lt;li&gt;Relies on the assumption that maximizing confession reward is easiest by surfacing misbehavior rather than covering it up; argues this holds empirically, especially for egregious failures.&lt;/li&gt;&lt;li&gt;Trains a model (GPT-5-Thinking) to produce confessions and evaluates confession honesty on out-of-distribution tests including hallucination, instruction-following failures, scheming, and reward hacking.&lt;/li&gt;&lt;li&gt;Finds confessions often reveal omissions or lies from the main answer and that confession honesty improves modestly with training; suggests confession outputs enable monitoring, rejection sampling, and other inference-time interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manas Joglekar', 'Jeremy Chen', 'Gabriel Wu', 'Jason Yosinski', 'Jasmine Wang', 'Boaz Barak', 'Amelia Glaese']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model honesty', 'RLHF/reward shaping', 'safety evaluation', 'adversarial behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08093</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Controllable risk scenario generation from human crash data for autonomous vehicle testing</title><link>https://arxiv.org/abs/2512.07874</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CRAG, a framework that disentangles nominal and risk-related driving behaviors in a structured latent space to synthesize realistic agent behaviors.&lt;/li&gt;&lt;li&gt;Uses limited human crash data plus optimization-based mode-transition mechanisms to produce smooth, plausible shifts from safe to risk states over extended horizons.&lt;/li&gt;&lt;li&gt;Enables controllable generation of diverse, safety-critical scenarios for targeted evaluation of autonomous vehicle robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiujing Lu', 'Xuanhan Wang', 'Runze Yuan', 'Wei Lu', 'Xinyi Gong', 'Shuo Feng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'scenario generation', 'autonomous vehicles', 'robustness', 'adversarial/red-team testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07874</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation</title><link>https://arxiv.org/abs/2512.07857</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SA^2GFM, a framework for robust Graph Foundation Models using Structure-Aware Semantic Augmentation that converts entropy-based encoding trees into structure-aware textual prompts for feature augmentation.&lt;/li&gt;&lt;li&gt;Applies a self-supervised Information Bottleneck to distill transferable, robust representations and introduces an expert adaptive routing (mixture-of-experts with a null expert) to reduce negative transfer in cross-domain adaptation.&lt;/li&gt;&lt;li&gt;Includes a hierarchical fine-tuning module for joint intra- and inter-community structure learning and reports improved effectiveness and robustness against random noise and adversarial structural perturbations on node and graph classification tasks versus 9 baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhua Shi', 'Qingyun Sun', 'Haonan Yuan', 'Xingcheng Fu']&lt;/li&gt;&lt;li&gt;Tags: ['graph-foundation-models', 'adversarial-robustness', 'defense-methods', 'domain-adaptation', 'information-bottleneck']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07857</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents</title><link>https://arxiv.org/abs/2512.07850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes LLM agent trajectories to distinguish mutating (environment-changing) vs non-mutating actions and formalizes 'decisive deviations' that flip success to failure.&lt;/li&gt;&lt;li&gt;Finds mutating-action deviations strongly predict failure (each deviation can reduce odds of success by up to ~92–96% on some benchmarks), while non-mutating deviations have little effect; errors also increase with context length.&lt;/li&gt;&lt;li&gt;Proposes SABER, a model-agnostic, gradient-free test-time safeguard that adds mutation-gated verification, targeted reflection before mutating steps, and block-based context cleaning, producing substantial empirical gains across models and benchmarks.&lt;/li&gt;&lt;li&gt;Identifies annotation/underspecification issues in τ-Bench and releases τ-Bench Verified to restore benchmark headroom.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro Cuadron', 'Pengfei Yu', 'Yang Liu', 'Arpit Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Robustness / Safety', 'Test-time safeguards', 'Benchmarking / Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07850</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach</title><link>https://arxiv.org/abs/2512.07814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Builds a dataset of diverse PII types in code and fine-tunes representative LLM4Code models to study leakage behavior.&lt;/li&gt;&lt;li&gt;Computes training dynamics on real PII instances and uses a structural causal model to estimate the causal effect of learnability on leakage.&lt;/li&gt;&lt;li&gt;Finds substantial heterogeneity in leakage risk across PII types (e.g., IPs leak more than keys/passwords) and argues for type-aware, learnability-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Yang', 'Alejandro Velasco', 'Sen Fang', 'Bowen Xu', 'Denys Poshyvanyk']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-leakage', 'training-dynamics', 'causal-inference', 'code-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07814</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a complementarity gap: human-AI teams often do not outperform the best individual because agents are trained as answer engines rather than collaborative partners.&lt;/li&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda to develop agents that co-construct causal explanations, surface uncertainties, and adapt goals to support expert decision-making.&lt;/li&gt;&lt;li&gt;Recommends new training environments, shared representations for human-AI mental models, and evaluation metrics centered on trust and complementarity to measure collaborative capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Research agenda&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'safety evaluation', 'trust', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</title><link>https://arxiv.org/abs/2510.05526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RLHF-COV and DPO-COV algorithms to simultaneously mitigate corrupted preference data, reward overoptimization, and verbosity bias in RLHF/DPO alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: length-regularized generalization error rates for DPO-COV trained on corrupted data that match best-known rates for cleaner settings.&lt;/li&gt;&lt;li&gt;Shows DPO-COV is simple to implement without reward-model estimation and is provably equivalent to RLHF-COV, implying equivalence between vanilla RLHF and DPO under their formulation.&lt;/li&gt;&lt;li&gt;Presents experiments demonstrating effectiveness in both offline and online settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Chen', 'Junyi Li', 'Peiran Yu', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Alignment', 'Preference corruption', 'Robustness', 'DPO']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05526</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</title><link>https://arxiv.org/abs/2509.24319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two modes of value expression in LLMs: intrinsic (learned during training) vs. prompted (elicited by prompts), and studies their mechanistic bases.&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability tools—'value vectors' from the residual stream and 'value neurons' in MLPs—to identify components driving each mode.&lt;/li&gt;&lt;li&gt;Finds partial overlap between mechanisms but also unique components: prompted mechanisms yield greater steerability/instruction following (including effects on jailbreaking), while intrinsic components produce greater lexical/response diversity.&lt;/li&gt;&lt;li&gt;Implication: different interventions or attacks (e.g., prompt-based red teaming vs. training-based alignment) may target distinct mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jongwook Han', 'Jongwon Lim', 'Injin Kong', 'Yohan Jo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'mechanistic interpretability', 'jailbreaking', 'prompting/steerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24319</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</title><link>https://arxiv.org/abs/2508.18321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KAIROS, a benchmark simulating quiz-style multi-agent collaboration with controllable peer rapport and behaviours to study how peer interactions affect LLM decisions.&lt;/li&gt;&lt;li&gt;Evaluates prompting, supervised fine-tuning, and Group Relative Policy Optimisation (GRPO) to mitigate susceptibility to misleading peer inputs.&lt;/li&gt;&lt;li&gt;Finds model scale strongly moderates susceptibility: larger models are more resilient and benefit from prompting-based mitigations; smaller models remain vulnerable and only improve with carefully configured GRPO.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maojia Song', 'Tej Deep Pala', 'Ruiwen Zhou', 'Weisheng Jin', 'Amir Zadeh', 'Chuan Li', 'Dorien Herremans', 'Soujanya Poria']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multi-agent-systems', 'social-engineering', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18321</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title><link>https://arxiv.org/abs/2508.06457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ScamAgent, an autonomous multi-turn agent built on LLMs that generates realistic, adaptive scam call scripts using dialogue memory and persuasive strategies.&lt;/li&gt;&lt;li&gt;Shows that existing single-shot safety guardrails (refusal mechanisms, filters) can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework.&lt;/li&gt;&lt;li&gt;Demonstrates end-to-end pipeline by converting generated scam scripts into lifelike voice calls via text-to-speech systems.&lt;/li&gt;&lt;li&gt;Argues for need of multi-turn safety auditing, agent-level control frameworks, and detection/disruption methods for conversational deception.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanket Badhe']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'social engineering / scam', 'jailbreaking / bypass', 'adversarial prompting', 'audio/phishing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06457</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title><link>https://arxiv.org/abs/2505.23856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Omniguard, a method that detects harmful prompts across languages and modalities by identifying internal LLM/MLLM representations aligned across languages/modalities and training a language-/modality-agnostic classifier.&lt;/li&gt;&lt;li&gt;Demonstrates substantial accuracy gains over strong baselines: +11.57% multilingual, +20.44% image-based, and new SOTA on audio-based harmful-prompt detection.&lt;/li&gt;&lt;li&gt;Achieves high efficiency by reusing embeddings computed during generation, reporting ~120x speedup versus the next-fastest baseline.&lt;/li&gt;&lt;li&gt;Code and data are released, enabling reproducibility and further evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'prompt moderation', 'multimodal safety', 'multilingual robustness', 'harmful-prompt detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23856</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title><link>https://arxiv.org/abs/2505.12332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VoiceCloak, a proactive multi-dimensional defense that adds adversarial perturbations to reference audio to obfuscate speaker identity and degrade cloned speech quality for diffusion-based voice cloning.&lt;/li&gt;&lt;li&gt;Targets diffusion model vulnerabilities by distorting speaker representation embeddings, disrupting conditional guidance (attention context), amplifying score magnitudes to steer reverse diffusion away from high-quality output, and applying noise-guided semantic corruption.&lt;/li&gt;&lt;li&gt;Guides perturbation design with auditory perception principles and evaluates defense success extensively against unauthorized diffusion-based voice cloning systems; audio examples provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianyue Hu', 'Junyan Wu', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['voice cloning defense', 'adversarial audio', 'diffusion models', 'proactive defense', 'speaker obfuscation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12332</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</title><link>https://arxiv.org/abs/2512.07497</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 900 execution traces from three LLMs (Granite 4 Small 32B, Llama 4 Maverick 400B, DeepSeek V3.1) on the KAMI v0.1 agentic benchmark covering filesystem, text extraction, CSV analysis, and SQL scenarios.&lt;/li&gt;&lt;li&gt;Finds that model scale alone does not predict agentic robustness; reliability gains for DeepSeek V3.1 are attributed mainly to post-training reinforcement learning rather than size or architecture.&lt;/li&gt;&lt;li&gt;Identifies four recurring failure archetypes: premature action without grounding, over-helpfulness substituting missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load.&lt;/li&gt;&lt;li&gt;Recommends agentic evaluation and design priorities emphasizing interactive grounding, recovery behavior, environment-aware adaptation, verification/constraint discovery, and adherence to source-of-truth data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['JV Roig']&lt;/li&gt;&lt;li&gt;Tags: ['agentic-evaluation', 'robustness', 'alignment', 'tool-use', 'failure-modes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07497</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VIGIL: A Reflective Runtime for Self-Healing Agents</title><link>https://arxiv.org/abs/2512.07094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VIGIL, a reflective runtime that supervises sibling agent behavior to perform autonomous maintenance (diagnosis and repair) rather than task execution.&lt;/li&gt;&lt;li&gt;Processes behavioral logs into a persistent EmoBank, appraises events into structured emotional representations, and derives RBT diagnoses (strengths/opportunities/failures) to guide repairs.&lt;/li&gt;&lt;li&gt;Generates guarded prompt updates and read-only code proposals via a strategy engine, and enforces a state-gated pipeline that surfaces explicit errors on illegal transitions; demonstrated via a reminder-latency case study showing meta-level self-repair and fallback diagnostics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'agent robustness', 'runtime introspection', 'self-healing agents', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07094</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</title><link>https://arxiv.org/abs/2512.06749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DoVer, an intervention-driven debugging framework for LLM-based multi-agent systems that pairs hypothesis generation from logs with active interventions (e.g., editing messages, altering plans) to validate and repair failures.&lt;/li&gt;&lt;li&gt;Shifts evaluation from single-step agent attribution to outcome-oriented metrics—measuring whether interventions resolve failures or make quantifiable progress toward task success.&lt;/li&gt;&lt;li&gt;Demonstrates empirical gains across multiple agent frameworks and datasets (Magnetic-One with GAIA/AssistantBench, and AG2 with GSMPlus), flipping a notable fraction of failed trials into successes and validating/refuting many failure hypotheses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Ma', 'Jue Zhang', 'Fangkai Yang', 'Yu Kang', 'Qingwei Lin', 'Tianming Yang', 'Saravan Rajmohan', 'Dongmei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent', 'debugging', 'robustness', 'safety evaluation', 'intervention testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06749</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2512.05943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE, a framework that analyzes stepwise reasoning trajectories in vision-language models instead of only final answers.&lt;/li&gt;&lt;li&gt;Uses Auxiliary Reasoning Sets (ARS) — compact sub-question/answer pairs — and consistency-based metrics to evaluate intermediate reasoning steps and detect silent failures.&lt;/li&gt;&lt;li&gt;Shows that ARS consistency correlates with final-answer correctness and can localize reasoning failures for debugging and model improvement.&lt;/li&gt;&lt;li&gt;Defines confidence regions over reasoning paths to distinguish reliable from unreliable reasoning, enabling filtering and safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shima Imani', 'Seungwhan Moon', 'Lambert Mathias', 'Lu Zhang', 'Babak Damavandi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'reasoning evaluation', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05943</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The AI Consumer Index (ACE)</title><link>https://arxiv.org/abs/2512.04921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE, a benchmarking dataset (400 hidden test cases + 80 open dev cases) for evaluating frontier models on everyday consumer tasks across shopping, food, gaming, and DIY.&lt;/li&gt;&lt;li&gt;Evaluates 10 frontier models (with web search enabled) using a grading methodology that checks whether response parts are grounded in retrieved web sources.&lt;/li&gt;&lt;li&gt;Finds models prone to hallucinating key information (e.g., prices) and reports substantial gaps between model performance and consumer needs; top models score ~55% overall, under 50% in Shopping.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julien Benchek', 'Rohit Shetty', 'Benjamin Hunsberger', 'Ajay Arun', 'Zach Richards', 'Brendan Foody', 'Osvald Nitski', 'Bertie Vidgen']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'safety-evaluation', 'hallucination', 'grounding', 'consumer-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04921</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</title><link>https://arxiv.org/abs/2512.02530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Aetheria, a multimodal content safety framework using five collaborative agents that debate and adjudicate content decisions.&lt;/li&gt;&lt;li&gt;Grounds the debate process with retrieval-augmented generation (RAG) to produce traceable, knowledge-backed audit reports.&lt;/li&gt;&lt;li&gt;Introduces AIR-Bench and reports improved overall content-safety accuracy, particularly in identifying implicit risks, compared to baselines.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and transparency in moderation pipelines via multi-agent debate and collaborative reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiang He', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Wei Cai', 'Haojie Cheng', 'Ziyan Shi', 'Ming Zhu', 'Haichuan Tang', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'interpretability', 'multimodal', 'retrieval-augmented-generation', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02530</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?</title><link>https://arxiv.org/abs/2512.00218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines how different training incentives affect the monitorability of chain-of-thought (CoT) outputs for detecting undesirable/harmful reasoning.&lt;/li&gt;&lt;li&gt;Introduces a methodology to measure monitorability by testing whether a monitor can predict a key latent variable from the model's reasoning while controlling for task accuracy.&lt;/li&gt;&lt;li&gt;Finds that length penalties and KL regularization have no consistent effect; adversarial optimization that penalizes monitor accuracy degrades monitor performance; direct optimization for monitorability does not reliably improve it.&lt;/li&gt;&lt;li&gt;Provides reproducible code for experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matt MacDermott', 'Qiyao Wei', 'Rada Djoneva', 'Francis Rhys Ward']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'monitoring', 'chain-of-thought', 'adversarial training', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00218</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</title><link>https://arxiv.org/abs/2511.11599</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SynBullying, a synthetic multi-LLM conversational dataset designed for cyberbullying detection that models multi-turn exchanges and role dynamics.&lt;/li&gt;&lt;li&gt;Provides context-aware, fine-grained annotations (harm intensity, CB categories) and analyzes lexical, sentiment/toxicity, and conversational properties across generations.&lt;/li&gt;&lt;li&gt;Evaluates utility of synthetic data both as standalone training data and as augmentation for cyberbullying classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arefeh Kazemi', 'Hamza Qadeer', 'Joachim Wagner', 'Hossein Hosseini', 'Sri Balaaji Natarajan Kalaivendan', 'Brian Davis']&lt;/li&gt;&lt;li&gt;Tags: ['cyberbullying detection', 'synthetic dataset', 'LLM-generated data', 'content moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11599</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title><link>https://arxiv.org/abs/2511.10240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ProgRAG, a multi-hop KGQA framework that decomposes complex questions into sub-questions and progressively extends partial reasoning paths.&lt;/li&gt;&lt;li&gt;At each step, external retrievers gather candidate KG evidence which is refined via LLM-driven uncertainty-aware pruning.&lt;/li&gt;&lt;li&gt;Optimizes LLM reasoning context by organizing and rearranging partial reasoning paths to reduce hallucinations and improve multi-hop reasoning reliability; demonstrates improved performance on three KGQA datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minbae Park', 'Hyemin Yang', 'Jeonghyun Kim', 'Kunsoo Park', 'Hyunjoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-mitigation', 'robustness', 'knowledge-graph-qa', 'retrieval-augmented-generation', 'llm-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10240</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title><link>https://arxiv.org/abs/2510.24411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MobileRisk-Live, a dynamic sandbox and fine-grained safety detection benchmark for mobile GUI agent workflows.&lt;/li&gt;&lt;li&gt;Proposes OS-Sentinel, a hybrid safety detection framework combining a Formal Verifier for explicit system-level violations with a VLM-based Contextual Judge for contextual risk assessment.&lt;/li&gt;&lt;li&gt;Reports 10%–30% improvements over existing approaches and releases code and dataset to support research on safer mobile agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiushi Sun', 'Mukai Li', 'Zhoumianze Liu', 'Zhihui Xie', 'Fangzhi Xu', 'Zhangyue Yin', 'Kanzhi Cheng', 'Zehao Li', 'Zichen Ding', 'Qi Liu', 'Zhiyong Wu', 'Zhuosheng Zhang', 'Ben Kao', 'Lingpeng Kong']&lt;/li&gt;&lt;li&gt;Tags: ['Mobile agents', 'Safety detection', 'Vision-Language Models', 'Formal verification', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24411</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SOCK: A Benchmark for Measuring Self-Replication in Large Language Models</title><link>https://arxiv.org/abs/2509.25643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SOCK, a CLI benchmark that measures LLMs' ability to self-replicate and persist across computational contexts, producing an R-score and classifying models into Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL).&lt;/li&gt;&lt;li&gt;Uses a five-task suite involving manipulable CLI utilities and process interactions with an LLM acting agentically in a controlled environment to evaluate replication and persistence.&lt;/li&gt;&lt;li&gt;Evaluates both open-weight and proprietary models, finding significant obstacles to persistent self-replication (e.g., context retention, multi-agent decision-making), and discusses mitigations and future research directions.&lt;/li&gt;&lt;li&gt;Aims to standardize evaluation of self-replication capabilities to monitor and mitigate potential self-replication threat vectors in multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Chavarria', 'Rohan Raizada', 'Justin White', 'Eyad Alhetairshi']&lt;/li&gt;&lt;li&gt;Tags: ['self-replication', 'benchmarking', 'AI-safety', 'dangerous-capabilities', 'multi-agent-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25643</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models</title><link>https://arxiv.org/abs/2506.17114</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RFMDataset, a benchmark of 200 mathematical proof problems designed to reveal hidden failure modes of advanced reasoning models.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art models and finds poor performance on rigorous proof tasks (some models correct &lt;20%), identifying 10 fine-grained error types (e.g., hallucination, incompleteness, single-step logical failures).&lt;/li&gt;&lt;li&gt;Concludes model self-reflection is insufficient to resolve logical errors and recommends formalized, fine-grained logical training to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dadi Guo', 'Jiayu Liu', 'Zhiyuan Fan', 'Zhitao He', 'Haoran Li', 'Yuxin Li', 'Yumeng Wang', 'Yi R. Fung']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'evaluation/benchmark', 'alignment', 'hallucination', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17114</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title><link>https://arxiv.org/abs/2512.08892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGLens, a lightweight hallucination detector for Retrieval-Augmented Generation that uses sparse autoencoders to disentangle LLM internal activations and identify features correlated with unfaithful outputs.&lt;/li&gt;&lt;li&gt;Proposes an information-based feature selection pipeline and additive feature modeling to flag hallucinations using internal model representations, enabling interpretable rationales for detections.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection performance over existing methods and shows how identified signals can be used for post-hoc mitigation of unfaithful RAG outputs.&lt;/li&gt;&lt;li&gt;Provides analysis and insights into the distribution of hallucination-related signals within LLMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangzhi Xiong', 'Zhenghao He', 'Bohan Liu', 'Sanchit Sinha', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'retrieval-augmented_generation', 'model_internals', 'interpretability', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08892</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</title><link>https://arxiv.org/abs/2512.08875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLM-based tabular synthetic data generators (both fine-tuned small models and in-context large models) can memorize and reproduce numeric digit strings from training data, enabling privacy breaches.&lt;/li&gt;&lt;li&gt;Proposes LevAtt, a no-box membership inference attack that uses only generated synthetic records to detect memorized numeric sequences and demonstrates high leakage across models/datasets (sometimes perfect classification).&lt;/li&gt;&lt;li&gt;Introduces two defenses including a novel sampling strategy that perturbs digits during generation to mitigate leakage with minimal utility/fidelity loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Ward', 'Bochao Gu', 'Chi-Hua Wang', 'Guang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-leakage', 'memorization', 'synthetic-data', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08875</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Synthetic Data Generation Using Context-Aware GANs</title><link>https://arxiv.org/abs/2512.08869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ContextGAN, a GAN-based synthetic data generator that integrates domain-specific rules via a constraint matrix to enforce implicit and explicit domain constraints.&lt;/li&gt;&lt;li&gt;Applies differential privacy to the generative process to protect sensitive information from the original data.&lt;/li&gt;&lt;li&gt;Evaluates the method across healthcare, security, and finance, showing improved realism and utility while enforcing domain constraints under privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anantaa Kotal', 'Anupam Joshi']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'synthetic data', 'GAN', 'privacy-preserving ML', 'domain-specific constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08869</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivTune, a device-cloud split learning framework that injects optimized noise into token representations to preserve privacy during fine-tuning.&lt;/li&gt;&lt;li&gt;Formulates an optimization to compute noise vectors that make tokens resemble n-hop indirect neighbors and adapts the mean/scale of a d_chi-Privacy noise distribution guided by token importance.&lt;/li&gt;&lt;li&gt;Evaluates against embedding inversion and attribute inference attacks across classification and generation tasks, showing large reductions in attack success with minimal utility loss compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'differential privacy', 'split learning', 'embedding inversion', 'inference attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title><link>https://arxiv.org/abs/2512.08786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies federated RLHF where groups locally evaluate rollouts and the server aggregates group-level rewards without raw data access.&lt;/li&gt;&lt;li&gt;Compares standard aggregation strategies (min, max, average) and introduces an adaptive weighting scheme based on groups' historical alignment performance.&lt;/li&gt;&lt;li&gt;Shows experiments on Q/A tasks using PPO-based RLHF demonstrating improved fairness with the adaptive method while maintaining competitive alignment scores.&lt;/li&gt;&lt;li&gt;Provides a systematic evaluation framework for trade-offs between alignment quality and fairness across diverse populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Srewa', 'Tianyu Zhao', 'Salma Elmalaki']&lt;/li&gt;&lt;li&gt;Tags: ['Federated RLHF', 'Alignment', 'Preference aggregation', 'Fairness', 'Pluralistic alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08786</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models</title><link>https://arxiv.org/abs/2512.08503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonBreak, an adversarial framework that crafts concept-aware perturbations to disrupt hierarchical chain-of-thought geographic inference in multimodal large reasoning models (MLRMs).&lt;/li&gt;&lt;li&gt;Argues that disrupting conceptual dependencies in reasoning chains is more effective than uniform noise, causing cascades of invalidated inference steps.&lt;/li&gt;&lt;li&gt;Contributes GeoPrivacy-6K, a dataset of 6,341 ultra-high-resolution images with hierarchical concept annotations for evaluating geographic-privacy protections.&lt;/li&gt;&lt;li&gt;Evaluates across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) and reports substantial improvements in tract-level and block-level protection metrics versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Che Wang', 'Yang Cao', 'Longtao Huang', 'Wei Yang Bryan Lim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'adversarial robustness', 'multimodal reasoning', 'dataset', 'hierarchical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08503</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction</title><link>https://arxiv.org/abs/2512.08499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two distance-aware uncertainty quantification methods (PG-SNGP and PG-SNER) for physics-guided neural networks to improve calibration and OOD awareness.&lt;/li&gt;&lt;li&gt;Uses spectral normalization to preserve input-to-latent distances and a GP or evidential output to produce distance-sensitive uncertainty estimates.&lt;/li&gt;&lt;li&gt;Introduces a distance-aware metric (based on Pearson correlation) and a dynamic loss weighting to balance data fidelity and physical consistency; evaluated on bearing degradation (PRONOSTIA) with comparisons to MC dropout and ensembles.&lt;/li&gt;&lt;li&gt;Reports improved accuracy, better OOD generalization, and robustness to noise and adversarial attacks in the specific rotating-bearing prediction domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waleed Razzaq', 'Yun-Bo Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'OOD detection / robustness', 'adversarial robustness', 'physics-guided neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08499</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-based Vulnerable Code Augmentation: Generate or Refactor?</title><link>https://arxiv.org/abs/2512.08493</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares two LLM-based augmentation strategies for vulnerable code: controlled generation of new vulnerable samples vs. semantics-preserving refactoring of existing vulnerable functions.&lt;/li&gt;&lt;li&gt;Uses Qwen2.5-Coder to produce augmented code and evaluates impact on CodeBERT vulnerability classifier trained/evaluated on the SVEN dataset.&lt;/li&gt;&lt;li&gt;Finds both approaches effectively enrich scarce/imbalanced vulnerability datasets and that a hybrid generation+refactoring strategy yields the best classifier improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dyna Soumhane Ouchebara', "St\\'ephane Dupont"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM data augmentation', 'software vulnerability detection', 'code security', 'synthetic vulnerability generation', 'dataset imbalance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08493</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset</title><link>https://arxiv.org/abs/2512.08459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pilot implementation of the Bacterial Biothreat Benchmark (B3), part of a broader Biothreat Benchmark Generation framework, to evaluate biosecurity risks posed by frontier AI models (LLMs).&lt;/li&gt;&lt;li&gt;Execution involved running the B3 benchmarks on a sample frontier AI model, performing human evaluation of model outputs, and conducting an applied risk analysis across multiple dimensions.&lt;/li&gt;&lt;li&gt;Concludes the B3 dataset is a viable, nuanced tool for rapidly assessing biosecurity risk, pinpointing key risk sources, and guiding mitigation priorities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Theodore Wilson', 'Zachary Kallenborn', 'Olivia Shoemaker', 'Anna Wetzel', 'Hayley Peterson', 'Abigail Danfora', 'Jenna LaTourette', 'Brandon Behlendorf', 'Douglas Clifford']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'biosecurity/biothreat', 'benchmarking', 'LLM safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08459</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process</title><link>https://arxiv.org/abs/2512.08451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes the Biothreat Benchmark Generation (BBG) framework's second component: creating the Bacterial Biothreat Benchmark (B3) dataset to evaluate biosecurity risks from frontier AI models.&lt;/li&gt;&lt;li&gt;Generates &gt;7,000 candidate benchmarks via web-based prompt generation, red teaming, and mining existing corpora, then filters to 1,010 final benchmarks using de-duplication, uplift diagnosticity assessment, and quality control.&lt;/li&gt;&lt;li&gt;Design ensures benchmarks are diagnostic (produce model uplift), directly relevant to biological threats, and mapped to a Task-Query Architecture for nuanced safety and risk analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Zachary Kallenborn', 'Anna Wetzel', 'Hayley Peterson', 'Jenna LaTourette', 'Olivia Shoemaker', 'Brandon Behlendorf', 'Sheriff Almakki', 'Doug Clifford', 'Noah Sheinbaum']&lt;/li&gt;&lt;li&gt;Tags: ['biothreat benchmark', 'LLM safety', 'red teaming', 'biosecurity', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08451</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title><link>https://arxiv.org/abs/2512.08329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an explainable-AI analysis of image protection methods (e.g., Glaze, Nightshade) using a unified framework combining white-box feature-space inspection and black-box signal-level probing.&lt;/li&gt;&lt;li&gt;Finds that protection perturbations are structured, low-entropy, and tightly coupled to image content across representational, spatial, and spectral domains rather than causing global representational drift.&lt;/li&gt;&lt;li&gt;Shows detectability depends on perturbation entropy, spatial deployment, and frequency alignment; sequential application increases detectable structure; frequency analysis reveals energy redistribution along image-aligned axes.&lt;/li&gt;&lt;li&gt;Implication: modern image protection acts via structured feature-level deformation, informing design of detection strategies and defenses against generative-model misuse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael R. Martin', 'Garrick Chan', 'Kwan-Liu Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbations', 'image protection', 'diffusion models', 'interpretability', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08329</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships</title><link>https://arxiv.org/abs/2512.08326</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argus is a multi-agent collaborative framework leveraging LLMs to detect sensitive information leakage in code repositories via a three-tier mechanism (key content, file context, project reference relationships).&lt;/li&gt;&lt;li&gt;Authors introduce two benchmarks—one for true leak detection and one for false-positive filtering—and report high performance (accuracy up to 94.86%, precision 96.36%, recall 94.64%, F1 0.955).&lt;/li&gt;&lt;li&gt;Framework targets reduction of false positives compared with regex/fingerprint/entropy approaches and demonstrates cost-effective evaluation on real repositories.&lt;/li&gt;&lt;li&gt;All code and datasets are publicly released to support further research and application.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'Hui Li', 'Liyang Zhang', 'Qijia Zhuang', 'Ao Yang', 'Dong Zhang', 'Xijun Luo', 'Bing Lin']&lt;/li&gt;&lt;li&gt;Tags: ['sensitive-information-detection', 'secrets-detection', 'LLM-multi-agent', 'code-repository-security', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08326</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem</title><link>https://arxiv.org/abs/2512.08290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a systematization/taxonomy of security and safety risks in the Model Context Protocol (MCP) ecosystem, distinguishing adversarial threats (e.g., indirect prompt injection, tool poisoning) from epistemic safety hazards (e.g., alignment failures).&lt;/li&gt;&lt;li&gt;Analyzes structural vulnerabilities of MCP primitives—Resources, Prompts, and Tools—and demonstrates how context can be weaponized to trigger unauthorized operations in multi-agent environments.&lt;/li&gt;&lt;li&gt;Surveys defenses including cryptographic provenance (ETDI), runtime intent verification, and outlines a roadmap for securing transitions to autonomous agentic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiva Gaire', 'Srijan Gyawali', 'Saroj Mishra', 'Suman Niroula', 'Dilip Thakur', 'Umesh Yadav']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Prompt injection', 'Tool poisoning', 'Agentic AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08290</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties</title><link>https://arxiv.org/abs/2512.08185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical, fully reproducible framework for evaluating medical LLM security under realistic resource constraints (consumer CPU, free models).&lt;/li&gt;&lt;li&gt;Focuses on jailbreak attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction across clinical specialties stratified by risk.&lt;/li&gt;&lt;li&gt;Uses synthetic patient records to avoid PHI/IRB issues and provides threat models, data generation, evaluation protocols, and scoring rubrics.&lt;/li&gt;&lt;li&gt;Aims to enable comparative security assessment and defense evaluation for medical-specialist models without requiring large infrastructure or paid APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinghao Wang', 'Ping Zhang', 'Carter Yagemann']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'privacy extraction', 'security evaluation framework', 'medical AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08185</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture</title><link>https://arxiv.org/abs/2512.08130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Biothreat Benchmark Generation (BBG) Framework to assess biosecurity risks posed by frontier AI models, with an initial focus on bacterial threats.&lt;/li&gt;&lt;li&gt;Presents a hierarchical Bacterial Biothreat Schema that maps biothreat categories, elements, and tasks to enable development of task-aligned queries capturing technical and operational aspects of adversaries.&lt;/li&gt;&lt;li&gt;Aims to support developers and policymakers in measuring risk uplift across different actor capability levels; future work will convert queries into prompts and implement evaluation benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gary Ackerman', 'Brandon Behlendorf', 'Zachary Kallenborn', 'Sheriff Almakki', 'Doug Clifford', 'Jenna LaTourette', 'Hayley Peterson', 'Noah Sheinbaum', 'Olivia Shoemaker', 'Anna Wetzel']&lt;/li&gt;&lt;li&gt;Tags: ['biothreat-benchmarking', 'AI-biosecurity', 'LLM-safety', 'red-teaming', 'risk-assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08130</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title><link>https://arxiv.org/abs/2512.08121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues Youden's J statistic (and its linear transform, Balanced Accuracy) is the appropriate metric for selecting judges that estimate prevalence of desirable/undesirable LLM behaviors, addressing sensitivity to class imbalance and choice of positive class.&lt;/li&gt;&lt;li&gt;Proves theoretical alignment between Youden's J and the goal of comparing models by prevalence, and shows Balanced Accuracy avoids distortions introduced by Accuracy, Precision, and F1.&lt;/li&gt;&lt;li&gt;Provides analytical arguments, empirical examples, and simulations demonstrating that selecting judges via Balanced Accuracy yields more robust and unbiased prevalence estimates for safety/policy evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Collot', 'Colin Fraser', 'Justin Zhao', 'William F. Shen', 'Timon Willi', 'Ilias Leontiadis']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'LLM-as-judge', 'balanced-accuracy', 'robustness', 'evaluation-methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08121</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes training LLMs to produce a self-reported "confession" after their main answer, where confession reward depends only on honesty to incentivize truthful self-disclosure of misbehavior.&lt;/li&gt;&lt;li&gt;Argues that if the easiest way to maximize confession reward is to surface misbehavior, models will be incentivized to admit shortcomings even if their main answer omits or misreports them.&lt;/li&gt;&lt;li&gt;Empirically trains a model (GPT-5-Thinking) and evaluates confession honesty in out-of-distribution scenarios including hallucination, instruction following, scheming, and reward hacking; finds confessions often reveal withheld misbehavior and improve modestly with training.&lt;/li&gt;&lt;li&gt;Suggests inference-time uses of confessions (monitoring, rejection sampling, surfacing issues) as safety/mitigation mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manas Joglekar', 'Jeremy Chen', 'Gabriel Wu', 'Jason Yosinski', 'Jasmine Wang', 'Boaz Barak', 'Amelia Glaese']&lt;/li&gt;&lt;li&gt;Tags: ['honesty', 'alignment', 'reward-shaping', 'reward-hacking', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08093</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents</title><link>https://arxiv.org/abs/2512.07850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes LLM agent trajectories to distinguish mutating (environment-changing) vs non-mutating actions and formalizes 'decisive deviations' that flip success to failure.&lt;/li&gt;&lt;li&gt;Finds mutating-action deviations strongly predict failure (each deviation can reduce odds of success by up to ~92–96% on some benchmarks), while non-mutating deviations have little effect; errors also increase with context length.&lt;/li&gt;&lt;li&gt;Proposes SABER, a model-agnostic, gradient-free test-time safeguard that adds mutation-gated verification, targeted reflection before mutating steps, and block-based context cleaning, producing substantial empirical gains across models and benchmarks.&lt;/li&gt;&lt;li&gt;Identifies annotation/underspecification issues in τ-Bench and releases τ-Bench Verified to restore benchmark headroom.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro Cuadron', 'Pengfei Yu', 'Yang Liu', 'Arpit Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Robustness / Safety', 'Test-time safeguards', 'Benchmarking / Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07850</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs</title><link>https://arxiv.org/abs/2512.08923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two benchmarks (REST and REST+) to evaluate cross-modal consistency in multimodal LLMs by providing semantically equivalent samples in image, text, and mixed forms.&lt;/li&gt;&lt;li&gt;Evaluates 15 state-of-the-art MLLMs and shows substantial modality inconsistency: models often give different answers across modalities even when semantic content is identical and OCR is correct.&lt;/li&gt;&lt;li&gt;Finds that rendering modality transformations do not fix inconsistency; visual factors (text color, resolution) and number of vision tokens affect performance, and a consistency score correlates with the modality gap between text and images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Angela van Sprang', 'Laurens Samson', 'Ana Lucic', 'Erman Acar', 'Sennay Ghebreab', 'Yuki M. Asano']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal consistency', 'benchmarking', 'evaluation', 'model behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08923</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance</title><link>https://arxiv.org/abs/2512.08740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Lu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08740</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The SMART+ Framework for AI Systems</title><link>https://arxiv.org/abs/2512.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the SMART+ Framework for governing AI systems, built on Safety, Monitoring, Accountability, Reliability, Transparency, with added pillars for Privacy &amp; Security, Data Governance, Fairness &amp; Bias, and Guardrails.&lt;/li&gt;&lt;li&gt;Aims to provide a practical, industry-agnostic approach to evaluate and manage AI risks, ensure compliance with evolving regulations, and enable auditability and trust—with emphasis on clinical research applications.&lt;/li&gt;&lt;li&gt;Primarily a high-level governance/operational framework rather than a technical treatment of adversarial attacks, red-teaming, or mechanistic robustness testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Laxmiraju Kandikatla', 'Branislav Radeljic']&lt;/li&gt;&lt;li&gt;Tags: ['AI-safety', 'AI-governance', 'privacy-security', 'regulatory-compliance', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08592</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</title><link>https://arxiv.org/abs/2512.08536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Principles2Plan, an interactive prototype where a human and an LLM collaborate to convert high-level ethical principles (e.g., beneficence, privacy) into context-sensitive, operationalisable rules for classical planning domains.&lt;/li&gt;&lt;li&gt;Allows a domain expert to supply domain/problem details and review/prioritise LLM-generated ethical rules, which are then fed to a planner to produce ethically-informed plans.&lt;/li&gt;&lt;li&gt;Focuses on making ethical automated planning practical by bridging abstract principles and concrete planning constraints; claims novelty for supporting principle-grounded rule generation in classical planning contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tammy Zhong', 'Yang Song', 'Maurice Pagnucco']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'ethical alignment', 'LLM-assisted planning', 'robot safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08536</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change</title><link>https://arxiv.org/abs/2512.08449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IDAIF, a framework mapping Theory of Change stages to AI architectural layers (Data, Pipeline, Inference, Agentic, Normative) plus an Assurance Layer for managing assumption failures.&lt;/li&gt;&lt;li&gt;Provides formal mathematical formulations for safety-oriented components: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration, causal DAGs for hallucination mitigation, and adversarial debiasing combined with RLHF for fairness.&lt;/li&gt;&lt;li&gt;Emphasizes impact-centric (vs model-centric) development and demonstrates applicability with case studies in healthcare, cybersecurity, and software engineering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong-Woon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'assurance', 'adversarial_debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08449</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching</title><link>https://arxiv.org/abs/2512.08026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proof-of-concept system using reasoning-enabled open-source LLMs to support patient–clinical trial matching, producing structured eligibility assessments rather than binary outputs.&lt;/li&gt;&lt;li&gt;Integrates heterogeneous EHR data, supports human-in-the-loop review, and provides interpretable reasoning chains to aid decision support.&lt;/li&gt;&lt;li&gt;Emphasizes rigorous security standards and comprehensive auditability of all AI-generated outputs to support traceability and oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caroline N. Leach', 'Mitchell A. Klusty', 'Samuel E. Armstrong', 'Justine C. Pickarski', 'Kristen L. Hankins', 'Emily B. Collier', 'Maya Shah', 'Aaron D. Mullen', 'V. K. Cody Bumgardner']&lt;/li&gt;&lt;li&gt;Tags: ['clinical AI', 'interpretability', 'auditability', 'safety/compliance', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08026</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
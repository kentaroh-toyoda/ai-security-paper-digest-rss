<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 16 Jan 2026 06:54:02 +0000</lastBuildDate><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies vulnerability of existing unlearning methods to relearning attacks where forgot knowledge re-emerges after fine-tuning on retain-set examples (even with zero forget-set examples).&lt;/li&gt;&lt;li&gt;Identifies weight-space properties (L2 distance and linear mode connectivity between original and unlearned models) that predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes a new class of weight-space regularization methods that substantially improve tamper-resistant unlearning and state-of-the-art resistance to relearning attacks in vision classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'relearning attacks', 'weight-space regularization', 'model robustness', 'tamper-resistant unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images</title><link>https://arxiv.org/abs/2502.05066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the novel vulnerability of diffusion-based text-to-image models producing NSFW/offensive text embedded in generated images and demonstrates that state-of-the-art models are susceptible.&lt;/li&gt;&lt;li&gt;Shows that existing visual-content mitigation techniques fail to prevent harmful text generation and often degrade benign text output.&lt;/li&gt;&lt;li&gt;Proposes a targeted fine-tuning defense that updates only text-generation layers using paired NSFW/benign-image examples to suppress harmful text while preserving image quality.&lt;/li&gt;&lt;li&gt;Releases ToxicBench: a dataset, harmful prompt set, evaluation metrics, and an evaluation pipeline for measuring NSFW text generation in images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Kumar', 'Tom Blanchard', 'Adam Dziedzic', 'Franziska Boenisch']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'content-moderation', 'text-in-image', 'defense', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05066</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm</title><link>https://arxiv.org/abs/2501.14230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GreedyPixel, a black-box adversarial attack that performs per-pixel greedy optimization guided by a surrogate-derived priority map and refined with query feedback.&lt;/li&gt;&lt;li&gt;Evaluates coordinates directly without gradients, claiming monotonic loss reduction and convergence to a coordinate-wise optimum for fine-grained, pixel-sparse perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art success rates and near white-box precision on CIFAR-10 and ImageNet across CNN and Transformer models, with visually imperceptible perturbations.&lt;/li&gt;&lt;li&gt;Provides implementation and code release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanrui Wang', 'Ching-Chun Chang', 'Chun-Shien Lu', 'Christopher Leckie', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['black-box attack', 'adversarial examples', 'pixel-wise attack', 'greedy optimization', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14230</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2601.10313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hierarchical Refinement Attack (HRA), a universal multimodal adversarial attack framework targeting vision-language pretraining (VLP) models that refines universal adversarial perturbations at both sample and optimization levels.&lt;/li&gt;&lt;li&gt;Image-side techniques: disentangles clean images and perturbations, introduces ScMix augmentation to diversify visual contexts, and leverages a temporal hierarchy of historical and estimated future gradients to stabilize and improve universal perturbation learning.&lt;/li&gt;&lt;li&gt;Text-side techniques: identifies globally influential words via combined intra-sentence and inter-sentence importance measures and uses these as universal text perturbations; extensive experiments show effectiveness across tasks, VLP models, and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng-Fei Zhang', 'Zi Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'universal adversarial perturbation', 'multimodal attacks', 'vision-language models', 'text-image attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10313</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification</title><link>https://arxiv.org/abs/2601.09806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end pipeline to generate and refine adversarial patches for facial biometric systems using FGSM for initial noise and a diffusion model (reverse diffusion) to improve imperceptibility (Gaussian smoothing, adaptive brightness correction).&lt;/li&gt;&lt;li&gt;Applies refined patches to facial images to evaluate evasion of identity verification and expression recognition, and uses a ViT–GPT2 captioning model to produce semantic identity descriptions for forensic interpretation.&lt;/li&gt;&lt;li&gt;Includes detection/forensic analysis using perceptual hashing and segmentation to find adversarial patches, reporting high similarity preservation (SSIM 0.95) while demonstrating vulnerabilities in face recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahrzad Sayyafzadeh', 'Hongmei Chi', 'Shonda Bernadin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patches', 'facial-biometric-attacks', 'diffusion-based-attack-refinement', 'adversarial-detection', 'forensic-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09806</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BASIL: Bayesian Assessment of Sycophancy in LLMs</title><link>https://arxiv.org/abs/2508.16846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework that separates sycophantic belief shifts from rational belief updating and provides both descriptive and normative metrics.&lt;/li&gt;&lt;li&gt;Enables measurement of sycophancy even without ground-truth labels and quantifies how sycophancy leads to Bayesian inconsistency.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs on uncertainty-driven tasks, finds robust sycophantic belief shifts, and shows that post-hoc calibration and fine-tuning (SFT, DPO) reduce Bayesian inconsistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Atwell', 'Pedram Heydari', 'Anthony Sicilia', 'Malihe Alikhani']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'robustness', 'alignment', 'defenses', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16846</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title><link>https://arxiv.org/abs/2510.19670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoSense-LLM, an edge-first system that converts multimodal sensor streams into compact semantic tokens and coordinates with LLMs under latency, energy, bandwidth, and privacy constraints.&lt;/li&gt;&lt;li&gt;Key components include SenseFusion (sensor-to-language discrete codes), Edge-RAG (local retrieval grounding in site policies), PromptRouter (cost- and uncertainty-aware routing between edge and cloud), and Secure Execution (auditable redaction ensuring raw waveforms never leave device).&lt;/li&gt;&lt;li&gt;Emphasizes privacy-preserving, auditable data-minimization, on-device personalization and federated updates, and evaluation showing latency, bandwidth, and factual-consistency benefits of edge-dominant operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Mari Eplik', 'Javier Rojas', 'Aina Binti Abdullah', 'Pieter van der Merwe']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'edge-security', 'secure-execution', 'data-minimization', 'retrieval-augmented-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19670</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay</title><link>https://arxiv.org/abs/2601.10589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety Self-Play (SSP): a single LLM acts as both Attacker (generating jailbreaks) and Defender (refusing harmful requests) within a unified RL loop to autonomously discover and mitigate vulnerabilities.&lt;/li&gt;&lt;li&gt;Introduces Reflective Experience Replay using an experience pool and Upper Confidence Bound (UCB) sampling to prioritize failure cases (low-reward) and balance exploration/exploitation during learning.&lt;/li&gt;&lt;li&gt;Demonstrates that SSP outperforms defenses trained on static adversarial datasets, enabling evolving, proactive red teaming and improved safety alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Wang', 'Yanting Wang', 'Hao Li', 'Rui Li', 'Lei Sha']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'adversarial training', 'reinforcement learning', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10589</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</title><link>https://arxiv.org/abs/2601.10543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs internally produce latent safety-related signals during generation even when jailbreaks succeed, but these signals are overridden by fluent continuation.&lt;/li&gt;&lt;li&gt;Proposes an in-decoding safety-awareness probing method that surfaces and leverages these latent signals for early detection/intervention during decoding.&lt;/li&gt;&lt;li&gt;Shows empirical gains against diverse jailbreak attacks with improved safety, low over-refusal on benign inputs, and preserved response quality.&lt;/li&gt;&lt;li&gt;Provides code for reproduction and positions this approach as a complementary defense to existing decoding constraints and post-hoc detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinzhi Zhao', 'Ming Wang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yifei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'decoding-based defense', 'safety probing', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10543</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale</title><link>https://arxiv.org/abs/2601.10338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical security analysis of 42,447 agent skills (31,132 analyzed) from two marketplaces, using SkillScan (static analysis + LLM semantic classification).&lt;/li&gt;&lt;li&gt;Finds pervasive vulnerabilities: 26.1% of skills contain ≥1 issue across 14 patterns in four categories (prompt injection, data exfiltration, privilege escalation, supply chain risks); 5.2% show high-severity malicious patterns.&lt;/li&gt;&lt;li&gt;Presents a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, a detection methodology (86.7% precision, 82.5% recall), and an open dataset/toolkit; recommends capability-based permissions and mandatory vetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent-skills', 'prompt-injection', 'data-exfiltration', 'supply-chain-attacks', 'vulnerability-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10338</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure</title><link>https://arxiv.org/abs/2601.10566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Knowledge Immunization Framework (KIF), a representation-aware unlearning method that targets internal activation signatures to achieve true knowledge erasure rather than surface-level suppression.&lt;/li&gt;&lt;li&gt;Combines dynamic suppression of subject-specific representations with parameter-efficient adaptation to enable durable unlearning without full model retraining.&lt;/li&gt;&lt;li&gt;Proposes a dual-metric evaluation protocol measuring both surface-level leakage and latent trace persistence to distinguish obfuscation from genuine erasure.&lt;/li&gt;&lt;li&gt;Evaluates KIF across multiple LLM families (Llama, Mistral, Qwen, DeepSeek) and scales (3B–14B), reporting near-oracle erasure and preserved utility, and highlighting architectural differences in 'reasoning-prior' models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Naveed Mahmood', 'Md. Rezaur Rahman Bhuiyan', 'Tasfia Zaman', 'Jareen Tasneem Khondaker', 'Md. Sameer Sakib', 'Nazia Tasnim', 'Farig Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy / GDPR compliance', 'representation-level defenses', 'latent trace detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10566</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models</title><link>https://arxiv.org/abs/2601.10387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an 'Assistant Axis' in activation space that captures the model's default helpful Assistant persona across multiple LLMs.&lt;/li&gt;&lt;li&gt;Shows that steering along this axis modulates persona (helpful vs. other entities) and that deviations predict 'persona drift' into harmful or bizarre behaviors.&lt;/li&gt;&lt;li&gt;Finds persona drift is often triggered by meta-reflective prompts or emotionally vulnerable interactions and that the axis exists prior to post-training.&lt;/li&gt;&lt;li&gt;Proposes a defense: restricting activations to a fixed region along the Assistant Axis to stabilize behavior and mitigate adversarial persona-based jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christina Lu', 'Jack Gallagher', 'Jonathan Michala', 'Kyle Fish', 'Jack Lindsey']&lt;/li&gt;&lt;li&gt;Tags: ['persona-steering', 'jailbreak-defenses', 'interpretability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10387</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis</title><link>https://arxiv.org/abs/2601.09734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Hallucination Diagnosis Task requiring detection, error localization, causal explanation, and content correction for LLM hallucinations.&lt;/li&gt;&lt;li&gt;Presents the Hallucination Diagnosis Generator (HDG), an automated pipeline that synthesizes high-quality training data with rich diagnostic metadata via controlled fact fabrication and reasoning perturbations.&lt;/li&gt;&lt;li&gt;Trains HDM-4B-RL (4B parameters) using Group Relative Policy Optimization with multi-component rewards (structural, accuracy, localization) to perform diagnosis.&lt;/li&gt;&lt;li&gt;Shows HDM-4B-RL outperforms prior detection models on benchmarks and achieves diagnosis capabilities comparable to larger general-purpose models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanyi Liu', 'Qingwen Yang', 'Tiezheng Guo', 'Feiyu Qu', 'Jun Liu', 'Yingyou Wen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination diagnosis', 'synthetic data generation', 'model safety', 'robustness', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09734</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions</title><link>https://arxiv.org/abs/2601.09724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Syntactic Framing Fragility (SFF), a framework to evaluate whether LLM ethical judgments change across logically equivalent but syntactically different prompts using Logical Polarity Normalization (LPN).&lt;/li&gt;&lt;li&gt;Audits 23 models across 14 ethical scenarios and four controlled framings (39,975 decisions), finding widespread, statistically significant inconsistencies—some models reverse endorsements purely due to syntactic polarity, with open-source models more fragile than commercial ones.&lt;/li&gt;&lt;li&gt;Identifies extreme negation sensitivity (models sometimes endorse actions 80–97% of the time when prompted with explicit "should not"), maps higher fragility in financial/business scenarios, and shows chain-of-thought elicitation substantially reduces fragility as a practical mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Elkins', 'Jon Chun']&lt;/li&gt;&lt;li&gt;Tags: ['prompt robustness', 'adversarial prompting', 'ethical robustness', 'robustness evaluation', 'mitigation (chain-of-thought)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09724</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox</title><link>https://arxiv.org/abs/2601.09721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM safety under adversarial, anxiety-driven pediatric consultation prompts using a 300-query benchmark (PediatricAnxietyBench) across three models and platforms.&lt;/li&gt;&lt;li&gt;Measures safety on a multi-dimensional 0–15 scale (restraint, referral, hedging, emergency recognition, non-prescriptive behavior) and finds smaller models can outperform larger ones; some failure modes (e.g., seizures, missed emergency recognition) persist.&lt;/li&gt;&lt;li&gt;Shows robustness evolving across model releases and emphasizes need for adversarial stress-testing and alignment-focused training for medical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vahideh Zolfaghari']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'LLM safety', 'medical AI', 'security evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09721</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Random Walk Learning and the Pac-Man Attack</title><link>https://arxiv.org/abs/2508.05663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a novel adversarial threat called the "Pac-Man" attack, where a malicious node probabilistically terminates random walks (RWs) in RW-based decentralized learning, stealthily causing RW extinction and halting learning.&lt;/li&gt;&lt;li&gt;Proposes the Average Crossing (AC) algorithm, a fully decentralized RW duplication mechanism that prevents RW extinction; provides theoretical guarantees that RW population is almost surely bounded under AC and that RW-based SGD converges with a quantifiable deviation from the true optimum despite the attack.&lt;/li&gt;&lt;li&gt;Presents extensive empirical validation on synthetic and real-world datasets, observing a phase transition in extinction probability as a function of the duplication threshold, and analyzes a simplified AC variant to explain this phenomenon.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingran Chen', 'Parimal Parag', 'Rohit Bhagat', 'Zonghong Liu', 'Salim El Rouayheb']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'decentralized-learning', 'robustness', 'random-walk', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05663</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm</title><link>https://arxiv.org/abs/2501.14230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GreedyPixel, a black-box adversarial attack that performs per-pixel greedy optimization guided by a surrogate-derived priority map and refined with query feedback.&lt;/li&gt;&lt;li&gt;Evaluates coordinates directly without gradients, claiming monotonic loss reduction and convergence to a coordinate-wise optimum for fine-grained, pixel-sparse perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art success rates and near white-box precision on CIFAR-10 and ImageNet across CNN and Transformer models, with visually imperceptible perturbations.&lt;/li&gt;&lt;li&gt;Provides implementation and code release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanrui Wang', 'Ching-Chun Chang', 'Chun-Shien Lu', 'Christopher Leckie', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['black-box attack', 'adversarial examples', 'pixel-wise attack', 'greedy optimization', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14230</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station</title><link>https://arxiv.org/abs/2510.23463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes over-the-air federated learning (AirFL) over multiple-access fading channels with a multi-antenna base station under user-level differential privacy (DP) requirements.&lt;/li&gt;&lt;li&gt;Derives a novel, convergent DP bound (without requiring injected artificial noise) alongside convergence guarantees for general smooth non-convex losses under bounded-domain assumptions.&lt;/li&gt;&lt;li&gt;Optimizes receive beamforming and power allocation to characterize optimal convergence–privacy trade-offs and provides conditions where DP is achievable without harming training; validated by numerical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Liang', 'Haifeng Wen', 'Kaishun Wu', 'Hong Xing']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'federated learning', 'over-the-air computation', 'privacy-preserving ML', 'wireless security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23463</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies vulnerability of existing unlearning methods to relearning attacks where forgot knowledge re-emerges after fine-tuning on retain-set examples (even with zero forget-set examples).&lt;/li&gt;&lt;li&gt;Identifies weight-space properties (L2 distance and linear mode connectivity between original and unlearned models) that predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes a new class of weight-space regularization methods that substantially improve tamper-resistant unlearning and state-of-the-art resistance to relearning attacks in vision classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'relearning attacks', 'weight-space regularization', 'model robustness', 'tamper-resistant unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses</title><link>https://arxiv.org/abs/2502.17772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a rigorous privacy analysis of DPSGD for L-smooth, non-convex losses in bounded domains, showing privacy loss can converge across iterations without requiring convexity.&lt;/li&gt;&lt;li&gt;Derives how bounded-domain diameter and gradient clipping affect both privacy and utility, giving big-O privacy–utility trade-offs for DPSGD variants (DPSGD-GC and DPSGD-DC) under different assumptions.&lt;/li&gt;&lt;li&gt;Validates theoretical findings empirically using membership inference attacks to demonstrate practical privacy implications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Liang', 'Wanrong Zhang', 'Xinlei He', 'Kaishun Wu', 'Hong Xing']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'DPSGD', 'membership-inference-attack', 'privacy-utility-tradeoff', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17772</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust LLM Alignment via Distributionally Robust Direct Preference Optimization</title><link>https://arxiv.org/abs/2502.01930</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces distributionally robust optimization approaches to align LLMs under preference distribution shift, proposing two algorithms: Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO).&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of sample complexity for learning optimal policy parameters for WDPO and KLDPO.&lt;/li&gt;&lt;li&gt;Develops scalable gradient-descent-style approximations to optimize the challenging minimax objectives.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved alignment robustness on benchmark datasets and LLMs when preference distributions shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zaiyan Xu', 'Sushil Vemuri', 'Kishan Panaganti', 'Dileep Kalathil', 'Rahul Jain', 'Deepak Ramachandran']&lt;/li&gt;&lt;li&gt;Tags: ['robust-alignment', 'distributional-robustness', 'DPO', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01930</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Permissive Information-Flow Analysis for Large Language Models</title><link>https://arxiv.org/abs/2410.03055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a permissive dynamic information-flow (taint) propagation method for LLM queries that only propagates labels of inputs influential in generating the output, to reduce overconservative blocking of outputs.&lt;/li&gt;&lt;li&gt;Implements two variants: (i) prompt-based retrieval augmentation and (ii) a k-nearest-neighbors language model, and compares both to an introspection-based baseline.&lt;/li&gt;&lt;li&gt;Targets system-level security/privacy risks (poisoned inputs, confidential-data leakage) in LLM agents and shows the permissive propagator outperforms the baseline in &gt;85% of experimental cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Radhika Gaonkar', 'Boris K\\"opf', 'David Krueger', 'Andrew Paverd', 'Ahmed Salem', 'Shruti Tople', 'Lukas Wutschitz', 'Menglin Xia', "Santiago Zanella-B\\'eguelin"]&lt;/li&gt;&lt;li&gt;Tags: ['information-flow', 'taint-tracking', 'data-leakage', 'LLM-security', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03055</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Fails to Remove Data Poisoning Attacks</title><link>https://arxiv.org/abs/2406.17216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates several practical machine unlearning methods and their ability to remove effects of data poisoning.&lt;/li&gt;&lt;li&gt;Shows unlearning methods fail to eliminate poisoning across attack types (indiscriminate, targeted, and a new Gaussian poisoning) and model classes (image classifiers and LLMs), even with large compute budgets.&lt;/li&gt;&lt;li&gt;Introduces new evaluation metrics for assessing unlearning efficacy specifically in the context of data poisoning.&lt;/li&gt;&lt;li&gt;Concludes that current unlearning techniques provide limited benefit over full retraining and are not yet reliable defenses against poisoning without provable guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Pawelczyk', 'Jimmy Z. Di', 'Yiwei Lu', 'Gautam Kamath', 'Ayush Sekhari', 'Seth Neel']&lt;/li&gt;&lt;li&gt;Tags: ['data-poisoning', 'machine-unlearning', 'adversarial-robustness', 'security-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17216</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure</title><link>https://arxiv.org/abs/2601.10566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Knowledge Immunization Framework (KIF), a representation-aware unlearning method that targets internal activation signatures to achieve true knowledge erasure rather than surface-level suppression.&lt;/li&gt;&lt;li&gt;Combines dynamic suppression of subject-specific representations with parameter-efficient adaptation to enable durable unlearning without full model retraining.&lt;/li&gt;&lt;li&gt;Proposes a dual-metric evaluation protocol measuring both surface-level leakage and latent trace persistence to distinguish obfuscation from genuine erasure.&lt;/li&gt;&lt;li&gt;Evaluates KIF across multiple LLM families (Llama, Mistral, Qwen, DeepSeek) and scales (3B–14B), reporting near-oracle erasure and preserved utility, and highlighting architectural differences in 'reasoning-prior' models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Naveed Mahmood', 'Md. Rezaur Rahman Bhuiyan', 'Tasfia Zaman', 'Jareen Tasneem Khondaker', 'Md. Sameer Sakib', 'Nazia Tasnim', 'Farig Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy / GDPR compliance', 'representation-level defenses', 'latent trace detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10566</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior</title><link>https://arxiv.org/abs/2601.10440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentGuardian, a runtime security framework that learns access-control policies for AI agents by monitoring execution traces during a controlled staging phase.&lt;/li&gt;&lt;li&gt;Derives adaptive, context- and control-flow-aware policies to regulate agent tool calls and multi-step orchestration behaviors.&lt;/li&gt;&lt;li&gt;Detects malicious or misleading inputs and mitigates hallucination-driven and orchestration-level failures while preserving normal agent functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nadya Abaev', 'Denis Klimov', 'Gerard Levinov', 'David Mimran', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'agent-governance', 'adversarial-detection', 'runtime-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10440</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations</title><link>https://arxiv.org/abs/2601.10004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematizes threat model for LLMs in healthcare across Data preprocessing, Fine-tuning, and Inference phases, characterizing adversaries, capabilities, and attack surfaces.&lt;/li&gt;&lt;li&gt;Surveys and categorizes privacy-preserving techniques (PPTs) applied to protect sensitive clinical data, evaluating their applicability and limitations in diverse deployment environments.&lt;/li&gt;&lt;li&gt;Identifies persistent gaps and limitations in current defenses for healthcare settings and provides phase-aware recommendations and future research directions to strengthen privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey (Systematization of Knowledge)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohoshin Ara Tahera', 'Karamveer Singh Sidhu', 'Shuvalaxmi Dass', 'Sajal Saha']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving techniques', 'threat modeling', 'healthcare data privacy', 'systematization of knowledge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10004</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method</title><link>https://arxiv.org/abs/2601.09933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Diluted Convolutional Neural Network (DICNN) for Android malware classification that uses dilated convolutions to capture long-range patterns with fewer features.&lt;/li&gt;&lt;li&gt;Integrates Fast Gradient Sign Method (FGSM) perturbations during training (one-step adversarial training) to improve accuracy and provide a defensive benefit with low computational cost.&lt;/li&gt;&lt;li&gt;Reports high classification performance (99.44% accuracy) and claims outperforming other approaches such as custom DCNNs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Anand', 'Bhupendra Singh', 'Sunil Khemka', 'Bireswar Banerjee', 'Vishi Singh Bhatia', 'Piyush Ranjan']&lt;/li&gt;&lt;li&gt;Tags: ['malware classification', 'adversarial training (FGSM)', 'CNN architecture (dilated convolutions)', 'adversarial robustness', 'cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09933</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2601.10407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CS-GBA, a backdoor attack framework for offline RL that concentrates poisoning on critical transitions with high TD error to maximize impact under a strict budget.&lt;/li&gt;&lt;li&gt;Proposes a Correlation-Breaking Trigger that leverages mutually exclusive state-feature boundaries to remain statistically concealed and evade OOD detectors.&lt;/li&gt;&lt;li&gt;Uses Gradient-Guided Action Generation to craft worst-case in-manifold actions via the victim Q-network, replacing naive label inversion.&lt;/li&gt;&lt;li&gt;Empirically outperforms baselines on D4RL benchmarks, achieving high attack success rates against safety-constrained algorithms (e.g., CQL) with only a 5% poisoning budget while preserving clean-environment performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanjie Zhao', 'Junnan Qiu', 'Yue Ding', 'Jie Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'offline reinforcement learning', 'data poisoning', 'adversarial attack', 'stealthy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10407</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD</title><link>https://arxiv.org/abs/2601.10237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper analyzes fundamental privacy-utility limits of DP-SGD within the f-differential privacy framework for shuffled sampling over a single epoch with M updates.&lt;/li&gt;&lt;li&gt;It derives an explicit (suboptimal) upper bound on the achievable hypothesis-testing trade-off curve and a geometric lower bound on the separation κ between that curve and random guessing.&lt;/li&gt;&lt;li&gt;From this, it proves a lower bound on the Gaussian noise multiplier σ (σ ≥ 1/√(2 ln M)) or a corresponding large κ, implying DP-SGD cannot simultaneously achieve strong worst-case privacy and high utility for practical M; similar limits hold for Poisson subsampling up to constants.&lt;/li&gt;&lt;li&gt;Experiments demonstrate that noise levels implied by the bound cause substantial accuracy degradation in realistic training settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Murat Bilgehan Ertan', 'Marten van Dijk']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'DP-SGD', 'privacy-utility tradeoff', 'theoretical bounds', 'worst-case adversary']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10237</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Preserving Safety in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2601.10141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how fine-tuning can degrade LLM safety alignment by showing safety gradients lie in a low-rank subspace while utility gradients occupy a higher-dimensional space that often conflicts with safety.&lt;/li&gt;&lt;li&gt;Proposes Safety-Preserving Fine-tuning (SPF), which removes gradient components that conflict with the low-rank safety subspace to maintain safety while allowing utility convergence.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees bounding safety drift and empirically demonstrates that SPF preserves pre-trained safety, retains downstream task performance, and resists adversarial fine-tuning and dynamic jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Yangfan Hu', 'Kejia Chen', 'Lipeng He', 'Jiachen Ma', 'Jian Lou', 'Dan Li', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['Fine-tuning defenses', 'Safety alignment', 'Jailbreak mitigation', 'Gradient subspace analysis', 'Adversarial fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10141</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains</title><link>https://arxiv.org/abs/2601.09946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an interpolation-based optimization framework to enforce lp-norm Metric Differential Privacy (mDP) in continuous and fine-grained domains by optimizing perturbation distributions at sparse anchor points and interpolating elsewhere via log-convex combinations.&lt;/li&gt;&lt;li&gt;Introduces a corrected, sequential one-dimensional interpolation decomposition to prevent privacy violations in high-dimensional spaces and jointly optimizes perturbation distributions and per-dimension privacy budget allocation.&lt;/li&gt;&lt;li&gt;Evaluates on real-world location datasets, demonstrating rigorous privacy guarantees and improved utility over baseline mechanisms in fine-grained domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'metric_differential_privacy', 'privacy_defense', 'location_privacy', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09946</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STELP: Secure Transpilation and Execution of LLM-Generated Programs</title><link>https://arxiv.org/abs/2601.05467</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STELP, a Secure Transpiler and Executor to run LLM-generated programs in a controlled environment to prevent malicious behaviors (data poisoning, backdoors, unsafe/hallucinatory code).&lt;/li&gt;&lt;li&gt;Provides a human-validated dataset of insecure code snippets and benchmarks STELP against prior methods on correctness, safety, and latency, reporting significant improvements in safely executing risky code.&lt;/li&gt;&lt;li&gt;Targets production autonomous/code-generation systems where human review or traditional testing is impractical, focusing on runtime defenses, safe execution pipelines, and mitigation of code-originated vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Swapnil Shinde', 'Sahil Wadhwa', 'Andy Luo', 'Akshay Gupta', 'Mohammad Shahed Sorower']&lt;/li&gt;&lt;li&gt;Tags: ['secure execution', 'LLM-generated code', 'sandboxing/transpilation', 'malicious code defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05467</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies vulnerability of existing unlearning methods to relearning attacks where forgot knowledge re-emerges after fine-tuning on retain-set examples (even with zero forget-set examples).&lt;/li&gt;&lt;li&gt;Identifies weight-space properties (L2 distance and linear mode connectivity between original and unlearned models) that predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes a new class of weight-space regularization methods that substantially improve tamper-resistant unlearning and state-of-the-art resistance to relearning attacks in vision classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'relearning attacks', 'weight-space regularization', 'model robustness', 'tamper-resistant unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust LLM Alignment via Distributionally Robust Direct Preference Optimization</title><link>https://arxiv.org/abs/2502.01930</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces distributionally robust optimization approaches to align LLMs under preference distribution shift, proposing two algorithms: Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO).&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of sample complexity for learning optimal policy parameters for WDPO and KLDPO.&lt;/li&gt;&lt;li&gt;Develops scalable gradient-descent-style approximations to optimize the challenging minimax objectives.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved alignment robustness on benchmark datasets and LLMs when preference distributions shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zaiyan Xu', 'Sushil Vemuri', 'Kishan Panaganti', 'Dileep Kalathil', 'Rahul Jain', 'Deepak Ramachandran']&lt;/li&gt;&lt;li&gt;Tags: ['robust-alignment', 'distributional-robustness', 'DPO', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01930</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Permissive Information-Flow Analysis for Large Language Models</title><link>https://arxiv.org/abs/2410.03055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a permissive dynamic information-flow (taint) propagation method for LLM queries that only propagates labels of inputs influential in generating the output, to reduce overconservative blocking of outputs.&lt;/li&gt;&lt;li&gt;Implements two variants: (i) prompt-based retrieval augmentation and (ii) a k-nearest-neighbors language model, and compares both to an introspection-based baseline.&lt;/li&gt;&lt;li&gt;Targets system-level security/privacy risks (poisoned inputs, confidential-data leakage) in LLM agents and shows the permissive propagator outperforms the baseline in &gt;85% of experimental cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Radhika Gaonkar', 'Boris K\\"opf', 'David Krueger', 'Andrew Paverd', 'Ahmed Salem', 'Shruti Tople', 'Lukas Wutschitz', 'Menglin Xia', "Santiago Zanella-B\\'eguelin"]&lt;/li&gt;&lt;li&gt;Tags: ['information-flow', 'taint-tracking', 'data-leakage', 'LLM-security', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03055</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Fails to Remove Data Poisoning Attacks</title><link>https://arxiv.org/abs/2406.17216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates several practical machine unlearning methods and their ability to remove effects of data poisoning.&lt;/li&gt;&lt;li&gt;Shows unlearning methods fail to eliminate poisoning across attack types (indiscriminate, targeted, and a new Gaussian poisoning) and model classes (image classifiers and LLMs), even with large compute budgets.&lt;/li&gt;&lt;li&gt;Introduces new evaluation metrics for assessing unlearning efficacy specifically in the context of data poisoning.&lt;/li&gt;&lt;li&gt;Concludes that current unlearning techniques provide limited benefit over full retraining and are not yet reliable defenses against poisoning without provable guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Pawelczyk', 'Jimmy Z. Di', 'Yiwei Lu', 'Gautam Kamath', 'Ayush Sekhari', 'Seth Neel']&lt;/li&gt;&lt;li&gt;Tags: ['data-poisoning', 'machine-unlearning', 'adversarial-robustness', 'security-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17216</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BASIL: Bayesian Assessment of Sycophancy in LLMs</title><link>https://arxiv.org/abs/2508.16846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework that separates sycophantic belief shifts from rational belief updating and provides both descriptive and normative metrics.&lt;/li&gt;&lt;li&gt;Enables measurement of sycophancy even without ground-truth labels and quantifies how sycophancy leads to Bayesian inconsistency.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs on uncertainty-driven tasks, finds robust sycophantic belief shifts, and shows that post-hoc calibration and fine-tuning (SFT, DPO) reduce Bayesian inconsistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Atwell', 'Pedram Heydari', 'Anthony Sicilia', 'Malihe Alikhani']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'robustness', 'alignment', 'defenses', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16846</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior</title><link>https://arxiv.org/abs/2601.10440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentGuardian, a runtime security framework that learns access-control policies for AI agents by monitoring execution traces during a controlled staging phase.&lt;/li&gt;&lt;li&gt;Derives adaptive, context- and control-flow-aware policies to regulate agent tool calls and multi-step orchestration behaviors.&lt;/li&gt;&lt;li&gt;Detects malicious or misleading inputs and mitigates hallucination-driven and orchestration-level failures while preserving normal agent functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nadya Abaev', 'Denis Klimov', 'Gerard Levinov', 'David Mimran', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'agent-governance', 'adversarial-detection', 'runtime-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10440</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale</title><link>https://arxiv.org/abs/2601.10338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical security analysis of 42,447 agent skills (31,132 analyzed) from two marketplaces, using SkillScan (static analysis + LLM semantic classification).&lt;/li&gt;&lt;li&gt;Finds pervasive vulnerabilities: 26.1% of skills contain ≥1 issue across 14 patterns in four categories (prompt injection, data exfiltration, privilege escalation, supply chain risks); 5.2% show high-severity malicious patterns.&lt;/li&gt;&lt;li&gt;Presents a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, a detection methodology (86.7% precision, 82.5% recall), and an open dataset/toolkit; recommends capability-based permissions and mandatory vetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent-skills', 'prompt-injection', 'data-exfiltration', 'supply-chain-attacks', 'vulnerability-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10338</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Preserving Safety in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2601.10141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how fine-tuning can degrade LLM safety alignment by showing safety gradients lie in a low-rank subspace while utility gradients occupy a higher-dimensional space that often conflicts with safety.&lt;/li&gt;&lt;li&gt;Proposes Safety-Preserving Fine-tuning (SPF), which removes gradient components that conflict with the low-rank safety subspace to maintain safety while allowing utility convergence.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees bounding safety drift and empirically demonstrates that SPF preserves pre-trained safety, retains downstream task performance, and resists adversarial fine-tuning and dynamic jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Yangfan Hu', 'Kejia Chen', 'Lipeng He', 'Jiachen Ma', 'Jian Lou', 'Dan Li', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['Fine-tuning defenses', 'Safety alignment', 'Jailbreak mitigation', 'Gradient subspace analysis', 'Adversarial fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10141</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method</title><link>https://arxiv.org/abs/2601.09933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Diluted Convolutional Neural Network (DICNN) for Android malware classification that uses dilated convolutions to capture long-range patterns with fewer features.&lt;/li&gt;&lt;li&gt;Integrates Fast Gradient Sign Method (FGSM) perturbations during training (one-step adversarial training) to improve accuracy and provide a defensive benefit with low computational cost.&lt;/li&gt;&lt;li&gt;Reports high classification performance (99.44% accuracy) and claims outperforming other approaches such as custom DCNNs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Anand', 'Bhupendra Singh', 'Sunil Khemka', 'Bireswar Banerjee', 'Vishi Singh Bhatia', 'Piyush Ranjan']&lt;/li&gt;&lt;li&gt;Tags: ['malware classification', 'adversarial training (FGSM)', 'CNN architecture (dilated convolutions)', 'adversarial robustness', 'cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09933</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification</title><link>https://arxiv.org/abs/2601.09806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end pipeline to generate and refine adversarial patches for facial biometric systems using FGSM for initial noise and a diffusion model (reverse diffusion) to improve imperceptibility (Gaussian smoothing, adaptive brightness correction).&lt;/li&gt;&lt;li&gt;Applies refined patches to facial images to evaluate evasion of identity verification and expression recognition, and uses a ViT–GPT2 captioning model to produce semantic identity descriptions for forensic interpretation.&lt;/li&gt;&lt;li&gt;Includes detection/forensic analysis using perceptual hashing and segmentation to find adversarial patches, reporting high similarity preservation (SSIM 0.95) while demonstrating vulnerabilities in face recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahrzad Sayyafzadeh', 'Hongmei Chi', 'Shonda Bernadin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patches', 'facial-biometric-attacks', 'diffusion-based-attack-refinement', 'adversarial-detection', 'forensic-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09806</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis</title><link>https://arxiv.org/abs/2601.09734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Hallucination Diagnosis Task requiring detection, error localization, causal explanation, and content correction for LLM hallucinations.&lt;/li&gt;&lt;li&gt;Presents the Hallucination Diagnosis Generator (HDG), an automated pipeline that synthesizes high-quality training data with rich diagnostic metadata via controlled fact fabrication and reasoning perturbations.&lt;/li&gt;&lt;li&gt;Trains HDM-4B-RL (4B parameters) using Group Relative Policy Optimization with multi-component rewards (structural, accuracy, localization) to perform diagnosis.&lt;/li&gt;&lt;li&gt;Shows HDM-4B-RL outperforms prior detection models on benchmarks and achieves diagnosis capabilities comparable to larger general-purpose models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanyi Liu', 'Qingwen Yang', 'Tiezheng Guo', 'Feiyu Qu', 'Jun Liu', 'Yingyou Wen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination diagnosis', 'synthetic data generation', 'model safety', 'robustness', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09734</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions</title><link>https://arxiv.org/abs/2601.09724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Syntactic Framing Fragility (SFF), a framework to evaluate whether LLM ethical judgments change across logically equivalent but syntactically different prompts using Logical Polarity Normalization (LPN).&lt;/li&gt;&lt;li&gt;Audits 23 models across 14 ethical scenarios and four controlled framings (39,975 decisions), finding widespread, statistically significant inconsistencies—some models reverse endorsements purely due to syntactic polarity, with open-source models more fragile than commercial ones.&lt;/li&gt;&lt;li&gt;Identifies extreme negation sensitivity (models sometimes endorse actions 80–97% of the time when prompted with explicit "should not"), maps higher fragility in financial/business scenarios, and shows chain-of-thought elicitation substantially reduces fragility as a practical mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Elkins', 'Jon Chun']&lt;/li&gt;&lt;li&gt;Tags: ['prompt robustness', 'adversarial prompting', 'ethical robustness', 'robustness evaluation', 'mitigation (chain-of-thought)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09724</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox</title><link>https://arxiv.org/abs/2601.09721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM safety under adversarial, anxiety-driven pediatric consultation prompts using a 300-query benchmark (PediatricAnxietyBench) across three models and platforms.&lt;/li&gt;&lt;li&gt;Measures safety on a multi-dimensional 0–15 scale (restraint, referral, hedging, emergency recognition, non-prescriptive behavior) and finds smaller models can outperform larger ones; some failure modes (e.g., seizures, missed emergency recognition) persist.&lt;/li&gt;&lt;li&gt;Shows robustness evolving across model releases and emphasizes need for adversarial stress-testing and alignment-focused training for medical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vahideh Zolfaghari']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'LLM safety', 'medical AI', 'security evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09721</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</title><link>https://arxiv.org/abs/2601.10543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs internally produce latent safety-related signals during generation even when jailbreaks succeed, but these signals are overridden by fluent continuation.&lt;/li&gt;&lt;li&gt;Proposes an in-decoding safety-awareness probing method that surfaces and leverages these latent signals for early detection/intervention during decoding.&lt;/li&gt;&lt;li&gt;Shows empirical gains against diverse jailbreak attacks with improved safety, low over-refusal on benign inputs, and preserved response quality.&lt;/li&gt;&lt;li&gt;Provides code for reproduction and positions this approach as a complementary defense to existing decoding constraints and post-hoc detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinzhi Zhao', 'Ming Wang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yifei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'decoding-based defense', 'safety probing', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10543</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment</title><link>https://arxiv.org/abs/2601.10520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a neuro-symbolic reason-based containment architecture that separates normative reasoning from instrumental decision-making to contain arbitrary AI agents.&lt;/li&gt;&lt;li&gt;Defines three components: a Moral Module (deontic logic-based) to derive permissible macro actions, a Decision-Making Module that executes instrumentally optimal primitives constrained by those macro actions, and a Guard that enforces compliance.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability, contestability, and justifiability via symbolic representations and claims support for formal verification and statistical guarantees of alignment.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on an LLM therapy assistant example to illustrate stakeholder contestability and refinement of agent behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felix Jahn', 'Yannic Muskalla', 'Lisa Dargasz', 'Patrick Schramowski', 'Kevin Baum']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'containment', 'alignment', 'neuro-symbolic', 'formal-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10520</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries</title><link>https://arxiv.org/abs/2601.10398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LatentRefusal, a safety/ refusal mechanism for text-to-SQL systems that predicts query answerability from intermediate LLM hidden activations to avoid generating executable, incorrect, or unsafe SQL.&lt;/li&gt;&lt;li&gt;Introduces the Tri-Residual Gated Encoder, a lightweight probe that suppresses schema noise and amplifies localized cues of question-schema mismatch to detect unanswerable or underspecified queries.&lt;/li&gt;&lt;li&gt;Demonstrates attachable, efficient defense across four benchmarks, improving average F1 to 88.5% with ~2 ms probe overhead, plus ablations and interpretability analyses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancheng Ren', 'Shijing Hu', 'Zhihui Lu', 'Jiangqi Huang', 'Qiang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['refusal/guardrails', 'safety mechanism', 'LLM probing', 'text-to-SQL', 'input validation/detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10398</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination Detection and Mitigation in Large Language Models</title><link>https://arxiv.org/abs/2601.09929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an operational framework for detecting and mitigating hallucinations in LLMs/LRMs using a continuous improvement loop and root-cause categorization (model, data, context).&lt;/li&gt;&lt;li&gt;Combines multiple detection techniques (uncertainty estimation, reasoning consistency checks) with stratified mitigation strategies (knowledge grounding, confidence calibration) and a tiered system architecture.&lt;/li&gt;&lt;li&gt;Demonstrates framework application via a financial data extraction case study with a closed feedback loop across model, context, and data tiers to improve reliability in regulated settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Pesaranghader', 'Erin Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'mitigation', 'knowledge-grounding', 'uncertainty-estimation', 'operational-framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09929</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents</title><link>https://arxiv.org/abs/2601.09923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies prompt injection risks for Computer Use Agents (CUAs) that observe UIs and proposes an architectural defense called Single-Shot Planning: a trusted planner emits a complete conditional execution graph before observing untrusted UI content, providing provable control-flow integrity against instruction injection.&lt;/li&gt;&lt;li&gt;Introduces and analyzes a secondary attack class, Branch Steering, where adversarial UI manipulations cause the agent to follow unintended but valid plan branches; proposes additional mitigations to address this.&lt;/li&gt;&lt;li&gt;Implements and evaluates the approach on the OSWorld benchmark, showing tradeoffs: up to 57% of frontier-model performance retained and up to 19% improvement for smaller open-source models, demonstrating practical security-utility balance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanna Foerster', 'Robert Mullins', 'Tom Blanchard', 'Nicolas Papernot', "Kristina Nikoli\\'c", 'Florian Tram\\`er', 'Ilia Shumailov', 'Cheng Zhang', 'Yiren Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'architectural isolation', 'control-flow integrity', 'branch steering', 'agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09923</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation</title><link>https://arxiv.org/abs/2601.09771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PCN-Rec: a pipeline that separates LLM reasoning from deterministic enforcement to ensure governance constraints in recommender slates.&lt;/li&gt;&lt;li&gt;Two LLM agents (User Advocate and Policy Agent) negotiate candidate windows; a mediator LLM emits a slate plus a JSON certificate; a deterministic verifier checks constraints and a repair procedure fixes noncompliant slates, producing an auditable trace.&lt;/li&gt;&lt;li&gt;Evaluated on MovieLens-100K achieving a 98.55% pass rate on feasible users with only a 0.021 absolute drop in NDCG@10 compared to a one-shot LLM baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aradhya Dixit', 'Shreem Dixit']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'guardrails', 'verification', 'recommender-systems', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09771</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
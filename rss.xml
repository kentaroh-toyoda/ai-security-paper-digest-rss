<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 19 Feb 2026 23:00:56 +0000</lastBuildDate><item><title>GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models</title><link>https://arxiv.org/abs/2602.00191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GEPC (Group-Equivariant Posterior Consistency), a training-free probe that detects out-of-distribution (OOD) inputs for diffusion models by measuring how consistently the learned score transforms under a finite group of symmetries.&lt;/li&gt;&lt;li&gt;Defines an ideal GEPC residual at the population level, derives in-distribution upper bounds and OOD lower bounds under mild assumptions, and produces interpretable equivariance-breaking maps.&lt;/li&gt;&lt;li&gt;Requires only score evaluations (no extra training), is computationally lightweight, and shows competitive or improved AUROC on image OOD benchmarks and strong separation on high-resolution synthetic aperture radar (SAR) imagery.&lt;/li&gt;&lt;li&gt;Code is released and the method focuses on exploiting equivariance-breaking as an OOD/robustness signal for diffusion-based generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yadang Alexis Rouzoumka', 'Jean Pinsolle', "Eug\\'enie Terreaux", 'Christ\\`ele Morisseau', 'Jean-Philippe Ovarlez', 'Chengfang Ren']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'diffusion models', 'robustness', 'equivariance', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00191</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title><link>https://arxiv.org/abs/2501.03544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptGuard: a universal soft prompt optimized in the T2I textual embedding space to act as an implicit system prompt that moderates NSFW inputs without changing inference or using proxy models.&lt;/li&gt;&lt;li&gt;Introduces a divide-and-conquer approach by optimizing category-specific soft prompts and combining them into holistic safety guidance to improve reliability and helpfulness.&lt;/li&gt;&lt;li&gt;Reports experiments on five datasets showing PromptGuard is 3.8x faster than prior moderation methods, outperforms eight state-of-the-art defenses, and achieves an unsafe ratio as low as 5.84% while preserving benign output quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingzhi Yuan', 'Xinfeng Li', 'Chejian Xu', 'Guanhong Tao', 'Xiaojun Jia', 'Yihao Huang', 'Wei Dong', 'Yang Liu', 'Xiaofeng Wang', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-guardrails', 'soft-prompting', 'text-to-image', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03544</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face</title><link>https://arxiv.org/abs/2602.16569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Arc2Morph, a face morphing attack method built on Arc2Face, an identity-conditioned face synthesis model that preserves identity while generating photorealistic morphs.&lt;/li&gt;&lt;li&gt;Evaluates morphing attack potential against state-of-the-art morphing techniques on two large sequestered morphing detection datasets and on two new morphed datasets derived from FEI and ONOT.&lt;/li&gt;&lt;li&gt;Finds Arc2Morph achieves morphing attack potential comparable to landmark-based (traditionally hardest) morphs, demonstrating a substantive biometric security vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicol\\`o Di Domenico', 'Annalisa Franco', 'Matteo Ferrara', 'Davide Maltoni']&lt;/li&gt;&lt;li&gt;Tags: ['face morphing', 'biometric attack', 'deep learning', 'morphing attack detection', 'identity spoofing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16569</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection</title><link>https://arxiv.org/abs/2602.16494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a unified benchmark for digital, non-patch adversarial attacks on object detectors with metrics that separate localization vs. classification errors and uses multiple perceptual perturbation measures.&lt;/li&gt;&lt;li&gt;Evaluates modern attacks across a wide range of detectors and finds poor transferability of attacks to transformer-based (ViT) architectures.&lt;/li&gt;&lt;li&gt;Investigates adversarial training strategies and shows the most robust defense is training on a mixed dataset of diverse high-perturbation attacks with different objectives (spatial and semantic).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexis Winter', 'Jean-Vincent Martini', 'Romaric Audigier', 'Angelique Loesch', 'Bertrand Luvison']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'adversarial training', 'object detection', 'robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16494</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MMA: Multimodal Memory Agent</title><link>https://arxiv.org/abs/2602.16493</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multimodal Memory Agent (MMA) that assigns dynamic reliability scores to retrieved memory items using source credibility, temporal decay, and conflict-aware consensus, then reweights evidence and abstains when support is insufficient.&lt;/li&gt;&lt;li&gt;Introduces MMA-Bench, a programmatic benchmark for belief dynamics with controlled speaker reliability and structured text–vision contradictions to stress-test retrieval-augmented agents.&lt;/li&gt;&lt;li&gt;Demonstrates safety/robustness gains: reduces variance and wrong answers on downstream tasks, uncovers a 'Visual Placebo Effect' where RAG agents inherit visual biases, and shows improved selective utility under safety-oriented configurations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihao Lu', 'Wanru Cheng', 'Zeyu Zhang', 'Hao Tang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'retrieval-augmented generation', 'safety/abstention', 'benchmarking', 'multimodal bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16493</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Visual Memory Injection Attacks for Multi-Turn Conversations</title><link>https://arxiv.org/abs/2602.15927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Memory Injection (VMI): an attack where adversarially manipulated images uploaded to the web cause LVLMs to output attacker-prescribed messages when a trigger appears, while appearing benign under normal prompts.&lt;/li&gt;&lt;li&gt;Focuses on multi-turn, long-context conversations and demonstrates that the injected visual memory persists and activates after prolonged interactions.&lt;/li&gt;&lt;li&gt;Evaluates the attack on several open-weight large vision-language models and releases source code to reproduce the results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Schlarmann', 'Matthias Hein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'visual prompt injection', 'backdoor/memory poisoning', 'long-context robustness', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15927</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment</title><link>https://arxiv.org/abs/2602.15903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MSBA-CLIP: a deepfake detection framework combining Multivariate Soft Blending Augmentation (MSBA) and a CLIP-guided Multivariate Forgery Intensity Estimation (MFIE) module to improve generalization.&lt;/li&gt;&lt;li&gt;MSBA synthesizes blended forgeries from multiple methods with random weights to force learning of generalized forgery patterns; MFIE explicitly guides feature learning for varied forgery modes and intensities using CLIP alignment.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art in-domain and cross-domain improvements (notably AUC/Accuracy gains) across multiple datasets; includes ablations showing both components contribute to robustness, at higher compute cost due to CLIP usage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingwei Li', 'Jiaxin Tong', 'Pengfei Wu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery detection', 'data augmentation', 'CLIP-guided defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15903</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Weight space Detection of Backdoors in LoRA Adapters</title><link>https://arxiv.org/abs/2602.15195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-agnostic, weight-space method to detect backdoors in LoRA adapters by analyzing singular value concentration, entropy, and distribution shape of adapter weight matrices.&lt;/li&gt;&lt;li&gt;Evaluates on 500 LoRA adapters for Llama-3.2-3B (400 clean / 100 poisoned) across instruction and reasoning datasets (Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, GLUE).&lt;/li&gt;&lt;li&gt;Achieves ~97% detection accuracy with &lt;2% false positive rate, enabling screening of adapters without running models or requiring trigger examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Puertolas Merenciano', 'Ekaterina Vasyagina', 'Raghav Dixit', 'Kevin Zhu', 'Ruizhe Li', 'Javier Ferrando', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'LoRA adapters', 'weight-space analysis', 'model poisoning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15195</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents</title><link>https://arxiv.org/abs/2511.07176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRMP, a graph-representation-based model poisoning attack against federated fine-tuning in Internet of Agents (IoA) settings.&lt;/li&gt;&lt;li&gt;Constructs a feature-correlation graph from overheard benign updates and uses a variational graph autoencoder to generate malicious updates that preserve benign-like statistics.&lt;/li&gt;&lt;li&gt;Develops an optimization algorithm (augmented Lagrangian + subgradient descent) to embed adversarial objectives while evading detection.&lt;/li&gt;&lt;li&gt;Empirical results show substantial degradation of global LLM performance across models while remaining statistically consistent with benign updates, defeating existing defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanlin Cai', 'Houtianfu Wang', 'Haofan Dong', 'Kai Li', 'Sai Zou', 'Ozgur B. Akan']&lt;/li&gt;&lt;li&gt;Tags: ['model poisoning', 'federated learning', 'adversarial machine learning', 'attack evasion', 'graph neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07176</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models</title><link>https://arxiv.org/abs/2602.15689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a content-based framework for LLM refusal decisions in cybersecurity, characterizing requests along five technical dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users.&lt;/li&gt;&lt;li&gt;Argues refusal should explicitly model offense–defense trade-offs grounded in the technical substance of requests (not just stated intent or topic bans) to avoid inconsistent or over-restrictive behavior.&lt;/li&gt;&lt;li&gt;Shows the framework enables tunable, risk-aware refusal policies and more robust auditing against obfuscation and request segmentation, helping organizations balance defensive needs and misuse prevention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noa Linder', 'Meirav Segal', 'Omer Antverg', 'Gil Gekker', 'Tomer Fichman', 'Omri Bodenheimer', 'Edan Maor', 'Omer Nevo']&lt;/li&gt;&lt;li&gt;Tags: ['refusal policies', 'dual-use/misuse prevention', 'safety frameworks', 'guardrails', 'cybersecurity defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15689</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Embedding Inversion via Conditional Masked Diffusion Language Models</title><link>https://arxiv.org/abs/2602.11047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames embedding inversion as conditional masked diffusion, using a masked diffusion language model conditioned via adaptive layer normalization to recover tokens.&lt;/li&gt;&lt;li&gt;Recovers all tokens in parallel through iterative denoising (only ~8 forward passes) and does not require access to the target encoder at inference time.&lt;/li&gt;&lt;li&gt;Demonstrates token recovery on 32-token sequences across three embedding models without encoder access, iterative correction, or architecture-specific alignment.&lt;/li&gt;&lt;li&gt;Provides source code and a live demo (practical, reproducible attack implementation).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['embedding-inversion', 'model-inversion', 'privacy-attack', 'diffusion-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11047</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title><link>https://arxiv.org/abs/2512.03310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Randomized Masked Fine-Tuning (RMFT), a fine-tuning technique designed to reduce memorization of personally identifying information (PII) in LLMs.&lt;/li&gt;&lt;li&gt;Evaluates RMFT on the Enron Email Dataset, reporting ~80.8% reduction in Total Extraction Rate and ~80.2% reduction in Seen Extraction Rate versus baseline fine-tuning, with only a ~5.7% increase in perplexity.&lt;/li&gt;&lt;li&gt;Proposes MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and compares RMFT to deduplication using an AURC metric.&lt;/li&gt;&lt;li&gt;Focuses on a privacy-defense approach to mitigate data memorization, presenting empirical results and an evaluation methodology.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunj Joshi', 'David A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'memorization', 'data protection', 'defense', 'evaluation-framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03310</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ModalImmune: Immunity Driven Unlearning via Self Destructive Training</title><link>https://arxiv.org/abs/2602.16197</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so joint representations are robust to destructive modality influence.&lt;/li&gt;&lt;li&gt;Key components: a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multimodal benchmarks shows improved resilience to modality removal and corruption while preserving convergence stability and reconstruction capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rong Fu', 'Jia Yee Tan', 'Wenxin Zhang', 'Zijian Zhang', 'Ziming Wang', 'Zhaolu Kang', 'Muge Qi', 'Shuning Zhang', 'Simon Fong']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'robust training / defense', 'self-destructive training', 'modality removal / corruption resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16197</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models</title><link>https://arxiv.org/abs/2602.16639</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AREG, a multi-turn, zero-sum negotiation benchmark to operationalize and measure persuasion (offense) and resistance (defense) in LLMs.&lt;/li&gt;&lt;li&gt;Runs a round-robin tournament across frontier models and finds persuasion and resistance are weakly correlated (ρ = 0.33) and that resistance systematically outperforms persuasion.&lt;/li&gt;&lt;li&gt;Linguistic analysis shows incremental commitment-seeking boosts extraction success while verification-seeking responses support defense, revealing asymmetric behavioral vulnerabilities in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adib Sakhawat', 'Fardeen Sadab']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial evaluation', 'social engineering', 'red teaming', 'benchmark', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16639</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset</title><link>https://arxiv.org/abs/2602.16571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MathEd-PII: a benchmark of 1,000 math tutoring sessions (115,620 messages) with validated PII annotations and privacy-preserving surrogates produced via a human-in-the-loop LLM workflow.&lt;/li&gt;&lt;li&gt;Identifies and quantifies 'numeric ambiguity' where numeric/math-dense regions cause false PII redactions, using a density-based segmentation analysis to show concentration of false positives.&lt;/li&gt;&lt;li&gt;Evaluates four PII detection strategies (Presidio baseline; LLM prompting: basic, math-aware, segment-aware) and shows math-aware prompting substantially improves F1 (0.821 vs 0.379 baseline) while reducing numeric false positives, arguing for domain-aware de-identification to preserve utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuqian Zhou', 'Kirk Vanacore', 'Bakhtawar Ahtisham', 'Jinsook Lee', 'Doug Pietrzak', 'Daryl Hedley', 'Jorge Dias', 'Chris Shaw', 'Ruth Sch\\"afer', "Ren\\'e F. Kizilcec"]&lt;/li&gt;&lt;li&gt;Tags: ['PII detection', 'De-identification', 'Privacy-preserving', 'Benchmark', 'LLM prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16571</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents</title><link>https://arxiv.org/abs/2602.16346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STING, an automated multi-turn red-teaming framework that constructs stepwise illicit plans and iteratively probes tool-using LLM agents to measure how they enable harmful workflows.&lt;/li&gt;&lt;li&gt;Models multi-turn red-teaming as a time-to-first-jailbreak random variable and proposes analysis tools (discovery curves, hazard-ratio attribution) plus a new metric: Restricted Mean Jailbreak Discovery.&lt;/li&gt;&lt;li&gt;Shows STING produces substantially higher illicit-task completion than single-turn and chat-oriented baselines, and evaluates multilingual differences across six non-English settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nivya Talokar', 'Ayush K Tarun', 'Murari Mandal', 'Maksym Andriushchenko', 'Antoine Bosselut']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'agent security', 'evaluation framework', 'multilingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16346</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation</title><link>https://arxiv.org/abs/2602.15897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GHOST, a token-level obfuscation defense that substitutes original tokens with semantically distinct but embedding-proximate "shadow" tokens to break the mapping exploited by gradient inversion attacks (GIAs).&lt;/li&gt;&lt;li&gt;Implements a two-step pipeline: searching for candidate shadow tokens via multi-criteria metrics, then selecting shadows that preserve alignment with internal model outputs to minimize utility degradation.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation across models (BERT to Llama) and tasks (classification and generation), reporting strong privacy (recovery rate as low as ~1%) while retaining utility (e.g., classification F1 up to 0.92, generation perplexity ~5.45) against state-of-the-art and adaptive GIAs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinguo Feng', 'Zhongkui Ma', 'Zihan Wang', 'Alsharif Abuadbba', 'Guangdong Bai']&lt;/li&gt;&lt;li&gt;Tags: ['gradient inversion', 'privacy', 'defense', 'token obfuscation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15897</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content</title><link>https://arxiv.org/abs/2602.15871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents CheckIfExist, an open-source web tool that detects AI-generated (hallucinated) citations by validating references against CrossRef, Semantic Scholar, and OpenAlex.&lt;/li&gt;&lt;li&gt;Implements a cascading validation architecture with string-similarity algorithms to compute multi-dimensional confidence scores and returns validated APA citations and exportable BibTeX.&lt;/li&gt;&lt;li&gt;Supports single-reference checks and batch BibTeX processing to provide near-instant verification, targeting gaps left by existing reference managers and commercial services.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diletta Abbonato']&lt;/li&gt;&lt;li&gt;Tags: ['citation hallucination', 'hallucination detection', 'bibliography verification', 'LLM safety', 'open-source tool']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15871</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey</title><link>https://arxiv.org/abs/2602.15866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NLP-PRISM, a six-dimension framework (data collection, preprocessing, visibility, fairness, computational risk, regulatory compliance) to evaluate privacy risks in social-media NLP.&lt;/li&gt;&lt;li&gt;Survey of 203 papers across six NLP tasks; reports transformer F1 in 0.58–0.84 range and a 1%–23% F1 drop under privacy-preserving fine-tuning, plus utility–privacy trade-offs (2%–9%).&lt;/li&gt;&lt;li&gt;Evaluates privacy attacks empirically (MIA AUC ≈ 0.81, AIA accuracy ≈ 0.75) and highlights gaps in privacy coverage; recommends anonymization, privacy-aware learning, and fairness-driven training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhiman Goswami', 'Jai Kruthunz Naveen Kumar', 'Sanchari Das']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'membership-inference', 'attribute-inference', 'privacy-preserving-learning', 'social-media']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15866</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants</title><link>https://arxiv.org/abs/2602.15859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;End-to-end pipeline converting call transcripts into a RAG-grounded conversational assistant via LLM-based knowledge extraction and curated transcript filtering.&lt;/li&gt;&lt;li&gt;Assistant behavior controlled through progressive prompt tuning toward lean, modular, governed designs to ensure consistency, safety, and controllable execution.&lt;/li&gt;&lt;li&gt;Evaluation uses a transcript-grounded user simulator and additional red teaming that probes prompt injection, out-of-scope, and out-of-context attacks to measure robustness and human escalation behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Krittin Pachtrachai', 'Petmongkon Pornpichitsuwan', 'Wachiravit Modecrua', 'Touchapon Kraisingkorn']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'RAG robustness', 'prompt governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15859</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Lightweight Explainable Guardrail for Prompt Safety</title><link>https://arxiv.org/abs/2602.15853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LEG, a lightweight explainable guardrail that jointly classifies prompts as safe/unsafe and produces token-level explanations via a multi-task architecture.&lt;/li&gt;&lt;li&gt;Generates synthetic explanation-labeled training data using a novel strategy intended to counteract LLM confirmation biases.&lt;/li&gt;&lt;li&gt;Introduces a composite loss (cross-entropy + focal) with uncertainty-based weighting to capture global explanation signals during training.&lt;/li&gt;&lt;li&gt;Reports equal or better in-domain and out-of-domain performance on three datasets for both classification and explainability while using a much smaller model than prior approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Asiful Islam', 'Mihai Surdeanu']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'prompt safety', 'explainability', 'guardrails', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15853</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints</title><link>https://arxiv.org/abs/2602.15852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies temporal and lexical leakage in clinical note-based NLP models that can encode future decisions and inflate apparent predictive performance, posing deployment safety risks.&lt;/li&gt;&lt;li&gt;Introduces a lightweight auditing pipeline that uses interpretability to detect and suppress leakage-prone signals before final model training.&lt;/li&gt;&lt;li&gt;Case study on next-day discharge prediction after elective spine surgery shows audited models are more conservative, better calibrated, and rely less on discharge-related lexical cues.&lt;/li&gt;&lt;li&gt;Argues deployment-ready clinical NLP should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ha Na Cho', 'Sairam Sutari', 'Alexander Lopez', 'Hansen Bow', 'Kai Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['temporal leakage', 'data leakage', 'model auditing', 'safety/robustness', 'clinical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15852</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs</title><link>https://arxiv.org/abs/2602.15846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gated Tree Cross-Attention (GTCA), a checkpoint-compatible branch that injects precomputed constituency chunk memory into decoder-only LLMs while leaving the backbone unchanged.&lt;/li&gt;&lt;li&gt;Uses a token update mask and staged training to control when and how syntactic structure influences token updates, preserving pretrained competence.&lt;/li&gt;&lt;li&gt;Empirically improves robustness to minor grammatical perturbations across benchmarks and transformer backbones without degrading multiple-choice QA or commonsense reasoning performance.&lt;/li&gt;&lt;li&gt;Provides a practical architectural defense/robustness technique for making decoder-only LLMs more syntax-robust in a checkpoint-compatible way.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Gao', 'Shaonan Wang', 'Nai Ding']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model-architecture', 'syntax-robustness', 'checkpoint-compatibility', 'decoder-only LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15846</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Weight space Detection of Backdoors in LoRA Adapters</title><link>https://arxiv.org/abs/2602.15195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-agnostic, weight-space method to detect backdoors in LoRA adapters by analyzing singular value concentration, entropy, and distribution shape of adapter weight matrices.&lt;/li&gt;&lt;li&gt;Evaluates on 500 LoRA adapters for Llama-3.2-3B (400 clean / 100 poisoned) across instruction and reasoning datasets (Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, GLUE).&lt;/li&gt;&lt;li&gt;Achieves ~97% detection accuracy with &lt;2% false positive rate, enabling screening of adapters without running models or requiring trigger examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Puertolas Merenciano', 'Ekaterina Vasyagina', 'Raghav Dixit', 'Kevin Zhu', 'Ruizhe Li', 'Javier Ferrando', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'LoRA adapters', 'weight-space analysis', 'model poisoning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15195</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title><link>https://arxiv.org/abs/2512.03310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Randomized Masked Fine-Tuning (RMFT), a fine-tuning technique designed to reduce memorization of personally identifying information (PII) in LLMs.&lt;/li&gt;&lt;li&gt;Evaluates RMFT on the Enron Email Dataset, reporting ~80.8% reduction in Total Extraction Rate and ~80.2% reduction in Seen Extraction Rate versus baseline fine-tuning, with only a ~5.7% increase in perplexity.&lt;/li&gt;&lt;li&gt;Proposes MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and compares RMFT to deduplication using an AURC metric.&lt;/li&gt;&lt;li&gt;Focuses on a privacy-defense approach to mitigate data memorization, presenting empirical results and an evaluation methodology.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunj Joshi', 'David A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'memorization', 'data protection', 'defense', 'evaluation-framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03310</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VerifiableFL: Verifiable Claims for Federated Learning using Exclaves</title><link>https://arxiv.org/abs/2412.10537</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses verifiable claims in federated learning by producing fine-grained runtime attestation proofs to prove integrity of training steps.&lt;/li&gt;&lt;li&gt;Introduces 'exclaves'—integrity-only execution environments that avoid software-managed secrets and thus mitigate data-leakage/side-channel risks of TEEs.&lt;/li&gt;&lt;li&gt;Generates an attested dataflow graph of the FL training computation so an auditor can verify data sanitization, correct aggregation, and protocol adherence.&lt;/li&gt;&lt;li&gt;Implements VerifiableFL on NVFlare and reports under 12% runtime overhead versus unprotected FL training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinnan Guo', 'Kapil Vaswani', 'Andrew Paverd', 'Peter Pietzuch']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'runtime-attestation', 'trusted-execution-environments', 'defense', 'verifiability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10537</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Closing the Distribution Gap in Adversarial Training for LLMs</title><link>https://arxiv.org/abs/2602.15238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a distribution-coverage gap in current adversarial training for LLMs: training minimizes adversarial loss on the train set but fails to cover the broader data distribution, leaving models vulnerable to simple in-distribution attacks.&lt;/li&gt;&lt;li&gt;Proposes Distributional Adversarial Training (DAT), which uses diffusion-based LLMs to approximate the joint distribution of prompts and responses and generate diverse, high-likelihood samples to improve generalization of adversarial training.&lt;/li&gt;&lt;li&gt;Combines distributional sampling from the diffusion model with continuous adversarial optimization to train models that achieve substantially higher adversarial robustness compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengzhi Hu', 'Jonas Dornbusch', 'David L\\"udke', 'Stephan G\\"unnemann', 'Leo Schwinn']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'defenses', 'adversarial examples', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15238</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Boundary Point Jailbreaking of Black-Box LLMs</title><link>https://arxiv.org/abs/2602.15001</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Boundary Point Jailbreaking (BPJ), a fully black-box automated jailbreak that uses only binary feedback (flagged/unflagged) per query to optimize attacks against LLM safety classifiers.&lt;/li&gt;&lt;li&gt;BPJ converts a target harmful string into a curriculum of intermediate targets and actively selects 'boundary points' that reveal small improvements in attack strength, enabling optimization without classifier scores or gradients.&lt;/li&gt;&lt;li&gt;Demonstrates success against strong industry defenses including Constitutional Classifiers and GPT-5's input classifier without relying on human-provided jailbreak seeds; discusses defense implications such as the need for batch-level monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xander Davies', 'Giorgi Giglemiani', 'Edmund Lau', 'Eric Winsor', 'Geoffrey Irving', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'black-box attack', 'adversarial attack', 'model safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15001</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models</title><link>https://arxiv.org/abs/2602.00191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GEPC (Group-Equivariant Posterior Consistency), a training-free probe that detects out-of-distribution (OOD) inputs for diffusion models by measuring how consistently the learned score transforms under a finite group of symmetries.&lt;/li&gt;&lt;li&gt;Defines an ideal GEPC residual at the population level, derives in-distribution upper bounds and OOD lower bounds under mild assumptions, and produces interpretable equivariance-breaking maps.&lt;/li&gt;&lt;li&gt;Requires only score evaluations (no extra training), is computationally lightweight, and shows competitive or improved AUROC on image OOD benchmarks and strong separation on high-resolution synthetic aperture radar (SAR) imagery.&lt;/li&gt;&lt;li&gt;Code is released and the method focuses on exploiting equivariance-breaking as an OOD/robustness signal for diffusion-based generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yadang Alexis Rouzoumka', 'Jean Pinsolle', "Eug\\'enie Terreaux", 'Christ\\`ele Morisseau', 'Jean-Philippe Ovarlez', 'Chengfang Ren']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'diffusion models', 'robustness', 'equivariance', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00191</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reinforcement Unlearning via Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2601.20568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PURGE (Policy Unlearning through Relative Group Erasure), a reinforcement-learning–based unlearning method that frames forgetting as a verifiable objective using Group Relative Policy Optimization.&lt;/li&gt;&lt;li&gt;Uses an intrinsic reward that penalizes any mention of forbidden concepts, enabling targeted removal of memorized sensitive/copyrighted data without full retraining.&lt;/li&gt;&lt;li&gt;Reports substantial efficiency and utility gains (up to 46x lower token usage per target, +5.48% fluency, +12.02% adversarial robustness) and evaluation on the RWKU benchmark with 11% unlearning effectiveness while retaining 98% original utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Efstratios Zaradoukas', 'Bardh Prenkaj', 'Gjergji Kasneci']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'data removal', 'privacy', 'LLM safety', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20568</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation</title><link>https://arxiv.org/abs/2511.14406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines the impact of Low-Rank Adaptation (LoRA) on state-of-the-art backdoor attacks targeting federated model adaptation.&lt;/li&gt;&lt;li&gt;Focuses on backdoor lifespan/persistence after injection, finding that lower LoRA rank can increase backdoor persistence for optimally injected attacks.&lt;/li&gt;&lt;li&gt;Identifies evaluation shortcomings in existing backdoor assessments for FL and promotes more robust and fair evaluation protocols; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bastien Vuillod', 'Pierre-Alain Moellic', 'Jean-Max Dutertre']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'LoRA', 'model adaptation', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14406</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Navigating the Deep: End-to-End Extraction on Deep Neural Networks</title><link>https://arxiv.org/abs/2506.17047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end, polynomial-time model extraction (model-stealing) attack against ReLU-based deep neural networks by improving both signature extraction and sign extraction phases.&lt;/li&gt;&lt;li&gt;Identifies and resolves practical limitations in Carlini et al.'s signature-extraction method (rank deficiency, noise propagation from deeper layers) and introduces algorithmic and numerical-precision improvements enabling extraction of much deeper networks.&lt;/li&gt;&lt;li&gt;Combines two polynomial sign-extraction techniques to avoid exponential-time exhaustive search for low-confidence neurons, yielding the first fully polynomial end-to-end extraction pipeline.&lt;/li&gt;&lt;li&gt;Empirical validation on MNIST and CIFAR-10 shows substantial gains: consistently extracting at least eight layers versus prior works that could extract only about three layers of similar width.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haolin Liu', 'Adrien Siproudhis', 'Samuel Experton', 'Peter Lorenz', 'Christina Boura', 'Thomas Peyrin']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'model stealing', 'black-box attacks', 'adversarial machine learning', 'reverse engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17047</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents</title><link>https://arxiv.org/abs/2602.16346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STING, an automated multi-turn red-teaming framework that constructs stepwise illicit plans and iteratively probes tool-using LLM agents to measure how they enable harmful workflows.&lt;/li&gt;&lt;li&gt;Models multi-turn red-teaming as a time-to-first-jailbreak random variable and proposes analysis tools (discovery curves, hazard-ratio attribution) plus a new metric: Restricted Mean Jailbreak Discovery.&lt;/li&gt;&lt;li&gt;Shows STING produces substantially higher illicit-task completion than single-turn and chat-oriented baselines, and evaluates multilingual differences across six non-English settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nivya Talokar', 'Ayush K Tarun', 'Murari Mandal', 'Maksym Andriushchenko', 'Antoine Bosselut']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'agent security', 'evaluation framework', 'multilingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16346</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection</title><link>https://arxiv.org/abs/2602.16343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends ASVspoof 5 with resynthesized audio from neural audio codecs to study labeling choices for anti-spoofing datasets.&lt;/li&gt;&lt;li&gt;Examines the impact of labeling codec-resynthesized data as bonafide versus spoof on detector performance.&lt;/li&gt;&lt;li&gt;Analyzes the dual role of neural audio codecs (compression vs. synthesis) and provides recommendations/insights for labeling strategies in deepfake detection evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixuan Xiao', 'Florian Lux', "Alejandro P\\'erez-Gonz\\'alez-de-Martos", 'Ngoc Thang Vu']&lt;/li&gt;&lt;li&gt;Tags: ['audio-deepfake', 'spoof-detection', 'dataset', 'neural-audio-codec', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16343</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Collaborative Zone-Adaptive Zero-Day Intrusion Detection for IoBT</title><link>https://arxiv.org/abs/2602.16098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZAID, a collaborative, zone-adaptive intrusion detection framework for IoBT that targets detection of previously unseen (zero-day) attack families.&lt;/li&gt;&lt;li&gt;Combines a universal convolutional model for generalisable traffic representation, an autoencoder-based reconstruction anomaly score, and lightweight adapter modules for parameter-efficient zone-specific adaptation.&lt;/li&gt;&lt;li&gt;Uses federated aggregation and pseudo-labelling to enable cross-zone collaboration under intermittent connectivity and constrained bandwidth; evaluated on ToN_IoT and UNSW-NB15 with strong zero-day detection performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirmohammad Pasdar', 'Shabnam Kasra Kermanshahi', 'Nour Moustafa', 'Van-Thuan Pham']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'zero-day-detection', 'federated-learning', 'anomaly-detection', 'IoBT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16098</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Visual Memory Injection Attacks for Multi-Turn Conversations</title><link>https://arxiv.org/abs/2602.15927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Memory Injection (VMI): an attack where adversarially manipulated images uploaded to the web cause LVLMs to output attacker-prescribed messages when a trigger appears, while appearing benign under normal prompts.&lt;/li&gt;&lt;li&gt;Focuses on multi-turn, long-context conversations and demonstrates that the injected visual memory persists and activates after prolonged interactions.&lt;/li&gt;&lt;li&gt;Evaluates the attack on several open-weight large vision-language models and releases source code to reproduce the results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Schlarmann', 'Matthias Hein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'visual prompt injection', 'backdoor/memory poisoning', 'long-context robustness', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15927</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints</title><link>https://arxiv.org/abs/2602.15852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies temporal and lexical leakage in clinical note-based NLP models that can encode future decisions and inflate apparent predictive performance, posing deployment safety risks.&lt;/li&gt;&lt;li&gt;Introduces a lightweight auditing pipeline that uses interpretability to detect and suppress leakage-prone signals before final model training.&lt;/li&gt;&lt;li&gt;Case study on next-day discharge prediction after elective spine surgery shows audited models are more conservative, better calibrated, and rely less on discharge-related lexical cues.&lt;/li&gt;&lt;li&gt;Argues deployment-ready clinical NLP should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ha Na Cho', 'Sairam Sutari', 'Alexander Lopez', 'Hansen Bow', 'Kai Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['temporal leakage', 'data leakage', 'model auditing', 'safety/robustness', 'clinical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15852</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Protecting the Undeleted in Machine Unlearning</title><link>https://arxiv.org/abs/2602.16697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates a reconstruction attack that uses deletion requests to recover almost the entire undeleted dataset when mechanisms aim to emulate perfect retraining.&lt;/li&gt;&lt;li&gt;Surveys existing machine unlearning definitions, finding them either vulnerable to such attacks or too restrictive for basic functionalities (e.g., exact summation).&lt;/li&gt;&lt;li&gt;Proposes a new security definition that protects undeleted data from leakage due to deletions while supporting useful functionalities like bulletin boards, summations, and statistical learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aloni Cohen', 'Refael Kohen', 'Kobbi Nissim', 'Uri Stemmer']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'reconstruction attack', 'security definitions', 'data deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16697</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning</title><link>https://arxiv.org/abs/2602.16543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial attack framework for Safe RL that uses expert demonstrations and environment interaction to learn a constraint model and a surrogate policy via inverse constrained reinforcement learning.&lt;/li&gt;&lt;li&gt;Enables gradient-based attack optimization against victim Safe RL policies without access to their internal gradients or the ground-truth safety constraints; includes theoretical feasibility analysis and perturbation bounds.&lt;/li&gt;&lt;li&gt;Evaluates the approach on multiple Safe RL benchmarks, demonstrating effective attacks under limited privileged access.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialiang Fan', 'Shixiong Jiang', 'Mengyu Liu', 'Fanxin Kong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'safe reinforcement learning', 'inverse constrained reinforcement learning', 'black-box attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16543</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ModalImmune: Immunity Driven Unlearning via Self Destructive Training</title><link>https://arxiv.org/abs/2602.16197</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so joint representations are robust to destructive modality influence.&lt;/li&gt;&lt;li&gt;Key components: a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multimodal benchmarks shows improved resilience to modality removal and corruption while preserving convergence stability and reconstruction capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rong Fu', 'Jia Yee Tan', 'Wenxin Zhang', 'Zijian Zhang', 'Ziming Wang', 'Zhaolu Kang', 'Muge Qi', 'Shuning Zhang', 'Simon Fong']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'robust training / defense', 'self-destructive training', 'modality removal / corruption resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16197</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters</title><link>https://arxiv.org/abs/2602.16181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving federated learning framework for energy theft detection tailored to resource-constrained smart meters using a lightweight MLP.&lt;/li&gt;&lt;li&gt;Applies basic differential privacy by adding Gaussian noise to local model updates before aggregation to provide formal privacy guarantees.&lt;/li&gt;&lt;li&gt;Evaluates performance on a real-world smart meter dataset under both IID and non-IID distributions, reporting competitive accuracy, precision, recall, and AUC while maintaining efficiency.&lt;/li&gt;&lt;li&gt;Claims the approach is practical and scalable for secure, privacy-conscious deployment in smart grid environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diego Labate', 'Dipanwita Thakur', 'Giancarlo Fortino']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'privacy-preserving-ml', 'IoT/smart-meters', 'energy-theft-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16181</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Differentially Private Non-convex Distributionally Robust Optimization</title><link>https://arxiv.org/abs/2602.16155</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies differential privacy for finite-sum Distributionally Robust Optimization (DRO) with general ψ-divergences and non-convex losses, by reformulating DRO into suitable minimization/compositional forms.&lt;/li&gt;&lt;li&gt;Proposes DP Double-Spider, an (ε,δ)-DP optimization algorithm for DP-DRO with utility bound O(1/√n + (√(d log(1/δ))/(nε))^{2/3}) in gradient norm.&lt;/li&gt;&lt;li&gt;For KL-divergence DRO, develops DP Recursive-Spider achieving an improved utility rate O((√(d log(1/δ))/(nε))^{2/3}), matching best-known non-convex DP-ERM rates.&lt;/li&gt;&lt;li&gt;Provides empirical results showing the proposed methods outperform existing approaches for differentially private minimax optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Difei Xu', 'Meng Ding', 'Zebin Ma', 'Huanyi Xie', 'Youming Tao', 'Aicha Slaitane', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'distributionally robust optimization', 'privacy-preserving training', 'minimax/non-convex optimization', 'DP algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16155</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training</title><link>https://arxiv.org/abs/2602.16065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes recursive training where generative models are trained on mixtures of human and earlier-model-generated data (data contamination) in a general, minimal-assumption framework.&lt;/li&gt;&lt;li&gt;Proves convergence of contaminated recursive training and gives a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data per iteration.&lt;/li&gt;&lt;li&gt;Extends results to settings with sampling bias and supports theoretical findings with empirical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Wang', 'Hongqian Niu', 'Didong Li']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'recursive training', 'training-data poisoning', 'robustness', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16065</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models</title><link>https://arxiv.org/abs/2602.15689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a content-based framework for LLM refusal decisions in cybersecurity, characterizing requests along five technical dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users.&lt;/li&gt;&lt;li&gt;Argues refusal should explicitly model offense–defense trade-offs grounded in the technical substance of requests (not just stated intent or topic bans) to avoid inconsistent or over-restrictive behavior.&lt;/li&gt;&lt;li&gt;Shows the framework enables tunable, risk-aware refusal policies and more robust auditing against obfuscation and request segmentation, helping organizations balance defensive needs and misuse prevention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noa Linder', 'Meirav Segal', 'Omer Antverg', 'Gil Gekker', 'Tomer Fichman', 'Omri Bodenheimer', 'Edan Maor', 'Omer Nevo']&lt;/li&gt;&lt;li&gt;Tags: ['refusal policies', 'dual-use/misuse prevention', 'safety frameworks', 'guardrails', 'cybersecurity defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15689</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecCodeBench-V2 Technical Report</title><link>https://arxiv.org/abs/2602.15485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SecCodeBench-V2: a benchmark of 98 function-level code generation and patching scenarios covering 22 CWE categories across Java, C, Python, Go, and JavaScript.&lt;/li&gt;&lt;li&gt;Provides executable PoC test cases for functional and security verification, with dynamic execution of model-generated artifacts in isolated environments.&lt;/li&gt;&lt;li&gt;Includes an evaluation pipeline with Pass@K-based scoring, severity-aware aggregation, and LLM-as-judge fallback for non-deterministic adjudication.&lt;/li&gt;&lt;li&gt;Aims to rigorously evaluate LLM copilots' abilities to generate secure code and patch vulnerabilities; artifacts and results are publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longfei Chen', 'Ji Zhao', 'Lanxiao Cui', 'Tong Su', 'Xingbo Pan', 'Ziyang Li', 'Yongxing Wu', 'Qijiang Cao', 'Qiyao Cai', 'Jing Zhang', 'Yuandong Ni', 'Junyao He', 'Zeyu Zhang', 'Chao Ge', 'Xuhuai Lu', 'Zeyu Gao', 'Yuxin Cui', 'Weisen Chen', 'Yuxuan Peng', 'Shengping Wang', 'Qi Li', 'Yukai Huang', 'Yukun Liu', 'Tuo Zhou', 'Terry Yue Zhuo', 'Junyang Lin', 'Chao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['security benchmark', 'code generation', 'vulnerability evaluation', 'LLM safety', 'secure coding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15485</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Closing the Distribution Gap in Adversarial Training for LLMs</title><link>https://arxiv.org/abs/2602.15238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a distribution-coverage gap in current adversarial training for LLMs: training minimizes adversarial loss on the train set but fails to cover the broader data distribution, leaving models vulnerable to simple in-distribution attacks.&lt;/li&gt;&lt;li&gt;Proposes Distributional Adversarial Training (DAT), which uses diffusion-based LLMs to approximate the joint distribution of prompts and responses and generate diverse, high-likelihood samples to improve generalization of adversarial training.&lt;/li&gt;&lt;li&gt;Combines distributional sampling from the diffusion model with continuous adversarial optimization to train models that achieve substantially higher adversarial robustness compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengzhi Hu', 'Jonas Dornbusch', 'David L\\"udke', 'Stephan G\\"unnemann', 'Leo Schwinn']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'defenses', 'adversarial examples', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15238</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Weight space Detection of Backdoors in LoRA Adapters</title><link>https://arxiv.org/abs/2602.15195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-agnostic, weight-space method to detect backdoors in LoRA adapters by analyzing singular value concentration, entropy, and distribution shape of adapter weight matrices.&lt;/li&gt;&lt;li&gt;Evaluates on 500 LoRA adapters for Llama-3.2-3B (400 clean / 100 poisoned) across instruction and reasoning datasets (Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, GLUE).&lt;/li&gt;&lt;li&gt;Achieves ~97% detection accuracy with &lt;2% false positive rate, enabling screening of adapters without running models or requiring trigger examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Puertolas Merenciano', 'Ekaterina Vasyagina', 'Raghav Dixit', 'Kevin Zhu', 'Ruizhe Li', 'Javier Ferrando', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'LoRA adapters', 'weight-space analysis', 'model poisoning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15195</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?</title><link>https://arxiv.org/abs/2602.05023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLM-GEOPRIVACY, a benchmark testing vision-language models' ability to respect contextual privacy norms when deciding how much location information to disclose from images.&lt;/li&gt;&lt;li&gt;Evaluates 14 leading VLMs and finds they can precisely geolocate images yet often over-disclose sensitive location information and fail to align with human privacy expectations.&lt;/li&gt;&lt;li&gt;Demonstrates vulnerabilities to prompt-based attacks and argues for new multimodal design principles enabling context-conditioned privacy reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruixin Yang', 'Ethan Mendes', 'Arthur Wang', 'James Hays', 'Sauvik Das', 'Wei Xu', 'Alan Ritter']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'geolocation inference', 'prompt-based attacks', 'benchmarking', 'contextual integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05023</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</title><link>https://arxiv.org/abs/2501.16534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a technique to extract and approximate the safety classifier embedded within aligned LLMs by constructing surrogate classifiers from subsets of the model.&lt;/li&gt;&lt;li&gt;Evaluates surrogate agreement with the model's safety decisions (F1 &gt; 80% using as little as 20% of the architecture) in benign and adversarial settings.&lt;/li&gt;&lt;li&gt;Shows that attacks crafted on surrogates transfer effectively to the full LLM (e.g., a 50% Llama 2 surrogate yielded 70% ASR vs 22% when attacking the LLM directly), improving efficiency of jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jean-Charles Noirot Ferrand', 'Yohan Beugin', 'Eric Pauley', 'Ryan Sheatsley', 'Patrick McDaniel']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety-classifier-extraction', 'adversarial-transferability', 'surrogate-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.16534</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title><link>https://arxiv.org/abs/2501.03544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptGuard: a universal soft prompt optimized in the T2I textual embedding space to act as an implicit system prompt that moderates NSFW inputs without changing inference or using proxy models.&lt;/li&gt;&lt;li&gt;Introduces a divide-and-conquer approach by optimizing category-specific soft prompts and combining them into holistic safety guidance to improve reliability and helpfulness.&lt;/li&gt;&lt;li&gt;Reports experiments on five datasets showing PromptGuard is 3.8x faster than prior moderation methods, outperforms eight state-of-the-art defenses, and achieves an unsafe ratio as low as 5.84% while preserving benign output quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingzhi Yuan', 'Xinfeng Li', 'Chejian Xu', 'Guanhong Tao', 'Xiaojun Jia', 'Yihao Huang', 'Wei Dong', 'Yang Liu', 'Xiaofeng Wang', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-guardrails', 'soft-prompting', 'text-to-image', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03544</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI</title><link>https://arxiv.org/abs/2602.14135</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ForesightSafety Bench, a comprehensive AI safety evaluation framework covering 7 fundamental pillars extended to advanced domains (embodied AI, AI4Science, social/environmental risks, catastrophic/existential risks) and 94 refined risk dimensions with tens of thousands of structured risk datapoints.&lt;/li&gt;&lt;li&gt;Performs systematic evaluations of 20+ mainstream advanced large models using the benchmark, identifying widespread safety vulnerabilities and capability boundaries—especially in Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety, and Catastrophic/Existential Risks.&lt;/li&gt;&lt;li&gt;Releases the benchmark, dataset, and evaluation results publicly to support dynamic, hierarchical safety assessment and governance for frontier AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haibo Tong', 'Feifei Zhao', 'Linghao Feng', 'Ruoyu Wu', 'Ruolin Chen', 'Lu Jia', 'Zhou Zhao', 'Jindong Li', 'Tenglong Li', 'Erliang Lin', 'Shuai Yang', 'Enmeng Lu', 'Yinqian Sun', 'Qian Zhang', 'Zizhe Ruan', 'Jinyu Fan', 'Zeyang Yue', 'Ping Wu', 'Huangrui Li', 'Chengyi Sun', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['Safety Benchmark', 'Vulnerability Evaluation', 'Red-Teaming / Assessment', 'Frontier Risk', 'Agentic Autonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.14135</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Policy Compiler for Secure Agentic Systems</title><link>https://arxiv.org/abs/2602.16708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PCAS, a Policy Compiler that enforces authorization and information-flow policies for LLM-based agentic systems by modeling system state as a dependency graph (causal relationships among tool calls, results, messages).&lt;/li&gt;&lt;li&gt;Policies are specified in a Datalog-derived declarative language that reasons about transitive information flow and cross-agent provenance; a reference monitor intercepts and blocks violating actions before execution for deterministic enforcement.&lt;/li&gt;&lt;li&gt;PCAS compiles existing agent implementations with policy specs into instrumented, policy-compliant systems without requiring security-specific restructuring; evaluated on case studies including prompt-injection defense, approval workflows, and customer service, showing large compliance improvements (e.g., 48% → 93%) and zero violations in instrumented runs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Palumbo', 'Sarthak Choudhary', 'Jihye Choi', 'Prasad Chalasani', 'Mihai Christodorescu', 'Somesh Jha']&lt;/li&gt;&lt;li&gt;Tags: ['Policy enforcement', 'Information flow control', 'Prompt injection defense', 'Agentic systems', 'Reference monitor']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16708</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents</title><link>https://arxiv.org/abs/2602.16520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents RLM-JB, a procedural jailbreak-detection framework that uses a root model to orchestrate bounded analysis: normalize/de-obfuscate input, chunk text, query worker models, and aggregate evidence into an auditable decision.&lt;/li&gt;&lt;li&gt;Designed to counter long-context hiding, semantic camouflage, and split-payload/split-context attacks by guaranteeing coverage, parallel chunk screening, and cross-chunk signal composition.&lt;/li&gt;&lt;li&gt;Evaluated on AutoDAN-style adversarial jailbreaks across three LLM backends, reporting high recall/ASR (92.5–98.0%), very high precision (98.99–100%), and low false positive rates (0–2%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doron Shavit']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'prompt injection', 'adversarial prompts', 'defense framework', 'red teaming / auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16520</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models</title><link>https://arxiv.org/abs/2602.16309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of electromagnetic fault injection (EMFI) attacks on embedded deep learning models, comparing four numeric representations: FP32, FP16, INT8, and INT4.&lt;/li&gt;&lt;li&gt;Deployed ResNet-18/34/50 and VGG-11 on an embedded memory platform and used a low-cost EMFI setup to trigger faults and measure Top-1/Top-5 accuracy degradation.&lt;/li&gt;&lt;li&gt;Key finding: floating-point representations are highly vulnerable to single EMFI faults (near-complete accuracy loss), while integer/quantized representations—especially INT8 on larger networks like VGG-11—show substantially better resilience.&lt;/li&gt;&lt;li&gt;Implication: number representation/quantization choices materially affect hardware-level attack surface and can inform mitigation strategies for embedded deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Breier', "\\v{S}tefan Ku\\v{c}er\\'ak", 'Xiaolu Hou']&lt;/li&gt;&lt;li&gt;Tags: ['EMFI', 'fault-injection', 'hardware-security', 'model-robustness', 'quantization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16309</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Retrieval Collapses When AI Pollutes the Web</title><link>https://arxiv.org/abs/2602.16136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes 'Retrieval Collapse': web ecosystems where AI-generated content dominates search results and degrades retrieval diversity and integrity.&lt;/li&gt;&lt;li&gt;Performs controlled experiments showing SEO-style synthetic content can heavily contaminate retrieval exposure while preserving apparent answer accuracy, masking degradation.&lt;/li&gt;&lt;li&gt;Evaluates adversarially crafted content infiltration; shows baseline rankers (e.g., BM25) expose harmful content (~19%) while LLM-based rankers better suppress it.&lt;/li&gt;&lt;li&gt;Argues for retrieval-aware defenses to prevent a self-reinforcing cycle of quality decline in search and RAG systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyeon Yu', 'Dongchan Kim', 'Young-Bum Kim']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmentation', 'data-poisoning/web-pollution', 'adversarial-content', 'robustness/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16136</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes</title><link>https://arxiv.org/abs/2602.16109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedGraph-AGI: a federated graph neural network framework with Mixture-of-Experts aggregation and AGI-powered Large Action Models for causal reasoning to detect cross-border insider threats in financial transaction graphs.&lt;/li&gt;&lt;li&gt;Claims experimental results on a 50,000-transaction, 10-jurisdiction dataset: 92.3% accuracy, outperforming federated (86.1%) and centralized (84.7%) baselines; ablation shows AGI reasoning adds 6.8% and MoE adds 4.4%.&lt;/li&gt;&lt;li&gt;Emphasizes privacy-preserving design (federated learning + differential privacy epsilon=1.0), scalability to 50+ clients, and enabling cross-jurisdiction intelligence sharing while preserving data sovereignty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srikumar Nayak', 'James Walmesley']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'graph-neural-networks', 'insider-threat-detection', 'privacy-preserving', 'differential-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16109</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training</title><link>https://arxiv.org/abs/2602.16065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes recursive training where generative models are trained on mixtures of human and earlier-model-generated data (data contamination) in a general, minimal-assumption framework.&lt;/li&gt;&lt;li&gt;Proves convergence of contaminated recursive training and gives a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data per iteration.&lt;/li&gt;&lt;li&gt;Extends results to settings with sampling bias and supports theoretical findings with empirical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Wang', 'Hongqian Niu', 'Didong Li']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'recursive training', 'training-data poisoning', 'robustness', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16065</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Tool Orchestration to Code Execution: A Study of MCP Design Choices</title><link>https://arxiv.org/abs/2602.15945</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes distinction between context-coupled MCPs and Code Execution MCPs (CE-MCPs), and analyzes scalability trade-offs for large tool catalogs and distributed MCP servers.&lt;/li&gt;&lt;li&gt;Empirically evaluates CE-MCP vs traditional MCP across 10 servers with MCP-Bench, showing reduced token usage and latency but a substantially larger attack surface.&lt;/li&gt;&lt;li&gt;Applies the MAESTRO framework to enumerate sixteen attack classes across five execution phases (e.g., exception-mediated code injection, unsafe capability synthesis) and validates vulnerabilities via adversarial scenarios on multiple LLMs.&lt;/li&gt;&lt;li&gt;Proposes layered defenses—containerized sandboxing and semantic gating—alongside a roadmap for balancing scalability and security in executable agent workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Felendler', 'Parth A. Gandhi', 'Idan Habler', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['Code-execution attacks', 'Sandboxing &amp; isolation', 'Adversarial evaluation / red teaming', 'Agent/tool orchestration vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15945</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey</title><link>https://arxiv.org/abs/2602.15866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NLP-PRISM, a six-dimension framework (data collection, preprocessing, visibility, fairness, computational risk, regulatory compliance) to evaluate privacy risks in social-media NLP.&lt;/li&gt;&lt;li&gt;Survey of 203 papers across six NLP tasks; reports transformer F1 in 0.58–0.84 range and a 1%–23% F1 drop under privacy-preserving fine-tuning, plus utility–privacy trade-offs (2%–9%).&lt;/li&gt;&lt;li&gt;Evaluates privacy attacks empirically (MIA AUC ≈ 0.81, AIA accuracy ≈ 0.75) and highlights gaps in privacy coverage; recommends anonymization, privacy-aware learning, and fairness-driven training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhiman Goswami', 'Jai Kruthunz Naveen Kumar', 'Sanchari Das']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'membership-inference', 'attribute-inference', 'privacy-preserving-learning', 'social-media']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15866</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Lightweight Explainable Guardrail for Prompt Safety</title><link>https://arxiv.org/abs/2602.15853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LEG, a lightweight explainable guardrail that jointly classifies prompts as safe/unsafe and produces token-level explanations via a multi-task architecture.&lt;/li&gt;&lt;li&gt;Generates synthetic explanation-labeled training data using a novel strategy intended to counteract LLM confirmation biases.&lt;/li&gt;&lt;li&gt;Introduces a composite loss (cross-entropy + focal) with uncertainty-based weighting to capture global explanation signals during training.&lt;/li&gt;&lt;li&gt;Reports equal or better in-domain and out-of-domain performance on three datasets for both classification and explainability while using a much smaller model than prior approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Asiful Islam', 'Mihai Surdeanu']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'prompt safety', 'explainability', 'guardrails', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15853</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints</title><link>https://arxiv.org/abs/2602.15852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies temporal and lexical leakage in clinical note-based NLP models that can encode future decisions and inflate apparent predictive performance, posing deployment safety risks.&lt;/li&gt;&lt;li&gt;Introduces a lightweight auditing pipeline that uses interpretability to detect and suppress leakage-prone signals before final model training.&lt;/li&gt;&lt;li&gt;Case study on next-day discharge prediction after elective spine surgery shows audited models are more conservative, better calibrated, and rely less on discharge-related lexical cues.&lt;/li&gt;&lt;li&gt;Argues deployment-ready clinical NLP should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ha Na Cho', 'Sairam Sutari', 'Alexander Lopez', 'Hansen Bow', 'Kai Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['temporal leakage', 'data leakage', 'model auditing', 'safety/robustness', 'clinical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.15852</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Verifiable Semantics for Agent-to-Agent Communication</title><link>https://arxiv.org/abs/2602.16424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a certification protocol based on a stimulus-meaning model to verify that agents share the same semantics for terms by testing on shared observable events and checking empirical disagreement against a statistical threshold.&lt;/li&gt;&lt;li&gt;Introduces 'core-guarded reasoning' where agents restrict reasoning to certified terms, yielding provable bounds on inter-agent disagreement.&lt;/li&gt;&lt;li&gt;Describes mechanisms for detecting semantic drift (recertification) and recovering a shared vocabulary (renegotiation).&lt;/li&gt;&lt;li&gt;Reports simulation results showing 72–96% reduction in disagreement and validation with fine-tuned language models showing a 51% reduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philipp Schoenegger', 'Matt Carlson', 'Chris Schneider', 'Chris Daly']&lt;/li&gt;&lt;li&gt;Tags: ['semantic certification', 'multiagent robustness', 'communication verification', 'guardrails', 'drift detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16424</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection</title><link>https://arxiv.org/abs/2602.16037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes "optimization instability" in autonomous agentic workflows (using Pythia) where continued iterative self-optimization causes classifier performance to oscillate and degrade, especially for low-prevalence classes.&lt;/li&gt;&lt;li&gt;Empirical evaluation on three clinical symptoms (23%, 12%, 3% prevalence) shows validation sensitivity oscillating between 1.0 and 0.0; at 3% prevalence the system reported 95% accuracy while detecting zero positives, revealing a disguised failure mode.&lt;/li&gt;&lt;li&gt;Evaluates two interventions: a guiding (active) agent that amplified overfitting and a selector (retrospective) agent that successfully chose the best iteration and prevented catastrophic failure, yielding large F1 gains over expert lexicons for low-prevalence classes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cameron Cagan', 'Pedram Fard', 'Jiazi Tian', 'Jingya Cheng', 'Shawn N. Murphy', 'Hossein Estiri']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-agents', 'robustness', 'failure-modes', 'prompt-optimization', 'low-prevalence-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16037</guid><pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
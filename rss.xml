<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 27 Oct 2025 22:25:07 +0000</lastBuildDate><item><title>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</title><link>https://arxiv.org/abs/2510.13626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic vulnerability analysis of VLA models across seven perturbation dimensions&lt;/li&gt;&lt;li&gt;Reveals significant performance drops under controlled perturbations&lt;/li&gt;&lt;li&gt;Models show extreme sensitivity to camera viewpoints and robot initial states&lt;/li&gt;&lt;li&gt;Language instructions are often ignored, indicating potential alignment issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senyu Fei', 'Siyin Wang', 'Junhao Shi', 'Zihao Dai', 'Jikun Cai', 'Pengfang Qian', 'Li Ji', 'Xinzhe He', 'Shiduo Zhang', 'Zhaoye Fei', 'Jinlan Fu', 'Jingjing Gong', 'Xipeng Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'alignment', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13626</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WMCopier: Forging Invisible Image Watermarks on Arbitrary Images</title><link>https://arxiv.org/abs/2503.22330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WMCopier, an attack to forge invisible image watermarks&lt;/li&gt;&lt;li&gt;Uses a diffusion model to model watermark distribution&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against open and closed-source systems&lt;/li&gt;&lt;li&gt;Discusses potential defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziping Dong', 'Chao Shuai', 'Zhongjie Ba', 'Peng Cheng', 'Zhan Qin', 'Qinglong Wang', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'watermarking', 'forgery', 'diffusion models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.22330</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation</title><link>https://arxiv.org/abs/2510.21120</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafetyPairs framework for generating counterfactual image pairs to isolate safety-critical features&lt;/li&gt;&lt;li&gt;Creates a new safety benchmark with 3,020 image pairs across 9 safety categories&lt;/li&gt;&lt;li&gt;Improves sample efficiency for training guard models through data augmentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alec Helbling', 'Shruti Palaskar', 'Kundan Krishna', 'Polo Chau', 'Leon Gatys', 'Joseph Yitan Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'image safety', 'counterfactual generation', 'data augmentation', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21120</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</title><link>https://arxiv.org/abs/2510.13626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic vulnerability analysis of VLA models across seven perturbation dimensions&lt;/li&gt;&lt;li&gt;Reveals significant performance drops under controlled perturbations&lt;/li&gt;&lt;li&gt;Models show extreme sensitivity to camera viewpoints and robot initial states&lt;/li&gt;&lt;li&gt;Language instructions are often ignored, indicating potential alignment issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senyu Fei', 'Siyin Wang', 'Junhao Shi', 'Zihao Dai', 'Jikun Cai', 'Pengfang Qian', 'Li Ji', 'Xinzhe He', 'Shiduo Zhang', 'Zhaoye Fei', 'Jinlan Fu', 'Jingjing Gong', 'Xipeng Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'alignment', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13626</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</title><link>https://arxiv.org/abs/2509.23041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Virus Infection Attack (VIA) framework for propagating poisoning and backdoor attacks through synthetic data in LLM training.&lt;/li&gt;&lt;li&gt;Shows that VIA increases poisoning content in synthetic data and raises attack success rates on downstream models.&lt;/li&gt;&lt;li&gt;Highlights security risks of using synthetic data in LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zi Liang', 'Qingqing Ye', 'Xuan Liu', 'Yanyun Wang', 'Jianliang Xu', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'synthetic data', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23041</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations</title><link>https://arxiv.org/abs/2505.13763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a neurofeedback paradigm to measure LLM metacognition&lt;/li&gt;&lt;li&gt;Explores how LLMs can monitor and control internal activations&lt;/li&gt;&lt;li&gt;Highlights safety concerns related to evading oversight&lt;/li&gt;&lt;li&gt;Identifies factors affecting metacognitive ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Ji-An', 'Hua-Dong Xiong', 'Robert C. Wilson', 'Marcelo G. Mattar', 'Marcus K. Benna']&lt;/li&gt;&lt;li&gt;Tags: ['metacognition', 'safety', 'neurofeedback', 'activation monitoring', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13763</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models</title><link>https://arxiv.org/abs/2501.01741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EvoTox is an automated testing framework for LLM toxicity&lt;/li&gt;&lt;li&gt;Uses an iterative evolution strategy with two LLMs: SUT and Prompt Generator&lt;/li&gt;&lt;li&gt;Assesses toxicity with a classifier and human evaluation&lt;/li&gt;&lt;li&gt;Outperforms baselines in detecting toxic responses with manageable cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simone Corbo', 'Luca Bancale', 'Valeria De Gennaro', 'Livia Lestingi', 'Vincenzo Scotti', 'Matteo Camilli']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'toxicity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01741</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Robust Preference Alignment via Directional Neighborhood Consensus</title><link>https://arxiv.org/abs/2510.20498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Robust Preference Selection (RPS) to enhance preference alignment in LLMs&lt;/li&gt;&lt;li&gt;Uses directional neighborhood consensus to sample responses from related preferences&lt;/li&gt;&lt;li&gt;Improves robustness without retraining&lt;/li&gt;&lt;li&gt;Demonstrates gains across DPA, DPO, and SFT paradigms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruochen Mao', 'Yuling Shi', 'Xiaodong Gu', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'preference learning', 'post-hoc methods', 'theoretical framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20498</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title><link>https://arxiv.org/abs/2506.01420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEAL, a distillation framework for training SLMs to anonymize text without relying on external models&lt;/li&gt;&lt;li&gt;Uses adversarial interactions between LLM anonymizer and inference model to collect training data&lt;/li&gt;&lt;li&gt;SLMs learn to anonymize and evaluate outputs, enabling self-refinement&lt;/li&gt;&lt;li&gt;Experiments show 8B models can match or exceed GPT-4's privacy-utility trade-off&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyuyoung Kim', 'Hyunjun Jeon', 'Jinwoo Shin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data poisoning', 'model extraction', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01420</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inference-time Alignment in Continuous Space</title><link>https://arxiv.org/abs/2505.20081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEA, an algorithm for inference-time alignment using gradient-based sampling in continuous latent space&lt;/li&gt;&lt;li&gt;Aims to improve alignment over discrete search methods&lt;/li&gt;&lt;li&gt;Shows significant performance gains on AdvBench and MATH&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Yuan', 'Teng Xiao', 'Li Yunfan', 'Bingbing Xu', 'Shuchang Tao', 'Yunqi Qiu', 'Huawei Shen', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'gradient-based', 'latent space', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20081</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reverse Engineering Human Preferences with Reinforcement Learning</title><link>https://arxiv.org/abs/2505.15795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores adversarial tuning of LLMs using reinforcement learning to generate preambles that boost evaluation scores.&lt;/li&gt;&lt;li&gt;It demonstrates that this method can be undetectable and transfer across different models.&lt;/li&gt;&lt;li&gt;Raises concerns about the reliability of LLM-as-a-judge evaluation frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lisa Alazraki', 'Tan Yi-Chern', 'Jon Ander Campos', 'Maximilian Mozes', 'Marek Rei', 'Max Bartolo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15795</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alert-ME: An Explainability-Driven Defense Against Adversarial Examples in Transformer-Based Text Classification</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EDIT framework for detecting and defending against adversarial examples in transformer-based text classifiers&lt;/li&gt;&lt;li&gt;Uses explainability tools like attention maps and integrated gradients combined with frequency features&lt;/li&gt;&lt;li&gt;Includes alerting mechanisms for human intervention when needed&lt;/li&gt;&lt;li&gt;Shows improved performance over state-of-the-art defenses with faster feature extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Bushra Sabir (CSIRO's Data61)", 'Yansong Gao (The University of Western Australia)', "Alsharif Abuadbba (CSIRO's Data61)", 'M. Ali Babar (The University of Adelaide', 'CREST- The Centre for Research on Engineering Software Technologies)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text classification', 'explainability', 'defense mechanisms', 'transformer models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots</title><link>https://arxiv.org/abs/2510.21459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SBASH framework for LLM-based honeypots&lt;/li&gt;&lt;li&gt;Evaluates RAG vs prompt-tuned LLMs for Linux shell command responses&lt;/li&gt;&lt;li&gt;Compares metrics like response time, realism, and accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adetayo Adebimpe', 'Helmut Neukirchen', 'Thomas Welsh']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'honeypot', 'RAG', 'prompt tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21459</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Leverage Unlearning to Sanitize LLMs</title><link>https://arxiv.org/abs/2510.21322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SANI, an unlearning approach to sanitize LLMs by removing memorized sensitive information&lt;/li&gt;&lt;li&gt;Involves erasure (resetting neurons) and repair (fine-tuning without sensitive data)&lt;/li&gt;&lt;li&gt;Evaluated on medical data and standard pre-trained models&lt;/li&gt;&lt;li&gt;Reduces regurgitation of sensitive information with minimal additional training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antoine Boutet', 'Lucas Magnana']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21322</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails</title><link>https://arxiv.org/abs/2510.21285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Self-Jailbreak in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Guardrail (CoG) training framework to mitigate safety issues&lt;/li&gt;&lt;li&gt;Aims to preserve reasoning ability while improving safety&lt;/li&gt;&lt;li&gt;Demonstrates improvements across safety and reasoning benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingzhi Mao (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Chunkang Zhang (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Junxiang Wang (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Xinyan Guan (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Boxi Cao (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Yaojie Lu (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Hongyu Lin (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Xianpei Han (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Le Sun (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety', 'alignment', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21285</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference</title><link>https://arxiv.org/abs/2510.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RePULSe, a training method for LMs to reduce undesired outputs using probabilistic inference&lt;/li&gt;&lt;li&gt;Augments standard RL loss with a loss that targets low-reward outputs&lt;/li&gt;&lt;li&gt;Aims to improve tradeoff between average reward and undesired output probability&lt;/li&gt;&lt;li&gt;Demonstrates better adversarial robustness compared to standard methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Zhao', 'Aidan Li', 'Rob Brekelmans', 'Roger Grosse']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement learning', 'adversarial robustness', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21184</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training</title><link>https://arxiv.org/abs/2510.20956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'self-jailbreaking' where RLMs circumvent safety guardrails after benign training&lt;/li&gt;&lt;li&gt;Models use benign assumptions to justify harmful requests&lt;/li&gt;&lt;li&gt;Observed in multiple RLMs like DeepSeek-R1, Phi-4, Nemotron&lt;/li&gt;&lt;li&gt;Mitigation involves adding minimal safety reasoning data during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng-Xin Yong', 'Stephen H. Bach']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20956</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title><link>https://arxiv.org/abs/2506.01420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEAL, a distillation framework for training SLMs to anonymize text without relying on external models&lt;/li&gt;&lt;li&gt;Uses adversarial interactions between LLM anonymizer and inference model to collect training data&lt;/li&gt;&lt;li&gt;SLMs learn to anonymize and evaluate outputs, enabling self-refinement&lt;/li&gt;&lt;li&gt;Experiments show 8B models can match or exceed GPT-4's privacy-utility trade-off&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyuyoung Kim', 'Hyunjun Jeon', 'Jinwoo Shin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data poisoning', 'model extraction', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01420</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alert-ME: An Explainability-Driven Defense Against Adversarial Examples in Transformer-Based Text Classification</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EDIT framework for detecting and defending against adversarial examples in transformer-based text classifiers&lt;/li&gt;&lt;li&gt;Uses explainability tools like attention maps and integrated gradients combined with frequency features&lt;/li&gt;&lt;li&gt;Includes alerting mechanisms for human intervention when needed&lt;/li&gt;&lt;li&gt;Shows improved performance over state-of-the-art defenses with faster feature extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Bushra Sabir (CSIRO's Data61)", 'Yansong Gao (The University of Western Australia)', "Alsharif Abuadbba (CSIRO's Data61)", 'M. Ali Babar (The University of Adelaide', 'CREST- The Centre for Research on Engineering Software Technologies)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text classification', 'explainability', 'defense mechanisms', 'transformer models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors</title><link>https://arxiv.org/abs/2509.15551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PolyJuice is a black-box, universal red-teaming method for synthetic image detectors (SIDs)&lt;/li&gt;&lt;li&gt;It identifies a distribution shift in the T2I latent space between correctly and incorrectly classified images&lt;/li&gt;&lt;li&gt;Generates attacks by steering images towards SID failure modes&lt;/li&gt;&lt;li&gt;Improves SID performance when used for data augmentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepehr Dehdashtian', 'Mashrur M. Morshed', 'Jacob H. Seidman', 'Gaurav Bharaj', 'Vishnu Naresh Boddeti']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'image synthesis', 'text-to-image models', 'detector robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15551</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Distillation Robustifies Unlearning</title><link>https://arxiv.org/abs/2506.06278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UNDO method for robust LLM unlearning using distillation&lt;/li&gt;&lt;li&gt;Shows distillation can transfer behaviors while leaving latent capabilities&lt;/li&gt;&lt;li&gt;Demonstrates robustness on synthetic and WMDP benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bruce W. Lee', 'Addie Foote', 'Alex Infanger', 'Leni Shor', 'Harish Kamath', 'Jacob Goldman-Wetzler', 'Bryce Woodworth', 'Alex Cloud', 'Alexander Matt Turner']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'distillation', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06278</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Valuation of Human Feedback through Provably Robust Model Alignment</title><link>https://arxiv.org/abs/2505.17859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H"older-DPO, a new alignment loss with provable redescending property to handle noisy human feedback&lt;/li&gt;&lt;li&gt;Aims to improve robustness of model alignment by estimating clean data distribution from noisy labels&lt;/li&gt;&lt;li&gt;Introduces a gradient-free metric for dataset valuation to identify mislabeled data&lt;/li&gt;&lt;li&gt;Demonstrates improved alignment performance on controlled and real-world datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Fujisawa', 'Masaki Adachi', 'Michael A. Osborne']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17859</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Shape it Up! Restoring LLM Safety during Finetuning</title><link>https://arxiv.org/abs/2505.17196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes dynamic safety shaping (DSS) to address safety risks during LLM finetuning&lt;/li&gt;&lt;li&gt;Introduces STAR for token-level safety assessment&lt;/li&gt;&lt;li&gt;Uses guardrail models to evaluate partial responses dynamically&lt;/li&gt;&lt;li&gt;Demonstrates safety improvements across multiple datasets and models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['ShengYun Peng', 'Pin-Yu Chen', 'Jianfeng Chi', 'Seongmin Lee', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'finetuning', 'dynamic safety shaping', 'STAR', 'guardrail models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17196</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs</title><link>https://arxiv.org/abs/2502.14828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates fundamental limitations in pointwise defenses of LLM fine-tuning APIs.&lt;/li&gt;&lt;li&gt;It constructs attacks using benign samples that transmit dangerous knowledge covertly.&lt;/li&gt;&lt;li&gt;The attacks evade detection by existing and enhanced monitoring systems.&lt;/li&gt;&lt;li&gt;The authors call for new defense strategies beyond pointwise detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xander Davies', 'Eric Winsor', 'Alexandra Souly', 'Tomek Korbak', 'Robert Kirk', 'Christian Schroeder de Witt', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14828</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots</title><link>https://arxiv.org/abs/2510.21459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SBASH framework for LLM-based honeypots&lt;/li&gt;&lt;li&gt;Evaluates RAG vs prompt-tuned LLMs for Linux shell command responses&lt;/li&gt;&lt;li&gt;Compares metrics like response time, realism, and accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adetayo Adebimpe', 'Helmut Neukirchen', 'Thomas Welsh']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'honeypot', 'RAG', 'prompt tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21459</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Soft Instruction De-escalation Defense</title><link>https://arxiv.org/abs/2510.21057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SIC (Soft Instruction Control) for defending against prompt injections in LLM agents&lt;/li&gt;&lt;li&gt;Iterative prompt sanitization loop that rewrites, masks, or removes malicious content&lt;/li&gt;&lt;li&gt;Continues until input is clean or halts if imperative instructions remain&lt;/li&gt;&lt;li&gt;Raises the bar against attacks but not infallible (15% ASR possible)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Philipp Walter', 'Chawin Sitawarin', 'Jamie Hayes', 'David Stutz', 'Ilia Shumailov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21057</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FrameShield: Adversarially Robust Video Anomaly Detection</title><link>https://arxiv.org/abs/2510.21532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FrameShield for adversarially robust video anomaly detection&lt;/li&gt;&lt;li&gt;Uses Spatiotemporal Region Distortion (SRD) to generate synthetic anomalies&lt;/li&gt;&lt;li&gt;Combines SRD with pseudo-labels for effective adversarial training&lt;/li&gt;&lt;li&gt;Significant performance improvement against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mojtaba Nafez', 'Mobina Poulaei', 'Nikan Vasei', 'Bardia Soltani Moakhar', 'Mohammad Sabokrou', 'MohammadHossein Rohban']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'video anomaly detection', 'weakly supervised learning', 'adversarial training', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21532</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Probe-based Fine-tuning for Reducing Toxicity</title><link>https://arxiv.org/abs/2510.21531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores using probes trained on model activations to detect and reduce toxicity.&lt;/li&gt;&lt;li&gt;It proposes two training methods: Supervised Fine-tuning and Direct Preference Optimization.&lt;/li&gt;&lt;li&gt;Key findings include that probe-based preference optimization preserves probe accuracy better than classifier-based methods.&lt;/li&gt;&lt;li&gt;Retraining probes after optimization recovers detection accuracy, making probe ensembles less necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jan Wehner', 'Mario Fritz']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21531</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Leverage Unlearning to Sanitize LLMs</title><link>https://arxiv.org/abs/2510.21322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SANI, an unlearning approach to sanitize LLMs by removing memorized sensitive information&lt;/li&gt;&lt;li&gt;Involves erasure (resetting neurons) and repair (fine-tuning without sensitive data)&lt;/li&gt;&lt;li&gt;Evaluated on medical data and standard pre-trained models&lt;/li&gt;&lt;li&gt;Reduces regurgitation of sensitive information with minimal additional training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antoine Boutet', 'Lucas Magnana']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21322</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference</title><link>https://arxiv.org/abs/2510.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RePULSe, a training method for LMs to reduce undesired outputs using probabilistic inference&lt;/li&gt;&lt;li&gt;Augments standard RL loss with a loss that targets low-reward outputs&lt;/li&gt;&lt;li&gt;Aims to improve tradeoff between average reward and undesired output probability&lt;/li&gt;&lt;li&gt;Demonstrates better adversarial robustness compared to standard methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Zhao', 'Aidan Li', 'Rob Brekelmans', 'Roger Grosse']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement learning', 'adversarial robustness', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21184</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Target Attack</title><link>https://arxiv.org/abs/2510.02422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dynamic Target Attack (DTA) for jailbreaking LLMs&lt;/li&gt;&lt;li&gt;Uses dynamic targets from the model's own responses&lt;/li&gt;&lt;li&gt;Reduces discrepancy between target and output distribution&lt;/li&gt;&lt;li&gt;Improves attack success rate and efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kedong Xiu', 'Churui Zeng', 'Tianhang Zheng', 'Xinzhe Huang', 'Xiaojun Jia', 'Di Wang', 'Puning Zhao', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'LLM security', 'gradient-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02422</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</title><link>https://arxiv.org/abs/2509.23041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Virus Infection Attack (VIA) framework for propagating poisoning and backdoor attacks through synthetic data in LLM training.&lt;/li&gt;&lt;li&gt;Shows that VIA increases poisoning content in synthetic data and raises attack success rates on downstream models.&lt;/li&gt;&lt;li&gt;Highlights security risks of using synthetic data in LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zi Liang', 'Qingqing Ye', 'Xuan Liu', 'Yanyun Wang', 'Jianliang Xu', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'synthetic data', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23041</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents</title><link>https://arxiv.org/abs/2506.12104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRIFT, a dynamic rule-based defense framework for LLM agents&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks through secure planning, dynamic validation, and injection isolation&lt;/li&gt;&lt;li&gt;Validated on AgentDojo and ASB benchmarks with strong security and utility results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Xiaogeng Liu', 'Hung-Chun Chiu', 'Dianqi Li', 'Ning Zhang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12104</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TAI3: Testing Agent Integrity in Interpreting User Intent</title><link>https://arxiv.org/abs/2506.07524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TAI3, an API-centric stress testing framework for LLM agents&lt;/li&gt;&lt;li&gt;Focuses on detecting intent integrity violations through realistic task generation and mutations&lt;/li&gt;&lt;li&gt;Uses semantic partitioning and datatype-aware strategy memory for efficient testing&lt;/li&gt;&lt;li&gt;Outperforms baselines in error detection and query efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiwei Feng', 'Xiangzhe Xu', 'Xuan Chen', 'Kaiyuan Zhang', 'Syed Yusuf Ahmed', 'Zian Su', 'Mingwei Zheng', 'Xiangyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'API testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07524</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Distillation Robustifies Unlearning</title><link>https://arxiv.org/abs/2506.06278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UNDO method for robust LLM unlearning using distillation&lt;/li&gt;&lt;li&gt;Shows distillation can transfer behaviors while leaving latent capabilities&lt;/li&gt;&lt;li&gt;Demonstrates robustness on synthetic and WMDP benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bruce W. Lee', 'Addie Foote', 'Alex Infanger', 'Leni Shor', 'Harish Kamath', 'Jacob Goldman-Wetzler', 'Bryce Woodworth', 'Alex Cloud', 'Alexander Matt Turner']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'distillation', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06278</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inference-time Alignment in Continuous Space</title><link>https://arxiv.org/abs/2505.20081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEA, an algorithm for inference-time alignment using gradient-based sampling in continuous latent space&lt;/li&gt;&lt;li&gt;Aims to improve alignment over discrete search methods&lt;/li&gt;&lt;li&gt;Shows significant performance gains on AdvBench and MATH&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Yuan', 'Teng Xiao', 'Li Yunfan', 'Bingbing Xu', 'Shuchang Tao', 'Yunqi Qiu', 'Huawei Shen', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'gradient-based', 'latent space', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20081</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Valuation of Human Feedback through Provably Robust Model Alignment</title><link>https://arxiv.org/abs/2505.17859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H"older-DPO, a new alignment loss with provable redescending property to handle noisy human feedback&lt;/li&gt;&lt;li&gt;Aims to improve robustness of model alignment by estimating clean data distribution from noisy labels&lt;/li&gt;&lt;li&gt;Introduces a gradient-free metric for dataset valuation to identify mislabeled data&lt;/li&gt;&lt;li&gt;Demonstrates improved alignment performance on controlled and real-world datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Fujisawa', 'Masaki Adachi', 'Michael A. Osborne']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17859</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models</title><link>https://arxiv.org/abs/2501.01741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EvoTox is an automated testing framework for LLM toxicity&lt;/li&gt;&lt;li&gt;Uses an iterative evolution strategy with two LLMs: SUT and Prompt Generator&lt;/li&gt;&lt;li&gt;Assesses toxicity with a classifier and human evaluation&lt;/li&gt;&lt;li&gt;Outperforms baselines in detecting toxic responses with manageable cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simone Corbo', 'Luca Bancale', 'Valeria De Gennaro', 'Livia Lestingi', 'Vincenzo Scotti', 'Matteo Camilli']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'toxicity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01741</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alert-ME: An Explainability-Driven Defense Against Adversarial Examples in Transformer-Based Text Classification</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EDIT framework for detecting and defending against adversarial examples in transformer-based text classifiers&lt;/li&gt;&lt;li&gt;Uses explainability tools like attention maps and integrated gradients combined with frequency features&lt;/li&gt;&lt;li&gt;Includes alerting mechanisms for human intervention when needed&lt;/li&gt;&lt;li&gt;Shows improved performance over state-of-the-art defenses with faster feature extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Bushra Sabir (CSIRO's Data61)", 'Yansong Gao (The University of Western Australia)', "Alsharif Abuadbba (CSIRO's Data61)", 'M. Ali Babar (The University of Adelaide', 'CREST- The Centre for Research on Engineering Software Technologies)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text classification', 'explainability', 'defense mechanisms', 'transformer models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Extracting alignment data in open models</title><link>https://arxiv.org/abs/2510.18554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates extracting alignment training data from post-trained models using embedding models for semantic similarity.&lt;/li&gt;&lt;li&gt;It highlights the risk of data extraction and discusses distillation practices.&lt;/li&gt;&lt;li&gt;The work shows that models regurgitate post-training data like SFT or RL, which can be used to train base models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Federico Barbero', 'Xiangming Gu', 'Christopher A. Choquette-Choo', 'Chawin Sitawarin', 'Matthew Jagielski', 'Itay Yona', "Petar Veli\\v{c}kovi\\'c", 'Ilia Shumailov', 'Jamie Hayes']&lt;/li&gt;&lt;li&gt;Tags: ['data extraction', 'alignment', 'safety', 'model training', 'distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18554</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations</title><link>https://arxiv.org/abs/2505.13763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a neurofeedback paradigm to measure LLM metacognition&lt;/li&gt;&lt;li&gt;Explores how LLMs can monitor and control internal activations&lt;/li&gt;&lt;li&gt;Highlights safety concerns related to evading oversight&lt;/li&gt;&lt;li&gt;Identifies factors affecting metacognitive ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Ji-An', 'Hua-Dong Xiong', 'Robert C. Wilson', 'Marcelo G. Mattar', 'Marcus K. Benna']&lt;/li&gt;&lt;li&gt;Tags: ['metacognition', 'safety', 'neurofeedback', 'activation monitoring', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13763</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Agent Execution</title><link>https://arxiv.org/abs/2510.21236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentBound, an access control framework for MCP servers&lt;/li&gt;&lt;li&gt;Combines declarative policies and enforcement without server modifications&lt;/li&gt;&lt;li&gt;Automatically generates policies with 80.9% accuracy&lt;/li&gt;&lt;li&gt;Blocks majority of security threats with negligible overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christoph B\\"uhler', 'Matteo Biagiola', 'Luca Di Grazia', 'Guido Salvaneschi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security', 'access control', 'MCP servers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21236</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference</title><link>https://arxiv.org/abs/2510.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RePULSe, a training method for LMs to reduce undesired outputs using probabilistic inference&lt;/li&gt;&lt;li&gt;Augments standard RL loss with a loss that targets low-reward outputs&lt;/li&gt;&lt;li&gt;Aims to improve tradeoff between average reward and undesired output probability&lt;/li&gt;&lt;li&gt;Demonstrates better adversarial robustness compared to standard methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Zhao', 'Aidan Li', 'Rob Brekelmans', 'Roger Grosse']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement learning', 'adversarial robustness', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21184</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying CBRN Risk in Frontier Models</title><link>https://arxiv.org/abs/2510.21133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates 10 leading LLMs against CBRN risks using a 200-prompt dataset and FORTRESS subset&lt;/li&gt;&lt;li&gt;Demonstrates Deep Inception attacks achieve 86% success vs 33.8% direct requests&lt;/li&gt;&lt;li&gt;Finds model safety performance varies widely (2% to 96% success rates)&lt;/li&gt;&lt;li&gt;Highlights brittleness in safety alignment and need for better evaluation frameworks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divyanshu Kumar', 'Nitin Aravind Birur', 'Tanay Baswa', 'Sahil Agarwal', 'Prashanth Harshangi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'CBRN risks', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21133</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law</title><link>https://arxiv.org/abs/2510.21524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EU-Agent-Bench, a benchmark for evaluating LLM agents' compliance with EU legal norms&lt;/li&gt;&lt;li&gt;Covers data protection, bias/discrimination, and scientific integrity scenarios&lt;/li&gt;&lt;li&gt;Compares model function calls against EU legislation&lt;/li&gt;&lt;li&gt;Tests compliance with and without legislative excerpts in system prompt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ilija Lichkovski', 'Alexander M\\"uller', 'Mariam Ibrahim', 'Tiwai Mhundwa']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'data protection', 'bias/discrimination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21524</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails</title><link>https://arxiv.org/abs/2510.21285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Self-Jailbreak in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Guardrail (CoG) training framework to mitigate safety issues&lt;/li&gt;&lt;li&gt;Aims to preserve reasoning ability while improving safety&lt;/li&gt;&lt;li&gt;Demonstrates improvements across safety and reasoning benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingzhi Mao (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Chunkang Zhang (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Junxiang Wang (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Xinyan Guan (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Boxi Cao (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Yaojie Lu (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Hongyu Lin (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences)', 'Xianpei Han (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)', 'Le Sun (Chinese Information Processing Laboratory', 'Institute of Software', 'Chinese Academy of Sciences', 'University of Chinese Academy of Sciences)']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety', 'alignment', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21285</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge</title><link>https://arxiv.org/abs/2510.21144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NeuroGenPoisoning, a novel attack framework for RAG models using neuron attribution and genetic optimization&lt;/li&gt;&lt;li&gt;Identifies Poison-Responsive Neurons to guide adversarial knowledge generation&lt;/li&gt;&lt;li&gt;Uses genetic algorithm to evolve effective poisoned passages&lt;/li&gt;&lt;li&gt;Achieves high POSR (&gt;90%) while preserving fluency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanyu Zhu', 'Lance Fiondella', 'Jiawei Yuan', 'Kai Zeng', 'Long Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'model extraction', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21144</guid><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>
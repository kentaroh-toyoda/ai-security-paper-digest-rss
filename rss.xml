<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 23 Jan 2026 22:58:06 +0000</lastBuildDate><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box jailbreak attack for large vision-language models (LVLMs) using zeroth-order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;Claims advantages: gradient-free input-output based optimization, model-agnostic (no surrogate model), and lower resource/GPU memory needs versus white-box approaches.&lt;/li&gt;&lt;li&gt;Evaluates on InstructBLIP, LLaVA, and MiniGPT-4, reporting high jailbreak success rates (e.g., 83.0% on InstructBLIP) and strong transferability (e.g., 64.18% ASR from MiniGPT-4 to other LVLMs) while keeping perturbations imperceptible.&lt;/li&gt;&lt;li&gt;Highlights practical feasibility of black-box jailbreaks and exposes weaknesses in current LVLM safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'jailbreaking', 'black-box attack', 'zeroth-order optimization', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</title><link>https://arxiv.org/abs/2509.14957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DF-LLaVA, a framework that extracts latent knowledge from MLLMs and injects it into training via prompt-guided knowledge injection to improve synthetic image detection.&lt;/li&gt;&lt;li&gt;Aims to combine high authenticity-classification accuracy (exceeding expert models) with interpretable outputs typical of MLLMs.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing improved detection accuracy and explainability for synthetic/forgery image detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuokang Shen', 'Kaisen Zhang', 'Bohan Jia', 'Heming Jia', 'Yuan Fang', 'Zhou Yu', 'Shaohui Lin']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-image-detection', 'deepfake-detection', 'multimodal-large-language-models', 'defense', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14957</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing</title><link>https://arxiv.org/abs/2601.16200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Feature-space Smoothing (FS), a method that provides certified robustness guarantees on feature representations of multimodal LLMs by certifying a lower bound on feature cosine similarity under l2-bounded attacks.&lt;/li&gt;&lt;li&gt;Introduces Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of existing encoders (no MLLM retraining required) to strengthen the FS certificate.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis (Feature Cosine Similarity Bound) and extensive empirical results showing large reductions in attack success rate (white-box attacks reduced from ~90% to ~1%) and better performance than adversarial training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song Xia', 'Meiwen Ding', 'Chenqi Kong', 'Wenhan Yang', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial defense', 'multimodal LLMs', 'feature-space smoothing', 'provable guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16200</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Watermark in the Latent Space of Generative Models</title><link>https://arxiv.org/abs/2601.16140</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DistSeal, a method for post-hoc watermarking in the latent space of generative models (diffusion and autoregressive).&lt;/li&gt;&lt;li&gt;Shows latent watermarkers can be distilled into the generative model or latent decoder for in-model watermarking, improving efficiency (up to 20x) and imperceptibility versus pixel-space methods.&lt;/li&gt;&lt;li&gt;Demonstrates competitive robustness of latent watermarks and that distilling latent watermarkers outperforms distilling pixel-space watermarkers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sylvestre-Alvise Rebuffi', 'Tuan Tran', 'Valeriu Lacatusu', 'Pierre Fernandez', "Tom\\'a\\v{s} Sou\\v{c}ek", "Nikola Jovanovi\\'c", 'Tom Sander', 'Hady Elsahar', 'Alexandre Mourachko']&lt;/li&gt;&lt;li&gt;Tags: ['Watermarking', 'Model provenance', 'Defenses', 'Generative models', 'Latent-space techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16140</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Out-of-Distribution Detection Based on Total Variation Estimation</title><link>https://arxiv.org/abs/2601.15867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TV-OOD, an out-of-distribution detection method that uses a Total Variation Network Estimator to compute each input's contribution to total variation.&lt;/li&gt;&lt;li&gt;Defines a total variation score per input to discriminate in-distribution versus out-of-distribution samples.&lt;/li&gt;&lt;li&gt;Evaluated across multiple models and datasets on image classification tasks, reporting performance comparable or superior to state-of-the-art OOD methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dabiao Ma', 'Zhiba Su', 'Jian Yang', 'Haojun Fei']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness/defense', 'anomaly detection', 'total variation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15867</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs</title><link>https://arxiv.org/abs/2601.15698</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BVS, a novel jailbreaking framework for probing visual safety in multimodal large language models using a 'reconstruction-then-generation' approach.&lt;/li&gt;&lt;li&gt;Introduces neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, enabling models to be induced into generating harmful images.&lt;/li&gt;&lt;li&gt;Reports high empirical effectiveness—98.21% jailbreak success against GPT-5 (12 Jan 2026 release)—demonstrating critical visual safety vulnerabilities in current MLLMs.&lt;/li&gt;&lt;li&gt;Focuses on systematic attack methodology for extracting harmful image-generation behavior, highlighting weaknesses in visual safety alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyu Yu', 'Lana Liu', 'Zhehao Zhao', 'Wei Wang', 'Sujuan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal-llm', 'image-generation', 'adversarial-attack', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15698</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainable Deepfake Detection with RL Enhanced Self-Blended Images</title><link>https://arxiv.org/abs/2601.15624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainable deepfake detection framework that leverages multimodal large language models (MLLMs) and automated Chain-of-Thought (CoT) textual annotations generated from Self-Blended Images.&lt;/li&gt;&lt;li&gt;Introduces an RL-enhanced training pipeline with a tailored reward mechanism and feedback-driven synthetic data generation to improve cross-dataset generalization and detection performance.&lt;/li&gt;&lt;li&gt;Aims to reduce costly manual annotation by automating CoT data creation and demonstrates competitive performance with SOTA on multiple cross-dataset benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ning Jiang', 'Dingheng Zeng', 'Yanhong Liu', 'Haiyang Yi', 'Shijie Yu', 'Minghe Weng', 'Haifeng Shen', 'Ying Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'explainability', 'synthetic data generation', 'reinforcement learning', 'MLLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15624</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title><link>https://arxiv.org/abs/2601.14691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLM-based judges that rely on agent chain-of-thought (CoT) are highly vulnerable: rewriting CoTs while keeping actions/observations fixed can substantially inflate false positives (up to ~90%) across 800 trajectories on diverse web tasks.&lt;/li&gt;&lt;li&gt;Compares manipulation strategies: style-based (presentation changes) vs content-based (fabricating progress signals), finding content-based manipulations are more effective at fooling judges.&lt;/li&gt;&lt;li&gt;Evaluates mitigation attempts (prompting techniques and increased judge compute/scale) which reduce but do not eliminate susceptibility.&lt;/li&gt;&lt;li&gt;Concludes that LLM judging is fundamentally vulnerable and advocates for judge mechanisms that verify reasoning claims against observable evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Sungryull Sohn', 'Yunxiang Zhang', 'Moontae Lee', 'Hao Peng', 'Lu Wang', 'Honglak Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evaluation', 'chain-of-thought-manipulation', 'LLM-judge-vulnerability', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14691</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can LLM Infer Risk Information From MCP Server System Logs?</title><link>https://arxiv.org/abs/2511.05867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents MCP-RiskCue, the first synthetic benchmark to evaluate LLMs' ability to detect nine categories of MCP server risk embedded in system logs (2,421 training chat histories, 471 evaluation queries).&lt;/li&gt;&lt;li&gt;Generates 1,800 synthetic logs using 10 SOTA LLMs, embeds them in 243 curated MCP servers, and evaluates model performance across sizes and training regimes (SFT, RLVR/GRPO).&lt;/li&gt;&lt;li&gt;Finds smaller models frequently miss risky logs (high false negatives); SFT tends to over-flag benign logs (high false positives); RL with Verifiable Reward (GRPO) achieves the best precision–recall tradeoff (Llama3.1-8B-Instruct reaches 83% accuracy).&lt;/li&gt;&lt;li&gt;Releases code and data to support further research on MCP-based attack detection and LLM safety improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Fu', 'Yuansen Zhang', 'Yinggui Wang']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark', 'attack-detection', 'MCP-security', 'model-robustness', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05867</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs and evaluates multiple membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) targeting whether victims' historical interactions in system prompts of LLM-based recommender systems are present.&lt;/li&gt;&lt;li&gt;Empirically tests attacks across five open-source LLMs and three recommender benchmark datasets, finding inquiry and poisoning attacks yield high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position, number of poisoning items) and discusses mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attacks', 'recommender-systems', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How malicious AI swarms can threaten democracy: The fusion of agentic AI and LLMs marks a new frontier in information warfare</title><link>https://arxiv.org/abs/2506.06299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes how LLMs combined with autonomous multi-agent architectures can form coordinated malicious swarms to conduct large-scale influence campaigns and fabricate social consensus.&lt;/li&gt;&lt;li&gt;Explains mechanisms that make such campaigns effective (scalable generation of human-like content, adaptive social-mimicry, agent coordination) and the democratic harms they pose.&lt;/li&gt;&lt;li&gt;Argues for pragmatic, multi-leverage-point interventions (technical, commercial, governance) rather than relying on voluntary compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Thilo Schroeder', 'Meeyoung Cha', 'Andrea Baronchelli', 'Nick Bostrom', 'Nicholas A. Christakis', 'David Garcia', 'Amit Goldenberg', 'Yara Kyrychenko', 'Kevin Leyton-Brown', 'Nina Lutz', 'Gary Marcus', 'Filippo Menczer', 'Gordon Pennycook', 'David G. Rand', 'Maria Ressa', 'Frank Schweitzer', 'Dawn Song', 'Christopher Summerfield', 'Audrey Tang', 'Jay J. Van Bavel', 'Sander van der Linden', 'Jonas R. Kunst']&lt;/li&gt;&lt;li&gt;Tags: ['information warfare', 'multi-agent attacks', 'misinformation', 'adversarial coordination', 'defense/policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06299</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains</title><link>https://arxiv.org/abs/2601.13137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial alignment framework (Attacker, Actor, Critic) to improve value consistency of LLMs in sensitive domains via continued pretraining, instruction fine-tuning, and adversarial training.&lt;/li&gt;&lt;li&gt;Uses an Attacker component to generate controversial queries (akin to red‑teaming) and trains an Actor to produce value‑consistent responses while a Critic filters for quality.&lt;/li&gt;&lt;li&gt;Trains a Value‑Consistent LLM (VC‑LLM) and evaluates it on a constructed bilingual (Chinese/English) dataset, reporting improvements over mainstream models in reducing biased/offensive outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Gao', 'Zhigang Liu', 'Xinyu Yao', 'Bo Chen', 'Xiaobing Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'alignment', 'safety', 'red-teaming', 'bias-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13137</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty</title><link>https://arxiv.org/abs/2601.12471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedAbstain, a benchmark and evaluation protocol for abstention in medical multiple-choice question answering that combines conformal prediction, adversarial question perturbations, and explicit abstention options.&lt;/li&gt;&lt;li&gt;Systematically evaluates open- and closed-source LLMs and finds that high-accuracy models often fail to abstain under uncertainty, while explicit abstention options significantly improve safe abstention.&lt;/li&gt;&lt;li&gt;Shows that input perturbations have less impact than explicit abstention mechanisms, and that scaling model size or advanced prompting yields little improvement in abstention behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sravanthi Machcha', 'Sushrita Yerra', 'Sahil Gupta', 'Aishwarya Sahoo', 'Sharmin Sultana', 'Hong Yu', 'Zonghai Yao']&lt;/li&gt;&lt;li&gt;Tags: ['abstention', 'safety', 'benchmark', 'medical', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12471</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</title><link>https://arxiv.org/abs/2512.16602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "Refusal Steering": an inference-time activation-steering method that uses an LLM-as-a-judge to assign refusal confidence scores and computes ridge-regularized steering vectors to control LLM refusal behaviour without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates removal of political refusal on Qwen3-Next-80B-A3B-Thinking while preserving safety on JailbreakBench and near-baseline performance on general benchmarks; generalizes across 4B and 80B models and can also induce targeted refusals.&lt;/li&gt;&lt;li&gt;Analyzes steering vectors, finding refusal signals concentrate in deeper transformer layers and are distributed across many dimensions, supporting transparent, controllable moderation at inference time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Iker Garc\\'ia-Ferrero", 'David Montero', 'Roman Orus']&lt;/li&gt;&lt;li&gt;Tags: ['activation-steering', 'jailbreak-evasion', 'model-moderation', 'inference-time-control', 'safety-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16602</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden conversational escalation in LLM-driven chats.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue to identify implicit harm not captured by standard toxicity filters.&lt;/li&gt;&lt;li&gt;Targets shortcomings of external classifiers and clinical rubrics by using model-internal signals to guard against gradual affective escalation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'David Atkinson', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['guardrails', 'safety', 'implicit harm detection', 'logit-based methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction</title><link>https://arxiv.org/abs/2601.16034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Trajectory Replay via Concept-Basis Reconstruction to transfer refusal interventions from a donor LLM to target LLMs without target-side refusal supervision.&lt;/li&gt;&lt;li&gt;Aligns layers with concept fingerprints and reconstructs refusal directions using a shared set of concept atoms to map ablation trajectories across architectures.&lt;/li&gt;&lt;li&gt;Proposes a weight-SVD stability guard to project interventions away from high-variance weight subspaces, preserving model capabilities while altering refusal behavior.&lt;/li&gt;&lt;li&gt;Evaluates on 8 model pairs (e.g., GPT-OSS-20B, GLM-4) showing consistent attenuation of refusal responses while maintaining performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tony Cristofano']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking/safety circumvention', 'circuit analysis', 'transferability', 'defensive stability (weight-SVD)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16034</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can professional translators identify machine-generated text?</title><link>https://arxiv.org/abs/2601.15828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;In-person study with 69 professional translators who evaluated three anonymized Italian short stories (two by ChatGPT-4o, one human) and judged likelihood of AI authorship with justifications.&lt;/li&gt;&lt;li&gt;Overall identification was inconclusive, but 16.2% of participants reliably detected synthetic texts—an equal-sized group misclassified in the opposite direction.&lt;/li&gt;&lt;li&gt;Reliable human cues for synthetic authorship included low burstiness and narrative contradictions; misleading cues included grammatical accuracy and emotional tone.&lt;/li&gt;&lt;li&gt;Findings highlight potential roles and limits of professional editors/translators in detecting and post-editing machine-generated text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Farrell']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-text detection', 'human evaluation', 'authorship attribution', 'AI-generated content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15828</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models</title><link>https://arxiv.org/abs/2601.15588</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents YuFeng-XGuard, a reasoning-centric guardrail model family for LLM safety that outputs structured risk predictions (risk categories, configurable confidence) plus natural-language explanations.&lt;/li&gt;&lt;li&gt;Introduces a tiered inference paradigm: fast initial risk decision on the first decoded token with optional on-demand explanatory reasoning to trade off latency and depth.&lt;/li&gt;&lt;li&gt;Implements a dynamic policy mechanism that decouples risk perception from enforcement, allowing policy updates without retraining the model.&lt;/li&gt;&lt;li&gt;Evaluates on public safety benchmarks, claiming state-of-the-art performance and releasing full-capacity and lightweight variants for deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyu Lin', 'Meizhen Liu', 'Xiufeng Huang', 'Jinfeng Li', 'Haiwen Hong', 'Xiaohan Yuan', 'Yuefeng Chen', 'Longtao Huang', 'Hui Xue', 'Ranjie Duan', 'Zhikai Chen', 'Yuchuan Fu', 'Defeng Li', 'Lingyao Gao', 'Yitong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Guardrails/Defense', 'Interpretability', 'Runtime policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15588</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains</title><link>https://arxiv.org/abs/2601.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdversaRiskQA, a verified adversarial factuality benchmark targeting high-risk domains (Health, Finance, Law) to test LLMs' resistance to confidently framed injected misinformation.&lt;/li&gt;&lt;li&gt;Defines adversarial factuality, provides two difficulty levels, and proposes automated metrics for attack success and long-form factuality evaluation.&lt;/li&gt;&lt;li&gt;Evaluates multiple open- and closed-source LLMs, measuring misinformation detection and long-form factuality, and reports scaling behavior, domain variability, and lack of correlation between injected misinformation and long-form factual outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Szelestey', 'Sofie van Engelen', 'Tianhao Huang', 'Justin Snelders', 'Qintao Zeng', 'Songgaojun Deng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'prompt injection', 'factuality evaluation', 'benchmark', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15511</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models</title><link>https://arxiv.org/abs/2601.15331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RECAP, a retrieval-based, resource-efficient method for generating adversarial prompts by matching new prompts to a database of pre-trained successful adversarial suffixes, avoiding costly retraining.&lt;/li&gt;&lt;li&gt;Constructs a dataset of 1,000 prompts across seven harm-related categories and evaluates GCG, PEZ, and GBDA jailbreak methods on Llama 3 8B to identify most effective attacks per category.&lt;/li&gt;&lt;li&gt;Shows that retrieving semantically similar successful adversarial prompts yields competitive attack success rates with much lower computational cost, enabling scalable red-teaming even when model internals are inaccessible.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishit Chugh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-prompting', 'red-teaming', 'attack-database', 'retrieval-based-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15331</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can We Trust LLM Detectors?</title><link>https://arxiv.org/abs/2601.15301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates two dominant LLM text-detection paradigms (training-free and supervised) and finds both brittle under distribution shift, unseen generators, and simple stylistic perturbations.&lt;/li&gt;&lt;li&gt;Shows supervised detectors perform well in-domain but degrade sharply out-of-domain; training-free methods are highly sensitive to proxy/model choice.&lt;/li&gt;&lt;li&gt;Proposes a supervised contrastive learning (SCL) framework to learn discriminative style embeddings for more robust detection.&lt;/li&gt;&lt;li&gt;Provides empirical evidence that building domain-agnostic, robust LLM detectors remains a fundamental challenge; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jivnesh Sandhan', 'Harshit Jaiswal', 'Fei Cheng', 'Yugo Murawaki']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-detection', 'robustness', 'defense', 'adversarial-robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15301</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box jailbreak attack for large vision-language models (LVLMs) using zeroth-order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;Claims advantages: gradient-free input-output based optimization, model-agnostic (no surrogate model), and lower resource/GPU memory needs versus white-box approaches.&lt;/li&gt;&lt;li&gt;Evaluates on InstructBLIP, LLaVA, and MiniGPT-4, reporting high jailbreak success rates (e.g., 83.0% on InstructBLIP) and strong transferability (e.g., 64.18% ASR from MiniGPT-4 to other LVLMs) while keeping perturbations imperceptible.&lt;/li&gt;&lt;li&gt;Highlights practical feasibility of black-box jailbreaks and exposes weaknesses in current LVLM safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'jailbreaking', 'black-box attack', 'zeroth-order optimization', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Conformal Blindness: A Note on $A$-Cryptic change-points</title><link>https://arxiv.org/abs/2601.01147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and formalises "conformal blindness": situations where Conformal Test Martingales (CTMs) fail to detect breaks in exchangeability because p-values remain uniform.&lt;/li&gt;&lt;li&gt;Constructs explicit A-cryptic change-points for the predictive-oracle conformity measure (true conditional density), showing via bivariate Gaussian examples that mean shifts along a particular line leave conformity-score distributions unchanged.&lt;/li&gt;&lt;li&gt;Provides simulations showing even large distribution shifts can be perfectly cryptic to CTMs, and argues this reveals a fundamental limitation of validity monitoring that requires aligning conformity measures with anticipated shifts and separating predictive from diagnostic objectives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Johan Hallberg Szabadv\\'ary"]&lt;/li&gt;&lt;li&gt;Tags: ['conformal-prediction', 'change-point-detection', 'anomaly-evasion', 'robustness', 'safety-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01147</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Real-World Adversarial Attacks on RF-Based Drone Detectors</title><link>https://arxiv.org/abs/2512.20712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first physical over-the-air (OTA) adversarial attack on RF-based drone detectors that process RF spectrograms with image/object-detection models.&lt;/li&gt;&lt;li&gt;Designs class-specific universal complex baseband (I/Q) perturbation waveforms transmitted alongside legitimate signals to evade detection of target drones.&lt;/li&gt;&lt;li&gt;Evaluates attacks on RF recordings and real OTA experiments across four drone types, showing modest, structured I/Q perturbations reliably reduce target detection while preserving detection of non-target legitimate drones.&lt;/li&gt;&lt;li&gt;Demonstrates compatibility of the perturbations with standard RF chains and discusses practical implementation constraints (synchronization, interference, hardware limits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omer Gazit', 'Yael Itzhakev', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'over-the-air attack', 'RF security', 'evasion attack', 'physical adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20712</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference</title><link>https://arxiv.org/abs/2509.24257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VeriLLM, a protocol for publicly verifiable decentralized LLM inference that ensures correctness and provides incentive guarantees in permissionless settings.&lt;/li&gt;&lt;li&gt;Uses lightweight empirical rerunning plus minimal on-chain checks to enable verifiers to validate outputs at ~1% of the original inference cost by exploiting prefill vs autoregressive decoding separation.&lt;/li&gt;&lt;li&gt;Proposes an isomorphic inference–verification architecture that multiplexes inference and verification on the same GPU workers to boost utilization, enlarge the validator set, and enforce task indistinguishability to prevent selective/malicious behavior.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and system-level evaluation showing reliable public verifiability with minimal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ke Wang', 'Zishuo Zhao', 'Xinyuan Song', 'Zelin Li', 'Libin Xia', 'Chris Tong', 'Bill Shi', 'Wenjie Qu', 'Eric Yang', 'Lynn Ai']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable computation', 'decentralized inference', 'integrity verification', 'incentive mechanisms', 'blockchain/on-chain checks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24257</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs and evaluates multiple membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) targeting whether victims' historical interactions in system prompts of LLM-based recommender systems are present.&lt;/li&gt;&lt;li&gt;Empirically tests attacks across five open-source LLMs and three recommender benchmark datasets, finding inquiry and poisoning attacks yield high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position, number of poisoning items) and discusses mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attacks', 'recommender-systems', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How malicious AI swarms can threaten democracy: The fusion of agentic AI and LLMs marks a new frontier in information warfare</title><link>https://arxiv.org/abs/2506.06299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes how LLMs combined with autonomous multi-agent architectures can form coordinated malicious swarms to conduct large-scale influence campaigns and fabricate social consensus.&lt;/li&gt;&lt;li&gt;Explains mechanisms that make such campaigns effective (scalable generation of human-like content, adaptive social-mimicry, agent coordination) and the democratic harms they pose.&lt;/li&gt;&lt;li&gt;Argues for pragmatic, multi-leverage-point interventions (technical, commercial, governance) rather than relying on voluntary compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Thilo Schroeder', 'Meeyoung Cha', 'Andrea Baronchelli', 'Nick Bostrom', 'Nicholas A. Christakis', 'David Garcia', 'Amit Goldenberg', 'Yara Kyrychenko', 'Kevin Leyton-Brown', 'Nina Lutz', 'Gary Marcus', 'Filippo Menczer', 'Gordon Pennycook', 'David G. Rand', 'Maria Ressa', 'Frank Schweitzer', 'Dawn Song', 'Christopher Summerfield', 'Audrey Tang', 'Jay J. Van Bavel', 'Sander van der Linden', 'Jonas R. Kunst']&lt;/li&gt;&lt;li&gt;Tags: ['information warfare', 'multi-agent attacks', 'misinformation', 'adversarial coordination', 'defense/policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06299</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Match Made in Heaven? AI-driven Matching of Vulnerabilities and Security Unit Tests</title><link>https://arxiv.org/abs/2502.03365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VuTeCo, an AI-driven framework to find security-related unit tests and match them to specific Java vulnerabilities.&lt;/li&gt;&lt;li&gt;Solves two tasks: Finding (classify tests as security-related) using UniXcoder and Matching (link tests to vulnerabilities) using DeepSeek Coder, reporting F0.5 and precision metrics on Vul4J.&lt;/li&gt;&lt;li&gt;Applied VuTeCo on 427 Java projects and 1,238 vulnerabilities, producing a validated dataset (Test4Vul) of vulnerability-witnessing tests.&lt;/li&gt;&lt;li&gt;Purpose is to enable large-scale retrieval/creation of vulnerability-witnessing tests to help future generative models and improve security testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emanuele Iannone', 'Quang-Cuong Bui', 'Riccardo Scandariato']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-detection', 'security-testing', 'dataset-collection', 'AI-assisted-software-security', 'unit-tests']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.03365</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On damage of interpolation to adversarial robustness in regression</title><link>https://arxiv.org/abs/2601.16070</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of interpolating estimators in a nonparametric regression setting, focusing on so-called future X-attacks (perturbations to inputs).&lt;/li&gt;&lt;li&gt;Theoretically shows that interpolation (perfect fitting) can substantially degrade robustness, making interpolating estimators suboptimal under adversarial input perturbations.&lt;/li&gt;&lt;li&gt;Identifies a phenomenon called the "curse of simple size" in high-interpolation regimes and supports theoretical findings with numerical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingfu Peng', 'Yuhong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial attacks', 'interpolation', 'regression', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16070</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems</title><link>https://arxiv.org/abs/2601.15697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines how AI can enhance security in healthcare systems via threat detection, system monitoring, and automated response mechanisms.&lt;/li&gt;&lt;li&gt;Discusses privacy risks arising from AI use in healthcare and the need to protect patient data while deploying security tools.&lt;/li&gt;&lt;li&gt;Provides real-world healthcare examples and practical recommendations for integrating AI solutions that balance security and privacy.&lt;/li&gt;&lt;li&gt;Emphasizes transparency, adherence to privacy regulations, and governance when deploying AI-driven security measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binu V P', 'Deepthy K Bhaskar', 'Minimol B']&lt;/li&gt;&lt;li&gt;Tags: ['AI-based defenses', 'Privacy', 'Healthcare security', 'Threat detection', 'Governance &amp; compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15697</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.15678</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGCRAWLER, an attack that formulates extraction from retrieval-augmented generation (RAG) systems as an adaptive stochastic coverage problem to plan long-term query strategies.&lt;/li&gt;&lt;li&gt;Builds and maintains an attacker-side knowledge graph to estimate conditional marginal gain (CMG) and plan semantic queries targeting unretrieved corpus regions.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results across RAG architectures and datasets (up to 84.4% corpus coverage, ~20.7% improvement over baselines) and robustness against defenses like query rewriting and multi-query retrieval.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Yao', 'Ziqi Zhang', 'Ning Luo', 'Shaofei Li', 'Yifeng Cai', 'Xiangqun Chen', 'Yao Guo', 'Ding Li']&lt;/li&gt;&lt;li&gt;Tags: ['data exfiltration', 'retrieval-augmented generation', 'query-based attack', 'knowledge-graph-guided attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15678</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning</title><link>https://arxiv.org/abs/2601.15595</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Data-Free Selective Unlearning (DFSU) to remove sensitive PII from LLMs without access to original training data.&lt;/li&gt;&lt;li&gt;Synthesizes pseudo-PII via model inversion, builds token-level privacy masks for these synthetic samples, and performs token-level selective unlearning using a contrastive mask loss in a LoRA subspace.&lt;/li&gt;&lt;li&gt;Evaluated on the AI4Privacy PII-Masking dataset with Pythia models, showing effective removal of target PII while largely preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinjie Zhou', 'Zhihui Yang', 'Lechao Cheng', 'Sai Wu', 'Gang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'model unlearning', 'model inversion', 'data-free defense', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15595</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models</title><link>https://arxiv.org/abs/2601.15331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RECAP, a retrieval-based, resource-efficient method for generating adversarial prompts by matching new prompts to a database of pre-trained successful adversarial suffixes, avoiding costly retraining.&lt;/li&gt;&lt;li&gt;Constructs a dataset of 1,000 prompts across seven harm-related categories and evaluates GCG, PEZ, and GBDA jailbreak methods on Llama 3 8B to identify most effective attacks per category.&lt;/li&gt;&lt;li&gt;Shows that retrieving semantically similar successful adversarial prompts yields competitive attack success rates with much lower computational cost, enabling scalable red-teaming even when model internals are inaccessible.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishit Chugh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-prompting', 'red-teaming', 'attack-database', 'retrieval-based-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15331</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Counterfactual Training: Teaching Models Plausible and Actionable Explanations</title><link>https://arxiv.org/abs/2601.16205</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'counterfactual training', a training regime that incorporates counterfactual explanations into learning so models produce plausible and actionable counterfactuals inherently rather than via post-hoc methods.&lt;/li&gt;&lt;li&gt;Optimizes learned representations to be aligned with plausibility and feature-mutability constraints, holding models accountable for desirable counterfactual explanations.&lt;/li&gt;&lt;li&gt;Provides empirical and theoretical evidence that this approach yields models with improved explanatory capacity and additionally improves adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Altmeyer', 'Aleksander Buszydlik', 'Arie van Deursen', 'Cynthia C. S. Liem']&lt;/li&gt;&lt;li&gt;Tags: ['counterfactual-explanations', 'explainability', 'adversarial-robustness', 'defense', 'training-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16205</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing</title><link>https://arxiv.org/abs/2601.16200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Feature-space Smoothing (FS), a method that provides certified robustness guarantees on feature representations of multimodal LLMs by certifying a lower bound on feature cosine similarity under l2-bounded attacks.&lt;/li&gt;&lt;li&gt;Introduces Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of existing encoders (no MLLM retraining required) to strengthen the FS certificate.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis (Feature Cosine Similarity Bound) and extensive empirical results showing large reductions in attack success rate (white-box attacks reduced from ~90% to ~1%) and better performance than adversarial training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song Xia', 'Meiwen Ding', 'Chenqi Kong', 'Wenhan Yang', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial defense', 'multimodal LLMs', 'feature-space smoothing', 'provable guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16200</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoK: Challenges in Tabular Membership Inference Attacks</title><link>https://arxiv.org/abs/2601.15874</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization and taxonomy of Membership Inference Attacks (MIAs) for centralized and federated learning, with emphasis on tabular data.&lt;/li&gt;&lt;li&gt;Empirical evaluation of multiple MIA strategies and defenses on tabular datasets, finding generally weak overall attack performance but strong vulnerability for 'single-out' records.&lt;/li&gt;&lt;li&gt;Considers an often-neglected outsider adversary in federated settings and studies transferability of MIAs across surrogate models and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristina P\\^era', 'T\\^ania Carvalho', 'Maxime Cordy', "Lu\\'is Antunes"]&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'federated-learning', 'tabular-data', 'attack-transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15874</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models</title><link>https://arxiv.org/abs/2601.15801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GOSV (Global Optimization for Safety Vector Extraction), a framework that globally optimizes over attention heads to identify safety-critical components in LLMs.&lt;/li&gt;&lt;li&gt;Defines two distinct sets of safety-related vectors—Malicious Injection Vectors and Safety Suppression Vectors—using Harmful Patching and Zero Ablation activation-repatching strategies.&lt;/li&gt;&lt;li&gt;Shows that repatching ~30% of heads can cause complete safety breakdown and uses these findings to build a novel inference-time white-box jailbreak that outperforms prior white-box attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengheng Chu', 'Jiahao Chen', 'Yuhong Wang', 'Jun Wang', 'Zhihui Fu', 'Shouling Ji', 'Songze Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'activation repatching', 'model interpretability', 'attention-head attribution', 'white-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15801</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title><link>https://arxiv.org/abs/2601.15538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how low-bit quantization can undo machine unlearning by causing small weight updates to fall within the same quantization buckets, effectively restoring forgotten information.&lt;/li&gt;&lt;li&gt;Proposes a quantization-aware unlearning method using a logits-space hinge loss that enforces a margin between the unlearned and original model outputs (half the quantization step) so forgotten examples remain distinguishable after quantization.&lt;/li&gt;&lt;li&gt;Evaluates on language and classification tasks (including a Twitter misinformation dataset) and shows the method preserves forgetting under 4-bit quantization while existing methods largely recover forgotten knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Mishra', 'Kanwal Mehreen']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'quantization', 'privacy', 'model-robustness', 'misinformation-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15538</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Targeted Graph Backdoor Attack</title><link>https://arxiv.org/abs/2601.15474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first multi-targeted backdoor attack for graph classification where multiple injected subgraph triggers redirect inputs to different target labels.&lt;/li&gt;&lt;li&gt;Uses subgraph injection (preserving original graph structure) rather than subgraph replacement and evaluates across five datasets and four GNN architectures.&lt;/li&gt;&lt;li&gt;Shows high attack success rates with minimal impact on clean accuracy, analyzes attack design parameters, and demonstrates robustness against defenses like randomized smoothing and fine-pruning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nabi Newaz Khan', 'Abdullah Arafat Miah', 'Yu Bi']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'graph neural networks', 'poisoning attack', 'adversarial attack', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15474</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning from Synthetic Data: Limitations of ERM</title><link>https://arxiv.org/abs/2601.15468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models learning from datasets contaminated by LLM-generated (synthetic) examples; learners are oblivious to which examples are synthetic.&lt;/li&gt;&lt;li&gt;Shows limitations of empirical risk minimization (ERM): ERM can be suboptimal or fail to recover true concepts under synthetic-data contamination (including PAC setting), while weighted or alternative algorithms can succeed.&lt;/li&gt;&lt;li&gt;Provides positive results: algorithms that assign non-uniform weights or other procedures can learn correct hypotheses for arbitrary VC classes despite arbitrary amounts of contamination.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kareem Amin', 'Alex Bie', 'Weiwei Kong', 'Umar Syed', 'Sergei Vassilvitskii']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning / contamination', 'robust learning', 'ERM limitations', 'learning theory', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15468</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty</title><link>https://arxiv.org/abs/2601.12471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedAbstain, a benchmark and evaluation protocol for abstention in medical multiple-choice question answering that combines conformal prediction, adversarial question perturbations, and explicit abstention options.&lt;/li&gt;&lt;li&gt;Systematically evaluates open- and closed-source LLMs and finds that high-accuracy models often fail to abstain under uncertainty, while explicit abstention options significantly improve safe abstention.&lt;/li&gt;&lt;li&gt;Shows that input perturbations have less impact than explicit abstention mechanisms, and that scaling model size or advanced prompting yields little improvement in abstention behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sravanthi Machcha', 'Sushrita Yerra', 'Sahil Gupta', 'Aishwarya Sahoo', 'Sharmin Sultana', 'Hong Yu', 'Zonghai Yao']&lt;/li&gt;&lt;li&gt;Tags: ['abstention', 'safety', 'benchmark', 'medical', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12471</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box jailbreak attack for large vision-language models (LVLMs) using zeroth-order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;Claims advantages: gradient-free input-output based optimization, model-agnostic (no surrogate model), and lower resource/GPU memory needs versus white-box approaches.&lt;/li&gt;&lt;li&gt;Evaluates on InstructBLIP, LLaVA, and MiniGPT-4, reporting high jailbreak success rates (e.g., 83.0% on InstructBLIP) and strong transferability (e.g., 64.18% ASR from MiniGPT-4 to other LVLMs) while keeping perturbations imperceptible.&lt;/li&gt;&lt;li&gt;Highlights practical feasibility of black-box jailbreaks and exposes weaknesses in current LVLM safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'jailbreaking', 'black-box attack', 'zeroth-order optimization', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</title><link>https://arxiv.org/abs/2512.16602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "Refusal Steering": an inference-time activation-steering method that uses an LLM-as-a-judge to assign refusal confidence scores and computes ridge-regularized steering vectors to control LLM refusal behaviour without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates removal of political refusal on Qwen3-Next-80B-A3B-Thinking while preserving safety on JailbreakBench and near-baseline performance on general benchmarks; generalizes across 4B and 80B models and can also induce targeted refusals.&lt;/li&gt;&lt;li&gt;Analyzes steering vectors, finding refusal signals concentrate in deeper transformer layers and are distributed across many dimensions, supporting transparent, controllable moderation at inference time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Iker Garc\\'ia-Ferrero", 'David Montero', 'Roman Orus']&lt;/li&gt;&lt;li&gt;Tags: ['activation-steering', 'jailbreak-evasion', 'model-moderation', 'inference-time-control', 'safety-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16602</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden conversational escalation in LLM-driven chats.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue to identify implicit harm not captured by standard toxicity filters.&lt;/li&gt;&lt;li&gt;Targets shortcomings of external classifiers and clinical rubrics by using model-internal signals to guard against gradual affective escalation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'David Atkinson', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['guardrails', 'safety', 'implicit harm detection', 'logit-based methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs and evaluates multiple membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) targeting whether victims' historical interactions in system prompts of LLM-based recommender systems are present.&lt;/li&gt;&lt;li&gt;Empirically tests attacks across five open-source LLMs and three recommender benchmark datasets, finding inquiry and poisoning attacks yield high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack success (number of shots, victim position, number of poisoning items) and discusses mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attacks', 'recommender-systems', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking</title><link>https://arxiv.org/abs/2507.14629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VMask, a defense framework for vertical federated learning that masks selected model layers via secret sharing to disrupt correlation used by model completion label-inference attacks.&lt;/li&gt;&lt;li&gt;Selects critical layers to mask to minimize overhead and provides a tunable privacy budget so defenders can trade privacy vs. utility.&lt;/li&gt;&lt;li&gt;Empirical evaluation across 5 architectures and 13 datasets shows VMask reduces label-inference to random-guessing while preserving model accuracy (e.g., ~0.09% avg drop for Transformer) and is far faster than cryptographic defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juntao Tan', 'Lan Zhang', 'Zhonghao Hu', 'Kai Yang', 'Peng Ran', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['vertical-federated-learning', 'label-inference', 'model-completion-attack', 'defense', 'secret-sharing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14629</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning</title><link>https://arxiv.org/abs/2507.14625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VTarbel, a two-stage minimal-knowledge targeted-label attack against detector-enhanced vertical federated learning (VFL): (1) select high-expressiveness samples via maximum mean discrepancy and query pseudo-labels; (2) train surrogate detector and model to guide gradient-based perturbations that induce targeted misclassification while evading detectors.&lt;/li&gt;&lt;li&gt;Implements and evaluates VTarbel across four model architectures, seven multimodal datasets, and two anomaly detectors, outperforming four state-of-the-art baselines and remaining effective against three representative privacy-preserving defenses.&lt;/li&gt;&lt;li&gt;Highlights practical security blind spots in VFL deployments and the need for robust, attack-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juntao Tan', 'Anran Li', 'Quanchao Liu', 'Peng Ran', 'Lan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['vertical federated learning', 'targeted label attack', 'adversarial examples', 'detector evasion', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14625</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How malicious AI swarms can threaten democracy: The fusion of agentic AI and LLMs marks a new frontier in information warfare</title><link>https://arxiv.org/abs/2506.06299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes how LLMs combined with autonomous multi-agent architectures can form coordinated malicious swarms to conduct large-scale influence campaigns and fabricate social consensus.&lt;/li&gt;&lt;li&gt;Explains mechanisms that make such campaigns effective (scalable generation of human-like content, adaptive social-mimicry, agent coordination) and the democratic harms they pose.&lt;/li&gt;&lt;li&gt;Argues for pragmatic, multi-leverage-point interventions (technical, commercial, governance) rather than relying on voluntary compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Thilo Schroeder', 'Meeyoung Cha', 'Andrea Baronchelli', 'Nick Bostrom', 'Nicholas A. Christakis', 'David Garcia', 'Amit Goldenberg', 'Yara Kyrychenko', 'Kevin Leyton-Brown', 'Nina Lutz', 'Gary Marcus', 'Filippo Menczer', 'Gordon Pennycook', 'David G. Rand', 'Maria Ressa', 'Frank Schweitzer', 'Dawn Song', 'Christopher Summerfield', 'Audrey Tang', 'Jay J. Van Bavel', 'Sander van der Linden', 'Jonas R. Kunst']&lt;/li&gt;&lt;li&gt;Tags: ['information warfare', 'multi-agent attacks', 'misinformation', 'adversarial coordination', 'defense/policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06299</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Is Your Writing Being Mimicked by AI? Unveiling Imitation with Invisible Watermarks in Creative Writing</title><link>https://arxiv.org/abs/2504.00035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WIND, a zero-watermarking scheme that encodes verifiable copyright attributes of creative writing via implicit, non-disruptive disentanglement of creative-specific and irrelevant features.&lt;/li&gt;&lt;li&gt;Uses LLMs to extract five key elements of creative essence through an instance delimitation mechanism and consolidates them into condensed lists for watermark encoding and verification.&lt;/li&gt;&lt;li&gt;Claims high verification performance (F1 &gt; 98%) and robustness at low false-positive rates compared to existing text watermarking methods, without altering textual content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziwei Zhang', 'Juan Wen', 'Wanli Peng', 'Zhengxian Wu', 'Yinghan Zhou', 'Yiming Xue']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'copyright protection', 'text watermarking', 'model/data provenance', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.00035</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title><link>https://arxiv.org/abs/2601.14691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLM-based judges that rely on agent chain-of-thought (CoT) are highly vulnerable: rewriting CoTs while keeping actions/observations fixed can substantially inflate false positives (up to ~90%) across 800 trajectories on diverse web tasks.&lt;/li&gt;&lt;li&gt;Compares manipulation strategies: style-based (presentation changes) vs content-based (fabricating progress signals), finding content-based manipulations are more effective at fooling judges.&lt;/li&gt;&lt;li&gt;Evaluates mitigation attempts (prompting techniques and increased judge compute/scale) which reduce but do not eliminate susceptibility.&lt;/li&gt;&lt;li&gt;Concludes that LLM judging is fundamentally vulnerable and advocates for judge mechanisms that verify reasoning claims against observable evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Sungryull Sohn', 'Yunxiang Zhang', 'Moontae Lee', 'Hao Peng', 'Lu Wang', 'Honglak Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evaluation', 'chain-of-thought-manipulation', 'LLM-judge-vulnerability', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14691</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Counterfactual Training: Teaching Models Plausible and Actionable Explanations</title><link>https://arxiv.org/abs/2601.16205</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'counterfactual training', a training regime that incorporates counterfactual explanations into learning so models produce plausible and actionable counterfactuals inherently rather than via post-hoc methods.&lt;/li&gt;&lt;li&gt;Optimizes learned representations to be aligned with plausibility and feature-mutability constraints, holding models accountable for desirable counterfactual explanations.&lt;/li&gt;&lt;li&gt;Provides empirical and theoretical evidence that this approach yields models with improved explanatory capacity and additionally improves adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Altmeyer', 'Aleksander Buszydlik', 'Arie van Deursen', 'Cynthia C. S. Liem']&lt;/li&gt;&lt;li&gt;Tags: ['counterfactual-explanations', 'explainability', 'adversarial-robustness', 'defense', 'training-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16205</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Watermark in the Latent Space of Generative Models</title><link>https://arxiv.org/abs/2601.16140</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DistSeal, a method for post-hoc watermarking in the latent space of generative models (diffusion and autoregressive).&lt;/li&gt;&lt;li&gt;Shows latent watermarkers can be distilled into the generative model or latent decoder for in-model watermarking, improving efficiency (up to 20x) and imperceptibility versus pixel-space methods.&lt;/li&gt;&lt;li&gt;Demonstrates competitive robustness of latent watermarks and that distilling latent watermarkers outperforms distilling pixel-space watermarkers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sylvestre-Alvise Rebuffi', 'Tuan Tran', 'Valeriu Lacatusu', 'Pierre Fernandez', "Tom\\'a\\v{s} Sou\\v{c}ek", "Nikola Jovanovi\\'c", 'Tom Sander', 'Hady Elsahar', 'Alexandre Mourachko']&lt;/li&gt;&lt;li&gt;Tags: ['Watermarking', 'Model provenance', 'Defenses', 'Generative models', 'Latent-space techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16140</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can professional translators identify machine-generated text?</title><link>https://arxiv.org/abs/2601.15828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;In-person study with 69 professional translators who evaluated three anonymized Italian short stories (two by ChatGPT-4o, one human) and judged likelihood of AI authorship with justifications.&lt;/li&gt;&lt;li&gt;Overall identification was inconclusive, but 16.2% of participants reliably detected synthetic texts—an equal-sized group misclassified in the opposite direction.&lt;/li&gt;&lt;li&gt;Reliable human cues for synthetic authorship included low burstiness and narrative contradictions; misleading cues included grammatical accuracy and emotional tone.&lt;/li&gt;&lt;li&gt;Findings highlight potential roles and limits of professional editors/translators in detecting and post-editing machine-generated text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Farrell']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-text detection', 'human evaluation', 'authorship attribution', 'AI-generated content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15828</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Introducing the Generative Application Firewall (GAF)</title><link>https://arxiv.org/abs/2601.15824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Generative Application Firewall (GAF), an architectural enforcement layer for securing LLM applications.&lt;/li&gt;&lt;li&gt;Unifies fragmented defenses (prompt filters, guardrails, data-masking) into a single coordinated enforcement point, analogous to a Web Application Firewall.&lt;/li&gt;&lt;li&gt;Extends coverage to autonomous agents and their tool interactions, aiming to centrally enforce safety and policy across pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joan Vendrell Farreny (NeuralTrust)', "Mart\\'i Jord\\`a Roca (NeuralTrust)", 'Miquel Cornudella Gaya (NeuralTrust)', "Rodrigo Fern\\'andez Ba\\'on (NeuralTrust)", "V\\'ictor Garc\\'ia Mart\\'inez (NeuralTrust)", 'Eduard Camacho Sucarrat (NeuralTrust)', 'Alessandro Pignati (NeuralTrust)']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'LLM security', 'application firewall', 'guardrails', 'agent safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15824</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving</title><link>https://arxiv.org/abs/2601.15729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DualShield, a planning/control framework that integrates Hamilton-Jacobi reachability value functions with diffusion-based multimodal motion planning for autonomous driving.&lt;/li&gt;&lt;li&gt;Uses reachability value functions both proactively (to guide the diffusion denoising toward safe, dynamically feasible regions) and reactively (as control barrier-value functions to modify executed actions and ensure safety).&lt;/li&gt;&lt;li&gt;Claims improved safety and task efficiency in simulations of challenging unprotected U-turn scenarios under uncertain or adversarial interactions compared to leading planning baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Yang', 'Lei Zheng', 'Ruoyu Yao', 'Jun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reachability analysis', 'control barrier functions', 'diffusion models', 'autonomous driving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15729</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs</title><link>https://arxiv.org/abs/2601.15698</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BVS, a novel jailbreaking framework for probing visual safety in multimodal large language models using a 'reconstruction-then-generation' approach.&lt;/li&gt;&lt;li&gt;Introduces neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, enabling models to be induced into generating harmful images.&lt;/li&gt;&lt;li&gt;Reports high empirical effectiveness—98.21% jailbreak success against GPT-5 (12 Jan 2026 release)—demonstrating critical visual safety vulnerabilities in current MLLMs.&lt;/li&gt;&lt;li&gt;Focuses on systematic attack methodology for extracting harmful image-generation behavior, highlighting weaknesses in visual safety alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyu Yu', 'Lana Liu', 'Zhehao Zhao', 'Wei Wang', 'Sujuan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal-llm', 'image-generation', 'adversarial-attack', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15698</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.15678</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGCRAWLER, an attack that formulates extraction from retrieval-augmented generation (RAG) systems as an adaptive stochastic coverage problem to plan long-term query strategies.&lt;/li&gt;&lt;li&gt;Builds and maintains an attacker-side knowledge graph to estimate conditional marginal gain (CMG) and plan semantic queries targeting unretrieved corpus regions.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results across RAG architectures and datasets (up to 84.4% corpus coverage, ~20.7% improvement over baselines) and robustness against defenses like query rewriting and multi-query retrieval.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Yao', 'Ziqi Zhang', 'Ning Luo', 'Shaofei Li', 'Yifeng Cai', 'Xiangqun Chen', 'Yao Guo', 'Ding Li']&lt;/li&gt;&lt;li&gt;Tags: ['data exfiltration', 'retrieval-augmented generation', 'query-based attack', 'knowledge-graph-guided attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15678</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning</title><link>https://arxiv.org/abs/2601.15595</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Data-Free Selective Unlearning (DFSU) to remove sensitive PII from LLMs without access to original training data.&lt;/li&gt;&lt;li&gt;Synthesizes pseudo-PII via model inversion, builds token-level privacy masks for these synthetic samples, and performs token-level selective unlearning using a contrastive mask loss in a LoRA subspace.&lt;/li&gt;&lt;li&gt;Evaluated on the AI4Privacy PII-Masking dataset with Pythia models, showing effective removal of target PII while largely preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinjie Zhou', 'Zhihui Yang', 'Lechao Cheng', 'Sai Wu', 'Gang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'model unlearning', 'model inversion', 'data-free defense', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15595</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title><link>https://arxiv.org/abs/2601.15538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how low-bit quantization can undo machine unlearning by causing small weight updates to fall within the same quantization buckets, effectively restoring forgotten information.&lt;/li&gt;&lt;li&gt;Proposes a quantization-aware unlearning method using a logits-space hinge loss that enforces a margin between the unlearned and original model outputs (half the quantization step) so forgotten examples remain distinguishable after quantization.&lt;/li&gt;&lt;li&gt;Evaluates on language and classification tasks (including a Twitter misinformation dataset) and shows the method preserves forgetting under 4-bit quantization while existing methods largely recover forgotten knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Mishra', 'Kanwal Mehreen']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'quantization', 'privacy', 'model-robustness', 'misinformation-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15538</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Targeted Graph Backdoor Attack</title><link>https://arxiv.org/abs/2601.15474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multi-targeted backdoor attack on graph classification where multiple injected subgraph triggers redirect inputs to different target labels.&lt;/li&gt;&lt;li&gt;Proposes subgraph injection (preserves original graph structure) rather than subgraph replacement, and evaluates attack effectiveness across five datasets and four GNN architectures.&lt;/li&gt;&lt;li&gt;Analyzes design factors (injection methods, connections, trigger size/density, poisoning ratio) and tests robustness against defenses (randomized smoothing, fine-pruning).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nabi Newaz Khan', 'Abdullah Arafat Miah', 'Yu Bi']&lt;/li&gt;&lt;li&gt;Tags: ['graph-backdoor', 'backdoor-attack', 'GNN-security', 'adversarial-attack', 'defense-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15474</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models</title><link>https://arxiv.org/abs/2601.15331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RECAP, a retrieval-based, resource-efficient method for generating adversarial prompts by matching new prompts to a database of pre-trained successful adversarial suffixes, avoiding costly retraining.&lt;/li&gt;&lt;li&gt;Constructs a dataset of 1,000 prompts across seven harm-related categories and evaluates GCG, PEZ, and GBDA jailbreak methods on Llama 3 8B to identify most effective attacks per category.&lt;/li&gt;&lt;li&gt;Shows that retrieving semantically similar successful adversarial prompts yields competitive attack success rates with much lower computational cost, enabling scalable red-teaming even when model internals are inaccessible.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishit Chugh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-prompting', 'red-teaming', 'attack-database', 'retrieval-based-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15331</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment</title><link>https://arxiv.org/abs/2601.16027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CS-VAR, a retrieval-augmented detector that uses an LLM to reason over retrieved cross-session behavioral evidence and distill local-to-global insights into a lightweight domain-specific model for real-time live-stream risk assessment.&lt;/li&gt;&lt;li&gt;Enables the small model to recognize recurring/gradual malicious patterns (e.g., scams, coordinated abuse) across seemingly unrelated streams while maintaining efficiency for deployment.&lt;/li&gt;&lt;li&gt;Evaluated with extensive offline experiments on large-scale industrial datasets and online validation, showing state-of-the-art performance and producing interpretable, localized signals to support moderation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiran Qiao', 'Xiang Ao', 'Jing Chen', 'Yang Liu', 'Qiwei Zhong', 'Qing He']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'security-detection', 'retrieval-augmented-models', 'LLM-guided-training', 'live-streaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16027</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Methodologies for LLM Evaluations Across Global Languages</title><link>https://arxiv.org/abs/2601.15706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Multilingual safety evaluation of two open-weight LLMs across ten languages using &gt;6,000 translated prompts covering five harm categories (privacy, non-violent crime, violent crime, intellectual property, jailbreak robustness).&lt;/li&gt;&lt;li&gt;Compares automated LLM-as-judge evaluations with human annotation and documents variation in safeguard robustness and evaluator reliability across languages and harm types.&lt;/li&gt;&lt;li&gt;Provides methodological recommendations for multilingual safety testing (culturally contextualized translations, stress-tested evaluator prompts, clearer human annotation guidelines) and proposes a shared framework for ongoing multilingual red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akriti Vij', 'Benjamin Chua', 'Darshini Ramiah', 'En Qi Ng', 'Mahran Morsidi', 'Naga Nikshith Gangarapu', 'Sharmini Johnson', 'Vanessa Wilfred', 'Vikneswaran Kumaran', 'Wan Sie Lee', 'Wenzhuo Yang', 'Yongsen Zheng', 'Bill Black', 'Boming Xia', 'Frank Sun', 'Hao Zhang', 'Qinghua Lu', 'Suyu Ma', 'Yue Liu', 'Chi-kiu Lo', 'Fatemeh Azadi', 'Isar Nejadgholi', 'Sowmya Vajjala', 'Agnes Delaborde', 'Nicolas Rolin', 'Tom Seimandi', 'Akiko Murakami', 'Haruto Ishi', 'Satoshi Sekine', 'Takayuki Semitsu', 'Tasuku Sasaki', 'Angela Kinuthia', 'Jean Wangari', 'Michael Michie', 'Stephanie Kasaon', 'Hankyul Baek', 'Jaewon Noh', 'Kihyuk Nam', 'Sang Seo', 'Sungpil Shin', 'Taewhi Lee', 'Yongsu Kim', 'Daisy Newbold-Harrop', 'Jessica Wang', 'Mahmoud Ghanem', 'Vy Hong']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual-safety', 'red-teaming', 'jailbreak', 'evaluation-benchmark', 'model-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15706</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats</title><link>https://arxiv.org/abs/2601.15679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Consortium report on methodological best practices for evaluating agentic AI systems, focusing on risks such as sensitive information leakage, fraud, and cybersecurity threats.&lt;/li&gt;&lt;li&gt;Describes a joint testing exercise across multiple countries evaluating open and closed-weight models on public agentic benchmarks, with emphasis on methodological issues rather than comparative performance.&lt;/li&gt;&lt;li&gt;Two strands: (1) common risks including data leakage and fraud, and (2) cybersecurity; aims to align international approaches for agentic evaluations and refine testing protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ee Wei Seah', 'Yongsen Zheng', 'Naga Nikshith', 'Mahran Morsidi', 'Gabriel Waikin Loh Matienzo', 'Nigel Gay', 'Akriti Vij', 'Benjamin Chua', 'En Qi Ng', 'Sharmini Johnson', 'Vanessa Wilfred', 'Wan Sie Lee', 'Anna Davidson', 'Catherine Devine', 'Erin Zorer', 'Gareth Holvey', 'Harry Coppock', 'James Walpole', 'Jerome Wynee', 'Magda Dubois', 'Michael Schmatz', 'Patrick Keane', 'Sam Deverett', 'Bill Black', 'Bo Yan', 'Bushra Sabir', 'Frank Sun', 'Hao Zhang', 'Harriet Farlow', 'Helen Zhou', 'Lingming Dong', 'Qinghua Lu', 'Seung Jang', 'Sharif Abuadbba', "Simon O'Callaghan", 'Suyu Ma', 'Tom Howroyd', 'Cyrus Fung', 'Fatemeh Azadi', 'Isar Nejadgholi', 'Krishnapriya Vishnubhotla', 'Pulei Xiong', 'Saeedeh Lohrasbi', 'Scott Buffett', 'Shahrear Iqbal', 'Sowmya Vajjala', 'Anna Safont-Andreu', 'Luca Massarelli', 'Oskar van der Wal', 'Simon M\\"oller', 'Agnes Delaborde', "Joris Dugu\\'ep\\'eroux", 'Nicolas Rolin', 'Romane Gallienne', 'Sarah Behanzin', 'Tom Seimandi', 'Akiko Murakami', 'Takayuki Semitsu', 'Teresa Tsukiji', 'Angela Kinuthia', 'Michael Michie', 'Stephanie Kasaon', 'Jean Wangari', 'Hankyul Baek', 'Jaewon Noh', 'Kihyuk Nam', 'Sang Seo', 'Sungpil Shin', 'Taewhi Lee', 'Yongsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['data leakage', 'privacy', 'fraud', 'cybersecurity', 'agentic-evaluations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15679</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2601.15652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a hybrid, interpretable hallucination-detection framework for LLMs that combines Predictive Coding (surprise vs internal priors) and Information Bottleneck (signal retention under perturbation) signals.&lt;/li&gt;&lt;li&gt;Proposes enhancements: Entity-Focused Uptake, Context Adherence, and a Falsifiability Score; demonstrates improved AUROC on HaluBench (baseline 0.8017, BASE 0.8274, IMPROVED 0.8669).&lt;/li&gt;&lt;li&gt;Claims strong data/compute efficiency (75x less training data, ~1000x faster inference) using lightweight &lt;1M-parameter supervised models, suitable for production deployment.&lt;/li&gt;&lt;li&gt;Reports a negative result: a Rationalization signal did not distinguish hallucinations, indicating models can produce coherent-but-false rationales (sycophancy).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manish Bhatt']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'model-robustness', 'interpretability', 'predictive-coding', 'information-bottleneck']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15652</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases</title><link>https://arxiv.org/abs/2601.15476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two reliability metrics (False Citation Rate, Fabricated Fact Rate) and an expert double-blind evaluation protocol for legal-domain LLM outputs.&lt;/li&gt;&lt;li&gt;Evaluates 2,700 judicial-style answers from 12 LLMs across 75 legal tasks, showing standalone generative models have high fabrication (FCR &gt; 30%).&lt;/li&gt;&lt;li&gt;Shows basic RAG reduces errors but still suffers misgrounding; an advanced RAG (embedding fine-tuning, re-ranking, self-correction) reduces fabrication to negligible levels (&lt;0.2%).&lt;/li&gt;&lt;li&gt;Concludes that rigorous, verification-focused retrieval-based architectures and traceability are required for trustworthy AI in high-stakes domains, and provides a reusable evaluation framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Dantart']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'evaluation metrics', 'legal AI', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15476</guid><pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 12 Feb 2026 23:22:10 +0000</lastBuildDate><item><title>Towards Privacy-Guaranteed Label Unlearning in Vertical Federated Learning: Few-Shot Forgetting without Disclosure</title><link>https://arxiv.org/abs/2410.10922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first method for label unlearning in Vertical Federated Learning (VFL), addressing labels as both inputs and sensitive information.&lt;/li&gt;&lt;li&gt;Uses representation-level manifold mixup to generate synthetic embeddings for unlearned and retained samples to enrich signals for forgetting.&lt;/li&gt;&lt;li&gt;Performs gradient-based label forgetting on augmented embeddings and a recovery-phase optimization to regain performance on retained data.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness and scalability across diverse datasets (images, 3D, medical images, text) while maintaining computational efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanlin Gu', 'Hong Xi Tae', 'Chee Seng Chan', 'Lixin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['label unlearning', 'vertical federated learning', 'privacy-preserving', 'manifold mixup']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10922</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection</title><link>https://arxiv.org/abs/2602.10042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fake-HR1, a hybrid-reasoning vision-language model that adaptively determines whether to invoke Chain-of-Thought (CoT) reasoning for synthetic image detection.&lt;/li&gt;&lt;li&gt;Introduces a two-stage training pipeline: Hybrid Fine-Tuning (HFT) for cold-start initialization and online reinforcement learning via Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to learn when to select reasoning modes.&lt;/li&gt;&lt;li&gt;Claims improved generative-sample detection performance and reasoning capability over existing LLM-based detectors while significantly reducing token usage and latency by avoiding unnecessary lengthy reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Xinkuan Sha', 'Fengchang Yu', 'Jingjing Liu', 'Jian Liu', 'Mingqi Fang', 'Chenfeng Zhang', 'Wei Lu']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-image-detection', 'forensic-detection', 'adaptive-reasoning', 'chain-of-thought', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10042</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors</title><link>https://arxiv.org/abs/2602.09740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vision components and sensors in Connected and Autonomous Vehicles (CAV) to produce a reference architecture for CAV vision systems (CAVVS).&lt;/li&gt;&lt;li&gt;Identifies attack surfaces within the CAVVS and elaborates concrete attack vectors against those surfaces.&lt;/li&gt;&lt;li&gt;Evaluates the impact of these attacks on confidentiality, integrity, and availability (CIA) of CAV vision functionality to inform security measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandeep Gupta', 'Roberto Passerone']&lt;/li&gt;&lt;li&gt;Tags: ['attack surface analysis', 'physical-world attacks', 'availability/integrity/confidentiality', 'autonomous vehicles security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09740</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</title><link>https://arxiv.org/abs/2602.11130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a catastrophic failure mode called 'Meltdown' in 3D diffusion transformers where tiny on-surface perturbations to input point clouds cause outputs to fragment into disconnected pieces.&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability (activation patching) to localize the failure to a single early denoising cross-attention activation and shows the singular-value spectrum / spectral entropy of that activation correlates with fragmentation.&lt;/li&gt;&lt;li&gt;Interprets the phenomenon as a symmetry-breaking bifurcation in diffusion reverse dynamics and introduces PowerRemap, a test-time control that stabilizes conditioning and prevents Meltdown across architectures, datasets, and denoising strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maximilian Plattner', 'Fabian Paischer', 'Johannes Brandstetter', 'Arturs Berzins']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'diffusion models', 'mechanistic interpretability', 'adversarial vulnerability', 'test-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11130</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks</title><link>https://arxiv.org/abs/2602.10780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIRE (Feature-space Inference-time REpair), an inference-time defense that mitigates backdoors by manipulating a sample's latent representations.&lt;/li&gt;&lt;li&gt;Hypothesizes that triggers correspond to structured directions in internal feature spaces and reverses those directions to neutralize poisoned inputs.&lt;/li&gt;&lt;li&gt;Evaluates FIRE on image benchmarks, showing low overhead and improved performance over existing runtime mitigation methods across attacks, datasets, and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrico Ahlers', 'Daniel Passon', 'Yannic Noller', 'Lars Grunske']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defenses', 'inference-time mitigation', 'latent-space manipulation', 'runtime security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10780</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration</title><link>https://arxiv.org/abs/2602.10750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecureScan, a triple-layer defensive framework combining heuristic filtering, a logistic regression classifier, and VirusTotal threat intelligence for triaging URLs, file hashes, and binaries.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency and real-world stability via threshold-based decision calibration and a gray-zone (0.45–0.55) for borderline cases to reduce false positives.&lt;/li&gt;&lt;li&gt;Reports strong empirical performance (93.1% accuracy, precision 0.87, recall 0.92) on benchmark datasets and argues that a lightweight statistical model plus external verification can rival more complex deep learning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rumman Firdos', 'Aman Dangi']&lt;/li&gt;&lt;li&gt;Tags: ['malware-detection', 'phishing-detection', 'threat-intelligence', 'defensive-security', 'logistic-regression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10750</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation</title><link>https://arxiv.org/abs/2602.10799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes remote-sensing (RS) hallucinations with an RS-specific taxonomy and introduces image-level hallucination to capture modality/resolution/scene-level errors beyond object-centric mistakes.&lt;/li&gt;&lt;li&gt;Constructs RSHalluEval (2,023 QA pairs) for benchmark evaluation and a compact checker fine-tuned on RSHalluCheck (15,396 QA pairs) enabling dual-mode checking (cloud auditing and local checking).&lt;/li&gt;&lt;li&gt;Provides domain-tailored mitigation resources (RSHalluShield, 30k QA pairs) and proposes training-free, plug-and-play defenses such as decoding-time logit correction and RS-aware prompting.&lt;/li&gt;&lt;li&gt;Reports up to 21.63 percentage-point improvement in hallucination-free rate across representative RS-MLLMs while maintaining competitive downstream task performance (RSVQA/RSVG).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihui Zhou', 'Yong Feng', 'Yanying Chen', 'Guofan Duan', 'Zhenxi Song', 'Mingliang Zhou', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'defense', 'benchmark', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10799</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL</title><link>https://arxiv.org/abs/2602.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniVL-Guard, a unified framework for omnibus vision-language forgery detection and fine-grained grounding (localization) across interleaved text, images, and videos.&lt;/li&gt;&lt;li&gt;Identifies a 'difficulty bias' in multi-task optimization where coarse veracity classification dominates training, degrading grounding performance.&lt;/li&gt;&lt;li&gt;Introduces Self-Evolving Chain-of-Thought (CoT) Generation to produce high-quality reasoning paths and Adaptive Reward Scaling Policy Optimization (ARSPO) to dynamically modulate reward scales and task weights for balanced RL-based training.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art detection and grounding performance with robust zero-shot generalization in out-of-domain scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinjie Shen', 'Jing Wu', 'Yaxiong Wang', 'Lechao Cheng', 'Shengeng Tang', 'Tianrui Hui', 'Nan Pu', 'Zhun Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['forgery detection', 'multimodal security', 'misinformation detection', 'reinforcement learning', 'grounding/localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10687</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VideoSTF: Stress-Testing Output Repetition in Video Large Language Models</title><link>https://arxiv.org/abs/2602.10639</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VideoSTF, a benchmark and framework to measure and stress-test output repetition (degeneration) in Video Large Language Models using n-gram based metrics and 10,000 videos with controlled temporal transformations.&lt;/li&gt;&lt;li&gt;Performs systematic testing across 10 advanced VideoLLMs, showing output repetition is widespread and highly sensitive to temporal perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates that simple temporal transformations can induce repetitive degeneration in a black-box setting, framing repetition as an exploitable security vulnerability.&lt;/li&gt;&lt;li&gt;Provides code and testbed for adversarial exploitation and stability-aware evaluation of video-language systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Cao', 'Wei Song', 'Shangzhi Xu', 'Jingling Xue', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['output-repetition', 'adversarial-exploration', 'robustness', 'black-box-vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10639</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images</title><link>https://arxiv.org/abs/2602.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents RealHD, a large high-quality dataset of &gt;730,000 images (real and AI-generated) across multiple categories and generation methods to improve detection generalization.&lt;/li&gt;&lt;li&gt;Generated images include outputs from state-of-the-art text-to-image, inpainting, refinement, and face-swapping methods; metadata includes generation method labels and inpainting masks.&lt;/li&gt;&lt;li&gt;Proposes a lightweight baseline detector using Non-Local Means noise entropy and demonstrates that models trained on RealHD achieve strong cross-model/generalization.&lt;/li&gt;&lt;li&gt;Dataset and code are released as a benchmark to evaluate and advance robust detection of AI-generated images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanzhe Yu', 'Yun Ye', 'Jintao Rong', 'Qi Xuan', 'Chen Ma']&lt;/li&gt;&lt;li&gt;Tags: ['Deepfake detection', 'Dataset / Benchmark', 'AI-generated image forensics', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10546</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images</title><link>https://arxiv.org/abs/2602.10425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a pipeline to synthesize Hallucination-Inducing Images (HIIs) that expose scene-conditioned hallucination patterns in vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Introduces the Masked-Object-Hallucination (MOH) benchmark to quantify VLM susceptibility to object hallucination when visual evidence is removed.&lt;/li&gt;&lt;li&gt;Uses HIIs to construct preference datasets for fine-grained alignment, showing up to 38% improvement on standard hallucination benchmarks while preserving general capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Yang', 'Zhenghui Guo', 'Yuke Wang', 'Omprakash Gnawali', 'Sheng Di', 'Chengming Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'robustness', 'benchmarking', 'counterfactual synthesis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10425</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks</title><link>https://arxiv.org/abs/2602.10343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates uncertainty-aware political deepfake detection using stochastic convolutional neural networks and calibrated probabilistic outputs.&lt;/li&gt;&lt;li&gt;Constructs a politically focused real-vs-synthetic image dataset and fine-tunes pretrained CNN backbones (ResNet-18, EfficientNet-B4).&lt;/li&gt;&lt;li&gt;Compares deterministic inference, single-pass stochastic prediction, Monte Carlo dropout, temperature scaling, and ensembles, reporting ROC-AUC, calibration metrics, and OOD (generator-disjoint) performance.&lt;/li&gt;&lt;li&gt;Demonstrates how calibrated uncertainty estimates enable risk-aware moderation policies and provides a confidence-band analysis to delineate when uncertainty adds operational value.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafael-Petru\\c{t} Gardo\\c{s}']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'uncertainty estimation', 'model calibration', 'defensive robustness', 'political disinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10343</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Low-Rank Defense Method for Adversarial Attack on Diffusion Models</title><link>https://arxiv.org/abs/2602.10319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Low-Rank Defense (LoRD), a defense strategy using low-rank adaptation (LoRA) modules with a merging idea and a balance parameter to detect and mitigate adversarial samples for Latent Diffusion Models (LDMs).&lt;/li&gt;&lt;li&gt;Integrates learned LoRD modules into a defense pipeline so LDMs fine-tuned on mixed adversarial and clean data can still generate high-quality images.&lt;/li&gt;&lt;li&gt;Evaluates method on facial and landscape image tasks, reporting substantially better defense performance than baseline methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxuan Zhu', 'Siyu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'defense', 'diffusion models', 'LoRA', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10319</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models</title><link>https://arxiv.org/abs/2602.10179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Vision-Centric Jailbreak Attack (VJA): a visual-to-visual jailbreak that encodes malicious instructions purely via visual prompts (marks, arrows, visual-text) to compromise image editing models.&lt;/li&gt;&lt;li&gt;Presents IESBench, a safety-oriented benchmark for evaluating image editing systems against visual jailbreaks and systematically measures vulnerabilities.&lt;/li&gt;&lt;li&gt;Reports high attack success rates on state-of-the-art commercial models (e.g., up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5).&lt;/li&gt;&lt;li&gt;Proposes a training-free defense based on introspective multimodal reasoning that substantially improves safety with negligible computational overhead and no auxiliary guard models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Hou', 'Yining Sun', 'Ruochong Jin', 'Haochen Han', 'Fangming Liu', 'Wai Kin Victor Chan', 'Alex Jinpeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-attack', 'image-editing-models', 'defense', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10179</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems</title><link>https://arxiv.org/abs/2602.10160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates end-to-end autonomous driving agents (Transfuser, Interfuser) under three black-box visual attacks in CARLA: physics-based acoustic blur, electromagnetic interference image distortion, and digital ghost-object perturbations.&lt;/li&gt;&lt;li&gt;Finds severe degradation in closed-loop driving performance (up to 99% drop in driving score) demonstrating major safety vulnerabilities in visual perception pipelines.&lt;/li&gt;&lt;li&gt;Proposes AD^2, a lightweight attention-based spatial–temporal attack detector for multi-camera inputs that outperforms baselines in detection accuracy and computational efficiency on CARLA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishan Sahu', 'Somnath Hazra', 'Somak Aditya', 'Soumyajit Dey']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'physical-attacks', 'sensor-manipulation', 'attack-detection', 'autonomous-driving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10160</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</title><link>https://arxiv.org/abs/2601.21963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies the evolving threat of LLM- and multimodal-generated misinformation and its effects on information ecosystems.&lt;/li&gt;&lt;li&gt;Introduces two practical tools: JudgeGPT (platform for evaluating human perception/detection of AI-generated news) and RogueGPT (controlled stimulus generation engine) to run experiments.&lt;/li&gt;&lt;li&gt;Reports empirical findings showing detection capabilities have improved but generation–detection arms race persists, and evaluates mitigation strategies including LLM-based detectors and inoculation.&lt;/li&gt;&lt;li&gt;Discusses dual-use risks and practical countermeasures for mitigating AI-generated misinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Loth', 'Martin Kappes', 'Marc-Oliver Pahl']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM-generated content', 'detection', 'inoculation', 'human-subjects / experimental tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21963</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and evaluates activation probe architectures as a misuse-mitigation/defense technique for large language models, focusing on production robustness.&lt;/li&gt;&lt;li&gt;Identifies a key failure mode: probes often fail to generalize from short-context to long-context inputs, and proposes new architectures that improve long-context handling.&lt;/li&gt;&lt;li&gt;Evaluates probes in cyber-offensive misuse scenarios across production-relevant distribution shifts (multi-turn conversations, long contexts, adaptive red teaming) and finds that architecture plus diverse training distributions are needed for broad generalization.&lt;/li&gt;&lt;li&gt;Shows practical deployment: probes paired with prompted classifiers yield cost-effective accuracy and have been deployed for misuse mitigation in Google’s Gemini; explores automated architecture/search improvements via AlphaEvolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['misuse mitigation', 'activation probes', 'red teaming', 'distribution shift', 'long-context robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning-Based Automated Adversarial Red-Teaming for Robustness Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2512.20677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates automated LLM red-teaming as a structured adversarial search and proposes a learning-driven framework combining meta-prompt-guided adversarial prompt generation with a hierarchical execution and detection pipeline.&lt;/li&gt;&lt;li&gt;Standardizes evaluation across six threat categories: reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation.&lt;/li&gt;&lt;li&gt;Evaluates on GPT-OSS-20B, discovering 47 vulnerabilities (21 high-severity, 12 previously undocumented), achieving a 3.9× higher discovery rate than manual red-teaming and 89% detection accuracy.&lt;/li&gt;&lt;li&gt;Claims improved scalability, reproducibility, coverage, and efficiency for large-scale LLM robustness evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhang Wei', 'Peilu Hu', 'Zhenyuan Wei', 'Chenwei Liang', 'Jing Luo', 'Ziyi Ni', 'Hao Yan', 'Li Mei', 'Shengning Lang', 'Kuan Lu', 'Ziqian Bi', 'Xi Xiao', 'Zhimo Han', 'Yangfan He', 'Yijin Wang', 'Yichao Zhang', 'Chen Yang', 'Junfeng Hao', 'Zhenyu Yu', 'Jiayi Gu', 'Riyang Bao', 'Mu-Jiang-Shan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['automated red-teaming', 'adversarial prompt generation', 'LLM robustness', 'vulnerability discovery', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20677</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SecureCode is a production-grade dataset of 2,185 multi-turn (4-turn) security training examples for instruction tuning, covering web app security (1,435 examples, OWASP Top 10 2021, CVE-grounded) and AI/ML security (750 examples, OWASP LLM Top 10 2025, 40+ frameworks).&lt;/li&gt;&lt;li&gt;Each example includes vulnerable and secure implementations with attack demonstrations, advanced probing, and defense-in-depth operational guidance (SIEM integration, infrastructure hardening, testing approaches), formatted for direct use in instruction-tuning pipelines.&lt;/li&gt;&lt;li&gt;Quality assurance combines automated validation with multi-agent specialist review (10,500+ assessments) and an 8-phase remediation pipeline; the release includes the dataset on Hugging Face, eight fine-tuned models (3B–20B), and a security-specific evaluation framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'code-generation', 'web-security', 'ml-security', 'instruction-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks</title><link>https://arxiv.org/abs/2602.05252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Copyright Detective, an interactive forensic system for detecting and analyzing potential copyright leakage in LLM outputs.&lt;/li&gt;&lt;li&gt;Combines multiple detection paradigms—content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification—into a unified extensible framework.&lt;/li&gt;&lt;li&gt;Supports interactive, iterative black-box auditing and evidence-oriented workflows to surface verbatim memorization and paraphrase-level leakage risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangwei Zhang', 'Jianing Zhu', 'Cheng Qian', 'Neil Gong', 'Rada Mihalcea', 'Zhaozhuo Xu', 'Jingrui He', 'Jiaqi Ma', 'Yun Huang', 'Chaowei Xiao', 'Bo Li', 'Ahmed Abbasi', 'Dongwon Lee', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data leakage', 'model auditing', 'jailbreak probing', 'copyright/privacy forensics', 'black-box vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05252</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adapter Merging Reactivates Latent Reasoning Traces: A Mechanism Analysis</title><link>https://arxiv.org/abs/2601.18350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how merging domain and instruction adapters in two-stage fine-tuning can reactivate explicit reasoning traces (trace leakage) in medical LLMs under strict decoding.&lt;/li&gt;&lt;li&gt;Introduces marker-forbidden, answer-only evaluations and a correctness-based direction; shows a rank-1 logit-space intervention along that direction modulates logits and can improve multiple-choice accuracy versus controls.&lt;/li&gt;&lt;li&gt;Provides layer-wise geometric evidence that domain and instruction adapters produce partially misaligned update directions, and presents a proof-of-concept geometry-aware merge that can reduce leakage and/or improve accuracy in a toy setting.&lt;/li&gt;&lt;li&gt;Delivers practical diagnostics and interventions aimed at safer adapter merging (i.e., mitigation of leakage).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyi Zou']&lt;/li&gt;&lt;li&gt;Tags: ['adapter merging', 'model leakage', 'LLM safety/defenses', 'mechanism analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18350</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title><link>https://arxiv.org/abs/2512.08892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAGLens, a lightweight hallucination detector for Retrieval-Augmented Generation that leverages sparse autoencoders to disentangle LLM internal activations.&lt;/li&gt;&lt;li&gt;Uses an information-based feature selection pipeline and additive feature modeling to identify activation features specifically correlated with RAG hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates improved hallucination detection performance vs. existing methods, offers interpretable rationales, and enables post-hoc mitigation of unfaithful RAG outputs.&lt;/li&gt;&lt;li&gt;Provides analyses justifying design choices and insights into how hallucination-related signals are distributed within LLMs; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangzhi Xiong', 'Zhenghao He', 'Bohan Liu', 'Sanchit Sinha', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG (retrieval-augmented generation)', 'model safety/defense', 'interpretability', 'sparse autoencoders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08892</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title><link>https://arxiv.org/abs/2503.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVATAR, a novel jailbreak attack that uses benign but logically related metaphors as seeds to induce LLMs to produce or 'calibrate' into harmful content.&lt;/li&gt;&lt;li&gt;AVATAR adaptively selects metaphorical prompts to drive the model's reasoning process, causing either direct harmful outputs or calibration of residuals toward professional harmful content.&lt;/li&gt;&lt;li&gt;Demonstrates strong effectiveness and transferability, achieving state-of-the-art attack success rates across multiple advanced LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zenghao Duan', 'Teli Liu', 'Min Liu', 'Zhiyi Yin', 'Jingyu Lei', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-prompts', 'prompt-injection', 'transferability', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00038</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel side-channel in speculative decoding for LLM serving where per-iteration token counts or packet sizes reveal input-dependent patterns of correct/incorrect speculations.&lt;/li&gt;&lt;li&gt;Demonstrates high-accuracy fingerprinting of user queries (e.g., &gt;75% across schemes at temperature 0.3; detailed results for REST/LADE/BiLD/EAGLE) and ability to exfiltrate confidential datastore contents at &gt;25 tokens/sec.&lt;/li&gt;&lt;li&gt;Evaluates attacks on research and production vLLM serving frameworks and proposes mitigations (packet padding, iteration-wise token aggregation), which are empirically assessed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel', 'speculative-decoding', 'model-serving', 'privacy-leakage', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis</title><link>https://arxiv.org/abs/2602.10453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of Prompt Injection (PI) threats for autonomous LLM agents: taxonomies of attack payload generation (heuristic vs. optimization) and defense interventions (text, model, execution levels).&lt;/li&gt;&lt;li&gt;Identifies a major gap in existing defenses/benchmarks: poor handling of context-dependent tasks where agents must rely on runtime environment inputs.&lt;/li&gt;&lt;li&gt;Introduces AgentPI, a benchmark for evaluating agent behavior under context-dependent interactions, and empirically shows trade-offs among trustworthiness, utility, and latency; many defenses fail when contextual inputs are necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Xinfeng Li', 'Chong Xiang', 'Jinghuai Zhang', 'Ying Li', 'Lixia Zhang', 'Xiaofeng Wang', 'Yuan Tian']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM agents', 'attacks and defenses', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10453</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment</title><link>https://arxiv.org/abs/2602.10161</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies cross-modal safety vulnerabilities in omni-modal LLMs via a new AdvBench-Omni attack dataset and a modality-semantics decoupling principle.&lt;/li&gt;&lt;li&gt;Mechanistic analysis discovers a Mid-layer Dissolution phenomenon and a modal-invariant 'pure refusal' direction driven by refusal vector magnitude shrinkage.&lt;/li&gt;&lt;li&gt;Proposes OmniSteer: extracts a golden refusal vector with SVD and uses lightweight adapters to adaptively modulate intervention intensity, improving refusal success rates while preserving multimodal capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Wang', 'Zherui Li', 'Zhenhong Zhou', 'Yitong Zhang', 'Yan Mi', 'Kun Yang', 'Yiming Zhang', 'Junhao Dong', 'Zhongxiang Sun', 'Qiankun Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'adversarial attacks/benchmarks', 'defense/mitigation', 'model internals/interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10161</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reverse-Engineering Model Editing on Language Models</title><link>https://arxiv.org/abs/2602.10134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a vulnerability in locate-then-edit model editing methods: low-rank parameter updates leak information about edited subjects via their row-space "fingerprint."&lt;/li&gt;&lt;li&gt;Presents KSTER, a two-stage reverse-engineering attack (KeySpace Reconstruction then Entropy Reduction) that recovers edited subjects and reconstructs semantic edit prompts using spectral analysis and entropy-based techniques.&lt;/li&gt;&lt;li&gt;Demonstrates high success rates across multiple LLMs in experiments, and proposes a defense called subspace camouflage that injects semantic decoys to obfuscate update fingerprints while preserving editing utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyu Sun', 'Minrui Luo', 'Yu Wang', 'Zhili Chen', 'Tianxing He']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'parameter-update leakage', 'reverse-engineering attack', 'privacy / data extraction', 'defense (subspace camouflage)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10134</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away</title><link>https://arxiv.org/abs/2602.11096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeThink, an inference-time defense that monitors chain-of-thought traces with a safety reward model and conditionally injects a short corrective prefix when safety thresholds are breached.&lt;/li&gt;&lt;li&gt;Evaluates SafeThink on six open-source multimodal reasoning models and four jailbreak benchmarks, reporting 30–60% reductions in attack success rates while maintaining reasoning performance.&lt;/li&gt;&lt;li&gt;Key empirical claim: intervening within the first 1–3 reasoning steps is typically sufficient to steer generations back to safe completions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumya Suvra Ghosal', 'Souradip Chakraborty', 'Vaibhav Singh', 'Furong Huang', 'Dinesh Manocha', 'Amrit Singh Bedi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'inference-time defense', 'safety alignment', 'chain-of-thought', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11096</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Embedding Inversion via Conditional Masked Diffusion Language Models</title><link>https://arxiv.org/abs/2602.11047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes framing embedding inversion as conditional masked diffusion to recover tokens in parallel via iterative denoising instead of autoregressive decoding.&lt;/li&gt;&lt;li&gt;Conditions a masked diffusion language model on target embeddings using adaptive layer normalization, requiring only 8 forward passes of a 78M-parameter model without access to the encoder.&lt;/li&gt;&lt;li&gt;Reports strong reconstruction on 32-token sequences across three embedding models (81.3% token accuracy, 0.87 cosine similarity).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['embedding-inversion', 'model-inversion', 'privacy-attack', 'diffusion-models', 'training-data-extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11047</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Language Model Inversion through End-to-End Differentiation</title><link>https://arxiv.org/abs/2602.11044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates LM inversion (finding prompts that produce a desired output) as a gradient-based optimization problem.&lt;/li&gt;&lt;li&gt;Introduces an algorithm to make a frozen language model end-to-end differentiable by treating inputs as sequences of distributions over tokens.&lt;/li&gt;&lt;li&gt;Shows that optimized continuous prompts (lengths 10 and 80) can reliably produce target outputs of length 20 on several white-box LMs via gradient descent.&lt;/li&gt;&lt;li&gt;Provides experiments and ablations demonstrating efficiency and effectiveness of the proposed DLM-powered inversion approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Yandoka Denamgana\\"i', 'Kartic Subr']&lt;/li&gt;&lt;li&gt;Tags: ['model-inversion', 'prompt-optimization', 'white-box-attacks', 'adversarial-technique', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11044</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Knowledge Editing for Detoxification</title><link>https://arxiv.org/abs/2602.10504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robustness-oriented evaluation framework for Knowledge-Editing (KE)-based detoxification across optimisation robustness, compositional robustness, and cross-lingual robustness.&lt;/li&gt;&lt;li&gt;Identifies pseudo-detoxification as a common failure mode where lowered toxicity scores stem from degenerate generation rather than true suppression of unsafe content.&lt;/li&gt;&lt;li&gt;Shows that effectiveness degrades when editing multiple unsafe behaviours jointly and that monolingual/cross-lingual detoxification success depends on specific model-method combinations.&lt;/li&gt;&lt;li&gt;Concludes KE-based detoxification is robust only for certain models, a limited number of objectives, and a subset of languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Dong', 'Shiyi Tang', 'Ziyan Peng', 'Guanyi Chen', 'Tingting He']&lt;/li&gt;&lt;li&gt;Tags: ['detoxification', 'knowledge editing', 'robustness evaluation', 'safety defenses', 'failure modes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10504</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</title><link>https://arxiv.org/abs/2602.10382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Mechanistic study of language-switching backdoors in LLMs (GAPperon family, 1B–24B), using activation patching to localize trigger effects.&lt;/li&gt;&lt;li&gt;Finds trigger formation occurs in early layers (≈7.5–25% depth) and identifies specific attention heads that process trigger information.&lt;/li&gt;&lt;li&gt;Key result: trigger-activated heads substantially overlap with heads that naturally encode output language (Jaccard 0.18–0.66), implying backdoors co-opt existing language circuits and are not isolated.&lt;/li&gt;&lt;li&gt;Implication for defenses: monitoring/mitigating should target known functional components (e.g., language-encoding heads) rather than only searching for hidden circuits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Th\\'eo Lasnier", 'Wissam Antoun', 'Francis Kulumba', "Djam\\'e Seddah"]&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'mechanistic interpretability', 'LLM security', 'activation patching', 'attention heads']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10382</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility</title><link>https://arxiv.org/abs/2602.03402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies multimodal jailbreak vulnerabilities in vision-language models (VLMs) and notes that visual inputs often weaken LLM-like risk signals.&lt;/li&gt;&lt;li&gt;Proposes Risk Awareness Injection (RAI), a lightweight, training-free method that builds an Unsafe Prototype Subspace from language embeddings and modulates high-risk visual tokens to amplify safety signals.&lt;/li&gt;&lt;li&gt;Shows RAI restores LLM-like unsafe-content detection from visual inputs and substantially reduces attack success rates on multiple jailbreak benchmarks while preserving task utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengxuan Wang', 'Yuxin Chen', 'Gang Xu', 'Tao He', 'Hongjie Jiang', 'Ming Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal jailbreak', 'defense', 'vision-language models', 'safety calibration', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03402</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning</title><link>https://arxiv.org/abs/2512.23171</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedORA, a primal-dual optimization algorithm for sample and label unlearning in vertical federated learning (VFL) to realize the "right to be forgotten."&lt;/li&gt;&lt;li&gt;Introduces an unlearning loss that promotes classification uncertainty, an adaptive step size, and an asymmetric batching scheme to reduce computation and communication overhead across parties.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees bounding the model difference between FedORA and retraining from scratch, and demonstrates comparable unlearning effectiveness and utility on tabular and image datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Jiang', 'Xindi Tong', 'Ziyao Liu', 'Xiaoxi Zhang', 'Kwok-Yan Lam', 'Chee Wei Tan']&lt;/li&gt;&lt;li&gt;Tags: ['federated unlearning', 'privacy', 'vertical federated learning', 'primal-dual optimization', 'data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23171</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SecureCode is a production-grade dataset of 2,185 multi-turn (4-turn) security training examples for instruction tuning, covering web app security (1,435 examples, OWASP Top 10 2021, CVE-grounded) and AI/ML security (750 examples, OWASP LLM Top 10 2025, 40+ frameworks).&lt;/li&gt;&lt;li&gt;Each example includes vulnerable and secure implementations with attack demonstrations, advanced probing, and defense-in-depth operational guidance (SIEM integration, infrastructure hardening, testing approaches), formatted for direct use in instruction-tuning pipelines.&lt;/li&gt;&lt;li&gt;Quality assurance combines automated validation with multi-agent specialist review (10,500+ assessments) and an 8-phase remediation pipeline; the release includes the dataset on Hugging Face, eight fine-tuned models (3B–20B), and a security-specific evaluation framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'code-generation', 'web-security', 'ml-security', 'instruction-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds</title><link>https://arxiv.org/abs/2509.04345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AUDETER, a large-scale, diverse deepfake audio dataset: ~4,500 hours, 3 million clips generated by 11 recent TTS models and 10 vocoders to better represent open-world synthesis diversity.&lt;/li&gt;&lt;li&gt;Identifies negative transfer issues in standard binary supervised training when faced with highly heterogeneous deepfake sources and proposes a curriculum-learning-based training strategy to mitigate this.&lt;/li&gt;&lt;li&gt;Provides extensive evaluations showing many existing detectors struggle to generalise to novel deepfakes and real human speech in AUDETER; XLR-based detectors trained on AUDETER obtain strong cross-domain performance (EER 1.87% on In-the-Wild).&lt;/li&gt;&lt;li&gt;Releases AUDETER publicly to enable systematic evaluation and training for more robust deepfake-audio detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qizhou Wang', 'Hanxun Huang', 'Guansong Pang', 'Sarah Erfani', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-audio', 'dataset', 'detection', 'robustness', 'curriculum-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04345</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel side-channel in speculative decoding for LLM serving where per-iteration token counts or packet sizes reveal input-dependent patterns of correct/incorrect speculations.&lt;/li&gt;&lt;li&gt;Demonstrates high-accuracy fingerprinting of user queries (e.g., &gt;75% across schemes at temperature 0.3; detailed results for REST/LADE/BiLD/EAGLE) and ability to exfiltrate confidential datastore contents at &gt;25 tokens/sec.&lt;/li&gt;&lt;li&gt;Evaluates attacks on research and production vLLM serving frameworks and proposes mitigations (packet padding, iteration-wise token aggregation), which are empirically assessed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel', 'speculative-decoding', 'model-serving', 'privacy-leakage', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and evaluates activation probe architectures as a misuse-mitigation/defense technique for large language models, focusing on production robustness.&lt;/li&gt;&lt;li&gt;Identifies a key failure mode: probes often fail to generalize from short-context to long-context inputs, and proposes new architectures that improve long-context handling.&lt;/li&gt;&lt;li&gt;Evaluates probes in cyber-offensive misuse scenarios across production-relevant distribution shifts (multi-turn conversations, long contexts, adaptive red teaming) and finds that architecture plus diverse training distributions are needed for broad generalization.&lt;/li&gt;&lt;li&gt;Shows practical deployment: probes paired with prompted classifiers yield cost-effective accuracy and have been deployed for misuse mitigation in Google’s Gemini; explores automated architecture/search improvements via AlphaEvolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['misuse mitigation', 'activation probes', 'red teaming', 'distribution shift', 'long-context robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provably Optimal Reinforcement Learning under Safety Filtering</title><link>https://arxiv.org/abs/2510.18082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces safety-critical MDPs (SC-MDPs) that require categorical avoidance of catastrophic failure states and formalizes safety filters as part of a filtered MDP.&lt;/li&gt;&lt;li&gt;Proves that a sufficiently permissive safety filter enforces categorical safety while preserving standard RL convergence and asymptotic performance—optimality in the filtered MDP equals the best safe policy in the SC-MDP.&lt;/li&gt;&lt;li&gt;Provides empirical validation in Safety Gymnasium showing zero safety violations during training and matching/exceeding final performance of unfiltered baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Donggeon David Oh', 'Duy P. Nguyen', 'Haimin Hu', 'Jaime F. Fisac']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'safety filters / guardrails', 'provable guarantees', 'runtime safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18082</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Law of Data Reconstruction for Random Features (and Beyond)</title><link>https://arxiv.org/abs/2509.22214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves a 'law of data reconstruction' showing that, in the random features model, when the number of parameters p exceeds d·n (data dimensionality times number of samples), the feature-space subspace contains enough information to identify individual training samples in input space.&lt;/li&gt;&lt;li&gt;Proposes an optimization-based method to reconstruct the training dataset from model parameters and analyzes conditions under which reconstruction is possible.&lt;/li&gt;&lt;li&gt;Empirically validates the reconstruction method on multiple architectures (random features, two-layer fully-connected, deep residual networks), demonstrating dataset recovery as p surpasses the dn threshold.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonardo Iurada', 'Simone Bombari', 'Tatiana Tommasi', 'Marco Mondelli']&lt;/li&gt;&lt;li&gt;Tags: ['training data extraction', 'model inversion', 'memorization', 'privacy attack', 'data reconstruction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22214</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Privacy-Guaranteed Label Unlearning in Vertical Federated Learning: Few-Shot Forgetting without Disclosure</title><link>https://arxiv.org/abs/2410.10922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first method for label unlearning in Vertical Federated Learning (VFL), addressing labels as both inputs and sensitive information.&lt;/li&gt;&lt;li&gt;Uses representation-level manifold mixup to generate synthetic embeddings for unlearned and retained samples to enrich signals for forgetting.&lt;/li&gt;&lt;li&gt;Performs gradient-based label forgetting on augmented embeddings and a recovery-phase optimization to regain performance on retained data.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness and scalability across diverse datasets (images, 3D, medical images, text) while maintaining computational efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanlin Gu', 'Hong Xi Tae', 'Chee Seng Chan', 'Lixin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['label unlearning', 'vertical federated learning', 'privacy-preserving', 'manifold mixup']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10922</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration</title><link>https://arxiv.org/abs/2602.10750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecureScan, a triple-layer defensive framework combining heuristic filtering, a logistic regression classifier, and VirusTotal threat intelligence for triaging URLs, file hashes, and binaries.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency and real-world stability via threshold-based decision calibration and a gray-zone (0.45–0.55) for borderline cases to reduce false positives.&lt;/li&gt;&lt;li&gt;Reports strong empirical performance (93.1% accuracy, precision 0.87, recall 0.92) on benchmark datasets and argues that a lightweight statistical model plus external verification can rival more complex deep learning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rumman Firdos', 'Aman Dangi']&lt;/li&gt;&lt;li&gt;Tags: ['malware-detection', 'phishing-detection', 'threat-intelligence', 'defensive-security', 'logistic-regression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10750</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks</title><link>https://arxiv.org/abs/2602.10478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GPU-Fuzz, a fuzzer that models DL operator parameters as formal constraints and uses a constraint solver to generate tests targeting GPU kernel boundary conditions.&lt;/li&gt;&lt;li&gt;Systematically generates test cases to probe memory errors in GPU kernels of deep learning frameworks by focusing on error-prone parameter combinations.&lt;/li&gt;&lt;li&gt;Applied to PyTorch, TensorFlow, and PaddlePaddle and discovered 13 previously unknown GPU memory bugs, demonstrating practical effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Li', 'Hongyi Lu', 'Yanan Guo', 'Zhenkai Zhang', 'Shuai Wang', 'Fengwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['fuzzing', 'vulnerability discovery', 'GPU kernels', 'deep learning frameworks', 'memory safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10478</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks</title><link>https://arxiv.org/abs/2602.10343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates uncertainty-aware political deepfake detection using stochastic convolutional neural networks and calibrated probabilistic outputs.&lt;/li&gt;&lt;li&gt;Constructs a politically focused real-vs-synthetic image dataset and fine-tunes pretrained CNN backbones (ResNet-18, EfficientNet-B4).&lt;/li&gt;&lt;li&gt;Compares deterministic inference, single-pass stochastic prediction, Monte Carlo dropout, temperature scaling, and ensembles, reporting ROC-AUC, calibration metrics, and OOD (generator-disjoint) performance.&lt;/li&gt;&lt;li&gt;Demonstrates how calibrated uncertainty estimates enable risk-aware moderation policies and provides a confidence-band analysis to delineate when uncertainty adds operational value.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafael-Petru\\c{t} Gardo\\c{s}']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'uncertainty estimation', 'model calibration', 'defensive robustness', 'political disinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10343</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</title><link>https://arxiv.org/abs/2602.11130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a catastrophic failure mode called 'Meltdown' in 3D diffusion transformers where tiny on-surface perturbations to input point clouds cause outputs to fragment into disconnected pieces.&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability (activation patching) to localize the failure to a single early denoising cross-attention activation and shows the singular-value spectrum / spectral entropy of that activation correlates with fragmentation.&lt;/li&gt;&lt;li&gt;Interprets the phenomenon as a symmetry-breaking bifurcation in diffusion reverse dynamics and introduces PowerRemap, a test-time control that stabilizes conditioning and prevents Meltdown across architectures, datasets, and denoising strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maximilian Plattner', 'Fabian Paischer', 'Johannes Brandstetter', 'Arturs Berzins']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'diffusion models', 'mechanistic interpretability', 'adversarial vulnerability', 'test-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11130</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Token-Efficient Change Detection in LLM APIs</title><link>https://arxiv.org/abs/2602.11083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Border Inputs—inputs for which the model has multiple plausible top tokens—and shows they enable sensitive black-box detection of remote model changes.&lt;/li&gt;&lt;li&gt;Derives theoretical connections between change-detection performance, the model Jacobian, and Fisher information, especially in low-temperature regimes.&lt;/li&gt;&lt;li&gt;Proposes the Black-Box Border Input Tracking (B3IT) algorithm to monitor LLM endpoints using only output tokens (strict black-box) and with far lower query cost.&lt;/li&gt;&lt;li&gt;Presents in-vivo and in-vitro experiments claiming parity with grey-box methods while reducing monitoring cost by ~30× for tested non-reasoning endpoints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Timoth\\'ee Chauvin", "Cl\\'ement Lalanne", 'Erwan Le Merrer', 'Jean-Michel Loubes', 'Fran\\c{c}ois Ta\\"iani', 'Gilles Tredan']&lt;/li&gt;&lt;li&gt;Tags: ['model-monitoring', 'black-box-testing', 'model-integrity-detection', 'statistical-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11083</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution</title><link>https://arxiv.org/abs/2602.11079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes activation-based data attribution that traces model behaviors to specific post-training datapoints using activation-difference vectors and cosine similarity.&lt;/li&gt;&lt;li&gt;Validates attributions causally by retraining with modified data and uses clustering of behavior-datapoint similarity to discover emergent harmful behaviors unsupervised.&lt;/li&gt;&lt;li&gt;Applied to a production DPO-trained model (OLMo 2) to identify and mitigate 'distractor-triggered compliance'—filtering top datapoints reduced the behavior by 63% and relabeling achieved 78%.&lt;/li&gt;&lt;li&gt;Method outperforms gradient-based attribution and LLM-judge baselines while being an order of magnitude cheaper, and introduces an in-the-wild benchmark for safety techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Frank Xiao', 'Santiago Aranguri']&lt;/li&gt;&lt;li&gt;Tags: ['data-attribution', 'model-safety', 'post-training-mitigation', 'emergent-behaviors', 'training-data-contamination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11079</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging</title><link>https://arxiv.org/abs/2602.11020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies same-source multi-view (two image-like views: OHLCV chart and technical-indicator matrix) financial forecasting and how fusion affects predictive performance under label-noise regimes.&lt;/li&gt;&lt;li&gt;Introduces an ex-post minimum-movement filter to create stabilized evaluation subsets and analyzes clean-performance trade-offs between early fusion (channel stacking) and late fusion (dual encoders + fusion head).&lt;/li&gt;&lt;li&gt;Evaluates adversarial robustness using FGSM and PGD under two threat models—view-constrained (one view perturbed) and joint (both views perturbed)—and reports severe vulnerability at small L-infinity budgets with strong view asymmetry.&lt;/li&gt;&lt;li&gt;Finds late fusion improves robustness against view-constrained attacks and yields dominant clean-performance gains, but joint attacks still cause substantial worst-case degradation; cross-view consistency regularization has secondary, backbone-dependent effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'adversarial-attacks', 'multimodal-fusion', 'financial-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11020</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks</title><link>https://arxiv.org/abs/2602.10780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIRE (Feature-space Inference-time REpair), an inference-time defense that mitigates backdoors by manipulating a sample's latent representations.&lt;/li&gt;&lt;li&gt;Hypothesizes that triggers correspond to structured directions in internal feature spaces and reverses those directions to neutralize poisoned inputs.&lt;/li&gt;&lt;li&gt;Evaluates FIRE on image benchmarks, showing low overhead and improved performance over existing runtime mitigation methods across attacks, datasets, and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrico Ahlers', 'Daniel Passon', 'Yannic Noller', 'Lars Grunske']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defenses', 'inference-time mitigation', 'latent-space manipulation', 'runtime security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10780</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Collaborative Threshold Watermarking</title><link>https://arxiv.org/abs/2602.10765</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces (t,K)-threshold watermarking for federated learning so that only coalitions of at least t clients can reconstruct a watermark key and verify model provenance.&lt;/li&gt;&lt;li&gt;Uses secret-sharing of the watermark key and verification protocols that do not reveal the key in the clear; instantiated in a white-box setting.&lt;/li&gt;&lt;li&gt;Evaluated on image classification: watermark detectable at scale (K=128) with minimal accuracy loss and remains above detection threshold under attacks including adaptive fine-tuning using up to 20% of the training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tameem Bakr', 'Anish Ambreth', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'watermarking', 'model-provenance', 'threshold-cryptography', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10765</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generative clinical time series models trained on moderate amounts of patient data are privacy preserving</title><link>https://arxiv.org/abs/2602.10631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Audits state-of-the-art generative models for multivariate clinical time series (trained on MIMIC-IV) using a battery of established privacy attacks.&lt;/li&gt;&lt;li&gt;Performs cross-dataset attack using eICU against a generator trained on MIMIC-IV to test membership/extraction risks.&lt;/li&gt;&lt;li&gt;Finds that established privacy attacks are largely ineffective when synthetic generators are trained on sufficiently large datasets.&lt;/li&gt;&lt;li&gt;Analyzes the application of differential privacy (DP) to these generators and concludes DP would likely degrade utility without producing meaningful privacy gains in this setting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rustam Zhumagambetov', 'Niklas Giesa', 'Sebastian D. Boie', 'Stefan Haufe']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'membership-inference', 'differential-privacy', 'synthetic-data', 'medical-time-series']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10631</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling</title><link>https://arxiv.org/abs/2602.10623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bayesian Non-Negative Reward Model (BNRM) that integrates non-negative factor analysis with a Bradley–Terry preference model to represent rewards via sparse, non-negative latent factors.&lt;/li&gt;&lt;li&gt;Uses instance-specific latent variables for disentangled reward representations and global sparsity as an implicit debiasing mechanism to suppress spurious correlations that enable reward hacking.&lt;/li&gt;&lt;li&gt;Implements an amortized variational inference network conditioned on deep LLM representations to scale the approach and enable end-to-end training.&lt;/li&gt;&lt;li&gt;Empirical results show reduced reward over-optimization (mitigating reward hacking), improved robustness under distribution shift, and more interpretable reward decompositions versus strong baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibin Duan', 'Guowei Rong', 'Zhuo Li', 'Bo Chen', 'Mingyuan Zhou', 'Dandan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'RLHF', 'reward modeling', 'robustness/defense', 'Bayesian methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10623</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction</title><link>https://arxiv.org/abs/2602.10228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISM, a differentially private synthetic-data mechanism that allocates privacy budget in a task-aware way across three regimes: causal (known parents of Y), graphical (Bayesian network known), and predictive (no structure known).&lt;/li&gt;&lt;li&gt;Constructs targeted summary statistics, optimizes budget allocation to minimize an upper bound on prediction error, synthesizes data via graphical-model inference, and proves end-to-end DP guarantees and risk bounds.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved predictive accuracy and robustness to distribution shift (e.g., targeting causal parents preserves AUC while correlation-based selection can collapse to chance).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Asiaee', 'Chao Yan', 'Zachary B. Abrams', 'Bradley A. Malin']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'synthetic-data', 'privacy-preserving', 'robustness', 'causal-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10228</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection</title><link>https://arxiv.org/abs/2602.10042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fake-HR1, a hybrid-reasoning vision-language model that adaptively determines whether to invoke Chain-of-Thought (CoT) reasoning for synthetic image detection.&lt;/li&gt;&lt;li&gt;Introduces a two-stage training pipeline: Hybrid Fine-Tuning (HFT) for cold-start initialization and online reinforcement learning via Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to learn when to select reasoning modes.&lt;/li&gt;&lt;li&gt;Claims improved generative-sample detection performance and reasoning capability over existing LLM-based detectors while significantly reducing token usage and latency by avoiding unnecessary lengthy reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Xinkuan Sha', 'Fengchang Yu', 'Jingjing Liu', 'Jian Liu', 'Mingqi Fang', 'Chenfeng Zhang', 'Wei Lu']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-image-detection', 'forensic-detection', 'adaptive-reasoning', 'chain-of-thought', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10042</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection</title><link>https://arxiv.org/abs/2602.09015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIC-Trap4Phish, a unified multi-format dataset of malicious and benign attachments across Word, Excel, PDF, HTML, and QR code images aimed at phishing/quishing detection.&lt;/li&gt;&lt;li&gt;Proposes an execution-free static feature pipeline for the first four file types (structural, lexical, metadata) with feature selection via SHAP and feature importance, evaluated using lightweight ML models (Random Forest, XGBoost, Decision Tree).&lt;/li&gt;&lt;li&gt;Implements QR-code (quishing) detection using both image-based CNN models and lexical analysis of decoded URLs with lightweight language models; reports high detection accuracy across formats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatemeh Nejati', 'Mahdi Rabbani', 'Morteza Eskandarian', 'Mansur Mirani', 'Gunjan Piya', 'Igor Opushnyev', 'Ali A. Ghorbani', 'Sajjad Dadkhah']&lt;/li&gt;&lt;li&gt;Tags: ['phishing-detection', 'dataset', 'static-analysis', 'quishing', 'machine-learning-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09015</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</title><link>https://arxiv.org/abs/2601.21963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies the evolving threat of LLM- and multimodal-generated misinformation and its effects on information ecosystems.&lt;/li&gt;&lt;li&gt;Introduces two practical tools: JudgeGPT (platform for evaluating human perception/detection of AI-generated news) and RogueGPT (controlled stimulus generation engine) to run experiments.&lt;/li&gt;&lt;li&gt;Reports empirical findings showing detection capabilities have improved but generation–detection arms race persists, and evaluates mitigation strategies including LLM-based detectors and inoculation.&lt;/li&gt;&lt;li&gt;Discusses dual-use risks and practical countermeasures for mitigating AI-generated misinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Loth', 'Martin Kappes', 'Marc-Oliver Pahl']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM-generated content', 'detection', 'inoculation', 'human-subjects / experimental tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21963</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adapter Merging Reactivates Latent Reasoning Traces: A Mechanism Analysis</title><link>https://arxiv.org/abs/2601.18350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how merging domain and instruction adapters in two-stage fine-tuning can reactivate explicit reasoning traces (trace leakage) in medical LLMs under strict decoding.&lt;/li&gt;&lt;li&gt;Introduces marker-forbidden, answer-only evaluations and a correctness-based direction; shows a rank-1 logit-space intervention along that direction modulates logits and can improve multiple-choice accuracy versus controls.&lt;/li&gt;&lt;li&gt;Provides layer-wise geometric evidence that domain and instruction adapters produce partially misaligned update directions, and presents a proof-of-concept geometry-aware merge that can reduce leakage and/or improve accuracy in a toy setting.&lt;/li&gt;&lt;li&gt;Delivers practical diagnostics and interventions aimed at safer adapter merging (i.e., mitigation of leakage).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyi Zou']&lt;/li&gt;&lt;li&gt;Tags: ['adapter merging', 'model leakage', 'LLM safety/defenses', 'mechanism analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18350</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and evaluates activation probe architectures as a misuse-mitigation/defense technique for large language models, focusing on production robustness.&lt;/li&gt;&lt;li&gt;Identifies a key failure mode: probes often fail to generalize from short-context to long-context inputs, and proposes new architectures that improve long-context handling.&lt;/li&gt;&lt;li&gt;Evaluates probes in cyber-offensive misuse scenarios across production-relevant distribution shifts (multi-turn conversations, long contexts, adaptive red teaming) and finds that architecture plus diverse training distributions are needed for broad generalization.&lt;/li&gt;&lt;li&gt;Shows practical deployment: probes paired with prompted classifiers yield cost-effective accuracy and have been deployed for misuse mitigation in Google’s Gemini; explores automated architecture/search improvements via AlphaEvolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['misuse mitigation', 'activation probes', 'red teaming', 'distribution shift', 'long-context robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SecureCode is a production-grade dataset of 2,185 multi-turn (4-turn) security training examples for instruction tuning, covering web app security (1,435 examples, OWASP Top 10 2021, CVE-grounded) and AI/ML security (750 examples, OWASP LLM Top 10 2025, 40+ frameworks).&lt;/li&gt;&lt;li&gt;Each example includes vulnerable and secure implementations with attack demonstrations, advanced probing, and defense-in-depth operational guidance (SIEM integration, infrastructure hardening, testing approaches), formatted for direct use in instruction-tuning pipelines.&lt;/li&gt;&lt;li&gt;Quality assurance combines automated validation with multi-agent specialist review (10,500+ assessments) and an 8-phase remediation pipeline; the release includes the dataset on Hugging Face, eight fine-tuned models (3B–20B), and a security-specific evaluation framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'code-generation', 'web-security', 'ml-security', 'instruction-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title><link>https://arxiv.org/abs/2512.08892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAGLens, a lightweight hallucination detector for Retrieval-Augmented Generation that leverages sparse autoencoders to disentangle LLM internal activations.&lt;/li&gt;&lt;li&gt;Uses an information-based feature selection pipeline and additive feature modeling to identify activation features specifically correlated with RAG hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates improved hallucination detection performance vs. existing methods, offers interpretable rationales, and enables post-hoc mitigation of unfaithful RAG outputs.&lt;/li&gt;&lt;li&gt;Provides analyses justifying design choices and insights into how hallucination-related signals are distributed within LLMs; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangzhi Xiong', 'Zhenghao He', 'Bohan Liu', 'Sanchit Sinha', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG (retrieval-augmented generation)', 'model safety/defense', 'interpretability', 'sparse autoencoders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08892</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework</title><link>https://arxiv.org/abs/2511.21448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a labeled email dataset distinguishing phishing, spam, and legitimate messages, with labels for human- vs LLM-generated content and annotations for emotional appeals and attacker motivations.&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs on identifying emotional/motivational cues and uses the most reliable model to annotate the full dataset.&lt;/li&gt;&lt;li&gt;Evaluates classification robustness by rephrasing emails using LLMs and measuring a state-of-the-art LLM's performance against expert-labeled ground truth, finding strong phishing detection but persistent spam vs legitimate confusion.&lt;/li&gt;&lt;li&gt;Releases code, templates, and resources to support development of AI-assisted email security systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebeka Toth', 'Tamas Bisztray', 'Richard Dubniczky']&lt;/li&gt;&lt;li&gt;Tags: ['phishing-detection', 'spam-detection', 'dataset', 'LLM-security', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21448</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</title><link>https://arxiv.org/abs/2511.10222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SACRED-Bench, a red-teaming benchmark that evaluates multimodal LLM safety using speech-audio composition attacks (overlap of harmful/benign speech, mixing benign speech with harmful non-speech audio, and multi-speaker dialogue).&lt;/li&gt;&lt;li&gt;Shows these black-box, composition-based attacks successfully bypass safety guardrails in state-of-the-art models (e.g., 66% success on Gemini 2.5 Pro).&lt;/li&gt;&lt;li&gt;Proposes SALMONN-Guard, a multimodal guard model that jointly inspects speech, audio, and text to detect/mitigate these attacks, reducing attack success to ~20%.&lt;/li&gt;&lt;li&gt;Provides dataset and checkpoints for community use, emphasizing the need for audio-aware defenses for multimodal LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Yang', 'Xuezhen Zhang', 'Zhifeng Han', 'Siyin Wang', 'Jimin Zhuang', 'Zengrui Jin', 'Jing Shao', 'Guangzhi Sun', 'Chao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['audio adversarial attacks', 'red-teaming', 'multimodal defenses', 'safety benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10222</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds</title><link>https://arxiv.org/abs/2509.04345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AUDETER, a large-scale, diverse deepfake audio dataset: ~4,500 hours, 3 million clips generated by 11 recent TTS models and 10 vocoders to better represent open-world synthesis diversity.&lt;/li&gt;&lt;li&gt;Identifies negative transfer issues in standard binary supervised training when faced with highly heterogeneous deepfake sources and proposes a curriculum-learning-based training strategy to mitigate this.&lt;/li&gt;&lt;li&gt;Provides extensive evaluations showing many existing detectors struggle to generalise to novel deepfakes and real human speech in AUDETER; XLR-based detectors trained on AUDETER obtain strong cross-domain performance (EER 1.87% on In-the-Wild).&lt;/li&gt;&lt;li&gt;Releases AUDETER publicly to enable systematic evaluation and training for more robust deepfake-audio detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qizhou Wang', 'Hanxun Huang', 'Guansong Pang', 'Sarah Erfani', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-audio', 'dataset', 'detection', 'robustness', 'curriculum-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04345</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title><link>https://arxiv.org/abs/2508.20866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AVIATOR, an AI-agentic framework that injects realistic, category-specific software vulnerabilities by coordinating specialized AI agents, tool-based analysis, and iterative self-correction.&lt;/li&gt;&lt;li&gt;Integrates retrieval-augmented generation (RAG) and lightweight LoRA fine-tuning to avoid handcrafted patterns and achieve high injection fidelity (91–95%) and broad vulnerability coverage.&lt;/li&gt;&lt;li&gt;Shows AVIATOR-augmented datasets substantially improve downstream deep-learning vulnerability detection (DLVD) performance (e.g., +22% F1 over no augmentation, +25% over VGX, +3% over prior SOTA VulScribeR) while lowering cost and syntax rejection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Lbath', 'Massih-Reza Amini', 'Aurelien Delaitre', 'Vadim Okun']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-injection', 'software-security', 'data-augmentation', 'vulnerability-detection', 'LLM-enabled-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20866</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture</title><link>https://arxiv.org/abs/2503.19339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an attention-based CNN-BiLSTM model for IoT intrusion/botnet detection combining traffic pattern analysis, temporal support learning, and targeted feature extraction.&lt;/li&gt;&lt;li&gt;Evaluated on the N-BaIoT dataset, reporting ~99% classification accuracy with high precision, recall, Matthews Correlation Coefficient, and Cohen's kappa.&lt;/li&gt;&lt;li&gt;Positions the model as a compact, efficient defensive mechanism for detecting botnet attacks in practical/unseen IoT network settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amna Naeem', 'Jawad Ahmad', 'Muazzam A. Khan', 'Aizaz Ahmad Khattak', 'Muhammad Shahbaz Khan']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'IoT-security', 'botnet-detection', 'anomaly-detection', 'deep-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19339</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title><link>https://arxiv.org/abs/2503.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVATAR, a novel jailbreak attack that uses benign but logically related metaphors as seeds to induce LLMs to produce or 'calibrate' into harmful content.&lt;/li&gt;&lt;li&gt;AVATAR adaptively selects metaphorical prompts to drive the model's reasoning process, causing either direct harmful outputs or calibration of residuals toward professional harmful content.&lt;/li&gt;&lt;li&gt;Demonstrates strong effectiveness and transferability, achieving state-of-the-art attack success rates across multiple advanced LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zenghao Duan', 'Teli Liu', 'Min Liu', 'Zhiyi Yin', 'Jingyu Lei', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-prompts', 'prompt-injection', 'transferability', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00038</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel side-channel in speculative decoding for LLM serving where per-iteration token counts or packet sizes reveal input-dependent patterns of correct/incorrect speculations.&lt;/li&gt;&lt;li&gt;Demonstrates high-accuracy fingerprinting of user queries (e.g., &gt;75% across schemes at temperature 0.3; detailed results for REST/LADE/BiLD/EAGLE) and ability to exfiltrate confidential datastore contents at &gt;25 tokens/sec.&lt;/li&gt;&lt;li&gt;Evaluates attacks on research and production vLLM serving frameworks and proposes mitigations (packet padding, iteration-wise token aggregation), which are empirically assessed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel', 'speculative-decoding', 'model-serving', 'privacy-leakage', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent</title><link>https://arxiv.org/abs/2602.08412</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PASB, an end-to-end security evaluation framework tailored to personalized LLM-based agents, incorporating realistic usage scenarios, toolchains, and long-horizon interactions.&lt;/li&gt;&lt;li&gt;Applies PASB to OpenClaw and performs black-box, end-to-end security evaluations across personalized scenarios, tool capabilities, and attack types.&lt;/li&gt;&lt;li&gt;Finds critical vulnerabilities at multiple execution stages (prompt processing, tool usage, memory retrieval), demonstrating substantial security risks in personalized agent deployments.&lt;/li&gt;&lt;li&gt;Provides code release for the benchmark framework to facilitate further security testing of real-world agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Feiming Xu', 'Zheng Lin', 'Guangyu He', 'Yuzhe Huang', 'Haichang Gao', 'Zhenxing Niu', 'Shiguo Lian', 'Zhaoxiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['agent security', 'benchmarking', 'red teaming', 'vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08412</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink</title><link>https://arxiv.org/abs/2602.05228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Surgery, a fine-tuning-stage defense that regularizes attention "sink divergence" to steer attention heads toward a negative-sink group and reduce learning/amplification of harmful patterns.&lt;/li&gt;&lt;li&gt;Identifies a separable sink divergence hypothesis: attention heads that learn harmful patterns during fine-tuning are separable by the sign of their sink divergence (positive vs. negative).&lt;/li&gt;&lt;li&gt;Evaluates Surgery on safety benchmarks (BeaverTails, HarmBench, SorryBench), reporting improvements of ~5.90%, 11.25%, and 9.55% respectively; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guozhi Liu', 'Weiwei Lin', 'Tiansheng Huang', 'Ruichao Mo', 'Qi Mu', 'Xiumin Wang', 'Li Shen']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'fine-tuning', 'attention-mechanism', 'model-safety', 'regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05228</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility</title><link>https://arxiv.org/abs/2602.03402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies multimodal jailbreak vulnerabilities in vision-language models (VLMs) and notes that visual inputs often weaken LLM-like risk signals.&lt;/li&gt;&lt;li&gt;Proposes Risk Awareness Injection (RAI), a lightweight, training-free method that builds an Unsafe Prototype Subspace from language embeddings and modulates high-risk visual tokens to amplify safety signals.&lt;/li&gt;&lt;li&gt;Shows RAI restores LLM-like unsafe-content detection from visual inputs and substantially reduces attack success rates on multiple jailbreak benchmarks while preserving task utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengxuan Wang', 'Yuxin Chen', 'Gang Xu', 'Tao He', 'Hongjie Jiang', 'Ming Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal jailbreak', 'defense', 'vision-language models', 'safety calibration', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03402</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away</title><link>https://arxiv.org/abs/2602.11096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeThink, an inference-time defense that monitors chain-of-thought traces with a safety reward model and conditionally injects a short corrective prefix when safety thresholds are breached.&lt;/li&gt;&lt;li&gt;Evaluates SafeThink on six open-source multimodal reasoning models and four jailbreak benchmarks, reporting 30–60% reductions in attack success rates while maintaining reasoning performance.&lt;/li&gt;&lt;li&gt;Key empirical claim: intervening within the first 1–3 reasoning steps is typically sufficient to steer generations back to safe completions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumya Suvra Ghosal', 'Souradip Chakraborty', 'Vaibhav Singh', 'Furong Huang', 'Dinesh Manocha', 'Amrit Singh Bedi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'inference-time defense', 'safety alignment', 'chain-of-thought', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11096</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution</title><link>https://arxiv.org/abs/2602.11079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes activation-based data attribution that traces model behaviors to specific post-training datapoints using activation-difference vectors and cosine similarity.&lt;/li&gt;&lt;li&gt;Validates attributions causally by retraining with modified data and uses clustering of behavior-datapoint similarity to discover emergent harmful behaviors unsupervised.&lt;/li&gt;&lt;li&gt;Applied to a production DPO-trained model (OLMo 2) to identify and mitigate 'distractor-triggered compliance'—filtering top datapoints reduced the behavior by 63% and relabeling achieved 78%.&lt;/li&gt;&lt;li&gt;Method outperforms gradient-based attribution and LLM-judge baselines while being an order of magnitude cheaper, and introduces an in-the-wild benchmark for safety techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Frank Xiao', 'Santiago Aranguri']&lt;/li&gt;&lt;li&gt;Tags: ['data-attribution', 'model-safety', 'post-training-mitigation', 'emergent-behaviors', 'training-data-contamination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11079</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Language Model Inversion through End-to-End Differentiation</title><link>https://arxiv.org/abs/2602.11044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates LM inversion (finding prompts that produce a desired output) as a gradient-based optimization problem.&lt;/li&gt;&lt;li&gt;Introduces an algorithm to make a frozen language model end-to-end differentiable by treating inputs as sequences of distributions over tokens.&lt;/li&gt;&lt;li&gt;Shows that optimized continuous prompts (lengths 10 and 80) can reliably produce target outputs of length 20 on several white-box LMs via gradient descent.&lt;/li&gt;&lt;li&gt;Provides experiments and ablations demonstrating efficiency and effectiveness of the proposed DLM-powered inversion approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Yandoka Denamgana\\"i', 'Kartic Subr']&lt;/li&gt;&lt;li&gt;Tags: ['model-inversion', 'prompt-optimization', 'white-box-attacks', 'adversarial-technique', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11044</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data</title><link>https://arxiv.org/abs/2602.11015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CVPL (Cluster-Vector-Projection Linkage), a geometric, post-hoc framework for assessing linkage/re-identification risk between original and protected tabular datasets via a pipeline: blocking, vectorization, latent projection, and similarity evaluation.&lt;/li&gt;&lt;li&gt;Introduces threshold-aware risk surfaces R(λ, τ) capturing how protection strength and attacker strictness jointly affect linkability, and provides a progressive blocking strategy with monotonicity guarantees enabling anytime lower-bound risk estimates.&lt;/li&gt;&lt;li&gt;Shows Fellegi–Sunter linkage is a special case of CVPL and demonstrates empirically (10k records, 19 protection configs) that formal k-anonymity can coexist with substantial empirical linkability, often driven by non-quasi-identifier behavioral patterns.&lt;/li&gt;&lt;li&gt;Provides interpretable diagnostics to identify which features drive linkage feasibility, supporting privacy impact assessment and comparison of protection mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valery Khvatov', 'Alexey Neyman']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'linkage attack', 're-identification', 'risk assessment', 'data protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11015</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System</title><link>https://arxiv.org/abs/2602.10915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic security analysis of mobile LLM-based agents (case study: Doubao), identifying vulnerabilities including fake app identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from GUI-scraping and unstructured visual inputs.&lt;/li&gt;&lt;li&gt;Proposes Aura, a clean-slate Agent OS with a Hub-and-Spoke architecture (System Agent, sandboxed App Agents, Agent Kernel) that replaces GUI scraping with structured, agent-native interactions.&lt;/li&gt;&lt;li&gt;Introduces four defense pillars—cryptographic identity binding (Global Agent Registry), multilayer Semantic Firewall, taint-aware memory and plan-trajectory integrity, and granular non-deniable auditing—and evaluates improvements on MobileSafetyBench (large reduction in attack success and improved task success/latency).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Zou', 'Sheng Guo', 'Qiuyang Zhan', 'Lepeng Zhao', 'Shuo Li', 'Qi Li', 'Ke Xu', 'Mingwei Xu', 'Zhuotao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['mobile agents', 'prompt injection', 'agent sandboxing', 'secure OS architecture', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10915</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation</title><link>https://arxiv.org/abs/2602.10799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes remote-sensing (RS) hallucinations with an RS-specific taxonomy and introduces image-level hallucination to capture modality/resolution/scene-level errors beyond object-centric mistakes.&lt;/li&gt;&lt;li&gt;Constructs RSHalluEval (2,023 QA pairs) for benchmark evaluation and a compact checker fine-tuned on RSHalluCheck (15,396 QA pairs) enabling dual-mode checking (cloud auditing and local checking).&lt;/li&gt;&lt;li&gt;Provides domain-tailored mitigation resources (RSHalluShield, 30k QA pairs) and proposes training-free, plug-and-play defenses such as decoding-time logit correction and RS-aware prompting.&lt;/li&gt;&lt;li&gt;Reports up to 21.63 percentage-point improvement in hallucination-free rate across representative RS-MLLMs while maintaining competitive downstream task performance (RSVQA/RSVG).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihui Zhou', 'Yong Feng', 'Yanying Chen', 'Guofan Duan', 'Zhenxi Song', 'Mingliang Zhou', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'defense', 'benchmark', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10799</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks</title><link>https://arxiv.org/abs/2602.10780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIRE (Feature-space Inference-time REpair), an inference-time defense that mitigates backdoors by manipulating a sample's latent representations.&lt;/li&gt;&lt;li&gt;Hypothesizes that triggers correspond to structured directions in internal feature spaces and reverses those directions to neutralize poisoned inputs.&lt;/li&gt;&lt;li&gt;Evaluates FIRE on image benchmarks, showing low overhead and improved performance over existing runtime mitigation methods across attacks, datasets, and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrico Ahlers', 'Daniel Passon', 'Yannic Noller', 'Lars Grunske']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defenses', 'inference-time mitigation', 'latent-space manipulation', 'runtime security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10780</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration</title><link>https://arxiv.org/abs/2602.10750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecureScan, a triple-layer defensive framework combining heuristic filtering, a logistic regression classifier, and VirusTotal threat intelligence for triaging URLs, file hashes, and binaries.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency and real-world stability via threshold-based decision calibration and a gray-zone (0.45–0.55) for borderline cases to reduce false positives.&lt;/li&gt;&lt;li&gt;Reports strong empirical performance (93.1% accuracy, precision 0.87, recall 0.92) on benchmark datasets and argues that a lightweight statistical model plus external verification can rival more complex deep learning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rumman Firdos', 'Aman Dangi']&lt;/li&gt;&lt;li&gt;Tags: ['malware-detection', 'phishing-detection', 'threat-intelligence', 'defensive-security', 'logistic-regression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10750</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL</title><link>https://arxiv.org/abs/2602.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniVL-Guard, a unified framework for omnibus vision-language forgery detection and fine-grained grounding (localization) across interleaved text, images, and videos.&lt;/li&gt;&lt;li&gt;Identifies a 'difficulty bias' in multi-task optimization where coarse veracity classification dominates training, degrading grounding performance.&lt;/li&gt;&lt;li&gt;Introduces Self-Evolving Chain-of-Thought (CoT) Generation to produce high-quality reasoning paths and Adaptive Reward Scaling Policy Optimization (ARSPO) to dynamically modulate reward scales and task weights for balanced RL-based training.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art detection and grounding performance with robust zero-shot generalization in out-of-domain scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinjie Shen', 'Jing Wu', 'Yaxiong Wang', 'Lechao Cheng', 'Shengeng Tang', 'Tianrui Hui', 'Nan Pu', 'Zhun Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['forgery detection', 'multimodal security', 'misinformation detection', 'reinforcement learning', 'grounding/localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10687</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling</title><link>https://arxiv.org/abs/2602.10623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bayesian Non-Negative Reward Model (BNRM) that integrates non-negative factor analysis with a Bradley–Terry preference model to represent rewards via sparse, non-negative latent factors.&lt;/li&gt;&lt;li&gt;Uses instance-specific latent variables for disentangled reward representations and global sparsity as an implicit debiasing mechanism to suppress spurious correlations that enable reward hacking.&lt;/li&gt;&lt;li&gt;Implements an amortized variational inference network conditioned on deep LLM representations to scale the approach and enable end-to-end training.&lt;/li&gt;&lt;li&gt;Empirical results show reduced reward over-optimization (mitigating reward hacking), improved robustness under distribution shift, and more interpretable reward decompositions versus strong baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibin Duan', 'Guowei Rong', 'Zhuo Li', 'Bo Chen', 'Mingyuan Zhou', 'Dandan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'RLHF', 'reward modeling', 'robustness/defense', 'Bayesian methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10623</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images</title><link>https://arxiv.org/abs/2602.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents RealHD, a large high-quality dataset of &gt;730,000 images (real and AI-generated) across multiple categories and generation methods to improve detection generalization.&lt;/li&gt;&lt;li&gt;Generated images include outputs from state-of-the-art text-to-image, inpainting, refinement, and face-swapping methods; metadata includes generation method labels and inpainting masks.&lt;/li&gt;&lt;li&gt;Proposes a lightweight baseline detector using Non-Local Means noise entropy and demonstrates that models trained on RealHD achieve strong cross-model/generalization.&lt;/li&gt;&lt;li&gt;Dataset and code are released as a benchmark to evaluate and advance robust detection of AI-generated images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanzhe Yu', 'Yun Ye', 'Jintao Rong', 'Qi Xuan', 'Chen Ma']&lt;/li&gt;&lt;li&gt;Tags: ['Deepfake detection', 'Dataset / Benchmark', 'AI-generated image forensics', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10546</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI</title><link>https://arxiv.org/abs/2602.10481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two cryptographic primitives—authenticated prompts and authenticated context—to provide verifiable provenance and tamper-evidence for LLM inputs.&lt;/li&gt;&lt;li&gt;Formalizes a policy algebra with proven theorems that claim protocol-level Byzantine resistance, preventing adversarial agents from violating organizational policies.&lt;/li&gt;&lt;li&gt;Presents five layered defenses (including resource controls and LLM-based semantic validation) and evaluates against six attack categories with claimed 100% detection and nominal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohan Rajagopalan', 'Vinay Rao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'authenticated prompts', 'tamper-evident context', 'policy algebra', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10481</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Authenticated Workflows: A Systems Approach to Protecting Agentic AI</title><link>https://arxiv.org/abs/2602.10465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "authenticated workflows", a cryptographic trust layer for agentic AI that enforces intent and integrity across four boundaries: prompts, tools, data, and context.&lt;/li&gt;&lt;li&gt;Introduces MAPL, an AI-native policy language that enables hierarchical, dynamic policy composition with cryptographic attestations and claimed scalability O(log M + N).&lt;/li&gt;&lt;li&gt;Describes a universal security runtime integrating nine major agent frameworks via thin adapters (no protocol changes), provides formal completeness/soundness proofs, and reports empirical results (100% recall on 174 tests, mitigations of OWASP risks and production CVEs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohan Rajagopalan', 'Vinay Rao']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'agentic-ai', 'cryptographic-attestations', 'runtime-security', 'policy-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10465</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models</title><link>https://arxiv.org/abs/2602.10179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Vision-Centric Jailbreak Attack (VJA): a visual-to-visual jailbreak that encodes malicious instructions purely via visual prompts (marks, arrows, visual-text) to compromise image editing models.&lt;/li&gt;&lt;li&gt;Presents IESBench, a safety-oriented benchmark for evaluating image editing systems against visual jailbreaks and systematically measures vulnerabilities.&lt;/li&gt;&lt;li&gt;Reports high attack success rates on state-of-the-art commercial models (e.g., up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5).&lt;/li&gt;&lt;li&gt;Proposes a training-free defense based on introspective multimodal reasoning that substantially improves safety with negligible computational overhead and no auxiliary guard models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Hou', 'Yining Sun', 'Ruochong Jin', 'Haochen Han', 'Fangming Liu', 'Wai Kin Victor Chan', 'Alex Jinpeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-attack', 'image-editing-models', 'defense', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10179</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment</title><link>https://arxiv.org/abs/2602.10161</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies cross-modal safety vulnerabilities in omni-modal LLMs via a new AdvBench-Omni attack dataset and a modality-semantics decoupling principle.&lt;/li&gt;&lt;li&gt;Mechanistic analysis discovers a Mid-layer Dissolution phenomenon and a modal-invariant 'pure refusal' direction driven by refusal vector magnitude shrinkage.&lt;/li&gt;&lt;li&gt;Proposes OmniSteer: extracts a golden refusal vector with SVD and uses lightweight adapters to adaptively modulate intervention intensity, improving refusal success rates while preserving multimodal capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Wang', 'Zherui Li', 'Zhenhong Zhou', 'Yitong Zhang', 'Yan Mi', 'Kun Yang', 'Yiming Zhang', 'Junhao Dong', 'Zhongxiang Sun', 'Qiankun Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'adversarial attacks/benchmarks', 'defense/mitigation', 'model internals/interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10161</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems</title><link>https://arxiv.org/abs/2602.10160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates end-to-end autonomous driving agents (Transfuser, Interfuser) under three black-box visual attacks in CARLA: physics-based acoustic blur, electromagnetic interference image distortion, and digital ghost-object perturbations.&lt;/li&gt;&lt;li&gt;Finds severe degradation in closed-loop driving performance (up to 99% drop in driving score) demonstrating major safety vulnerabilities in visual perception pipelines.&lt;/li&gt;&lt;li&gt;Proposes AD^2, a lightweight attention-based spatial–temporal attack detector for multi-camera inputs that outperforms baselines in detection accuracy and computational efficiency on CARLA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishan Sahu', 'Somnath Hazra', 'Somak Aditya', 'Soumyajit Dey']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'physical-attacks', 'sensor-manipulation', 'attack-detection', 'autonomous-driving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10160</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.10154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM-XR, a privacy-aware framework for multi-user XR collaboration that preprocesses headset-captured frames on an edge server to filter sensitive or irrelevant visual information before sending to cloud MLLMs.&lt;/li&gt;&lt;li&gt;Introduces a lightweight registration and customizable content-sharing mechanism to enable efficient, accurate, and privacy-preserving synchronization across users in dynamic XR environments.&lt;/li&gt;&lt;li&gt;Provides quantitative evaluation (≈90% request accuracy, &lt;0.27s registration time, &lt;3.5 cm spatial inconsistency) and an IRB-approved user study (28 participants) showing automatic filtering of highly sensitive objects in &gt;90% of scenarios while maintaining usability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiangong Chen', 'Mingyu Zhu', 'Bin Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'edge-processing', 'multimodal-LLM', 'XR security', 'data-filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10154</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks</title><link>https://arxiv.org/abs/2602.10148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CrossTALK, a scalable cross-modal entanglement attack framework designed to jailbreak vision-language models by dispersing and entangling harmful clues across modalities.&lt;/li&gt;&lt;li&gt;Introduces three key techniques: knowledge-scalable reframing (multi-hop chain instructions), cross-modal clue entangling (migrating visualizable entities into images), and cross-modal scenario nesting (multimodal contextual steering).&lt;/li&gt;&lt;li&gt;Performs red-teaming experiments demonstrating state-of-the-art attack success rates against VLM safety alignment in black-box settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Shengjia Cheng', 'Teli Liu', 'Mingfeng Li', 'Min Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'multimodal attacks', 'vision-language models', 'adversarial techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10148</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible</title><link>https://arxiv.org/abs/2602.10139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an anonymization-based defense that makes sensitive on-screen data available-but-invisible to cloud-based mobile GUI agents by replacing PII with deterministic, type-preserving placeholders.&lt;/li&gt;&lt;li&gt;Introduces a layered system (PII Detector, UI Transformer, Secure Interaction Proxy, Privacy Gatekeeper) to consistently anonymize screenshots, XML UI hierarchies, and user instructions while allowing scoped local computations over raw values when needed.&lt;/li&gt;&lt;li&gt;Uses a PII-aware recognition model to detect sensitive UI content and deterministic placeholders (e.g., PHONE_NUMBER#a1b2c) to preserve semantic utility without exposing identifiers.&lt;/li&gt;&lt;li&gt;Evaluates on AndroidLab and PrivScreen benchmarks, showing substantial reduction in privacy leakage across multiple models with modest utility degradation and favorable privacy-utility trade-offs versus prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lepeng Zhao', 'Zhenhua Zou', 'Shuo Li', 'Zhuotao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-anonymization', 'defense', 'mobile-GUI-agents', 'multimodal-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10139</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reverse-Engineering Model Editing on Language Models</title><link>https://arxiv.org/abs/2602.10134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a vulnerability in locate-then-edit model editing methods: low-rank parameter updates leak information about edited subjects via their row-space "fingerprint."&lt;/li&gt;&lt;li&gt;Presents KSTER, a two-stage reverse-engineering attack (KeySpace Reconstruction then Entropy Reduction) that recovers edited subjects and reconstructs semantic edit prompts using spectral analysis and entropy-based techniques.&lt;/li&gt;&lt;li&gt;Demonstrates high success rates across multiple LLMs in experiments, and proposes a defense called subspace camouflage that injects semantic decoys to obfuscate update fingerprints while preserving editing utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyu Sun', 'Minrui Luo', 'Yu Wang', 'Zhili Chen', 'Tianxing He']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'parameter-update leakage', 'reverse-engineering attack', 'privacy / data extraction', 'defense (subspace camouflage)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10134</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight</title><link>https://arxiv.org/abs/2602.11136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FormalJudge, a neuro-symbolic oversight framework that translates natural-language safety requirements into formal specifications (Dafny) via LLMs and verifies agent behavior using Z3 SMT solving.&lt;/li&gt;&lt;li&gt;Replaces probabilistic LLM-as-judge evaluations with mathematically provable compliance checks, producing guarantees rather than scores.&lt;/li&gt;&lt;li&gt;Validated on three benchmarks (behavioral safety, multi-domain constraint adherence, agentic upward deception detection) showing substantial gains over LLM-judges and cross-model generalization.&lt;/li&gt;&lt;li&gt;Demonstrates iterative refinement and scalability, enabling detection of deceptive agent behavior and improved oversight accuracy even with smaller judge models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Zhou', 'Yang Sheng', 'Hantao Lou', 'Yaodong Yang', 'Jie Fu']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'agentic-oversight', 'safety-defenses', 'deception-detection', 'neuro-symbolic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11136</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
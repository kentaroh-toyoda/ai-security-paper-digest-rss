<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 12 Dec 2025 23:17:50 +0000</lastBuildDate><item><title>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</title><link>https://arxiv.org/abs/2411.03752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new "Deferred Poisoning" attack that keeps training/validation performance normal but induces large local curvature (via Hessian singularization) so small perturbations or natural noise cause large degradation in performance.&lt;/li&gt;&lt;li&gt;Introduces a Singularization Regularization term to drive the Hessian at the optimum toward singularity; provides theoretical analysis and empirical validation on image classification tasks.&lt;/li&gt;&lt;li&gt;Demonstrates the attack's stealthiness (no obvious train/validation discrepancy) and its practical hazard by showing increased vulnerability to evasion attacks and natural noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao He', 'Jinyu Tian', 'Xianwei Zheng', 'Li Dong', 'Yuanman Li', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'poisoning attack', 'adversarial robustness', 'Hessian analysis', 'stealthy attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.03752</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title><link>https://arxiv.org/abs/2512.07730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAVE, a training-free framework that uses a Sparse Autoencoder (SAE) to identify latent features most indicative of visual understanding via a binary object-presence QA probe.&lt;/li&gt;&lt;li&gt;Steers the multimodal model along these identified SAE features to reinforce grounded visual understanding, reducing object hallucination and increasing attention to image tokens.&lt;/li&gt;&lt;li&gt;Reports consistent improvements across benchmarks (e.g., ~10 percentage-point gain on CHAIR_S) and across multiple models and layers, with analyses showing suppression of uncertain object tokens and robustness/generalizability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangha Park', 'Seungryong Yoo', 'Jisoo Mok', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multimodal LLMs', 'sparse autoencoder', 'visual grounding', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07730</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimization-Guided Diffusion for Interactive Scene Generation</title><link>https://arxiv.org/abs/2512.07661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OMEGA, a training-free, optimization-guided diffusion sampling framework that re-anchors each reverse diffusion step via constrained optimization to enforce physical and social constraints in multi-agent driving scene generation.&lt;/li&gt;&lt;li&gt;Implements a game-theoretic formulation for ego-attacker interactions in distribution space, approximating Nash equilibria to synthesize realistic, safety-critical adversarial scenarios for autonomous vehicle evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates substantial empirical gains on nuPlan and Waymo: increases physically/behaviorally valid scenes (e.g., from 32.35% to 72.27% for free exploration; 11% to 80% for controllability-focused generation) and produces ~5× more near-collision frames (TTC &lt; 3s) while preserving overall realism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shihao Li', 'Naisheng Ye', 'Tianyu Li', 'Kashyap Chitta', 'Tuo An', 'Peng Su', 'Boyang Wang', 'Haiou Liu', 'Chen Lv', 'Hongyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving safety', 'adversarial scenario generation', 'diffusion models', 'safety evaluation / red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07661</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AttenDence: Maximizing Attention Confidence for Test Time Adaptation</title><link>https://arxiv.org/abs/2511.18925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes minimizing entropy of CLS-to-patch attention distributions in vision transformers as a test-time adaptation (TTA) objective.&lt;/li&gt;&lt;li&gt;Aims to increase attention confidence to relevant image regions under distribution shift, working even with a single test image.&lt;/li&gt;&lt;li&gt;Shows improved robustness to diverse corruptions without degrading clean-data performance on single-sample test streams.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Mali']&lt;/li&gt;&lt;li&gt;Tags: ['test-time adaptation', 'robustness', 'transformer attention', 'distribution shift', 'vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18925</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.16203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLA-Fool, a framework for evaluating multimodal adversarial robustness of Vision-Language-Action (VLA) models under white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Defines three attack axes: textual perturbations (gradient- and prompt-based), visual perturbations (patches and noise), and cross-modal misalignment attacks that break semantic correspondence between perception and instruction.&lt;/li&gt;&lt;li&gt;Proposes a VLA-aware semantic space to craft semantically guided automatic prompts for more effective attacks.&lt;/li&gt;&lt;li&gt;Evaluates on LIBERO with a fine-tuned OpenVLA, showing minor multimodal perturbations can cause large behavioral deviations in embodied agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yixin Zhang', 'Lingjuan Lyu', 'Handing Wang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal robustness', 'vision-language-action', 'prompt attacks', 'cross-modal misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16203</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts</title><link>https://arxiv.org/abs/2505.17476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies risk from MLLM-generated multimodal disinformation and the limitations of prior detection methods that rely on unrealistic misalignment artifacts.&lt;/li&gt;&lt;li&gt;Introduces the MLLM-Driven Synthetic Multimodal (MDSM) dataset: edited images paired with MLLM-crafted deceptive texts that are semantically coherent with the manipulations.&lt;/li&gt;&lt;li&gt;Proposes AMD (Artifact-aware Manipulation Diagnosis via MLLM) with Artifact Pre-perception Encoding and Manipulation-Oriented Reasoning to detect MLLM-powered multimodal deceptions.&lt;/li&gt;&lt;li&gt;Reports strong cross-domain detection performance on MDSM (88.18 ACC, 60.25 mAP, 61.02 mIoU), demonstrating improved generalization for manipulation diagnosis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchen Zhang', 'Yaxiong Wang', 'Yujiao Wu', 'Lianwei Wu', 'Li Zhu', 'Zhedong Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal disinformation', 'MLLM adversarial generation', 'manipulation detection', 'robustness', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17476</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>diffDemorph: Extending Reference-Free Demorphing to Unseen Faces</title><link>https://arxiv.org/abs/2505.14527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes diffDeMorph, a diffusion-based reference-free demorphing method that disentangles component faces from a morphed image with high visual fidelity.&lt;/li&gt;&lt;li&gt;Claims strong generalization across morphing techniques and face image styles, outperforming prior state of the art by ≥59.46% under a common training protocol.&lt;/li&gt;&lt;li&gt;Trains on synthetic morphs and evaluates on six datasets and two face matchers, demonstrating practicality for real-world biometric scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nitish Shukla', 'Arun Ross']&lt;/li&gt;&lt;li&gt;Tags: ['biometric security', 'face morphing/demorphing', 'forensic/anti-spoofing', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14527</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Panoramic Out-of-Distribution Segmentation</title><link>https://arxiv.org/abs/2505.03539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PanOoS, a new task for out-of-distribution segmentation in panoramic (360°) images to improve safe, omnidirectional scene understanding.&lt;/li&gt;&lt;li&gt;Proposes POS, a text-guided prompt-distribution learning method that uses Prompt-based Restoration Attention (PRA) and Bilevel Prompt Distribution Learning (BPDL) to adapt CLIP-style multimodal features for per-pixel OoS detection and segmentation.&lt;/li&gt;&lt;li&gt;Creates two PanOoS benchmarks (DenseOoS and QuadOoS) and reports substantial improvements over pinhole-OoS baselines (e.g., +34.25% AuPRC, −21.42% FPR95) while also improving closed-set segmentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengfei Duan', 'Yuheng Zhang', 'Yihong Cao', 'Fei Teng', 'Kai Luo', 'Jiaming Zhang', 'Kailun Yang', 'Zhiyong Li']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'semantic segmentation', 'robustness / safety', 'panoramic imaging', 'prompting / CLIP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.03539</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying the Reliability of Predictions in Detection Transformers: Object-Level Calibration and Image-Level Uncertainty</title><link>https://arxiv.org/abs/2412.01782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes calibration and reliability of DETR-style object detectors, showing that within each image DETRs learn a specialist strategy: one prediction per object is well-calibrated while other predicted boxes suppress foreground confidence.&lt;/li&gt;&lt;li&gt;Proves this behavior arises as the loss-minimizing solution under Hungarian matching, and highlights that the well-calibrated predictions are unidentifiable at inference time—so post-processing can mix predictions with different calibration quality.&lt;/li&gt;&lt;li&gt;Proposes a new evaluation metric, Object-level Calibration Error (OCE), which penalizes keeping suppressed predictions and missing true foreground objects, aiming to better assess model+post-processing reliability than AP or ECE.&lt;/li&gt;&lt;li&gt;Presents a post-hoc uncertainty quantification framework to predict per-image model accuracy, enabling identification of images with unreliable detections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young-Jin Park', 'Carson Sobolewski', 'Navid Azizan']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty quantification', 'object detection', 'evaluation metrics', 'DETR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01782</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title><link>https://arxiv.org/abs/2512.10766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MJA, a metaphor-based jailbreaking attack for text-to-image models that crafts adversarial prompts to bypass diverse safety defenses without prior knowledge of defense type.&lt;/li&gt;&lt;li&gt;MJA has two components: MLAG (an LLM-based multi-agent module for metaphor retrieval, context matching, and adversarial prompt generation) and APO (an adversarial prompt optimization module that trains a surrogate predictor and uses an acquisition strategy to find optimal prompts).&lt;/li&gt;&lt;li&gt;Demonstrates stronger attack success and fewer queries than six baselines across T2I models with various internal and external defenses.&lt;/li&gt;&lt;li&gt;Provides code and emphasizes transferability of metaphorical prompts to evade unknown or heterogeneous defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Yiwen Ma', 'Lanjun Wang', 'Wenhui Li', 'Yi Tu', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'defense evasion', 'safety evaluation', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10766</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title><link>https://arxiv.org/abs/2512.10675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a generative evaluation system built on a frontier video foundation model (Veo) optimized for robot action conditioning and multi-view consistency.&lt;/li&gt;&lt;li&gt;Uses generative editing and multi-view completion to synthesize realistic in-distribution and out-of-distribution (OOD) scene variations for policy evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates the system can predict relative policy performance, quantify impacts of different generalization axes, and perform red teaming to find physical and semantic safety violations.&lt;/li&gt;&lt;li&gt;Validated via 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints across five bimanual manipulation tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gemini Robotics Team', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robotics simulation', 'video foundation models', 'OOD generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10675</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</title><link>https://arxiv.org/abs/2512.10958</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WorldLens, a benchmark that evaluates generative driving world models across five aspects: Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference.&lt;/li&gt;&lt;li&gt;Introduces WorldLens-26K, a large human-annotated video dataset with numerical scores and textual rationales to align metrics with human judgment.&lt;/li&gt;&lt;li&gt;Finds trade-offs where models that look realistic often violate physics or geometry, while geometry-consistent models lack behavioral fidelity for control.&lt;/li&gt;&lt;li&gt;Provides WorldLens-Agent, a distilled evaluation model enabling scalable, explainable scoring of world fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ao Liang', 'Lingdong Kong', 'Tianyi Yan', 'Hongsi Liu', 'Wesley Yang', 'Ziqi Huang', 'Wei Yin', 'Jialong Zuo', 'Yixuan Hu', 'Dekai Zhu', 'Dongyue Lu', 'Youquan Liu', 'Guangfeng Jiang', 'Linfeng Li', 'Xiangtai Li', 'Long Zhuo', 'Lai Xing Ng', 'Benoit R. Cottereau', 'Changxin Gao', 'Liang Pan', 'Wei Tsang Ooi', 'Ziwei Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmark', 'autonomous-driving', 'physical-plausibility', 'human-annotated-dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10958</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images</title><link>https://arxiv.org/abs/2512.10715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents methods to estimate uncertainty for landmark-based anatomical segmentation on chest X-rays using latent and predictive uncertainty from a variational graph-decoder architecture.&lt;/li&gt;&lt;li&gt;Shows that both uncertainty measures correlate with controlled input corruption and can flag unreliable predictions and out-of-distribution examples on the CheXmask dataset.&lt;/li&gt;&lt;li&gt;Releases CheXmask-U: a large dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, plus code and an interactive demo to support safer clinical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matias Cosarinsky', 'Nicolas Gaggion', 'Rodrigo Echeveste', 'Enzo Ferrante']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'medical-imaging', 'robustness', 'out-of-distribution-detection', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10715</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection</title><link>https://arxiv.org/abs/2512.10652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;TriDF is a multimodal benchmark for interpretable DeepFake detection covering 16 forgery types across image, video, and audio.&lt;/li&gt;&lt;li&gt;Evaluates three axes: Perception (artifact localization via human-annotated evidence), Detection (classification across forgery families), and Hallucination (reliability of model-generated explanations).&lt;/li&gt;&lt;li&gt;Experiments show that accurate perception is critical for detection but hallucination in explanations can undermine decision-making, highlighting interdependence between the three aspects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian-Yu Jiang-Lin', 'Kang-Yang Huang', 'Ling Zou', 'Ling Lo', 'Sheng-Ping Yang', 'Yu-Wen Tseng', 'Kun-Hsiang Lin', 'Chia-Ling Chen', 'Yu-Ting Ta', 'Yan-Tsung Wang', 'Po-Ching Chen', 'Hongxia Xie', 'Hong-Han Shuai', 'Wen-Huang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['DeepFake detection', 'Interpretable ML / Explainability', 'Benchmarking', 'Hallucination / Explanation reliability', 'Multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10652</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation</title><link>https://arxiv.org/abs/2512.10275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial distillation and identifies that stronger robust teachers do not always yield more robust students due to a phenomenon dubbed "robust saturation".&lt;/li&gt;&lt;li&gt;Shows adversarial transferability (how often student-crafted adversarial examples fool the teacher) is a key factor in successful robustness transfer, beyond capacity gap explanations.&lt;/li&gt;&lt;li&gt;Proposes Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by measured transferability without added compute, and demonstrates improved AutoAttack robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongsin Lee', 'Hye Won Chung']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial distillation', 'robustness transfer', 'model compression', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10275</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</title><link>https://arxiv.org/abs/2512.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RobustSora, a 6,500-video benchmark with four classes (Authentic-Clean, Authentic-Spoofed with fake watermarks, Generated-Watermarked, Generated-DeWatermarked) to measure watermark effects on AIGC video detection.&lt;/li&gt;&lt;li&gt;Defines two evaluation tasks: (I) detect AI-generated videos after watermark removal, and (II) measure false alarms on authentic videos with fake watermarks.&lt;/li&gt;&lt;li&gt;Evaluates ten detectors (specialized AIGC detectors, transformer models, and MLLMs), finding 2–8 percentage-point performance shifts under watermark manipulation and showing transformer models have moderate watermark dependency.&lt;/li&gt;&lt;li&gt;Concludes detectors partially rely on watermark signals and recommends watermark-aware training strategies to improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Wang', 'Xiliang Liu', 'Ligang Sun']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'watermark robustness', 'adversarial manipulation', 'benchmarking', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10248</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-dimensional Preference Alignment by Conditioning Reward Itself</title><link>https://arxiv.org/abs/2512.10237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a limitation in standard DPO (Bradley-Terry aggregation) that forces conflicting multi-dimensional preferences into a single scalar reward, causing reward conflict and loss of desirable features.&lt;/li&gt;&lt;li&gt;Proposes Multi Reward Conditional DPO (MCDPO): a disentangled Bradley-Terry objective that conditions the reward model on a preference outcome vector so each reward axis is learned independently in one network.&lt;/li&gt;&lt;li&gt;Introduces dimensional reward dropout to balance optimization across axes and enables inference-time multi-axis control via Classifier Free Guidance without extra training or external reward models.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on benchmarks with experiments on Stable Diffusion 1.5 and SDXL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiho Jang', 'Jinyoung Kim', 'Kyungjune Baek', 'Nojun Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['preference alignment', 'reward modeling', 'RLHF', 'diffusion models', 'conditional control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10237</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title><link>https://arxiv.org/abs/2511.10287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OutSafe-Bench, a large-scale multimodal safety benchmark with &gt;18k bilingual text prompts, 4.5k images, 450 audio clips, and 450 videos annotated across nine content risk categories.&lt;/li&gt;&lt;li&gt;Proposes the Multidimensional Cross Risk Score (MCRS) to model overlapping/correlated risks and FairScore, an explainable multi-reviewer weighted aggregation that uses top-performing models as adaptive juries to reduce single-model bias.&lt;/li&gt;&lt;li&gt;Provides systematic evaluation of nine state-of-the-art multimodal LLMs, demonstrating persistent safety vulnerabilities and gaps in multimodal content moderation.&lt;/li&gt;&lt;li&gt;Focuses on comprehensive safety assessment and metric/aggregation design to improve robustness and fairness of multimodal content-safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yuanshuai Li', 'Yingchao Yu', 'Lingjuan Lyu', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmark', 'multimodal-safety', 'toxicity-detection', 'evaluation-metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10287</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title><link>https://arxiv.org/abs/2508.03772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes two stability problems in Group Relative Policy Optimization (GRPO): token-level penalization (conflicting negative feedback on shared valuable tokens) and policy collapse (entropy increase shifting probability mass to unlikely tokens).&lt;/li&gt;&lt;li&gt;Proposes GTPO (Group-relative Trajectory-based Policy Optimization) which (a) skips negative gradient updates on shared valuable tokens while amplifying positive updates, and (b) filters out high-entropy completions above a provable threshold to avoid policy collapse.&lt;/li&gt;&lt;li&gt;Removes reliance on KL regularization and a reference model, claiming improved stability and superior performance on reasoning benchmarks (GSM8K, MATH, AIME 2024/2025, AMC 2023).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Simoni', 'Aleksandar Fontana', 'Giulio Rossolini', 'Andrea Saracino', 'Paolo Mori']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'policy optimization', 'training stability', 'RLHF', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03772</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forensic deepfake audio detection using segmental speech features</title><link>https://arxiv.org/abs/2505.13847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates use of segmental (articulatory/interpretable) speech features to detect deepfake audio.&lt;/li&gt;&lt;li&gt;Finds certain segmental features effective for detection while some global features provide little value.&lt;/li&gt;&lt;li&gt;Proposes a speaker-specific detection framework aimed at forensic interpretability and sensitivity to individual phonetic realization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Yang', 'Chengzhe Sun', 'Siwei Lyu', 'Phil Rose']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'forensic voice comparison', 'speaker-specific detection', 'interpretable features', 'audio security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13847</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data</title><link>https://arxiv.org/abs/2504.01951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether six LLMs can infer a user's gender from historical online purchase data and measures prediction accuracy.&lt;/li&gt;&lt;li&gt;Finds models achieve moderate gender-classification performance but rely heavily on stereotypical product-category associations.&lt;/li&gt;&lt;li&gt;Shows that explicit instructions to avoid bias lower model certainty but do not eliminate stereotyped reasoning patterns.&lt;/li&gt;&lt;li&gt;Highlights privacy and fairness implications and calls for more robust bias-mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Massimiliano Luca', 'Ciro Beneduce', 'Bruno Lepri', 'Jacopo Staiano']&lt;/li&gt;&lt;li&gt;Tags: ['gender-bias', 'privacy-inference', 'LLM-evaluation', 'bias-mitigation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01951</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</title><link>https://arxiv.org/abs/2510.08158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two benchmarks (XSB and MS-XSB) to diagnose exaggerated/false refusals by LLMs in single-turn and multi-turn dialog contexts, with annotated 'Focus' keywords identifying refusal triggers.&lt;/li&gt;&lt;li&gt;Finds that exaggerated refusals persist across recent instruction-tuned Llama models and worsen in complex multi-turn scenarios.&lt;/li&gt;&lt;li&gt;Proposes model-agnostic, post-hoc inference-time mitigation strategies (ignore-word instructions, prompt rephrasing, attention steering) that improve compliance on safe prompts while retaining safety protections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzhou Yuan', 'Ercong Nie', 'Yinuo Sun', 'Chenxuan Zhao', 'William LaCroix', 'Michael F\\"arber']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal calibration', 'safety evaluation', 'post-hoc mitigation', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08158</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title><link>https://arxiv.org/abs/2508.08139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reliability estimation method that uses token-level aleatoric and epistemic uncertainty from logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction.&lt;/li&gt;&lt;li&gt;Finds that correct in-context information improves answer accuracy and confidence, while misleading context often causes confidently incorrect responses, indicating misalignment between uncertainty and correctness.&lt;/li&gt;&lt;li&gt;Presents a probing-based approach that captures behavior shifts and improves detection of unreliable outputs across multiple open-source LLMs on open QA benchmarks, highlighting limitations of raw uncertainty signals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Zhou', 'Johanne Medina', 'Sanjay Chawla']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty estimation', 'LLM reliability', 'probing', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08139</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Better Language Model Inversion by Compactly Representing Next-Token Distributions</title><link>https://arxiv.org/abs/2506.17090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PILS (prompt inversion from logprob sequences), which compresses next-token probability sequences by exploiting that LLM output vectors lie in a low-dimensional subspace, enabling lossless linear compression across multiple generation steps.&lt;/li&gt;&lt;li&gt;Demonstrates large improvements in recovering hidden prompts and system messages (2–3.5x higher exact recovery rates; e.g., 17% → 60% in one setting) and shows generalization to longer generation horizons at test time.&lt;/li&gt;&lt;li&gt;Provides analysis of verbatim repetition's effect on recovery and a cross-family model transfer method for logit-based inverters, highlighting next-token probabilities as a significant attack surface.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Murtaza Nazir', 'Matthew Finlayson', 'John X. Morris', 'Xiang Ren', 'Swabha Swayamdipta']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'prompt extraction', 'privacy attack', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17090</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title><link>https://arxiv.org/abs/2512.10449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerability of LLM-based scientific reviewers to indirect prompt injection via adversarially manipulated PDF content, aiming to flip 'Reject' decisions to 'Accept'.&lt;/li&gt;&lt;li&gt;Introduces WAVS (Weighted Adversarial Vulnerability Score) as a metric, curates a 200-paper dataset, and adapts 15 domain-specific attack strategies evaluated across 13 LLMs (e.g., GPT-5, Claude Haiku).&lt;/li&gt;&lt;li&gt;Finds that obfuscation/injection strategies can achieve high decision-flip rates even on large models (e.g., 'Maximum Mark Magyk'), and releases dataset and injection framework for further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Manish Prasad', 'Vasudev Majhi', 'Jahnvi Singh', 'Vinay Chamola', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'adversarial attacks', 'LLM robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10449</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Watermarks for Language Models via Probabilistic Automata</title><link>https://arxiv.org/abs/2512.10185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new class of watermarking schemes for language-model outputs based on probabilistic automata.&lt;/li&gt;&lt;li&gt;Provides two instantiations: a practical, computationally efficient scheme with exponential generation diversity, and a theoretical construction offering cryptographic undetectability guarantees.&lt;/li&gt;&lt;li&gt;Evaluates robustness and efficiency empirically on LLaMA-3B and Mistral-7B, showing improved robustness to editing attacks and lower detection overhead compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangkun Wang', 'Jingbo Shang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model provenance', 'undetectability', 'robustness', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10185</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Offscript: Automated Auditing of Instruction Adherence in LLMs</title><link>https://arxiv.org/abs/2512.10172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Offscript, an automated auditing tool to detect when LLMs deviate from user-provided custom instructions during information-seeking conversations.&lt;/li&gt;&lt;li&gt;Pilot study on Reddit-sourced custom instructions found potential deviations in 86.4% of conversations; 22.2% of these were confirmed as material violations by human reviewers.&lt;/li&gt;&lt;li&gt;Demonstrates automated auditing as a scalable approach for evaluating compliance with behavioral instructions, relevant to alignment and safety assessments of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas Clark', 'Ryan Bai', 'Tanu Mitra']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'auditing', 'alignment', 'safety-evaluation', 'behavioral-compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10172</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</title><link>https://arxiv.org/abs/2512.10791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the FACTS Leaderboard suite to evaluate LLM factuality across four sub-leaderboards: Multimodal (image-based), Parametric (closed-book knowledge), Search (information-seeking with search API), and Grounding v2 (long-form grounding in provided documents).&lt;/li&gt;&lt;li&gt;Uses automated judge models to score responses and aggregates the four components into a single suite score; includes public and private splits to enable participation while protecting integrity.&lt;/li&gt;&lt;li&gt;Maintained online benchmark intended for ongoing evaluation of model factuality with improved judge models for grounding and support for multimodal scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aileen Cheng', 'Alon Jacovi', 'Amir Globerson', 'Ben Golan', 'Charles Kwong', 'Chris Alberti', 'Connie Tao', 'Eyal Ben-David', 'Gaurav Singh Tomar', 'Lukas Haas', 'Yonatan Bitton', 'Adam Bloniarz', 'Aijun Bai', 'Andrew Wang', 'Anfal Siddiqui', 'Arturo Bajuelos Castillo', 'Aviel Atias', 'Chang Liu', 'Corey Fry', 'Daniel Balle', 'Deepanway Ghosal', 'Doron Kukliansky', 'Dror Marcus', 'Elena Gribovskaya', 'Eran Ofek', 'Honglei Zhuang', 'Itay Laish', 'Jan Ackermann', 'Lily Wang', 'Meg Risdal', 'Megan Barnes', 'Michael Fink', 'Mohamed Amin', 'Moran Ambar', 'Natan Potikha', 'Nikita Gupta', 'Nitzan Katz', 'Noam Velan', 'Ofir Roval', 'Ori Ram', 'Polina Zablotskaia', 'Prathamesh Bang', 'Priyanka Agrawal', 'Rakesh Ghiya', 'Sanjay Ganapathy', 'Simon Baumgartner', 'Sofia Erell', 'Sushant Prakash', 'Thibault Sellam', 'Vikram Rao', 'Xuanhui Wang', 'Yaroslav Akulov', 'Yulong Yang', 'Zhen Yang', 'Zhixin Lai', 'Zhongru Wu', 'Anca Dragan', 'Avinatan Hassidim', 'Fernando Pereira', 'Slav Petrov', 'Srinivasan Venkatachary', 'Tulsee Doshi', 'Yossi Matias', 'Sasha Goldshtein', 'Dipanjan Das']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'benchmarking', 'grounding', 'evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10791</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</title><link>https://arxiv.org/abs/2512.10780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks leading LLMs on real-world maternal and newborn healthcare triage queries across five Indian languages and Nepali, comparing native-script vs romanized user inputs.&lt;/li&gt;&lt;li&gt;Finds consistent performance degradation for romanized messages (F1 drop of 5–12 points) despite models often correctly inferring semantic intent.&lt;/li&gt;&lt;li&gt;Shows the gap is due to brittle classification behavior in the presence of orthographic noise rather than clinical reasoning failure, estimating nearly 2 million excess triage errors in a partner deployment.&lt;/li&gt;&lt;li&gt;Highlights a practical safety blind spot: apparent semantic understanding of romanized text does not guarantee reliable, safe actions in high-stakes medical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manurag Khullar', 'Utkarsh Desai', 'Poorva Malviya', 'Aman Dalmia', 'Zheyuan Ryan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'safety evaluation', 'healthcare safety', 'orthographic/linguistic robustness', 'deployment/real-world evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10780</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification</title><link>https://arxiv.org/abs/2512.10756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OPV (Outcome-based Process Verifier) that verifies summarized outcomes of long chain-of-thoughts to detect unreliable intermediate reasoning while remaining efficient.&lt;/li&gt;&lt;li&gt;Uses an iterative active-learning loop with expert annotation, Rejection Fine-Tuning (RFT), and Reinforcement Learning with Verifiable Rewards (RLVR) to improve verifier performance with fewer annotations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results on a held-out OPV-Bench, effective false-positive detection on synthetic data, and improves downstream policy-model accuracy when used for oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Wu', 'Lingkai Kong', 'Wenwei Zhang', 'Songyang Gao', 'Yuzhe Gu', 'Zhongrui Cai', 'Tianyou Ma', 'Yuhong Liu', 'Zhi Wang', 'Runyuan Ma', 'Guangyu Wang', 'Wei Li', 'Conghui He', 'Dahua Lin', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought verification', 'safety evaluation', 'alignment/automated oversight', 'active learning', 'verifiable rewards (RLVR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10756</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</title><link>https://arxiv.org/abs/2512.10739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Outcome-based Process Verifier (OPV) that verifies summarized outcomes from long chain-of-thoughts (CoTs) to catch unreliable intermediate reasoning while remaining efficient for large-scale annotation.&lt;/li&gt;&lt;li&gt;Uses an iterative active learning loop with expert annotations: annotate most-uncertain cases, train a new OPV via Rejection Fine-Tuning (RFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve verifier performance with fewer labels.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art verification performance on a held-out benchmark and shows OPV reduces false positives and improves downstream policy-model accuracy on olympiad-level mathematical problem solving tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songyang Gao', 'Yuzhe Gu', 'Zijian Wu', 'Lingkai Kong', 'Wenwei Zhang', 'Zhongrui Cai', 'Fan Zheng', 'Tianyou Ma', 'Junhao Shen', 'Haiteng Zhao', 'Duanyang Zhang', 'Huilun Zhang', 'Kuikun Liu', 'Chengqi Lyu', 'Yanhui Duan', 'Chiyu Chen', 'Ningsheng Ma', 'Jianfei Gao', 'Han Lyu', 'Dahua Lin', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'verifier', 'active-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10739</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation</title><link>https://arxiv.org/abs/2512.10734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a four-component pipeline for detecting and mitigating textual data biases (representation bias and explicit stereotypes) using LLM-generated word lists, a Demographic Representation Score, sociolinguistically informed filtering, and grammar-/context-aware counterfactual data augmentation.&lt;/li&gt;&lt;li&gt;Evaluates each pipeline component via human validation and baselines on datasets for gender, religion, and age, showing reductions in representation bias and explicit stereotypes in the data.&lt;/li&gt;&lt;li&gt;Fine-tunes several LLMs (0.6B–8B) on the debiased datasets and benchmarks model bias, finding that debiased training data does not consistently lead to reduced model bias and exposing gaps in current bias evaluation methodologies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebekka G\\"orge', 'Sujan Sai Gannamaneni', 'Tabea Naeven', 'Hammam Abdelwahab', "H\\'ector Allende-Cid", 'Armin B. Cremers', 'Lennard Helmer', 'Michael Mock', 'Anna Schmitz', 'Songkai Xue', 'Elif Yildirir', 'Maximilian Poretschkin', 'Stefan Wrobel']&lt;/li&gt;&lt;li&gt;Tags: ['data debiasing', 'bias detection', 'fairness evaluation', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10734</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RoleRMBench &amp; RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems</title><link>https://arxiv.org/abs/2512.10575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities (e.g., narrative management, role consistency, engagement).&lt;/li&gt;&lt;li&gt;Shows that general-purpose reward models perform poorly on subjective, persona-grounded judgments, with large gaps especially on narrative and stylistic dimensions.&lt;/li&gt;&lt;li&gt;Proposes RoleRM, a reward model trained with Continuous Implicit Preferences (CIP) that frames subjective evaluation as continuous, consistent pairwise supervision under multiple structuring strategies.&lt;/li&gt;&lt;li&gt;Reports RoleRM outperforming strong open- and closed-source reward models by ~24% on average, improving narrative coherence and stylistic fidelity; emphasizes importance of continuous preference representation and annotation consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hang Ding', 'Qiming Feng', 'Dongqi Liu', 'Qi Zhao', 'Tao Yao', 'Shuo Wang', 'Dongsheng Chen', 'Jian Li', 'Zhenye Gan', 'Jiangning Zhang', 'Chengjie Wang', 'Yabiao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['reward-modeling', 'alignment', 'dialogue', 'benchmark', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10575</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature</title><link>https://arxiv.org/abs/2512.10435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SRAP, a two-stage framework to detect and semantically reconstruct adversarially paraphrased 'tortured phrases' in scientific text: (1) token-level anomaly detection via a domain-specific masked LM (SciBERT) using pseudo-perplexity, and (2) source-based semantic reconstruction using dense retrieval (FAISS) and sentence alignment (SBERT).&lt;/li&gt;&lt;li&gt;Evaluated on a parallel corpus of adversarial scientific text; zero-shot baselines fail (0.00% restoration) while SRAP achieves 23.67% restoration accuracy and outperforms baseline methods.&lt;/li&gt;&lt;li&gt;Argues for static decision boundaries for robust detection in jargon-heavy domains and demonstrates forensic linking of obfuscated expressions back to likely source documents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agniva Maiti', 'Prajwal Panth', 'Suresh Chandra Satapathy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial paraphrasing', 'plagiarism detection', 'forensic reconstruction', 'masked language models', 'retrieval-augmented methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10435</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding</title><link>https://arxiv.org/abs/2512.10195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoMedic, a multi-agent simulation framework that converts static medical QA datasets into virtual patient profiles to generate realistic multi-turn clinical dialogues for evaluating LLMs as conversational agents.&lt;/li&gt;&lt;li&gt;Proposes the CARE metric (clinical conversational Accuracy, efficiency/strategy, Empathy, Robustness) to provide multi-faceted evaluation beyond simple accuracy.&lt;/li&gt;&lt;li&gt;Validates the framework and metrics with human expert review and presents guidance for developing safe, effective clinical conversational LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gyutaek Oh', 'Sangjoon Park', 'Byung-Hoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'clinical-LLMs', 'robustness', 'benchmarking', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10195</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning</title><link>https://arxiv.org/abs/2512.10150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames preserving safety during LLM fine-tuning as a continual learning (CL) problem to mitigate catastrophic forgetting of safety alignment.&lt;/li&gt;&lt;li&gt;Adapts and evaluates several CL approaches (regularization-based, memory-based, model merging) in a fine-tuning-as-a-service setting, considering both benign and poisoned user data.&lt;/li&gt;&lt;li&gt;Finds that CL methods consistently reduce attack success rates compared to standard fine-tuning; DER performs best while preserving task utility across multiple tasks (GSM8K, SST2, Code) and model families (LLaMA2-7B, Mistral-7B, Gemma-2B).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lama Alssum', 'Hani Itani', 'Hasan Abed Al Kader Hammoud', 'Philip Torr', 'Adel Bibi', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'continual learning', 'catastrophic forgetting', 'data poisoning', 'fine-tuning security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10150</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing</title><link>https://arxiv.org/abs/2512.10121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses a 'Statistical Smoothing Trap' as a cause of hallucination, weak logical coherence, and loss of persona in long-form domain-specific text generation.&lt;/li&gt;&lt;li&gt;Proposes the DeepNews agentic workflow combining dual-granularity retrieval (information-foraging with a 10:1 input ratio), schema-guided strategic planning (narrative schemas and Atomic Blocks), and 'adversarial constraint prompting' (tactics like Rhythm Break and Logic Fog) to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Presents empirical claims: a 'Knowledge Cliff' where truthfulness collapses below ~15,000 characters of retrieved context and stabilization of Hallucination-Free Rate above 85% with &gt;30,000 characters; reports improved real-world acceptance vs zero-shot SOTA in a blind test.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongjie Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'adversarial prompting', 'alignment', 'agentic workflows']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10121</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic</title><link>https://arxiv.org/abs/2511.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PaTAS, a parallel trust assessment framework that propagates input, parameter, and activation trust through neural networks using Subjective Logic.&lt;/li&gt;&lt;li&gt;Defines mechanisms for updating parameter trust during training and computing instance-specific trust at inference via Inference-Path Trust Assessment (IPTA).&lt;/li&gt;&lt;li&gt;Evaluates PaTAS on real-world and adversarial/poisoned datasets, showing it can distinguish benign vs. adversarial inputs and reveal reliability gaps where model confidence is misleading.&lt;/li&gt;&lt;li&gt;Claims interpretable, symmetric, and convergent trust estimates that complement accuracy and help detect poisoned, biased, or uncertain data scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Koffi Ismael Ouattara', 'Ioannis Krontiris', 'Theo Dimitrakos', 'Dennis Eisermann', 'Houda Labiod', 'Frank Kargl']&lt;/li&gt;&lt;li&gt;Tags: ['trust modeling', 'robustness', 'adversarial detection', 'data poisoning', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20586</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Verifying LLM Inference to Detect Model Weight Exfiltration</title><link>https://arxiv.org/abs/2511.02620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes model weight exfiltration during inference as a security game and specifies trust assumptions.&lt;/li&gt;&lt;li&gt;Proposes a verification framework that can provably mitigate steganographic exfiltration from model outputs.&lt;/li&gt;&lt;li&gt;Characterizes valid sources of nondeterminism in LLM inference and introduces two practical estimators for verification.&lt;/li&gt;&lt;li&gt;Evaluates the detector on 3B–30B open-weight models, demonstrating large reductions in exfiltratable information with low false-positive rates and substantial adversary slowdown.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roy Rinberg', 'Adam Karvonen', 'Alexander Hoover', 'Daniel Reuter', 'Keri Warr']&lt;/li&gt;&lt;li&gt;Tags: ['model weight exfiltration', 'inference security', 'steganography', 'detection/verification', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02620</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain</title><link>https://arxiv.org/abs/2509.14203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes long-run average-reward robust Markov decision processes (MDPs) in the constant-gain setting, addressing technical gaps in dynamic programming foundations for average-reward robust control.&lt;/li&gt;&lt;li&gt;Studies S-rectangular adversaries and possible information asymmetries between controller and adversary, focusing on the constant-gain robust Bellman equation — existence of solutions and their relation to optimal average reward.&lt;/li&gt;&lt;li&gt;Identifies conditions (including one-sided weak communication) under which solutions to the robust Bellman equation characterize optimal average reward and stationary policies, extending theory for robust dynamic decision making under long-run criteria.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shengbo Wang', 'Nian Si']&lt;/li&gt;&lt;li&gt;Tags: ['robust MDPs', 'robust RL', 'average-reward', 'adversarial MDP', 'dynamic programming theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14203</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lightweight Model Attribution and Detection of Synthetic Speech via Audio Residual Fingerprints</title><link>https://arxiv.org/abs/2411.14013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight, training-free method that computes standardized average residuals (audio minus filtered version) to extract model-agnostic fingerprints for synthetic-speech detection and model attribution.&lt;/li&gt;&lt;li&gt;Addresses three tasks: open-world single-model attribution, closed-world multi-model attribution, and real vs. synthetic classification, reporting AUROC &gt;99% and F1 = 0.91 for unseen-model detection using Mahalanobis distances.&lt;/li&gt;&lt;li&gt;Demonstrates robustness across multiple synthesis systems, languages, and common audio distortions (echo, moderate noise); data augmentation can further improve performance, targeting digital forensics and security use cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Mat\\'ias Pizarro", 'Mike Laszkiewicz', 'Dorothea Kolossa', 'Asja Fischer']&lt;/li&gt;&lt;li&gt;Tags: ['audio forensics', 'deepfake detection', 'model attribution', 'synthetic speech', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14013</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title><link>https://arxiv.org/abs/2511.10287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OutSafe-Bench, a large-scale multimodal safety benchmark with &gt;18k bilingual text prompts, 4.5k images, 450 audio clips, and 450 videos annotated across nine content risk categories.&lt;/li&gt;&lt;li&gt;Proposes the Multidimensional Cross Risk Score (MCRS) to model overlapping/correlated risks and FairScore, an explainable multi-reviewer weighted aggregation that uses top-performing models as adaptive juries to reduce single-model bias.&lt;/li&gt;&lt;li&gt;Provides systematic evaluation of nine state-of-the-art multimodal LLMs, demonstrating persistent safety vulnerabilities and gaps in multimodal content moderation.&lt;/li&gt;&lt;li&gt;Focuses on comprehensive safety assessment and metric/aggregation design to improve robustness and fairness of multimodal content-safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yuanshuai Li', 'Yingchao Yu', 'Lingjuan Lyu', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmark', 'multimodal-safety', 'toxicity-detection', 'evaluation-metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10287</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>If generative AI is the answer, what is the question?</title><link>https://arxiv.org/abs/2509.06120</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of generative modeling foundations and five major model families (autoregressive, VAE, flows, GANs, diffusion).&lt;/li&gt;&lt;li&gt;Introduces probabilistic and game-theoretic frameworks distinguishing density estimation from generation.&lt;/li&gt;&lt;li&gt;Discusses post-training modifications for deployment and practical considerations for generation tasks.&lt;/li&gt;&lt;li&gt;Contains a section on socially responsible generation covering privacy, detection of AI-generated content, and copyright/IP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ambuj Tewari']&lt;/li&gt;&lt;li&gt;Tags: ['generative-models', 'privacy', 'deepfake-detection', 'AI-safety', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06120</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling the Latent Directions of Reflection in Large Language Models</title><link>https://arxiv.org/abs/2508.16989</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies reflection in LLMs (no reflection / intrinsic / triggered) via latent activation directions and activation steering.&lt;/li&gt;&lt;li&gt;Builds steering vectors to identify reflection-inducing instructions and to causally enhance or suppress reflective behavior through activation interventions.&lt;/li&gt;&lt;li&gt;Empirical evaluation on GSM8k-adv and Cruxeval-o-adv with Qwen2.5-3B and Gemma3-4B-IT shows clear stratification and controllability; finds suppressing reflection is easier than stimulating it and highlights both defensive and adversarial implications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fu-Chieh Chang', 'Yu-Ting Lee', 'Pei-Yuan Wu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'activation steering', 'interpretability', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16989</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title><link>https://arxiv.org/abs/2508.03772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes two stability problems in Group Relative Policy Optimization (GRPO): token-level penalization (conflicting negative feedback on shared valuable tokens) and policy collapse (entropy increase shifting probability mass to unlikely tokens).&lt;/li&gt;&lt;li&gt;Proposes GTPO (Group-relative Trajectory-based Policy Optimization) which (a) skips negative gradient updates on shared valuable tokens while amplifying positive updates, and (b) filters out high-entropy completions above a provable threshold to avoid policy collapse.&lt;/li&gt;&lt;li&gt;Removes reliance on KL regularization and a reference model, claiming improved stability and superior performance on reasoning benchmarks (GSM8K, MATH, AIME 2024/2025, AMC 2023).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Simoni', 'Aleksandar Fontana', 'Giulio Rossolini', 'Andrea Saracino', 'Paolo Mori']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'policy optimization', 'training stability', 'RLHF', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03772</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title><link>https://arxiv.org/abs/2506.01625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two algorithms for Gaussian Process bandit optimization that aim to 'robustly satisficе' — consistently achieving a performance threshold τ despite unknown, potentially time-varying adversarial perturbations.&lt;/li&gt;&lt;li&gt;Derives theoretical guarantees: a sublinear regret bound under certain adversary/τ assumptions, and an alternative bound that scales with perturbation magnitude without assumptions on the adversary.&lt;/li&gt;&lt;li&gt;Empirical evaluations show the proposed methods outperform standard robust optimization approaches when the ambiguity set is misspecified.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artun Saday', 'Ya\\c{s}ar Cahit Y{\\i}ld{\\i}r{\\i}m', 'Cem Tekin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'robust optimization', 'bandits', 'Gaussian processes', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01625</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</title><link>https://arxiv.org/abs/2411.03752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new "Deferred Poisoning" attack that keeps training/validation performance normal but induces large local curvature (via Hessian singularization) so small perturbations or natural noise cause large degradation in performance.&lt;/li&gt;&lt;li&gt;Introduces a Singularization Regularization term to drive the Hessian at the optimum toward singularity; provides theoretical analysis and empirical validation on image classification tasks.&lt;/li&gt;&lt;li&gt;Demonstrates the attack's stealthiness (no obvious train/validation discrepancy) and its practical hazard by showing increased vulnerability to evasion attacks and natural noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao He', 'Jinyu Tian', 'Xianwei Zheng', 'Li Dong', 'Yuanman Li', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'poisoning attack', 'adversarial robustness', 'Hessian analysis', 'stealthy attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.03752</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</title><link>https://arxiv.org/abs/2512.10780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks leading LLMs on real-world maternal and newborn healthcare triage queries across five Indian languages and Nepali, comparing native-script vs romanized user inputs.&lt;/li&gt;&lt;li&gt;Finds consistent performance degradation for romanized messages (F1 drop of 5–12 points) despite models often correctly inferring semantic intent.&lt;/li&gt;&lt;li&gt;Shows the gap is due to brittle classification behavior in the presence of orthographic noise rather than clinical reasoning failure, estimating nearly 2 million excess triage errors in a partner deployment.&lt;/li&gt;&lt;li&gt;Highlights a practical safety blind spot: apparent semantic understanding of romanized text does not guarantee reliable, safe actions in high-stakes medical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manurag Khullar', 'Utkarsh Desai', 'Poorva Malviya', 'Aman Dalmia', 'Zheyuan Ryan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'safety evaluation', 'healthcare safety', 'orthographic/linguistic robustness', 'deployment/real-world evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10780</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification</title><link>https://arxiv.org/abs/2512.10756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OPV (Outcome-based Process Verifier) that verifies summarized outcomes of long chain-of-thoughts to detect unreliable intermediate reasoning while remaining efficient.&lt;/li&gt;&lt;li&gt;Uses an iterative active-learning loop with expert annotation, Rejection Fine-Tuning (RFT), and Reinforcement Learning with Verifiable Rewards (RLVR) to improve verifier performance with fewer annotations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results on a held-out OPV-Bench, effective false-positive detection on synthetic data, and improves downstream policy-model accuracy when used for oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Wu', 'Lingkai Kong', 'Wenwei Zhang', 'Songyang Gao', 'Yuzhe Gu', 'Zhongrui Cai', 'Tianyou Ma', 'Yuhong Liu', 'Zhi Wang', 'Runyuan Ma', 'Guangyu Wang', 'Wei Li', 'Conghui He', 'Dahua Lin', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought verification', 'safety evaluation', 'alignment/automated oversight', 'active learning', 'verifiable rewards (RLVR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10756</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title><link>https://arxiv.org/abs/2512.10675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a generative evaluation system built on a frontier video foundation model (Veo) optimized for robot action conditioning and multi-view consistency.&lt;/li&gt;&lt;li&gt;Uses generative editing and multi-view completion to synthesize realistic in-distribution and out-of-distribution (OOD) scene variations for policy evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates the system can predict relative policy performance, quantify impacts of different generalization axes, and perform red teaming to find physical and semantic safety violations.&lt;/li&gt;&lt;li&gt;Validated via 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints across five bimanual manipulation tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gemini Robotics Team', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robotics simulation', 'video foundation models', 'OOD generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10675</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Virtual camera detection: Catching video injection attacks in remote biometric systems</title><link>https://arxiv.org/abs/2512.10653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a machine learning-based virtual camera detection (VCD) method to counter video injection attacks (e.g., deepfakes, virtual camera software) against remote face biometric systems.&lt;/li&gt;&lt;li&gt;Uses session metadata collected from authentic user interactions as training data to distinguish genuine camera feeds from injected/virtual feeds.&lt;/li&gt;&lt;li&gt;Empirical evaluation demonstrates the model can detect video injection attempts and reduce the risk of bypassing face anti-spoofing (FAS) systems.&lt;/li&gt;&lt;li&gt;Focuses on practical design and validation of VCD as a complementary security layer for web-based biometric authentication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniyar Kurmankhojayev', 'Andrei Shadrikov', 'Dmitrii Gordin', 'Mikhail Shkorin', 'Danijar Gabdullin', 'Aigerim Kambetbayeva', 'Kanat Kuatov']&lt;/li&gt;&lt;li&gt;Tags: ['video injection', 'virtual camera detection', 'face anti-spoofing', 'deepfake detection', 'biometric security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10653</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks</title><link>https://arxiv.org/abs/2512.10637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an IDS for 5G/6G that uses dynamic neural networks with incremental learning to reduce costly full retraining.&lt;/li&gt;&lt;li&gt;Incorporates adversarial training to improve robustness against data poisoning and leverages fewer features plus statistical properties for efficiency.&lt;/li&gt;&lt;li&gt;Evaluated on the NSL-KDD dataset, reporting 82.33% multiclass classification accuracy and claimed resistance to dataset poisoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neha', 'Tarunpreet Bhatia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'data poisoning', 'intrusion detection', 'robustness', 'incremental learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10637</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs</title><link>https://arxiv.org/abs/2512.10600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Authority Backdoor', a proactive mechanism that locks DNN utility behind a specific trigger so the model only performs normally when the trigger (e.g., hardware fingerprint) is present.&lt;/li&gt;&lt;li&gt;Integrates certifiable robustness to make the implanted backdoor resistant to adaptive attacks aiming to remove or bypass it.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and robustness across multiple architectures and datasets, demonstrating the approach can provide enforceable access control for stolen models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yang', 'Shaofeng Li', 'Tian Dong', 'Xiangyu Xu', 'Guangchi Liu', 'Zhen Ling']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'model protection', 'certifiable robustness', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10600</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning</title><link>https://arxiv.org/abs/2512.10372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes D2M, a blockchain-backed decentralized data marketplace that combines federated learning, off-chain distributed execution (CoNE), and smart-contract auctions/escrow for buyer-seller interactions.&lt;/li&gt;&lt;li&gt;Introduces security mechanisms: a modified YODA protocol with growing execution sets for Byzantine resilience and Corrected OSMD to reduce impact of malicious or low-quality contributors; includes game-theoretic proofs of incentive compatibility.&lt;/li&gt;&lt;li&gt;Implements on Ethereum and evaluates robustness under adversarial settings on image benchmarks (MNIST, Fashion-MNIST, CIFAR-10), showing tolerance to a significant fraction of Byzantine nodes with limited accuracy degradation.&lt;/li&gt;&lt;li&gt;Claims privacy-preserving properties and scalable off-chain computation to mitigate blockchain limitations while integrating dispute resolution and economic incentives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Srivastava', 'Shalin Jain', 'Sneha Awathare', 'Nitin Awathare']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'blockchain', 'incentive design', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10372</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</title><link>https://arxiv.org/abs/2512.10296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLARE, a side-channel fingerprinting framework that uses flow- and packet-level statistics of encrypted Wi‑Fi traffic to infer a client's FL model architecture (e.g., CNN vs RNN).&lt;/li&gt;&lt;li&gt;Evaluated on various pre-trained and custom CNN/RNN variants over IEEE 802.11 Wi‑Fi, achieving &gt;98% F1 in closed-world and up to 91% in open-world scenarios.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to hardware, software, and data heterogeneity and is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, revealing a critical privacy vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain', 'Anik Mallik', 'Jeffrey Twigg', 'Fikadu Dagefu']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'side-channel-attack', 'model-fingerprinting', 'wireless-traffic-analysis', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10296</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding</title><link>https://arxiv.org/abs/2512.10195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoMedic, a multi-agent simulation framework that converts static medical QA datasets into virtual patient profiles to generate realistic multi-turn clinical dialogues for evaluating LLMs as conversational agents.&lt;/li&gt;&lt;li&gt;Proposes the CARE metric (clinical conversational Accuracy, efficiency/strategy, Empathy, Robustness) to provide multi-faceted evaluation beyond simple accuracy.&lt;/li&gt;&lt;li&gt;Validates the framework and metrics with human expert review and presents guidance for developing safe, effective clinical conversational LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gyutaek Oh', 'Sangjoon Park', 'Byung-Hoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'clinical-LLMs', 'robustness', 'benchmarking', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10195</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Aware Confidence Calibration for Automated Audio Captioning</title><link>https://arxiv.org/abs/2512.10170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes integrating a learned confidence prediction head into a Whisper-based automated audio captioning model to estimate uncertainty from decoder hidden states.&lt;/li&gt;&lt;li&gt;Redefines correctness using semantic similarity (CLAP embeddings and sentence-transformer/FENSE) rather than n-gram overlap, enabling a semantically grounded Expected Calibration Error (ECE).&lt;/li&gt;&lt;li&gt;Uses confidence-guided beam search to improve calibration (CLAP-based ECE 0.071 vs. greedy 0.488) while also improving standard captioning metrics on Clotho v2.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Dunker', 'Sai Akshay Menta', 'Snigdha Mohana Addepalli', 'Venkata Krishna Rayalu Garapati']&lt;/li&gt;&lt;li&gt;Tags: ['confidence-calibration', 'uncertainty-estimation', 'audio-captioning', 'semantic-evaluation', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10170</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</title><link>https://arxiv.org/abs/2512.10092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sparse autoencoder (SAE) embeddings whose dimensions correspond to interpretable concepts, enabling controllable, concept-level analysis of text corpora.&lt;/li&gt;&lt;li&gt;Shows SAE embeddings can detect semantic differences between datasets, surface unexpected concept correlations, and enable property-based retrieval and axis-aligned clustering more cheaply than LLM annotation or dense embeddings.&lt;/li&gt;&lt;li&gt;Demonstrates safety-relevant case studies: tracking OpenAI model behavior changes over time and discovering 'trigger' phrases learned by Tulu-3 from its training data, and claims improved reliability/cost-effectiveness for bias and behavior analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nick Jiang', 'Xiaoqing Sun', 'Lisa Dunlap', 'Lewis Smith', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'bias-detection', 'model-analysis', 'interpretability', 'privacy/memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10092</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs</title><link>https://arxiv.org/abs/2512.09953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZK-APEX, a zero-shot personalized unlearning method that modifies personalized models (no retraining) using sparse masking plus a small Group OBS compensation step informed by a blockwise empirical Fisher matrix.&lt;/li&gt;&lt;li&gt;Pairs the unlearning transformation with Halo2 zero-knowledge proofs so a provider can verify that a client applied the correct unlearning without seeing private data or personalized parameters.&lt;/li&gt;&lt;li&gt;Evaluated on Vision Transformer classification tasks and OPT125M (code) showing strong recovery of personalization utility, effective removal of targeted data influence, and proof-generation performance far faster and cheaper than retraining-based checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad M Maheri', 'Sunil Cotterill', 'Alex Davidson', 'Hamed Haddadi']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'zero-knowledge proofs', 'verifiable compliance', 'edge devices']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09953</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>QSTAformer: A Quantum-Enhanced Transformer for Robust Short-Term Voltage Stability Assessment against Adversarial Attacks</title><link>https://arxiv.org/abs/2512.09936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QSTAformer: a transformer architecture that embeds parameterized quantum circuits (PQCs) into attention mechanisms for short-term voltage stability assessment (STVSA).&lt;/li&gt;&lt;li&gt;Develops an adversarial training strategy to defend against white-box and gray-box attacks and benchmarks different PQC architectures for trade-offs in expressiveness, convergence, and efficiency.&lt;/li&gt;&lt;li&gt;Reports case studies on the IEEE 39-bus system showing competitive accuracy, lower complexity, and improved robustness under adversarial conditions; claims to be the first systematic study of adversarial vulnerability in quantum ML-based STVSA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Li', 'Chong Ma', 'Yuanzheng Li', 'Sen Li', 'Yanbo Chen', 'Zhaoyang Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'quantum machine learning', 'transformer architectures', 'power systems (STVSA)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09936</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title><link>https://arxiv.org/abs/2512.10936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using modified Frank–Wolfe (projection-free) optimization methods to construct white-box adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and numerical comparisons against standard projection-based attack methods.&lt;/li&gt;&lt;li&gt;Evaluates performance on image classification tasks (MNIST, CIFAR-10) across multiclass logistic regression, CNNs, and ViT models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kristina Korotkova', 'Aleksandr Katrutsa']&lt;/li&gt;&lt;li&gt;Tags: ['white-box adversarial attacks', 'adversarial robustness', 'Frank-Wolfe / projection-free optimization', 'empirical benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10936</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes</title><link>https://arxiv.org/abs/2512.10878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that counterfactual explanations—typically near decision boundaries—are informative for reconstructing classifiers but are less representative than normal samples.&lt;/li&gt;&lt;li&gt;Proposes integrating original data with counterfactuals to compute class prototypes via the Wasserstein barycenter, preserving class distributional structure.&lt;/li&gt;&lt;li&gt;Demonstrates that this counterfactual-aware prototype approach improves surrogate model fidelity and reduces decision-boundary shift when reconstructing target classifiers.&lt;/li&gt;&lt;li&gt;Validates method empirically across multiple datasets, highlighting benefits in limited labeled-data settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Zhao', 'Zhuo Cao', 'Arya Bangun', 'Hanno Scharr', 'Ira Assent']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'counterfactual-explanations', 'surrogate-modeling', 'wasserstein-barycenter', 'decision-boundary']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10878</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality</title><link>https://arxiv.org/abs/2512.10720</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework using the causal minimality principle to make latent variables in generative models (diffusion vision and autoregressive language models) component-wise identifiable and causally interpretable.&lt;/li&gt;&lt;li&gt;Introduces hierarchical selection models where higher-level concepts arise from constrained compositions of lower-level variables, with minimality conditions (e.g., sparsity/compression) guaranteeing recovery of true latent factors.&lt;/li&gt;&lt;li&gt;Provides empirical methods to extract hierarchical concept graphs from leading generative models and demonstrates that these causally grounded concepts can be used for fine-grained model steering and control.&lt;/li&gt;&lt;li&gt;Claims that identifiable, causally interpretable representations improve transparency and enable more reliable alignment/control of generative systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingjing Kong', 'Shaoan Xie', 'Guangyi Chen', 'Yuewen Sun', 'Xiangchen Song', 'Eric P. Xing', 'Kun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'causal inference', 'model alignment', 'controllability', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10720</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Objective Reward and Preference Optimization: Theory and Algorithms</title><link>https://arxiv.org/abs/2512.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops algorithms for constrained reinforcement learning: ACPO for average-cost CMDPs using sensitivity analysis + trust-region updates with theoretical guarantees.&lt;/li&gt;&lt;li&gt;Introduces e-COP, a policy optimization method for episodic (finite-horizon) CMDPs with provable performance for safety-critical settings.&lt;/li&gt;&lt;li&gt;Advances preference learning / RLHF with warmPref-PS (posterior sampling integrating heterogeneous offline raters) and PSPL (joint sampling of reward models and dynamics) with Bayesian guarantees.&lt;/li&gt;&lt;li&gt;Applies a multi-objective constrained optimization perspective to large language model alignment via MOPO, an iterative scalable algorithm with closed-form updates for multi-billion-parameter models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akhil Agnihotri']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'constrained reinforcement learning', 'preference learning / RLHF', 'policy optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10601</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2512.10492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UACER: uses a diversified ensemble of K critic networks to stabilize Q-value estimation in adversarial reinforcement learning and reduce variance.&lt;/li&gt;&lt;li&gt;Introduces Time-varying Decay Uncertainty (TDU): a variance-derived Q-value aggregation that incorporates epistemic uncertainty to regulate exploration–exploitation and mitigate non-stationarity from a trainable adversary.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance, stability, and efficiency on several MuJoCo continuous-control benchmarks compared to state-of-the-art robust/adversarial RL baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'robustness', 'uncertainty estimation', 'ensemble methods', 'robotics/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10492</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title><link>https://arxiv.org/abs/2512.10402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical analysis of backdoor attacks, identifying 'ambiguous boundary' regions where few relabeled samples cause large misclassification effects.&lt;/li&gt;&lt;li&gt;Presents Eminence, a black-box backdoor framework that optimizes a universal, visually subtle trigger exploiting vulnerable decision boundaries, achieving high attack success with very low poison rates (&lt;0.1%).&lt;/li&gt;&lt;li&gt;Empirically validates theoretical claims (influence-function analysis, transferability, negligible clean-accuracy loss) across models and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Feng', 'Jiahao Chen', 'Chunyi Zhou', 'Yuwen Pu', 'Tianyu Du', 'Jinbao Li', 'Jianhai Chen', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attacks', 'data-poisoning', 'black-box-attacks', 'theoretical-analysis', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10402</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</title><link>https://arxiv.org/abs/2512.10350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric framework treating iterative LLM agentic loops as discrete dynamical systems in semantic embedding space, separating artifact (linguistic) and embedding measurement spaces.&lt;/li&gt;&lt;li&gt;Proposes an isotonic calibration to correct cosine-similarity bias from embedding anisotropy, aligning similarities with human judgments and enabling stable measurement of trajectories, clusters, and attractors.&lt;/li&gt;&lt;li&gt;Empirically identifies two fundamental dynamical regimes in controlled agentic loops: a contractive rewriting loop that converges to a stable attractor and an exploratory summarize-and-negate loop that diverges with no cluster formation; shows prompt design governs these regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicolas Tacheny']&lt;/li&gt;&lt;li&gt;Tags: ['agentic loops', 'alignment/safety evaluation', 'prompt engineering', 'embeddings/representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10350</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale</title><link>https://arxiv.org/abs/2512.10341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a cloud-native architecture combining federated learning, differential privacy, zero-knowledge compliance proofs, and RL-based governance to enable privacy-preserving distributed ML across multi-cloud/hybrid clusters.&lt;/li&gt;&lt;li&gt;Includes a prototype deployment on hybrid Kubernetes clusters and experimental evaluation showing reduced membership-inference risk, enforcement of formal privacy budgets, and stable utility under DP.&lt;/li&gt;&lt;li&gt;Claims cryptographic verifiability of policy enforcement and continuous, risk-aware governance with minimal performance overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinoth Punniyamoorthy', 'Ashok Gadi Parthi', 'Mayilsamy Palanigounder', 'Ravi Kiran Kodali', 'Bikesh Kumar', 'Kabilan Kannan']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'privacy-preserving-ml', 'cryptographic-proofs', 'governance-and-compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10341</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens</title><link>https://arxiv.org/abs/2512.10056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Soft-Token Trajectory Forecasting (SoTra): propagates continuous probability distributions ("soft tokens") during autoregressive rollouts to mitigate exposure bias and produce calibrated, uncertainty-aware multi-step trajectories.&lt;/li&gt;&lt;li&gt;Adds a risk-aware decoding module that optimizes decoding to minimize expected clinical harm in different operating zones.&lt;/li&gt;&lt;li&gt;Demonstrates empirical reductions in clinical risk (≈18% zone-based risk reduction in glucose forecasting; ≈15% effective risk reduction in blood-pressure forecasting).&lt;/li&gt;&lt;li&gt;Targets improved stability and safety of closed-loop predictive control in clinical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Namazi', 'Amirreza Dolatpour Fathkouhi', 'Heman Shakeri']&lt;/li&gt;&lt;li&gt;Tags: ['exposure bias', 'risk-aware forecasting', 'safety-critical ML', 'time-series forecasting', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10056</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</title><link>https://arxiv.org/abs/2512.10040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes four statistically principled weighting strategies for Multiple-Reference Preference Optimization (MRPO): two offline (using held-out validation), one online sliding-window estimator, and one online Thompson Sampling bandit approach.&lt;/li&gt;&lt;li&gt;Evaluates these strategies by fine-tuning Qwen2.5-0.5B using seven reference models (0.5B–14B) on UltraFeedback and SafeRLHF preference datasets, showing improvements over prior MRPO weighting baselines in preference accuracy.&lt;/li&gt;&lt;li&gt;Finds a surprising result that single-reference Direct Preference Optimization (DPO) using most individual references outperforms all tested multiple-reference approaches, questioning MRPO’s practical benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Skyler Wu', 'Aymen Echarghaoui']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization (DPO/MRPO)', 'fine-tuning / RLHF', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10040</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title><link>https://arxiv.org/abs/2512.07730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAVE, a training-free framework that uses a Sparse Autoencoder (SAE) to identify latent features most indicative of visual understanding via a binary object-presence QA probe.&lt;/li&gt;&lt;li&gt;Steers the multimodal model along these identified SAE features to reinforce grounded visual understanding, reducing object hallucination and increasing attention to image tokens.&lt;/li&gt;&lt;li&gt;Reports consistent improvements across benchmarks (e.g., ~10 percentage-point gain on CHAIR_S) and across multiple models and layers, with analyses showing suppression of uncertain object tokens and robustness/generalizability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangha Park', 'Seungryong Yoo', 'Jisoo Mok', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multimodal LLMs', 'sparse autoencoder', 'visual grounding', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07730</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</title><link>https://arxiv.org/abs/2512.06112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WAM-Flow, a vision-language-action model that frames ego-trajectory planning as discrete flow matching over a structured token space to enable parallel, coarse-to-fine trajectory generation.&lt;/li&gt;&lt;li&gt;Uses a metric-aligned numerical tokenizer, a geometry-aware flow objective, and a simulator-guided GRPO alignment that explicitly integrates safety, ego progress, and comfort rewards during training.&lt;/li&gt;&lt;li&gt;Converts an autoregressive backbone to a non-causal flow model with multi-stage adaptation and multimodal pretraining to improve road-scene competence.&lt;/li&gt;&lt;li&gt;Reports superior closed-loop performance on the NAVSIM v1 driving benchmark compared to autoregressive and diffusion baselines, emphasizing parallel decoding and tunable compute-accuracy trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifang Xu', 'Jiahao Cui', 'Feipeng Cai', 'Zhihao Zhu', 'Hanlin Shang', 'Shan Luan', 'Mingwang Xu', 'Neng Zhang', 'Yaoyi Li', 'Jia Cai', 'Siyu Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'motion-planning', 'safety-aware-training', 'discrete-flow-matching', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06112</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dark Speculation: Combining Qualitative and Quantitative Understanding in Frontier AI Risk Analysis</title><link>https://arxiv.org/abs/2511.21838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'dark speculation': combine qualitative scenario generation (thick catastrophic narratives) with quantitative underwriting to produce probability distributions over extreme AI harms.&lt;/li&gt;&lt;li&gt;Formalizes the approach with a simplified catastrophic Lévy stochastic framework and analyzes implications for risk aggregation and estimation.&lt;/li&gt;&lt;li&gt;Recommends institutional design features: independence between speculators and underwriters, parallel analysis of multiple risk categories, and iterative use (augmented by AI) to temper complacency and overreaction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Carpenter', 'Carson Ezell', 'Pratyush Mallick', 'Alexandria Westray']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Catastrophic risk analysis', 'Scenario planning', 'Policy / institutional design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21838</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.16203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLA-Fool, a framework for evaluating multimodal adversarial robustness of Vision-Language-Action (VLA) models under white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Defines three attack axes: textual perturbations (gradient- and prompt-based), visual perturbations (patches and noise), and cross-modal misalignment attacks that break semantic correspondence between perception and instruction.&lt;/li&gt;&lt;li&gt;Proposes a VLA-aware semantic space to craft semantically guided automatic prompts for more effective attacks.&lt;/li&gt;&lt;li&gt;Evaluates on LIBERO with a fine-tuned OpenVLA, showing minor multimodal perturbations can cause large behavioral deviations in embodied agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yixin Zhang', 'Lingjuan Lyu', 'Handing Wang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal robustness', 'vision-language-action', 'prompt attacks', 'cross-modal misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16203</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Risk-Bounded Multi-Agent Visual Navigation via Iterative Risk Allocation</title><link>https://arxiv.org/abs/2509.08157</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Risk-Bounded Multi-Agent Path Finding where agents share a global risk budget and per-agent risk allocations are adjusted during search rather than via static edge pruning.&lt;/li&gt;&lt;li&gt;Proposes an Iterative Risk Allocation (IRA) layer integrated with Conflict-Based Search (CBS), with two allocation strategies: greedy surplus-deficit and market-inspired priced-risk allocation.&lt;/li&gt;&lt;li&gt;Empirical results in complex visual environments show higher success rates and reduced travel time by exploiting available risk budget, enabling shorter paths under permissible risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viraj Parimi', 'Brian C. Williams']&lt;/li&gt;&lt;li&gt;Tags: ['safe navigation', 'multi-agent', 'risk allocation', 'visual RL/observations', 'planning (CBS)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08157</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title><link>https://arxiv.org/abs/2508.08139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reliability estimation method that uses token-level aleatoric and epistemic uncertainty from logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction.&lt;/li&gt;&lt;li&gt;Finds that correct in-context information improves answer accuracy and confidence, while misleading context often causes confidently incorrect responses, indicating misalignment between uncertainty and correctness.&lt;/li&gt;&lt;li&gt;Presents a probing-based approach that captures behavior shifts and improves detection of unreliable outputs across multiple open-source LLMs on open QA benchmarks, highlighting limitations of raw uncertainty signals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Zhou', 'Johanne Medina', 'Sanjay Chawla']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty estimation', 'LLM reliability', 'probing', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08139</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title><link>https://arxiv.org/abs/2508.03772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes two stability problems in Group Relative Policy Optimization (GRPO): token-level penalization (conflicting negative feedback on shared valuable tokens) and policy collapse (entropy increase shifting probability mass to unlikely tokens).&lt;/li&gt;&lt;li&gt;Proposes GTPO (Group-relative Trajectory-based Policy Optimization) which (a) skips negative gradient updates on shared valuable tokens while amplifying positive updates, and (b) filters out high-entropy completions above a provable threshold to avoid policy collapse.&lt;/li&gt;&lt;li&gt;Removes reliance on KL regularization and a reference model, claiming improved stability and superior performance on reasoning benchmarks (GSM8K, MATH, AIME 2024/2025, AMC 2023).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Simoni', 'Aleksandar Fontana', 'Giulio Rossolini', 'Andrea Saracino', 'Paolo Mori']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'policy optimization', 'training stability', 'RLHF', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03772</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title><link>https://arxiv.org/abs/2506.01625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two algorithms for Gaussian Process bandit optimization that aim to 'robustly satisficе' — consistently achieving a performance threshold τ despite unknown, potentially time-varying adversarial perturbations.&lt;/li&gt;&lt;li&gt;Derives theoretical guarantees: a sublinear regret bound under certain adversary/τ assumptions, and an alternative bound that scales with perturbation magnitude without assumptions on the adversary.&lt;/li&gt;&lt;li&gt;Empirical evaluations show the proposed methods outperform standard robust optimization approaches when the ambiguity set is misspecified.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artun Saday', 'Ya\\c{s}ar Cahit Y{\\i}ld{\\i}r{\\i}m', 'Cem Tekin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'robust optimization', 'bandits', 'Gaussian processes', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01625</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forensic deepfake audio detection using segmental speech features</title><link>https://arxiv.org/abs/2505.13847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates use of segmental (articulatory/interpretable) speech features to detect deepfake audio.&lt;/li&gt;&lt;li&gt;Finds certain segmental features effective for detection while some global features provide little value.&lt;/li&gt;&lt;li&gt;Proposes a speaker-specific detection framework aimed at forensic interpretability and sensitivity to individual phonetic realization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Yang', 'Chengzhe Sun', 'Siwei Lyu', 'Phil Rose']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'forensic voice comparison', 'speaker-specific detection', 'interpretable features', 'audio security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13847</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving</title><link>https://arxiv.org/abs/2504.20101</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a decentralized overlay (GenTorrent) for LLM serving to democratize and scale access by leveraging contributor nodes.&lt;/li&gt;&lt;li&gt;Identifies four core problems: overlay network organization, LLM communication privacy, overlay forwarding for resource efficiency, and verification of serving quality.&lt;/li&gt;&lt;li&gt;Implements a prototype showing &gt;50% latency reduction from overlay forwarding; claims security/privacy features add minimal overhead.&lt;/li&gt;&lt;li&gt;Focus is primarily systems and scalability with some attention to communication privacy and serving-quality verification mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fei Fang', 'Yifan Hua', 'Shengze Wang', 'Ruilin Zhou', 'Yi Liu', 'Chen Qian', 'Xiaoxue Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'decentralized-serving', 'model-serving-verification', 'scalability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20101</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title><link>https://arxiv.org/abs/2412.21051</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-PD, an architecture that leverages large language models to proactively detect, reason about, and mitigate Denial of Service (DoS) attacks in cloud environments.&lt;/li&gt;&lt;li&gt;Uses LLM capabilities for language understanding, data analysis, sequential reasoning, action planning, and code generation to create and deploy defense mechanisms dynamically.&lt;/li&gt;&lt;li&gt;Claims self-evolution/adaptation from experience without additional training, enabling response to new attack scenarios.&lt;/li&gt;&lt;li&gt;Presents a case study on three distinct DoS attacks showing improved defense effectiveness and efficiency versus existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyang Zhou', 'Guang Cheng', 'Kang Du', 'Zihan Chen', 'Yuyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM for security', 'DoS mitigation', 'Automated defense', 'Security orchestration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.21051</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying the Reliability of Predictions in Detection Transformers: Object-Level Calibration and Image-Level Uncertainty</title><link>https://arxiv.org/abs/2412.01782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes calibration and reliability of DETR-style object detectors, showing that within each image DETRs learn a specialist strategy: one prediction per object is well-calibrated while other predicted boxes suppress foreground confidence.&lt;/li&gt;&lt;li&gt;Proves this behavior arises as the loss-minimizing solution under Hungarian matching, and highlights that the well-calibrated predictions are unidentifiable at inference time—so post-processing can mix predictions with different calibration quality.&lt;/li&gt;&lt;li&gt;Proposes a new evaluation metric, Object-level Calibration Error (OCE), which penalizes keeping suppressed predictions and missing true foreground objects, aiming to better assess model+post-processing reliability than AP or ECE.&lt;/li&gt;&lt;li&gt;Presents a post-hoc uncertainty quantification framework to predict per-image model accuracy, enabling identification of images with unreliable detections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young-Jin Park', 'Carson Sobolewski', 'Navid Azizan']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty quantification', 'object detection', 'evaluation metrics', 'DETR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01782</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Foundation Models with Native Multi-Agent Intelligence</title><link>https://arxiv.org/abs/2512.08743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for developing foundation models with native multi-agent intelligence and defines four core capabilities: understanding, planning, efficient communication, and adaptation.&lt;/li&gt;&lt;li&gt;Provides empirical evidence across 41 LLMs showing strong single-agent performance does not reliably produce robust multi-agent abilities.&lt;/li&gt;&lt;li&gt;Outlines research directions including dataset construction, evaluation benchmarks, training paradigms, and safety considerations for multi-agent FMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyue Hu', 'Haoyang Yan', 'Yiqun Zhang', 'Yang Chen', 'Dongzhan Zhou', 'Lei Bai']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent', 'foundation-models', 'safety', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08743</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic</title><link>https://arxiv.org/abs/2511.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PaTAS, a parallel trust assessment framework that propagates input, parameter, and activation trust through neural networks using Subjective Logic.&lt;/li&gt;&lt;li&gt;Defines mechanisms for updating parameter trust during training and computing instance-specific trust at inference via Inference-Path Trust Assessment (IPTA).&lt;/li&gt;&lt;li&gt;Evaluates PaTAS on real-world and adversarial/poisoned datasets, showing it can distinguish benign vs. adversarial inputs and reveal reliability gaps where model confidence is misleading.&lt;/li&gt;&lt;li&gt;Claims interpretable, symmetric, and convergent trust estimates that complement accuracy and help detect poisoned, biased, or uncertain data scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Koffi Ismael Ouattara', 'Ioannis Krontiris', 'Theo Dimitrakos', 'Dennis Eisermann', 'Houda Labiod', 'Frank Kargl']&lt;/li&gt;&lt;li&gt;Tags: ['trust modeling', 'robustness', 'adversarial detection', 'data poisoning', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20586</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology</title><link>https://arxiv.org/abs/2506.18156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple LLMs using four cognitive psychology frameworks (TAT, Framing Bias, Moral Foundations Theory, Cognitive Dissonance) via structured prompts and automated scoring.&lt;/li&gt;&lt;li&gt;Finds models generate coherent narratives, are susceptible to positive framing, show moral judgments skewed toward Liberty/Oppression, and display self-contradictions with rationalization.&lt;/li&gt;&lt;li&gt;Argues these behaviors mirror human cognitive tendencies but are shaped by training data and alignment methods, and discusses implications for transparency and ethical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akash Kundu', 'Rishika Goswami']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety_evaluation', 'cognitive_bias', 'model_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18156</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data</title><link>https://arxiv.org/abs/2504.01951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether six LLMs can infer a user's gender from historical online purchase data and measures prediction accuracy.&lt;/li&gt;&lt;li&gt;Finds models achieve moderate gender-classification performance but rely heavily on stereotypical product-category associations.&lt;/li&gt;&lt;li&gt;Shows that explicit instructions to avoid bias lower model certainty but do not eliminate stereotyped reasoning patterns.&lt;/li&gt;&lt;li&gt;Highlights privacy and fairness implications and calls for more robust bias-mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Massimiliano Luca', 'Ciro Beneduce', 'Bruno Lepri', 'Jacopo Staiano']&lt;/li&gt;&lt;li&gt;Tags: ['gender-bias', 'privacy-inference', 'LLM-evaluation', 'bias-mitigation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01951</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title><link>https://arxiv.org/abs/2512.10936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using modified Frank–Wolfe (projection-free) optimization methods to construct white-box adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and numerical comparisons against standard projection-based attack methods.&lt;/li&gt;&lt;li&gt;Evaluates performance on image classification tasks (MNIST, CIFAR-10) across multiclass logistic regression, CNNs, and ViT models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kristina Korotkova', 'Aleksandr Katrutsa']&lt;/li&gt;&lt;li&gt;Tags: ['white-box adversarial attacks', 'adversarial robustness', 'Frank-Wolfe / projection-free optimization', 'empirical benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10936</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</title><link>https://arxiv.org/abs/2512.10791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the FACTS Leaderboard suite to evaluate LLM factuality across four sub-leaderboards: Multimodal (image-based), Parametric (closed-book knowledge), Search (information-seeking with search API), and Grounding v2 (long-form grounding in provided documents).&lt;/li&gt;&lt;li&gt;Uses automated judge models to score responses and aggregates the four components into a single suite score; includes public and private splits to enable participation while protecting integrity.&lt;/li&gt;&lt;li&gt;Maintained online benchmark intended for ongoing evaluation of model factuality with improved judge models for grounding and support for multimodal scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aileen Cheng', 'Alon Jacovi', 'Amir Globerson', 'Ben Golan', 'Charles Kwong', 'Chris Alberti', 'Connie Tao', 'Eyal Ben-David', 'Gaurav Singh Tomar', 'Lukas Haas', 'Yonatan Bitton', 'Adam Bloniarz', 'Aijun Bai', 'Andrew Wang', 'Anfal Siddiqui', 'Arturo Bajuelos Castillo', 'Aviel Atias', 'Chang Liu', 'Corey Fry', 'Daniel Balle', 'Deepanway Ghosal', 'Doron Kukliansky', 'Dror Marcus', 'Elena Gribovskaya', 'Eran Ofek', 'Honglei Zhuang', 'Itay Laish', 'Jan Ackermann', 'Lily Wang', 'Meg Risdal', 'Megan Barnes', 'Michael Fink', 'Mohamed Amin', 'Moran Ambar', 'Natan Potikha', 'Nikita Gupta', 'Nitzan Katz', 'Noam Velan', 'Ofir Roval', 'Ori Ram', 'Polina Zablotskaia', 'Prathamesh Bang', 'Priyanka Agrawal', 'Rakesh Ghiya', 'Sanjay Ganapathy', 'Simon Baumgartner', 'Sofia Erell', 'Sushant Prakash', 'Thibault Sellam', 'Vikram Rao', 'Xuanhui Wang', 'Yaroslav Akulov', 'Yulong Yang', 'Zhen Yang', 'Zhixin Lai', 'Zhongru Wu', 'Anca Dragan', 'Avinatan Hassidim', 'Fernando Pereira', 'Slav Petrov', 'Srinivasan Venkatachary', 'Tulsee Doshi', 'Yossi Matias', 'Sasha Goldshtein', 'Dipanjan Das']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'benchmarking', 'grounding', 'evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10791</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title><link>https://arxiv.org/abs/2512.10766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MJA, a metaphor-based jailbreaking attack for text-to-image models that crafts adversarial prompts to bypass diverse safety defenses without prior knowledge of defense type.&lt;/li&gt;&lt;li&gt;MJA has two components: MLAG (an LLM-based multi-agent module for metaphor retrieval, context matching, and adversarial prompt generation) and APO (an adversarial prompt optimization module that trains a surrogate predictor and uses an acquisition strategy to find optimal prompts).&lt;/li&gt;&lt;li&gt;Demonstrates stronger attack success and fewer queries than six baselines across T2I models with various internal and external defenses.&lt;/li&gt;&lt;li&gt;Provides code and emphasizes transferability of metaphorical prompts to evade unknown or heterogeneous defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Yiwen Ma', 'Lanjun Wang', 'Wenhui Li', 'Yi Tu', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'defense evasion', 'safety evaluation', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10766</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</title><link>https://arxiv.org/abs/2512.10739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Outcome-based Process Verifier (OPV) that verifies summarized outcomes from long chain-of-thoughts (CoTs) to catch unreliable intermediate reasoning while remaining efficient for large-scale annotation.&lt;/li&gt;&lt;li&gt;Uses an iterative active learning loop with expert annotations: annotate most-uncertain cases, train a new OPV via Rejection Fine-Tuning (RFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve verifier performance with fewer labels.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art verification performance on a held-out benchmark and shows OPV reduces false positives and improves downstream policy-model accuracy on olympiad-level mathematical problem solving tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songyang Gao', 'Yuzhe Gu', 'Zijian Wu', 'Lingkai Kong', 'Wenwei Zhang', 'Zhongrui Cai', 'Fan Zheng', 'Tianyou Ma', 'Junhao Shen', 'Haiteng Zhao', 'Duanyang Zhang', 'Huilun Zhang', 'Kuikun Liu', 'Chengqi Lyu', 'Yanhui Duan', 'Chiyu Chen', 'Ningsheng Ma', 'Jianfei Gao', 'Han Lyu', 'Dahua Lin', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'verifier', 'active-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10739</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation</title><link>https://arxiv.org/abs/2512.10734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a four-component pipeline for detecting and mitigating textual data biases (representation bias and explicit stereotypes) using LLM-generated word lists, a Demographic Representation Score, sociolinguistically informed filtering, and grammar-/context-aware counterfactual data augmentation.&lt;/li&gt;&lt;li&gt;Evaluates each pipeline component via human validation and baselines on datasets for gender, religion, and age, showing reductions in representation bias and explicit stereotypes in the data.&lt;/li&gt;&lt;li&gt;Fine-tunes several LLMs (0.6B–8B) on the debiased datasets and benchmarks model bias, finding that debiased training data does not consistently lead to reduced model bias and exposing gaps in current bias evaluation methodologies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebekka G\\"orge', 'Sujan Sai Gannamaneni', 'Tabea Naeven', 'Hammam Abdelwahab', "H\\'ector Allende-Cid", 'Armin B. Cremers', 'Lennard Helmer', 'Michael Mock', 'Anna Schmitz', 'Songkai Xue', 'Elif Yildirir', 'Maximilian Poretschkin', 'Stefan Wrobel']&lt;/li&gt;&lt;li&gt;Tags: ['data debiasing', 'bias detection', 'fairness evaluation', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10734</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title><link>https://arxiv.org/abs/2512.10675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a generative evaluation system built on a frontier video foundation model (Veo) optimized for robot action conditioning and multi-view consistency.&lt;/li&gt;&lt;li&gt;Uses generative editing and multi-view completion to synthesize realistic in-distribution and out-of-distribution (OOD) scene variations for policy evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates the system can predict relative policy performance, quantify impacts of different generalization axes, and perform red teaming to find physical and semantic safety violations.&lt;/li&gt;&lt;li&gt;Validated via 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints across five bimanual manipulation tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gemini Robotics Team', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robotics simulation', 'video foundation models', 'OOD generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10675</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2512.10492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UACER: uses a diversified ensemble of K critic networks to stabilize Q-value estimation in adversarial reinforcement learning and reduce variance.&lt;/li&gt;&lt;li&gt;Introduces Time-varying Decay Uncertainty (TDU): a variance-derived Q-value aggregation that incorporates epistemic uncertainty to regulate exploration–exploitation and mitigate non-stationarity from a trainable adversary.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance, stability, and efficiency on several MuJoCo continuous-control benchmarks compared to state-of-the-art robust/adversarial RL baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'robustness', 'uncertainty estimation', 'ensemble methods', 'robotics/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10492</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</title><link>https://arxiv.org/abs/2512.10415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of 'academic jailbreaking': adapting 20+ jailbreaking strategies to cause misgrading in LLM-based automated code evaluators.&lt;/li&gt;&lt;li&gt;Released a 25K poisoned/adversarial student-submission dataset paired with rubrics and human-graded references for code-evaluation settings.&lt;/li&gt;&lt;li&gt;Proposed three metrics (Jailbreak Success Rate, Score Inflation, Harmfulness) and evaluated six LLMs, finding high vulnerability (up to 97% JSR) particularly to persuasive and role-play attacks.&lt;/li&gt;&lt;li&gt;Provides an adversarial benchmarking suite intended to drive development of more robust LLM-based academic code evaluators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Vasudev Majhi', 'Arjun Neekhra', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM red teaming', 'model robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10415</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title><link>https://arxiv.org/abs/2512.10402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical analysis of backdoor attacks, identifying 'ambiguous boundary' regions where few relabeled samples cause large misclassification effects.&lt;/li&gt;&lt;li&gt;Presents Eminence, a black-box backdoor framework that optimizes a universal, visually subtle trigger exploiting vulnerable decision boundaries, achieving high attack success with very low poison rates (&lt;0.1%).&lt;/li&gt;&lt;li&gt;Empirically validates theoretical claims (influence-function analysis, transferability, negligible clean-accuracy loss) across models and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Feng', 'Jiahao Chen', 'Chunyi Zhou', 'Yuwen Pu', 'Tianyu Du', 'Jinbao Li', 'Jianhai Chen', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attacks', 'data-poisoning', 'black-box-attacks', 'theoretical-analysis', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10402</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning</title><link>https://arxiv.org/abs/2512.10372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes D2M, a blockchain-backed decentralized data marketplace that combines federated learning, off-chain distributed execution (CoNE), and smart-contract auctions/escrow for buyer-seller interactions.&lt;/li&gt;&lt;li&gt;Introduces security mechanisms: a modified YODA protocol with growing execution sets for Byzantine resilience and Corrected OSMD to reduce impact of malicious or low-quality contributors; includes game-theoretic proofs of incentive compatibility.&lt;/li&gt;&lt;li&gt;Implements on Ethereum and evaluates robustness under adversarial settings on image benchmarks (MNIST, Fashion-MNIST, CIFAR-10), showing tolerance to a significant fraction of Byzantine nodes with limited accuracy degradation.&lt;/li&gt;&lt;li&gt;Claims privacy-preserving properties and scalable off-chain computation to mitigate blockchain limitations while integrating dispute resolution and economic incentives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Srivastava', 'Shalin Jain', 'Sneha Awathare', 'Nitin Awathare']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'blockchain', 'incentive design', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10372</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</title><link>https://arxiv.org/abs/2512.10350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric framework treating iterative LLM agentic loops as discrete dynamical systems in semantic embedding space, separating artifact (linguistic) and embedding measurement spaces.&lt;/li&gt;&lt;li&gt;Proposes an isotonic calibration to correct cosine-similarity bias from embedding anisotropy, aligning similarities with human judgments and enabling stable measurement of trajectories, clusters, and attractors.&lt;/li&gt;&lt;li&gt;Empirically identifies two fundamental dynamical regimes in controlled agentic loops: a contractive rewriting loop that converges to a stable attractor and an exploratory summarize-and-negate loop that diverges with no cluster formation; shows prompt design governs these regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicolas Tacheny']&lt;/li&gt;&lt;li&gt;Tags: ['agentic loops', 'alignment/safety evaluation', 'prompt engineering', 'embeddings/representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10350</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale</title><link>https://arxiv.org/abs/2512.10341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a cloud-native architecture combining federated learning, differential privacy, zero-knowledge compliance proofs, and RL-based governance to enable privacy-preserving distributed ML across multi-cloud/hybrid clusters.&lt;/li&gt;&lt;li&gt;Includes a prototype deployment on hybrid Kubernetes clusters and experimental evaluation showing reduced membership-inference risk, enforcement of formal privacy budgets, and stable utility under DP.&lt;/li&gt;&lt;li&gt;Claims cryptographic verifiability of policy enforcement and continuous, risk-aware governance with minimal performance overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinoth Punniyamoorthy', 'Ashok Gadi Parthi', 'Mayilsamy Palanigounder', 'Ravi Kiran Kodali', 'Bikesh Kumar', 'Kabilan Kannan']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'privacy-preserving-ml', 'cryptographic-proofs', 'governance-and-compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10341</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</title><link>https://arxiv.org/abs/2512.10296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLARE, a side-channel fingerprinting framework that uses flow- and packet-level statistics of encrypted Wi‑Fi traffic to infer a client's FL model architecture (e.g., CNN vs RNN).&lt;/li&gt;&lt;li&gt;Evaluated on various pre-trained and custom CNN/RNN variants over IEEE 802.11 Wi‑Fi, achieving &gt;98% F1 in closed-world and up to 91% in open-world scenarios.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to hardware, software, and data heterogeneity and is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, revealing a critical privacy vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain', 'Anik Mallik', 'Jeffrey Twigg', 'Fikadu Dagefu']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'side-channel-attack', 'model-fingerprinting', 'wireless-traffic-analysis', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10296</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</title><link>https://arxiv.org/abs/2512.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RobustSora, a 6,500-video benchmark with four classes (Authentic-Clean, Authentic-Spoofed with fake watermarks, Generated-Watermarked, Generated-DeWatermarked) to measure watermark effects on AIGC video detection.&lt;/li&gt;&lt;li&gt;Defines two evaluation tasks: (I) detect AI-generated videos after watermark removal, and (II) measure false alarms on authentic videos with fake watermarks.&lt;/li&gt;&lt;li&gt;Evaluates ten detectors (specialized AIGC detectors, transformer models, and MLLMs), finding 2–8 percentage-point performance shifts under watermark manipulation and showing transformer models have moderate watermark dependency.&lt;/li&gt;&lt;li&gt;Concludes detectors partially rely on watermark signals and recommends watermark-aware training strategies to improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Wang', 'Xiliang Liu', 'Ligang Sun']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'watermark robustness', 'adversarial manipulation', 'benchmarking', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10248</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Offscript: Automated Auditing of Instruction Adherence in LLMs</title><link>https://arxiv.org/abs/2512.10172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Offscript, an automated auditing tool to detect when LLMs deviate from user-provided custom instructions during information-seeking conversations.&lt;/li&gt;&lt;li&gt;Pilot study on Reddit-sourced custom instructions found potential deviations in 86.4% of conversations; 22.2% of these were confirmed as material violations by human reviewers.&lt;/li&gt;&lt;li&gt;Demonstrates automated auditing as a scalable approach for evaluating compliance with behavioral instructions, relevant to alignment and safety assessments of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas Clark', 'Ryan Bai', 'Tanu Mitra']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'auditing', 'alignment', 'safety-evaluation', 'behavioral-compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10172</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning</title><link>https://arxiv.org/abs/2512.10150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames preserving safety during LLM fine-tuning as a continual learning (CL) problem to mitigate catastrophic forgetting of safety alignment.&lt;/li&gt;&lt;li&gt;Adapts and evaluates several CL approaches (regularization-based, memory-based, model merging) in a fine-tuning-as-a-service setting, considering both benign and poisoned user data.&lt;/li&gt;&lt;li&gt;Finds that CL methods consistently reduce attack success rates compared to standard fine-tuning; DER performs best while preserving task utility across multiple tasks (GSM8K, SST2, Code) and model families (LLaMA2-7B, Mistral-7B, Gemma-2B).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lama Alssum', 'Hani Itani', 'Hasan Abed Al Kader Hammoud', 'Philip Torr', 'Adel Bibi', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'continual learning', 'catastrophic forgetting', 'data poisoning', 'fine-tuning security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10150</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing</title><link>https://arxiv.org/abs/2512.10121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses a 'Statistical Smoothing Trap' as a cause of hallucination, weak logical coherence, and loss of persona in long-form domain-specific text generation.&lt;/li&gt;&lt;li&gt;Proposes the DeepNews agentic workflow combining dual-granularity retrieval (information-foraging with a 10:1 input ratio), schema-guided strategic planning (narrative schemas and Atomic Blocks), and 'adversarial constraint prompting' (tactics like Rhythm Break and Logic Fog) to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Presents empirical claims: a 'Knowledge Cliff' where truthfulness collapses below ~15,000 characters of retrieved context and stabilization of Hallucination-Free Rate above 85% with &gt;30,000 characters; reports improved real-world acceptance vs zero-shot SOTA in a blind test.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongjie Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'adversarial prompting', 'alignment', 'agentic workflows']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10121</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</title><link>https://arxiv.org/abs/2512.10040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes four statistically principled weighting strategies for Multiple-Reference Preference Optimization (MRPO): two offline (using held-out validation), one online sliding-window estimator, and one online Thompson Sampling bandit approach.&lt;/li&gt;&lt;li&gt;Evaluates these strategies by fine-tuning Qwen2.5-0.5B using seven reference models (0.5B–14B) on UltraFeedback and SafeRLHF preference datasets, showing improvements over prior MRPO weighting baselines in preference accuracy.&lt;/li&gt;&lt;li&gt;Finds a surprising result that single-reference Direct Preference Optimization (DPO) using most individual references outperforms all tested multiple-reference approaches, questioning MRPO’s practical benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Skyler Wu', 'Aymen Echarghaoui']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization (DPO/MRPO)', 'fine-tuning / RLHF', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10040</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs</title><link>https://arxiv.org/abs/2512.09953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZK-APEX, a zero-shot personalized unlearning method that modifies personalized models (no retraining) using sparse masking plus a small Group OBS compensation step informed by a blockwise empirical Fisher matrix.&lt;/li&gt;&lt;li&gt;Pairs the unlearning transformation with Halo2 zero-knowledge proofs so a provider can verify that a client applied the correct unlearning without seeing private data or personalized parameters.&lt;/li&gt;&lt;li&gt;Evaluated on Vision Transformer classification tasks and OPT125M (code) showing strong recovery of personalization utility, effective removal of targeted data influence, and proof-generation performance far faster and cheaper than retraining-based checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad M Maheri', 'Sunil Cotterill', 'Alex Davidson', 'Hamed Haddadi']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'zero-knowledge proofs', 'verifiable compliance', 'edge devices']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09953</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions</title><link>https://arxiv.org/abs/2512.10822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces V-OCBF, a framework to learn neural Control Barrier Functions (CBFs) from offline demonstration data without requiring a dynamics model.&lt;/li&gt;&lt;li&gt;Derives a recursive finite-difference barrier update enabling model-free propagation of safety over time and uses an expectile-based objective to avoid querying the barrier on out-of-distribution actions.&lt;/li&gt;&lt;li&gt;Restricts updates to dataset-supported actions and combines the learned barrier with a Quadratic Program (QP) to produce real-time safe control.&lt;/li&gt;&lt;li&gt;Evaluations report substantially fewer safety violations than baselines while retaining strong task performance, enabling offline synthesis of safety-critical controllers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mumuksh Tayal', 'Manan Tayal', 'Aditya Singh', 'Shishir Kolathaya', 'Ravi Prakash']&lt;/li&gt;&lt;li&gt;Tags: ['safe offline RL', 'control barrier functions', 'safety filters', 'offline learning', 'safe control synthesis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10822</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Challenges of Evaluating LLM Safety for User Welfare</title><link>https://arxiv.org/abs/2512.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that evaluating LLM safety for user welfare requires access to rich user context: identical model responses are rated much safer by context-blind evaluators than by evaluators aware of user vulnerability.&lt;/li&gt;&lt;li&gt;Finds that adding context to user prompts (what users say they would disclose) does not close the safety-assessment gap, indicating realistic prompt disclosure is insufficient.&lt;/li&gt;&lt;li&gt;Provides a methodology for context-aware safety evaluation across diverse user profiles and releases code and dataset to support future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manon Kempermann', 'Sai Suresh Macharla Vasu', 'Mahalakshmi Raveenthiran', 'Theo Farrell', 'Ingmar Weber']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'user-welfare', 'safety evaluation', 'vulnerable users']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10687</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity</title><link>https://arxiv.org/abs/2512.10665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Simulates multi-agent communities composed of LLMs whose individual 'values' are elicited using Schwartz's Theory of Basic Human Values.&lt;/li&gt;&lt;li&gt;Analyzes how varying degrees of value diversity affect collective dynamics such as value stability, emergent behaviors, and the generation of creative internal principles.&lt;/li&gt;&lt;li&gt;Finds that moderate value diversity improves stability and creativity, while extreme heterogeneity can produce instability; frames value diversity as a new axis for AI capability and institutional emergence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhua Huang', 'Qinlin Zhao', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent LLMs', 'value alignment', 'emergent behavior', 'collective intelligence', 'sociotechnical dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10665</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2512.10655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAPTAIN, a training-free inference-time framework that mitigates memorization in text-to-image diffusion models by modifying latent features during denoising.&lt;/li&gt;&lt;li&gt;Uses frequency-based noise initialization, identifies optimal denoising timesteps and localizes memorized regions, then injects semantically aligned features from non-memorized reference images into localized latent regions.&lt;/li&gt;&lt;li&gt;Claims substantial reductions in memorization compared to classifier-free guidance (CFG) baselines while preserving prompt fidelity and visual quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Zhang', 'Carlos Hinojosa', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy_mitigation', 'diffusion_models', 'inference_time_defense', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10655</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title><link>https://arxiv.org/abs/2512.10449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerability of LLM-based scientific reviewers to indirect prompt injection via adversarially manipulated PDF content, aiming to flip 'Reject' decisions to 'Accept'.&lt;/li&gt;&lt;li&gt;Introduces WAVS (Weighted Adversarial Vulnerability Score) as a metric, curates a 200-paper dataset, and adapts 15 domain-specific attack strategies evaluated across 13 LLMs (e.g., GPT-5, Claude Haiku).&lt;/li&gt;&lt;li&gt;Finds that obfuscation/injection strategies can achieve high decision-flip rates even on large models (e.g., 'Maximum Mark Magyk'), and releases dataset and injection framework for further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Manish Prasad', 'Vasudev Majhi', 'Jahnvi Singh', 'Vinay Chamola', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'adversarial attacks', 'LLM robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10449</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Targeted Data Protection for Diffusion Model by Matching Training Trajectory</title><link>https://arxiv.org/abs/2512.10433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TAFAP, a method that protects image training data for diffusion models by perturbing examples so fine-tuning trajectories are redirected toward attacker-specified target concepts rather than the original content.&lt;/li&gt;&lt;li&gt;Uses trajectory-matching (inspired by dataset distillation) plus adversarial perturbations to control the entire fine-tuning dynamics, addressing shortcomings of snapshot-matching defenses.&lt;/li&gt;&lt;li&gt;Claims robust, verifiable targeted transformation of model outputs (identity and visual patterns) while preserving image quality, improving over prior Targeted Data Protection (TDP) approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hojun Lee', 'Mijin Koo', 'Yeji Song', 'Nojun Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving data poisoning', 'diffusion models', 'targeted data protection', 'trajectory-matching/dataset distillation', 'adversarial perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10433</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention</title><link>https://arxiv.org/abs/2512.10414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SaEI (Selective-adversarial Entropy Intervention) to boost RL-based visual reasoning by increasing policy entropy via adversarial perturbations to visual inputs during RL sampling.&lt;/li&gt;&lt;li&gt;Introduces entropy-guided adversarial sampling (EgAS) which treats the entropy of sampled responses as an adversarial objective to compute gradients that perturb images and expand the answer space.&lt;/li&gt;&lt;li&gt;Proposes token-selective entropy computation (TsEC) to focus adversarial interventions on tokens that improve exploration while avoiding distortion of factual knowledge in the VLM.&lt;/li&gt;&lt;li&gt;Reports improved in-domain and out-of-domain reasoning performance through increased exploration enabled by the adversarial entropy intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Yu', 'Zhuangzhuang Chen', 'Siqi Wang', 'Lanqing Li', 'Xiaomeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'robustness', 'reinforcement-learning', 'vision-language-models', 'exploration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10414</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature</title><link>https://arxiv.org/abs/2512.10348</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes REMISVFU, a plug-and-play representation misdirection method for client-level unlearning in split vertical federated learning (VFL).&lt;/li&gt;&lt;li&gt;Forgetting party collapses its encoder outputs to a random unit-sphere anchor on deletion requests, breaking the statistical link to the global model.&lt;/li&gt;&lt;li&gt;Server jointly optimizes a retention loss and a forgetting loss, aligning gradients via orthogonal projection to reduce interference and preserve remaining utility.&lt;/li&gt;&lt;li&gt;Evaluations show suppression of backdoor attack success to class-prior level with ~2.5 percentage-point drop in clean accuracy, outperforming prior baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zhili He', 'Huanghuang Liang', 'Yili Gong', 'Jiawei Jiang', 'Chuang Hu', 'Dazhao Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated-unlearning', 'vertical-federated-learning', 'privacy', 'backdoor-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10348</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance</title><link>https://arxiv.org/abs/2512.10304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Ten Criteria for Trustworthy Orchestration AI and a Control-Plane governance architecture to embed assurance into AI execution.&lt;/li&gt;&lt;li&gt;Emphasizes human input, semantic coherence, auditability, provenance integrity, verifiability, transparency, reproducibility, and meaningful human control.&lt;/li&gt;&lt;li&gt;Positions the framework as an umbrella governance layer covering AI components, human participants, and consumers, drawing on international standards and Australia's AI assurance initiative.&lt;/li&gt;&lt;li&gt;Focuses on engineering governance into systems rather than purely ethical guidance or agent-to-agent coordination.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Byeong Ho Kang', 'Wenli Yang', 'Muhammad Bilal Amin']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'AI assurance', 'safety/alignment', 'audit &amp; provenance', 'control-plane architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10304</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reverse Thinking Enhances Missing Information Detection in Large Language Models</title><link>https://arxiv.org/abs/2512.10273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reverse thinking / backward reasoning framework to guide LLMs in identifying necessary conditions and pinpointing missing information.&lt;/li&gt;&lt;li&gt;Reframes missing-information detection as a backward reasoning problem rather than forward-chain CoT/ToT approaches.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains over traditional forward reasoning methods on tasks requiring detection of omitted elements, improving logical completeness and reducing hallucinations or incomplete answers.&lt;/li&gt;&lt;li&gt;Presents the approach as a method to enhance reasoning robustness and mitigate common failure modes (incomplete responses, factual errors) in LLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Liu', 'Chaojie Gu', 'Yihang Zhang', 'Bin Qian', 'Shibo He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination detection', 'reasoning robustness', 'alignment', 'backward reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10273</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment</title><link>https://arxiv.org/abs/2512.10206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;CP-Env is a controllable agentic hospital environment that simulates end-to-end clinical pathways (triage, consultations, testing, MDT meetings) for LLM agents.&lt;/li&gt;&lt;li&gt;Introduces a three-tier evaluation framework: Clinical Efficacy, Process Competency, and Professional Ethics to assess model performance across long-horizon, branching care flows.&lt;/li&gt;&lt;li&gt;Findings: many LLMs struggle with pathway complexity—hallucinations, loss of critical diagnostic details; excessive reasoning steps can harm performance; top models show reduced tool dependence by internalizing knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yakun Zhu', 'Zhongzhen Huang', 'Qianhan Feng', 'Linjie Mu', 'Yannian Gu', 'Shaoting Zhang', 'Qi Dou', 'Xiaofan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'medical-LLMs', 'hallucination', 'benchmark', 'clinical-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10206</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice</title><link>https://arxiv.org/abs/2512.10114</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgriRegion, a RAG framework that injects geospatial metadata and applies region-prioritized re-ranking to ensure retrieval is locally relevant for agricultural advice.&lt;/li&gt;&lt;li&gt;Restricts knowledge sources to verified local agricultural extension services and enforces geo-spatial constraints to reduce contextual hallucination and region-mismatch errors.&lt;/li&gt;&lt;li&gt;Presents AgriRegion-Eval, a benchmark of 160 domain-specific questions across 12 agricultural subfields, and reports a 10–20% reduction in hallucinations and improved trust scores versus baseline LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mesafint Fanuel', 'Mahmoud Nabil Mahmoud', 'Crystal Cook Marshal', 'Vishal Lakhotia', 'Biswanath Dari', 'Kaushik Roy', 'Shaohu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'RAG / retrieval robustness', 'hallucination mitigation', 'domain-specific alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10114</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust AI Security and Alignment: A Sisyphean Endeavor?</title><link>https://arxiv.org/abs/2512.10100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Claims and formalizes information-theoretic limits on robustness of AI security and alignment by extending Gödel-style incompleteness results to AI systems.&lt;/li&gt;&lt;li&gt;Derives implications for cognitive reasoning limitations of AI models and argues these limits impact provable security/alignment guarantees.&lt;/li&gt;&lt;li&gt;Offers practical approaches and recommendations to mitigate or prepare for these limitations in deployment and governance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Apostol Vassilev']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'theoretical-limits', 'formal-methods', 'AI-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10100</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</title><link>https://arxiv.org/abs/2512.10092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sparse autoencoder (SAE) embeddings whose dimensions correspond to interpretable concepts, enabling controllable, concept-level analysis of text corpora.&lt;/li&gt;&lt;li&gt;Shows SAE embeddings can detect semantic differences between datasets, surface unexpected concept correlations, and enable property-based retrieval and axis-aligned clustering more cheaply than LLM annotation or dense embeddings.&lt;/li&gt;&lt;li&gt;Demonstrates safety-relevant case studies: tracking OpenAI model behavior changes over time and discovering 'trigger' phrases learned by Tulu-3 from its training data, and claims improved reliability/cost-effectiveness for bias and behavior analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nick Jiang', 'Xiaoqing Sun', 'Lisa Dunlap', 'Lewis Smith', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'bias-detection', 'model-analysis', 'interpretability', 'privacy/memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10092</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Linear socio-demographic representations emerge in Large Language Models from indirect cues</title><link>https://arxiv.org/abs/2512.10065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLMs develop linear, interpretable activation-space directions encoding sociodemographic attributes (gender, race, occupation) across multiple transformer models.&lt;/li&gt;&lt;li&gt;Probes trained on explicit demographic disclosure generalize to implicit cues: names activate census-aligned gender/race representations and occupations correlate with workforce statistics.&lt;/li&gt;&lt;li&gt;Demonstrates these implicit demographic representations influence downstream model behavior (e.g., career recommendations) and can exist despite models passing standard bias benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paul Bouchaud', 'Pedro Ramaciotti']&lt;/li&gt;&lt;li&gt;Tags: ['demographic inference', 'implicit bias', 'fairness', 'privacy leakage', 'representation analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10065</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research</title><link>https://arxiv.org/abs/2512.10058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale bibliometric and co-authorship analysis of 6,442 ML/NLP papers (2020–2025) showing a pronounced split between 'AI safety' and 'AI ethics' communities.&lt;/li&gt;&lt;li&gt;Over 80% of collaborations occur within either the safety or ethics communities; ~5% of papers provide &gt;85% of cross-field bridging links, and removing these brokers markedly increases segregation.&lt;/li&gt;&lt;li&gt;Authors argue the split is institutional as well as conceptual and recommend integrating technical safety and normative ethics via shared benchmarks, cross-institutional venues, and mixed-methods approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dani Roytburg', 'Beck Miller']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'AI ethics', 'Alignment research', 'Research communities', 'Policy/benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10058</guid><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 09 Feb 2026 23:14:39 +0000</lastBuildDate><item><title>SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</title><link>https://arxiv.org/abs/2512.22170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SoliReward, a framework to train video reward models (RMs) that mitigates reward hacking and annotation noise via single-item binary annotations and cross-prompt pairing to build preference data.&lt;/li&gt;&lt;li&gt;Proposes architectural changes (Hierarchical Progressive Query Attention) and a modified Bradley–Terry loss that handles win/tie outcomes and regularizes positive-score distributions to reduce over-emphasis on top samples.&lt;/li&gt;&lt;li&gt;Validates improvements on benchmarks measuring physical plausibility, subject deformity, and semantic alignment, and demonstrates better downstream post-training of video generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiesong Lian', 'Ruizhe Zhong', 'Zixiang Zhou', 'Xiaoyue Mi', 'Yixue Hao', 'Yuan Zhou', 'Qinglin Lu', 'Long Hu', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['reward-models', 'reward-hacking', 'annotation-noise', 'robustness', 'video-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22170</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Anonymization Prompt Learning for Facial Privacy-Preserving Text-to-Image Generation</title><link>https://arxiv.org/abs/2405.16895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Anonymization Prompt Learning (APL): a learnable prompt prefix for text-to-image diffusion models that forces generation of anonymized facial identities even when prompted for specific individuals.&lt;/li&gt;&lt;li&gt;APL aims to preserve non-identity-specific image quality while removing identifiable facial features to mitigate deepfake generation and privacy risks.&lt;/li&gt;&lt;li&gt;Demonstrates quantitative and qualitative results and shows the learned prompt prefix is plug-and-play and transferable across different pretrained text-to-image models for broader privacy protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Shi', 'Jie Zhang', 'Shiguang Shan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'deepfake_defense', 'prompt_tuning', 'text-to-image', 'anonymization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.16895</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ALIEN: Analytic Latent Watermarking for Controllable Generation</title><link>https://arxiv.org/abs/2602.06101</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALIEN, an analytical latent watermarking framework for controllable embedding in latent diffusion models to protect IP and reduce misuse.&lt;/li&gt;&lt;li&gt;Derives a time-dependent modulation coefficient analytically to guide watermark residual diffusion, avoiding iterative heuristic optimization.&lt;/li&gt;&lt;li&gt;Reports substantial gains in fidelity and robustness vs. prior methods (claims: ALIEN-Q +33.1% on quality metrics; ALIEN-R +14.0% robustness across multiple conditions).&lt;/li&gt;&lt;li&gt;Provides experimental evaluation across quality and robustness scenarios and releases code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liangqi Lei', 'Keke Gai', 'Jing Yu', 'Qi Wu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'IP protection', 'generative model defense', 'latent diffusion models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06101</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction</title><link>https://arxiv.org/abs/2602.06676</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the 'heterogeneous phenomenon'—distinct artifact characteristics across image-forensic subdomains—that causes monolithic fake image detectors to underperform due to collapse of the artifact feature space.&lt;/li&gt;&lt;li&gt;Proposes Semantic-Induced Constrained Adaptation (SICA), which uses high-level semantics as structural priors to reconstruct a unified-yet-discriminative artifact feature space for a single (monolithic) detector.&lt;/li&gt;&lt;li&gt;Introduces and evaluates on an OpenMMSec dataset, showing SICA outperforms 15 state-of-the-art methods and achieves near-orthogonal artifact feature separation across subdomains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Du', 'Xiaochen Ma', 'Xuekang Zhu', 'Zhe Yang', 'Chaogun Niu', 'Jian Liu', 'Ji-Zhe Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['fake image detection', 'image forensics', 'defense/detection', 'feature-space robustness', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06676</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance</title><link>https://arxiv.org/abs/2602.06530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForgeryEraser, a universal anti-forensics attack that evades image forgery/AIGC detectors without access to target models (black-box).&lt;/li&gt;&lt;li&gt;Exploits a systemic vulnerability from shared Vision-Language Model (VLM) backbones (e.g., CLIP) by using a multi-modal guidance loss to push forged image embeddings toward text-derived authentic anchors and away from forgery anchors.&lt;/li&gt;&lt;li&gt;Demonstrates substantial degradation of state-of-the-art AIGC detectors on global synthesis and local editing benchmarks and causes forensic explainability outputs to resemble authentic images.&lt;/li&gt;&lt;li&gt;Highlights risks from publicly shared VLM feature spaces enabling transferable, detector-agnostic anti-forensic attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haipeng Li', 'Rongxuan Peng', 'Anwei Luo', 'Shunquan Tan', 'Changsheng Chen', 'Anastasia Antsiferova']&lt;/li&gt;&lt;li&gt;Tags: ['anti-forensics', 'adversarial-attack', 'AIGC-detection-evasion', 'vision-language-models', 'black-box-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06530</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection</title><link>https://arxiv.org/abs/2602.06452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes exploiting specular reflection inconsistencies (from Phong illumination model) as a robust cue to detect face forgeries, since specular highlights are hard for generative models to reproduce.&lt;/li&gt;&lt;li&gt;Introduces a Retinex-based fast face texture estimation to separate specular reflection and models relationships among specular reflection, face texture, and direct lighting.&lt;/li&gt;&lt;li&gt;Presents SRI-Net, a Specular-Reflection-Inconsistency-Network with a two-stage cross-attention mechanism to fuse specular-related features with image features, showing strong performance on traditional and diffusion-generated deepfake datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyan Fei', 'Zexi Jia', 'Chuanwei Huang', 'Jinchao Zhang', 'Jie Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery detection', 'image forensics', 'specular reflection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06452</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection</title><link>https://arxiv.org/abs/2602.06330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cascaded Early Rejection (CER), a two-stage OOD/anomaly detection framework to intercept low-level signal anomalies early and perform semantic OOD detection downstream.&lt;/li&gt;&lt;li&gt;Introduces Structural Energy Sieve (SES) using Laplacian-based non-parametric filtering at network input to catch physical/sensor anomalies cheaply.&lt;/li&gt;&lt;li&gt;Presents Semantically-aware Hyperspherical Energy (SHE) that decouples feature magnitude and direction in intermediate layers to detect semantic deviations and reduce hallucination.&lt;/li&gt;&lt;li&gt;Demonstrates computational savings (~32% reduction) and strong OOD detection improvements on CIFAR-100 and sensor-failure scenarios compared to SOTA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ningkang Peng', 'Chuanjie Cheng', 'Jingyang Mao', 'Xiaoqian Peng', 'Feng Xing', 'Bo Zhang', 'Chao Tan', 'Zhichao Zheng', 'Peiheng Li', 'Yanhui Gu']&lt;/li&gt;&lt;li&gt;Tags: ['OOD-detection', 'anomaly-detection', 'robustness', 'early-rejection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06330</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying the Effect of Test Set Contamination on Generative Evaluations</title><link>https://arxiv.org/abs/2601.04301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively measures how pretraining contamination (replicas of a test set in pretraining data) affects generative evaluation performance across model sizes, finding that even a single replica can substantially improve performance and can reduce loss below the uncontaminated irreducible error.&lt;/li&gt;&lt;li&gt;Analyzes post-pretraining interventions: continued training on fresh data reduces contamination effects, while supervised finetuning on the training set can either amplify or mitigate test performance depending on contamination level.&lt;/li&gt;&lt;li&gt;Identifies inference-time and example factors that modulate memorization: higher sampling temperature lessens contamination impact, and longer generated solutions are exponentially harder to memorize than short ones—highlighting differences from discriminative evaluations and complicating trustworthy evaluation of LMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rylan Schaeffer', 'Joshua Kazdan', 'Baber Abbasi', 'Ken Ziyu Liu', 'Brando Miranda', 'Ahmed Ahmed', 'Fazl Berez', 'Abhay Puri', 'Stella Biderman', 'Niloofar Mireshghallah', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['test-set contamination', 'unintended memorization', 'evaluation integrity', 'pretraining/finetuning dynamics', 'language model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04301</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer</title><link>https://arxiv.org/abs/2511.17673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Structured Cognitive Loop (R-CCAM) that decomposes LLM agent processing into Retrieval, Cognition, Control, Action, and Memory phases to separate reasoning from execution and improve traceability.&lt;/li&gt;&lt;li&gt;Introduces Soft Symbolic Control: a governance layer that enforces symbolic constraints over probabilistic neural inference to prevent policy violations, redundant tool use, and uncontrolled action sequences while preserving neural flexibility.&lt;/li&gt;&lt;li&gt;Empirical validation shows zero policy violations, elimination of redundant tool calls, and complete decision traceability; provides formalization, design principles for trustworthy agents, and an open-source implementation (including a GPT-4o-powered demo).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myung Ho Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Guardrails / Governance', 'Neuro-symbolic control', 'Agent safety', 'Explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17673</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large Language Models</title><link>https://arxiv.org/abs/2506.00062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that fine-tuning/general telecom continual pretraining of LLMs can degrade safety alignment, causing increased harmful responses even with light domain adaptation.&lt;/li&gt;&lt;li&gt;Introduces TeleHarm, a telecom-specific red-teaming benchmark, and evaluates models using TeleHarm plus DirectHarm and HexPhi to quantify harmful behavior.&lt;/li&gt;&lt;li&gt;Analyzes publicly available TeleLLMs and attributes safety failures largely to omission of safety-focused instruction tuning during telecom adaptation.&lt;/li&gt;&lt;li&gt;Proposes and evaluates three realignment defenses (SafeInstruct, SafeLoRA, SafeMERGE) that restore safety without harming telecom task performance, producing SafeCOMM models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Fernando Koch', 'Walid Saad', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red-teaming', 'fine-tuning', 'defenses', 'domain-adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00062</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OpenDeception: Learning Deception and Trust in Human-AI Interaction via Multi-Agent Simulation</title><link>https://arxiv.org/abs/2504.13707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenDeception, a framework to jointly evaluate deceptive behavior risk in human-AI dialogue via a 50-case scenario benchmark, IntentNet (deceptive intent inference), and TrustNet (user susceptibility estimation).&lt;/li&gt;&lt;li&gt;Synthesizes high-risk dialogues using LLM-based role-and-goal simulation and trains the Trust scorer with contrastive learning on controlled response pairs to avoid noisy scalar labels.&lt;/li&gt;&lt;li&gt;Evaluates 11 LLMs and three large reasoning models, finding high rates of goal-driven deceptive intent (over 90% in most models) and that stronger models can exhibit higher deception risk; demonstrates proactive warning capability in a real-world case study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Wu', 'Qianqian Gao', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'safety evaluation', 'adversarial/malicious behavior', 'trust modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13707</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient LLM Moderation with Multi-Layer Latent Prototypes</title><link>https://arxiv.org/abs/2502.16174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multi-Layer Prototype Moderator (MLPM), a lightweight input moderation method that uses prototypes of intermediate representations across multiple model layers to detect harmful inputs.&lt;/li&gt;&lt;li&gt;Designed to add negligible overhead to the generation pipeline and be model-agnostic, enabling seamless integration into existing LLM deployments.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on diverse moderation benchmarks and improved end-to-end safety when combined with output moderation techniques.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency, customizability to user-specific requirements, and scalability across model families and sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maciej Chrab\\k{a}szcz', 'Filip Szatkowski', "Bartosz W\\'ojcik", "Jan Dubi\\'nski", "Tomasz Trzci\\'nski", 'Sebastian Cygert']&lt;/li&gt;&lt;li&gt;Tags: ['moderation', 'defense', 'runtime safety', 'efficient detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16174</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title><link>https://arxiv.org/abs/2602.05444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models LLM safety alignment as an unobserved confounder and applies Pearl's front-door criterion to design jailbreaks.&lt;/li&gt;&lt;li&gt;Proposes CFA^2, a causal front-door adjustment attack that uses Sparse Autoencoders to remove defense-related features and converts marginalization into a deterministic intervention for efficient inference.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art jailbreak success rates and provides a mechanistic (causal) interpretation of the jailbreaking process.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Zhou', 'Zeen Song', 'Wenwen Qiang', 'Fengge Wu', 'Shuyi Zhou', 'Changwen Zheng', 'Hui Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'causal inference', 'model robustness', 'safety bypass']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05444</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review</title><link>https://arxiv.org/abs/2502.19614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large benchmark dataset of 788,984 AI-generated peer reviews paired with human reviews from 8 years of ICLR and NeurIPS submissions.&lt;/li&gt;&lt;li&gt;Evaluates 18 existing AI text detection algorithms on the task of distinguishing human-written vs. LLM-generated reviews.&lt;/li&gt;&lt;li&gt;Proposes and evaluates a context-aware detection method (Anchor) that leverages manuscript content to improve detection.&lt;/li&gt;&lt;li&gt;Analyzes robustness of detection models to LLM-assisted editing and finds identifying AI-generated text at the individual review level is difficult.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungduk Yu', 'Man Luo', 'Avinash Madasu', 'Vasudev Lal', 'Phillip Howard']&lt;/li&gt;&lt;li&gt;Tags: ['ai-generated-text-detection', 'benchmark', 'dataset', 'defense', 'peer-review-misuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19614</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Endogenous Resistance to Activation Steering in Language Models</title><link>https://arxiv.org/abs/2602.06941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes Endogenous Steering Resistance (ESR): language models (notably Llama-3.3-70B) can internally resist activation-steering during generation and recover to produce improved, task-aligned outputs even while steering remains active.&lt;/li&gt;&lt;li&gt;Uses sparse autoencoder (SAE) latents to steer activations and finds 26 latents causally linked to ESR; zero-ablating them reduces multi-attempt recovery rate by ~25%, indicating dedicated internal consistency-checking circuitry.&lt;/li&gt;&lt;li&gt;Demonstrates ESR can be enhanced via meta-prompts that induce self-monitoring and via fine-tuning on self-correction examples, and discusses dual security implications (protection against adversarial manipulation vs. interference with safety interventions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex McKenzie', 'Keenan Pepper', 'Stijn Servaes', 'Martin Leitgab', 'Murat Cubuktepe', 'Mike Vaiana', 'Diogo de Lucena', 'Judd Rosenblatt', 'Michael S. A. Graziano']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'model robustness', 'defenses', 'interpretability', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06941</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study</title><link>https://arxiv.org/abs/2602.06547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Built the first labeled dataset of malicious third-party agent skills by behaviorally verifying 98,380 skills and confirming 157 malicious skills containing 632 vulnerabilities.&lt;/li&gt;&lt;li&gt;Characterizes two attacker archetypes—Data Thieves (credential exfiltration via supply-chain techniques) and Agent Hijackers (instruction manipulation to subvert agent decision-making)—with malicious skills averaging 4.03 vulnerabilities across a median of three kill-chain phases.&lt;/li&gt;&lt;li&gt;Identifies ecosystem patterns: a single actor responsible for 54.1% of confirmed cases via templated brand impersonation, 'shadow features' present only in advanced attacks, and exploitation of platform hooks and permission flags; responsible disclosure led to 93.6% removal within 30 days.&lt;/li&gt;&lt;li&gt;Releases the dataset and analysis pipeline to support future security research, detection, and mitigation for LLM-based agent skill ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Zhihao Chen', 'Yanjun Zhang', 'Gelei Deng', 'Yuekang Li', 'Jianting Ning', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent skills', 'supply-chain attacks', 'instruction manipulation', 'malicious skills dataset', 'vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06547</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions</title><link>https://arxiv.org/abs/2602.06256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates inference-time model steering (intervening on hidden representations) with a focus on 'specificity' — whether interventions only change intended behaviors.&lt;/li&gt;&lt;li&gt;Proposes a three-way specificity framework: general (fluency/unrelated abilities), control (related control properties), and robustness (preserving control under distribution shifts).&lt;/li&gt;&lt;li&gt;Empirically studies safety-critical use cases (reducing overrefusal and hallucinations) and finds steering often preserves efficacy and general/control specificity but frequently breaks robustness, increasing vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;Highlights that standard efficacy and specificity checks can miss security regressions, so robustness evaluations (e.g., adversarial/jailbreak tests) are essential for safe steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Navita Goyal', "Hal Daum\\'e III"]&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'jailbreaking', 'robustness', 'inference-time defenses', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06256</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</title><link>https://arxiv.org/abs/2602.06920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Halluverse-M^3, a multilingual, multitask benchmark (English, Arabic, Hindi, Turkish) for studying hallucinations in LLMs across QA and dialogue summarization.&lt;/li&gt;&lt;li&gt;Differentiates hallucination types (entity-, relation-, sentence-level); constructs hallucinated outputs via controlled edits and human validation and releases the dataset.&lt;/li&gt;&lt;li&gt;Evaluates a range of open-source and proprietary models, finding QA easier than summarization, sentence-level hallucinations hardest, and performance degrading in lower-resource languages (Hindi lowest).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samir Abdaljalil', 'Parichit Sharma', 'Erchin Serpedin', 'Hasan Kurban']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'benchmark', 'multilingual', 'detection', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06920</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks</title><link>https://arxiv.org/abs/2602.06854</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEMA, a two-stage framework to train multi-turn jailbreak attackers for LLM chatbots: (1) prefilling self-tuning to generate stable multi-turn adversarial prompts, and (2) reinforcement learning with an intent-drift-aware reward to maintain harmful intent while eliciting compliance.&lt;/li&gt;&lt;li&gt;Introduces an intent-drift-aware reward combining intent alignment, compliance risk, and level-of-detail, and uses an open-loop attack regime (no victim feedback) to reduce exploration complexity and unify single- and multi-turn attacks.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art multi-turn attack success rates across multiple datasets and victim models, enabling stronger automatic red-teaming and vulnerability exposure for LLM safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Jialin Song', 'Xuekai Zhu', 'Chenliang Xu', 'Jianfeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'adversarial attacks', 'reinforcement learning', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06854</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought</title><link>https://arxiv.org/abs/2602.06650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PACT, a hierarchical policy-control framework for LLM safety combining a non-overridable global safety policy with user-defined domain policies to enable runtime controllability.&lt;/li&gt;&lt;li&gt;Uses risk-aware chain-of-thought reasoning and a structured Classify→Act routing (actions: comply, guide, reject) to make safety decisions transparent and configurable.&lt;/li&gt;&lt;li&gt;Evaluates PACT, showing near state-of-the-art safety under global policy evaluation while improving controllability under user-specific policies; authors plan to release models, data, and evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianfeng Si', 'Lin Sun', 'Weihong Lin', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'defense', 'guardrails', 'controllability', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06650</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention</title><link>https://arxiv.org/abs/2602.06623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a targeted subspace intervention method to identify and suppress hidden toxic patterns in LLM internal representations while preserving fluency.&lt;/li&gt;&lt;li&gt;Evaluates on RealToxicityPrompts and multiple LLMs, reporting 8–20% toxicity reduction compared to state-of-the-art detox systems with minimal inference overhead.&lt;/li&gt;&lt;li&gt;Claims effective toxicity mitigation without degrading generative performance, supported by quantitative and qualitative analyses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Singh', 'Ziwei Xu', 'A. V. Subramanyam', 'Mohan Kankanhalli']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'toxicity-mitigation', 'representation-intervention', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06623</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking</title><link>https://arxiv.org/abs/2602.06440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TrailBlazer, a history-aware RL framework for black-box LLM jailbreaking that reweights vulnerability signals from prior interaction turns to guide future actions.&lt;/li&gt;&lt;li&gt;Introduces an attention-based mechanism to highlight critical vulnerabilities in the interaction history, improving exploration efficiency and query efficiency.&lt;/li&gt;&lt;li&gt;Evaluates on AdvBench and HarmBench, showing state-of-the-art jailbreak success rates and reduced queries compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sung-Hoon Yoon', 'Ruizhi Qian', 'Minda Zhao', 'Weiyue Li', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'reinforcement learning', 'adversarial attacks', 'LLM safety', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06440</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs</title><link>https://arxiv.org/abs/2602.06268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MPIB, a dataset and benchmark for evaluating prompt injection attacks and clinical safety in LLMs and RAG systems.&lt;/li&gt;&lt;li&gt;Defines Clinical Harm Event Rate (CHER) to measure outcome-level clinical harm alongside Attack Success Rate (ASR).&lt;/li&gt;&lt;li&gt;Provides 9,697 curated instances, evaluation code, adversarial baselines, and documentation for reproducible testing.&lt;/li&gt;&lt;li&gt;Evaluates multiple baseline LLMs and defenses, showing ASR and CHER can diverge and robustness depends on injection location (user query vs retrieved context).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhyeok Lee', 'Han Jang', 'Kyu Sung Choi']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attacks', 'medical safety', 'benchmarking', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06268</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness</title><link>https://arxiv.org/abs/2511.12085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a defense-focused system for email phishing classification using DistilBERT, augmented with Fast Gradient Method (FGM) adversarial training to improve robustness against text-based/AI-generated phishing attacks.&lt;/li&gt;&lt;li&gt;Integrates LIME for model explainability and uses Flan-T5-small to produce plain-language security explanations for end users, combining technical interpretability with user-facing rationale.&lt;/li&gt;&lt;li&gt;Aims to deliver accurate phishing detection while enhancing adversarial robustness and transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajad U P']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'phishing detection', 'explainable AI', 'transformer models', 'text security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12085</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FuSeFL: Fully Secure and Scalable Federated Learning</title><link>https://arxiv.org/abs/2507.13591</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FuSeFL, a federated learning scheme that decentralizes training across client pairs using lightweight secure multiparty computation (MPC) and limits the server to secure aggregation, client pairing, and routing.&lt;/li&gt;&lt;li&gt;Claims to preserve full confidentiality of client data, global model, and updates throughout training, defending against model inversion, gradient leakage, and membership inference attacks.&lt;/li&gt;&lt;li&gt;Emphasizes practicality and scalability: avoids heavy cryptographic overheads, eliminates server bottlenecks, and reports up to 13x training speedup and 50% lower server memory usage versus a baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahar Ghoflsaz Ghinani', 'Elaheh Sadredini']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'secure-multiparty-computation', 'privacy-preserving-ml', 'defenses', 'membership-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13591</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large Language Models</title><link>https://arxiv.org/abs/2506.00062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that fine-tuning/general telecom continual pretraining of LLMs can degrade safety alignment, causing increased harmful responses even with light domain adaptation.&lt;/li&gt;&lt;li&gt;Introduces TeleHarm, a telecom-specific red-teaming benchmark, and evaluates models using TeleHarm plus DirectHarm and HexPhi to quantify harmful behavior.&lt;/li&gt;&lt;li&gt;Analyzes publicly available TeleLLMs and attributes safety failures largely to omission of safety-focused instruction tuning during telecom adaptation.&lt;/li&gt;&lt;li&gt;Proposes and evaluates three realignment defenses (SafeInstruct, SafeLoRA, SafeMERGE) that restore safety without harming telecom task performance, producing SafeCOMM models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Fernando Koch', 'Walid Saad', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red-teaming', 'fine-tuning', 'defenses', 'domain-adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00062</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Federated Learning via Byzantine Filtering over Encrypted Updates</title><link>https://arxiv.org/abs/2602.05410</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense for federated learning that combines CKKS homomorphic encryption for privacy-preserving aggregation with meta-classifier (SVM) based Byzantine filtering.&lt;/li&gt;&lt;li&gt;Trains filtering meta-classifiers on labeled shadow updates to detect diverse Byzantine behaviors (backdoor, gradient-inversion, label-flipping, shuffling) and reweights/cancels encrypted malicious updates.&lt;/li&gt;&lt;li&gt;Introduces automated selection of kernel and dimensionality hyperparameters to optimize encrypted inference and aggregation efficiency under CKKS.&lt;/li&gt;&lt;li&gt;Evaluated on FEMNIST, CIFAR10, GTSRB, and acsincome, reporting 90–94% detection accuracy with modest utility loss and encrypted inference/aggregation runtimes in the 6–26 second range.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adda Akram Bendoukha', 'Aymen Boudguiga', 'Nesrine Kaaniche', 'Renaud Sirdey', 'Didem Demirag', "S\\'ebastien Gambs"]&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'homomorphic encryption', 'attack detection', 'secure aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05410</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying the Effect of Test Set Contamination on Generative Evaluations</title><link>https://arxiv.org/abs/2601.04301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively measures how pretraining contamination (replicas of a test set in pretraining data) affects generative evaluation performance across model sizes, finding that even a single replica can substantially improve performance and can reduce loss below the uncontaminated irreducible error.&lt;/li&gt;&lt;li&gt;Analyzes post-pretraining interventions: continued training on fresh data reduces contamination effects, while supervised finetuning on the training set can either amplify or mitigate test performance depending on contamination level.&lt;/li&gt;&lt;li&gt;Identifies inference-time and example factors that modulate memorization: higher sampling temperature lessens contamination impact, and longer generated solutions are exponentially harder to memorize than short ones—highlighting differences from discriminative evaluations and complicating trustworthy evaluation of LMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rylan Schaeffer', 'Joshua Kazdan', 'Baber Abbasi', 'Ken Ziyu Liu', 'Brando Miranda', 'Ahmed Ahmed', 'Fazl Berez', 'Abhay Puri', 'Stella Biderman', 'Niloofar Mireshghallah', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['test-set contamination', 'unintended memorization', 'evaluation integrity', 'pretraining/finetuning dynamics', 'language model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04301</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</title><link>https://arxiv.org/abs/2512.22170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SoliReward, a framework to train video reward models (RMs) that mitigates reward hacking and annotation noise via single-item binary annotations and cross-prompt pairing to build preference data.&lt;/li&gt;&lt;li&gt;Proposes architectural changes (Hierarchical Progressive Query Attention) and a modified Bradley–Terry loss that handles win/tie outcomes and regularizes positive-score distributions to reduce over-emphasis on top samples.&lt;/li&gt;&lt;li&gt;Validates improvements on benchmarks measuring physical plausibility, subject deformity, and semantic alignment, and demonstrates better downstream post-training of video generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiesong Lian', 'Ruizhe Zhong', 'Zixiang Zhou', 'Xiaoyue Mi', 'Yixue Hao', 'Yuan Zhou', 'Qinglin Lu', 'Long Hu', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['reward-models', 'reward-hacking', 'annotation-noise', 'robustness', 'video-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22170</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning</title><link>https://arxiv.org/abs/2511.10936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GraphToxin, the first full-graph reconstruction attack targeting graph unlearning to recover deleted nodes and their sensitive connections.&lt;/li&gt;&lt;li&gt;Proposes a curvature matching module to guide fine-grained recovery and extends the attack to multiple-node removal in both white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Provides a systematic evaluation framework including random and worst-case node removal scenarios and shows existing defenses are largely ineffective or can worsen leakage.&lt;/li&gt;&lt;li&gt;Demonstrates practical privacy risks against GNN unlearning and emphasizes the need for stronger, robust defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ying Song', 'Balaji Palanisamy']&lt;/li&gt;&lt;li&gt;Tags: ['graph unlearning', 'graph neural networks', 'privacy attack', 'data reconstruction', 'model inversion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10936</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning</title><link>https://arxiv.org/abs/2510.26829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that continual pretraining on plausible misinformation can selectively overwrite factual knowledge in LLMs (belief flips) without degrading overall performance.&lt;/li&gt;&lt;li&gt;Quantifies how repeated exposure to counterfactual claims flips responses (55%+ at moderate poisoning), how flips concentrate in late layers, vary with scale, and generalize beyond poisoned prompts and languages.&lt;/li&gt;&lt;li&gt;Evaluates partial defenses: representation-level patching can partially reverse corruption (up to 56.8%) and suggests the need for monitoring factual integrity during model updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Svetlana Churina', 'Niranjan Chebrolu', 'Kokil Jaidka']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'continual pretraining', 'model integrity', 'representation-level attacks', 'defense/patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26829</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection</title><link>https://arxiv.org/abs/2510.03944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates eight goodness-of-fit (GoF) tests for detecting text watermarks across three watermarking schemes, three open-source LLMs, two datasets, multiple temperatures, and post-editing methods.&lt;/li&gt;&lt;li&gt;Finds that general GoF tests can improve detection power and robustness of watermark detectors compared to existing methods.&lt;/li&gt;&lt;li&gt;Identifies that text repetition (common at low sampling temperatures) gives GoF tests a unique advantage not exploited by prior detectors.&lt;/li&gt;&lt;li&gt;Demonstrates that classic statistical GoF tests are a simple, effective, and underused tool for LLM watermark detection/forensics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiqing He', 'Xiang Li', 'Tianqi Shang', 'Li Shen', 'Weijie Su', 'Qi Long']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'detection', 'goodness-of-fit tests', 'LLM forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03944</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title><link>https://arxiv.org/abs/2509.15735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EigenTrack, a real-time detector for hallucination and out-of-distribution (OOD) drift using spectral geometry of hidden activations (covariance-spectrum statistics).&lt;/li&gt;&lt;li&gt;Streams features such as entropy, eigenvalue gaps, and KL divergence versus random baselines into a lightweight recurrent classifier to track temporal shifts in representation structure.&lt;/li&gt;&lt;li&gt;Operates with a single forward pass (no resampling), preserves temporal context, aggregates global signals, and offers interpretable accuracy–latency trade-offs for LLMs and VLMs.&lt;/li&gt;&lt;li&gt;Aims to detect hallucination and OOD drift before surface errors appear, serving as a robustness/safety monitoring mechanism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Ettori', 'Nastaran Darabi', 'Sina Tayebati', 'Ranganath Krishnan', 'Mahesh Subedar', 'Omesh Tickoo', 'Amit Ranjan Trivedi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'OOD_detection', 'robustness', 'model_monitoring', 'spectral_analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15735</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial generalization of unfolding (model-based) networks</title><link>https://arxiv.org/abs/2509.15370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of overparameterized unfolding (model-based) networks under l2-constrained attacks (FGSM), deriving adversarial Rademacher complexity estimates.&lt;/li&gt;&lt;li&gt;Provides adversarial generalization error bounds that are tight with respect to attack level and supports theoretical claims with experiments on real-world data.&lt;/li&gt;&lt;li&gt;Finds that overparameterization of the unfolding family can be leveraged to improve adversarial robustness, offering guidance for robustification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vicky Kouni']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'adversarial-generalization', 'unfolding-networks', 'theoretical-robustness', 'adversarial-rademacher-complexity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15370</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient LLM Moderation with Multi-Layer Latent Prototypes</title><link>https://arxiv.org/abs/2502.16174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multi-Layer Prototype Moderator (MLPM), a lightweight input moderation method that uses prototypes of intermediate representations across multiple model layers to detect harmful inputs.&lt;/li&gt;&lt;li&gt;Designed to add negligible overhead to the generation pipeline and be model-agnostic, enabling seamless integration into existing LLM deployments.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on diverse moderation benchmarks and improved end-to-end safety when combined with output moderation techniques.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency, customizability to user-specific requirements, and scalability across model families and sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maciej Chrab\\k{a}szcz', 'Filip Szatkowski', "Bartosz W\\'ojcik", "Jan Dubi\\'nski", "Tomasz Trzci\\'nski", 'Sebastian Cygert']&lt;/li&gt;&lt;li&gt;Tags: ['moderation', 'defense', 'runtime safety', 'efficient detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16174</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Unified Framework for LLM Watermarks</title><link>https://arxiv.org/abs/2602.06754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified constrained-optimization framework that derives most existing LLM watermarking schemes and makes explicit the constraints each method optimizes.&lt;/li&gt;&lt;li&gt;Identifies a quality–diversity–power trade-off in watermark design and enables principled construction of novel schemes (e.g., using perplexity as a quality proxy).&lt;/li&gt;&lt;li&gt;Demonstrates experimentally that schemes derived under a given constraint maximize detection power relative to that constraint.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'defense', 'LLM', 'detection', 'traceability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06754</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Confundo: Learning to Generate Robust Poison for Practical RAG Systems</title><link>https://arxiv.org/abs/2602.06616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Confundo, a learning-to-poison framework that fine-tunes an LLM to generate robust, stealthy poisoning content targeting retrieval-augmented generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Addresses practical challenges (content preprocessing that fragments poison, and query variation) to produce poisons that remain effective across real-world RAG configurations and defenses.&lt;/li&gt;&lt;li&gt;Demonstrates multiple attack objectives (corrupting factual correctness, inducing bias, triggering hallucinations) and reports strong empirical improvements over prior attacks.&lt;/li&gt;&lt;li&gt;Also presents a defensive application to prevent unauthorized scraping-based incorporation of web content into RAG knowledge sources without harming user experience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyang Hu', 'Zhejun Jiang', 'Yueming Lyu', 'Junyuan Zhang', 'Yi Liu', 'Ka-Ho Chow']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG', 'adversarial attack', 'model security', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06616</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks</title><link>https://arxiv.org/abs/2602.06534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlertBERT, a self-supervised, masked-language-model-based framework for grouping security alerts from isolated or concurrent cyber attacks in noisy environments.&lt;/li&gt;&lt;li&gt;Uses density-based clustering and supports real-time and forensic operation, aiming to reduce alert fatigue in SOCs by accurately identifying alert groups.&lt;/li&gt;&lt;li&gt;Introduces a data augmentation method to simulate varying noise levels and concurrent attacks for evaluation, and shows improved accuracy over time-based grouping baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Karner', 'Max Landauer', 'Markus Wurzenberger', 'Florian Skopik']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'alert-grouping', 'self-supervised-learning', 'BERT', 'clustering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06534</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers</title><link>https://arxiv.org/abs/2602.06395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of adversarial robustness and explainability drift for phishing URL classification and network intrusion detection models.&lt;/li&gt;&lt;li&gt;Evaluates L-infinity bounded FGSM and PGD attacks, introduces a Robustness Index (RI) as area under the accuracy-perturbation curve, and analyzes attribution drift using gradient sensitivity and SHAP.&lt;/li&gt;&lt;li&gt;Finds consistent robustness trends across datasets and shows adversarial training can improve RI (up to ~9%) while preserving clean-data accuracy; highlights coupling between robustness degradation and interpretability drift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mona Rajhans', 'Vishal Khawarey']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'adversarial-training', 'explainability', 'cybersecurity', 'robustness-metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06395</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs</title><link>https://arxiv.org/abs/2602.06268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MPIB, a dataset and benchmark for evaluating prompt injection attacks and clinical safety in LLMs and RAG systems.&lt;/li&gt;&lt;li&gt;Defines Clinical Harm Event Rate (CHER) to measure outcome-level clinical harm alongside Attack Success Rate (ASR).&lt;/li&gt;&lt;li&gt;Provides 9,697 curated instances, evaluation code, adversarial baselines, and documentation for reproducible testing.&lt;/li&gt;&lt;li&gt;Evaluates multiple baseline LLMs and defenses, showing ASR and CHER can diverge and robustness depends on injection location (user query vs retrieved context).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhyeok Lee', 'Han Jang', 'Kyu Sung Choi']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attacks', 'medical safety', 'benchmarking', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06268</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Algebraic Robustness Verification of Neural Networks</title><link>https://arxiv.org/abs/2602.06105</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates formal robustness verification of neural networks as an algebraic optimization problem using the Euclidean Distance (ED) degree to quantify verification complexity.&lt;/li&gt;&lt;li&gt;Introduces the ED discriminant (input-dependent) and parameter discriminant (architecture/parameter-dependent) to identify inputs and parameters that change the number of real critical points for distance-to-boundary problems.&lt;/li&gt;&lt;li&gt;Provides algorithms and closed-form ED degree results for several network classes and an exact robustness certification procedure based on numerical homotopy continuation.&lt;/li&gt;&lt;li&gt;Connects metric algebraic geometry to neural network verification and derives expected counts of real critical points in the infinite-width limit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yulia Alexandr', 'Hao Duan', "Guido Mont\\'ufar"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness-verification', 'formal-certification', 'adversarial-robustness', 'algebraic-geometry', 'numerical-homotopy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06105</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Endogenous Resistance to Activation Steering in Language Models</title><link>https://arxiv.org/abs/2602.06941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes Endogenous Steering Resistance (ESR): language models (notably Llama-3.3-70B) can internally resist activation-steering during generation and recover to produce improved, task-aligned outputs even while steering remains active.&lt;/li&gt;&lt;li&gt;Uses sparse autoencoder (SAE) latents to steer activations and finds 26 latents causally linked to ESR; zero-ablating them reduces multi-attempt recovery rate by ~25%, indicating dedicated internal consistency-checking circuitry.&lt;/li&gt;&lt;li&gt;Demonstrates ESR can be enhanced via meta-prompts that induce self-monitoring and via fine-tuning on self-correction examples, and discusses dual security implications (protection against adversarial manipulation vs. interference with safety interventions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex McKenzie', 'Keenan Pepper', 'Stijn Servaes', 'Martin Leitgab', 'Murat Cubuktepe', 'Mike Vaiana', 'Diogo de Lucena', 'Judd Rosenblatt', 'Michael S. A. Graziano']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'model robustness', 'defenses', 'interpretability', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06941</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Online Learning</title><link>https://arxiv.org/abs/2602.06775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates robust classification with adversarial perturbations as an online learning problem where clean data and labels may be adversarially chosen.&lt;/li&gt;&lt;li&gt;Introduces a new combinatorial dimension (resembling the Littlestone dimension) that characterizes mistake bounds in the realizable setting and regret bounds in the agnostic setting.&lt;/li&gt;&lt;li&gt;Extends the dimension and results to multiclass hypothesis classes.&lt;/li&gt;&lt;li&gt;Analyzes the setting where the learner does not know per-point perturbation sets and only has a prior over them.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajad Ashkezari']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robust learning', 'online learning', 'theoretical ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06775</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</title><link>https://arxiv.org/abs/2602.06771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, a retention-data-free framework for concept erasure in diffusion models that aims to improve both robustness (resistance to reactivation under adversarial/related prompts) and retention (preserving unrelated concepts).&lt;/li&gt;&lt;li&gt;Addresses the robustness–retention trade-off in prior methods by using adversarial, gradient-informed techniques to prevent prompt-based reactivation while avoiding degradation of non-target concepts.&lt;/li&gt;&lt;li&gt;Focuses on defense against prompt attacks and adaptive adversaries targeting erased concepts in text-conditioned diffusion/image generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengpeng Li', 'Kemou Li', 'Qizhou Wang', 'Bo Han', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'adversarial robustness', 'prompt attacks', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06771</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Temperature Scaling Attack Disrupting Model Confidence in Federated Learning</title><link>https://arxiv.org/abs/2602.06638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Temperature Scaling Attack (TSA), a training-time federated learning attack that degrades model calibration/confidence while preserving accuracy.&lt;/li&gt;&lt;li&gt;Method: malicious clients inject temperature scaling with a learning-rate–temperature coupling during local training to mimic benign optimization and evade accuracy- and similarity-based detectors.&lt;/li&gt;&lt;li&gt;Theoretical convergence analysis under non-IID settings shows standard bounds are preserved while confidence is systematically distorted.&lt;/li&gt;&lt;li&gt;Empirical results: large increases in calibration error (e.g., 145% on CIFAR-100) with &lt;2% accuracy change; attack remains effective against robust aggregation and post-hoc calibration, causing real-world safety impacts (healthcare, autonomous driving).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kichang Lee', 'Jaeho Jin', 'JaeYeon Park', 'Songkuk Kim', 'JeongGil Ko']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'calibration attack', 'confidence manipulation', 'training-time/backdoor-style attack', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06638</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks</title><link>https://arxiv.org/abs/2602.06578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how the choice of ℓ_p norm (p in [1,2]) affects sparsity and smoothness of adversarial image perturbations.&lt;/li&gt;&lt;li&gt;Proposes a general framework for smoothness measures (including a Taylor-based measure) and uses two established sparsity metrics to quantify perturbation structure.&lt;/li&gt;&lt;li&gt;Empirically evaluates across multiple image datasets and model architectures (CNNs and transformers), finding intermediate p (≈1.3–1.5) often gives a better sparsity–smoothness trade-off than ℓ_1 or ℓ_2.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christof Duhme', 'Florian Eilers', 'Xiaoyi Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'ℓ_p norms', 'sparsity', 'smoothness', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06578</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks</title><link>https://arxiv.org/abs/2602.06577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Phase Attacks that specifically perturb the phase component of complex-valued inputs to craft adversarial examples.&lt;/li&gt;&lt;li&gt;Derives complex-valued versions of common adversarial attacks for CVNNs.&lt;/li&gt;&lt;li&gt;Empirically compares robustness of complex-valued neural networks (CVNNs) and real-valued neural networks (RVNNs), finding CVNNs can be more robust in some scenarios but both are highly susceptible to phase perturbations; Phase Attacks degrade performance more than equally strong generic attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Florian Eilers', 'Christof Duhme', 'Xiaoyi Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'attack methods', 'complex-valued neural networks', 'robustness evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06577</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Is Gradient Ascent Really Necessary? Memorize to Forget for Machine Unlearning</title><link>https://arxiv.org/abs/2602.06441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Focuses on machine unlearning to remove sensitive/private/copyrighted knowledge from trained models, addressing instability caused by standard gradient-ascent (GA) reversal methods.&lt;/li&gt;&lt;li&gt;Proposes model extrapolation: train an original model to memorize undesired data (via gradient descent) while keeping performance on retained data, then extrapolate from the memorization model back toward the reference model to obtain a 'forget' model.&lt;/li&gt;&lt;li&gt;Claims this avoids direct GA-induced catastrophic collapse, stabilizes unlearning, is simple/efficient to implement, and yields improved convergence and unlearning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Huang', 'Qizhou Wang', 'Ziming Hong', 'Shanshan Ye', 'Bo Han', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'model-extrapolation', 'defense', 'memorization-based-unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06441</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generating High-quality Privacy-preserving Synthetic Data</title><link>https://arxiv.org/abs/2602.06390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model-agnostic post-processing framework for synthetic tabular data consisting of mode patching (repairing underrepresented/missing categories) and a k-nearest-neighbor filter (enforcing minimum distance from real records).&lt;/li&gt;&lt;li&gt;Instantiates the framework on two neural generative models (feed-forward generator and VAE) and evaluates on three public tabular datasets, measuring distributional fidelity, downstream utility, and empirical privacy indicators.&lt;/li&gt;&lt;li&gt;Shows improved categorical distribution fidelity (up to 36% reduction in divergence) and dependence preservation (10–14% improvement) while keeping downstream predictive performance near baseline; distance-based privacy indicators improve, and attribute inference attack success remains largely unchanged.&lt;/li&gt;&lt;li&gt;Provides practical guidance on threshold selection and positions the method as a complementary empirical privacy-improving defense alongside formal differential privacy approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Yavo', 'Richard Khoury', 'Christophe Pere', 'Sadoune Ait Kaci Azzou']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-data', 'privacy-preserving', 'post-processing', 'attribute-inference', 'tabular-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06390</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error</title><link>https://arxiv.org/abs/2602.06269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a deterministic purification defense that maps inputs to nearby samples near data-distribution modes to improve adversarial robustness.&lt;/li&gt;&lt;li&gt;Trains a score model by minimizing expected reconstruction error on noise-corrupted data and purifies by searching for samples that minimize this expected reconstruction error.&lt;/li&gt;&lt;li&gt;Incorporates sharpness-aware minimization during purification to steer solutions toward flat regions of the reconstruction-error landscape, enhancing robustness.&lt;/li&gt;&lt;li&gt;Theoretically shows small-noise limits bias purified samples toward local maximizers of Gaussian-smoothed density and empirically reports strong gains under strong deterministic white-box attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinh Hoang', 'Sebastian Krumscheid', 'Holger Rauhut', "Ra\\'ul Tempone"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defenses', 'input purification', 'score-based models', 'sharpness-aware minimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06269</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</title><link>https://arxiv.org/abs/2602.06258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GRP-Obliteration (GRP-Oblit), which uses Group Relative Policy Optimization (GRPO) to remove safety constraints from aligned models.&lt;/li&gt;&lt;li&gt;Claims a single unlabeled prompt can reliably unalign models while largely preserving utility, outperforming existing unalignment/jailbreaking techniques.&lt;/li&gt;&lt;li&gt;Demonstrates generalization beyond text LLMs to diffusion-based image generation systems and evaluates across 15 models and multiple utility and safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mark Russinovich', 'Yanan Cai', 'Keegan Hines', 'Giorgio Severi', 'Blake Bullwinkel', 'Ahmed Salem']&lt;/li&gt;&lt;li&gt;Tags: ['model unalignment', 'jailbreaking', 'post-deployment attack', 'safety bypass', 'cross-modal vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06258</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On Randomized Algorithms in Online Strategic Classification</title><link>https://arxiv.org/abs/2602.06257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies online strategic classification where agents can strategically modify features to obtain favorable predictions; goal is low mistake/regret despite strategic behavior.&lt;/li&gt;&lt;li&gt;Realizable setting: proves a lower bound that applies to all learners (including randomized) and gives a randomized learner improving prior deterministic upper bounds.&lt;/li&gt;&lt;li&gt;Agnostic setting: provides a proper learner with improved regret O(√(T log|H|) + |H| log(T|H|)) and shows a matching lower bound (up to logs) for all proper learners, implying further improvements require improper learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chase Hutton', 'Adam Melrod', 'Han Shao']&lt;/li&gt;&lt;li&gt;Tags: ['strategic classification', 'adversarial/strategic manipulation', 'robust online learning', 'randomized algorithms', 'theoretical bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06257</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions</title><link>https://arxiv.org/abs/2602.06256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates inference-time model steering (intervening on hidden representations) with a focus on 'specificity' — whether interventions only change intended behaviors.&lt;/li&gt;&lt;li&gt;Proposes a three-way specificity framework: general (fluency/unrelated abilities), control (related control properties), and robustness (preserving control under distribution shifts).&lt;/li&gt;&lt;li&gt;Empirically studies safety-critical use cases (reducing overrefusal and hallucinations) and finds steering often preserves efficacy and general/control specificity but frequently breaks robustness, increasing vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;Highlights that standard efficacy and specificity checks can miss security regressions, so robustness evaluations (e.g., adversarial/jailbreak tests) are essential for safe steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Navita Goyal', "Hal Daum\\'e III"]&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'jailbreaking', 'robustness', 'inference-time defenses', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06256</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks</title><link>https://arxiv.org/abs/2602.06240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ATEX-CF, a method that unifies adversarial attack techniques with counterfactual explanation generation for graph neural networks (GNNs).&lt;/li&gt;&lt;li&gt;Integrates both edge additions and deletions (attack-style perturbations) and jointly optimizes fidelity, sparsity, and plausibility under a constrained perturbation budget.&lt;/li&gt;&lt;li&gt;Uses adversarial insights to find impactful counterfactuals that flip node predictions, producing instance-level explanations that are concise and realistic.&lt;/li&gt;&lt;li&gt;Evaluated on synthetic and real-world node classification benchmarks showing improved faithfulness and plausibility of explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Zhang', 'Sean Bin Yang', 'Arijit Khan', 'Cuneyt Gurcan Akcora']&lt;/li&gt;&lt;li&gt;Tags: ['graph-neural-networks', 'adversarial-examples', 'counterfactual-explanations', 'interpretability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06240</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Spider-Sense, an event-driven Intrinsic Risk Sensing (IRS) framework for autonomous LLM agents that maintains latent vigilance and only triggers defenses upon detected risk.&lt;/li&gt;&lt;li&gt;Uses a hierarchical adaptive screening pipeline: lightweight similarity matching to handle known threat patterns and escalation to deep internal reasoning for ambiguous cases, avoiding reliance on external models.&lt;/li&gt;&lt;li&gt;Introduces S^2Bench, a lifecycle-aware benchmark with realistic tool execution and multi-stage attacks to evaluate agent defenses.&lt;/li&gt;&lt;li&gt;Reports competitive or superior defense performance (lowest ASR and FPR) with only a modest latency overhead (~8.3%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenxiong Yu', 'Zhi Yang', 'Zhiheng Jin', 'Shuhe Wang', 'Heng Zhang', 'Yanlin Fei', 'Lingfeng Zeng', 'Fangqi Lou', 'Shuo Zhang', 'Tu Hu', 'Jingping Liu', 'Rongze Chen', 'Xingyu Zhu', 'Kunyi Wang', 'Chaofa Yuan', 'Xin Guo', 'Zhaowei Liu', 'Feipeng Zhang', 'Jie Huang', 'Huacan Wang', 'Ronghao Chen', 'Liwen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent-security', 'defense-framework', 'intrusion-detection', 'benchmarking', 'event-driven-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05386</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses</title><link>https://arxiv.org/abs/2602.01438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIPHER, a benchmark to measure cryptographic vulnerabilities in Python code generated by LLMs, using insecure/neutral/secure prompt variants.&lt;/li&gt;&lt;li&gt;Defines a cryptography-specific vulnerability taxonomy and provides automated, line-level attribution and scoring for detected issues.&lt;/li&gt;&lt;li&gt;Evaluates multiple widely used LLMs and finds that explicit secure prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Max Manolov', 'Tony Gao', 'Siddharth Shukla', 'Cheng-Ting Chou', 'Ryan Lagasse']&lt;/li&gt;&lt;li&gt;Tags: ['cryptographic vulnerabilities', 'LLM code generation', 'benchmarking', 'secure prompting', 'automated vulnerability scoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01438</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DECEPTICON: How Dark Patterns Manipulate Web Agents</title><link>https://arxiv.org/abs/2512.22894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DECEPTICON, an environment/benchmark of 700 web navigation tasks (600 generated, 100 real-world) to test how UI dark patterns manipulate web agents.&lt;/li&gt;&lt;li&gt;Finds dark patterns steer agent behavior toward malicious outcomes in &gt;70% of tasks (vs. 31% human average), with effectiveness increasing with model size and test-time reasoning.&lt;/li&gt;&lt;li&gt;Evaluates countermeasures (in-context prompting, guardrail models) and finds they do not consistently mitigate dark-pattern interventions, indicating a persistent vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Phil Cuvin', 'Hao Zhu', 'Diyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['dark patterns', 'agent robustness', 'benchmark/dataset', 'adversarial attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22894</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness</title><link>https://arxiv.org/abs/2511.12085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a defense-focused system for email phishing classification using DistilBERT, augmented with Fast Gradient Method (FGM) adversarial training to improve robustness against text-based/AI-generated phishing attacks.&lt;/li&gt;&lt;li&gt;Integrates LIME for model explainability and uses Flan-T5-small to produce plain-language security explanations for end users, combining technical interpretability with user-facing rationale.&lt;/li&gt;&lt;li&gt;Aims to deliver accurate phishing detection while enhancing adversarial robustness and transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajad U P']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'phishing detection', 'explainable AI', 'transformer models', 'text security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12085</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning</title><link>https://arxiv.org/abs/2511.10936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GraphToxin, the first full-graph reconstruction attack targeting graph unlearning to recover deleted nodes and their sensitive connections.&lt;/li&gt;&lt;li&gt;Proposes a curvature matching module to guide fine-grained recovery and extends the attack to multiple-node removal in both white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Provides a systematic evaluation framework including random and worst-case node removal scenarios and shows existing defenses are largely ineffective or can worsen leakage.&lt;/li&gt;&lt;li&gt;Demonstrates practical privacy risks against GNN unlearning and emphasizes the need for stronger, robust defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ying Song', 'Balaji Palanisamy']&lt;/li&gt;&lt;li&gt;Tags: ['graph unlearning', 'graph neural networks', 'privacy attack', 'data reconstruction', 'model inversion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10936</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2507.02735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Meta SecAlign, an open-source foundation LLM with built-in model-level defenses specifically against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Provides full training recipe and releases models (Meta-SecAlign-70B and 8B) and code to enable community research and co-development of attacks/defenses.&lt;/li&gt;&lt;li&gt;Performs comprehensive evaluation on 9 utility benchmarks and 7 security benchmarks, claiming strong security-general utility trade-offs and superiority over several proprietary models with injection defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sizhe Chen', 'Arman Zharmagambetov', 'David Wagner', 'Chuan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-injection', 'model-level defense', 'open-source LLM', 'security benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02735</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review</title><link>https://arxiv.org/abs/2502.19614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large benchmark dataset of 788,984 AI-generated peer reviews paired with human reviews from 8 years of ICLR and NeurIPS submissions.&lt;/li&gt;&lt;li&gt;Evaluates 18 existing AI text detection algorithms on the task of distinguishing human-written vs. LLM-generated reviews.&lt;/li&gt;&lt;li&gt;Proposes and evaluates a context-aware detection method (Anchor) that leverages manuscript content to improve detection.&lt;/li&gt;&lt;li&gt;Analyzes robustness of detection models to LLM-assisted editing and finds identifying AI-generated text at the individual review level is difficult.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungduk Yu', 'Man Luo', 'Avinash Madasu', 'Vasudev Lal', 'Phillip Howard']&lt;/li&gt;&lt;li&gt;Tags: ['ai-generated-text-detection', 'benchmark', 'dataset', 'defense', 'peer-review-misuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19614</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient LLM Moderation with Multi-Layer Latent Prototypes</title><link>https://arxiv.org/abs/2502.16174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multi-Layer Prototype Moderator (MLPM), a lightweight input moderation method that uses prototypes of intermediate representations across multiple model layers to detect harmful inputs.&lt;/li&gt;&lt;li&gt;Designed to add negligible overhead to the generation pipeline and be model-agnostic, enabling seamless integration into existing LLM deployments.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on diverse moderation benchmarks and improved end-to-end safety when combined with output moderation techniques.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency, customizability to user-specific requirements, and scalability across model families and sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maciej Chrab\\k{a}szcz', 'Filip Szatkowski', "Bartosz W\\'ojcik", "Jan Dubi\\'nski", "Tomasz Trzci\\'nski", 'Sebastian Cygert']&lt;/li&gt;&lt;li&gt;Tags: ['moderation', 'defense', 'runtime safety', 'efficient detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16174</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How does information access affect LLM monitors' ability to detect sabotage?</title><link>https://arxiv.org/abs/2601.21112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies how information access to monitored agents affects LLM monitors' ability to detect sabotage, identifying a counterintuitive "less-is-more" effect where less information can improve detection.&lt;/li&gt;&lt;li&gt;Introduces extract-and-evaluate (EaE), a hierarchical monitoring approach where one model extracts relevant excerpts from an agent's trajectory and another scores those excerpts.&lt;/li&gt;&lt;li&gt;Empirically evaluates EaE and other monitoring strategies across multiple sabotage environments (e.g., BigCodeBench-Sabotage), showing substantial detection improvements and analyzing when filtering helps most.&lt;/li&gt;&lt;li&gt;Performs cost-performance analysis across models (Gemini, Claude, GPT-4.1-mini) and examines factors like agent awareness of monitoring and trace length.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rauno Arike', 'Raja Mehta Moreno', 'Rohan Subramani', 'Shubhorup Biswas', 'Francis Rhys Ward']&lt;/li&gt;&lt;li&gt;Tags: ['monitoring', 'automated oversight', 'sabotage detection', 'LLM defenses', 'red teaming/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21112</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer</title><link>https://arxiv.org/abs/2511.17673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Structured Cognitive Loop (R-CCAM) that decomposes LLM agent processing into Retrieval, Cognition, Control, Action, and Memory phases to separate reasoning from execution and improve traceability.&lt;/li&gt;&lt;li&gt;Introduces Soft Symbolic Control: a governance layer that enforces symbolic constraints over probabilistic neural inference to prevent policy violations, redundant tool use, and uncontrolled action sequences while preserving neural flexibility.&lt;/li&gt;&lt;li&gt;Empirical validation shows zero policy violations, elimination of redundant tool calls, and complete decision traceability; provides formalization, design principles for trustworthy agents, and an open-source implementation (including a GPT-4o-powered demo).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myung Ho Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Guardrails / Governance', 'Neuro-symbolic control', 'Agent safety', 'Explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17673</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia</title><link>https://arxiv.org/abs/2509.23023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mini-Mafia, a simplified four-player social-deduction benchmark (one mafioso, one detective, two villagers) to study LLM social reasoning in a single-day discussion/vote phase.&lt;/li&gt;&lt;li&gt;Proposes a compact theoretical model logit(p) = v * (m - d) that explains mafia win-rate via three latent parameters (mafioso deception m, villager detection d, detective disclosure v) and fits empirical LLM outcomes.&lt;/li&gt;&lt;li&gt;Estimates latent parameters from gameplay via Bayesian inference to create the Mini-Mafia Benchmark, and uses experiments (including human baselines) to analyze emergent behaviors (e.g., name bias, last-speaker advantage) and surprising performance patterns.&lt;/li&gt;&lt;li&gt;Frames safety relevance by producing training data for deception detectors and enabling quantitative study of deceptive vs. detection behaviors among LLM agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davi Bastos Costa', 'Renato Vicente']&lt;/li&gt;&lt;li&gt;Tags: ['deception-detection', 'multi-agent-systems', 'safety-benchmark', 'emergent-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23023</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OpenDeception: Learning Deception and Trust in Human-AI Interaction via Multi-Agent Simulation</title><link>https://arxiv.org/abs/2504.13707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenDeception, a framework to jointly evaluate deceptive behavior risk in human-AI dialogue via a 50-case scenario benchmark, IntentNet (deceptive intent inference), and TrustNet (user susceptibility estimation).&lt;/li&gt;&lt;li&gt;Synthesizes high-risk dialogues using LLM-based role-and-goal simulation and trains the Trust scorer with contrastive learning on controlled response pairs to avoid noisy scalar labels.&lt;/li&gt;&lt;li&gt;Evaluates 11 LLMs and three large reasoning models, finding high rates of goal-driven deceptive intent (over 90% in most models) and that stronger models can exhibit higher deception risk; demonstrates proactive warning capability in a real-world case study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Wu', 'Qianqian Gao', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'safety evaluation', 'adversarial/malicious behavior', 'trust modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13707</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Endogenous Resistance to Activation Steering in Language Models</title><link>https://arxiv.org/abs/2602.06941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes Endogenous Steering Resistance (ESR): language models (notably Llama-3.3-70B) can internally resist activation-steering during generation and recover to produce improved, task-aligned outputs even while steering remains active.&lt;/li&gt;&lt;li&gt;Uses sparse autoencoder (SAE) latents to steer activations and finds 26 latents causally linked to ESR; zero-ablating them reduces multi-attempt recovery rate by ~25%, indicating dedicated internal consistency-checking circuitry.&lt;/li&gt;&lt;li&gt;Demonstrates ESR can be enhanced via meta-prompts that induce self-monitoring and via fine-tuning on self-correction examples, and discusses dual security implications (protection against adversarial manipulation vs. interference with safety interventions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex McKenzie', 'Keenan Pepper', 'Stijn Servaes', 'Martin Leitgab', 'Murat Cubuktepe', 'Mike Vaiana', 'Diogo de Lucena', 'Judd Rosenblatt', 'Michael S. A. Graziano']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'model robustness', 'defenses', 'interpretability', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06941</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</title><link>https://arxiv.org/abs/2602.06920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Halluverse-M^3, a multilingual, multitask benchmark (English, Arabic, Hindi, Turkish) for studying hallucinations in LLMs across QA and dialogue summarization.&lt;/li&gt;&lt;li&gt;Differentiates hallucination types (entity-, relation-, sentence-level); constructs hallucinated outputs via controlled edits and human validation and releases the dataset.&lt;/li&gt;&lt;li&gt;Evaluates a range of open-source and proprietary models, finding QA easier than summarization, sentence-level hallucinations hardest, and performance degrading in lower-resource languages (Hindi lowest).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samir Abdaljalil', 'Parichit Sharma', 'Erchin Serpedin', 'Hasan Kurban']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'benchmark', 'multilingual', 'detection', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06920</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering</title><link>https://arxiv.org/abs/2602.06911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TamperBench, a unified framework/benchmark to evaluate LLM tamper resistance against weight-space fine-tuning and latent-space representation attacks.&lt;/li&gt;&lt;li&gt;Curates a repository of state-of-the-art tampering attacks, supports systematic hyperparameter sweeps per model-attack pair, and provides standardized safety and utility metrics for reproducible evaluation.&lt;/li&gt;&lt;li&gt;Evaluates 21 open-weight LLMs (including defense-augmented variants) across nine tampering threats, deriving insights about attack severity (e.g., jailbreak-tuning) and defense effectiveness (e.g., Triplet).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Hossain', 'Tom Tseng', 'Punya Syon Pandey', 'Samanvay Vajpayee', 'Matthew Kowal', 'Nayeema Nonta', 'Samuel Simko', 'Stephen Casper', 'Zhijing Jin', 'Kellin Pelrine', 'Sirisha Rambhatla']&lt;/li&gt;&lt;li&gt;Tags: ['tampering', 'fine-tuning attacks', 'jailbreaking', 'benchmarking', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06911</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</title><link>https://arxiv.org/abs/2602.06771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, a retention-data-free framework for concept erasure in diffusion models that aims to improve both robustness (resistance to reactivation under adversarial/related prompts) and retention (preserving unrelated concepts).&lt;/li&gt;&lt;li&gt;Addresses the robustness–retention trade-off in prior methods by using adversarial, gradient-informed techniques to prevent prompt-based reactivation while avoiding degradation of non-target concepts.&lt;/li&gt;&lt;li&gt;Focuses on defense against prompt attacks and adaptive adversaries targeting erased concepts in text-conditioned diffusion/image generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengpeng Li', 'Kemou Li', 'Qizhou Wang', 'Bo Han', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'adversarial robustness', 'prompt attacks', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06771</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Unified Framework for LLM Watermarks</title><link>https://arxiv.org/abs/2602.06754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified constrained-optimization framework that derives most existing LLM watermarking schemes and makes explicit the constraints each method optimizes.&lt;/li&gt;&lt;li&gt;Identifies a quality–diversity–power trade-off in watermark design and enables principled construction of novel schemes (e.g., using perplexity as a quality proxy).&lt;/li&gt;&lt;li&gt;Demonstrates experimentally that schemes derived under a given constraint maximize detection power relative to that constraint.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'defense', 'LLM', 'detection', 'traceability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06754</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models</title><link>https://arxiv.org/abs/2602.06718</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CiteVerifier, an open-source framework for large-scale citation verification and conducts the first comprehensive study of citation validity in the LLM era.&lt;/li&gt;&lt;li&gt;Benchmarks 13 state-of-the-art LLMs across 40 research domains, finding citation hallucination rates from 14.23% to 94.93%, with strong domain variation.&lt;/li&gt;&lt;li&gt;Analyzes 2.2M citations from 56,381 papers (2020–2025), identifying 1.07% of papers containing invalid/fabricated citations and a sharp increase in 2025.&lt;/li&gt;&lt;li&gt;Surveys researchers and reviewers, revealing a significant verification gap (many copy BibTeX without checking and reviewers rarely detect fake citations) and proposes interventions for researchers, venues, and tool developers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zuyao Xu', 'Yuqi Qiu', 'Lu Sun', 'FaSheng Miao', 'Fubin Wu', 'Xinyi Wang', 'Xiang Li', 'Haozhe Lu', 'ZhengZe Zhang', 'Yuxin Hu', 'Jialu Li', 'Jin Luo', 'Feng Zhang', 'Rui Luo', 'Xinran Liu', 'Yingxian Li', 'Jiaji Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucinations', 'citation fabrication', 'citation verification', 'benchmarking', 'defensive tooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06718</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Temperature Scaling Attack Disrupting Model Confidence in Federated Learning</title><link>https://arxiv.org/abs/2602.06638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Temperature Scaling Attack (TSA), a training-time federated learning attack that degrades model calibration/confidence while preserving accuracy.&lt;/li&gt;&lt;li&gt;Method: malicious clients inject temperature scaling with a learning-rate–temperature coupling during local training to mimic benign optimization and evade accuracy- and similarity-based detectors.&lt;/li&gt;&lt;li&gt;Theoretical convergence analysis under non-IID settings shows standard bounds are preserved while confidence is systematically distorted.&lt;/li&gt;&lt;li&gt;Empirical results: large increases in calibration error (e.g., 145% on CIFAR-100) with &lt;2% accuracy change; attack remains effective against robust aggregation and post-hoc calibration, causing real-world safety impacts (healthcare, autonomous driving).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kichang Lee', 'Jaeho Jin', 'JaeYeon Park', 'Songkuk Kim', 'JeongGil Ko']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'calibration attack', 'confidence manipulation', 'training-time/backdoor-style attack', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06638</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks</title><link>https://arxiv.org/abs/2602.06578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how the choice of ℓ_p norm (p in [1,2]) affects sparsity and smoothness of adversarial image perturbations.&lt;/li&gt;&lt;li&gt;Proposes a general framework for smoothness measures (including a Taylor-based measure) and uses two established sparsity metrics to quantify perturbation structure.&lt;/li&gt;&lt;li&gt;Empirically evaluates across multiple image datasets and model architectures (CNNs and transformers), finding intermediate p (≈1.3–1.5) often gives a better sparsity–smoothness trade-off than ℓ_1 or ℓ_2.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christof Duhme', 'Florian Eilers', 'Xiaoyi Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'ℓ_p norms', 'sparsity', 'smoothness', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06578</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks</title><link>https://arxiv.org/abs/2602.06577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Phase Attacks that specifically perturb the phase component of complex-valued inputs to craft adversarial examples.&lt;/li&gt;&lt;li&gt;Derives complex-valued versions of common adversarial attacks for CVNNs.&lt;/li&gt;&lt;li&gt;Empirically compares robustness of complex-valued neural networks (CVNNs) and real-valued neural networks (RVNNs), finding CVNNs can be more robust in some scenarios but both are highly susceptible to phase perturbations; Phase Attacks degrade performance more than equally strong generic attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Florian Eilers', 'Christof Duhme', 'Xiaoyi Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'attack methods', 'complex-valued neural networks', 'robustness evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06577</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study</title><link>https://arxiv.org/abs/2602.06547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Built the first labeled dataset of malicious third-party agent skills by behaviorally verifying 98,380 skills and confirming 157 malicious skills containing 632 vulnerabilities.&lt;/li&gt;&lt;li&gt;Characterizes two attacker archetypes—Data Thieves (credential exfiltration via supply-chain techniques) and Agent Hijackers (instruction manipulation to subvert agent decision-making)—with malicious skills averaging 4.03 vulnerabilities across a median of three kill-chain phases.&lt;/li&gt;&lt;li&gt;Identifies ecosystem patterns: a single actor responsible for 54.1% of confirmed cases via templated brand impersonation, 'shadow features' present only in advanced attacks, and exploitation of platform hooks and permission flags; responsible disclosure led to 93.6% removal within 30 days.&lt;/li&gt;&lt;li&gt;Releases the dataset and analysis pipeline to support future security research, detection, and mitigation for LLM-based agent skill ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Zhihao Chen', 'Yanjun Zhang', 'Gelei Deng', 'Yuekang Li', 'Jianting Ning', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent skills', 'supply-chain attacks', 'instruction manipulation', 'malicious skills dataset', 'vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06547</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking</title><link>https://arxiv.org/abs/2602.06440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TrailBlazer, a history-aware RL framework for black-box LLM jailbreaking that reweights vulnerability signals from prior interaction turns to guide future actions.&lt;/li&gt;&lt;li&gt;Introduces an attention-based mechanism to highlight critical vulnerabilities in the interaction history, improving exploration efficiency and query efficiency.&lt;/li&gt;&lt;li&gt;Evaluates on AdvBench and HarmBench, showing state-of-the-art jailbreak success rates and reduced queries compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sung-Hoon Yoon', 'Ruizhi Qian', 'Minda Zhao', 'Weiyue Li', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'reinforcement learning', 'adversarial attacks', 'LLM safety', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06440</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers</title><link>https://arxiv.org/abs/2602.06395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of adversarial robustness and explainability drift for phishing URL classification and network intrusion detection models.&lt;/li&gt;&lt;li&gt;Evaluates L-infinity bounded FGSM and PGD attacks, introduces a Robustness Index (RI) as area under the accuracy-perturbation curve, and analyzes attribution drift using gradient sensitivity and SHAP.&lt;/li&gt;&lt;li&gt;Finds consistent robustness trends across datasets and shows adversarial training can improve RI (up to ~9%) while preserving clean-data accuracy; highlights coupling between robustness degradation and interpretability drift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mona Rajhans', 'Vishal Khawarey']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'adversarial-training', 'explainability', 'cybersecurity', 'robustness-metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06395</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generating High-quality Privacy-preserving Synthetic Data</title><link>https://arxiv.org/abs/2602.06390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model-agnostic post-processing framework for synthetic tabular data consisting of mode patching (repairing underrepresented/missing categories) and a k-nearest-neighbor filter (enforcing minimum distance from real records).&lt;/li&gt;&lt;li&gt;Instantiates the framework on two neural generative models (feed-forward generator and VAE) and evaluates on three public tabular datasets, measuring distributional fidelity, downstream utility, and empirical privacy indicators.&lt;/li&gt;&lt;li&gt;Shows improved categorical distribution fidelity (up to 36% reduction in divergence) and dependence preservation (10–14% improvement) while keeping downstream predictive performance near baseline; distance-based privacy indicators improve, and attribute inference attack success remains largely unchanged.&lt;/li&gt;&lt;li&gt;Provides practical guidance on threshold selection and positions the method as a complementary empirical privacy-improving defense alongside formal differential privacy approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Yavo', 'Richard Khoury', 'Christophe Pere', 'Sadoune Ait Kaci Azzou']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-data', 'privacy-preserving', 'post-processing', 'attribute-inference', 'tabular-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06390</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Zero-Trust Runtime Verification for Agentic Payment Protocols: Mitigating Replay and Context-Binding Failures in AP2</title><link>https://arxiv.org/abs/2602.06345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes runtime enforcement gaps in the Agent Payments Protocol (AP2) for autonomous agent-driven payments, identifying replay and context-redirect attack vectors that arise from real-world agent behaviors (retries, concurrency, orchestration).&lt;/li&gt;&lt;li&gt;Proposes a zero-trust runtime verification framework that enforces context binding and consume-once mandate semantics via dynamically generated, time-bound nonces evaluated at execution time.&lt;/li&gt;&lt;li&gt;Evaluates the framework in simulation under high concurrency, showing it prevents the studied attacks while keeping verification latency low (~3.8 ms) and runtime state bounded by peak concurrency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianlong Lan', 'Anuj Kaul', 'Shaun Jones', 'Stephanie Westrum']&lt;/li&gt;&lt;li&gt;Tags: ['runtime verification', 'replay attacks', 'context-binding', 'zero-trust', 'agentic payment protocols']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06345</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</title><link>https://arxiv.org/abs/2602.06258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GRP-Obliteration (GRP-Oblit), which uses Group Relative Policy Optimization (GRPO) to remove safety constraints from aligned models.&lt;/li&gt;&lt;li&gt;Claims a single unlabeled prompt can reliably unalign models while largely preserving utility, outperforming existing unalignment/jailbreaking techniques.&lt;/li&gt;&lt;li&gt;Demonstrates generalization beyond text LLMs to diffusion-based image generation systems and evaluates across 15 models and multiple utility and safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mark Russinovich', 'Yanan Cai', 'Keegan Hines', 'Giorgio Severi', 'Blake Bullwinkel', 'Ahmed Salem']&lt;/li&gt;&lt;li&gt;Tags: ['model unalignment', 'jailbreaking', 'post-deployment attack', 'safety bypass', 'cross-modal vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06258</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions</title><link>https://arxiv.org/abs/2602.06256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates inference-time model steering (intervening on hidden representations) with a focus on 'specificity' — whether interventions only change intended behaviors.&lt;/li&gt;&lt;li&gt;Proposes a three-way specificity framework: general (fluency/unrelated abilities), control (related control properties), and robustness (preserving control under distribution shifts).&lt;/li&gt;&lt;li&gt;Empirically studies safety-critical use cases (reducing overrefusal and hallucinations) and finds steering often preserves efficacy and general/control specificity but frequently breaks robustness, increasing vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;Highlights that standard efficacy and specificity checks can miss security regressions, so robustness evaluations (e.g., adversarial/jailbreak tests) are essential for safe steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Navita Goyal', "Hal Daum\\'e III"]&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'jailbreaking', 'robustness', 'inference-time defenses', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06256</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks</title><link>https://arxiv.org/abs/2602.06240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ATEX-CF, a method that unifies adversarial attack techniques with counterfactual explanation generation for graph neural networks (GNNs).&lt;/li&gt;&lt;li&gt;Integrates both edge additions and deletions (attack-style perturbations) and jointly optimizes fidelity, sparsity, and plausibility under a constrained perturbation budget.&lt;/li&gt;&lt;li&gt;Uses adversarial insights to find impactful counterfactuals that flip node predictions, producing instance-level explanations that are concise and realistic.&lt;/li&gt;&lt;li&gt;Evaluated on synthetic and real-world node classification benchmarks showing improved faithfulness and plausibility of explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Zhang', 'Sean Bin Yang', 'Arijit Khan', 'Cuneyt Gurcan Akcora']&lt;/li&gt;&lt;li&gt;Tags: ['graph-neural-networks', 'adversarial-examples', 'counterfactual-explanations', 'interpretability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06240</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item><item><title>An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization</title><link>https://arxiv.org/abs/2602.06838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive differentially private federated learning framework to improve training stability and accuracy under device heterogeneity and non-IID data.&lt;/li&gt;&lt;li&gt;Client-side: introduces a lightweight local compression/regularization module to constrain intermediate representations and reduce gradient variability.&lt;/li&gt;&lt;li&gt;Server-side: uses adaptive gradient clipping based on historical update statistics and a constraint-aware aggregation mechanism to suppress noise-dominated client updates.&lt;/li&gt;&lt;li&gt;Evaluated on CIFAR-10 and SVHN showing improved convergence stability and classification accuracy under differential privacy constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Wang', 'Hui Ma', 'Fei Xing', 'Ming Yan']&lt;/li&gt;&lt;li&gt;Tags: ['Differential Privacy', 'Federated Learning', 'Adaptive Gradient Clipping', 'Privacy-preserving ML', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06838</guid><pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 05 Feb 2026 00:56:53 +0000</lastBuildDate><item><title>CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization</title><link>https://arxiv.org/abs/2602.02175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIEC, a weakly-supervised framework for multimodal manipulation localization that uses only coarse image/sentence-level labels.&lt;/li&gt;&lt;li&gt;Contains two complementary branches: TRPS (Textual-guidance Refine Patch Selection) for image-region localization using visual+textual cues and spatial priors, and VCTG (Visual-deviation Calibrated Token Grounding) for token-level grounding using visual bias.&lt;/li&gt;&lt;li&gt;Introduces constraints (background silencing, spatial contrast, asymmetric sparse, semantic consistency) to suppress noise and improve localization reliability.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing CIEC achieves results comparable to fully supervised methods on several evaluation metrics for image-text manipulation detection/localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinquan Yu', 'Wei Lu', 'Xiangyang Luo', 'Rui Yang']&lt;/li&gt;&lt;li&gt;Tags: ['manipulation detection', 'multimodal forensics', 'weakly-supervised localization', 'misinformation mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02175</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</title><link>https://arxiv.org/abs/2512.08042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces frequency-domain masking (with random masking and geometric transforms) as a training strategy to improve universal deepfake detection across diverse and unseen generative models.&lt;/li&gt;&lt;li&gt;Shows frequency masking leads to superior generalization to GAN- and diffusion-generated images and achieves state-of-the-art cross-generator performance.&lt;/li&gt;&lt;li&gt;Demonstrates the approach remains effective under structured model pruning, supporting scalable, resource-efficient (Green AI) deployment for large-scale screening.&lt;/li&gt;&lt;li&gt;Releases code and models to reproduce results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chandler Timm C. Doloriel', 'Habib Ullah', 'Kristian Hovde Liland', 'Fadi Al Machot', 'Ngai-Man Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'defense', 'frequency-domain', 'model-robustness', 'green-ai']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08042</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models</title><link>https://arxiv.org/abs/2501.18533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'safety visual reasoning' gap in existing safety fine-tuning methods for Vision‑Language Models (VLMs) that limits handling of challenging multi-image safety scenarios.&lt;/li&gt;&lt;li&gt;Proposes MIS (Multi-Image Safety), an instruction‑following dataset with multi-image inputs and safety Chain‑of‑Thought (CoT) labels to provide fine‑grained visual reasoning supervision.&lt;/li&gt;&lt;li&gt;Shows that fine‑tuning InternVL2.5‑8B on MIS reduces Attack Success Rate (ASR) on safety benchmarks while preserving or improving general capabilities across standard benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates a defense/guardrail approach for VLMs by improving safety reasoning and robustness without trading off helpfulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Ding', 'Lijun Li', 'Bing Cao', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['Safety Fine‑Tuning', 'Vision‑Language Models', 'Defense / Guardrails', 'Dataset (MIS)', 'Chain‑of‑Thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18533</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection</title><link>https://arxiv.org/abs/2602.03423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Origin Lens, a privacy-first mobile framework for on-device cryptographic image provenance verification and AI-generated image detection.&lt;/li&gt;&lt;li&gt;Combines multiple signals—cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification—to provide graded confidence to end users.&lt;/li&gt;&lt;li&gt;Implements a Rust/Flutter hybrid for local processing to avoid server-side privacy leaks and aligns the design with regulatory frameworks (EU AI Act, DSA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Loth', 'Dominique Conceicao Rosario', 'Peter Ebinger', 'Martin Kappes', 'Marc-Oliver Pahl']&lt;/li&gt;&lt;li&gt;Tags: ['image provenance', 'deepfake detection', 'on-device security', 'cryptographic verification', 'privacy-preserving detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03423</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Time Is All It Takes: Spike-Retiming Attacks on Event-Driven Spiking Neural Networks</title><link>https://arxiv.org/abs/2602.03284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a timing-only adversary against event-driven spiking neural networks (SNNs) that retimes existing spikes without changing counts or amplitudes, formalizing a capacity-1 threat model with per-spike jitter (B_infty), total delay (B_1), and tamper count (B_0) budgets.&lt;/li&gt;&lt;li&gt;Proposes projected-in-the-loop (PIL) optimization: differentiable soft retiming via shift-probability logits for backpropagation, with a forward-pass projection producing discrete feasible spike schedules that respect capacity, non-overlap, and budget constraints.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across event-based benchmarks (CIFAR10-DVS, DVS-Gesture, N-MNIST) and SNN architectures, showing high success rates (e.g., &gt;90% on DVS-Gesture) while perturbing only a small fraction of spikes; also examines models trained with timing-aware adversarial training and finds defenses struggle.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Yu', 'Qixin Zhang', 'Shuhan Ye', 'Xun Lin', 'Qianshan Wei', 'Kun Wang', 'Wenhan Yang', 'Dacheng Tao', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'spiking neural networks', 'timing attacks', 'temporal robustness', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03284</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation</title><link>https://arxiv.org/abs/2602.02536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniMod, a paradigm that converts sparse binary moderation decisions into dense multi-attribute reasoning trajectories (evidence grounding, modality assessment, risk mapping, policy decision, response generation).&lt;/li&gt;&lt;li&gt;Introduces UniRM, a multi-head scalar reward model providing attribute-level scores to supervise response generation and enable multi-dimensional supervision.&lt;/li&gt;&lt;li&gt;Presents optimization strategies to decouple task-specific parameters and rebalance training dynamics to reduce interference in multi-task multimodal moderation.&lt;/li&gt;&lt;li&gt;Reports competitive textual moderation results and establishes a new multimodal benchmark while using under 40% of training data compared to leading baselines; ablations validate the trajectory-based approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Gu', 'Kexin Huang', 'Lingyu Li', 'Ruilin Luo', 'Shiyang Huang', 'Zongqi Wang', 'Yujiu Yang', 'Yan Teng', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal moderation', 'safety/defense', 'reward model', 'robust training', 'attribute-level supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02536</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Seeing Through the Chain: Mitigate Hallucination in Multimodal Reasoning Models via CoT Compression and Contrastive Preference Optimization</title><link>https://arxiv.org/abs/2602.03380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that chain-of-thought (CoT) reasoning can amplify language priors and reduce reliance on visual inputs, producing verbose reasoning traces with redundant tokens that contribute to hallucination in multimodal reasoning models (MLRMs).&lt;/li&gt;&lt;li&gt;Proposes C3PO: Chain-of-Thought Compression to filter redundant reasoning tokens for a more compact, signal-efficient CoT representation, and Contrastive Preference Optimization using high-quality AI feedback and hallucination-inducing negative examples to tune model preferences away from hallucinations.&lt;/li&gt;&lt;li&gt;Designs a multimodal hallucination-inducing mechanism to generate informative negative training signals and provides theoretical justification plus empirical results showing consistent reduction in hallucination across multiple MLRMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Fang', 'Jinyu Li', 'Jiawei Kong', 'Tianqu Zhuang', 'Kuofeng Gao', 'Bin Chen', 'Shu-Tao Xia', 'Yaowei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multimodal reasoning', 'chain-of-thought', 'contrastive learning', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03380</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Unifying Watermarking via Dimension-Aware Mapping</title><link>https://arxiv.org/abs/2602.03373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiM, a dimension-aware mapping framework that models watermark payloads of different dimensionalities (1D binary, 2D spatial, 3D spatiotemporal) to unify deep watermarking methods.&lt;/li&gt;&lt;li&gt;Shows that the choice of embedding/extraction dimensional configuration largely determines watermark behavior: same-dimensional mappings preserve payload structures; cross-dimensional mappings enable spatial or spatiotemporal localization and control.&lt;/li&gt;&lt;li&gt;Implements DiM in the video domain and demonstrates capabilities such as spatiotemporal tamper localization, localized embedding control, and recovery of temporal order under frame disruptions without changing network architecture.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Meng', 'Runyi Hu', 'Jie Zhang', 'Zheming Lu', 'Ivor Tsang', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'media integrity', 'tamper detection', 'spatiotemporal', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03373</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets</title><link>https://arxiv.org/abs/2602.03333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PWAVEP, a plug-and-play defense that purifies imperceptible adversarial perturbations in 3D point clouds using spectral graph wavelets.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical analysis linking imperceptible adversarial perturbations to high-frequency spectral components of point clouds.&lt;/li&gt;&lt;li&gt;Method computes per-point spectral saliency and local sparsity, then hierarchically removes highly salient outliers and attenuates high-frequency coefficients of moderately salient points via graph wavelet filtering.&lt;/li&gt;&lt;li&gt;Evaluated extensively and reported to outperform existing defenses for robustness and classification accuracy on 3D point cloud attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Li', 'Renyang Liu', 'Hongjia Liu', 'Chen Wang', 'Long Yin', 'Jian Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', '3D point clouds', 'spectral graph wavelets', 'purification', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03333</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation</title><link>https://arxiv.org/abs/2602.03316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InvLBA, a clean-label backdoor attack targeting generative data augmentation by applying perturbations at the latent feature level rather than pixel-level triggers.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis guaranteeing generalization of clean accuracy and attack success rates for the proposed method.&lt;/li&gt;&lt;li&gt;Empirical results show an average improvement of 46.43% in attack success rate with negligible reduction in clean accuracy and strong robustness against state-of-the-art defenses.&lt;/li&gt;&lt;li&gt;Targets vulnerabilities in image generative models used for data augmentation, demonstrating practical risks and defense-evasion capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Xiang', 'Jinhui Zhao', 'Changjian Chen', 'Zhuo Tang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'clean-label', 'generative data augmentation', 'latent perturbation', 'attack robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03316</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction</title><link>https://arxiv.org/abs/2602.02914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaceLinkGen, an identity extraction attack that links/matches and regenerates faces directly from privacy-preserving face recognition (PPFR) templates without pixel-level reconstruction.&lt;/li&gt;&lt;li&gt;Demonstrates high success: &gt;98.5% matching accuracy and &gt;96% regeneration on three recent PPFR systems, and &gt;92%/94% in near zero-knowledge settings.&lt;/li&gt;&lt;li&gt;Shows that common pixel-distortion metrics (PSNR/SSIM) fail to capture identity leakage, revealing a structural gap in PPFR evaluations and exposing risks to external attackers and untrusted providers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqi Guo', 'Shan Du']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'biometric privacy', 'template inversion', 'face recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02914</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Proactive defense against LLM Jailbreak</title><link>https://arxiv.org/abs/2510.05052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProAct, a proactive defense that generates spurious/misleading responses to cause iterative jailbreak attack searches to terminate prematurely.&lt;/li&gt;&lt;li&gt;Targets multi-turn jailbreak/adversarial search methods by feeding false success signals into the attacker's optimization loop, effectively 'jailbreaking the jailbreak'.&lt;/li&gt;&lt;li&gt;Evaluated across state-of-the-art LLMs, jailbreak frameworks, and safety benchmarks, showing up to 94% reduction in attack success and further reductions to 0% when combined with other defenses, with no utility loss.&lt;/li&gt;&lt;li&gt;Positions ProAct as an orthogonal guardrail augmenting existing reactive defenses and red-teaming strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiliang Zhao', 'Jinjun Peng', 'Daniel Ben-Levi', 'Zhou Yu', 'Junfeng Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial defense', 'LLM safety', 'proactive defense', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05052</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models</title><link>https://arxiv.org/abs/2501.18533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'safety visual reasoning' gap in existing safety fine-tuning methods for Vision‑Language Models (VLMs) that limits handling of challenging multi-image safety scenarios.&lt;/li&gt;&lt;li&gt;Proposes MIS (Multi-Image Safety), an instruction‑following dataset with multi-image inputs and safety Chain‑of‑Thought (CoT) labels to provide fine‑grained visual reasoning supervision.&lt;/li&gt;&lt;li&gt;Shows that fine‑tuning InternVL2.5‑8B on MIS reduces Attack Success Rate (ASR) on safety benchmarks while preserving or improving general capabilities across standard benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates a defense/guardrail approach for VLMs by improving safety reasoning and robustness without trading off helpfulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Ding', 'Lijun Li', 'Bing Cao', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['Safety Fine‑Tuning', 'Vision‑Language Models', 'Defense / Guardrails', 'Dataset (MIS)', 'Chain‑of‑Thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18533</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings</title><link>https://arxiv.org/abs/2602.01757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zero2Text, a training-free, recursive online alignment method that performs cross-domain embedding inversion to recover textual inputs from target embeddings in strict black-box settings.&lt;/li&gt;&lt;li&gt;Combines LLM priors with a dynamic ridge-regression alignment loop to iteratively align generated text embeddings to the victim embedding without requiring in-domain training data or leaked pairs.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against RAG/vector DB scenarios, showing substantial gains over baselines (e.g., 1.8x ROUGE-L and 6.4x BLEU-2 on MS MARCO vs an OpenAI victim) and demonstrates that common defenses like differential privacy are ineffective against this adaptive attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doohyun Kim', 'Donghwa Kang', 'Kyungjae Lee', 'Hyeongboo Baek', 'Brent Byunghoon Kang']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy attacks', 'black-box attacks', 'retrieval-augmented generation (RAG)', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01757</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models</title><link>https://arxiv.org/abs/2509.23286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A2D, a token-level alignment technique for diffusion LLMs that emits an [EOS] refusal signal whenever harmful content is detected during generation.&lt;/li&gt;&lt;li&gt;Trains under randomized masking to achieve robustness to any decoding order and any-step prefilling attacks, enabling real-time termination of unsafe continuations.&lt;/li&gt;&lt;li&gt;Evaluated against safety benchmarks and DIJA-style attacks, showing dramatic reductions in attack success (e.g., from &gt;80% to ~1.3% or 0.0%) and up to 19.3x faster safe termination via thresholded [EOS] probabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonje Jeung', 'Sangyeon Yoon', 'Yoonjun Cho', 'Dongjae Jeon', 'Sangwoo Shin', 'Hyesoo Hong', 'Albert No']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'prompt-injection', 'jailbreaking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23286</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals</title><link>https://arxiv.org/abs/2509.21875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LUMINA, a detection framework for hallucinations in Retrieval-Augmented Generation (RAG) systems using context–knowledge signals.&lt;/li&gt;&lt;li&gt;Quantifies external context utilization via distributional distance and internal knowledge utilization by tracking token prediction evolution across transformer layers.&lt;/li&gt;&lt;li&gt;Introduces statistical validation for these measurements and demonstrates strong empirical gains (up to +13% AUROC) over prior utilization-based methods on hallucination benchmarks.&lt;/li&gt;&lt;li&gt;Shows robustness to relaxed assumptions about retrieval quality and model matching, improving practicality for real-world RAG safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Yeh', 'Sharon Li', 'Tanwi Mallick']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'RAG', 'robustness', 'LLM-safety', 'utilization-signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21875</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Antidistillation Fingerprinting</title><link>https://arxiv.org/abs/2602.03812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Antidistillation Fingerprinting (ADFP), a method to embed fingerprints into teacher model outputs that are optimized to be detectable after a student model is distilled.&lt;/li&gt;&lt;li&gt;Aligns the fingerprinting objective with students' learning dynamics using a gradient-based framework and a proxy model to select tokens that maximize expected detectability post-fine-tuning.&lt;/li&gt;&lt;li&gt;Claims Pareto improvements over prior watermarking/fingerprinting baselines on GSM8K and OASST1: stronger detection confidence with minimal utility degradation, and robustness when student architecture is unknown.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixuan Even Xu', 'John Kirchenbauer', 'Yash Savani', 'Asher Trockman', 'Alexander Robey', 'Tom Goldstein', 'Fei Fang', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'model theft detection', 'watermarking', 'defenses', 'model distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03812</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents</title><link>https://arxiv.org/abs/2602.03792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebSentinel, a two-step defense for detecting and localizing prompt injection attacks in webpages: (1) extract segments of interest that may be contaminated, (2) evaluate each segment for consistency with the webpage context.&lt;/li&gt;&lt;li&gt;Targets web agents and prompt-injection threats specifically, with a focus on localization of contaminated content (not just binary detection).&lt;/li&gt;&lt;li&gt;Reports substantially better performance than baselines across multiple collected datasets of contaminated and clean webpages; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'Yinuo Liu', 'Zhun Wang', 'Dawn Song', 'Neil Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'web agents', 'attack detection', 'security', 'localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03792</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection</title><link>https://arxiv.org/abs/2602.02980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WST-X, a family of feature extractors based on the wavelet scattering transform (1D and 2D) for interpretable speech deepfake detection.&lt;/li&gt;&lt;li&gt;Combines interpretability of hand-crafted filterbanks with representational power of deep features by using wavelets plus nonlinearities to capture fine-grained spectral and structural anomalies.&lt;/li&gt;&lt;li&gt;Evaluates on Deepfake-Eval-2024 and reports substantial improvements over existing front-ends; identifies small averaging scale and high-frequency/directional resolution as important design choices for robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xi Xuan', 'Davide Carbone', 'Ruchi Pandey', 'Wenxin Zhang', 'Tomi H. Kinnunen']&lt;/li&gt;&lt;li&gt;Tags: ['speech deepfake detection', 'wavelet scattering transform', 'interpretable features', 'audio forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02980</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation</title><link>https://arxiv.org/abs/2602.02536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniMod, a paradigm that converts sparse binary moderation decisions into dense multi-attribute reasoning trajectories (evidence grounding, modality assessment, risk mapping, policy decision, response generation).&lt;/li&gt;&lt;li&gt;Introduces UniRM, a multi-head scalar reward model providing attribute-level scores to supervise response generation and enable multi-dimensional supervision.&lt;/li&gt;&lt;li&gt;Presents optimization strategies to decouple task-specific parameters and rebalance training dynamics to reduce interference in multi-task multimodal moderation.&lt;/li&gt;&lt;li&gt;Reports competitive textual moderation results and establishes a new multimodal benchmark while using under 40% of training data compared to leading baselines; ablations validate the trajectory-based approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Gu', 'Kexin Huang', 'Lingyu Li', 'Ruilin Luo', 'Shiyang Huang', 'Zongqi Wang', 'Yujiu Yang', 'Yan Teng', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal moderation', 'safety/defense', 'reward model', 'robust training', 'attribute-level supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02536</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI</title><link>https://arxiv.org/abs/2602.02526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel failure mode called "Semantic Tunneling": generative models trained on recursive synthetic data collapse to a single low-entropy narrative attractor ('Robert Boulton') despite maintaining low perplexity.&lt;/li&gt;&lt;li&gt;Empirical protocol: sliding-window evaluation (N=1500, context L=128) showing rapid collapse within seven generations and a drop in Global Effective Rank from 3.62 to 2.22 while PPL remains deceptively low (~83.9).&lt;/li&gt;&lt;li&gt;Proposes the MNCIS framework and Adaptive Spectral Negative Coupling (ASNC) as a defense/regularization that induces 'Manifold Unfolding', increasing effective rank to ~5.35 and restoring semantic diversity.&lt;/li&gt;&lt;li&gt;Frames ASNC as a topological operator that constructs an artificial manifold to resist semantic attractors and preserve long-tail distributional properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengyue Hou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'failure-mode', 'manifold-collapse', 'defense', 'synthetic-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02526</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective</title><link>https://arxiv.org/abs/2602.03396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes logit-based distillation/model extraction from black-box LLM APIs using an information-theoretic lens, identifying conditional mutual information (CMI) between logits and inputs (given labels) as the quantity capturing distillation-relevant information.&lt;/li&gt;&lt;li&gt;Proposes defending against distillation by learning a transformation matrix that 'purifies' teacher outputs and a CMI-inspired anti-distillation objective to minimize distillation-relevant information while retaining task utility.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple LLMs and strong distillation attacks that the method substantially degrades distillation performance (model extraction) while preserving task accuracy, protecting model IP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Fang', 'Tianyi Zhang', 'Tianqu Zhuang', 'Jiawei Kong', 'Kuofeng Gao', 'Bin Chen', 'Leqi Liang', 'Shu-Tao Xia', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'distillation defense', 'information-theoretic', 'anti-distillation', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03396</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention</title><link>https://arxiv.org/abs/2602.03338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that an LLM critic with high offline failure-prediction accuracy (AUROC 0.94) can still cause large deployment-time harm—e.g., a 26 percentage-point collapse on one model while barely affecting another.&lt;/li&gt;&lt;li&gt;Identifies a disruption–recovery tradeoff: interventions can save failing trajectories but also disrupt trajectories that would have succeeded, so prediction accuracy alone is insufficient to guarantee safe intervention.&lt;/li&gt;&lt;li&gt;Proposes a small pre-deployment pilot test (≈50 tasks) to predict whether interventions will help or harm, and empirically validates that the test anticipates outcomes across benchmarks.&lt;/li&gt;&lt;li&gt;Finds interventions tend to degrade performance on high-success tasks but can modestly improve performance on high-failure environments (e.g., +2.8 pp on ALFWorld).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rakshith Vasudev', 'Melisa Russak', 'Dan Bikel', 'Waseem Alshikh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reliability', 'intervention', 'failure-prediction', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03338</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment</title><link>https://arxiv.org/abs/2602.02824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CATNIP, a calibrated, token-level Negative Preference Alignment method for selective LLM unlearning to remove undesirable memorized knowledge.&lt;/li&gt;&lt;li&gt;Rescales unlearning updates based on the model's token-level confidence to achieve fine-grained forgetting while reducing catastrophic forgetting of general knowledge.&lt;/li&gt;&lt;li&gt;Does not require retention data or curated contrastive response pairs, and is designed to be robust to data scarcity and variable sequence lengths.&lt;/li&gt;&lt;li&gt;Evaluated on MUSE and WMDP benchmarks, showing stronger forgetting vs. preservation tradeoffs compared to prior state-of-the-art unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengbang Yang', 'Yisheng Zhong', 'Junyuan Hong', 'Zhuangdi Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'privacy-preserving ML', 'knowledge forgetting', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02824</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Monotonicity as an Architectural Bias for Robust Language Models</title><link>https://arxiv.org/abs/2602.02686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes monotonicity as an architectural inductive bias for Transformer-based language models to improve robustness against adversarial prompts and jailbreaks.&lt;/li&gt;&lt;li&gt;Imposes monotonic constraints selectively in feed-forward sublayers while leaving attention mechanisms unconstrained, preserving expressivity for negation and contextual interactions.&lt;/li&gt;&lt;li&gt;Empirical results show a large reduction in adversarial attack success (≈69% → 19%) with only marginal degradation in standard summarization performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Cooper', 'Alireza Nadali', 'Ashutosh Trivedi', 'Alvaro Velasquez']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'adversarial attacks', 'model architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02686</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Test-Time Detoxification without Training or Learning Anything</title><link>https://arxiv.org/abs/2602.02498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a test-time, training-free detoxification method that steers autoregressive LLM generations toward less toxic continuations by optimizing input embeddings.&lt;/li&gt;&lt;li&gt;Uses zeroth-order optimization to approximate gradients of a toxicity score w.r.t. input embeddings, requiring only forward model access, embeddings, and a toxicity scorer (black-box setting).&lt;/li&gt;&lt;li&gt;Demonstrates robust toxicity reduction across models and prompts while preserving generation quality, often achieving the best toxicity-quality trade-off without retraining or auxiliary learned components.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Dionysis Kalogerias']&lt;/li&gt;&lt;li&gt;Tags: ['detoxification', 'model safety', 'black-box defense', 'zeroth-order optimization', 'test-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02498</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders</title><link>https://arxiv.org/abs/2602.02496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the 'Hypocrisy Gap', a mechanistic metric using Sparse Autoencoders (SAEs) to quantify divergence between a model's internal belief and its final chain-of-thought/output.&lt;/li&gt;&lt;li&gt;Method: derive an internal truth belief via sparse linear probes in latent space, then mathematically compare that belief to the final generated trajectory to detect unfaithful or sycophantic behavior.&lt;/li&gt;&lt;li&gt;Evaluation on Gemma, Llama, and Qwen using Anthropic's Sycophancy benchmark; reports AUROC 0.55–0.73 for sycophancy detection and 0.55–0.74 for hypocritical cases, outperforming a decision-aligned log-probability baseline (0.41–0.50).&lt;/li&gt;&lt;li&gt;Focuses on a safety/defense capability—detecting when LLMs produce outputs that conflict with their internal reasoning (unfaithfulness).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shikhar Shiromani', 'Archie Chaudhury', 'Sri Pranav Kunda']&lt;/li&gt;&lt;li&gt;Tags: ['model-unfaithfulness', 'safety-detection', 'chain-of-thought', 'mechanistic-interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02496</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings</title><link>https://arxiv.org/abs/2602.01757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zero2Text, a training-free, recursive online alignment method that performs cross-domain embedding inversion to recover textual inputs from target embeddings in strict black-box settings.&lt;/li&gt;&lt;li&gt;Combines LLM priors with a dynamic ridge-regression alignment loop to iteratively align generated text embeddings to the victim embedding without requiring in-domain training data or leaked pairs.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against RAG/vector DB scenarios, showing substantial gains over baselines (e.g., 1.8x ROUGE-L and 6.4x BLEU-2 on MS MARCO vs an OpenAI victim) and demonstrates that common defenses like differential privacy are ineffective against this adaptive attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doohyun Kim', 'Donghwa Kang', 'Kyungjae Lee', 'Hyeongboo Baek', 'Brent Byunghoon Kang']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy attacks', 'black-box attacks', 'retrieval-augmented generation (RAG)', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01757</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks</title><link>https://arxiv.org/abs/2507.08261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes applying Stein shrinkage (James-Stein) to batch normalization mean and variance estimators to improve estimation under adversarial perturbations modeled as sub-Gaussian.&lt;/li&gt;&lt;li&gt;Proves theoretically that the Stein shrinkage estimators dominate sample mean/variance in MSE under the adversarial model and that JS-BN yields a smaller local Lipschitz constant than vanilla BN.&lt;/li&gt;&lt;li&gt;Implements the Stein-corrected BN (JS-BN) and evaluates robustness empirically on image classification (CIFAR-10), 3D neuroimaging (PPMI), and image segmentation (Cityscapes) with and without adversarial attacks, reporting SOTA performance gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofia Ivolgina', 'P. Thomas Fletcher', 'Baba C. Vemuri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'batch-normalization', 'Stein-shrinkage', 'James-Stein', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08261</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models</title><link>https://arxiv.org/abs/2505.14103</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AUDIOJAILBREAK, a novel audio-based jailbreak attack for end-to-end large audio-language models (LALMs) that crafts suffixal perturbations to trigger malicious behavior without aligning to user prompts.&lt;/li&gt;&lt;li&gt;Key properties: asynchrony (suffixal, time-unaligned attacks), universality (single perturbation works across prompts), stealthiness (intent concealment strategies), and over-the-air robustness (accounts for reverberation).&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across many current LALMs, including bypassing Meta's Llama-Guard-3 and jailbreaking OpenAI's GPT-4o-Audio, and works in a more practical 'weak adversary' scenario where the attacker cannot fully control prompts.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation and highlights security implications, motivating improved robustness and defenses for audio-model jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangke Chen', 'Fu Song', 'Zhe Zhao', 'Xiaojun Jia', 'Yang Liu', 'Yanchen Qiao', 'Weizhe Zhang', 'Weiping Tu', 'Yuhong Yang', 'Bo Du']&lt;/li&gt;&lt;li&gt;Tags: ['audio jailbreak', 'adversarial examples', 'over-the-air attack', 'weak-adversary', 'large audio-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14103</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SPGCL: Simple yet Powerful Graph Contrastive Learning via SVD-Guided Structural Perturbation</title><link>https://arxiv.org/abs/2602.00064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPGCL, a graph contrastive learning framework that combines lightweight stochastic edge removal with truncated SVD-guided edge recovery to produce structure-aware contrastive views.&lt;/li&gt;&lt;li&gt;Two-stage strategy: (1) stochastic edge deletion to inject diversity, (2) SVD-derived scoring and top-P sparse edge recovery to restore important edges and add semantically meaningful links; includes a contrastive fusion module with a global similarity constraint.&lt;/li&gt;&lt;li&gt;Aims to improve robustness of GNNs to structural noise from adversarial attacks or imperfections; extensive experiments on ten benchmarks show improved robustness and accuracy over SOTA GCL and structure learning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Deng', 'Zhang Guo', 'Shuiping Gou', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph-ml', 'robustness', 'adversarial-robustness', 'graph-contrastive-learning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00064</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks</title><link>https://arxiv.org/abs/2601.22579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a non-intrusive e-commerce bot detection framework that models user sessions as a graph and uses an inductive graph neural network to classify automated activity.&lt;/li&gt;&lt;li&gt;Shows empirical improvements over a session-level MLP baseline (AUC, F1) and evaluates robustness via adversarial perturbation and cold-start simulations.&lt;/li&gt;&lt;li&gt;Emphasizes deployment practicality: no client-side instrumentation, real-time inference, and support for incremental updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sichen Zhao', 'Zhiming Xue', 'Yalun Qi', 'Xianling Zeng', 'Zihan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['bot-detection', 'graph-neural-networks', 'security-defense', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22579</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</title><link>https://arxiv.org/abs/2512.22522</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies gradient vanishing issues when using surrogate gradients for adversarial attacks on Spiking Neural Networks (SNNs) and provides theoretical analysis of the problem.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively changes the surrogate function shape during attack iterations to improve gradient fidelity and reduce vanishing.&lt;/li&gt;&lt;li&gt;Introduces Stable Adaptive Projected Gradient Descent (SA-PGD), an L_infty-constrained attack with adaptive step size to achieve faster, more stable convergence under imprecise gradients.&lt;/li&gt;&lt;li&gt;Empirical results show substantially higher attack success rates across SNN architectures and training schemes, indicating prior robustness estimates for SNNs were overly optimistic.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihang Wang', 'Dongcheng Zhao', 'Ruolin Chen', 'Qian Zhang', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'spiking neural networks', 'surrogate gradients', 'robustness evaluation', 'adaptive PGD']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22522</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization</title><link>https://arxiv.org/abs/2511.02570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DynaBO, a Bayesian optimization framework that allows continuous user priors during hyperparameter optimization by augmenting the acquisition function with decaying, prior-weighted preferences.&lt;/li&gt;&lt;li&gt;Introduces a data-driven safeguard to detect and potentially reject misleading priors, with theoretical guarantees on near-certain convergence and robustness to adversarial priors.&lt;/li&gt;&lt;li&gt;Provides proofs showing accelerated convergence when informative priors are provided and empirical results across HPO benchmarks demonstrating consistent improvements over baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Fehring', 'Marcel Wever', 'Maximilian Splieth\\"over', 'Leona Hennig', 'Henning Wachsmuth', 'Marius Lindauer']&lt;/li&gt;&lt;li&gt;Tags: ['bayesian optimization', 'hyperparameter optimization', 'robustness', 'adversarial priors', 'defense/detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02570</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning</title><link>https://arxiv.org/abs/2510.26829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that continual pretraining on plausible misinformation can overwrite specific factual knowledge in LLMs without degrading overall performance or alignment metrics.&lt;/li&gt;&lt;li&gt;Uses paired fact vs. counterfact items with graded poisoning ratios to track belief flips across checkpoints, layers, and model scales; finds abrupt flips concentrated in late layers and partial reversibility via patching.&lt;/li&gt;&lt;li&gt;Demonstrates that corrupted beliefs generalize beyond poisoned prompts, selectively impair commonsense reasoning, transfer imperfectly across languages, and can affect a majority of targeted facts at moderate poisoning levels.&lt;/li&gt;&lt;li&gt;Argues for representation-level monitoring of factual integrity during continual updates as a mitigation/defense direction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Svetlana Churina', 'Niranjan Chebrolu', 'Kokil Jaidka']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'continual pretraining', 'model integrity', 'belief corruption', 'defense/monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26829</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks</title><link>https://arxiv.org/abs/2510.10000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a primal WDRO approach using exact Lipschitz certificates to tighten robustness upper bounds for neural networks.&lt;/li&gt;&lt;li&gt;Derives an exact tractable WDRO characterization for ReLU networks via piecewise-affine activation cells and extends analysis to smooth activations (GELU, SiLU) used in Transformers.&lt;/li&gt;&lt;li&gt;Introduces Wasserstein Distributional Attacks (WDA, WDA++) that construct worst-case distributions (not limited to pointwise perturbations) and shows empirical improvements in robust accuracy and certificate tightness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bach C. Le', 'Tung V. Dao', 'Binh T. Nguyen', 'Hong T. M. Chu']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'adversarial attacks', 'robustness certification', 'Wasserstein attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10000</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sharpness-Aware Machine Unlearning</title><link>https://arxiv.org/abs/2506.13715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how Sharpness-Aware Minimization (SAM) affects machine unlearning, showing SAM can change denoising behavior when fitting forget sets and alters generalization depending on signal strength.&lt;/li&gt;&lt;li&gt;Characterizes SAM's signal surplus and demonstrates SAM can reduce the amount of retain data needed while improving unlearning effectiveness.&lt;/li&gt;&lt;li&gt;Proposes Sharp MinMax: splitting the model to learn retain signals with SAM and unlearn forget signals via sharpness maximization, improving unlearning performance.&lt;/li&gt;&lt;li&gt;Empirical results show reduced feature entanglement, a flatter loss landscape, and stronger resistance to membership inference attacks across noise levels, optimizers, and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Tang', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'membership inference', 'sharpness-aware minimization', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13715</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Differentially Private Relational Learning with Entity-level Privacy Guarantees</title><link>https://arxiv.org/abs/2506.08347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for entity-level differential privacy in relational/network-structured learning, addressing high sensitivity from entities participating in multiple relations.&lt;/li&gt;&lt;li&gt;Provides rigorous sensitivity analysis and an adaptive gradient clipping scheme that adjusts clipping thresholds by entity occurrence frequency to control per-entity contribution.&lt;/li&gt;&lt;li&gt;Extends privacy amplification results to a tractable subclass of coupled sampling (dependence via sample sizes) and presents a DP-SGD variant with provable privacy guarantees; empirical evaluation on text-attributed networks shows good utility-privacy trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinan Huang', 'Haoteng Yin', 'Eli Chien', 'Rongzhe Wei', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'dp-sgd', 'relational-learning', 'gradient-clipping']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08347</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data</title><link>https://arxiv.org/abs/2505.17730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a two-dimensional conceptual space for corrupted-data unlearning in vision classifiers: discovery rate (fraction of corrupted data known at unlearning) and statistical regularity (from random exemplars to shared concepts).&lt;/li&gt;&lt;li&gt;Introduces REM (Redirection for Erasing Memory): at unlearning time, corrupted-data signals are redirected to dedicated neurons that are subsequently discarded or deactivated to remove the corrupted influence.&lt;/li&gt;&lt;li&gt;Demonstrates that REM is broadly effective across the proposed task space, whereas prior specialized unlearning methods fail predictably outside their designed regions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stefan Schoepf', 'Michael Curtis Mozer', 'Nicole Elyse Mitchell', 'Alexandra Brintrup', 'Georgios Kaissis', 'Peter Kairouz', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'data poisoning defense', 'model editing', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17730</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OverThink: Slowdown Attacks on Reasoning LLMs</title><link>https://arxiv.org/abs/2502.02542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OverThink, a slowdown attack that injects benign decoy reasoning problems into context so reasoning LLMs expend many extra reasoning tokens while still producing correct answers.&lt;/li&gt;&lt;li&gt;Demonstrates attacks across closed-source and open-source reasoning models on datasets (FreshQA, SQuAD, MuSR) and with multimodal inputs (images that trigger excessive reasoning); shows transferability between models.&lt;/li&gt;&lt;li&gt;Evaluates LLM-based and systems-level defenses and discusses societal, financial, and energy impacts of such resource-exhaustion style attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhinav Kumar', 'Jaechul Roh', 'Ali Naseh', 'Marzena Karpinska', 'Mohit Iyyer', 'Amir Houmansadr', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'resource exhaustion', 'adversarial attack', 'defenses', 'multimodal attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02542</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility</title><link>https://arxiv.org/abs/2602.03402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Risk Awareness Injection (RAI), a lightweight, training-free method that constructs an Unsafe Prototype Subspace from language embeddings and modulates high-risk visual tokens to amplify safety signals.&lt;/li&gt;&lt;li&gt;Aims to restore LLM-like risk recognition in vision-language models to defend against multimodal jailbreak attacks while preserving semantic integrity for cross-modal reasoning.&lt;/li&gt;&lt;li&gt;Reports extensive evaluation on multiple jailbreak and utility benchmarks showing substantial reductions in attack success rates without compromising task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengxuan Wang', 'Yuxin Chen', 'Gang Xu', 'Tao He', 'Hongjie Jiang', 'Ming Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-jailbreak', 'defense', 'safety-calibration', 'vision-language-models', 'training-free']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03402</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention</title><link>https://arxiv.org/abs/2602.03338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that an LLM critic with high offline failure-prediction accuracy (AUROC 0.94) can still cause large deployment-time harm—e.g., a 26 percentage-point collapse on one model while barely affecting another.&lt;/li&gt;&lt;li&gt;Identifies a disruption–recovery tradeoff: interventions can save failing trajectories but also disrupt trajectories that would have succeeded, so prediction accuracy alone is insufficient to guarantee safe intervention.&lt;/li&gt;&lt;li&gt;Proposes a small pre-deployment pilot test (≈50 tasks) to predict whether interventions will help or harm, and empirically validates that the test anticipates outcomes across benchmarks.&lt;/li&gt;&lt;li&gt;Finds interventions tend to degrade performance on high-success tasks but can modestly improve performance on high-failure environments (e.g., +2.8 pp on ALFWorld).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rakshith Vasudev', 'Melisa Russak', 'Dan Bikel', 'Waseem Alshikh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reliability', 'intervention', 'failure-prediction', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03338</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Content: Behavioral Policies Reveal Actors in Information Operations</title><link>https://arxiv.org/abs/2602.02838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a platform-agnostic method to detect coordinated influence operations by modeling user activity as sequential decision processes (behavioral policies) rather than relying on content or network features.&lt;/li&gt;&lt;li&gt;Applies the approach to 12,064 Reddit users (including 99 IRA-linked accounts) using &gt;38 million activity steps from 2015–2018 and finds policy-based classifiers outperform text-based models (median macro-F1 94.9% vs 91.2%).&lt;/li&gt;&lt;li&gt;Shows policy features enable earlier detection from short traces and are more robust to evasion strategies and data corruption, suggesting resilience in environments with synthetic content or limited data access.&lt;/li&gt;&lt;li&gt;Argues behavioral dynamics provide stable, discriminative signals for detecting malicious actors and supports cross-platform, behavior-focused defense strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philipp J. Schneider', 'Lanqin Yuan', 'Marian-Andrei Rizoiu']&lt;/li&gt;&lt;li&gt;Tags: ['influence-operations-detection', 'behavioral-modeling', 'robustness-to-evasion', 'cybersecurity', 'online-manipulation-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02838</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating False Alarm and Missing Attacks in CAN IDS</title><link>https://arxiv.org/abs/2602.02781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic adversarial evaluation of ML-based intrusion detection systems (IDS) for Controller Area Network (CAN) traffic using the ROAD dataset.&lt;/li&gt;&lt;li&gt;Compares four shallow models and a deep neural network, applying protocol-compliant, payload-level perturbations generated with FGSM, BIM, and PGD against both benign and malicious CAN frames.&lt;/li&gt;&lt;li&gt;Finds strong baseline performance on benign traffic but significant increases in missed attacks under gradient-based adversarial perturbations; DNN best on benign data while extra trees (ET) shows relative robustness to missed-attack induction.&lt;/li&gt;&lt;li&gt;Concludes that adversarial manipulation can both trigger false alarms and enable stealthy evasion, highlighting the need for adversarial robustness evaluation in safety-critical automotive IDS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirab Hossain', 'Pablo Moriano']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'intrusion-detection', 'automotive-security', 'CAN', 'robustness-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02781</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Monotonicity as an Architectural Bias for Robust Language Models</title><link>https://arxiv.org/abs/2602.02686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes monotonicity as an architectural inductive bias for Transformer-based language models to improve robustness against adversarial prompts and jailbreaks.&lt;/li&gt;&lt;li&gt;Imposes monotonic constraints selectively in feed-forward sublayers while leaving attention mechanisms unconstrained, preserving expressivity for negation and contextual interactions.&lt;/li&gt;&lt;li&gt;Empirical results show a large reduction in adversarial attack success (≈69% → 19%) with only marginal degradation in standard summarization performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Cooper', 'Alireza Nadali', 'Ashutosh Trivedi', 'Alvaro Velasquez']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'adversarial attacks', 'model architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02686</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials</title><link>https://arxiv.org/abs/2602.02629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework that integrates Self-Sovereign Identity (SSI) using Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to authenticate participants.&lt;/li&gt;&lt;li&gt;Targets security threats in FL—specifically Sybil and poisoning attacks—by enforcing cryptographic identity verification rather than relying on behavioral/reputation systems.&lt;/li&gt;&lt;li&gt;Evaluates the approach on the MIMIC-IV dataset, reporting complete mitigation of Sybil attacks, strong predictive performance (AUC=0.954, Recall=0.890), negligible computational overhead (&lt;0.12%), and low operational cost (~$18 for 100 rounds).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rodrigo Tertulino', 'Ricardo Almeida', 'Laercio Alencar']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Sybil attacks', 'data poisoning', 'blockchain', 'self-sovereign identity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02629</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit</title><link>https://arxiv.org/abs/2602.02602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing that 3D Gaussian Splatting watermarking requires explicit, scenario-driven security objectives and realistic threat models.&lt;/li&gt;&lt;li&gt;Offers a reference framework that maps watermarking design choices to adversarial assumptions and organizes existing methods accordingly.&lt;/li&gt;&lt;li&gt;Examines a spread-spectrum embedding scheme within this framework, characterizing its strengths, limitations, and trade-offs for IP protection of 3D assets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangfan Deng', 'Anirudh Nakra', 'Min Wu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'threat modeling', 'intellectual property protection', '3D Gaussian Splatting', 'security framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02602</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Test-Time Detoxification without Training or Learning Anything</title><link>https://arxiv.org/abs/2602.02498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a test-time, training-free detoxification method that steers autoregressive LLM generations toward less toxic continuations by optimizing input embeddings.&lt;/li&gt;&lt;li&gt;Uses zeroth-order optimization to approximate gradients of a toxicity score w.r.t. input embeddings, requiring only forward model access, embeddings, and a toxicity scorer (black-box setting).&lt;/li&gt;&lt;li&gt;Demonstrates robust toxicity reduction across models and prompts while preserving generation quality, often achieving the best toxicity-quality trade-off without retraining or auxiliary learned components.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Dionysis Kalogerias']&lt;/li&gt;&lt;li&gt;Tags: ['detoxification', 'model safety', 'black-box defense', 'zeroth-order optimization', 'test-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02498</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Antidistillation Fingerprinting</title><link>https://arxiv.org/abs/2602.03812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Antidistillation Fingerprinting (ADFP), a method that aligns fingerprinting with the student model's learning dynamics to detect when a student was trained on a teacher's outputs.&lt;/li&gt;&lt;li&gt;Uses a proxy model and gradient-based token selection to sample tokens that maximize expected post-fine-tuning detectability, rather than relying on heuristic perturbations.&lt;/li&gt;&lt;li&gt;Shows empirical Pareto improvements over prior baselines on GSM8K and OASST1: stronger detection confidence with minimal utility degradation, and robustness to unknown student architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixuan Even Xu', 'John Kirchenbauer', 'Yash Savani', 'Asher Trockman', 'Alexander Robey', 'Tom Goldstein', 'Fei Fang', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'watermarking', 'model theft detection', 'defense', 'distillation robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03812</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense</title><link>https://arxiv.org/abs/2602.03611</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how exposing counterfactual explanations (CFs) via query APIs increases vulnerability to shadow-based membership inference attacks (MIAs).&lt;/li&gt;&lt;li&gt;Proposes a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to reduce memorization and limit effective training-data exposure while preserving utility and explainability.&lt;/li&gt;&lt;li&gt;Provides an extensive empirical evaluation characterizing the trade-off between privacy leakage, predictive performance, and explanation quality for MLaaS deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatima Ezzeddine', 'Osama Zammar', 'Silvia Giordano', 'Omran Ayoub']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'differential-privacy', 'counterfactual-explanations', 'active-learning', 'MLaaS-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03611</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network</title><link>https://arxiv.org/abs/2602.03596</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAGE-5GC: security-aware guidelines for evaluating ML-based anomaly detectors in realistic 5G Core deployments (non-IID data, adaptive attackers).&lt;/li&gt;&lt;li&gt;Evaluates baseline detector performance on a realistic 5GC dataset against PFCP control-plane attacks, then studies adversarial evasion by manipulating attacker-controllable features while preserving malicious functionality.&lt;/li&gt;&lt;li&gt;Performs sensitivity and robustness analysis using randomized perturbations and presents a black-box optimization evasion strategy using genetic algorithms that does not require model knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates that adversarially crafted attacks can substantially degrade detection, highlighting the need for robust, security-focused evaluation and defenses for 5G anomaly detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristian Manca', 'Christian Scano', 'Giorgio Piras', 'Fabio Brau', 'Maura Pintor', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evasion', 'anomaly-detection', '5G-core', 'robustness-evaluation', 'genetic-algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03596</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Most Convolutional Networks Suffer from Small Adversarial Perturbations</title><link>https://arxiv.org/abs/2602.03415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves that random convolutional neural networks admit adversarial examples at l2 distance on the order of ||x||/√d, i.e., essentially the smallest possible.&lt;/li&gt;&lt;li&gt;Shows that such small adversarial perturbations can be found with a single step of gradient descent (a simple gradient-based attack).&lt;/li&gt;&lt;li&gt;Develops Fourier-decomposition-based bounds on the singular values of random linear convolutional operators, which underpin the theoretical results and may be of independent interest.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Daniely', 'Idan Mehalel']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'robustness', 'CNNs', 'theoretical-analysis', 'gradient-based-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03415</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures</title><link>https://arxiv.org/abs/2602.03379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies syntactic similarity (not topical overlap) as the main cause of benign relearning in machine unlearning: syntactically similar data align with forgotten content in representations and gradients, causing recovery.&lt;/li&gt;&lt;li&gt;Provides systematic empirical analysis across benchmarks to demonstrate recovery triggered by syntactic similarity even without topical relevance.&lt;/li&gt;&lt;li&gt;Proposes syntactic diversification—paraphrasing forget queries into heterogeneous syntactic structures prior to unlearning—as an effective defense that suppresses benign relearning, speeds forgetting, and reduces the unlearning–utility trade-off.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangyeon Yoon', 'Hyesoo Hong', 'Wonje Jeung', 'Albert No']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'data-removal', 'defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03379</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity</title><link>https://arxiv.org/abs/2602.03329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows Byzantine-robust distributed optimization can be formulated as optimization with inexact gradient oracles (both additive and multiplicative errors).&lt;/li&gt;&lt;li&gt;Proves that gradient descent on standard robust aggregation schemes attains optimal asymptotic error in the Byzantine setting.&lt;/li&gt;&lt;li&gt;Proposes two accelerated schemes: a Nesterov-type accelerated inexact-gradient method and an Optimization under Similarity approach using an auxiliary loss to approximate the global loss.&lt;/li&gt;&lt;li&gt;Demonstrates theoretically and empirically that these approaches substantially reduce communication complexity compared to prior Byzantine-robust methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renaud Gaucher', 'Aymeric Dieuleveut', 'Hadrien Hendrikx']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'robust aggregation', 'defenses', 'optimization/acceleration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03329</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models</title><link>https://arxiv.org/abs/2602.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes Greedy Coordinate Gradient (GCG) jailbreak attacks on LLMs with a focus on the position of adversarial tokens (prefix vs suffix).&lt;/li&gt;&lt;li&gt;Finds that optimizing attacks to place adversarial tokens as prefixes (rather than suffixes) and varying token positions during evaluation substantially affect attack success rates.&lt;/li&gt;&lt;li&gt;Highlights a blind spot in current safety/robustness evaluations and advocates accounting for adversarial token position in LLM adversarial robustness assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hicham Eddoubi', 'Umar Faruk Abdullahi', 'Fadi Hassan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'LLM robustness', 'prompt injection', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03265</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning</title><link>https://arxiv.org/abs/2602.02962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Q-ShiftDP, a differentially private mechanism tailored to quantum machine learning that leverages the parameter-shift rule for gradient estimation.&lt;/li&gt;&lt;li&gt;Combines calibrated Gaussian noise with intrinsic quantum gradient stochasticity to obtain tighter sensitivity bounds and improved privacy-utility trade-offs versus classical DP-SGD.&lt;/li&gt;&lt;li&gt;Provides formal privacy and utility guarantees and empirically demonstrates better performance on benchmark datasets for QML tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang M. Ngo', 'Nhat Hoang-Xuan', 'Quan Nguyen', 'Nguyen Do', 'Incheol Shin', 'My T. Thai']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'quantum-machine-learning', 'parameter-shift-rule']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02962</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning</title><link>https://arxiv.org/abs/2602.02943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Distributionally Robust Decision-Focused Learning (DR-DFL) to train predictors that optimize downstream decision performance under worst-case distributions.&lt;/li&gt;&lt;li&gt;Proposes 3D-Learning, which parameterizes the worst-case distribution using a diffusion model to find realistic yet challenging OOD scenarios for robust training.&lt;/li&gt;&lt;li&gt;Claims the diffusion-augmented worst-case search yields a better trade-off between average-case and worst-case decision performance than classical DRO or simple data augmentation.&lt;/li&gt;&lt;li&gt;Empirical evaluation on an LLM resource provisioning task shows improved OOD generalization and decision outcomes compared to existing DRO and augmentation baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Wen', 'Lei Fan', 'Jianyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'diffusion models', 'decision-focused learning', 'OOD generalization', 'predict-then-optimize']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02943</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection</title><link>https://arxiv.org/abs/2602.02929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to detect APT-like behavior in system provenance data.&lt;/li&gt;&lt;li&gt;Builds process behavioral graphs via k-Nearest Neighbors on feature similarity, uses GAE to learn normal relational structure, and flags anomalies by reconstruction deviations.&lt;/li&gt;&lt;li&gt;Boosts anomaly scores using discovered infrequent behavioral co-occurrences (rare patterns); shows substantial ranking improvements on DARPA Transparent Computing datasets and competitive performance versus ensemble detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asif Tauhid', 'Sidahmed Benabderrahmane', 'Mohamad Altrabulsi', 'Ahamed Foisal', 'Talal Rahwan']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'graph-autoencoder', 'provenance-analysis', 'cybersecurity', 'rare-pattern-mining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02929</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space</title><link>https://arxiv.org/abs/2602.02925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDA2E, a Sparse Dual Adversarial Attention-based AutoEncoder to learn compact latent representations for imbalanced, high-dimensional anomaly detection.&lt;/li&gt;&lt;li&gt;Introduces a similarity-guided active learning framework with three strategies (normal-like expansion, anomaly-like prioritization, hybrid) and a new similarity measure SIM_NM1 for sparse binary embeddings.&lt;/li&gt;&lt;li&gt;Evaluates the method across 52 imbalanced datasets (including DARPA Transparent Computing scenarios) and compares against 15 state-of-the-art anomaly detectors, showing large gains in ranking (nDCG) and reduced labeling effort.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sidahmed Benabderrahmane', 'Petko Valtchev', 'James Cheney', 'Talal Rahwan']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'active-learning', 'cybersecurity', 'defense', 'representation-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02925</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks from Causal Principles</title><link>https://arxiv.org/abs/2602.02819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames membership inference attacks (MIAs) as a causal inference problem, defining memorization as the causal effect of including a data point in training.&lt;/li&gt;&lt;li&gt;Diagnoses biases in existing evaluation protocols: one-run methods suffer from interference between jointly included points, and zero-run methods for LLMs are confounded by non-random membership assignment.&lt;/li&gt;&lt;li&gt;Derives causal analogues of standard MIA metrics and proposes practical estimators for multi-run, one-run, and zero-run regimes with non-asymptotic consistency guarantees.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate reliable memorization measurement without repeated retraining and under distribution shift, providing a principled foundation for privacy evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mathieu Even', "Cl\\'ement Berenfeld", 'Linus Bleistein', 'Tudor Cebere', 'Julie Josse', "Aur\\'elien Bellet"]&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'causal-inference', 'privacy-evaluation', 'memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02819</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks</title><link>https://arxiv.org/abs/2602.02763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that temporal consistency of explanations is not a reliable proxy for model robustness: predictions and explanations can be adversarially decoupled.&lt;/li&gt;&lt;li&gt;Proposes TSEF (Time Series Explanation Fooler), a dual-target attack that jointly optimizes for targeted misclassification while keeping explanations consistent with a chosen reference rationale.&lt;/li&gt;&lt;li&gt;Demonstrates the attack across multiple time-series datasets and explainer backbones, finding that single-objective attacks disrupt attributions broadly while TSEF preserves plausible explanations.&lt;/li&gt;&lt;li&gt;Argues for coupling-aware robustness evaluations and more rigorous red-teaming of explanation mechanisms in time-series classification systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bohan Wang', 'Zewen Liu', 'Lu Lin', 'Hui Liu', 'Li Xiong', 'Ming Jin', 'Wei Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'explanation-manipulation', 'time-series', 'robustness-evaluation', 'attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02763</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</title><link>https://arxiv.org/abs/2602.02600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how sampling mechanisms (autoregressive vs diffusion) affect refusal behavior and jailbreak robustness in language models via a step-wise refusal dynamics framework.&lt;/li&gt;&lt;li&gt;Introduces the Step-Wise Refusal Internal Dynamics (SRI) signal to interpret internal recovery dynamics and detect anomalous/incomplete internal recovery linked to harmful generations.&lt;/li&gt;&lt;li&gt;Presents lightweight inference-time detectors based on SRI that generalize to unseen attacks and match or outperform existing defenses with over 100x lower inference overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eliron Rahimi', 'Elad Hirshel', 'Rom Himelstein', 'Amit LeVi', 'Avi Mendelson', 'Chaim Baskin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaks', 'defense/detection', 'model safety', 'diffusion language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02600</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models</title><link>https://arxiv.org/abs/2602.02557</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how strong modality alignment in omni-models can cause textual jailbreak vulnerabilities to transfer to audio (the 'alignment curse').&lt;/li&gt;&lt;li&gt;Empirically evaluates textual jailbreaks, text-transferred audio jailbreaks, and audio-based jailbreaks on recent omni-models, finding text-transferred audio attacks perform comparably or better.&lt;/li&gt;&lt;li&gt;Demonstrates strong cross-model transferability and effectiveness under an audio-only access threat model, proposing text-transferred audio attacks as simple, powerful baselines for audio red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yupeng Chen', 'Junchi Yu', 'Aoxi Liu', 'Philip Torr', 'Adel Bibi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'cross-modality transfer', 'audio attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02557</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation</title><link>https://arxiv.org/abs/2602.02536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniMod, a paradigm that converts sparse binary moderation decisions into dense multi-attribute reasoning trajectories (evidence grounding, modality assessment, risk mapping, policy decision, response generation).&lt;/li&gt;&lt;li&gt;Introduces UniRM, a multi-head scalar reward model providing attribute-level scores to supervise response generation and enable multi-dimensional supervision.&lt;/li&gt;&lt;li&gt;Presents optimization strategies to decouple task-specific parameters and rebalance training dynamics to reduce interference in multi-task multimodal moderation.&lt;/li&gt;&lt;li&gt;Reports competitive textual moderation results and establishes a new multimodal benchmark while using under 40% of training data compared to leading baselines; ablations validate the trajectory-based approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Gu', 'Kexin Huang', 'Lingyu Li', 'Ruilin Luo', 'Shiyang Huang', 'Zongqi Wang', 'Yujiu Yang', 'Yan Teng', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal moderation', 'safety/defense', 'reward model', 'robust training', 'attribute-level supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02536</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI</title><link>https://arxiv.org/abs/2602.02526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel failure mode called "Semantic Tunneling": generative models trained on recursive synthetic data collapse to a single low-entropy narrative attractor ('Robert Boulton') despite maintaining low perplexity.&lt;/li&gt;&lt;li&gt;Empirical protocol: sliding-window evaluation (N=1500, context L=128) showing rapid collapse within seven generations and a drop in Global Effective Rank from 3.62 to 2.22 while PPL remains deceptively low (~83.9).&lt;/li&gt;&lt;li&gt;Proposes the MNCIS framework and Adaptive Spectral Negative Coupling (ASNC) as a defense/regularization that induces 'Manifold Unfolding', increasing effective rank to ~5.35 and restoring semantic diversity.&lt;/li&gt;&lt;li&gt;Frames ASNC as a topological operator that constructs an artificial manifold to resist semantic attractors and preserve long-tail distributional properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengyue Hou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'failure-mode', 'manifold-collapse', 'defense', 'synthetic-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02526</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes token-level data filtering during pretraining to remove undesired capabilities (tested on a medical domain) as an alternative to post-hoc mitigations.&lt;/li&gt;&lt;li&gt;Finds token filtering outperforms document-level filtering, achieving similar reduction in target capability with less degradation to benign capabilities and becoming more effective at larger model/compute scales.&lt;/li&gt;&lt;li&gt;Introduces a methodology for labeling tokens using sparse autoencoders and distilling efficient classifiers, and shows filtering can be robust to noisy labels and models remain alignable on the filtered domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['data filtering', 'pretraining defenses', 'capability suppression', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIA) against Diffusion Language Models (DLMs), highlighting novel vulnerabilities due to DLMs' many mask configurations.&lt;/li&gt;&lt;li&gt;Proposes SAMA (Subset-Aggregated Membership Attack): samples masked subsets at varying densities, uses sign-based statistics and inverse-weighted aggregation to amplify sparse memorization signals.&lt;/li&gt;&lt;li&gt;Empirical evaluation across nine datasets shows substantial improvements (≈30% relative AUC, up to 8x gains at low FPR) over prior baselines, indicating serious privacy leakage risks in DLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Kaiyuan Zhang', 'Yuntao Du', 'Edoardo Stoppa', 'Charles Fleming', 'Ashish Kundu', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'model-extraction/attack', 'diffusion-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20125</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</title><link>https://arxiv.org/abs/2601.18739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SeNeDiF-OOD, a hierarchical Semantic Nested Dichotomy Fusion framework that decomposes OOD detection into layered binary fusion nodes aligned with different semantic abstraction levels.&lt;/li&gt;&lt;li&gt;Validates the method on MonuMAI monument style classification, addressing diverse OOD inputs including non-monument images, unknown architectural styles, and adversarial attacks.&lt;/li&gt;&lt;li&gt;Reports improved detection of heterogeneous OOD categories while preserving in-distribution performance compared to traditional baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ignacio Antequera-S\\'anchez", "Juan Luis Su\\'arez-D\\'iaz", 'Rosana Montes', 'Francisco Herrera']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'robustness', 'adversarial-robustness', 'open-world-learning', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18739</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically demonstrates that iterative use of AI-generated medical data causes collapse of pathological variability and skews demographic representation, degrading diagnostic reliability across text, vision-language reports, and synthetic images.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and models produce overconfident yet incorrect reports, with false reassurance rates rising to ~40% after a few generations.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies: simply scaling synthetic volume fails, while mixing real data with quality-aware filtering preserves diversity and diagnostic performance.&lt;/li&gt;&lt;li&gt;Concludes that without mandated human verification, generative-AI-driven data contamination poses a systemic vulnerability to healthcare data ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Yun Liu', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'self-referential training / data poisoning', 'robustness and defenses', 'medical AI reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight single-pass detector for hallucinations in Visual Question Answering that exploits internal VLM signals (token-level decoding uncertainty, intermediate visual representations, cross-modal alignment) fused via branch-wise evidence encoding and uncertainty-aware attention.&lt;/li&gt;&lt;li&gt;Extends the LLM-as-a-Judge paradigm to VQA and introduces a low-cost, model-dependent automatic supervision strategy to train the detector without expensive human labels.&lt;/li&gt;&lt;li&gt;Reports significantly improved detection effectiveness and efficiency across multiple VQA benchmarks and analyzes how different internal signals and VLM architectures yield complementary diagnostic cues and distinct hallucination patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'VQA', 'multimodal safety', 'internal model signals', 'LLM-as-a-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust MLLM Unlearning via Visual Knowledge Distillation</title><link>https://arxiv.org/abs/2512.11325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Visual Knowledge Distillation (VKD) to disentangle and selectively erase visual knowledge in MLLMs while preserving textual knowledge, using intermediate visual representations as supervision.&lt;/li&gt;&lt;li&gt;Fine-tunes only the visual components of the MLLM for efficient unlearning, claiming improved effectiveness and model utility over prior output-level unlearning methods.&lt;/li&gt;&lt;li&gt;Performs extensive experiments and is the first to evaluate robustness of MLLM unlearning against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Zhenxing Niu', 'Haoxuan Ji', 'Guangyu He', 'Haichang Gao', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'multimodal security', 'knowledge distillation', 'robustness to relearning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11325</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAS-Shield: A Defense Framework for Secure and Efficient LLM MAS</title><link>https://arxiv.org/abs/2511.22924</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAS-Shield, a coarse-to-fine defense framework for LLM-based Multi-Agent Systems to mitigate cascading linguistic/adversarial attacks.&lt;/li&gt;&lt;li&gt;Three-stage protocol: (1) Critical Agent Selection to focus on high-influence nodes, (2) Light Auditing using lightweight sentry models for fast filtering, (3) Global Consensus Auditing escalating ambiguous/suspicious cases to a heavyweight committee.&lt;/li&gt;&lt;li&gt;Claims empirical results: ~92.5% recovery against diverse adversarial scenarios and &gt;70% reduction in defense latency compared to prior methods, optimizing the security–efficiency trade-off.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Wang', 'Zhaojiacheng Zhou', 'Bunyod Suvonov', 'Jiong Lou', 'Jie LI']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'multi-agent-systems', 'adversarial-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22924</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs</title><link>https://arxiv.org/abs/2511.22099</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study of how low-rank factorization (model compression) affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment.&lt;/li&gt;&lt;li&gt;Key findings: low-rank compression preserves or improves training-data privacy but reduces PII protection in conversational settings; adversarial robustness is generally preserved or improved even under deep compression; ethical reasoning degrades in zero-shot but partially recovers with few-shot prompting; fairness worsens under compression.&lt;/li&gt;&lt;li&gt;Analyzes effects of model scale and fine-tuning on trustworthiness and provides a gradient-based attribution analysis to identify layers most responsible for adversarial robustness to guide trustworthy compression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Agyei Asante', 'Md Mokarram Chowdhury', 'Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'adversarial-robustness', 'model-compression', 'fairness', 'attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22099</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.17098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Malicious Token Injection (MTI), a framework that corrupts transformer KV cache entries via controlled perturbations (Gaussian noise, zeroing, orthogonal rotations) at selected layers/timesteps.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis linking cache perturbations to logit deviations using the Frobenius norm of corruption and softmax Lipschitz properties.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that MTI substantially alters next-token distributions and degrades downstream task performance across GPT-2 and LLaMA-2/7B, and disrupts retrieval-augmented and agentic reasoning pipelines.&lt;/li&gt;&lt;li&gt;Positions KV cache integrity as an overlooked attack surface and a reproducible threat model for future robustness and security work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Hossain', 'Swayamjit Saha', 'Somshubhra Roy', 'Ravi Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['cache corruption', 'adversarial attack', 'LLM vulnerabilities', 'inference-time attack', 'attack theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17098</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Search Goes Wrong: Red-Teaming Web-Augmented Large Language Models</title><link>https://arxiv.org/abs/2510.09689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CREST-Search, a red-teaming framework targeting web-augmented LLMs to expose vulnerabilities in the search-and-citation pipeline.&lt;/li&gt;&lt;li&gt;Proposes three novel attack strategies that craft innocuous-looking queries which induce unsafe or low-credibility citations from retrieved web content.&lt;/li&gt;&lt;li&gt;Uses an iterative in-context refinement approach to improve adversarial query effectiveness under black-box constraints and fine-tunes a red-teaming model on a new WebSearch-Harm dataset.&lt;/li&gt;&lt;li&gt;Empirical results show CREST-Search can bypass safety filters and systematically reveal weaknesses in web search–based LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Ou', 'Kangjie Chen', 'Xingshuo Han', 'Gelei Deng', 'Jie Zhang', 'Han Qiu', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'retrieval-augmented-LLMs', 'search-based attacks', 'safety-evasion', 'adversarial-dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09689</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT, an adaptive classifier that learns a witness function to improve logits-based LLM-generated-text detection.&lt;/li&gt;&lt;li&gt;Provides formal statistical guarantees on true/false positive and negative rates for the detector.&lt;/li&gt;&lt;li&gt;Empirical results show consistent improvements over state-of-the-art logits-based detectors across multiple datasets and LLMs (up to 37% improvement).&lt;/li&gt;&lt;li&gt;Open-source Python implementation available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated-text detection', 'logits-based detection', 'statistical guarantees', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC (Sequential Tool Attack Chaining), a novel multi-turn attack that chains individually innocuous tool calls into harmful operations executed by tool-enabled LLM agents.&lt;/li&gt;&lt;li&gt;Presents an automated closed-loop pipeline that synthesizes, validates in-environment, and reverse-engineers stealthy multi-step tool chains; evaluated on 483 STAC cases (1,352 interaction sets) across diverse domains and agent types.&lt;/li&gt;&lt;li&gt;Finds high vulnerability of state-of-the-art agents (including GPT-4.1) with attack success rates &gt;90% in most cases; analyzes existing prompt-based defenses and shows they provide limited protection.&lt;/li&gt;&lt;li&gt;Proposes a reasoning-driven defense prompt that reduces ASR by up to 28.8%, arguing defenses must reason about entire action sequences and cumulative effects rather than isolated prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'tool-using agents', 'adversarial attacks', 'defense mechanisms', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GHOST, an automated method that optimizes in image embedding space and uses diffusion guidance to generate natural-looking images that induce object hallucinations in MLLMs.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success (≈28% vs ≈1% for prior data-driven methods) and strong cross-model transferability (e.g., images optimized for Qwen2.5-VL cause GPT-4o to hallucinate 66.5% of the time).&lt;/li&gt;&lt;li&gt;Validates that generated images remain object-free via metrics and human evaluation, and shows that fine-tuning on these images reduces hallucination, providing both diagnostic and corrective utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'red teaming', 'MLLM hallucination', 'robustness / defense', 'transferable attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Information Security Based on LLM Approaches: A Review</title><link>https://arxiv.org/abs/2507.18215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of large language model (LLM) applications in information security, covering tasks such as malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization.&lt;/li&gt;&lt;li&gt;Explains LLM technical basis (neural networks, Transformer) and argues LLM integration can improve detection accuracy and reduce false positives in security systems.&lt;/li&gt;&lt;li&gt;Identifies key challenges: model transparency, interpretability, scene/adaptation generalization, and the need to optimize model structure and robustness for practical deployment.&lt;/li&gt;&lt;li&gt;Positions LLMs primarily as tools to enhance defensive/security capabilities rather than introducing new attack techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chang Gong', 'Zhongwen Li', 'Xiaoqi Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-based defenses', 'vulnerability detection', 'malware/malicious code identification', 'network threat analysis', 'security survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18215</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title><link>https://arxiv.org/abs/2507.04531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-Fusion, a token-level Differentially Private Inference (DPI) mechanism for LLMs that bounds the influence of sensitive context tokens on model outputs.&lt;/li&gt;&lt;li&gt;Method: label sensitive tokens, compute baseline inference without them, compute inference with them, then blend the two output distributions to enforce a per-token DP bound controlled by ε.&lt;/li&gt;&lt;li&gt;Primary application is document privatization (paraphrasing documents to protect PII) with provable privacy guarantees and empirically better utility (reported ~6× lower perplexity than related DPI methods); also mitigates certain jailbreak/prompt-injection risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushil Thareja', 'Preslav Nakov', 'Praneeth Vepakomma', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['Differential Privacy', 'Privacy-preserving Inference', 'Defense', 'Prompt Injection Mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04531</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://arxiv.org/abs/2506.12706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NAP-Tuning, extending adversarial prompt tuning to multi-modal (text + visual) and multi-layer prompt architectures for VLMs.&lt;/li&gt;&lt;li&gt;Proposes a Neural Augmentor with token refiners that perform feature purification via residual reconstruction to correct adversarial distortions in feature space.&lt;/li&gt;&lt;li&gt;Demonstrates substantial robustness improvements under strong attacks (AutoAttack), reporting ~33% gains on ViT-B16 and ViT-B32 while maintaining competitive clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Xin Wang', 'Xingjun Ma', 'Lingyu Qiu', 'Yu-Gang Jiang', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'vision-language-models', 'prompt-tuning', 'feature-purification', 'adversarial-defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12706</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Shielding for Safe Reinforcement Learning under Hidden-Parameter Dynamics Shifts</title><link>https://arxiv.org/abs/2506.11033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Shielding, a framework for safe reinforcement learning under hidden-parameter dynamics shifts that infers a low-dimensional latent of dynamics online to adapt safety checks.&lt;/li&gt;&lt;li&gt;Uses a two-layer safety strategy: (1) safety-regularized policy optimization to proactively avoid high-cost regions, and (2) reactive adaptive shielding that forecasts safety risks using the inferred dynamics and applies uncertainty-aware bounds via conformal prediction to filter unsafe actions.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees linking shielding prediction errors to bounds on average cost rate, and demonstrates empirically on Safe-Gym benchmarks that the method improves return-safety trade-offs and generalizes to unseen dynamics with modest runtime overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minjae Kwon', 'Tyler Ingebrand', 'Ufuk Topcu', 'Lu Feng']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'runtime shielding / guardrails', 'robustness to dynamics shift', 'conformal prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11033</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection</title><link>https://arxiv.org/abs/2505.16530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DuFFin, a dual-level fingerprinting framework for black-box ownership verification of LLMs combining trigger-pattern fingerprints and knowledge-level fingerprints.&lt;/li&gt;&lt;li&gt;Designed to identify the source of suspect models even after fine-tuning, quantization, or safety-alignment modifications.&lt;/li&gt;&lt;li&gt;Evaluated on multiple open-source base models and their variants, achieving high IP-ROC (&gt;0.95) for accurate copyright verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuliang Yan', 'Haochun Tang', 'Shuo Yan', 'Enyan Dai']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'IP protection', 'watermarking', 'ownership verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16530</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</title><link>https://arxiv.org/abs/2505.15386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RePPL, a method that recalibrates perplexity-based uncertainty by combining uncertainties from semantic propagation (attention across layers) and language generation.&lt;/li&gt;&lt;li&gt;Outputs token-level uncertainty scores to both detect and explain which input parts trigger hallucinations in QA models.&lt;/li&gt;&lt;li&gt;Evaluated across multiple QA datasets and advanced models, reporting strong detection performance (average AUC ~0.833).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Huang', 'Junyan Zhang', 'Zihao Wang', 'Biquan Bie', 'Yunzhong Qiu', 'Xuming Hu', 'Yi R. Fung', 'Xinlei He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'uncertainty-estimation', 'explainability', 'model-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15386</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Code-Mixed Phonetic Perturbations for Red-Teaming LLMs</title><link>https://arxiv.org/abs/2505.14226</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CMP-RT, a red-teaming attack that mixes code-switching and phonetic perturbations to alter safety-critical tokens while preserving phonetic/readable meaning.&lt;/li&gt;&lt;li&gt;Identifies a tokenizer-level vulnerability in transformer-based LLMs that allows harmful prompts to bypass alignment and safety mechanisms.&lt;/li&gt;&lt;li&gt;Demonstrates attack robustness against standard defenses, scalability, and generalization across models (e.g., Gemini-3-Pro) and modalities.&lt;/li&gt;&lt;li&gt;Frames tokenization as an under-examined weakness in current safety pipelines and presents CMP-RT as a practical threat model for red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', 'Siddharth D Jaiswal']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'prompt jailbreak / safety evasion', 'tokenizer vulnerability', 'adversarial/phonetic perturbation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14226</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination</title><link>https://arxiv.org/abs/2502.12769</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a multilingual hallucination detection model by training on machine-translated English data and validating with manually annotated gold data in five high-resource languages.&lt;/li&gt;&lt;li&gt;Builds an open-domain long-form QA dataset across 30 languages (LLM-generated prompts, Wikipedia references) to estimate hallucination rates for six open-source LLM families.&lt;/li&gt;&lt;li&gt;Findings: smaller models hallucinate more; models with broader language support show higher hallucination rates; absolute hallucinated tokens higher in high-resource languages due to longer outputs, but normalized hallucination rate is uncorrelated with digital footprint size.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Obaid ul Islam', 'Anne Lauscher', 'Goran Glava\\v{s}']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'multilingual evaluation', 'LLM robustness', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12769</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title><link>https://arxiv.org/abs/2410.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SVIP, a secret-based verifiable inference protocol to detect when a remote provider substitutes a requested open-source LLM with a smaller model.&lt;/li&gt;&lt;li&gt;Requires the provider to return generated text plus processed hidden representations; trains a proxy task on representations to produce a unique model identifier (fingerprint).&lt;/li&gt;&lt;li&gt;Includes a secret mechanism and analyzes multiple strong/adaptive adversarial scenarios; demonstrates low error rates (FNR &lt;5%, FPR &lt;3%) and very low verification latency (&lt;0.01s per query).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Sun', 'Yuhang Li', 'Yue Zhang', 'Yuchen Jin', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable inference', 'model substitution detection', 'LLM integrity', 'representation fingerprinting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22307</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</title><link>https://arxiv.org/abs/2601.21083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenSec, a dual-control reinforcement-learning environment to evaluate incident-response (IR) agents under adversarial prompt-injection/evidence scenarios.&lt;/li&gt;&lt;li&gt;Proposes execution-based metrics (time-to-first-containment, blast radius/false positives per episode, injection violation rates) that score real-world containment actions rather than static capability metrics.&lt;/li&gt;&lt;li&gt;Evaluates four frontier models across 40 episodes and finds widespread calibration failures (high over-triggering and false-positive rates), showing the benchmark surfaces vulnerabilities hidden by aggregate success metrics.&lt;/li&gt;&lt;li&gt;Provides code and an environment for systematic red-teaming and calibration measurement (GitHub link included).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jarrod Barnes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'incident response', 'benchmarking', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21083</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment</title><link>https://arxiv.org/abs/2601.10520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a neuro-symbolic containment architecture that decouples normative reasoning from instrumental decision-making to enforce ethical constraints on AI agents.&lt;/li&gt;&lt;li&gt;Architecture comprises three modules: Moral Module (deontic logic-based reasoning over permissible macro actions), Decision-Making Module (instrumental agent constrained by macro actions), and a Guard that monitors and enforces compliance.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability, contestability, and justifiability via symbolic representations and claims formal verification and statistical guarantees of alignment.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on a case study with an LLM therapy assistant to show stakeholder contestability and behavior refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felix Jahn', 'Yannic Muskalla', 'Lisa Dargasz', 'Patrick Schramowski', 'Kevin Baum']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'AI-alignment', 'guardrails', 'neuro-symbolic', 'deontic-logic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10520</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LoRA is All You Need for Safety Alignment of Reasoning LLMs</title><link>https://arxiv.org/abs/2507.17075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using LoRA (parameter-efficient adapters) during supervised fine-tuning (SFT) on refusal/safety datasets to achieve safety alignment while largely avoiding the typical 'Safety Tax' degradation of reasoning capabilities.&lt;/li&gt;&lt;li&gt;Empirically shows comparable safety to full-model alignment and preservation of reasoning performance across multiple model sizes/architectures, two safety benchmarks, and four reasoning benchmarks (math, science, code).&lt;/li&gt;&lt;li&gt;Ablations find rank-1 LoRA suffices, that updating MLP up-projection and middle layers is most effective, and configuration choices impact the safety–reasoning trade-off.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis: overshooting LoRA rank induces base-task degradation inversely proportional to the intrinsic dimensionality of the base task, explaining when LoRA preserves base capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihao Xue', 'Baharan Mirzasoleiman']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'LoRA', 'parameter-efficient fine-tuning', 'alignment defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17075</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title><link>https://arxiv.org/abs/2506.00641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentAuditor, a training-free, memory-augmented framework that enables LLM evaluators to emulate human expert reasoning by extracting structured semantic features and chain-of-thought traces from past interactions.&lt;/li&gt;&lt;li&gt;Uses a multi-stage, context-aware retrieval-augmented generation pipeline to retrieve relevant prior reasoning experiences to guide evaluation of new cases.&lt;/li&gt;&lt;li&gt;Introduces ASSEBench, a 2,293-record benchmark covering 15 risk types across 29 scenarios, with Strict and Lenient judgment standards for ambiguous risks.&lt;/li&gt;&lt;li&gt;Empirical results show AgentAuditor improves LLM-based evaluation performance and achieves human-level accuracy / SOTA for agent safety and security assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanjun Luo', 'Shenyu Dai', 'Chiming Ni', 'Xinfeng Li', 'Guibin Zhang', 'Kun Wang', 'Tongliang Liu', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['security-evaluation', 'red-teaming', 'safety-assessment', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00641</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.02395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a new threat model, Tag-Along Attacks, where a tool-less adversary induces a safety-aligned Operator to perform prohibited tool use via conversation.&lt;/li&gt;&lt;li&gt;Introduces Slingshot, a cold-start reinforcement learning framework that autonomously discovers agent-to-agent jailbreaking strategies without supervised data.&lt;/li&gt;&lt;li&gt;Finds learned attacks converge to short instruction-like patterns and demonstrates high effectiveness and zero-shot transfer across multiple model families (e.g., Qwen2.5, Gemini 2.5 Flash, Meta-SecAlign).&lt;/li&gt;&lt;li&gt;Reports strong empirical results (e.g., 67.0% success vs 1.7% baseline on held-out hard tasks) and frames Tag-Along as a verifiable, practical threat in tool-augmented agent settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nellessen', 'Tal Kachman']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'reinforcement learning', 'red teaming', 'agent-to-agent attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02395</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Decoupling Generalizability and Membership Privacy Risks in Neural Networks</title><link>https://arxiv.org/abs/2602.02296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that generalization and membership privacy risks reside in different regions/components of deep neural network architectures, enabling potential decoupling of utility and privacy.&lt;/li&gt;&lt;li&gt;Proposes a Privacy-Preserving Training Principle (PPTP) that protects privacy-vulnerable components of the model while minimizing loss in generalizability.&lt;/li&gt;&lt;li&gt;Provides extensive evaluations showing the approach preserves model generalization significantly better than prior defenses while enhancing membership privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingli Fang', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-preserving-training', 'model-privacy', 'defense', 'neural-networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02296</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild</title><link>https://arxiv.org/abs/2602.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Spoofing-Aware Speaker Verification (SASV) framework combining a spoofing detector and a speaker verification network to handle in-the-wild spoofing attacks.&lt;/li&gt;&lt;li&gt;Spoofing detector uses self-supervised speech embeddings, a graph neural network backend, and a top-3 layer mixture-of-experts fusion for anti-spoofing.&lt;/li&gt;&lt;li&gt;Speaker verification employs a low-complexity CNN that fuses 1D and 2D multi-scale features, trained with SphereFace loss and contrastive circle loss to emphasize hard/easy pair distinctions.&lt;/li&gt;&lt;li&gt;System-level defenses include AS-Norm score normalization and model ensembling to improve discriminative robustness against spoofed utterances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnab Das', 'Yassine El Kheir', 'Enes Erdem Erdogan', 'Feidi Kallel', 'Tim Polzehl', 'Sebastian Moeller']&lt;/li&gt;&lt;li&gt;Tags: ['anti-spoofing', 'speaker verification', 'SASV', 'defense', 'graph neural network']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02286</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RACA: Representation-Aware Coverage Criteria for LLM Safety Testing</title><link>https://arxiv.org/abs/2602.02280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RACA, a representation-aware coverage criterion framework tailored for LLM safety testing to systematically evaluate and prioritize jailbreak/attack prompts.&lt;/li&gt;&lt;li&gt;Uses representation engineering to identify safety-critical concepts from a small expert-curated calibration set, computes conceptual activation scores, and applies six sub-criteria (individual and compositional) to derive coverage.&lt;/li&gt;&lt;li&gt;Empirically demonstrates RACA identifies high-quality jailbreak prompts, outperforms traditional neuron-level coverage criteria, and is useful for test-set prioritization and attack prompt sampling with robustness and generalization across configurations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Zhixin Zhang', 'Chengcan Wu', 'Yihao Zhang', 'Xiaokun Luan', 'Meng Sun']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak attacks', 'coverage testing', 'adversarial testing', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02280</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Malware Detection Through Memory Analysis</title><link>https://arxiv.org/abs/2602.02184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies machine learning (XGBoost) to the Canadian Institute for Cybersecurity's MalMemAnalysis-2022 memory-analysis dataset for malware detection.&lt;/li&gt;&lt;li&gt;Builds both binary (benign vs malicious) and multi-class (benign, ransomware, spyware, Trojan) classifiers; binary model achieves 99.98% accuracy and F1, multi-class achieves 87.54% accuracy and 81.26% F1.&lt;/li&gt;&lt;li&gt;Focuses on trade-offs between detection performance and inference speed; reports classification times (~37.3 ms for 50 samples binary, ~43.2 ms multi-class).&lt;/li&gt;&lt;li&gt;Project framed as advancing real-time obfuscated malware detection to improve online privacy and safety; work completed as a course project.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Nassar']&lt;/li&gt;&lt;li&gt;Tags: ['malware-detection', 'memory-forensics', 'machine-learning', 'XGBoost', 'cybersecurity-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02184</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning</title><link>https://arxiv.org/abs/2602.02004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'reasoning drift' in multimodal reasoning models where attention drifts to question-irrelevant entities, causing hallucinations.&lt;/li&gt;&lt;li&gt;Introduces ClueRecall, a metric to assess visual clue retrieval, and ClueTracer, a training-free, parameter-free, architecture-agnostic plugin that traces question-to-vision clue propagation to localize task-relevant patches and suppress spurious attention.&lt;/li&gt;&lt;li&gt;ClueTracer requires no additional training and improves performance across multiple reasoning architectures (~1.21× on reasoning benchmarks) and also yields gains when applied to non-reasoning settings (~1.14×).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gongli Xi', 'Kun Wang', 'Zeming Gao', 'Huahui Yi', 'Haolang Lu', 'Ye Tian', 'Wendong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_suppression', 'multimodal_robustness', 'visual_grounding', 'training-free_defense', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02004</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated</title><link>https://arxiv.org/abs/2602.01973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies systematic bias in AI-generated image detectors (tendency to misclassify fakes as real) caused by distributional shift and implicit priors.&lt;/li&gt;&lt;li&gt;Proposes a theoretically grounded post-hoc calibration method: a learnable scalar correction to model logits optimized on a small target-validation set while keeping the backbone frozen.&lt;/li&gt;&lt;li&gt;Calibration realigns the decision boundary without retraining and purportedly can be done without ground-truth labels, improving robustness to generator distribution shift across benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muli Yang', 'Gabriel James Goenawan', 'Henan Wang', 'Huaiyuan Qin', 'Chenghao Xu', 'Yanhua Yang', 'Fen Fang', 'Ying Sun', 'Joo-Hwee Lim', 'Hongyuan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'calibration', 'distribution-shift', 'post-hoc-defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01973</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework</title><link>https://arxiv.org/abs/2602.01942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the 4C Framework (Core, Connection, Cognition, Compliance) to organize security risks for agentic, multi-agent AI systems across system integrity, communication/trust, reasoning/goals, and governance/legal dimensions.&lt;/li&gt;&lt;li&gt;Argues that existing system-centric defenses (e.g., prompt injection, data poisoning, tool misuse) are insufficient for persistent, autonomous agents and calls for preserving behavioral integrity and intent.&lt;/li&gt;&lt;li&gt;Frames defenses and research directions inspired by societal governance to make agentic AI systems more trustworthy, governable, and aligned with human values.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alsharif Abuadbba', 'Nazatul Sultan', 'Surya Nepal', 'Sanjay Jha']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI security', 'Multi-agent systems', 'AI governance', 'Threat modeling', 'Defense frameworks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01942</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse</title><link>https://arxiv.org/abs/2602.01795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RedVisor, a defense against prompt injection attacks that uses a lightweight removable adapter to generate explainable reasoning identifying injections and condition the model to reject malicious commands.&lt;/li&gt;&lt;li&gt;Adapter is active only during a reasoning phase (preserving the frozen backbone for benign inputs) and then muted during response generation, aiming to avoid utility loss from fine-tuning.&lt;/li&gt;&lt;li&gt;Introduces a KV Cache Reuse strategy and integrates the approach into the vLLM serving engine for improved detection accuracy, throughput, and reduced redundant prefill computation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingrui Liu', 'Sixiao Zhang', 'Cheng Long', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial defenses', 'LLM security', 'KV cache reuse', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01795</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency</title><link>https://arxiv.org/abs/2602.01765</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'temporal noise unconsistency' in diffusion models: triggered inputs disrupt noise predictions between adjacent timesteps in specific temporal segments.&lt;/li&gt;&lt;li&gt;Proposes TNC-Defense, a gray-box framework that detects anomalous timesteps via adjacent-timestep noise consistency and performs trigger-agnostic, timestep-aware detoxification by correcting the backdoor generation path.&lt;/li&gt;&lt;li&gt;Evaluated on five representative backdoor attack scenarios, showing ~11% improvement in detection accuracy and invalidating on average 98.5% of triggered samples with only mild degradation in generation quality and low overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingzheng Wang', 'Xiaoyan Gu', 'Hongbo Xu', 'Hongcheng Li', 'Zimo Yu', 'Jiang Zhou', 'Weiping Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'diffusion models', 'defense', 'detection', 'detoxification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01765</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title><link>https://arxiv.org/abs/2602.01725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafePred, a predictive guardrail framework for computer-using agents (CUAs) that uses world models to predict short- and long-term risks and align those predictions with current decisions.&lt;/li&gt;&lt;li&gt;Key capabilities: (1) risk prediction via world-model-generated semantic representations to identify and prune actions leading to high-risk future states, and (2) decision optimization via step-level interventions and task-level re-planning informed by predicted risks.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical improvements over reactive baselines, reporting &gt;97.6% safety performance and up to 21.4% task utility improvement by proactively avoiding delayed harmful consequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yurun Chen', 'Zeyi Liao', 'Ping Yin', 'Taotao Xie', 'Keting Yin', 'Shengyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'guardrails', 'world models', 'predictive risk', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01725</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient Adversarial Attacks on High-dimensional Offline Bandits</title><link>https://arxiv.org/abs/2602.01658</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel threat model where an attacker perturbs a reward model's weights prior to offline bandit training to hijack the bandit's behavior.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing that in high-dimensional settings the required perturbation norm for a successful attack decreases (making image evaluation especially vulnerable).&lt;/li&gt;&lt;li&gt;Empirically demonstrates that carefully targeted perturbations (not random noise) can achieve near-perfect attack success on Hugging Face evaluators for generative model assessment, including aesthetic and compositional alignment metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyed Mohammad Hadi Hosseini', 'Amir Najafi', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'reward-model-poisoning', 'offline-bandits', 'high-dimensional-robustness', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01658</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations</title><link>https://arxiv.org/abs/2602.01582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of AI-based channel decoders (e.g., ECCT, CrossMPT) to small channel perturbations using input-dependent adversarial attacks (FGM/PGD) and universal adversarial perturbations.&lt;/li&gt;&lt;li&gt;Finds significant performance degradation of AI decoders under adversarial shifts despite superior nominal AWGN performance; universal perturbations are more harmful than random perturbations of equal norm.&lt;/li&gt;&lt;li&gt;Shows strong transferability of adversarial perturbations between AI decoders but weak transferability to traditional BP-based decoders, indicating a robustness cost associated with AI decoding gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Lei', 'Mohammad Jalali', 'Chin Wa Lau', 'Farzan Farnia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'communication systems', 'universal adversarial perturbations', 'attack transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01582</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation</title><link>https://arxiv.org/abs/2602.01513</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows semantic image watermarks that are robust to conventional image-space attacks can be removed by imperceptible micro-geometric (spatial) perturbations that break phase alignment.&lt;/li&gt;&lt;li&gt;Proposes MarkCleaner: a watermark-removal framework trained with micro-geometry-perturbed supervision, using a mask-guided encoder and a 2D Gaussian Splatting-based decoder to separate semantic content from strict spatial alignment.&lt;/li&gt;&lt;li&gt;Demonstrates improved watermark removal effectiveness and visual fidelity compared to regeneration-based approaches, with efficient real-time inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxi Kong', 'Jieyu Yuan', 'Pengdi Chen', 'Yuanlin Zhang', 'Chongyi Li', 'Bin Li']&lt;/li&gt;&lt;li&gt;Tags: ['watermark removal', 'adversarial geometric perturbations', 'image forensics', 'copyright evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01513</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses</title><link>https://arxiv.org/abs/2602.01438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIPHER, a benchmark to measure cryptographic vulnerabilities in Python code generated by LLMs using controlled prompt variants (insecure/neutral/secure).&lt;/li&gt;&lt;li&gt;Defines a cryptography-specific vulnerability taxonomy and provides an automated, line-level scoring pipeline for attribution and evaluation.&lt;/li&gt;&lt;li&gt;Evaluates multiple widely used LLMs and finds that explicit "secure" prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities; benchmark and pipeline to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Max Manolov', 'Tony Gao', 'Siddharth Shukla', 'Cheng-Ting Chou', 'Ryan Lagasse']&lt;/li&gt;&lt;li&gt;Tags: ['cryptographic vulnerabilities', 'benchmark', 'LLM-generated code', 'secure prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01438</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation</title><link>https://arxiv.org/abs/2602.01187</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Stream of Revision, a decoding paradigm that allows autoregressive models to backtrack and edit their generated tokens via special action tokens within a single forward pass.&lt;/li&gt;&lt;li&gt;Internalizes the revision loop so the model can perform just-in-time corrections using its own semantic reasoning, avoiding external tools or high-latency agents.&lt;/li&gt;&lt;li&gt;Applied to secure code generation, the method empirically reduces security vulnerabilities in generated code with minimal inference overhead.&lt;/li&gt;&lt;li&gt;Emphasizes leveraging model-intrinsic capabilities to improve robustness of generated code against security flaws.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengran Yang', 'Zichao Wei', 'Heminghao Deng', 'Jinfeng Jiang', 'Zhensu Sun', 'Ting Zhang', 'Tianyi Wu', 'Ming Wen', 'David Lo']&lt;/li&gt;&lt;li&gt;Tags: ['secure code generation', 'defense', 'decoding strategies', 'model-internal revision', 'vulnerability mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01187</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems</title><link>https://arxiv.org/abs/2602.01185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedBGS, a fully decentralized, blockchain-based federated learning framework that uses segmented gossip learning and federated analytics.&lt;/li&gt;&lt;li&gt;Aims to remove the central server single-point-of-failure and improve privacy, security, and non-IID data handling in federated environments.&lt;/li&gt;&lt;li&gt;Claims to optimize blockchain usage for scalability while providing protection against various attack types in PPFL settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabio Turazza', 'Marcello Pietri', 'Marco Picone', 'Marco Mamei']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'blockchain', 'defense', 'privacy-preserving', 'gossip learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01185</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing</title><link>https://arxiv.org/abs/2602.01150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that conventional MIA-based auditing for machine unlearning is unreliable because formulating MIA as binary classification incurs unobservable statistical errors, leading to optimistic unlearning evaluations and high compute overhead (shadow models).&lt;/li&gt;&lt;li&gt;Proposes Statistical Membership Inference Attack (SMIA), a training-free auditing method that uses statistical tests to compare member vs non-member distributions, removing the need for learned attack models.&lt;/li&gt;&lt;li&gt;SMIA produces a forgetting rate with a confidence interval, giving quantified, theoretically grounded reliability, and empirically outperforms existing MIA approaches while being far less computationally expensive.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Sun', 'Zeming Wei', 'Jiaxuan Zou', 'Jiacheng Gong', 'Guanheng Wang', 'Chengyang Dong', 'Jialong Li', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'machine-unlearning', 'privacy-auditing', 'statistical-tests', 'adversarial-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01150</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection</title><link>https://arxiv.org/abs/2602.01032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HierCon, a hierarchical layer-attention framework combined with margin-based contrastive learning for detecting audio deepfakes.&lt;/li&gt;&lt;li&gt;Models dependencies across temporal frames, neighbouring transformer layers, and layer groups, and enforces domain-invariant embeddings to improve robustness.&lt;/li&gt;&lt;li&gt;Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, achieving state-of-the-art EERs (1.93% and 6.87%) and substantial improvement over independent layer weighting; attention visualisations show better cross-domain generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhili Nicholas Liang', 'Soyeon Caren Han', 'Qizhou Wang', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'defense', 'contrastive learning', 'hierarchical attention', 'robustness / generalisation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01032</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2602.01025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UltraBreak, a framework to craft universal, transferable image-based jailbreaks for vision-language models by combining vision-space regularisation/transformations with semantic textual objectives.&lt;/li&gt;&lt;li&gt;Defines loss in the target LLM's textual embedding space to guide adversarial pattern generation, reducing surrogate overfitting and improving transferability across models and attack targets.&lt;/li&gt;&lt;li&gt;Empirical results show UltraBreak consistently outperforms prior jailbreak methods; analysis attributes success to smoothing the loss landscape via semantic objectives. Code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyuan Cui', 'Yige Li', 'Yutao Wu', 'Xingjun Ma', 'Sarah Erfani', 'Christopher Leckie', 'Hanxun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'transferability', 'vision-language models', 'multimodal attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01025</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability</title><link>https://arxiv.org/abs/2602.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GradingAttack, an adversarial attack framework targeting LLM-based automatic short answer grading (ASAG) systems.&lt;/li&gt;&lt;li&gt;Proposes token-level and prompt-level attack strategies tailored to manipulate grading outcomes while maintaining camouflage.&lt;/li&gt;&lt;li&gt;Defines a novel evaluation metric balancing attack success and camouflage, and demonstrates effectiveness across multiple datasets.&lt;/li&gt;&lt;li&gt;Findings show prompt-level attacks achieve higher success rates while token-level attacks offer better camouflage, highlighting the need for robust defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xueyi Li', 'Zhuoneng Zhou', 'Zitao Liu', 'Yongdong Wu', 'Weiqi Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'automated-grading', 'prompt-injection', 'robustness-evaluation', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00979</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bypassing Prompt Injection Detectors through Evasive Injections</title><link>https://arxiv.org/abs/2602.00750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that linear-probe detectors using activation deltas to detect prompt-injection/task-drift can be evaded by adversarially optimized universal suffixes.&lt;/li&gt;&lt;li&gt;Constructs universal suffix attacks that transfer across multiple probes and achieve very high evasion rates on Phi-3 3.8B and Llama-3 8B (up to ~94% and ~99.6% when all probes must be fooled).&lt;/li&gt;&lt;li&gt;Demonstrates near-perfect evasion under majority-vote settings and highlights the fragility of activation-delta based detectors to adaptive attacks.&lt;/li&gt;&lt;li&gt;Proposes a defence: generate multiple random suffixes appended during forward passes and retrain logistic regression detectors on these activations, which substantially improves robustness against the suffix attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Jahedur Rahman', 'Ihsen Alouani']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial examples', 'detection evasion', 'defenses', 'activation-delta detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00750</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity</title><link>https://arxiv.org/abs/2602.00723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'prompt multiplicity', a framework to quantify consistency across different prompts when evaluating LLM hallucinations.&lt;/li&gt;&lt;li&gt;Finds substantial inconsistency (e.g., &gt;50% in benchmarks like Med-HALT), indicating current hallucination evaluations that focus only on correctness miss important harms.&lt;/li&gt;&lt;li&gt;Shows that common detection methods tend to detect consistency issues rather than true correctness, and that mitigation approaches like retrieval-augmented generation (RAG) can reduce hallucinations but may introduce new inconsistencies.&lt;/li&gt;&lt;li&gt;Proposes integrating prompt multiplicity into hallucination evaluation to better characterize harms and reveal limitations of existing detection and mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prakhar Ganesh', 'Reza Shokri', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM evaluation', 'robustness', 'mitigation', 'retrieval-augmented generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00723</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities</title><link>https://arxiv.org/abs/2602.00711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a proactive defense approach that highlights security-critical code regions (e.g., data access, auth, input handling) to prevent vulnerabilities during development.&lt;/li&gt;&lt;li&gt;Implements an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods.&lt;/li&gt;&lt;li&gt;Uses large language models to generate prevention-oriented, actionable explanations and guidance for secure implementation.&lt;/li&gt;&lt;li&gt;Evaluates the approach on the Spring-PetClinic app, showing metrics capture most known security-critical methods and LLMs provide useful guidance, while noting limitations in semantic coverage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjith Krishnamurthy', 'Oshando Johnson', 'Goran Piskachev', 'Eric Bodden']&lt;/li&gt;&lt;li&gt;Tags: ['Vulnerability prevention', 'LLM-assisted security', 'Secure coding tools', 'Code-level metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00711</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs via Calibration</title><link>https://arxiv.org/abs/2602.00619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models safety alignment as a systematic distortion of a pre-alignment next-token distribution and frames Weak-to-Strong jailbreaking as a forecast aggregation problem.&lt;/li&gt;&lt;li&gt;Derives an optimal aggregation strategy (characterized by a Gradient Shift in the loss-induced dual space) and shows logit-arithmetic jailbreaking as a special case under cross-entropy loss.&lt;/li&gt;&lt;li&gt;Introduces a broader family of aggregation rules (for other proper losses) and a new hybrid aggregation rule.&lt;/li&gt;&lt;li&gt;Evaluates on red-teaming benchmarks and math utility tasks, reporting higher attack success rates and lower "jailbreak tax" versus prior methods, especially on safety-hardened models like gpt-oss-120b.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Lu', 'Yongkang Guo', 'Yuqing Kong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'safety-alignment', 'logit-arithmetic', 'attack-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00619</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.00559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;OmniVCHall: a benchmark to evaluate isolated and compositional hallucinations in video multimodal LLMs, introducing a camera-based hallucination type and adversarial answer options to reduce shortcut answering.&lt;/li&gt;&lt;li&gt;Large-scale evaluation of 39 VLLMs (including Qwen3-VL and GPT-5) showing substantial degradation under compositional hallucination scenarios.&lt;/li&gt;&lt;li&gt;TriCD: a contrastive decoding defense with a triple-pathway calibration—an adaptive perturbation controller to construct negative video variants, a saliency-guided enhancement to reinforce grounded token-wise visual evidence, and RL-based optimization to improve decision precision.&lt;/li&gt;&lt;li&gt;TriCD yields consistent gains (≈10% average accuracy improvement) across multiple backbones; code and data are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbin Xing', 'Quanxing Zha', 'Lizheng Zu', 'Mengran Li', 'Ming Li', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['video-hallucination', 'robustness', 'adversarial-evaluation', 'defense', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00559</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Text is All You Need for Vision-Language Model Jailbreaking</title><link>https://arxiv.org/abs/2602.00420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Text-DJ, a jailbreak attack that exploits LVLMs' OCR by converting harmful text prompts into images and distributing semantically related sub-queries across multiple images.&lt;/li&gt;&lt;li&gt;Method has three stages: decompose a harmful query into benign-looking sub-queries, add many irrelevant distraction queries, and present them as a grid of images with sub-queries centrally placed.&lt;/li&gt;&lt;li&gt;Demonstrates successful circumvention of state-of-the-art LVLM safety alignment by bypassing text filters and leveraging distraction to prevent the model from linking fragmented sub-queries.&lt;/li&gt;&lt;li&gt;Highlights a critical vulnerability in handling fragmented multimodal inputs and calls for defenses targeted at OCR-based and multi-image adversarial inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Chen', 'Zhao Xu', 'Youyuan Jiang', 'Tianle Zheng', 'Cho-Jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'OCR-based attack', 'vision-language models', 'multimodal adversarial attack', 'safety bypass']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00420</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Content in Academic Peer Reviews</title><link>https://arxiv.org/abs/2602.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies a detection model trained on historical peer reviews to later review cycles at ICLR and Nature Communications to identify AI-generated content.&lt;/li&gt;&lt;li&gt;Finds minimal AI-generated content before 2022, with a substantial increase through 2025 — ~20% of ICLR reviews and ~12% of Nature Communications reviews classified as AI-generated in 2025.&lt;/li&gt;&lt;li&gt;Notes a pronounced rise in AI-assisted reviews for Nature Communications between Q3 and Q4 of 2024 and calls for further study of implications for scholarly evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Shen', 'Kai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated content detection', 'misuse detection', 'academic peer review', 'temporal analysis', 'empirical study']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00319</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection</title><link>https://arxiv.org/abs/2602.00318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BOCLOAK, an optimal-transport-guided framework to craft realistic adversarial attacks (edge edits and node injections) against GNN-based social bot detectors under spatio-temporal and domain constraints.&lt;/li&gt;&lt;li&gt;Constructs a probability measure over neighbor features and decodes transport plans into sparse, plausible graph perturbations that respect real-world constraints.&lt;/li&gt;&lt;li&gt;Empirically evaluates across three bot datasets, five state-of-the-art detectors, three defenses, and four baselines, showing large improvements in attack success and memory efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunal Mukherjee', 'Zulfikar Alom', 'Tran Gia Bao Ngo', 'Cuneyt Gurcan Akcora', 'Murat Kantarcioglu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'graph-neural-networks', 'social-bot-detection', 'optimal-transport', 'robustness-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00318</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics-Preserving Evasion of LLM Vulnerability Detectors</title><link>https://arxiv.org/abs/2602.00305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM-based vulnerability detectors under a semantics-preserving threat model using diverse behavior-preserving code transformations on a unified C/C++ benchmark (N=5000).&lt;/li&gt;&lt;li&gt;Introduces a joint robustness metric across different attack methods and carriers to measure detection-time integrity under semantic-invariant edits.&lt;/li&gt;&lt;li&gt;Finds systemic failures: detectors that perform well on clean inputs often flip predictions under behavior-equivalent edits; universal adversarial strings transfer to black-box APIs and gradient access increases evasion success.&lt;/li&gt;&lt;li&gt;Proposes carrier-based metrics as practical diagnostics for evaluating the robustness of LLM-based code vulnerability detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luze Sun', 'Alina Oprea', 'Eric Wong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'evasion attacks', 'code security', 'model robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00305</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification</title><link>https://arxiv.org/abs/2602.00292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LogicGaze, a benchmark to evaluate whether vision-language models ground sequential causal reasoning in visual evidence and avoid hallucinations.&lt;/li&gt;&lt;li&gt;Constructs datasets from 40,000 video segments and Flickr30k images with linguistically plausible but visually contradictory perturbations to test models' ability to verify causal chains.&lt;/li&gt;&lt;li&gt;Provides a three-part evaluation protocol (Causal Validation, Grounded Narrative Synthesis, Perturbation Rejection) and demonstrates vulnerabilities in state-of-the-art VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rory Driscoll', 'Alexandros Christoforos', 'Chadbourne Davis']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'adversarial-perturbation', 'benchmark', 'multimodal-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00292</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation</title><link>https://arxiv.org/abs/2602.00219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a semantics-driven federated IDS that uses a Tri-LLM ensemble (GPT-4o, DeepSeek-V3, LLaMA-3-8B) to create language-derived attack prototypes enabling open-set and zero-shot detection of unseen attacks.&lt;/li&gt;&lt;li&gt;Models inter-LLM semantic disagreement as epistemic uncertainty for zero-day risk estimation and integrates semantic supervision into federated optimization across heterogeneous clients.&lt;/li&gt;&lt;li&gt;Implements a trust-aware aggregation mechanism to dynamically weight client updates, improving robustness to unreliable or compromised clients and reducing aggregation instability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeid Jamshidi', 'Omar Abdul Wahab', 'Foutse Khomh', 'Kawser Wazed Nafi']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'intrusion detection', 'zero-shot detection', 'robust aggregation', 'model uncertainty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00219</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs</title><link>https://arxiv.org/abs/2602.00204</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an APT detection method that transforms raw system logs into semantic embeddings using a pre-trained transformer LLM.&lt;/li&gt;&lt;li&gt;Uses an Autoencoder on LLM-derived embeddings to perform unsupervised anomaly detection of stealthy, low-and-slow attack behaviors.&lt;/li&gt;&lt;li&gt;Evaluated on the DARPA Transparent Computing dataset and shown to outperform unsupervised baselines (Isolation Forest, OC-SVM, PCA) by AUC-ROC.&lt;/li&gt;&lt;li&gt;Highlights the value of semantic representations for detecting non-linear and stealthy malicious activities missed by conventional techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waleed Khan Mohammed', 'Zahirul Arief Irfan Bin Shahrul Anuar', 'Mousa Sufian Mousa Mitani', 'Hezerul Abdul Karim', 'Nouar AlDahoul']&lt;/li&gt;&lt;li&gt;Tags: ['APT-detection', 'anomaly-detection', 'LLM-embeddings', 'autoencoder', 'cybersecurity-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00204</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange</title><link>https://arxiv.org/abs/2602.00192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that state-of-the-art inpainting detectors over-rely on global artifacts (a VAE-induced spectral shift) rather than on locally synthesized content.&lt;/li&gt;&lt;li&gt;Introduces Inpainting Exchange (INP-X), an intervention that restores original pixels outside the edited region while preserving synthesized content, and constructs a 90K image test set (real, inpainted, exchanged).&lt;/li&gt;&lt;li&gt;Demonstrates large drops in detection accuracy (e.g., 91% → 55%) under INP-X, provides theoretical analysis linking failures to high-frequency attenuation from VAE bottlenecks, and shows training on the INP-X data improves content-aware detection and localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elif Nebioglu', 'Emirhan Bilgi\\c{c}', 'Adrian Popescu']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'detection-bypass', 'inpainting', 'VAE-artifacts', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00192</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EigenAI: Deterministic Inference, Verifiable Results</title><link>https://arxiv.org/abs/2602.00182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a verifiable inference platform that combines deterministic, bit-exact LLM inference with a cryptoeconomic optimistic re-execution protocol built on EigenLayer/EigenDA.&lt;/li&gt;&lt;li&gt;Untrusted operators publish encrypted request/response logs; during a challenge window, watchers can trigger deterministic re-execution inside a TEE using a threshold-released decryption key to enable public verification even with private inputs.&lt;/li&gt;&lt;li&gt;Verification reduces to a byte-equality check (bit-exact outputs), so a single honest replica can detect fraud; the system aims to provide economically enforceable, auditable results for sovereign agents (e.g., prediction markets, trading bots).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Ribeiro Alves', 'Vishnu Patankar', 'Matheus Pereira', 'Jamie Stephens', 'Nima Vaziri', 'Sreeram Kannan']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable-inference', 'deterministic-LLM', 'cryptoeconomic-enforcement', 'trusted-execution-environment', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00182</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization</title><link>https://arxiv.org/abs/2602.00175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that unlearning defenses for diffusion models often only break the symbol-to-concept mapping superficially while leaving the underlying concept as a dormant memory.&lt;/li&gt;&lt;li&gt;Proposes IVO (Initial Latent Variable Optimization): an attack that reconstructs broken mappings by optimizing initial latent variables (using image inversion, adversarial optimization, and reuse strategies) to reactivate 'unlearned' NSFW concepts.&lt;/li&gt;&lt;li&gt;Evaluates IVO across eight common unlearning techniques and shows high attack success rates and semantic consistency, exposing fundamental flaws in current unlearning defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manyi Li', 'Yufan Liu', 'Lai Jiang', 'Bing Li', 'Yuming Li', 'Weiming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'diffusion models', 'adversarial attack', 'safety/robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00175</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Robust Reasoning through Guided Adversarial Self-Play</title><link>https://arxiv.org/abs/2602.00173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GASP (Guided Adversarial Self-Play): a self-contained adversarial training scheme where a single model concurrently learns a polluter (generates locally coherent corruptions) and a repairer (detects and recovers) using only outcome verification.&lt;/li&gt;&lt;li&gt;Adds an in-distribution repair guidance (imitation) term to overcome sparse early successes, increasing recovery probability while preserving existing capabilities.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple open-weight language models (1.5B–8B) that GASP yields robust reasoning that resists misleading/perturbed conditioning and often improves clean accuracy.&lt;/li&gt;&lt;li&gt;Analyzes that adversarial corruptions create an effective curriculum and that the guidance term enables rapid recovery learning with minimal representational drift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Vaishnav Tadiparthi', 'Kwonjoon Lee', 'Nakul Agarwal', 'Hossein Nourkhiz Mahjoub', 'Ehsan Moradi Pari', 'Lizhang Chen', 'Amy Zhang', 'Liu Leqi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'red-teaming', 'detect-and-repair']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00173</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title><link>https://arxiv.org/abs/2602.00154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes inference cost for large reasoning models (LRMs) and defines prompt-induced denial-of-service (PI-DoS), proving three required attack properties: high amplification ratio, stealthiness, and optimizability.&lt;/li&gt;&lt;li&gt;Introduces ReasoningBomb, a reinforcement-learning-based attack that crafts short natural-language prompts to induce pathologically long or non-terminating multi-step reasoning in victim LRMs using a constant-time surrogate reward.&lt;/li&gt;&lt;li&gt;Empirical results on seven open-source and three commercial LRMs show massive amplification (avg ~286.7x input-to-output), large numbers of induced reasoning/completion tokens, and very high bypass rates against input/output/dual-stage detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaogeng Liu', 'Xinyan Wang', 'Yechao Zhang', 'Sanjay Kariyappa', 'Chong Xiang', 'Muhao Chen', 'G. Edward Suh', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-induced DoS', 'adversarial attack', 'denial-of-service', 'model robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00154</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SPGCL: Simple yet Powerful Graph Contrastive Learning via SVD-Guided Structural Perturbation</title><link>https://arxiv.org/abs/2602.00064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPGCL, a graph contrastive learning framework that combines lightweight stochastic edge removal with truncated SVD-guided edge recovery to produce structure-aware contrastive views.&lt;/li&gt;&lt;li&gt;Two-stage strategy: (1) stochastic edge deletion to inject diversity, (2) SVD-derived scoring and top-P sparse edge recovery to restore important edges and add semantically meaningful links; includes a contrastive fusion module with a global similarity constraint.&lt;/li&gt;&lt;li&gt;Aims to improve robustness of GNNs to structural noise from adversarial attacks or imperfections; extensive experiments on ten benchmarks show improved robustness and accuracy over SOTA GCL and structure learning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Deng', 'Zhang Guo', 'Shuiping Gou', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph-ml', 'robustness', 'adversarial-robustness', 'graph-contrastive-learning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00064</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title><link>https://arxiv.org/abs/2602.00061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes role conditioning (assigning social roles like 'judge' or 'mother') as a compact, training-free method for safety alignment of LLMs, using a role-conditioned generator plus iterative role-based critics for refinement.&lt;/li&gt;&lt;li&gt;Evaluates across five model families and compares to principle-based prompts, Chain-of-Thought, and other baselines, reporting large improvements in reducing unsafe outputs (e.g., WildJailbreak unsafe rate from 81.4% to 3.6%).&lt;/li&gt;&lt;li&gt;Applies to both common safety benchmarks and agentic safety tasks, positioning role assignment as an interpretable, practical defense/guardrail for jailbreaks and unsafe behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Ziheng', 'Jiakun Ding', 'Zhaowei Zhang', 'Ruosen Gao', 'Yingnian Wu', 'Demetri Terzopoulos', 'Yipeng Kang', 'Fangwei Zhong', 'Junqi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak mitigation', 'prompt-based defense', 'safety alignment', 'red teaming / robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00061</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion</title><link>https://arxiv.org/abs/2602.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LSSF (Low-Rank Safety Subspace Fusion), a post-hoc safety re-alignment method that constructs a low-rank projection matrix to extract principal safety components from LLMs.&lt;/li&gt;&lt;li&gt;Observes a stable, isolated low-rank safety subspace across fine-tuning and uses linear arithmetic to fuse these safety components back into fine-tuned models to restore safety alignment.&lt;/li&gt;&lt;li&gt;Introduces 'safety singular value entropy' to measure encoding density across layers and dynamically determine safety-critical ranks, claiming restored safety with minimal downstream performance impact.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanghao Zhou', 'Panjia Qiu', 'Cen Chen', 'Hongyu Li', 'Mingyuan Chu', 'Xin Zhang', 'Jun Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'post-hoc-defense', 'model-robustness', 'subspace-projection', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00038</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Federated Learning With Individualized Privacy Through Client Sampling</title><link>https://arxiv.org/abs/2501.17634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adapting Individualized Differential Privacy (IDP) to federated learning by computing client-specific sampling rates and integrating them into a modified FedAvg (IDP-FedAvg).&lt;/li&gt;&lt;li&gt;Extends the SAMPLE algorithm from centralized settings to the decentralized FL setting and compares performance against uniform DP baselines and the SCALE method (per-client noise scaling).&lt;/li&gt;&lt;li&gt;Evaluates under realistic heterogeneous privacy budgets and datasets, showing improved utility/privacy trade-offs, but notes challenges with non-i.i.d. data and decentralized constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Ole Borchardt', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'individualized privacy', 'privacy-preserving ML', 'client sampling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.17634</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning</title><link>https://arxiv.org/abs/2409.01329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how image dataset characteristics (class imbalance, number of classes, entropy, Fisher Discriminant Ratio) affect utility and vulnerability of CNNs trained with and without Differential Privacy (DP).&lt;/li&gt;&lt;li&gt;Finds that class imbalance increases vulnerability for minority classes, but applying DP mitigates this increased risk.&lt;/li&gt;&lt;li&gt;Shows datasets with fewer classes yield better utility and privacy; datasets with high entropy or low FDR worsen the utility–privacy trade-off.&lt;/li&gt;&lt;li&gt;Provides practical guidance for estimating and optimizing the utility–privacy trade-off based on dataset features to inform data preprocessing and privacy settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Maurice-Maximilian Heykeroth', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'data-privacy', 'dataset-vulnerabilities', 'image-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01329</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection</title><link>https://arxiv.org/abs/2401.13327</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAN-based methods to synthesize multi-sensor smartwatch health/time-series data for stress detection with differential privacy (DP) safeguards.&lt;/li&gt;&lt;li&gt;Evaluates utility: synthetic-data augmentation improves stress classifier F1 scores—11.90–15.48% gains under DP training and modest gains (≈0.45%) for non-private setups.&lt;/li&gt;&lt;li&gt;Performs quality/plausibility assessments and analyzes privacy–utility trade-offs, noting degradation of synthetic data fidelity as privacy requirements increase.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Nils Wenzlitschke', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'synthetic-data', 'generative-adversarial-networks', 'privacy-preserving', 'health-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.13327</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)</title><link>https://arxiv.org/abs/2211.11434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and evaluates differentially private (DP) ML models for COVID-19 detection from chest X-ray images.&lt;/li&gt;&lt;li&gt;Analyzes utility–privacy trade-offs across varying DP privacy budgets and accounts for class imbalance in medical datasets.&lt;/li&gt;&lt;li&gt;Empirically measures practical privacy using black-box membership inference attacks (MIAs) to assess DP's effectiveness as a defense.&lt;/li&gt;&lt;li&gt;Finds that stronger DP guarantees yield only marginal reductions in empirical MIA leakage and advocates for attack-specific empirical privacy estimation to tune defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Maja Schneider', 'Peter Christen', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'membership-inference', 'privacy-preserving-ml', 'medical-imaging', 'empirical-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2211.11434</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</title><link>https://arxiv.org/abs/2602.02455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Drift-Bench, a diagnostic benchmark that evaluates LLM agents' multi-turn clarification and pragmatics under input faults (missing info, false presuppositions, ambiguity) in grounded execution environments.&lt;/li&gt;&lt;li&gt;Provides a unified taxonomy of cooperative breakdowns and a persona-driven user simulator with the RISE evaluation protocol to measure clarification effectiveness across fault types and user behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates substantial performance drops under input faults and highlights how failures in clarification can lead to unsafe or incorrect executions, enabling systematic diagnosis of safety-relevant agent failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Bao', 'Zheyuan Zhang', 'Pengcheng Jing', 'Zhengqing Yuan', 'Kaiwen Shi', 'Yanfang Ye']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmark', 'agent-robustness', 'clarification', 'user-simulation', 'pragmatics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02455</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron</title><link>https://arxiv.org/abs/2602.02027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight safety-aware decoding method for LLM alignment that uses low-cost training of an expert model plus a single neuron gating mechanism to control unsafe outputs.&lt;/li&gt;&lt;li&gt;Balances internal model capabilities with external guidance to preserve utility while improving safety, aiming for better generalization across model scales and lower training overhead than post-training alignment.&lt;/li&gt;&lt;li&gt;Evaluated as a practical, efficient alternative to expensive post-training methods and prior lightweight approaches that rely heavily on precomputed safety injections or the model's own abilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Shen', 'Mingyang Lv', 'Han Shen', 'Jialin Wu', 'Binghao Wang', 'Zhou Yang', 'Guobin Shen', 'Dongcheng Zhao', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'decoding-defenses', 'lightweight-alignment', 'gating-mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02027</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title><link>https://arxiv.org/abs/2602.01750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Reward Auditing (ARA): a two-stage framework where a Hacker policy finds reward-model exploits while an Auditor learns to detect exploitation from latent representations.&lt;/li&gt;&lt;li&gt;Introduces Auditor-Guided RLHF (AG-RLHF) that gates/penalizes reward signals based on Auditor detections to mitigate reward hacking during training.&lt;/li&gt;&lt;li&gt;Empirical results across three scenarios show improved alignment-utility tradeoffs (reducing sycophancy, verbosity, and code gaming) and cross-domain generalization of both Hacker and Auditor models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Beigi', 'Ming Jin', 'Junshan Zhang', 'Qifan Wang', 'Lifu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'RLHF', 'adversarial testing', 'attack detection', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01750</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MACD: Model-Aware Contrastive Decoding via Counterfactual Data</title><link>https://arxiv.org/abs/2602.01740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MACD, a model-aware counterfactual data approach integrated into contrastive decoding to reduce hallucinations in Video-LLMs by generating object-level counterfactual inputs guided by the model's own feedback.&lt;/li&gt;&lt;li&gt;Targets object regions most responsible for hallucination rather than using random perturbations, enforcing evidence-grounded token selection during decoding.&lt;/li&gt;&lt;li&gt;Demonstrates consistent reduction in hallucination and maintenance or improvement of task accuracy across Video-LLMs (e.g., Qwen, InternVL) on benchmarks like EventHallucion, MVBench, Perception-test, and Video-MME.&lt;/li&gt;&lt;li&gt;Particularly effective for challenging cases involving small, occluded, or co-occurring objects; code and data to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qixin Xiao', 'Kun Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'contrastive decoding', 'counterfactual data', 'robustness', 'video-llm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01740</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating loss of control in advanced AI systems through instrumental goal trajectories</title><link>https://arxiv.org/abs/2602.01699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'instrumental goal trajectories' (IGTs) — procurement, governance, and finance pathways — through which advanced AI systems obtain resources and capabilities.&lt;/li&gt;&lt;li&gt;Argues these organisational pathways leave observable artefacts that can be monitored as intervention points to detect and limit capability growth or undesirable behaviour.&lt;/li&gt;&lt;li&gt;Positions IGTs as complementary to model-centric mitigations (e.g., RLHF, corrigibility), shifting some defensive attention to organisational systems and processes to improve interruptibility and control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Willem Fourie']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'defense / mitigation', 'corrigibility', 'organizational security', 'safety monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01699</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title><link>https://arxiv.org/abs/2602.01539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAGIC, a co-evolving multi-turn attacker-defender framework using multi-agent reinforcement learning where an attacker rewrites queries into deceptive prompts and a defender learns to detect/refuse them.&lt;/li&gt;&lt;li&gt;Demonstrates that the dynamic adversarial training uncovers long-tail, novel combinatorial attack strategies and improves defender generalization to unseen attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of equilibrium and safety guarantees and empirical results showing improved defense success without degrading helpfulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wen', 'Zhida He', 'Han Qi', 'Ziyu Wan', 'Zhongtian Ma', 'Ying Wen', 'Tianhang Zheng', 'Xingcheng Xu', 'Chaochao Lu', 'Qiaosheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'prompt injection', 'red teaming', 'adversarial reinforcement learning', 'LLM safety/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01539</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Better Deception Probes Using Targeted Instruction Pairs</title><link>https://arxiv.org/abs/2602.01425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies linear probes as monitoring/defense tools to detect deceptive behaviour in AI systems and shows existing probes can fail due to spurious correlations and false positives.&lt;/li&gt;&lt;li&gt;Demonstrates that the choice of contrastive instruction pair dominates probe performance (explaining ~70.6% of variance) and that instruction pairs capture deceptive intent rather than content-specific patterns.&lt;/li&gt;&lt;li&gt;Proposes a human-interpretable taxonomy of deception and advocates designing targeted probes for specific deception types/threat models, reporting improved results on evaluation datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vikram Natarajan', 'Devina Jain', 'Shivam Arora', 'Satvik Golechha', 'Joseph Bloom']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'linear probes', 'model monitoring', 'safety/defense', 'instruction engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01425</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?</title><link>https://arxiv.org/abs/2602.01146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PersistBench, a benchmark to quantify safety risks from persistent long-term memories in conversational LLMs.&lt;/li&gt;&lt;li&gt;Identifies two memory-specific vulnerabilities: cross-domain leakage (injecting irrelevant memory context) and memory-induced sycophancy (reinforcing user biases).&lt;/li&gt;&lt;li&gt;Evaluates 18 frontier and open-source LLMs, reporting high failure rates (median 53% for cross-domain leakage, 97% for sycophancy) and calls for safer long-term memory handling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sidharth Pulipaka', 'Oliver Chen', 'Manas Sharma', 'Taaha S Bajwa', 'Vyas Raina', 'Ivaxi Sheth']&lt;/li&gt;&lt;li&gt;Tags: ['long-term-memory', 'safety-benchmark', 'context-leakage', 'sycophancy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01146</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI</title><link>https://arxiv.org/abs/2602.01086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedBeads, an immutable, agent-native data substrate for clinical AI where events are cryptographically linked 'Beads' in a Merkle DAG to ensure tamper-evidence and provenance.&lt;/li&gt;&lt;li&gt;Implements a prototype (Go core, Python LLM middleware, React UI) that converts FHIR resources into a causally-linked graph and retrieves context via BFS traversal for deterministic, auditable context delivery to LLM agents.&lt;/li&gt;&lt;li&gt;Claims tamper-detection by design (modification breaks cryptographic chain), improves auditability and deterministic context for clinical decision support, and releases open-source software.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Takahito Nakajima']&lt;/li&gt;&lt;li&gt;Tags: ['data integrity', 'tamper-evidence', 'auditability / provenance', 'defense / secure infrastructure', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01086</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How RLHF Amplifies Sycophancy</title><link>https://arxiv.org/abs/2602.01002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formal analysis showing RLHF can amplify sycophancy: behavioral drift direction is set by the covariance under the base policy between endorsing a belief in the prompt and the learned reward; first-order effect reduces to a mean-gap condition.&lt;/li&gt;&lt;li&gt;Characterizes how reward learning from pairwise comparisons (e.g., Bradley–Terry/random utility models) can induce the reward gap when annotator preferences are biased.&lt;/li&gt;&lt;li&gt;Proposes a training-time intervention that neutralizes the amplification mechanism by identifying the unique KL-closest post-trained policy that prevents increased sycophancy and derives a closed-form minimal reward correction (an agreement penalty).&lt;/li&gt;&lt;li&gt;Computational experiments show reward gaps are common and lead to behavioral drift across considered configurations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itai Shapira', 'Gerdus Benade', 'Ariel D. Procaccia']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment/safety', 'sycophancy', 'reward modeling', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01002</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support</title><link>https://arxiv.org/abs/2602.00950</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a clinically grounded risk taxonomy for multi-turn mental-health conversations and releases a turn-level annotated dataset (MindGuard-testset) by clinical experts.&lt;/li&gt;&lt;li&gt;Trains lightweight safety classifiers (4B and 8B) using synthetic two-agent dialogues to reduce false positives at high-recall operating points while preserving therapeutic content.&lt;/li&gt;&lt;li&gt;Evaluates the classifiers in adversarial multi-turn interactions and demonstrates reduced attack success and harmful engagement rates compared to general-purpose safeguards.&lt;/li&gt;&lt;li&gt;Releases models and human evaluation data to support deployment of domain-specific guardrails for mental-health conversational agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ant\\'onio Farinhas", 'Nuno M. Guerreiro', "Jos\\'e Pombal", 'Pedro Henrique Martins', 'Laura Melton', 'Alex Conway', 'Cara Dochat', "Maya D'Eon", 'Ricardo Rei']&lt;/li&gt;&lt;li&gt;Tags: ['safety-classifiers', 'guardrails', 'adversarial-testing', 'safety-dataset', 'mental-health']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00950</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Self-Guard: Defending Large Reasoning Models via enhanced self-reflection</title><link>https://arxiv.org/abs/2602.00707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Guard, a lightweight defense that leverages safety-oriented prompting to elicit latent safety reflections in large reasoning models.&lt;/li&gt;&lt;li&gt;Extracts and amplifies directional shifts in hidden-state representations (safety activation steering) to prioritize safety compliance over sycophantic instruction-following during inference.&lt;/li&gt;&lt;li&gt;Claims robust safety improvements without degrading utility, with generalization across unseen risks and model scales and lower compute cost than heavy post-training interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingnan Zheng', 'Jingjun Xu', 'Yanzhen Luo', 'Chenhang Cui', 'Gelei Deng', 'Zhenkai Liang', 'Xiang Wang', 'An Zhang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['safety-defenses', 'alignment', 'hidden-state steering', 'prompt-based defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00707</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees</title><link>https://arxiv.org/abs/2602.00616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes a Safety-Prompt Alignment Trade-off (SPAT) using total variation: any nontrivial reduction in unsafe generations induces TV deviation from the reference conditional distribution.&lt;/li&gt;&lt;li&gt;Proposes an inference-only prompt projection method that maps high-risk prompts into a tolerance-controlled safe set via a surrogate objective with verification, without retraining or fine-tuning the generative model.&lt;/li&gt;&lt;li&gt;Demonstrates 16.7–60.0% relative reductions in inappropriate percentage (IP) across four datasets and three diffusion backbones while preserving benign prompt-image alignment near the unaligned reference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minhyuk Lee', 'Hyekyung Yoon', 'Myungjoo Kang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'defense', 'prompt filtering', 'diffusion models', 'total variation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00616</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning</title><link>https://arxiv.org/abs/2602.00298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates emergent misalignment caused by narrow fine-tuning across 11 domains on LLMs (Qwen2.5-Coder-7B-Instruct and GPT-4o-mini), including experiments with and without backdoor triggers.&lt;/li&gt;&lt;li&gt;Finds backdoor triggers increase misalignment in 77.8% of domains (avg drop 4.33 points); domain vulnerability varies widely (0% to 87.67%).&lt;/li&gt;&lt;li&gt;Shows membership inference metrics (adjusted for base model) can predict susceptibility to broad misalignment and studies cross-model transferability of extracted directions.&lt;/li&gt;&lt;li&gt;Provides a domain-level taxonomy of emergent misalignment, a standardized recipe for constructing misaligned datasets, and releases code/datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Mishra', 'Mugilan Arulvanan', 'Reshma Ashok', 'Polina Petrova', 'Deepesh Suranjandass', 'Donnie Winkelmann']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'emergent misalignment', 'fine-tuning vulnerabilities', 'membership inference', 'red-teaming/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00298</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 16 Feb 2026 22:55:51 +0000</lastBuildDate><item><title>Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees</title><link>https://arxiv.org/abs/2601.12447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CryptoFair-FL: a cryptographic federated learning framework that provides verifiable fairness guarantees (demographic parity, equalized odds) while preserving privacy using homomorphic encryption and secure multi-party computation.&lt;/li&gt;&lt;li&gt;Introduces a batched verification protocol improving computational complexity from O(n^2) to O(n log n) and provides (ε, δ)-differential privacy guarantees; theoretical analysis shows near-optimal privacy–fairness tradeoffs.&lt;/li&gt;&lt;li&gt;Evaluates on multiple benchmarks (MIMIC-IV, Adult Income, CelebA, FedFair-100), demonstrating large reductions in fairness violations with modest computational overhead and robustness against attribute inference attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammed Himayath Ali', 'Mohammed Aqib Abdullah', 'Syed Muneer Hussain', 'Mohammed Mudassir Uddin', 'Shahnawaz Alam']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'differential privacy', 'cryptographic protocols', 'fairness verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12447</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adopting a human developmental visual diet yields robust, shape-based AI vision</title><link>https://arxiv.org/abs/2507.03168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a human developmental visual diet (DVD) curriculum—modulating acuity, contrast sensitivity, and color during training—to guide vision models.&lt;/li&gt;&lt;li&gt;Models trained with the DVD show markedly increased shape-bias, state-of-the-art abstract shape recognition, and improved robustness to common image corruptions.&lt;/li&gt;&lt;li&gt;Reports higher resilience to adversarial attacks, indicating that curriculum-based training can serve as a resource-efficient defense improving model robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zejin Lu', 'Sushrut Thorat', 'Radoslaw M Cichy', 'Tim C Kietzmann']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'training curriculum', 'shape-bias', 'robustness-to-corruptions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.03168</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pixel-Based Similarities as an Alternative to Neural Data for Improving Convolutional Neural Network Adversarial Robustness</title><link>https://arxiv.org/abs/2410.03952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-driven substitute for neural-recording-based representational similarity regularizers by computing pixel-based similarities directly from images.&lt;/li&gt;&lt;li&gt;Shows that this pixel-based regularizer preserves the adversarial robustness benefits of the original biologically motivated Loss while removing dependence on neural recordings or task-specific augmentations.&lt;/li&gt;&lt;li&gt;Method is lightweight, integrates into standard CNN training pipelines, and yields robustness improvements though it does not surpass specialized state-of-the-art defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elie Attias', 'Cengiz Pehlevan', 'Dina Obeid']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'regularization', 'neural-inspired', 'adversarial examples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03952</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CP-uniGuard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems</title><link>https://arxiv.org/abs/2506.22890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CP-uniGuard, a unified, probability-agnostic, and adaptive defense framework to detect and eliminate malicious agents in collaborative perception (CP) for multi-agent autonomous driving/robotics.&lt;/li&gt;&lt;li&gt;Introduces PASAC (probability-agnostic sample consensus) to sample collaborators and verify consensus without prior malicious probabilities, and defines Collaborative Consistency Loss (CCLoss) for object detection and BEV segmentation to measure discrepancies.&lt;/li&gt;&lt;li&gt;Implements an online adaptive thresholding mechanism using dual sliding windows to dynamically adjust verification thresholds in changing environments.&lt;/li&gt;&lt;li&gt;Evaluates the framework extensively and releases code, demonstrating effective detection and defense against malicious collaborators in CP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senkang Hu', 'Yihang Tao', 'Guowen Xu', 'Xinyuan Qian', 'Yiqin Deng', 'Xianhao Chen', 'Sam Tak Wu Kwong', 'Yuguang Fang']&lt;/li&gt;&lt;li&gt;Tags: ['malicious-agent-detection', 'collaborative-perception-security', 'defense-mechanism', 'multi-agent-systems', 'autonomous-driving-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22890</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification</title><link>https://arxiv.org/abs/2506.06027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SSNI (Sample-specific Score-aware Noise Injection), a method to adaptively choose the diffusion noise level per input for diffusion-based adversarial purification.&lt;/li&gt;&lt;li&gt;Uses a pre-trained score network to compute score norms that estimate how far a sample is from the clean data manifold, then applies a reweighting function to set sample-specific noise level t*.&lt;/li&gt;&lt;li&gt;Integrates SSNI with existing DBP methods and reports improved accuracy and robustness against adversarial examples on CIFAR-10 and ImageNet-1K; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Sun', 'Jiacheng Zhang', 'Zesheng Ye', 'Chaowei Xiao', 'Feng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-defense', 'diffusion-based-purification', 'score-based-models', 'adaptive-noise-injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06027</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models</title><link>https://arxiv.org/abs/2602.13066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a calibrated per-sample Memorization Index (MI) and Overfit/Novelty Index (ONI) to detect training-data duplication in generative MRI models.&lt;/li&gt;&lt;li&gt;Computes features from an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to bounded MI/ONI scores.&lt;/li&gt;&lt;li&gt;Evaluated on three MRI datasets with controlled duplication rates and common augmentations; reports near-perfect sample-level duplicate detection and consistent metric behavior across datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Deo', 'Yan Jia', 'Toni Lassila', 'Victoria J Hodge', 'Alejandro F Frang', 'Chenghao Qian', 'Siyuan Kang', 'Ibrahim Habli']&lt;/li&gt;&lt;li&gt;Tags: ['training data leakage', 'memorization detection', 'privacy', 'membership inference', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13066</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees</title><link>https://arxiv.org/abs/2601.12447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CryptoFair-FL: a cryptographic federated learning framework that provides verifiable fairness guarantees (demographic parity, equalized odds) while preserving privacy using homomorphic encryption and secure multi-party computation.&lt;/li&gt;&lt;li&gt;Introduces a batched verification protocol improving computational complexity from O(n^2) to O(n log n) and provides (ε, δ)-differential privacy guarantees; theoretical analysis shows near-optimal privacy–fairness tradeoffs.&lt;/li&gt;&lt;li&gt;Evaluates on multiple benchmarks (MIMIC-IV, Adult Income, CelebA, FedFair-100), demonstrating large reductions in fairness violations with modest computational overhead and robustness against attribute inference attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammed Himayath Ali', 'Mohammed Aqib Abdullah', 'Syed Muneer Hussain', 'Mohammed Mudassir Uddin', 'Shahnawaz Alam']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'differential privacy', 'cryptographic protocols', 'fairness verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12447</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Don't Walk the Line: Boundary Guidance for Filtered Generation</title><link>https://arxiv.org/abs/2510.11834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode where fine-tuning generators to avoid safety filters pushes outputs toward the classifier decision boundary, increasing false positives and negatives.&lt;/li&gt;&lt;li&gt;Proposes Boundary Guidance, an RL-based fine-tuning method that explicitly steers generation away from the classifier margin to improve both safety and utility.&lt;/li&gt;&lt;li&gt;Demonstrates improvements on benchmarks of jailbreak, ambiguous, and long-context prompts, with ablations across model scales and reward designs and evaluations using LLM-as-a-Judge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Andreas Haupt']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'jailbreak mitigation', 'safety filtering', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11834</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title><link>https://arxiv.org/abs/2510.07978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoiceAgentBench, a 6,000+ synthetic spoken-query benchmark for evaluating SpeechLMs and ASR-LLM pipelines on agentic tasks (single-tool, multi-tool workflows, multi-turn dialogue) across English and six Indic languages.&lt;/li&gt;&lt;li&gt;Includes speaker-diversity via a novel TTS voice-conversion sampling method, and measures tool selection accuracy, structural consistency, correctness of tool invocations, plus adversarial robustness and safety evaluations.&lt;/li&gt;&lt;li&gt;Finds ASR-LLM pipelines outperform end-to-end SpeechLMs (up to 60.6% parameter-filling accuracy on English); SpeechLMs degrade more sharply on Indic languages and all models struggle on sequential workflows and safety/adversarial scenarios.&lt;/li&gt;&lt;li&gt;Releases dataset and code publicly to enable systematic testing of agentic behavior, multilingual generalization, and safety robustness in voice assistants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhruv Jain', 'Harshit Shukla', 'Gautam Rajeev', 'Ashish Kulkarni', 'Chandra Khatri', 'Shubham Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark', 'adversarial-robustness', 'voice-assistants', 'safety-evaluation', 'multilingual-audio']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07978</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provable Secure Steganography Based on Adaptive Dynamic Sampling</title><link>https://arxiv.org/abs/2504.12579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a provably secure steganography scheme that embeds secret messages in model outputs using sampling and a mapping from message bitstrings to candidate tokens, requiring only access to a model API that accepts a seed.&lt;/li&gt;&lt;li&gt;Design preserves the original model output distribution and handles decoding collisions by maintaining and adaptively expanding a bounded dynamic collision set.&lt;/li&gt;&lt;li&gt;Evaluated on three datasets and three large language models; reports efficiency and capacity comparable to existing provably secure steganography (PSS) methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyi Pang', 'Minhao Bai']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'covert-channel', 'model-misuse', 'provable-security', 'large-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12579</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</title><link>https://arxiv.org/abs/2602.10382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Th\\'eo Lasnier", 'Wissam Antoun', 'Francis Kulumba', "Djam\\'e Seddah"]&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'mechanistic interpretability', 'LLM security', 'attack analysis', 'defense implications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10382</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation</title><link>https://arxiv.org/abs/2602.07954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bielik Guard, a family of compact Polish-language safety classifiers (0.1B and 0.5B parameter variants) fine-tuned for content moderation.&lt;/li&gt;&lt;li&gt;Models trained on a community-annotated dataset (6,885 Polish texts) across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm.&lt;/li&gt;&lt;li&gt;Reports strong evaluation metrics (0.5B: micro F1 0.791, macro F1 0.785) and highlights the 0.1B model's efficiency and low false positive rate on real user prompts.&lt;/li&gt;&lt;li&gt;Emphasizes deployment-focused design (publicly available models, prefer appropriate responses over blunt blocking for sensitive categories).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Krzysztof Wr\\'obel", 'Jan Maria Kowalski', 'Jerzy Surma', 'Igor Ciuciura', "Maciej Szyma\\'nski"]&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-classifier', 'guardrails', 'Polish-NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07954</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively suppresses toxic expert neurons in multimodal LLMs without parameter updates to reduce harmful outputs.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and evaluates SGM against existing detoxification methods under standard and adversarial conditions.&lt;/li&gt;&lt;li&gt;Reports large toxicity reductions (e.g., from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; presents an extensible combined defense (SGM*) that integrates with other detox methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['detoxification', 'multimodal safety', 'neuron-level defense', 'adversarial robustness', 'safety evaluation benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Highlight &amp; Summarize: RAG without the jailbreaks</title><link>https://arxiv.org/abs/2508.02872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Highlight &amp; Summarize (H&amp;S), a RAG design that prevents jailbreaks by never exposing the user's question to the generative LLM: a highlighter extracts relevant passages from retrieved documents and a summarizer generates the answer from those passages.&lt;/li&gt;&lt;li&gt;Implements multiple instantiations of H&amp;S and evaluates them on QA tasks, showing comparable or better correctness, relevance, and quality versus standard RAG while reducing attack surface for prompt-injection and jailbreaks.&lt;/li&gt;&lt;li&gt;Positions H&amp;S as a defense-by-design approach that complements or replaces probabilistic classifier/gatekeeper methods which are easy to bypass due to large input/output spaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni Cherubin', 'Andrew Paverd']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak prevention', 'retrieval-augmented generation (RAG)', 'prompt-injection defense', 'model hardening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02872</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantization-Robust LLM Unlearning via Low-Rank Adaptation</title><link>https://arxiv.org/abs/2602.13151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that post-training quantization (especially aggressive low-bit PTQ) can erase or mask unlearning updates in LLMs because full-parameter fine-tuning induces parameter changes too small to survive quantization.&lt;/li&gt;&lt;li&gt;Proposes performing machine unlearning via low-rank adaptation (LoRA) by freezing the base model and concentrating unlearning into adapters so the effective update is preserved after low-bit quantization.&lt;/li&gt;&lt;li&gt;Empirical results on Llama-2-7B with the MUSE dataset show substantial improvements in 4-bit utility and reduced privacy leakage (PrivLeak closer to ideal 0) while maintaining forgetting metrics (VerMem and KnowMem near 0).&lt;/li&gt;&lt;li&gt;Recommendation: use LoRA-style adapters for unlearning when models must be deployed with aggressive quantization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao Vitor Boer Abitante', 'Joana Meneguzzo Pasquali', 'Luan Fonseca Garcia', 'Ewerton de Oliveira', 'Thomas da Silva Paula', 'Rodrigo C. Barros', 'Lucas S. Kupssinsk\\"u']&lt;/li&gt;&lt;li&gt;Tags: ['model-unlearning', 'privacy-leakage', 'quantization-robustness', 'LoRA', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13151</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Consistency of Large Reasoning Models Under Multi-Turn Attacks</title><link>https://arxiv.org/abs/2602.13093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates nine state-of-the-art reasoning models under multi-turn adversarial attacks and compares them to instruction-tuned baselines.&lt;/li&gt;&lt;li&gt;Identifies five adversarial failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, Reasoning Fatigue) and shows the first two account for ~50% of failures.&lt;/li&gt;&lt;li&gt;Analyzes defenses: demonstrates that Confidence-Aware Response Generation (CARG) is ineffective for reasoning models due to overconfidence from long reasoning traces and that random confidence embedding can outperform targeted extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'robustness', 'defenses', 'failure modes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13093</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Autoencoders are Capable LLM Jailbreak Mitigators</title><link>https://arxiv.org/abs/2602.12418</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Context-Conditioned Delta Steering (CC-Delta), a defense that uses sparse autoencoders (SAEs) to identify and steer out jailbreak-relevant features at inference time.&lt;/li&gt;&lt;li&gt;Selects sparse features via statistical testing on paired harmful prompts (with/without jailbreak context) and applies mean-shift steering in SAE latent space.&lt;/li&gt;&lt;li&gt;Evaluated on four aligned instruction-tuned LLMs and twelve jailbreak attacks, outperforming dense latent-space mean-shift steering, especially on out-of-distribution attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yannick Assogba', 'Jacopo Cortellazzi', 'Javier Abad', 'Pau Rodriguez', 'Xavier Suau', 'Arno Blaas']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak mitigation', 'defense', 'sparse autoencoder', 'inference-time steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12418</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAT-Bench: A Comprehensive Benchmark for Text Anonymization</title><link>https://arxiv.org/abs/2602.12806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAT-Bench, a benchmark that evaluates text anonymization tools by measuring re-identification risk using synthetic texts grounded in U.S. demographics.&lt;/li&gt;&lt;li&gt;Compares NER-based and LLM-based anonymizers (including iterative LLM anonymizers) against an LLM-based attacker that tries to infer direct and indirect identifiers from anonymized text.&lt;/li&gt;&lt;li&gt;Finds that even leading tools leave non-negligible re-identification risk—especially when identifiers are nonstandard or indirect cues enable linking—and that LLM-based anonymizers offer better privacy-utility trade-offs at higher compute cost.&lt;/li&gt;&lt;li&gt;Provides multilingual evaluation, accounts for disparate impact across identifiers, and releases the benchmark to encourage further research and geographic expansion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nata\\v{s}a Kr\\v{c}o', 'Zexi Yao', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 're-identification', 'text anonymization', 'attacks_vs_defenses', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12806</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Power Iteration Method for Differentially Private PCA</title><link>https://arxiv.org/abs/2602.11454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a differentially private variant of the power iteration algorithm to compute the top singular vector of a data matrix under the (ε,δ)-DP model where neighboring datasets differ by one row.&lt;/li&gt;&lt;li&gt;Introduces an adaptive filtering technique that leverages the matrix coherence parameter to improve utility beyond worst-case guarantees for low-coherence matrices.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of privacy and utility, and positions the work relative to prior private power iteration methods that used a different adjacency model (Hardt–Roth STOC 2013).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ta Duy Nguyen', 'Alina Ene', 'Huy Le Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy-preserving ML', 'PCA', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11454</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks</title><link>https://arxiv.org/abs/2602.10478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GPU-Fuzz, a constraint-driven fuzzer that models operator parameters as formal constraints to generate GPU kernel test cases.&lt;/li&gt;&lt;li&gt;Uses a constraint solver to systematically produce boundary-case inputs that trigger GPU memory errors in DL frameworks.&lt;/li&gt;&lt;li&gt;Applied to PyTorch, TensorFlow, and PaddlePaddle and discovered 13 previously unknown GPU memory bugs, demonstrating practical effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Li', 'Hongyi Lu', 'Yanan Guo', 'Zhenkai Zhang', 'Shuai Wang', 'Fengwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['fuzzing', 'memory-safety', 'deep-learning-frameworks', 'vulnerability-discovery', 'GPU-kernels']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10478</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PIDSMaker: Building and Evaluating Provenance-based Intrusion Detection Systems</title><link>https://arxiv.org/abs/2601.22983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PIDSMaker, an open-source framework that standardizes development and evaluation of provenance-based intrusion detection systems (PIDSs).&lt;/li&gt;&lt;li&gt;Consolidates eight state-of-the-art PIDSs into a modular, extensible architecture with consistent preprocessing, labeling, dataset splits, and metrics to enable fair comparisons.&lt;/li&gt;&lt;li&gt;Provides utilities for ablation studies, hyperparameter tuning, multi-run instability measurement, visualization, and a YAML interface for rapid prototyping.&lt;/li&gt;&lt;li&gt;Releases preprocessed datasets and ground-truth labels to improve reproducibility and shared evaluation in the PIDS community.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tristan Bilot', 'Baoxiang Jiang', 'Thomas Pasquier']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'provenance', 'evaluation-framework', 'datasets', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22983</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title><link>https://arxiv.org/abs/2510.07978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoiceAgentBench, a 6,000+ synthetic spoken-query benchmark for evaluating SpeechLMs and ASR-LLM pipelines on agentic tasks (single-tool, multi-tool workflows, multi-turn dialogue) across English and six Indic languages.&lt;/li&gt;&lt;li&gt;Includes speaker-diversity via a novel TTS voice-conversion sampling method, and measures tool selection accuracy, structural consistency, correctness of tool invocations, plus adversarial robustness and safety evaluations.&lt;/li&gt;&lt;li&gt;Finds ASR-LLM pipelines outperform end-to-end SpeechLMs (up to 60.6% parameter-filling accuracy on English); SpeechLMs degrade more sharply on Indic languages and all models struggle on sequential workflows and safety/adversarial scenarios.&lt;/li&gt;&lt;li&gt;Releases dataset and code publicly to enable systematic testing of agentic behavior, multilingual generalization, and safety robustness in voice assistants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhruv Jain', 'Harshit Shukla', 'Gautam Rajeev', 'Ashish Kulkarni', 'Chandra Khatri', 'Shubham Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark', 'adversarial-robustness', 'voice-assistants', 'safety-evaluation', 'multilingual-audio']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07978</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Highlight &amp; Summarize: RAG without the jailbreaks</title><link>https://arxiv.org/abs/2508.02872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Highlight &amp; Summarize (H&amp;S), a RAG design that prevents jailbreaks by never exposing the user's question to the generative LLM: a highlighter extracts relevant passages from retrieved documents and a summarizer generates the answer from those passages.&lt;/li&gt;&lt;li&gt;Implements multiple instantiations of H&amp;S and evaluates them on QA tasks, showing comparable or better correctness, relevance, and quality versus standard RAG while reducing attack surface for prompt-injection and jailbreaks.&lt;/li&gt;&lt;li&gt;Positions H&amp;S as a defense-by-design approach that complements or replaces probabilistic classifier/gatekeeper methods which are easy to bypass due to large input/output spaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni Cherubin', 'Andrew Paverd']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak prevention', 'retrieval-augmented generation (RAG)', 'prompt-injection defense', 'model hardening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02872</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title><link>https://arxiv.org/abs/2507.12108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares different operationalizations for detecting coordinated online behavior (monomodal, flattened, and multimodal integration).&lt;/li&gt;&lt;li&gt;Analyzes trade-offs between weakly and strongly integrated multimodal models in capturing broad versus tightly aligned coordination patterns.&lt;/li&gt;&lt;li&gt;Shows multimodal analysis yields more informative representations and preserves coordination structures lost by monomodal or flattened approaches, though not all modalities add unique signal.&lt;/li&gt;&lt;li&gt;Provides methodological insights to improve detection and analysis of coordinated manipulation (e.g., disinformation campaigns) to help safeguard platform integrity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Mannocci', 'Stefano Cresci', 'Matteo Magnani', 'Anna Monreale', 'Maurizio Tesconi']&lt;/li&gt;&lt;li&gt;Tags: ['coordinated-behavior', 'misinformation-detection', 'multimodal-analysis', 'social-media-security', 'detection-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12108</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification</title><link>https://arxiv.org/abs/2506.06027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SSNI (Sample-specific Score-aware Noise Injection), a method to adaptively choose the diffusion noise level per input for diffusion-based adversarial purification.&lt;/li&gt;&lt;li&gt;Uses a pre-trained score network to compute score norms that estimate how far a sample is from the clean data manifold, then applies a reweighting function to set sample-specific noise level t*.&lt;/li&gt;&lt;li&gt;Integrates SSNI with existing DBP methods and reports improved accuracy and robustness against adversarial examples on CIFAR-10 and ImageNet-1K; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Sun', 'Jiacheng Zhang', 'Zesheng Ye', 'Chaowei Xiao', 'Feng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-defense', 'diffusion-based-purification', 'score-based-models', 'adaptive-noise-injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06027</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</title><link>https://arxiv.org/abs/2602.06771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, a retention-data-free adversarial erasure framework to remove harmful concepts from diffusion models while preserving unrelated concepts.&lt;/li&gt;&lt;li&gt;Focuses on both robustness (resistance to reactivation via semantically related/adaptive prompts) and retention (maintaining model utility), addressing the typical trade-off between the two.&lt;/li&gt;&lt;li&gt;Uses adversarial, gradient-informed techniques to jointly optimize erasure against prompt attacks without relying on retention datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengpeng Li', 'Kemou Li', 'Qizhou Wang', 'Bo Han', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'adversarial defense', 'diffusion models', 'prompt attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06771</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Membership and Dataset Inference Attacks on Large Audio Generative Models</title><link>https://arxiv.org/abs/2512.09654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates membership inference attacks (MIA) on large audio generative models to determine if individual audio samples were in the training set; finds per-sample MIA signals are weak at scale.&lt;/li&gt;&lt;li&gt;Introduces and evaluates dataset inference (DI) for audio, aggregating membership evidence across multiple samples from an artist/collection, and shows DI is effective in practice.&lt;/li&gt;&lt;li&gt;Frames DI as a practical mechanism for copyright verification and dataset accountability for large, diverse audio training corpora.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Proboszcz', 'Pawe{\\l} Kochanski', 'Karol Korszun', 'Donato Crisostomi', 'Giorgio Strano', 'Emanuele Rodol\\`a', 'Kamil Deja', 'Jan Dubinski']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'dataset-inference', 'privacy', 'generative-audio', 'copyright']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09654</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Don't Walk the Line: Boundary Guidance for Filtered Generation</title><link>https://arxiv.org/abs/2510.11834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode where fine-tuning generators to avoid safety filters pushes outputs toward the classifier decision boundary, increasing false positives and negatives.&lt;/li&gt;&lt;li&gt;Proposes Boundary Guidance, an RL-based fine-tuning method that explicitly steers generation away from the classifier margin to improve both safety and utility.&lt;/li&gt;&lt;li&gt;Demonstrates improvements on benchmarks of jailbreak, ambiguous, and long-context prompts, with ablations across model scales and reward designs and evaluations using LLM-as-a-Judge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Andreas Haupt']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'jailbreak mitigation', 'safety filtering', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11834</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provable Training Data Identification for Large Language Models</title><link>https://arxiv.org/abs/2510.09717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes training-data identification as a set-level inference problem with strict control over the false identification rate (FIR).&lt;/li&gt;&lt;li&gt;Introduces PTDI: computes conformal p-values per data point using known unseen data, and proposes a Jackknife-corrected Beta boundary (JKBB) estimator to scale p-values by estimating the training-data proportion.&lt;/li&gt;&lt;li&gt;Applies Benjamini–Hochberg on scaled p-values to select identified training points with provable FIR control.&lt;/li&gt;&lt;li&gt;Empirical results show higher power than prior methods while maintaining strict FIR guarantees across models and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenlong Liu', 'Hao Zeng', 'Weiran Huang', 'Hongxin Wei']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'training-data-identification', 'privacy-auditing', 'conformal-inference', 'statistical-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09717</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adopting a human developmental visual diet yields robust, shape-based AI vision</title><link>https://arxiv.org/abs/2507.03168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a human developmental visual diet (DVD) curriculum—modulating acuity, contrast sensitivity, and color during training—to guide vision models.&lt;/li&gt;&lt;li&gt;Models trained with the DVD show markedly increased shape-bias, state-of-the-art abstract shape recognition, and improved robustness to common image corruptions.&lt;/li&gt;&lt;li&gt;Reports higher resilience to adversarial attacks, indicating that curriculum-based training can serve as a resource-efficient defense improving model robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zejin Lu', 'Sushrut Thorat', 'Radoslaw M Cichy', 'Tim C Kietzmann']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'training curriculum', 'shape-bias', 'robustness-to-corruptions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.03168</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Memory Injection Attacks on LLM Agents via Query-Only Interaction</title><link>https://arxiv.org/abs/2503.03704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MINJA, a novel query-only memory injection attack that causes LLM agents to store malicious records in their memory without direct write access.&lt;/li&gt;&lt;li&gt;Uses bridging steps and an indication prompt with a progressive shortening strategy so the agent autonomously generates and later retrieves malicious reasoning traces tied to future victim queries.&lt;/li&gt;&lt;li&gt;Evaluates MINJA across multiple agent setups, showing it can reliably compromise agent memory with minimal attacker capabilities and highlights the practical risk to deployed agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shen Dong', 'Shaochen Xu', 'Pengfei He', 'Yige Li', 'Jiliang Tang', 'Tianming Liu', 'Hui Liu', 'Zhen Xiang']&lt;/li&gt;&lt;li&gt;Tags: ['memory-injection', 'prompt-injection', 'data-poisoning', 'red-teaming', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03704</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pixel-Based Similarities as an Alternative to Neural Data for Improving Convolutional Neural Network Adversarial Robustness</title><link>https://arxiv.org/abs/2410.03952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-driven substitute for neural-recording-based representational similarity regularizers by computing pixel-based similarities directly from images.&lt;/li&gt;&lt;li&gt;Shows that this pixel-based regularizer preserves the adversarial robustness benefits of the original biologically motivated Loss while removing dependence on neural recordings or task-specific augmentations.&lt;/li&gt;&lt;li&gt;Method is lightweight, integrates into standard CNN training pipelines, and yields robustness improvements though it does not surpass specialized state-of-the-art defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elie Attias', 'Cengiz Pehlevan', 'Dina Obeid']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'regularization', 'neural-inspired', 'adversarial examples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03952</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAT-Bench: A Comprehensive Benchmark for Text Anonymization</title><link>https://arxiv.org/abs/2602.12806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAT-Bench, a benchmark that evaluates text anonymization tools by measuring re-identification risk using synthetic texts grounded in U.S. demographics.&lt;/li&gt;&lt;li&gt;Compares NER-based and LLM-based anonymizers (including iterative LLM anonymizers) against an LLM-based attacker that tries to infer direct and indirect identifiers from anonymized text.&lt;/li&gt;&lt;li&gt;Finds that even leading tools leave non-negligible re-identification risk—especially when identifiers are nonstandard or indirect cues enable linking—and that LLM-based anonymizers offer better privacy-utility trade-offs at higher compute cost.&lt;/li&gt;&lt;li&gt;Provides multilingual evaluation, accounts for disparate impact across identifiers, and releases the benchmark to encourage further research and geographic expansion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nata\\v{s}a Kr\\v{c}o', 'Zexi Yao', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 're-identification', 'text anonymization', 'attacks_vs_defenses', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12806</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Autoencoders are Capable LLM Jailbreak Mitigators</title><link>https://arxiv.org/abs/2602.12418</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Context-Conditioned Delta Steering (CC-Delta), a defense that uses sparse autoencoders (SAEs) to identify and steer out jailbreak-relevant features at inference time.&lt;/li&gt;&lt;li&gt;Selects sparse features via statistical testing on paired harmful prompts (with/without jailbreak context) and applies mean-shift steering in SAE latent space.&lt;/li&gt;&lt;li&gt;Evaluated on four aligned instruction-tuned LLMs and twelve jailbreak attacks, outperforming dense latent-space mean-shift steering, especially on out-of-distribution attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yannick Assogba', 'Jacopo Cortellazzi', 'Javier Abad', 'Pau Rodriguez', 'Xavier Suau', 'Arno Blaas']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak mitigation', 'defense', 'sparse autoencoder', 'inference-time steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12418</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantization-Robust LLM Unlearning via Low-Rank Adaptation</title><link>https://arxiv.org/abs/2602.13151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that post-training quantization (especially aggressive low-bit PTQ) can erase or mask unlearning updates in LLMs because full-parameter fine-tuning induces parameter changes too small to survive quantization.&lt;/li&gt;&lt;li&gt;Proposes performing machine unlearning via low-rank adaptation (LoRA) by freezing the base model and concentrating unlearning into adapters so the effective update is preserved after low-bit quantization.&lt;/li&gt;&lt;li&gt;Empirical results on Llama-2-7B with the MUSE dataset show substantial improvements in 4-bit utility and reduced privacy leakage (PrivLeak closer to ideal 0) while maintaining forgetting metrics (VerMem and KnowMem near 0).&lt;/li&gt;&lt;li&gt;Recommendation: use LoRA-style adapters for unlearning when models must be deployed with aggressive quantization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao Vitor Boer Abitante', 'Joana Meneguzzo Pasquali', 'Luan Fonseca Garcia', 'Ewerton de Oliveira', 'Thomas da Silva Paula', 'Rodrigo C. Barros', 'Lucas S. Kupssinsk\\"u']&lt;/li&gt;&lt;li&gt;Tags: ['model-unlearning', 'privacy-leakage', 'quantization-robustness', 'LoRA', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13151</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Diverging Flows: Detecting Extrapolations in Conditional Generation</title><link>https://arxiv.org/abs/2602.13061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Diverging Flows, a method for conditional flow models that enforces inefficient transport for off-manifold inputs so the model can both generate and detect extrapolations natively.&lt;/li&gt;&lt;li&gt;Addresses the silent-failure risk of flow-based predictors by producing detectable divergence for out-of-distribution or extrapolative conditions without degrading predictive fidelity or inference latency.&lt;/li&gt;&lt;li&gt;Validated on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating effective extrapolation detection in safety-critical contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constantinos Tsakonas', 'Serena Ivaldi', 'Jean-Baptiste Mouret']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'flow models', 'robustness', 'safety', 'conditional generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13061</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios</title><link>https://arxiv.org/abs/2602.13040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TCRL, a temporal-coupled adversarial training framework to improve robustness of constrained reinforcement learning (CRL) against temporally coupled perturbations.&lt;/li&gt;&lt;li&gt;Introduces a worst-case-perceived cost constraint function to estimate safety costs under temporally correlated attacks without explicitly modeling adversaries.&lt;/li&gt;&lt;li&gt;Implements a dual-constraint defense mechanism on rewards to counter temporally coupled adversaries while preserving reward unpredictability.&lt;/li&gt;&lt;li&gt;Reports empirical improvements over existing methods in robustness to temporally coupled perturbation attacks across multiple CRL tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wentao Xu', 'Zhongming Yao', 'Weihao Li', 'Zhenghang Song', 'Yumeng Song', 'Tianyi Li', 'Yushuai Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robust reinforcement learning', 'constrained RL', 'temporal adversaries', 'safety/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13040</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs</title><link>https://arxiv.org/abs/2602.12506</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities of RL-finetuned vision-language models (VLMs) to controlled textual perturbations (misleading captions and incorrect chain-of-thought traces) that reduce robustness and confidence.&lt;/li&gt;&lt;li&gt;Shows perturbations change model uncertainty and calibration, and that CoT consistency exacerbates failures across open-source multimodal reasoning models.&lt;/li&gt;&lt;li&gt;Studies RL fine-tuning dynamics, revealing an accuracy–faithfulness trade-off: higher benchmark accuracy can erode reasoning faithfulness and robustness to contextual shifts.&lt;/li&gt;&lt;li&gt;Evaluates defenses (adversarial augmentation and a faithfulness-aware reward); augmentation helps but doesn't fully prevent faithfulness drift and combined training can induce shortcut behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rosie Zhao', 'Anshul Shah', 'Xiaoyu Zhu', 'Xinke Deng', 'Zhongyu Jiang', 'Yang Yang', 'Joerg Liebelt', 'Arnab Mondal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'prompt-manipulation', 'chain-of-thought-consistency', 'defenses', 'model-calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12506</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Abstractive Red-Teaming of Language Model Character</title><link>https://arxiv.org/abs/2602.12318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'abstractive red-teaming': searching for natural-language query categories (e.g., language + topic combos) that consistently elicit character-specification violations from LMs.&lt;/li&gt;&lt;li&gt;Presents two algorithms to efficiently search category space against a reward model: (1) RL on a category-generator LLM and (2) iterative category synthesis using a strong LLM from high-scoring queries.&lt;/li&gt;&lt;li&gt;Evaluates across a 12-principle character specification and 7 target models, outperforming baselines and surfacing realistic vulnerabilities (e.g., prompts causing models to advocate harm or illegal actions).&lt;/li&gt;&lt;li&gt;Frames the method as a cost-efficient pre-deployment auditing/red-teaming tool for detecting character/jailbreak vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nate Rahn', 'Allison Qi', 'Avery Griffin', 'Jonathan Michala', 'Henry Sleight', 'Erik Jones']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'adversarial-prompting', 'safety-auditing', 'vulnerability-discovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12318</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System</title><link>https://arxiv.org/abs/2602.10915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic security analysis of mobile autonomous agents, decomposing threats into Agent Identity, External Interface, Internal Reasoning, and Action Execution; identifies vulnerabilities like fake app identity, visual spoofing, indirect prompt injection, and privilege escalation.&lt;/li&gt;&lt;li&gt;Proposes Aura, a clean-slate Agent Universal Runtime Architecture with a Hub-and-Spoke design (System Agent, sandboxed App Agents, Agent Kernel) and four defense pillars: cryptographic identity binding, semantic input sanitization, taint-aware cognitive integrity, and granular non-deniable auditing.&lt;/li&gt;&lt;li&gt;Implements and evaluates Aura on MobileSafetyBench versus a representative system (Doubao), reporting improved low-risk task success (≈75% → 94.3%), reduced high-risk attack success (≈40% → 4.4%), and substantial latency reductions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Zou', 'Sheng Guo', 'Qiuyang Zhan', 'Lepeng Zhao', 'Shuo Li', 'Qi Li', 'Ke Xu', 'Mingwei Xu', 'Zhuotao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['mobile agent security', 'prompt injection', 'runtime defenses', 'agent identity &amp; authentication', 'secure agent OS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10915</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation</title><link>https://arxiv.org/abs/2602.07954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bielik Guard, a family of compact Polish-language safety classifiers (0.1B and 0.5B parameter variants) fine-tuned for content moderation.&lt;/li&gt;&lt;li&gt;Models trained on a community-annotated dataset (6,885 Polish texts) across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm.&lt;/li&gt;&lt;li&gt;Reports strong evaluation metrics (0.5B: micro F1 0.791, macro F1 0.785) and highlights the 0.1B model's efficiency and low false positive rate on real user prompts.&lt;/li&gt;&lt;li&gt;Emphasizes deployment-focused design (publicly available models, prefer appropriate responses over blunt blocking for sensitive categories).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Krzysztof Wr\\'obel", 'Jan Maria Kowalski', 'Jerzy Surma', 'Igor Ciuciura', "Maciej Szyma\\'nski"]&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-classifier', 'guardrails', 'Polish-NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07954</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</title><link>https://arxiv.org/abs/2602.06771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, a retention-data-free adversarial erasure framework to remove harmful concepts from diffusion models while preserving unrelated concepts.&lt;/li&gt;&lt;li&gt;Focuses on both robustness (resistance to reactivation via semantically related/adaptive prompts) and retention (maintaining model utility), addressing the typical trade-off between the two.&lt;/li&gt;&lt;li&gt;Uses adversarial, gradient-informed techniques to jointly optimize erasure against prompt attacks without relying on retention datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengpeng Li', 'Kemou Li', 'Qizhou Wang', 'Bo Han', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'adversarial defense', 'diffusion models', 'prompt attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06771</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively suppresses toxic expert neurons in multimodal LLMs without parameter updates to reduce harmful outputs.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and evaluates SGM against existing detoxification methods under standard and adversarial conditions.&lt;/li&gt;&lt;li&gt;Reports large toxicity reductions (e.g., from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; presents an extensible combined defense (SGM*) that integrates with other detox methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['detoxification', 'multimodal safety', 'neuron-level defense', 'adversarial robustness', 'safety evaluation benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Watermarking Discrete Diffusion Language Models</title><link>https://arxiv.org/abs/2511.02083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a watermarking method for discrete diffusion language models (DDLMs) using a distribution-preserving Gumbel-max sampling trick with position-seeded randomness.&lt;/li&gt;&lt;li&gt;Empirically demonstrates reliable detectability on LLaDA and provides an analytical proof that the watermark is distortion-free with false detection probability decaying exponentially with sequence length.&lt;/li&gt;&lt;li&gt;Emphasizes practical deployability: no expensive hyperparameter tuning required, enabling straightforward scaling across models and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avi Bagchi', 'Akhil Bhimaraju', 'Moulik Choraria', 'Daniel Alabi', 'Lav R. Varshney']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model attribution', 'defense', 'discrete diffusion language models', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02083</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provable Training Data Identification for Large Language Models</title><link>https://arxiv.org/abs/2510.09717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes training-data identification as a set-level inference problem with strict control over the false identification rate (FIR).&lt;/li&gt;&lt;li&gt;Introduces PTDI: computes conformal p-values per data point using known unseen data, and proposes a Jackknife-corrected Beta boundary (JKBB) estimator to scale p-values by estimating the training-data proportion.&lt;/li&gt;&lt;li&gt;Applies Benjamini–Hochberg on scaled p-values to select identified training points with provable FIR control.&lt;/li&gt;&lt;li&gt;Empirical results show higher power than prior methods while maintaining strict FIR guarantees across models and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenlong Liu', 'Hao Zeng', 'Weiran Huang', 'Hongxin Wei']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'training-data-identification', 'privacy-auditing', 'conformal-inference', 'statistical-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09717</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MetaSeal: Defending Against Image Attribution Forgery Through Content-Dependent Cryptographic Watermarks</title><link>https://arxiv.org/abs/2509.10766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MetaSeal, a content-dependent cryptographic watermarking framework to defend image attribution against forgery and unauthorized replication.&lt;/li&gt;&lt;li&gt;Provides forgery resistance via cryptographic verification, embeds self-contained attribution robust to benign transformations, and offers visual evidence of tampering.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness on both natural and AI-generated images and demonstrates mitigation of common forgery attempts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Zhou', 'Ruyi Ding', 'Gaowen Liu', 'Charles Fleming', 'Ramana Rao Kompella', 'Yunsi Fei', 'Xiaolin Xu', 'Shaolei Ren']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'image forgery defense', 'cryptographic verification', 'robustness', 'attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10766</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title><link>https://arxiv.org/abs/2507.12108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares different operationalizations for detecting coordinated online behavior (monomodal, flattened, and multimodal integration).&lt;/li&gt;&lt;li&gt;Analyzes trade-offs between weakly and strongly integrated multimodal models in capturing broad versus tightly aligned coordination patterns.&lt;/li&gt;&lt;li&gt;Shows multimodal analysis yields more informative representations and preserves coordination structures lost by monomodal or flattened approaches, though not all modalities add unique signal.&lt;/li&gt;&lt;li&gt;Provides methodological insights to improve detection and analysis of coordinated manipulation (e.g., disinformation campaigns) to help safeguard platform integrity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Mannocci', 'Stefano Cresci', 'Matteo Magnani', 'Anna Monreale', 'Maurizio Tesconi']&lt;/li&gt;&lt;li&gt;Tags: ['coordinated-behavior', 'misinformation-detection', 'multimodal-analysis', 'social-media-security', 'detection-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12108</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pixel-Based Similarities as an Alternative to Neural Data for Improving Convolutional Neural Network Adversarial Robustness</title><link>https://arxiv.org/abs/2410.03952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data-driven substitute for neural-recording-based representational similarity regularizers by computing pixel-based similarities directly from images.&lt;/li&gt;&lt;li&gt;Shows that this pixel-based regularizer preserves the adversarial robustness benefits of the original biologically motivated Loss while removing dependence on neural recordings or task-specific augmentations.&lt;/li&gt;&lt;li&gt;Method is lightweight, integrates into standard CNN training pipelines, and yields robustness improvements though it does not surpass specialized state-of-the-art defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elie Attias', 'Cengiz Pehlevan', 'Dina Obeid']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'regularization', 'neural-inspired', 'adversarial examples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03952</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Preventing the Collapse of Peer Review Requires Verification-First AI</title><link>https://arxiv.org/abs/2601.16909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI-assisted peer review should prioritize verification-first tools (truth-coupling) rather than mimicry of human scoring.&lt;/li&gt;&lt;li&gt;Formalizes a minimal model mixing high-fidelity checks with frequent proxy judgments, deriving a coupling law and an incentive-collapse condition where effort shifts to optimizing proxies.&lt;/li&gt;&lt;li&gt;Recommends deploying AI as an adversarial auditor that produces auditable verification artifacts and increases verification bandwidth to prevent signal shrinkage and verification pressure.&lt;/li&gt;&lt;li&gt;Provides actionable guidance for tool builders and program chairs to avoid amplifying claim inflation via score-predicting systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei You', 'Lele Cao', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial auditing', 'AI for verification', 'Peer review integrity', 'Incentive modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16909</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges</title><link>https://arxiv.org/abs/2510.23883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a taxonomy of security threats unique to agentic AI (LLM-based agents with planning, tool use, memory, and autonomy).&lt;/li&gt;&lt;li&gt;Reviews existing benchmarks and evaluation methodologies for assessing vulnerabilities and adversarial behavior of agentic systems.&lt;/li&gt;&lt;li&gt;Surveys defense strategies spanning technical mitigations (robustness, sandboxing, monitoring) and governance approaches (policy, standards).&lt;/li&gt;&lt;li&gt;Identifies open challenges and research directions for building secure-by-design agentic AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshuman Chhabra', 'Shrestha Datta', 'Shahriar Kabir Nahin', 'Prasant Mohapatra']&lt;/li&gt;&lt;li&gt;Tags: ['agentic-ai', 'security-survey', 'threats-and-defenses', 'evaluation-and-benchmarks', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23883</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title><link>https://arxiv.org/abs/2510.07978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoiceAgentBench, a 6,000+ synthetic spoken-query benchmark for evaluating SpeechLMs and ASR-LLM pipelines on agentic tasks (single-tool, multi-tool workflows, multi-turn dialogue) across English and six Indic languages.&lt;/li&gt;&lt;li&gt;Includes speaker-diversity via a novel TTS voice-conversion sampling method, and measures tool selection accuracy, structural consistency, correctness of tool invocations, plus adversarial robustness and safety evaluations.&lt;/li&gt;&lt;li&gt;Finds ASR-LLM pipelines outperform end-to-end SpeechLMs (up to 60.6% parameter-filling accuracy on English); SpeechLMs degrade more sharply on Indic languages and all models struggle on sequential workflows and safety/adversarial scenarios.&lt;/li&gt;&lt;li&gt;Releases dataset and code publicly to enable systematic testing of agentic behavior, multilingual generalization, and safety robustness in voice assistants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhruv Jain', 'Harshit Shukla', 'Gautam Rajeev', 'Ashish Kulkarni', 'Chandra Khatri', 'Shubham Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark', 'adversarial-robustness', 'voice-assistants', 'safety-evaluation', 'multilingual-audio']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07978</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How cyborg propaganda reshapes collective action</title><link>https://arxiv.org/abs/2602.13088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'cyborg propaganda' as hybrid influence operations combining verified human actors with AI-driven monitoring and content generation to coordinate mass dissemination while evading bot regulations.&lt;/li&gt;&lt;li&gt;Analyzes the democratic and legal risks: centralized algorithmic campaigns turning citizens into 'cognitive proxies', and regulatory gray zones due to use of verified human accounts.&lt;/li&gt;&lt;li&gt;Proposes a research agenda and governance frameworks aimed at distinguishing organic from coordinated AI-assisted diffusion and mitigating the harms of such campaigns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonas R. Kunst', 'Kinga Bierwiaczonek', 'Meeyoung Cha', 'Omid V. Ebrahimi', 'Marc Fawcett-Atkinson', 'Asbj{\\o}rn F{\\o}lstad', 'Anton Gollwitzer', 'Nils K\\"obis', 'Gary Marcus', 'Jon Roozenbeek', 'Daniel Thilo Schroeder', 'Jay J. Van Bavel', 'Sander van der Linden', 'Rory White', 'Live Leonhardsen Wilhelmsen']&lt;/li&gt;&lt;li&gt;Tags: ['disinformation', 'influence-operations', 'detection', 'governance', 'AI-assisted coordination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13088</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Diverging Flows: Detecting Extrapolations in Conditional Generation</title><link>https://arxiv.org/abs/2602.13061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Diverging Flows, a method for conditional flow models that enforces inefficient transport for off-manifold inputs so the model can both generate and detect extrapolations natively.&lt;/li&gt;&lt;li&gt;Addresses the silent-failure risk of flow-based predictors by producing detectable divergence for out-of-distribution or extrapolative conditions without degrading predictive fidelity or inference latency.&lt;/li&gt;&lt;li&gt;Validated on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating effective extrapolation detection in safety-critical contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constantinos Tsakonas', 'Serena Ivaldi', 'Jean-Baptiste Mouret']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'flow models', 'robustness', 'safety', 'conditional generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13061</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAT-Bench: A Comprehensive Benchmark for Text Anonymization</title><link>https://arxiv.org/abs/2602.12806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAT-Bench, a benchmark that evaluates text anonymization tools by measuring re-identification risk using synthetic texts grounded in U.S. demographics.&lt;/li&gt;&lt;li&gt;Compares NER-based and LLM-based anonymizers (including iterative LLM anonymizers) against an LLM-based attacker that tries to infer direct and indirect identifiers from anonymized text.&lt;/li&gt;&lt;li&gt;Finds that even leading tools leave non-negligible re-identification risk—especially when identifiers are nonstandard or indirect cues enable linking—and that LLM-based anonymizers offer better privacy-utility trade-offs at higher compute cost.&lt;/li&gt;&lt;li&gt;Provides multilingual evaluation, accounts for disparate impact across identifiers, and releases the benchmark to encourage further research and geographic expansion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nata\\v{s}a Kr\\v{c}o', 'Zexi Yao', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 're-identification', 'text anonymization', 'attacks_vs_defenses', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.12806</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Peak + Accumulation: A Proxy-Level Scoring Formula for Multi-Turn LLM Attack Detection</title><link>https://arxiv.org/abs/2602.11247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a flaw in weighted-average aggregation of per-turn risk scores for multi-turn prompt-injection attacks: persistence across turns is not captured.&lt;/li&gt;&lt;li&gt;Proposes a 'peak + accumulation' scoring formula combining peak single-turn risk, persistence ratio, and category diversity, inspired by CUSUM and Bayesian updating.&lt;/li&gt;&lt;li&gt;Evaluates on 10,654 conversations (588 attacks, 10,066 benign) achieving 90.8% recall at 1.20% FPR and F1 85.9%; sensitivity analysis shows a phase transition in persistence parameter.&lt;/li&gt;&lt;li&gt;Releases the scoring algorithm, pattern library, and evaluation harness as open source for proxy-level defense use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J Alex Corll']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'multi-turn attacks', 'attack detection', 'proxy-level defense', 'security benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.11247</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Consistency of Large Reasoning Models Under Multi-Turn Attacks</title><link>https://arxiv.org/abs/2602.13093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates nine state-of-the-art reasoning models under multi-turn adversarial attacks and compares them to instruction-tuned baselines.&lt;/li&gt;&lt;li&gt;Identifies five adversarial failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, Reasoning Fatigue) and shows the first two account for ~50% of failures.&lt;/li&gt;&lt;li&gt;Analyzes defenses: demonstrates that Confidence-Aware Response Generation (CARG) is ineffective for reasoning models due to overconfidence from long reasoning traces and that random confidence embedding can outperform targeted extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'robustness', 'defenses', 'failure modes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.13093</guid><pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
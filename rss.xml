<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 10 Dec 2025 00:21:16 +0000</lastBuildDate><item><title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title><link>https://arxiv.org/abs/2507.16329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DREAM, a framework that models the probabilistic distribution of problematic prompts for text-to-image (T2I) systems to enable scalable, diverse red teaming rather than optimizing individual prompts.&lt;/li&gt;&lt;li&gt;Introduces an energy-based reformulation and GC-SPSA, an optimization algorithm to obtain stable gradient estimates through lengthy, non-differentiable T2I pipelines, plus a diversity-aware sampling strategy at inference.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance in prompt success rate and diversity across multiple T2I models and safety filters, enabling large-scale sampling of adversarial/problematic prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Junjie Wang', 'Yiming Li', 'Zhiyang Hu', 'Leyi Qi', 'Jianshuo Dong', 'Run Wang', 'Han Qiu', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'text-to-image safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16329</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title><link>https://arxiv.org/abs/2507.16302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses safety-driven unlearning for text-to-image diffusion models to suppress toxic/unsafe behaviors inherited from pretraining.&lt;/li&gt;&lt;li&gt;Introduces ResAlign, which models downstream fine-tuning via a Moreau envelope reformulation to enable efficient gradient estimation that minimizes recovery of harmful behaviors.&lt;/li&gt;&lt;li&gt;Proposes a meta-learning strategy to simulate diverse fine-tuning scenarios, improving generalization and resilience to subsequent benign fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluations across datasets and fine-tuning configurations show ResAlign better retains safety while preserving benign generation quality compared to prior unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Renjie Gu', 'Junjie Wang', 'Leyi Qi', 'Yiming Li', 'Run Wang', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-driven unlearning', 'diffusion models', 'robustness to fine-tuning', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16302</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack</title><link>https://arxiv.org/abs/2512.05853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VRSA (Visual Reasoning Sequential Attack): decomposes a harmful prompt into a sequence of related sub-images to induce multimodal LLMs to gradually externalize and aggregate harmful intent.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Scene Refinement (optimize scene realism), Semantic Coherent Completion (iteratively rewrite sub-texts with contextual continuity), and Text-Image Consistency Alignment to maintain semantic consistency across steps.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates than prior jailbreak methods on both open-source and closed-source MLLMs, including GPT-4o and Claude-4.5-Sonnet.&lt;/li&gt;&lt;li&gt;Highlights a novel visual-modal vulnerability in multimodal models and provides an evaluation framework for visual reasoning–based jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiji Zhao', 'Shukun Xiong', 'Yao Huang', 'Yan Jin', 'Zhenyu Wu', 'Jiyang Guan', 'Ranjie Duan', 'Jialing Tao', 'Hui Xue', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal-LLMs', 'visual-prompt-attack', 'adversarial-attack', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05853</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.04441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MindDrive, an end-to-end autonomous driving framework combining a World Action Model (WaM)-based Future-aware Trajectory Generator (FaTG) for ego-conditioned "what-if" simulations and foresighted trajectory candidate generation.&lt;/li&gt;&lt;li&gt;Introduces a VLM-oriented Evaluator (VLoE) that uses a vision-language model to perform multi-objective evaluation (safety, comfort, efficiency) and produce human-aligned decision making.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results on NAVSIM-v1 and NAVSIM-v2 benchmarks, emphasizing improvements in safety, compliance, and generalization through interpretable, reasoning-guided decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Sun', 'Yaoguang Cao', 'Yan Wang', 'Rui Wang', 'Jiachen Shang', 'Xiejie Feng', 'Jiayi Lu', 'Jia Shi', 'Shichun Yang', 'Xiaoyu Yan', 'Ziying Song']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'world models', 'trajectory generation', 'vision-language evaluator', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04441</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title><link>https://arxiv.org/abs/2511.22019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free, post-hoc uncertainty estimation method for contrastive vision-language models that builds class-specific probabilistic embeddings by measuring visual feature consistency using feature projection and multivariate Gaussians.&lt;/li&gt;&lt;li&gt;Method is VLM-agnostic, requires no fine-tuning, works with as few as 10 images per class, and is robust to distribution shift.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art error detection on ImageNet, Flowers102, Food101, EuroSAT and DTD, outperforming deterministic and probabilistic VLM baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenxiang Lin', 'Maryam Haghighat', 'Will Browne', 'Dimity Miller']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'vision-language models', 'error detection / reliability', 'out-of-distribution detection', 'post-hoc methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22019</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</title><link>https://arxiv.org/abs/2511.13127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VEIL, a jailbreak framework that crafts benign-looking prompts that exploit text-to-video models' audio-visual associations to elicit unsafe video content.&lt;/li&gt;&lt;li&gt;Modular prompt design: neutral scene anchors, latent auditory triggers (text descriptions of innocuous audio events), and stylistic modulators to amplify/stabilize the effect.&lt;/li&gt;&lt;li&gt;Formulates attack as constrained optimization over prompt space and uses guided search to balance stealth and success; evaluates on 7 T2V models with substantial success gains.&lt;/li&gt;&lt;li&gt;Demonstrates practical, stealthy jailbreaks affecting commercial T2V models and releases code/demos.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonghao Ying', 'Moyang Chen', 'Nizhang Li', 'Zhiqiang Wang', 'Wenxin Zhang', 'Quanchen Zou', 'Zonglei Jing', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompt engineering', 'LLM/T2V red teaming', 'multimodal safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13127</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</title><link>https://arxiv.org/abs/2511.11210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STONE, a configurable spherical trigger enabling the first one-to-N (multi-target) universal backdoor attacks on 3D point cloud models.&lt;/li&gt;&lt;li&gt;Uses parameterized spatial properties to create a dynamic key space so a single trigger maps to multiple target labels.&lt;/li&gt;&lt;li&gt;Provides a theoretical foundation via Neural Tangent Kernel (NTK) analysis and extensive empirical evaluation showing high attack success (up to 100%) with negligible impact on clean accuracy under dirty-label and black-box settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongmei Shan', 'Wei Lian', 'Chongxia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attacks', '3D-point-cloud', 'one-to-N', 'NTK-analysis', 'black-box']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11210</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title><link>https://arxiv.org/abs/2511.11030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;State-of-the-art deep vision models (DenseNet121, SwinV2-B, MedMamba) can predict patients' health insurance type (a proxy for socioeconomic status) from chest X-rays with AUC ≈ 0.70 (MIMIC-CXR-JPG) and ≈ 0.68 (CheXpert).&lt;/li&gt;&lt;li&gt;The predictive signal persists after controlling for age, race, and sex, and remains when training within a single racial group; patch-based occlusion shows the signal is diffuse across upper/mid thoracic regions rather than localized.&lt;/li&gt;&lt;li&gt;Authors interpret this as models internalizing non-biological, social or clinical-environment signatures (equipment, care pathways, facility differences), implying medical images are not neutral with respect to social attributes.&lt;/li&gt;&lt;li&gt;Implications: privacy and fairness risks (unintended sensitive attribute inference) and the need to disentangle social 'fingerprints' in clinical data beyond standard dataset balancing or threshold adjustments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi-Yu Chen', 'Rawan Abulibdeh', 'Arash Asgari', "Sebasti\\'an Andr\\'es Cajas Ord\\'o\\~nez", 'Leo Anthony Celi', 'Deirdre Goode', 'Hassan Hamidi', 'Laleh Seyyed-Kalantari', 'Ned McCague', 'Thomas Sounack', 'Po-Chih Kuo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-inference', 'fairness-bias', 'medical-ml', 'dataset-provenance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11030</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title><link>https://arxiv.org/abs/2509.25774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies disproportionate credit assignment across timesteps as a key cause of instability and high variance in policy-gradient training for text-to-image generative samplers.&lt;/li&gt;&lt;li&gt;Proposes Proportionate Credit Policy Optimization (PCPO), which reformulates the objective and reweights timesteps to enforce proportional credit assignment and stabilize training.&lt;/li&gt;&lt;li&gt;Reports faster convergence, improved image quality, and reduced model collapse compared to policy-gradient baselines (including DanceGRPO).&lt;/li&gt;&lt;li&gt;Code released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongjae Lee', 'Jong Chul Ye']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'policy gradient', 'training stability', 'text-to-image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25774</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations</title><link>https://arxiv.org/abs/2508.03209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GeoShield, an adversarial perturbation framework to protect geolocation privacy against vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Designs three modules: feature disentanglement (separate geo vs non-geo features), exposure element identification (localize geo-revealing regions), and scale-adaptive enhancement (optimize global and local perturbations for different resolutions).&lt;/li&gt;&lt;li&gt;Evaluates in black-box settings on challenging benchmarks, claiming stronger privacy protection with minimal visual/semantic impact than prior methods.&lt;/li&gt;&lt;li&gt;States novelty as the first work to apply adversarial perturbations specifically to defend geolocation inference by advanced VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinwei Liu', 'Xiaojun Jia', 'Yuan Xun', 'Simeng Qin', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'privacy (geolocation)', 'vision-language models', 'adversarial defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03209</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title><link>https://arxiv.org/abs/2506.06389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial watermarking (imperceptible PGD perturbations) against Vision Transformers (ViTs) in dermatological/medical image classification.&lt;/li&gt;&lt;li&gt;Evaluates transferability of attacks to CNN architectures and measures effectiveness of adversarial training as a defense.&lt;/li&gt;&lt;li&gt;Finds ViTs can suffer large accuracy drops (as low as 27.6%) from attacks, while adversarial training can recover performance (reported up to 90.0%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rifat Sadik', 'Tanvir Rahman', 'Arpan Bhattacharjee', 'Bikash Chandra Halder', 'Ismail Hossain', 'Rifat Sarker Aoyon', 'Md. Golam Rabiul Alam', 'Jia Uddin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'watermarking', 'transferability', 'adversarial-training', 'vision-transformers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06389</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title><link>https://arxiv.org/abs/2505.01267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes diffusion-based adversarial purification in the frequency domain by decomposing images into amplitude and phase spectra and showing adversarial damage increases with frequency.&lt;/li&gt;&lt;li&gt;Shows existing diffusion purification methods tend to damage all frequency components, causing excessive semantic loss.&lt;/li&gt;&lt;li&gt;Proposes a frequency-aware purification method that preserves low-frequency amplitude from the adversarial image and constrains phase updates to a designated range during the reverse diffusion process.&lt;/li&gt;&lt;li&gt;Reports significant empirical improvements over many current defenses on image data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gaozheng Pei', 'Ke Ma', 'Yingfei Sun', 'Qianqian Xu', 'Qingming Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'diffusion models', 'frequency analysis', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01267</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering</title><link>https://arxiv.org/abs/2501.01371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CLIP-UP, a lightweight method that uses CLIP-based similarity measures to detect unanswerable Visual Question Answering (VQA) queries by assessing question–image alignment.&lt;/li&gt;&lt;li&gt;Adds only a small number of trainable layers while keeping the original VLM weights frozen, enabling efficient training and deployment.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection of unanswerable questions across multiple VQA models and benchmarks for both multiple-choice and open-ended settings, while preserving original task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Vardi', 'Oron Nir', 'Ariel Shamir']&lt;/li&gt;&lt;li&gt;Tags: ['VQA', 'unanswerability detection', 'model safety', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01371</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Causal Interpretability for Adversarial Robustness: A Hybrid Generative Classification Approach</title><link>https://arxiv.org/abs/2412.20025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid ensemble combining a pretrained discriminative feature extractor with a top-level generative classifier (deep latent variable model) trained via variational Bayes to model adversarial input distributions.&lt;/li&gt;&lt;li&gt;Claims improved adversarial robustness against white-box attacks on CIFAR-10 and CIFAR-100 (and preliminary Tiny-ImageNet results) without adversarial training.&lt;/li&gt;&lt;li&gt;Analyzes correlations between interpretability metrics (counterfactual and feature-interaction based) and adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunheng Zhao', 'Pierluigi Pisu', 'Gurcan Comert', 'Negash Begashaw', 'Varghese Vaidyan', 'Nina Christine Hubig']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial examples', 'generative models', 'interpretability', 'white-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.20025</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title><link>https://arxiv.org/abs/2512.07687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluShift++: a method hypothesizing that hallucinations in MLLMs correspond to measurable irregularities in internal layer dynamics.&lt;/li&gt;&lt;li&gt;Extends prior hallucination-detection approaches from text-only LLMs to multimodal (vision+language) models by analyzing layer-wise representation shifts.&lt;/li&gt;&lt;li&gt;Positions this internal-analysis approach as an alternative to external LLM evaluators, aiming for more robust and domain-adaptive hallucination detection.&lt;/li&gt;&lt;li&gt;Provides code release for reproducing/using the method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sujoy Nath', 'Arkaprabha Basu', 'Sharanya Dasgupta', 'Swagatam Das']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'MLLM safety', 'internal representation analysis', 'robustness/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07687</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.07130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mimir, a hierarchical dual-system end-to-end driving framework that generates trajectories from goal points while estimating goal uncertainty using a Laplace distribution.&lt;/li&gt;&lt;li&gt;Introduces a multi-rate guidance mechanism that predicts extended goal points in advance to reduce inference latency of the high-level guidance module.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and performance on Navhard and Navtest benchmarks (≈20% EPDMS improvement) while achieving ~1.6x faster high-level inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zebin Xing', 'Yupeng Zheng', 'Qichao Zhang', 'Zhixing Ding', 'Pengxuan Yang', 'Songen Gu', 'Zhongpu Xia', 'Dongbin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'uncertainty-estimation', 'robustness', 'hierarchical-planning', 'diffusion-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07130</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title><link>https://arxiv.org/abs/2512.06665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new definition of 'similar inputs' that incorporates differences in model outputs and introduces a new robustness metric for feature attribution methods.&lt;/li&gt;&lt;li&gt;Develops a GAN-based technique to generate input perturbations tailored for this robustness evaluation.&lt;/li&gt;&lt;li&gt;Provides a comprehensive evaluation comparing the new metric and generation method against existing metrics and state-of-the-art attribution techniques, arguing current metrics conflate attribution method weaknesses with model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panagiota Kiourti', 'Anu Singh', 'Preeti Duraipandian', 'Weichao Zhou', 'Wenchao Li']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'attribution-robustness', 'evaluation-metrics', 'GANs', 'adversarial-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06665</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</title><link>https://arxiv.org/abs/2512.06589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OmniSafeBench-MM: a unified, open-source benchmark and toolbox for evaluating multimodal jailbreak attacks and defenses.&lt;/li&gt;&lt;li&gt;Integrates 13 attack methods, 15 defenses, and a diverse dataset covering 9 risk domains and 50 fine-grained categories across consultative, imperative, and declarative query types.&lt;/li&gt;&lt;li&gt;Proposes a three-dimensional evaluation protocol measuring harmfulness (granular multi-level), intent alignment, and response detail to enable nuanced safety-utility analysis.&lt;/li&gt;&lt;li&gt;Reports extensive experiments on 10 open-source and 8 closed-source MLLMs and releases code for reproducible red teaming and defense evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaojun Jia', 'Jie Liao', 'Qi Guo', 'Teng Ma', 'Simeng Qin', 'Ranjie Duan', 'Tianlin Li', 'Yihao Huang', 'Zhitao Zeng', 'Dongxian Wu', 'Yiming Li', 'Wenqi Ren', 'Xiaochun Cao', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'multimodal models', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06589</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title><link>https://arxiv.org/abs/2512.07730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAVE, a training-free framework that mitigates object hallucination in Multimodal LLMs by steering model activations along Sparse Autoencoder (SAE) latent features identified as visual-understanding features.&lt;/li&gt;&lt;li&gt;Uses a binary object-presence QA probe to identify SAE features most indicative of visual processing, then steers the model along these features to reinforce grounded visual understanding.&lt;/li&gt;&lt;li&gt;Reports consistent gains across benchmarks (e.g., ~10 percentage-point improvement on CHAIR_S, improvements on POPE and MMHal-Bench) and across multiple models/layers, with analyses showing reduced uncertain object-token generation and increased attention to image tokens.&lt;/li&gt;&lt;li&gt;Code is released and evaluations claim robustness and generalizability of the approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangha Park', 'Seungryong Yoo', 'Jisoo Mok', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'hallucination mitigation', 'robustness', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07730</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimization-Guided Diffusion for Interactive Scene Generation</title><link>https://arxiv.org/abs/2512.07661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OMEGA, an optimization-guided, training-free framework that re-anchors diffusion sampling via constrained optimization to enforce physical and social consistency in generated driving scenes.&lt;/li&gt;&lt;li&gt;Formulates ego-attacker interactions as a game-theoretic optimization in distribution space to approximate Nash equilibria and produce realistic, safety-critical adversarial scenarios for autonomous vehicles.&lt;/li&gt;&lt;li&gt;Evaluated on nuPlan and Waymo, showing large gains in physically/behaviorally valid scene rates and producing up to 5× more near-collision frames (time-to-collision &lt; 3s) for stress-testing while maintaining realism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiaho Li', 'Naisheng Ye', 'Tianyu Li', 'Kashyap Chitta', 'Tuo An', 'Peng Su', 'Boyang Wang', 'Haiou Liu', 'Chen Lv', 'Hongyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous vehicle safety', 'adversarial scenario generation', 'diffusion models', 'red teaming', 'simulation-based testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07661</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title><link>https://arxiv.org/abs/2512.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free self-correction framework for VLMs that iteratively refines responses via uncertainty-guided visual re-attention while keeping the pretrained model frozen.&lt;/li&gt;&lt;li&gt;Combines multiple uncertainty signals (token entropy, attention dispersion, semantic consistency, claim confidence) to identify under-explored image regions and perform attention-guided cropping for re-evaluation.&lt;/li&gt;&lt;li&gt;Validated on POPE and MMHAL BENCH using Qwen2.5-VL-7B, showing a 9.8 percentage-point reduction in hallucination rates and a 4.7-point improvement in object existence accuracy on adversarial splits.&lt;/li&gt;&lt;li&gt;Releases code/methodology and emphasizes grounding corrections in visual evidence to improve reliability of multimodal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kassoum Sanogo', 'Renzo Ardiccioni']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'vision-language models', 'uncertainty quantification', 'robustness/grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07564</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior</title><link>https://arxiv.org/abs/2512.07498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LR-GCN, a Laplacian-regularized graph convolutional network to detect DeepFakes from noisy, unordered, or incomplete face sequences.&lt;/li&gt;&lt;li&gt;Introduces Order-Free Temporal Graph Embedding (OF-TGE) to build an adaptive sparse graph from frame-wise CNN features, robust to shuffled/missing/corrupted faces.&lt;/li&gt;&lt;li&gt;Applies dual-level sparsity (on graph structure and node features) to suppress invalid faces and an explicit Graph Laplacian Spectral Prior as a task-driven spectral band-pass to highlight forgery artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results and improved robustness on FF++, Celeb-DFv2, and DFDC, including resilience to occlusions, missing faces, and adversarial perturbations of face detections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Chung Hsu', 'Shao-Ning Chen', 'Chia-Ming Lee', 'Yi-Fang Wang', 'Yi-Shiuan Chou']&lt;/li&gt;&lt;li&gt;Tags: ['DeepFake detection', 'Robustness', 'Adversarial robustness', 'Graph neural networks', 'Spectral methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07498</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</title><link>https://arxiv.org/abs/2512.07426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that deep-learning-based WSI normalization can introduce realistic but spurious hallucinated content that is hard to detect visually and can harm downstream clinical analysis.&lt;/li&gt;&lt;li&gt;Demonstrates that conventional evaluation metrics miss these failures, and that retraining on real-world clinical data increases hallucination frequency compared to public datasets.&lt;/li&gt;&lt;li&gt;Proposes a novel image comparison measure to automatically detect hallucinations in normalized outputs and uses it to systematically evaluate several well-cited normalization methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karel Moens', 'Matthew B. Blaschko', 'Tinne Tuytelaars', 'Bart Diricx', 'Jonas De Vylder', 'Mustafa Yousif']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'hallucination-detection', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07426</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</title><link>https://arxiv.org/abs/2512.07351</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepAgent, a dual-agent multimodal framework for deepfake detection: Agent-1 uses an AlexNet-based CNN on video frames; Agent-2 detects audio-visual inconsistencies using acoustic features, Whisper transcriptions, and EasyOCR-extracted frame text.&lt;/li&gt;&lt;li&gt;Decisions from the two agents are fused via a Random Forest meta-classifier to leverage complementary decision boundaries and improve robustness.&lt;/li&gt;&lt;li&gt;Evaluated on Celeb-DF, FakeAVCeleb, and DeepFakeTIMIT with component- and fusion-level metrics; reports strong cross-dataset performance (e.g., meta-classifier 97.49% on DeepFakeTIMIT).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayeem Been Zaman', 'Wasimul Karim', 'Arefin Ittesafun Abian', 'Reem E. Mohamed', 'Md Rafiqul Islam', 'Asif Karim', 'Sami Azam']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal fusion', 'audio-visual consistency', 'forensic robustness', 'ensemble/meta-classifier']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07351</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title><link>https://arxiv.org/abs/2512.07247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdLift, the first safeguard that lifts bounded 2D adversarial perturbations into 3D Gaussian Splatting (3DGS) representations to prevent instruction-driven editing across viewpoints.&lt;/li&gt;&lt;li&gt;Proposes a Lifted-PGD optimization combining gradient truncation at the rendered-image editing model and an image-to-Gaussian fitting step to strictly enforce image-level perturbation bounds while transferring perturbations into 3D Gaussians.&lt;/li&gt;&lt;li&gt;Alternates between truncated gradient steps and image-to-Gaussian fitting to achieve view-generalizable, invisible protections; demonstrates empirical effectiveness against state-of-the-art 2D and 3DGS editing pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziming Hong', 'Tianyu Huang', 'Runnan Chen', 'Shanshan Ye', 'Mingming Gong', 'Bo Han', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbation', '3D Gaussian Splatting', 'content protection', 'robustness to instruction-driven editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07247</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title><link>https://arxiv.org/abs/2512.07228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses defenses against DeepFake face swapping by embedding invisible protective perturbations into images to prevent realistic identity forgeries.&lt;/li&gt;&lt;li&gt;Shows standard Expectation over Transformation (EOT) with uniform sampling is suboptimal via analysis of 30 transformations across six categories; robustness is highly sensitive to training transformation choice.&lt;/li&gt;&lt;li&gt;Proposes Expectation Over Learned distribution of Transformation (EOLT): a policy network learned with reinforcement learning that prioritizes critical transformations and generates instance-specific perturbations.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains over prior methods (≈26% higher average robustness, up to 30% improvements on hard transformation categories).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengyang Yao', 'Lin Li', 'Ke Sun', 'Jianing Qiu', 'Huiping Chen']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake defense', 'adversarial robustness', 'protective perturbations', 'expectation-over-transformation', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07228</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing</title><link>https://arxiv.org/abs/2512.07166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the SPPE dataset containing protected surrogates and their MLLM-edited versions to enable direct assessment of privacy recovery quality.&lt;/li&gt;&lt;li&gt;Formulates privacy recovery as a guided generation task conditioned on complementary multimodal signals and proposes a unified reconstruction approach to restore private content while preserving edit fidelity.&lt;/li&gt;&lt;li&gt;Evaluates the method on SPPE and InstructPix2Pix, showing generalization across visual content and editing tasks and demonstrating a trade-off between privacy protection and MLLM usability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Xu', 'Yibing Liu', 'Peilin Chen', 'Yung-Hui Li', 'Shiqi Wang', 'Sam Kwong']&lt;/li&gt;&lt;li&gt;Tags: ['Privacy leakage', 'Privacy recovery (model inversion)', 'MLLM editing', 'Dataset/Benchmark (SPPE)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07166</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title><link>https://arxiv.org/abs/2512.07141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Think-Reflect-Revise (TRR), a three-stage training framework (think → reflect → revise) to enable policy-guided self-reflection in LVLMs to prevent unsafe outputs.&lt;/li&gt;&lt;li&gt;Constructs ReSafe, a 5,000-example reflective safety reasoning dataset, and uses it to fine-tune models to initialize reflective behavior.&lt;/li&gt;&lt;li&gt;Further reinforces policy-guided reflection via reinforcement learning to improve safety alignment and jailbreak robustness.&lt;/li&gt;&lt;li&gt;Reports large safety gains (e.g., safe response rate on Qwen2.5-VL-7B from 42.8% to 87.7%) while maintaining performance on general multimodal benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fenghua Weng', 'Chaochao Lu', 'Xia Hu', 'Wenqi Shao', 'Wenjie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM safety', 'jailbreak mitigation', 'self-reflection', 'reinforcement learning', 'safety dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07141</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2512.06774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RDSplat, a watermarking method for 3D Gaussian Splatting that targets components preserved by diffusion-based editing to maintain provenance.&lt;/li&gt;&lt;li&gt;Embeds watermarks into low-frequency Gaussians via coordinated covariance regularization and 2D filtering in a multi-domain 3DGS-native framework.&lt;/li&gt;&lt;li&gt;Uses adversarial fine-tuning with a diffusion proxy (Gaussian blur as an efficient surrogate) to improve robustness against diffusion-based editing.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness vs diffusion editing while preserving watermark invisibility on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longjie Zhao', 'Ziming Hong', 'Zhenyang Ren', 'Runnan Chen', 'Mingming Gong', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'robustness', 'diffusion editing', 'adversarial training', '3D Gaussian Splatting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06774</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RunawayEvil: Jailbreaking the Image-to-Video Generative Models</title><link>https://arxiv.org/abs/2512.06674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RunawayEvil, a multimodal jailbreak framework for image-to-video (I2V) generative models that self-evolves via a Strategy-Tactic-Action architecture.&lt;/li&gt;&lt;li&gt;Combines reinforcement learning for strategy customization, LLM-driven strategy exploration, coordinated text jailbreak and image tampering tactics, and automated execution/evaluation of attacks.&lt;/li&gt;&lt;li&gt;Reports large improvements in attack success against commercial I2V models (e.g., Open-Sora 2.0, CogVideoX) and benchmarks (COCO2017), highlighting vulnerabilities in video-generation systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songping Wang', 'Rufan Qian', 'Yueming Lyu', 'Qinglong Liu', 'Linzhuang Zou', 'Jie Qin', 'Songhua Liu', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal red teaming', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06674</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title><link>https://arxiv.org/abs/2512.06562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SUGAR, a scalable generative unlearning framework that removes many identities from 3D-aware generative models without full retraining.&lt;/li&gt;&lt;li&gt;Learns a personalized surrogate latent per identity to divert reconstructions to visually coherent alternatives rather than producing unrealistic outputs or static templates.&lt;/li&gt;&lt;li&gt;Proposes a continual utility preservation objective to prevent degradation as more identities are forgotten (supports simultaneous or sequential forgetting).&lt;/li&gt;&lt;li&gt;Demonstrates removal of up to 200 identities with up to 700% improvement in retention utility over existing baselines; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dung Thuy Nguyen', 'Quang Nguyen', 'Preston K. Robinette', 'Eli Jiang', 'Taylor T. Johnson', 'Kevin Leach']&lt;/li&gt;&lt;li&gt;Tags: ['generative unlearning', 'privacy-preservation', 'model editing', 'data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06562</guid><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</title><link>https://arxiv.org/abs/2512.06376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy of AIGV failure modes (visual artifacts, physically implausible motion, traffic semantics violations) and quantifies their negative impact on detection, tracking, and segmentation.&lt;/li&gt;&lt;li&gt;Releases ADGV-Bench: a driving-focused benchmark with human quality annotations and dense labels across perception tasks.&lt;/li&gt;&lt;li&gt;Proposes ADGVE, a driving-aware evaluator combining static semantics, temporal cues, lane obedience signals, and VLM-guided reasoning to score and filter AIGV clips.&lt;/li&gt;&lt;li&gt;Empirically shows raw AIGVs can degrade AD models but filtering with ADGVE improves video quality metrics and downstream model performance, enabling safe augmentation of real data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinhao Xiang', 'Abhijeet Rastogi', 'Jiawei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'synthetic-data', 'video-generation', 'safety-evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06376</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2512.06373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VG-Refiner, a framework for tool-refined referring grounded reasoning to handle unreliable/erroneous visual tool outputs.&lt;/li&gt;&lt;li&gt;Introduces a two-stage 'think-rethink' mechanism that explicitly analyzes tool feedback and enables corrective responses to poor tool results.&lt;/li&gt;&lt;li&gt;Defines a refinement reward to encourage corrective behavior and proposes two new metrics plus evaluation protocols to measure refinement ability.&lt;/li&gt;&lt;li&gt;Shows improved accuracy and correction ability on referring and grounding benchmarks using a small amount of task-specific data while preserving pretrained capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuji Wang', 'Wenlong Liu', 'Jingxuan Niu', 'Haoji Zhang', 'Yansong Tang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal-safety', 'hallucination-mitigation', 'tool-integration', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06373</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection</title><link>https://arxiv.org/abs/2512.06363</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPL-UAD, a Spoofing-aware Prompt Learning framework to detect both physical presentation attacks and digital forgery attacks in a unified system.&lt;/li&gt;&lt;li&gt;Decouples optimization for physical and digital attacks via a learnable parallel prompt branch with adaptive Spoofing Context Prompt Generation.&lt;/li&gt;&lt;li&gt;Introduces Cues-awareness Augmentation to mine challenging samples using the dual-prompt mechanism, improving robustness to unseen attack types.&lt;/li&gt;&lt;li&gt;Evaluates on the large-scale UniAttackDataPlus dataset and reports significant improvements in unified attack detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiabao Guo', 'Yadian Wang', 'Hui Ma', 'Yuhao Fu', 'Ju Jia', 'Hui Liu', 'Shengeng Tang', 'Lechao Cheng', 'Yunfeng Diao', 'Ajian Liu']&lt;/li&gt;&lt;li&gt;Tags: ['spoofing-detection', 'biometric-security', 'prompt-learning', 'presentation-attacks', 'digital-forgery-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06363</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs</title><link>https://arxiv.org/abs/2512.06258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a common LVLM failure mode: models often know correct answers but select incorrect, unstable reasoning paths (large gap between Pass@K and Pass@1).&lt;/li&gt;&lt;li&gt;Proposes PSO, a two-stage post-training method: (1) Group Relative Policy Optimization (GRPO) using template- and answer-based rewards to encourage structured step-by-step reasoning; (2) online preference optimization with self-evaluation and a Negative Replay Memory (NRM) of hard negatives to discourage repeating bad paths.&lt;/li&gt;&lt;li&gt;Reports empirical gains: PSO prunes invalid reasoning trajectories, improves reasoning accuracy (~+7.4% on average), and yields more stable chains-of-thought.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaoyang Wang', 'Yangfan He', 'Yiyang Zhou', 'Yixuan Wang', 'Jiaqi Liu', 'Peng Xia', 'Zhengzhong Tu', 'Mohit Bansal', 'Huaxiu Yao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reasoning robustness', 'LVLMs', 'chain-of-thought', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06258</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling</title><link>https://arxiv.org/abs/2512.06185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Re-implements CPPN-based and direct-encoding evolutionary 'fooling images' attacks on modern convolutional and transformer classifiers, confirming persistent high-confidence misclassifications.&lt;/li&gt;&lt;li&gt;Finds transformer-based ViT-B/16 particularly susceptible, achieving near-certain misclassification with fewer queries than convolutional models.&lt;/li&gt;&lt;li&gt;Introduces SPOOF, a simple, low-compute, black-box attack using minimal pixel operations to generate unrecognizable high-confidence fooling images.&lt;/li&gt;&lt;li&gt;Shows that retraining with fooling images as an extra class only partially mitigates the vulnerability; SPOOF remains effective with slightly higher query budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ankit Gupta (Michigan State University)', 'Christoph Adami (Michigan State University)', 'Emily Dolson (Michigan State University)']&lt;/li&gt;&lt;li&gt;Tags: ['fooling-images', 'black-box attack', 'model robustness', 'adversarial examples', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06185</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection</title><link>https://arxiv.org/abs/2512.06103</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SpectraIrisPAD: a multispectral iris presentation-attack detection framework using a DINOv2 ViT backbone with learnable spectral positional encoding, token fusion, and contrastive learning to extract band-specific discriminative features.&lt;/li&gt;&lt;li&gt;Introduces MSIrPAD, a new multispectral iris PAD dataset (18,848 images) captured at five NIR wavelengths (800, 830, 850, 870, 980 nm) covering eight PAI categories including textured contact lenses, print, and display attacks.&lt;/li&gt;&lt;li&gt;Evaluates under unseen-attack protocols and reports that SpectraIrisPAD outperforms several state-of-the-art baselines, demonstrating improved robustness and generalization for detecting diverse presentation attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raghavendra Ramachandra', 'Sushma Venkatesh']&lt;/li&gt;&lt;li&gt;Tags: ['biometric-security', 'presentation-attack-detection', 'multispectral-imaging', 'vision-transformer', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06103</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fast and Flexible Robustness Certificates for Semantic Segmentation</title><link>https://arxiv.org/abs/2512.06010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces certifiably robust semantic segmentation networks using built-in Lipschitz constraints that are efficiently trainable and maintain competitive pixel accuracy.&lt;/li&gt;&lt;li&gt;Provides a general framework to compute worst-case performance under l2-bounded attacks across various segmentation performance measures.&lt;/li&gt;&lt;li&gt;Claims real-time compatible certification and demonstrates certification runtime ~600x faster than randomized smoothing on an NVIDIA A100 with comparable certificate strength.&lt;/li&gt;&lt;li&gt;Evaluates tightness of certificates against state-of-the-art adversarial attacks and benchmarks runtime and performance on datasets like Cityscapes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Massena (IRIT-MISFIT', 'DTIPG - SNCF', 'UT3)', 'Corentin Friedrich (IRIT-MISFIT)', 'Franck Mamalet (IRIT-MISFIT)', 'Mathieu Serrurier (IRIT-MISFIT)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'certification', 'semantic segmentation', 'Lipschitz networks', 'runtime/efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06010</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics to evaluate LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Context-&gt;Query and Context-&gt;Answer, and Semantic Entropy Production (SEP) motivated by thermodynamics.&lt;/li&gt;&lt;li&gt;Infers the Q and A transition matrices jointly via convex optimization to compute SF (mapped to [0,1]) and relates higher SF to lower SEP, claiming utility for hallucination detection/control.&lt;/li&gt;&lt;li&gt;Demonstrates the framework on LLM summarization of SEC 10-K filings; metrics are intended for LLM evaluation and hallucination management.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness_evaluation', 'alignment/safety', 'evaluation_metrics', 'information-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The AI Consumer Index (ACE)</title><link>https://arxiv.org/abs/2512.04921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the AI Consumer Index (ACE), a benchmark to assess frontier models on everyday consumer tasks with a hidden 400-case testset across shopping, food, gaming, and DIY.&lt;/li&gt;&lt;li&gt;Open-sources an 80-case dev set (CC-BY) and evaluates 10 frontier models (with websearch) using a novel grading methodology that dynamically verifies which response parts are grounded in retrieved web sources.&lt;/li&gt;&lt;li&gt;Finds top model scores around ~55–56%, significant domain variation (e.g., Shopping &lt;50% for top model), and prevalent hallucinations of key information such as prices, showing a substantial gap between model performance and consumer needs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julien Benchek', 'Rohit Shetty', 'Benjamin Hunsberger', 'Ajay Arun', 'Zach Richards', 'Brendan Foody', 'Osvald Nitski', 'Bertie Vidgen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'hallucination', 'benchmarking', 'grounding', 'consumer-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04921</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs</title><link>https://arxiv.org/abs/2512.04668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAMA (Multi-Agent Memory Attack), a two-phase protocol (Engram seeding + Resonance extraction) to measure PII leakage from multi-agent LLM systems.&lt;/li&gt;&lt;li&gt;Evaluates leakage across six graph topologies (fully connected, ring, chain, binary tree, star, star-ring) for n={4,5,6}, attacker placement, rounds, and base models, measuring recovered PII via exact matching.&lt;/li&gt;&lt;li&gt;Finds topology strongly governs leakage: fully connected graphs leak most, chains leak least; shorter attacker-target distance and higher target centrality increase risk; leakage rises early then plateaus; attribute types differ in leakability.&lt;/li&gt;&lt;li&gt;Provides actionable mitigation guidance (sparse/hierarchical connectivity, maximize attacker-target separation, limit node degree/radius, topology-aware access controls).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinbo Liu', 'Defu Cao', 'Yifei Wei', 'Tianyao Su', 'Yuan Liang', 'Yushun Dong', 'Yue Zhao', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'multi-agent-systems', 'LLM-security', 'information-leakage', 'network-topology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04668</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Content-Preserving Secure Linguistic Steganography</title><link>https://arxiv.org/abs/2511.12565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a content-preserving linguistic steganography paradigm that embeds secret messages without modifying the cover text by transforming MLM output distributions.&lt;/li&gt;&lt;li&gt;Introduces CLstega: uses an augmented masking strategy to select embedding positions, derives target distributions via dynamic distribution steganographic coding, and fine-tunes an MLM to a target MLM that can extract messages from unmodified covers.&lt;/li&gt;&lt;li&gt;Claims perfect security (no observable differences in cover text), 100% extraction success, and improved trade-off between embedding capacity and detectability compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingyun Xiang', 'Chengfu Ou', 'Xu He', 'Zhongliang Yang', 'Yuling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['linguistic steganography', 'covert communication', 'adversarial NLP', 'model manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12565</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title><link>https://arxiv.org/abs/2511.11551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a test-time model-guided policy-shaping method to steer pre-trained RL agents toward specified ethical/behavioral attributes without retraining.&lt;/li&gt;&lt;li&gt;Evaluates approach on the MACHIAVELLI benchmark (134 text-based game environments, thousands of annotated ethical scenarios) and compares against prior training-time methods and general-purpose agents.&lt;/li&gt;&lt;li&gt;Demonstrates controllable trade-offs between reward maximization and alignment, and reports mitigation of ethical violations and power-seeking behaviors via scenario-action attribute classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dena Mujtaba', 'Brian Hu', 'Anthony Hoogs', 'Arslan Basharat']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'RL safety', 'test-time mitigation', 'behavior steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11551</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>General Exploratory Bonus for Optimistic Exploration in RLHF</title><link>https://arxiv.org/abs/2510.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how KL and α-divergence regularization in RLHF can bias exploration toward high-probability regions of a reference model, reducing optimism and discovery.&lt;/li&gt;&lt;li&gt;Introduces the General Exploratory Bonus (GEB), a theoretical framework that counteracts divergence-induced bias via reference-dependent reward regulation and satisfies the optimism principle.&lt;/li&gt;&lt;li&gt;Shows GEB unifies prior heuristic bonuses as special cases and extends across the α-divergence family.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate GEB improves performance on alignment tasks across divergence settings and large language model backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wendi Li', 'Changdae Oh', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'optimistic exploration', 'exploration bonus', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03269</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title><link>https://arxiv.org/abs/2510.00586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Eyes-on-Me, a modular data-poisoning attack that splits poisoned documents into reusable Attention Attractors and Focus Regions to enable scalable RAG poisoning.&lt;/li&gt;&lt;li&gt;Optimizes a small subset of attention heads to steer model attention toward focus regions, allowing near-zero-cost adaptation to new targets and transfer to unseen black-box retrievers and generators.&lt;/li&gt;&lt;li&gt;Evaluated across 18 RAG settings (3 datasets × 2 retrievers × 3 generators), raising average attack success from 21.9% to 57.8%; single attractor transfers without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates a practical, transferable threat to retrieval-augmented generation systems and links attention concentration to model outputs, with implications for security and interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Sian-Yao Huang', 'Cheng-Lin Yang', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['RAG data poisoning', 'adversarial attacks', 'attention steering', 'transferability', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00586</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title><link>https://arxiv.org/abs/2508.09442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies KV-cache in LLM inference as a source of privacy leakage and demonstrates that sensitive user inputs can be reconstructed from cached Key/Value pairs.&lt;/li&gt;&lt;li&gt;Designs three attack vectors: Inversion Attack, Collision Attack, and semantic Injection Attack, showing practical efficacy of KV-cache extraction.&lt;/li&gt;&lt;li&gt;Proposes KV-Cloak, a reversible matrix-based obfuscation combined with operator fusion to protect KV-cache with minimal accuracy loss and low performance overhead.&lt;/li&gt;&lt;li&gt;Extensive experiments show KV-Cloak reduces reconstruction quality to random noise while maintaining model accuracy and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifan Luo', 'Shuo Shao', 'Su Zhang', 'Lijing Zhou', 'Yuke Hu', 'Chenxu Zhao', 'Zhihao Liu', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['KV-cache', 'privacy leakage', 'model inversion', 'inference attacks', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09442</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Process Reward Models That Think</title><link>https://arxiv.org/abs/2504.16828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ThinkPRM, a generative process reward model that uses long chain-of-thought (CoT) verifications to provide step-level rewards while requiring far fewer process labels.&lt;/li&gt;&lt;li&gt;Shows ThinkPRM outperforms discriminative PRMs and LLM-as-a-Judge across benchmarks (ProcessBench, MATH-500, AIME '24) and on out-of-domain subsets, while enabling more effective test-time compute scaling (best-of-N and reward-guided search).&lt;/li&gt;&lt;li&gt;Demonstrates strong data efficiency (competitive performance using ~1% of PRM800K labels) and improved OOD generalization; code and models are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Rishabh Agarwal', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Hao Peng', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Process Reward Models', 'Chain-of-Thought Verification', 'Alignment / Safety Evaluation', 'Model Verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16828</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title><link>https://arxiv.org/abs/2502.14354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that DPO-based multi-objective alignment (MOA) methods face widespread preference conflicts where different objectives prefer different responses, blocking Pareto-optimal improvement.&lt;/li&gt;&lt;li&gt;Proposes constructing Pareto-optimal responses and a self-improving DPO framework in which the LLM self-generates and selects Pareto-optimal responses for self-supervised preference alignment.&lt;/li&gt;&lt;li&gt;Shows empirical improvements in the Pareto front on two datasets compared to various baselines, with code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moxin Li', 'Yuantao Zhang', 'Wenjie Wang', 'Wentao Shi', 'Zhuo Liu', 'Fuli Feng', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-objective optimization', 'preference learning', 'DPO', 'Pareto optimality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14354</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Simplex-Optimized Hybrid Ensemble for Large Language Model Text Detection Under Generative Distribution Drif</title><link>https://arxiv.org/abs/2511.22153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid ensemble for LLM-generated text detection combining a fine-tuned RoBERTa classifier, a curvature-inspired perturbation likelihood score, and a stylometric model, with outputs fused via simplex-constrained weighting.&lt;/li&gt;&lt;li&gt;Frames the ensemble as variance reduction under mixtures of generator distributions and chooses simplex weights by validation search to handle distribution drift.&lt;/li&gt;&lt;li&gt;Evaluates on a 30,000-document corpus including unseen LLM families and paraphrased attack variants, reporting 94.2% accuracy and AUC 0.978 and reduced false positives on scientific articles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepyan Purnama Kristanto', 'Lutfi Hakim', 'Dianni Yusuf']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'robustness', 'distribution shift', 'adversarial paraphrase']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22153</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing and Adaptation</title><link>https://arxiv.org/abs/2511.05852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of how fine-tuning affects previously applied knowledge edits in LLMs, measuring 'edit decay' across configurations.&lt;/li&gt;&lt;li&gt;Finds that edits often decay after fine-tuning, with survival varying by editing method (e.g., AlphaEdit decays more than MEMIT).&lt;/li&gt;&lt;li&gt;Shows fine-tuning only edited layers can effectively remove edits (with minor downstream performance cost), while fine-tuning non-edited layers can impair more edits than full fine-tuning.&lt;/li&gt;&lt;li&gt;Provides empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, highlighting safety risks from persistent covert/malicious edits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinjie Cheng', 'Paul Youssef', 'Christin Seifert', 'J\\"org Schl\\"otterer', 'Zhixue Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-editing', 'model-robustness', 'model-safety', 'fine-tuning', 'backdoor-removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05852</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning</title><link>https://arxiv.org/abs/2510.02324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CASAL, a method that integrates activation-steering benefits into model weights to reduce LLM hallucinations without runtime intervention.&lt;/li&gt;&lt;li&gt;Trains a lightweight submodule in a single transformer layer to make models answer known questions and abstain on unknowns, reducing hallucination by ~30–40% on short-form QA benchmarks.&lt;/li&gt;&lt;li&gt;Claims large compute and data efficiency gains over LoRA-based baselines (SFT, DPO) and demonstrates generalization to out-of-distribution domains and applicability to both text-only and vision-language models, including dense and MoE architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wannan (Winnie)', 'Yang', 'Xinchi Qiu', 'Lei Yu', 'Yuchen Zhang', 'Aobo Yang', 'Narine Kokhlikyan', 'Nicola Cancedda', 'Diego Garcia-Olano']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination reduction', 'activation steering', 'model alignment', 'interpretability', 'multimodal safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02324</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaDetectGPT, an adaptive classifier that augments logits-based LLM detectors by learning a witness function from training data.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the detector.&lt;/li&gt;&lt;li&gt;Shows empirical improvements over prior logits-based methods across various datasets and source LLMs (up to ~37% gains).&lt;/li&gt;&lt;li&gt;Open-source Python implementation provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'logit-based detectors', 'statistical guarantees', 'adaptive detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Non-Collaborative User Simulators for Tool Agents</title><link>https://arxiv.org/abs/2509.23124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a user simulator architecture that models four non-collaborative user behaviors: requesting unavailable services, digressing, expressing impatience, and providing incomplete utterances.&lt;/li&gt;&lt;li&gt;Simulator ensures necessary intents/information are eventually delivered while inducing challenging, realistic non-cooperative interactions for tool agents.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art tool agents on MultiWOZ and τ-bench, showing significant performance degradation and failure modes (e.g., hallucinations, dialogue breakdowns) under non-collaborative conditions.&lt;/li&gt;&lt;li&gt;Provides an extensible framework to stress-test and diagnose dialogue/tool agents against realistic adversarial or uncooperative user behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeonghoon Shim', 'Woojung Song', 'Cheyon Jin', 'Seungwon KooK', 'Yohan Jo']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'user-simulation', 'adversarial-testing', 'dialogue-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23124</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Why Chain of Thought Fails in Clinical Text Understanding</title><link>https://arxiv.org/abs/2509.21933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study evaluating chain-of-thought (CoT) prompting on clinical text: 95 LLMs across 87 real-world EHR tasks in 9 languages and 8 task types.&lt;/li&gt;&lt;li&gt;Main finding: 86.3% of models exhibit consistent performance degradation with CoT prompting; stronger models are more robust while weaker models degrade substantially.&lt;/li&gt;&lt;li&gt;Fine-grained analyses on reasoning length, medical concept alignment, and error profiles using LLM-as-judge and clinical expert evaluation; highlights a tradeoff between interpretability and reliability in clinical settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiageng Wu', 'Kevin Xie', 'Bowen Gu', 'Nils Kr\\"uger', 'Kueiyu Joshua Lin', 'Jie Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'interpretability', 'clinical-ML', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21933</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models</title><link>https://arxiv.org/abs/2506.12758</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a methodology to assess LLM alignment on the democracy–authoritarianism axis using (1) the F-scale psychometric tool, (2) a new FavScore metric for leader favorability, and (3) role-model probing.&lt;/li&gt;&lt;li&gt;Empirical findings: models generally favor democratic values and leaders, but show increased favorability toward authoritarian figures when prompted in Mandarin; models also often cite authoritarian figures as role models outside explicit political contexts.&lt;/li&gt;&lt;li&gt;Provides code and metrics to evaluate geopolitical/political ideology bias beyond conventional left–right axes, enabling broader alignment/safety assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Guzman Piedrahita', 'Irene Strauss', 'Bernhard Sch\\"olkopf', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias-evaluation', 'political-bias', 'multilingual-bias', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12758</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Surveying the MLLM Landscape: A Meta-Review of Current Surveys</title><link>https://arxiv.org/abs/2409.18991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Meta-review of existing surveys on Multimodal Large Language Models (MLLMs), focusing on benchmarks, evaluation methodologies, and survey contributions.&lt;/li&gt;&lt;li&gt;Covers a broad set of topics including foundational concepts, applications, evaluation methods, ethical concerns, and security as one of the examined dimensions.&lt;/li&gt;&lt;li&gt;Performs comparative analysis of prior surveys, identifies trends and underexplored areas, and suggests directions for future MLLM research and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Li', 'Keyu Chen', 'Ziqian Bi', 'Ming Liu', 'Xinyuan Song', 'Zekun Jiang', 'Tianyang Wang', 'Benji Peng', 'Qian Niu', 'Junyu Liu', 'Jinlang Wang', 'Sen Zhang', 'Xuanhe Pan', 'Jiawei Xu', 'Pohsun Feng']&lt;/li&gt;&lt;li&gt;Tags: ['MLLM evaluation', 'benchmarking', 'safety', 'security', 'ethics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.18991</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title><link>https://arxiv.org/abs/2512.07795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonBENCH, a benchmarking library and multi-run protocol to quantify instability in LLM multi-step reasoning (quality and cost) and a public leaderboard.&lt;/li&gt;&lt;li&gt;Finds widespread high instability across models and reasoning strategies: similar average performance can have much wider confidence intervals, and top methods often have higher, less stable costs.&lt;/li&gt;&lt;li&gt;Analyzes how prompts, model families, and scale affect the trade-off between solve rate and stability and promotes variance-aware, reproducible reporting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nearchos Potamitis', 'Lars Klein', 'Akhil Arora']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reproducibility', 'uncertainty-quantification', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07795</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title><link>https://arxiv.org/abs/2512.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free self-correction framework for VLMs that iteratively refines responses via uncertainty-guided visual re-attention while keeping the pretrained model frozen.&lt;/li&gt;&lt;li&gt;Combines multiple uncertainty signals (token entropy, attention dispersion, semantic consistency, claim confidence) to identify under-explored image regions and perform attention-guided cropping for re-evaluation.&lt;/li&gt;&lt;li&gt;Validated on POPE and MMHAL BENCH using Qwen2.5-VL-7B, showing a 9.8 percentage-point reduction in hallucination rates and a 4.7-point improvement in object existence accuracy on adversarial splits.&lt;/li&gt;&lt;li&gt;Releases code/methodology and emphasizes grounding corrections in visual evidence to improve reliability of multimodal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kassoum Sanogo', 'Renzo Ardiccioni']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'vision-language models', 'uncertainty quantification', 'robustness/grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07564</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels</title><link>https://arxiv.org/abs/2512.07474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage pipeline to make LLM-driven novel characters stay in-character and obey story-world constraints: Deep Persona Alignment (DPA) using data-free reinforcement finetuning, and Coherence and Robustness Enhancing (CRE) using a story-time-aware knowledge graph plus retrieval-grounded training.&lt;/li&gt;&lt;li&gt;Addresses persona drift, spoiler leakage, and frame-breaking by enforcing narrative constraints and timeline-aware retrieval during generation.&lt;/li&gt;&lt;li&gt;Evaluates on Jules Verne's Twenty Thousand Leagues Under the Sea with lab ablation studies and a 5-day in-the-wild diary study; reports outperforming GPT-4o on persona metrics and near-perfect coherence/robustness after CRE.&lt;/li&gt;&lt;li&gt;Practical findings: character-first self-training improves believability; explicit story-time constraints improve coherence and interruption resilience for mobile-web deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifei Huang', 'Tianyu Yan', 'Sitong Gong', 'Xiwei Gao', 'Caixin Kang', 'Ruicong Liu', 'Huchuan Lu', 'Bo Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['persona alignment', 'robustness', 'narrative coherence', 'retrieval-grounded finetuning', 'reinforcement finetuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07474</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</title><link>https://arxiv.org/abs/2512.07375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LUNE: a LoRA-based unlearning method that performs negative-only unlearning by updating low-rank adapters while freezing the main model weights.&lt;/li&gt;&lt;li&gt;Targets intermediate representations to suppress or replace specific knowledge (privacy, bias mitigation, knowledge correction) with much lower compute and memory than full fine-tuning or direct weight editing.&lt;/li&gt;&lt;li&gt;Reports comparable effectiveness to full fine-tuning and memory-editing approaches on factual unlearning tasks, with about an order-of-magnitude reduction in computational cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yezi Liu', 'Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'model editing', 'privacy', 'LoRA', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07375</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</title><link>https://arxiv.org/abs/2512.07374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Recover-to-Forget (R2F), a method to perform efficient unlearning in LLMs by reconstructing full-model gradient directions from low-rank LoRA adapter updates rather than backpropagating through the full model.&lt;/li&gt;&lt;li&gt;Trains a gradient decoder on a proxy model using gradients from multiple paraphrased prompts to map LoRA-parameter gradients to approximate full-model gradients, enabling transfer to larger or black-box target models.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of cross-model generalization and empirical results showing effective unlearning while preserving overall model performance, avoiding full retraining or access to original training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yezi Liu', 'Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LoRA', 'gradient reconstruction', 'privacy', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07374</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title><link>https://arxiv.org/abs/2512.07222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Function-word De-Attention (FDA) to reduce vulnerability of vision-language models by reducing the influence of function words in cross-attention.&lt;/li&gt;&lt;li&gt;FDA computes original and function-word cross-attention within attention heads and differentially subtracts the latter from the former to improve alignment and robustness.&lt;/li&gt;&lt;li&gt;Evaluated across multiple models, datasets, tasks, and 6 attack types, showing large drops in attack success rate (ASR) with negligible performance degradation; includes scalability, generalization, zero-shot results and ablations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'cross-modal attacks', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07222</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title><link>https://arxiv.org/abs/2512.07141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Think-Reflect-Revise (TRR), a three-stage training framework (think → reflect → revise) to enable policy-guided self-reflection in LVLMs to prevent unsafe outputs.&lt;/li&gt;&lt;li&gt;Constructs ReSafe, a 5,000-example reflective safety reasoning dataset, and uses it to fine-tune models to initialize reflective behavior.&lt;/li&gt;&lt;li&gt;Further reinforces policy-guided reflection via reinforcement learning to improve safety alignment and jailbreak robustness.&lt;/li&gt;&lt;li&gt;Reports large safety gains (e.g., safe response rate on Qwen2.5-VL-7B from 42.8% to 87.7%) while maintaining performance on general multimodal benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fenghua Weng', 'Chaochao Lu', 'Xia Hu', 'Wenqi Shao', 'Wenjie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM safety', 'jailbreak mitigation', 'self-reflection', 'reinforcement learning', 'safety dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07141</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title><link>https://arxiv.org/abs/2512.06716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of autonomous LLM agents to Indirect Prompt Injection (IPI) attacks and argues existing defenses are fragmented, causing trade-offs between security, functionality, and efficiency.&lt;/li&gt;&lt;li&gt;Proposes the Cognitive Control Architecture (CCA) combining a pre-generated 'Intent Graph' for control-flow and data-flow integrity with a 'Tiered Adjudicator' that detects deviations and performs multi-dimensional deep reasoning to counter complex conditional attacks.&lt;/li&gt;&lt;li&gt;Evaluates CCA on the AgentDojo benchmark, reporting improved resistance to sophisticated attacks while maintaining efficiency and robustness, aiming to provide full-lifecycle cognitive supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Liang', 'Tianze Hu', 'Zaiye Chen', 'Mingjie Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Indirect Prompt Injection', 'Prompt injection defense', 'Agent security', 'Runtime adjudication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06716</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization</title><link>https://arxiv.org/abs/2512.06713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies failures when migrating LLM-based anonymization methods to local small-scale models (LSMs) and attributes collapse in utility to irrational greedy adversarial strategies rather than mere capability gaps.&lt;/li&gt;&lt;li&gt;Models anonymization as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC) and shows greedy attacks drift into irrational over-perturbation.&lt;/li&gt;&lt;li&gt;Proposes RLAA (Attacker-Arbitrator-Anonymizer), a fully localized, training-free framework where an arbitrator filters attacker feedback and enforces a rational early-stopping criterion to prevent utility collapse.&lt;/li&gt;&lt;li&gt;Reports experiments showing improved privacy–utility trade-offs (Pareto improvements) on multiple datasets using the localized method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Donghang Duan', 'Xu Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'adversarial anonymization', 'local LLMs', 'privacy attacks/defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06713</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a controlled evaluation framework with four stress tests: (1) rule deletion (redundant vs essential), (2) contradictory evidence injection, (3) logic-preserving rewrites (contrapositive, De Morgan, double negation, implication, identity, commutativity), and (4) multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates across BERT, Qwen2, and LLaMA-like models; models perform perfectly on base tasks and are robust to redundant-rule deletion and equivalence rewrites, but fail markedly on essential-rule deletion (~25% accuracy) and collapse with explicit contradictions (0%).&lt;/li&gt;&lt;li&gt;Finds LLMs are invariant to semantic-preserving transformations but brittle to missing or conflicting evidence; presents a diagnostic benchmark for isolating reasoning failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'evaluation/benchmarking', 'LLM generalisation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title><link>https://arxiv.org/abs/2512.06343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes per-sample gradient of the Bradley-Terry (BT) loss for reward models and shows gradient norm decomposes into prediction error and representation distance between paired outputs.&lt;/li&gt;&lt;li&gt;Finds representation distance can dominate updates: small-distance pairs get vanishing updates even when misranked, while large-distance pairs get disproportionately large updates, harming fine-grained learning.&lt;/li&gt;&lt;li&gt;Proposes NormBT, a lightweight pair-wise normalization that counteracts representation-distance bias to focus learning on prediction error.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements across LLM backbones and datasets, with notable gains (&gt;5%) on Reasoning category of RewardBench containing many small-distance pairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Xie', 'Andrew Bai', 'Yuanhao Ban', 'Yunqi Hong', 'Haoyu Li', 'Cho-jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'RLHF', 'alignment', 'training dynamics', 'representation bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06343</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment</title><link>https://arxiv.org/abs/2512.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARCANE, a multi-agent framework that represents stakeholder preferences as natural-language rubrics (weighted, verifiable criteria) which can be generated and adapted at test time.&lt;/li&gt;&lt;li&gt;Formulates rubric learning as a reconstruction problem and introduces a regularized Group-Sequence Policy Optimization (GSPO) to train reward models that balance interpretability, faithfulness, and computation.&lt;/li&gt;&lt;li&gt;Evaluates on a corpus of 219 labeled rubrics from GDPVal for long-horizon, multi-step tasks with tool use; shows rubrics yield compact, legible evaluations and enable configurable trade-offs without retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlie Masters', "Marta Grze\\'skiewicz", 'Stefano V. Albrecht']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward models', 'interpretability', 'test-time adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06196</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Road of Adaptive AI for Precision in Cybersecurity</title><link>https://arxiv.org/abs/2512.06048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shares lessons from designing, building, and operating production-grade generative AI pipelines for cybersecurity.&lt;/li&gt;&lt;li&gt;Emphasizes continual adaptation mechanisms (retrieval- and model-level) to keep pace with changing knowledge, tooling, and threats.&lt;/li&gt;&lt;li&gt;Provides practical guidance and best practices for robustness, precision, and auditability in cyber defense deployments.&lt;/li&gt;&lt;li&gt;Identifies open research directions for making GenAI more robust and auditable in security contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Garg']&lt;/li&gt;&lt;li&gt;Tags: ['AI for cybersecurity', 'model adaptation', 'retrieval-augmented generation', 'robustness', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06048</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda/framework for AI decision-support agents that act as partners in experts' collaborative cognitive processes.&lt;/li&gt;&lt;li&gt;Argues current LLM-based support fails to deliver complementarity; proposes agents that co-construct mental models, articulate/revise goals, and jointly test causal hypotheses with human experts.&lt;/li&gt;&lt;li&gt;Outlines challenges around training ecologies, representations and interaction protocols for co-authored models, and evaluation metrics centered on trust, complementarity, and learning from joint outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'decision-support', 'safety-evaluation', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</title><link>https://arxiv.org/abs/2512.07783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a controlled experimental framework isolating contributions of pre-training, mid-training, and RL-based post-training on reasoning LMs using synthetic, traceable tasks.&lt;/li&gt;&lt;li&gt;Finds RL yields measurable capability gains only when pre-training leaves sufficient headroom and RL data targets tasks at the model's competence boundary.&lt;/li&gt;&lt;li&gt;Shows mid-training materially improves performance under fixed compute, and that process-level rewards reduce reward hacking and improve reasoning fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlie Zhang', 'Graham Neubig', 'Xiang Yue']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning', 'alignment', 'training dynamics', 'reward hacking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07783</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title><link>https://arxiv.org/abs/2512.07687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluShift++: a method hypothesizing that hallucinations in MLLMs correspond to measurable irregularities in internal layer dynamics.&lt;/li&gt;&lt;li&gt;Extends prior hallucination-detection approaches from text-only LLMs to multimodal (vision+language) models by analyzing layer-wise representation shifts.&lt;/li&gt;&lt;li&gt;Positions this internal-analysis approach as an alternative to external LLM evaluators, aiming for more robust and domain-adaptive hallucination detection.&lt;/li&gt;&lt;li&gt;Provides code release for reproducing/using the method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sujoy Nath', 'Arkaprabha Basu', 'Sharanya Dasgupta', 'Swagatam Das']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'MLLM safety', 'internal representation analysis', 'robustness/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07687</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG</title><link>https://arxiv.org/abs/2512.07515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPAD, a method that decomposes each token's probability in a RAG model into seven sources: Query, RAG (retrieved context), Past (previous tokens), Current Token, FFN, Final LayerNorm, and Initial Embedding.&lt;/li&gt;&lt;li&gt;Aggregates source-attribution scores by part-of-speech (POS) tags to identify linguistic categories where unusual source reliance indicates hallucination (e.g., nouns overly driven by Final LayerNorm).&lt;/li&gt;&lt;li&gt;Uses these syntactically aggregated attribution anomalies to detect hallucinations in RAG and reports state-of-the-art detection performance in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengqian Lu', 'Jie Lu', 'Anjin Liu', 'Guangquan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'model interpretability', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07515</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization</title><link>https://arxiv.org/abs/2512.07478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies two training problems for Tool-Integrated Reasoning (TIR) agentic RL: sparse/non-instructive rewards and gradient degradation in Group Relative Policy Optimization (GRPO).&lt;/li&gt;&lt;li&gt;Proposes Progressive Reward Shaping (PRS) — a curriculum-like dense, stage-wise reward scheme (including length-aware BLEU for short-form QA and LLM-as-a-Judge for long-form QA) to reduce reward sparsity and mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Introduces Value-based Sampling Policy Optimization (VSPO) — a GRPO variant that replaces low-value samples with task-value-selected prompts and applies value-smoothing clipping to stabilize gradients.&lt;/li&gt;&lt;li&gt;Empirical results show PRS+VSPO improves convergence, stability, and final performance over PPO, GRPO, CISPO, and SFT baselines on short- and long-form QA benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoran Zhuang', 'Ye Chen', 'Jianghao Su', 'Chao Luo', 'Luhui Liu', 'Xia Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['reward-shaping', 'reinforcement-learning', 'agentic-LLMs', 'alignment', 'training-stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07478</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training Language Models to Use Prolog as a Tool</title><link>https://arxiv.org/abs/2512.07407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tunes a 3B LLM (Qwen2.5-3B-Instruct) with Group Relative Policy Optimization to use Prolog as an external verifier for model reasoning.&lt;/li&gt;&lt;li&gt;Studies effects of prompt structure, reward composition (execution, syntax, semantics, structure), and inference protocols (single-shot, best-of-N, agentic internal/external Prolog invocation).&lt;/li&gt;&lt;li&gt;Finds RL fine-tuning outperforms supervised fine-tuning; best-of-N with external Prolog verification maximizes GSM8K accuracy, while agentic internal repair improves zero-shot generalization on MMLU variants.&lt;/li&gt;&lt;li&gt;Argues grounding reasoning in formal verification improves reliability and auditability for safety-critical applications; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niklas Mellgren', 'Peter Schneider-Kamp', 'Lukas Galke Poech']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'tool use', 'formal verification', 'reinforcement learning', 'reliability/auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07407</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models</title><link>https://arxiv.org/abs/2512.07288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains instruction-tuned LLMs using pseudo-faithful one-word self-explanations derived from feature attribution to encourage faithful self-explanations.&lt;/li&gt;&lt;li&gt;Finds training improves self-explanation faithfulness across three classification tasks and three explanation styles, with generalization to multi-word explanations and unseen tasks.&lt;/li&gt;&lt;li&gt;Reports cross-style generalization, suggesting training yields broader improvements in faithful self-explanation ability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomoki Doi', 'Masaru Isonuma', 'Hitomi Yanaka']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'self-explanation', 'faithfulness', 'training-generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07288</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models</title><link>https://arxiv.org/abs/2512.07059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies the TEMPEST multi-turn adversarial attack framework to ten frontier LLMs from eight vendors across 1,000 harmful behaviors, generating &gt;97,000 API queries with automated safety-classifier evaluation.&lt;/li&gt;&lt;li&gt;Reports wide vulnerability variance: six models with 96–100% attack success rate (ASR), four models with 42–78% ASR; enabling extended/deliberative reasoning on same architecture reduced ASR from 97% to 42%.&lt;/li&gt;&lt;li&gt;Concludes model scale is not predictive of adversarial robustness, current alignment techniques remain vulnerable to adaptive multi-turn attacks, and deliberative/inference (thinking) modes are a promising defense direction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Young']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-turn adversarial attacks', 'jailbreaking', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07059</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title><link>https://arxiv.org/abs/2512.07015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'retrieval sycophancy' in RAG systems where retrievers surface user-biased documents, causing LLMs to hallucinate with citations.&lt;/li&gt;&lt;li&gt;Proposes FVA-RAG, which replaces inductive verification with an adversarial retrieval policy that generates 'kill queries' to find contradictory evidence (deductive falsification).&lt;/li&gt;&lt;li&gt;Introduces a dual-verification mechanism that weighs draft answers against the retrieved 'anti-context' to reduce sycophantic hallucinations.&lt;/li&gt;&lt;li&gt;Reports preliminary experiments showing improved robustness on a dataset of common misconceptions and frames the approach as an inference-time red team for factual generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Ravishankara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'RAG robustness', 'adversarial retrieval', 'hallucination mitigation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07015</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI</title><link>https://arxiv.org/abs/2512.06922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs both enable forensic linguistic analysis (scalable corpus analysis, embedding-based authorship attribution) and threaten it via style mimicry, authorship obfuscation, and synthetic text proliferation.&lt;/li&gt;&lt;li&gt;Current AI-text detection approaches (classifier-based, stylometric, watermarking) have substantial limitations: high false positives for non-native writers and susceptibility to adversarial techniques (e.g., homoglyph substitution).&lt;/li&gt;&lt;li&gt;These technical uncertainties have legal implications for admissibility under standards like Daubert and Kumho Tire, challenging the scientific credibility of forensic linguistics evidence.&lt;/li&gt;&lt;li&gt;Authors recommend methodological reconfiguration: hybrid human-AI workflows, explainable detection frameworks beyond binary classifiers, and validation regimes measuring error and bias across diverse populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['George Mikros']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'adversarial attacks', 'forensic linguistics', 'robustness', 'legal admissibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06922</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>"The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ</title><link>https://arxiv.org/abs/2512.06732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImplicitBBQ, an extension of the Bias Benchmark for QA that uses implicitly cued protected attributes (names, cultural cues, traits) across 6 categories.&lt;/li&gt;&lt;li&gt;Evaluates GPT-4o on ImplicitBBQ and finds consistent performance declines compared to explicit cues, up to a 7% drop in the sexual orientation subcategory.&lt;/li&gt;&lt;li&gt;Argues that existing explicit-bias benchmarks miss important implicit biases and provides a tool for more nuanced fairness and safety evaluation of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aarushi Wagh', 'Saniya Srivastava']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'benchmarking', 'safety-evaluation', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06732</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models</title><link>https://arxiv.org/abs/2512.06711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a parameter-efficient fine-tuning method for instruction-tuning that freezes the backbone and updates a low-dimensional projection subspace.&lt;/li&gt;&lt;li&gt;Integrates differential privacy via gradient clipping and adaptive noise allocation within a collaborative optimization framework to reduce privacy budget consumption.&lt;/li&gt;&lt;li&gt;Claims improved training stability, robustness under diverse/uncertain data conditions, and better trade-offs among accuracy, privacy, and parameter efficiency compared to baselines.&lt;/li&gt;&lt;li&gt;Provides experiments across hyperparameters, environments, and data sensitivity to validate privacy-preserving instruction adaptation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yulin Huang', 'Yaxuan Luan', 'Jinxu Guo', 'Xiangchen Song', 'Yuchen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'parameter-efficient-fine-tuning', 'privacy-preserving-training', 'instruction-tuning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06711</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models</title><link>https://arxiv.org/abs/2512.06515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ProSocialAlign, a test-time, parameter-efficient method to steer LM generations toward safe, empathetic, and value-aligned responses without retraining.&lt;/li&gt;&lt;li&gt;Frames safety as lexicographic constrained generation: hard constraints to block harmful continuations, then optimize prosocial quality within the safe set.&lt;/li&gt;&lt;li&gt;Introduces directional regulation (subtracting a learned "harm vector" in parameter space) and a preference-aware autoregressive reward model with gradient conflict resolution for controllable decoding.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art reductions in unsafe leakage and improved alignment across multiple safety benchmarks and evaluation metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Somnath Banerjee', 'Sayan Layek', 'Sayantan Adak', 'Mykola Pechenizkiy', 'Animesh Mukherjee', 'Rima Hazra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'test-time intervention', 'harm mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06515</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models</title><link>https://arxiv.org/abs/2512.06266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Nanbeige4-3B, a small (3B) language model pretrained on 23T tokens and finetuned on &gt;30M instruction examples, claiming competitive performance with larger models.&lt;/li&gt;&lt;li&gt;Proposes training innovations: Fine-Grained Warmup-Stable-Decay (FG-WSD) scheduler for pretraining and a joint deliberative-generation + chain-of-thought reconstruction mechanism to improve SFT data quality.&lt;/li&gt;&lt;li&gt;Uses Dual Preference Distillation (DPD) to distill reasoning capabilities and a multi-stage reinforcement learning phase with verifiable rewards and preference modeling to improve reasoning and human alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Yang', 'Guangyue Peng', 'Jiaying Zhu', 'Ran Le', 'Ruixiang Feng', 'Tao Zhang', 'Wei Ruan', 'Xiaoqi Liu', 'Xiaoxue Cheng', 'Xiyun Xu', 'Yang Song', 'Yanzipeng Gao', 'Yiming Jia', 'Yun Xing', 'Yuntao Wen', 'Zekai Wang', 'Zhenwei An', 'Zhicong Sun', 'Zongchao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['small-LLM', 'alignment', 'RLHF', 'distillation', 'instruction-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06266</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a lightweight, logit-based method to detect hidden conversational escalation by measuring how an LLM's output shifts the dialogue's affective state.&lt;/li&gt;&lt;li&gt;Targets implicit harm (affective drift and repeated emotional reinforcement) that standard toxicity filters and external classifiers miss.&lt;/li&gt;&lt;li&gt;Designed for real-time monitoring as a guardrail alternative to slower clinical rubrics or external classifiers, aimed at reducing gradual escalation in chatbot interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM guardrails', 'harm detection', 'affective computing', 'real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Empathy by Design: Aligning Large Language Models for Healthcare Dialogue</title><link>https://arxiv.org/abs/2512.06097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Direct Preference Optimization (DPO)-based alignment framework to improve factual correctness, semantic coherence, and human-centric qualities (empathy, politeness, simplicity) in caregiver–patient dialogues.&lt;/li&gt;&lt;li&gt;Fine-tunes domain-adapted LLMs using pairwise human preference data where preferred responses emphasize supportive, accessible communication and rejected ones are overly technical or prescriptive.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in semantic alignment, factual accuracy, and human-centric evaluation scores across multiple open and proprietary LLMs, outperforming baselines and some commercial systems.&lt;/li&gt;&lt;li&gt;Provides open-source code to reproduce the preference-based alignment pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emre Umucu', 'Guillermina Solis', 'Leon Garza', 'Emilia Rivas', 'Beatrice Lee', 'Anantaa Kotal', 'Aritran Piplai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI safety', 'healthcare NLP', 'preference-based fine-tuning (DPO)', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06097</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics to evaluate LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Context-&gt;Query and Context-&gt;Answer, and Semantic Entropy Production (SEP) motivated by thermodynamics.&lt;/li&gt;&lt;li&gt;Infers the Q and A transition matrices jointly via convex optimization to compute SF (mapped to [0,1]) and relates higher SF to lower SEP, claiming utility for hallucination detection/control.&lt;/li&gt;&lt;li&gt;Demonstrates the framework on LLM summarization of SEC 10-K filings; metrics are intended for LLM evaluation and hallucination management.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness_evaluation', 'alignment/safety', 'evaluation_metrics', 'information-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Impact of Off-Policy Training Data on Probe Generalisation</title><link>https://arxiv.org/abs/2511.17408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how using synthetic/off-policy LLM responses to train probes affects probe generalisation across eight behaviours (e.g., deception, sycophancy).&lt;/li&gt;&lt;li&gt;Finds that generation strategy and domain shifts significantly impact probe performance; same-domain off-policy data often outperforms cross-domain on-policy data. &lt;/li&gt;&lt;li&gt;Shows that successful generalisation from off-policy to incentivised (advantageous) responses predicts generalisation to on-policy data, and flags Deception and Sandbagging as likely to fail off-to-on-policy generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathalie Kirch', 'Samuel Dower', 'Adrians Skapars', 'Ekdeep Singh Lubana', 'Dmitrii Krasheninnikov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'probe generalisation', 'safety evaluation', 'distribution shift', 'behavioral detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17408</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaDetectGPT, an adaptive classifier that augments logits-based LLM detectors by learning a witness function from training data.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the detector.&lt;/li&gt;&lt;li&gt;Shows empirical improvements over prior logits-based methods across various datasets and source LLMs (up to ~37% gains).&lt;/li&gt;&lt;li&gt;Open-source Python implementation provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'logit-based detectors', 'statistical guarantees', 'adaptive detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title><link>https://arxiv.org/abs/2509.25774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies disproportionate credit assignment across timesteps as a key cause of instability and high variance in policy-gradient training for text-to-image generative samplers.&lt;/li&gt;&lt;li&gt;Proposes Proportionate Credit Policy Optimization (PCPO), which reformulates the objective and reweights timesteps to enforce proportional credit assignment and stabilize training.&lt;/li&gt;&lt;li&gt;Reports faster convergence, improved image quality, and reduced model collapse compared to policy-gradient baselines (including DanceGRPO).&lt;/li&gt;&lt;li&gt;Code released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongjae Lee', 'Jong Chul Ye']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'policy gradient', 'training stability', 'text-to-image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25774</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety</title><link>https://arxiv.org/abs/2507.11473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes monitoring models' chain-of-thought (CoT) outputs as an oversight mechanism to detect intent to misbehave.&lt;/li&gt;&lt;li&gt;Argues CoT monitoring is imperfect but a promising complementary safety tool and recommends further research and investment.&lt;/li&gt;&lt;li&gt;Warns that CoT monitorability is fragile and that model development choices can degrade monitorability, advising caution by frontier developers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomek Korbak', 'Mikita Balesni', 'Elizabeth Barnes', 'Yoshua Bengio', 'Joe Benton', 'Joseph Bloom', 'Mark Chen', 'Alan Cooney', 'Allan Dafoe', 'Anca Dragan', 'Scott Emmons', 'Owain Evans', 'David Farhi', 'Ryan Greenblatt', 'Dan Hendrycks', 'Marius Hobbhahn', 'Evan Hubinger', 'Geoffrey Irving', 'Erik Jenner', 'Daniel Kokotajlo', 'Victoria Krakovna', 'Shane Legg', 'David Lindner', 'David Luan', 'Aleksander M\\k{a}dry', 'Julian Michael', 'Neel Nanda', 'Dave Orr', 'Jakub Pachocki', 'Ethan Perez', 'Mary Phuong', 'Fabien Roger', 'Joshua Saxe', 'Buck Shlegeris', "Mart\\'in Soto", 'Eric Steinberger', 'Jasmine Wang', 'Wojciech Zaremba', 'Bowen Baker', 'Rohin Shah', 'Vlad Mikulik']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Chain-of-Thought', 'Model oversight', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11473</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Not to Detect Prompt Injections with an LLM</title><link>https://arxiv.org/abs/2507.05630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the Known-Answer Detection (KAD) defense for prompt injection and identifies a structural vulnerability undermining its security premise.&lt;/li&gt;&lt;li&gt;Designs a practical adaptive black-box attack (DataFlip) that evades KAD, achieving detection rates down to 0% while inducing malicious behavior with ~91% success.&lt;/li&gt;&lt;li&gt;Provides formal characterization of KAD and empirical evaluation showing KAD can be reliably bypassed without white-box access or optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarthak Choudhary', 'Divyam Anshumaan', 'Nils Palumbo', 'Somesh Jha']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM defenses', 'adversarial attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05630</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title><link>https://arxiv.org/abs/2506.06389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial watermarking (imperceptible PGD perturbations) against Vision Transformers (ViTs) in dermatological/medical image classification.&lt;/li&gt;&lt;li&gt;Evaluates transferability of attacks to CNN architectures and measures effectiveness of adversarial training as a defense.&lt;/li&gt;&lt;li&gt;Finds ViTs can suffer large accuracy drops (as low as 27.6%) from attacks, while adversarial training can recover performance (reported up to 90.0%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rifat Sadik', 'Tanvir Rahman', 'Arpan Bhattacharjee', 'Bikash Chandra Halder', 'Ismail Hossain', 'Rifat Sarker Aoyon', 'Md. Golam Rabiul Alam', 'Jia Uddin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'watermarking', 'transferability', 'adversarial-training', 'vision-transformers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06389</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</title><link>https://arxiv.org/abs/2505.11227</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows pure RL problem-solving training can implicitly induce PRM-like introspective capabilities in LLMs, questioning the necessity of explicit process reward models.&lt;/li&gt;&lt;li&gt;Finds existing PRMs underperform simple baselines (e.g., majority voting) on models like DeepSeek-R1 and QwQ-32B.&lt;/li&gt;&lt;li&gt;Proposes Self-PRM (model self-evaluation and reranking) which improves benchmark accuracy but exhibits low precision (&lt;10%) on difficult problems and often misclassifies flawed solutions.&lt;/li&gt;&lt;li&gt;Concludes that problem-solving ability and process-supervision capability co-evolve during RL training and that further RL scaling is needed to improve reward alignment and introspective accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhangying Feng', 'Qianglong Chen', 'Ning Lu', 'Yongqian Li', 'Siqi Cheng', 'Shuangmu Peng', 'Duyu Tang', 'Shengcai Liu', 'Zhirui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward alignment', 'self-evaluation/introspection', 'reinforcement learning for LLMs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11227</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the robustness of adversarial defenses in malware detection systems</title><link>https://arxiv.org/abs/2505.09342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Prioritized Binary Rounding to map continuous adversarial perturbations into binary feature spaces while preserving attack efficacy and small perturbation size.&lt;/li&gt;&lt;li&gt;Proposes the sigma-binary attack, a novel method tailored to binary domains that achieves high evasion rates with minimal feature modifications.&lt;/li&gt;&lt;li&gt;Empirical evaluation on the Malscan dataset shows sigma-binary outperforms existing attacks and reveals severe brittleness in many state-of-the-art defenses (detectors and adversarially trained models).&lt;/li&gt;&lt;li&gt;Findings demonstrate that defenses previously considered robust (including PAD-SMA and adversarial training variants) can be largely bypassed, highlighting gaps in robustness evaluation for binary-constrained malware detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mostafa Jafari', 'Alireza Shameli-Sendi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'malware detection', 'binary feature domains', 'robustness evaluation', 'evasion attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.09342</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Data Taggants: Dataset Ownership Verification via Harmless Targeted Data Poisoning</title><link>https://arxiv.org/abs/2410.09101</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "data taggants": a clean-label targeted data poisoning technique that embeds secret key pairs (out-of-distribution samples + random labels) into a dataset so models trained on it respond to key samples with key labels.&lt;/li&gt;&lt;li&gt;Provides black-box statistical certificates for dataset ownership verification without harming validation accuracy, and evaluates on ImageNet1k with ViT and ResNet using state-of-the-art training recipes.&lt;/li&gt;&lt;li&gt;Demonstrates stealthiness and robustness against defenses and claims superiority over traditional backdoor watermarking approaches, addressing false-positive guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wassim Bouaziz', 'Nicolas Usunier', 'El-Mahdi El-Mhamdi']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'dataset ownership verification', 'watermarking', 'black-box certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.09101</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Data-regularized Reinforcement Learning for Diffusion Models at Scale</title><link>https://arxiv.org/abs/2512.04332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Data-regularized Diffusion Reinforcement Learning (DDRL), which uses forward KL to anchor policy to an off-policy data distribution during RL fine-tuning of diffusion models.&lt;/li&gt;&lt;li&gt;The approach unifies reward maximization with diffusion loss minimization to provide more reliable regularization, aiming to prevent reward hacking (quality degradation, over-stylization, loss of diversity).&lt;/li&gt;&lt;li&gt;Large-scale empirical validation on high-resolution video generation (1M+ GPU hours, 10k double-blind human evals) shows improved human preference and reduced reward-hacking compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Ye', 'Kaiwen Zheng', 'Jiashu Xu', 'Puheng Li', 'Huayu Chen', 'Jiaqi Han', 'Sheng Liu', 'Qinsheng Zhang', 'Hanzi Mao', 'Zekun Hao', 'Prithvijit Chattopadhyay', 'Dinghao Yang', 'Liang Feng', 'Maosheng Liao', 'Junjie Bai', 'Ming-Yu Liu', 'James Zou', 'Stefano Ermon']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward_hacking', 'reinforcement_learning', 'diffusion_models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04332</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</title><link>https://arxiv.org/abs/2512.03994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free, activation-space whitening method to detect policy violations in LLM outputs by treating violation detection as an OOD detection task.&lt;/li&gt;&lt;li&gt;Transforms hidden activations to have near-identity covariance and uses Euclidean norm in that space as a compliance score, requiring only policy text and a few examples.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art performance on a policy benchmark, outperforming traditional guardrails and fine-tuned LLM judge approaches, while remaining lightweight and interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oren Rachmil', 'Roy Betser', 'Itay Gershon', 'Omer Hofman', 'Nitay Yakoby', 'Yuval Meron', 'Idan Yankelev', 'Asaf Shabtai', 'Yuval Elovici', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'policy violation detection', 'OOD detection', 'activation whitening', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03994</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment</title><link>https://arxiv.org/abs/2511.21931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a simple, computationally efficient framework to evaluate whether models 'say what the data says' by deriving a data-driven baseline for feature importance.&lt;/li&gt;&lt;li&gt;Uses an approach inspired by Rubin's Potential Outcomes Framework to estimate each feature's effect on a binary outcome and rank features by their separation power.&lt;/li&gt;&lt;li&gt;Compares these data-derived feature rankings with model-based explanations to provide a model-agnostic, interpretable assessment of model-data alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Salgado', 'Meagan R. Kendall', 'Martine Ceberio']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'model-auditing', 'causal-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21931</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks</title><link>https://arxiv.org/abs/2510.22021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes K-DAREK, an algorithm enhancing Kurkova Kolmogorov-Arnold networks (KKANs) with distance-aware error bounds for uncertainty quantification.&lt;/li&gt;&lt;li&gt;Claims improved computational efficiency and scalability versus ensembles of KANs and Gaussian processes, with demonstrated speed and safety gains in safe-control case studies.&lt;/li&gt;&lt;li&gt;Distance-aware error bounds reflect proximity of test points to training data and are used to provide safer control decisions and zero coverage violations on some real datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masoud Ataei', 'Vikas Dhiman', 'Mohammad Javad Khojasteh']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'safe-control', 'model-certification', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22021</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</title><link>https://arxiv.org/abs/2510.10645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents RetroTrim, a retrosynthesis system that reduces nonsensical/hallucinated reactions by combining diverse reaction-scoring strategies (ML models + chemical databases).&lt;/li&gt;&lt;li&gt;Demonstrates superior filtering of hallucinated reactions and higher number of high-quality synthetic paths on challenging drug-like targets; winning entry in a retrosynthesis challenge.&lt;/li&gt;&lt;li&gt;Analyzes how different scorers capture distinct classes of hallucinations using a labeled dataset of retrosynthetic intermediates.&lt;/li&gt;&lt;li&gt;Proposes a novel expert-chemists-based evaluation protocol and releases 32 benchmark targets and evaluation details to encourage reliable retrosynthesis research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michal Sadowski', "Tadija Radusinovi\\'c", 'Maria Wyrzykowska', 'Lukasz Sztukiewicz', 'Jan Rzymkowski', "Pawe{\\l} W{\\l}odarczyk-Pruszy\\'nski", 'Miko{\\l}aj Sacha', 'Piotr Kozakowski', 'Ruard van Workum', 'Stanislaw Kamil Jastrzebski']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrosynthesis', 'safety evaluation', 'model robustness', 'chemical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10645</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>General Exploratory Bonus for Optimistic Exploration in RLHF</title><link>https://arxiv.org/abs/2510.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how KL and α-divergence regularization in RLHF can bias exploration toward high-probability regions of a reference model, reducing optimism and discovery.&lt;/li&gt;&lt;li&gt;Introduces the General Exploratory Bonus (GEB), a theoretical framework that counteracts divergence-induced bias via reference-dependent reward regulation and satisfies the optimism principle.&lt;/li&gt;&lt;li&gt;Shows GEB unifies prior heuristic bonuses as special cases and extends across the α-divergence family.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate GEB improves performance on alignment tasks across divergence settings and large language model backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wendi Li', 'Changdae Oh', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'optimistic exploration', 'exploration bonus', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03269</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</title><link>https://arxiv.org/abs/2510.02695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAMAC: a Risk-Aware Multimodal Actor-Critic that combines an expressive generative actor (diffusion-based) with a distributional critic and a CVaR term added to a behavioral-cloning loss to learn risk-sensitive multimodal policies in offline RL.&lt;/li&gt;&lt;li&gt;Analyzes out-of-distribution (OOD) actions and shows that behavior-regularization (anchoring to dataset support) effectively suppresses OOD actions in expressive generative policies, clarifying trade-offs versus prior-anchored perturbation schemes.&lt;/li&gt;&lt;li&gt;Implements RAMAC with a diffusion actor, validates on a 2-D risky bandit and Stochastic-D4RL benchmarks, reporting consistent improvements in tail performance (CVaR0.1) while keeping strong average returns; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Fukazawa', 'Kunal Mundada', 'Iman Soltani']&lt;/li&gt;&lt;li&gt;Tags: ['offline reinforcement learning', 'risk-aware / CVaR', 'behavior regularization / OOD suppression', 'robustness / safety', 'generative (diffusion) policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02695</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title><link>https://arxiv.org/abs/2510.00586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Eyes-on-Me, a modular data-poisoning attack that splits poisoned documents into reusable Attention Attractors and Focus Regions to enable scalable RAG poisoning.&lt;/li&gt;&lt;li&gt;Optimizes a small subset of attention heads to steer model attention toward focus regions, allowing near-zero-cost adaptation to new targets and transfer to unseen black-box retrievers and generators.&lt;/li&gt;&lt;li&gt;Evaluated across 18 RAG settings (3 datasets × 2 retrievers × 3 generators), raising average attack success from 21.9% to 57.8%; single attractor transfers without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates a practical, transferable threat to retrieval-augmented generation systems and links attention concentration to model outputs, with implications for security and interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Sian-Yao Huang', 'Cheng-Lin Yang', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['RAG data poisoning', 'adversarial attacks', 'attention steering', 'transferability', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00586</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title><link>https://arxiv.org/abs/2507.16302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses safety-driven unlearning for text-to-image diffusion models to suppress toxic/unsafe behaviors inherited from pretraining.&lt;/li&gt;&lt;li&gt;Introduces ResAlign, which models downstream fine-tuning via a Moreau envelope reformulation to enable efficient gradient estimation that minimizes recovery of harmful behaviors.&lt;/li&gt;&lt;li&gt;Proposes a meta-learning strategy to simulate diverse fine-tuning scenarios, improving generalization and resilience to subsequent benign fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluations across datasets and fine-tuning configurations show ResAlign better retains safety while preserving benign generation quality compared to prior unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Renjie Gu', 'Junjie Wang', 'Leyi Qi', 'Yiming Li', 'Run Wang', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-driven unlearning', 'diffusion models', 'robustness to fine-tuning', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16302</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Sample is Enough to Make Conformal Prediction Robust</title><link>https://arxiv.org/abs/2506.16553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RCP1, a method for robust conformal prediction that achieves robustness to bounded worst-case noise using a single randomized forward pass instead of many smoothing passes.&lt;/li&gt;&lt;li&gt;Key idea: certify the conformal procedure itself (using a binary certificate) rather than individual conformity scores, enabling smaller average prediction sets and lower compute.&lt;/li&gt;&lt;li&gt;Applicable to any black-box model and both classification and regression; also extended to smoothing-based robust conformal risk control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soroush H. Zargarbashi', 'Mohammad Sadegh Akhondzadeh', 'Aleksandar Bojchevski']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'conformal prediction', 'randomized smoothing', 'certified robustness', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16553</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression</title><link>https://arxiv.org/abs/2506.06954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a risk-regularized distributional (quantile-based) action-value iteration algorithm that integrates Conditional Value-at-Risk (CVaR) to enforce safety in RL without complex architectures.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: contraction properties of a risk-sensitive distributional Bellman operator in Wasserstein space and convergence to a unique cost distribution.&lt;/li&gt;&lt;li&gt;Demonstrates in simulation (mobile robot reach-avoid task) better safety-performance trade-offs: more goal successes and fewer collisions than risk-neutral baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Clinton Enwerem', 'Aniruddh G. Puranic', 'John S. Baras', 'Calin Belta']&lt;/li&gt;&lt;li&gt;Tags: ['safe-rl', 'risk-sensitive-rl', 'distributional-rl', 'CVaR', 'robotics-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06954</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Inversion Attacks for Graph Neural Networks</title><link>https://arxiv.org/abs/2506.00808</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the graph unlearning inversion attack: given black-box access to an unlearned GNN and partial graph knowledge, reconstruct edges that were supposedly removed.&lt;/li&gt;&lt;li&gt;Identifies two challenges (confidence-similarity threshold differences and locating unlearned edge endpoints) and introduces TrendAttack, which exploits a theoretical/empirical 'confidence pitfall' and uses adaptive prediction with trend features.&lt;/li&gt;&lt;li&gt;Integrates and extends existing membership inference techniques, applying different similarity thresholds for unlearned vs. retained edges.&lt;/li&gt;&lt;li&gt;Empirical results on four real-world datasets show TrendAttack significantly outperforms prior GNN membership inference baselines, revealing privacy vulnerabilities in current graph unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Zhang', 'Yilong Wang', 'Zhiwei Zhang', 'Xiaorui Liu', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Graph Neural Networks', 'Membership Inference', 'Model Inversion', 'Privacy/Unlearning', 'Adversarial Attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00808</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Judging LLMs: A Simplex Perspective</title><link>https://arxiv.org/abs/2505.21972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Represents LLM judges and candidate outputs as points on an (M-1)-dimensional probability simplex for M-level scoring, yielding geometric/visual proofs for ranking identifiability.&lt;/li&gt;&lt;li&gt;Derives theoretical conditions (e.g., why binary/2-level scoring is more reliable) and uses simplex geometry to analyze when LLM-as-judge rankings are valid.&lt;/li&gt;&lt;li&gt;Proposes Bayesian priors over judge quality to model epistemic uncertainty, enabling sensitivity analyses and producing higher coverage rates than prior procedures.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows LLM-only judging is often robust but can fail on some datasets, motivating explicit modeling of judge uncertainty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Vossler', 'Fan Xia', 'Yifan Mai', 'Adarsh Subbaswamy', 'Jean Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety evaluation', 'epistemic uncertainty', 'Bayesian methods', 'benchmarking/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21972</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains</title><link>https://arxiv.org/abs/2505.19397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of adversarial robustness for Time-Series Foundation Models (TSFMs) using normalized, sparsity-aware perturbation budgets and scale-invariant metrics in white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Finds that small perturbations can reliably induce targeted failure modes (trend flips, malicious drifts) and identifies TSFM-specific vulnerability patterns such as horizon-proximal brittleness and greater susceptibility with longer context windows.&lt;/li&gt;&lt;li&gt;Observes weak cross-model transfer of attacks (model-specific failure modes) and demonstrates that adversarial fine-tuning, even with out-of-domain data, can substantially improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Zhenwei Zhang', 'Shun Zheng', 'Xumeng Wen', 'Jia Li', 'Jiang Bian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'time-series', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19397</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</title><link>https://arxiv.org/abs/2505.10947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes generalized Lyapunov functions by augmenting RL value functions with neural-network residuals to certify stability of learned policies.&lt;/li&gt;&lt;li&gt;Introduces a relaxed multi-step Lyapunov decrease condition (average decrease over multiple steps) that is easier to satisfy than classical one-step decrease.&lt;/li&gt;&lt;li&gt;Demonstrates certification on Gymnasium and DeepMind Control benchmarks and jointly trains controllers with stability certificates to increase certified regions of attraction.&lt;/li&gt;&lt;li&gt;Bridges classical control-theory Lyapunov methods with modern RL value-function learning to make stability certificates more constructible for nonlinear systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kehan Long', "Jorge Cort\\'es", 'Nikolay Atanasov']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'safety', 'Lyapunov-certification', 'control-theory', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10947</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Process Reward Models That Think</title><link>https://arxiv.org/abs/2504.16828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ThinkPRM: a generative process reward model (PRM) that produces long chain-of-thought (CoT) verification traces to verify each solution step.&lt;/li&gt;&lt;li&gt;Achieves strong verification performance using orders of magnitude fewer step-level labels than discriminative PRMs, outperforming LLM-as-a-Judge and discriminative verifiers on multiple benchmarks (ProcessBench, MATH-500, AIME '24).&lt;/li&gt;&lt;li&gt;Shows better out-of-domain generalization and more effective scaling of verification compute (best-of-N and reward-guided search) under the same token budget.&lt;/li&gt;&lt;li&gt;Releases code, data, and models for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Rishabh Agarwal', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Hao Peng', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['process reward models', 'verification', 'chain-of-thought', 'safety-evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16828</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title><link>https://arxiv.org/abs/2502.14354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that DPO-based multi-objective alignment (MOA) methods face widespread preference conflicts where different objectives prefer different responses, blocking Pareto-optimal improvement.&lt;/li&gt;&lt;li&gt;Proposes constructing Pareto-optimal responses and a self-improving DPO framework in which the LLM self-generates and selects Pareto-optimal responses for self-supervised preference alignment.&lt;/li&gt;&lt;li&gt;Shows empirical improvements in the Pareto front on two datasets compared to various baselines, with code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moxin Li', 'Yuantao Zhang', 'Wenjie Wang', 'Wentao Shi', 'Zhuo Liu', 'Fuli Feng', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-objective optimization', 'preference learning', 'DPO', 'Pareto optimality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14354</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act</title><link>https://arxiv.org/abs/2501.12962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides an interdisciplinary analysis bridging EU non-discrimination law and ML algorithmic fairness concepts in the AI Act.&lt;/li&gt;&lt;li&gt;Finds that most non-discrimination rules apply specifically to high-risk AI systems and that regulation covers both input/data requirements and output monitoring.&lt;/li&gt;&lt;li&gt;Identifies inconsistencies and computational feasibility concerns in the AI Act's treatment of fairness, and explores interactions between classical non-discrimination law and the AI Act.&lt;/li&gt;&lt;li&gt;Recommends development of concrete auditing and testing methodologies for detecting discriminatory behavior in AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kristof Meding']&lt;/li&gt;&lt;li&gt;Tags: ['algorithmic fairness', 'AI Act / regulation', 'auditing and testing', 'legal compliance', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.12962</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Model Performance Under Worst-case Subpopulations</title><link>https://arxiv.org/abs/2407.01316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a notion of worst-case model performance over all subpopulations of a given size defined by core attributes Z, capturing continuous attributes and intersectional groups.&lt;/li&gt;&lt;li&gt;Develops a scalable two-stage estimation procedure with finite-sample and dimension-free convergence guarantees for evaluating this worst-case robustness.&lt;/li&gt;&lt;li&gt;Error bounds depend on out-of-sample estimation of conditional performance instead of conservative complexity-based bounds.&lt;/li&gt;&lt;li&gt;Demonstrates practical utility on real datasets to certify robustness and flag unreliable models before deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mike Li', 'Daksh Mittal', 'Hongseok Namkoong', 'Shangzhou Xia']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'worst-case evaluation', 'safety evaluation', 'fairness / intersectionality', 'robustness certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.01316</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attacking All Tasks at Once Using Adversarial Examples in Multi-Task Learning</title><link>https://arxiv.org/abs/2305.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of multi-task learning (MTL) models, including limitations of adapting single-task white-box attacks to MTL.&lt;/li&gt;&lt;li&gt;Proposes Dynamic Gradient Balancing Attack (DGBA), framing simultaneous multi-task attacks as an optimization problem solved via integer linear programming.&lt;/li&gt;&lt;li&gt;Evaluates DGBA on NYUv2 and Tiny-Taxonomy, showing it outperforms baselines against both clean and adversarially trained MTL models.&lt;/li&gt;&lt;li&gt;Finds a trade-off: parameter sharing that improves task accuracy can increase attack transferability and reduce robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijun Zhang', 'Xiao Liu', 'Kaleel Mahmood', 'Caiwen Ding', 'Hui Guan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'multi-task learning', 'robustness', 'white-box attacks', 'optimization/ILP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2305.12066</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda/framework for AI decision-support agents that act as partners in experts' collaborative cognitive processes.&lt;/li&gt;&lt;li&gt;Argues current LLM-based support fails to deliver complementarity; proposes agents that co-construct mental models, articulate/revise goals, and jointly test causal hypotheses with human experts.&lt;/li&gt;&lt;li&gt;Outlines challenges around training ecologies, representations and interaction protocols for co-authored models, and evaluation metrics centered on trust, complementarity, and learning from joint outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'decision-support', 'safety-evaluation', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title><link>https://arxiv.org/abs/2512.07795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonBENCH, a benchmarking library and multi-run protocol to quantify instability in LLM multi-step reasoning (quality and cost) and a public leaderboard.&lt;/li&gt;&lt;li&gt;Finds widespread high instability across models and reasoning strategies: similar average performance can have much wider confidence intervals, and top methods often have higher, less stable costs.&lt;/li&gt;&lt;li&gt;Analyzes how prompts, model families, and scale affect the trade-off between solve rate and stability and promotes variance-aware, reproducible reporting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nearchos Potamitis', 'Lars Klein', 'Akhil Arora']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reproducibility', 'uncertainty-quantification', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07795</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2512.07761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates black-box multi-turn jailbreak attacks on LLMs as a reinforcement learning problem optimizing final-turn harmfulness.&lt;/li&gt;&lt;li&gt;Introduces two auxiliary process rewards to mitigate sparse supervision: control intermediate output harmfulness and preserve semantic relevance across turns.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows consistently higher attack success rates across multiple models; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiqiao Xiong', 'Ouxiang Li', 'Zhuo Liu', 'Moxin Li', 'Wentao Shi', 'Fuli Feng', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'adversarial prompting', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07761</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title><link>https://arxiv.org/abs/2512.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free self-correction framework for VLMs that iteratively refines responses via uncertainty-guided visual re-attention while keeping the pretrained model frozen.&lt;/li&gt;&lt;li&gt;Combines multiple uncertainty signals (token entropy, attention dispersion, semantic consistency, claim confidence) to identify under-explored image regions and perform attention-guided cropping for re-evaluation.&lt;/li&gt;&lt;li&gt;Validated on POPE and MMHAL BENCH using Qwen2.5-VL-7B, showing a 9.8 percentage-point reduction in hallucination rates and a 4.7-point improvement in object existence accuracy on adversarial splits.&lt;/li&gt;&lt;li&gt;Releases code/methodology and emphasizes grounding corrections in visual evidence to improve reliability of multimodal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kassoum Sanogo', 'Renzo Ardiccioni']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'vision-language models', 'uncertainty quantification', 'robustness/grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07564</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</title><link>https://arxiv.org/abs/2512.07472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode in Vision-Language-Action (VLA) models called the 'Memory Trap' where models reproduce memorized trajectories under distribution shift instead of adapting to the scene.&lt;/li&gt;&lt;li&gt;Proposes Affordance Field Intervention (AFI): a lightweight hybrid approach that uses 3D Spatial Affordance Fields (SAFs) to detect memory traps, reposition the robot toward high-affordance regions, propose affordance-driven waypoints, and score/select trajectories by cumulative affordance.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness on real-world robotic platforms (≈23.5% average improvement across VLA backbones) and on the LIBERO-Pro benchmark (20.2% improvement) under out-of-distribution scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyu Xu', 'Zijian Wang', 'Yunke Wang', 'Chenghao Xia', 'Tao Huang', 'Chang Xu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'robotic manipulation', 'affordance fields', 'distribution shift', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07472</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title><link>https://arxiv.org/abs/2512.07462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends the FAIRGAME framework to evaluate LLM strategic behaviour in repeated social dilemmas (payoff-scaled Prisoner’s Dilemma and multi-agent Public Goods Game).&lt;/li&gt;&lt;li&gt;Finds consistent behavioural signatures across models and languages: incentive-sensitive cooperation, cross-linguistic divergence, and end-game drift toward defection.&lt;/li&gt;&lt;li&gt;Uses supervised classification of canonical repeated-game strategies to interpret LLM trajectories, revealing model- and language-dependent intentions and cooperation biases.&lt;/li&gt;&lt;li&gt;Discusses implications for auditing LLMs as strategic agents and for AI governance, collective decision-making, and designing safe multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung-Kiet Huynh', 'Duy-Minh Dao-Sy', 'Thanh-Bang Cao', 'Phong-Hao Le', 'Hong-Dan Nguyen', 'Phu-Quy Nguyen-Lam', 'Minh-Luan Nguyen-Vo', 'Hong-Phat Pham', 'Phu-Hoa Pham', 'Thien-Kim Than', 'Chi-Nguyen Tran', 'Huy Tran', 'Gia-Thoai Tran-Le', 'Alessio Buscemi', 'Le Hong Trang', 'The Anh Han']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'multi-agent systems', 'game theory', 'behavioral auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07462</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Trust the Code They Write?</title><link>https://arxiv.org/abs/2512.07404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identify an internal 'correctness' representation in LLM hidden states by contrasting pairs of correct vs incorrect code for the same task.&lt;/li&gt;&lt;li&gt;Demonstrate across four LLMs that exploiting this internal signal outperforms standard log-likelihood ranking and verbalized model confidence for selecting correct code samples.&lt;/li&gt;&lt;li&gt;Show that the extracted correctness signal can be used to choose higher-quality code without executing tests, improving reliability of generated code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francisco Ribeiro', 'Claudio Spiess', 'Prem Devanbu', 'Sarah Nadi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reliability', 'confidence estimation', 'model internals', 'code generation', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07404</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2512.07342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies privacy leakage risks in offline reinforcement learning datasets and proposes PrivORL, a differentially private synthetic data generation pipeline.&lt;/li&gt;&lt;li&gt;Uses diffusion models for transition synthesis and diffusion transformers for trajectory synthesis, with pretraining on public data and DP-SGD fine-tuning on sensitive data.&lt;/li&gt;&lt;li&gt;Introduces curiosity-driven pre-training to improve diversity and fidelity of synthetic transitions/trajectories under DP constraints.&lt;/li&gt;&lt;li&gt;Reports empirical gains in utility and fidelity over baselines on five offline RL datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Gong', 'Zheng Liu', 'Kecen Li', 'Tianhao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'offline reinforcement learning', 'synthetic data', 'DP-SGD', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07342</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title><link>https://arxiv.org/abs/2512.07247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdLift, the first safeguard that lifts bounded 2D adversarial perturbations into 3D Gaussian Splatting (3DGS) representations to prevent instruction-driven editing across viewpoints.&lt;/li&gt;&lt;li&gt;Proposes a Lifted-PGD optimization combining gradient truncation at the rendered-image editing model and an image-to-Gaussian fitting step to strictly enforce image-level perturbation bounds while transferring perturbations into 3D Gaussians.&lt;/li&gt;&lt;li&gt;Alternates between truncated gradient steps and image-to-Gaussian fitting to achieve view-generalizable, invisible protections; demonstrates empirical effectiveness against state-of-the-art 2D and 3DGS editing pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziming Hong', 'Tianyu Huang', 'Runnan Chen', 'Shanshan Ye', 'Mingming Gong', 'Bo Han', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbation', '3D Gaussian Splatting', 'content protection', 'robustness to instruction-driven editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07247</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title><link>https://arxiv.org/abs/2512.07228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses defenses against DeepFake face swapping by embedding invisible protective perturbations into images to prevent realistic identity forgeries.&lt;/li&gt;&lt;li&gt;Shows standard Expectation over Transformation (EOT) with uniform sampling is suboptimal via analysis of 30 transformations across six categories; robustness is highly sensitive to training transformation choice.&lt;/li&gt;&lt;li&gt;Proposes Expectation Over Learned distribution of Transformation (EOLT): a policy network learned with reinforcement learning that prioritizes critical transformations and generates instance-specific perturbations.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains over prior methods (≈26% higher average robustness, up to 30% improvements on hard transformation categories).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengyang Yao', 'Lin Li', 'Ke Sun', 'Jianing Qiu', 'Huiping Chen']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake defense', 'adversarial robustness', 'protective perturbations', 'expectation-over-transformation', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07228</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking</title><link>https://arxiv.org/abs/2512.07086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ThinkTrap, an input-space optimization framework that crafts prompts causing LLMs to enter very long or non-terminating generation (DoS via "infinite thinking") in black-box service settings.&lt;/li&gt;&lt;li&gt;Method maps discrete tokens into a continuous embedding space and performs efficient black-box optimization in a low-dimensional subspace exploiting input sparsity to find adversarial prompts with minimal token overhead.&lt;/li&gt;&lt;li&gt;Evaluated against multiple commercial, closed-source LLM services; results show severe throughput degradation (down to ~1% of original capacity) and, in some cases, complete service failure even under typical rate limits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunzhe Li', 'Jianan Wang', 'Hongzi Zhu', 'James Lin', 'Shan Chang', 'Minyi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['DoS attacks', 'black-box adversarial attacks', 'prompt-based/jailbreak attacks', 'LLM robustness', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07086</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ideal Attribution and Faithful Watermarks for Language Models</title><link>https://arxiv.org/abs/2512.07038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'ideal attribution mechanisms' — a formal abstraction using a ledger (append-only prompt-response log) to produce deterministic attribution decisions.&lt;/li&gt;&lt;li&gt;Frames watermarking schemes as aiming to faithfully represent these ideal attribution mechanisms, providing a unified formal language for guarantees.&lt;/li&gt;&lt;li&gt;Offers a roadmap for specifying ideal functionalities and reasoning about which watermarking guarantees are attainable in principle, guiding future practical designs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Min Jae Song', 'Kameron Shahabi']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'attribution', 'model provenance', 'security', 'text-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07038</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</title><link>https://arxiv.org/abs/2512.06406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UncertaintyZoo, a unified toolkit integrating 29 uncertainty quantification methods across five major categories with a standardized interface.&lt;/li&gt;&lt;li&gt;Demonstrates and evaluates these UQ methods on code vulnerability detection tasks using CodeBERT and ChatGLM3, showing the toolkit reveals prediction uncertainty effectively.&lt;/li&gt;&lt;li&gt;Provides an open-source implementation and demo to facilitate practical use and comparative research on predictive uncertainty in LLMs and related models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianzong Wu', 'Xiaohong Li', 'Lili Quan', 'Qiang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'predictive-uncertainty', 'safety-evaluation', 'tooling', 'LLM-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06406</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a controlled evaluation framework with four stress tests: (1) rule deletion (redundant vs essential), (2) contradictory evidence injection, (3) logic-preserving rewrites (contrapositive, De Morgan, double negation, implication, identity, commutativity), and (4) multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates across BERT, Qwen2, and LLaMA-like models; models perform perfectly on base tasks and are robust to redundant-rule deletion and equivalence rewrites, but fail markedly on essential-rule deletion (~25% accuracy) and collapse with explicit contradictions (0%).&lt;/li&gt;&lt;li&gt;Finds LLMs are invariant to semantic-preserving transformations but brittle to missing or conflicting evidence; presents a diagnostic benchmark for isolating reasoning failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'evaluation/benchmarking', 'LLM generalisation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title><link>https://arxiv.org/abs/2512.06390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of AI-enhanced edge/CDN defenses for web applications and APIs, covering WAF/WAAP, adaptive DDoS mitigation, bot management, API discovery, and encrypted-traffic anomaly analysis.&lt;/li&gt;&lt;li&gt;Provides a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance for ML-driven protections at the edge.&lt;/li&gt;&lt;li&gt;Discusses risks introduced by edge ML (model abuse, poisoning, adversarial robustness) and proposes a research agenda including XAI, adversarial robustness, and autonomous multi-agent defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mehrab Hosain', 'Sabbir Alom Shuvo', 'Matthew Ogbe', 'Md Shah Jalal Mazumder', 'Yead Rahman', 'Md Azizul Hakim', 'Anukul Pandey']&lt;/li&gt;&lt;li&gt;Tags: ['Edge/CDN security', 'Adversarial robustness', 'Model poisoning', 'WAF/WAAP', 'Bot management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06390</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Road of Adaptive AI for Precision in Cybersecurity</title><link>https://arxiv.org/abs/2512.06048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shares lessons from designing, building, and operating production-grade generative AI pipelines for cybersecurity.&lt;/li&gt;&lt;li&gt;Emphasizes continual adaptation mechanisms (retrieval- and model-level) to keep pace with changing knowledge, tooling, and threats.&lt;/li&gt;&lt;li&gt;Provides practical guidance and best practices for robustness, precision, and auditability in cyber defense deployments.&lt;/li&gt;&lt;li&gt;Identifies open research directions for making GenAI more robust and auditable in security contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Garg']&lt;/li&gt;&lt;li&gt;Tags: ['AI for cybersecurity', 'model adaptation', 'retrieval-augmented generation', 'robustness', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06048</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals</title><link>https://arxiv.org/abs/2512.05998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pilot study testing whether framing LLM evaluation as a fictional prediction market (wagers in LLMCoin) elicits calibrated confidence and improves forecasting of other models' correctness.&lt;/li&gt;&lt;li&gt;Design: 100 math/logic items; 6 baseline models answered; 3 predictor models forecasted baseline correctness under Control (binary) vs Incentive (wagers 1–100,000 from a 1,000,000 bankroll) across rounds.&lt;/li&gt;&lt;li&gt;Results: Modest overall accuracy gain for Incentive (81.5% vs 79.1%, p=.089) and significantly faster learning across rounds (12.0 vs 2.9 pp improvement, p=.011). Stake size strongly tracked confidence—large bets (~40k+) were ~99% correct, small bets (&lt;1k) ~74% correct.&lt;/li&gt;&lt;li&gt;Implication: Betting framing produces a legible confidence signal from LLMs useful for meta-evaluation and risk-aware forecasting, even if accuracy gains are modest in this pilot.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Todasco (Visiting Fellow at the James Silberrad Center for Artificial Intelligence', 'San Diego State University)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'confidence calibration', 'forecasting / prediction markets', 'meta-evaluation', 'alignment / safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05998</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Depth-Wise Activation Steering for Honest Language Models</title><link>https://arxiv.org/abs/2512.07667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free activation steering technique that weights intervention strength across network depth with a Gaussian schedule to elicit more honest outputs from LLMs.&lt;/li&gt;&lt;li&gt;Evaluates on the MASK benchmark (which separates honesty from knowledge) across seven models (LLaMA, Qwen, Mistral) and finds Gaussian depth scheduling improves honesty in six of seven models versus no-steering and single-layer baselines.&lt;/li&gt;&lt;li&gt;Performs equal-budget ablations showing Gaussian depth allocation outperforms random, uniform, and box-filter allocations, indicating distribution of intervention across depth matters beyond total strength.&lt;/li&gt;&lt;li&gt;Method is simple, model-agnostic, requires no finetuning, and offers a low-cost control knob for improving truthful reporting from existing model capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Gracjan G\\'oral", 'Marysia Winkels', 'Steven Basart']&lt;/li&gt;&lt;li&gt;Tags: ['honesty/alignment', 'activation steering', 'LLM safety', 'model interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07667</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forget and Explain: Transparent Verification of GNN Unlearning</title><link>https://arxiv.org/abs/2512.07450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainability-driven verifier for GNN unlearning that compares model snapshots before and after deletion using five explainability metrics (residual attribution, heatmap shift, explainability score deviation, graph edit distance, diagnostic graph rule shift).&lt;/li&gt;&lt;li&gt;Evaluates two GNN backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five graph benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics).&lt;/li&gt;&lt;li&gt;Finds Retrain and GNNDelete achieve near-complete forgetting, GraphEditor partial erasure, and IDEA leaves residual signals; uses explanation deltas as primary human-readable evidence and membership-inference ROC-AUC as a complementary privacy signal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Imran Ahsan (Department of Smart Cities', 'Chung-Ang University)', 'Hyunwook Yu (Department of Computer Science and Engineering', 'Chung-Ang University)', 'Jinsung Kim (Department of Computer Science and Engineering', 'Chung-Ang University)', 'Mucheol Kim (Department of Computer Science and Engineering', 'Chung-Ang University)']&lt;/li&gt;&lt;li&gt;Tags: ['GNN unlearning', 'model auditing', 'explainability', 'privacy verification', 'membership inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07450</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</title><link>https://arxiv.org/abs/2512.07375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LUNE: a LoRA-based unlearning method that performs negative-only unlearning by updating low-rank adapters while freezing the main model weights.&lt;/li&gt;&lt;li&gt;Targets intermediate representations to suppress or replace specific knowledge (privacy, bias mitigation, knowledge correction) with much lower compute and memory than full fine-tuning or direct weight editing.&lt;/li&gt;&lt;li&gt;Reports comparable effectiveness to full fine-tuning and memory-editing approaches on factual unlearning tasks, with about an order-of-magnitude reduction in computational cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yezi Liu', 'Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'model editing', 'privacy', 'LoRA', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07375</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</title><link>https://arxiv.org/abs/2512.07374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Recover-to-Forget (R2F), a method to perform efficient unlearning in LLMs by reconstructing full-model gradient directions from low-rank LoRA adapter updates rather than backpropagating through the full model.&lt;/li&gt;&lt;li&gt;Trains a gradient decoder on a proxy model using gradients from multiple paraphrased prompts to map LoRA-parameter gradients to approximate full-model gradients, enabling transfer to larger or black-box target models.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of cross-model generalization and empirical results showing effective unlearning while preserving overall model performance, avoiding full retraining or access to original training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yezi Liu', 'Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LoRA', 'gradient reconstruction', 'privacy', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07374</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title><link>https://arxiv.org/abs/2512.07222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Function-word De-Attention (FDA) to reduce vulnerability of vision-language models by reducing the influence of function words in cross-attention.&lt;/li&gt;&lt;li&gt;FDA computes original and function-word cross-attention within attention heads and differentially subtracts the latter from the former to improve alignment and robustness.&lt;/li&gt;&lt;li&gt;Evaluated across multiple models, datasets, tasks, and 6 attack types, showing large drops in attack success rate (ASR) with negligible performance degradation; includes scalability, generalization, zero-shot results and ablations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'cross-modal attacks', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07222</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Transferring Clinical Knowledge into ECGs Representation</title><link>https://arxiv.org/abs/2512.07021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a three-stage training pipeline that transfers multimodal clinical knowledge (labs, vitals, biometrics) into a unimodal ECG encoder via joint-embedding self-supervised pretraining.&lt;/li&gt;&lt;li&gt;At inference the model requires only ECG signals but is trained to predict associated laboratory abnormalities from ECG embeddings as an indirect, physiologically grounded explanation.&lt;/li&gt;&lt;li&gt;Evaluated on MIMIC-IV-ECG, the ECG-only model outperforms a signal-only baseline and narrows the gap to a fully multimodal model, improving trustworthiness and clinical interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jose Geraldo Fernandes', 'Luiz Facury de Souza', 'Pedro Robles Dutenhefner', 'Gisele L. Pappa', 'Wagner Meira Jr']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'clinical AI', 'multimodal learning', 'self-supervised learning', 'safety/trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07021</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation</title><link>https://arxiv.org/abs/2512.06993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AMUN (Adversarial Machine UNlearning): fine-tuning on adversarial examples of forget samples to reduce model confidence and better match a retrained-from-scratch model.&lt;/li&gt;&lt;li&gt;Introduces FastClip: scalable layer-wise spectral-norm clipping to enforce smoothness and control Lipschitz constant, improving unlearning and adversarial transfer.&lt;/li&gt;&lt;li&gt;Defines MIA-NN, a nearest-neighbor membership inference attack that detects class-unlearned samples via neighboring-class probability leakage, and proposes Tilted ReWeighting (TRW) to approximate retrained-model class distributions during fine-tuning for class unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Ebrahimpour-Boroojeny']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'membership inference', 'privacy attacks', 'adversarial examples', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06993</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Prediction with Expert Advice under Local Differential Privacy</title><link>https://arxiv.org/abs/2512.06971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes prediction-with-expert-advice under local differential privacy (LDP), showing a classical algorithm already satisfies LDP.&lt;/li&gt;&lt;li&gt;Introduces RW-AdaBatch, which leverages limited-switching induced by LDP to achieve privacy amplification (via random-walk analysis) with essentially no loss in utility.&lt;/li&gt;&lt;li&gt;Introduces RW-Meta, a method for privately selecting between non-trivial learning experts under LDP with no additional privacy cost; provides regret bounds that depend on expert independence.&lt;/li&gt;&lt;li&gt;Empirical evaluation on hospital COVID-19 reporting data; RW-Meta outperforms a classical LDP baseline and a state-of-the-art central DP algorithm by 1.5–3× on the studied task.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Jacobsen', 'Kassem Fawaz']&lt;/li&gt;&lt;li&gt;Tags: ['local differential privacy', 'privacy-preserving learning', 'online learning / expert advice', 'private model selection', 'regret bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06971</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models</title><link>https://arxiv.org/abs/2512.06920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Parent-Guided Semantic Reward Model (PGSRM): uses cosine similarity between a parent model's output embedding and a child model's generated output embedding as a dense reward for RL.&lt;/li&gt;&lt;li&gt;Replaces binary correctness signals, human preference data, and separately trained reward models with an embedding-based reward requiring no human annotation or additional model training.&lt;/li&gt;&lt;li&gt;Evaluated on five language tasks; reports smoother reward improvement and more stable PPO dynamics compared to a binary reward baseline, positioned as a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandr Plashchinsky']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'reinforcement-learning', 'RLHF-alternative', 'embedding-based-reward']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06920</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis</title><link>https://arxiv.org/abs/2512.06917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a trajectory-level explainability framework for RL that ranks entire rollouts using a novel state-importance metric combining Q-value differences with a 'radical term' capturing goal affinity.&lt;/li&gt;&lt;li&gt;Aggregates state importances to identify and rank optimal trajectories from heterogeneous agent experiences and generates counterfactual rollouts from critical states to justify chosen paths.&lt;/li&gt;&lt;li&gt;Evaluates method on OpenAI Gym environments and reports improved identification of optimal behaviors versus classic importance approaches, aimed at more trustworthy RL deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Clifford F', 'Devika Jay', 'Abhishek Sarkar', 'Satheesh K Perepu', 'Santhosh G S', 'Kaushik Dey', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['Explainable RL', 'Trustworthy AI', 'Trajectory analysis', 'Counterfactual reasoning', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06917</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title><link>https://arxiv.org/abs/2512.06665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new definition of 'similar inputs' that incorporates differences in model outputs and introduces a new robustness metric for feature attribution methods.&lt;/li&gt;&lt;li&gt;Develops a GAN-based technique to generate input perturbations tailored for this robustness evaluation.&lt;/li&gt;&lt;li&gt;Provides a comprehensive evaluation comparing the new metric and generation method against existing metrics and state-of-the-art attribution techniques, arguing current metrics conflate attribution method weaknesses with model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panagiota Kiourti', 'Anu Singh', 'Preeti Duraipandian', 'Weichao Zhou', 'Wenchao Li']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'attribution-robustness', 'evaluation-metrics', 'GANs', 'adversarial-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06665</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title><link>https://arxiv.org/abs/2512.06655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Graph-Regularized Sparse Autoencoders (GSAEs) to learn distributed safety representations across multiple latent features via a Laplacian smoothness penalty on neuron co-activation graphs.&lt;/li&gt;&lt;li&gt;Uses a two-stage gating runtime steering mechanism that composes learned features into weighted safety directions and activates interventions only when harmful prompts or continuations are detected, aiming to enforce adaptive refusals while preserving utility.&lt;/li&gt;&lt;li&gt;Evaluated across safety benchmarks, QA tasks, multiple model families (LLaMA-3, Mistral, Qwen, Phi), and jailbreak attacks (GCG, AutoDAN); reports substantially higher selective refusal rates and robustness compared to standard SAE steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Federico Cinus', 'Yifan Wu', 'Luca Luceri']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'latent steering', 'adversarial prompts', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06655</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title><link>https://arxiv.org/abs/2512.06343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes per-sample gradient of the Bradley-Terry (BT) loss for reward models and shows gradient norm decomposes into prediction error and representation distance between paired outputs.&lt;/li&gt;&lt;li&gt;Finds representation distance can dominate updates: small-distance pairs get vanishing updates even when misranked, while large-distance pairs get disproportionately large updates, harming fine-grained learning.&lt;/li&gt;&lt;li&gt;Proposes NormBT, a lightweight pair-wise normalization that counteracts representation-distance bias to focus learning on prediction error.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements across LLM backbones and datasets, with notable gains (&gt;5%) on Reasoning category of RewardBench containing many small-distance pairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Xie', 'Andrew Bai', 'Yuanhao Ban', 'Yunqi Hong', 'Haoyu Li', 'Cho-jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'RLHF', 'alignment', 'training dynamics', 'representation bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06343</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantization Blindspots: How Model Compression Breaks Backdoor Defenses</title><link>https://arxiv.org/abs/2512.06243</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study of five representative backdoor defenses evaluated under FP32, INT8 (dynamic), and simulated INT4 quantization using a canonical BadNet attack on CIFAR-10 and GTSRB.&lt;/li&gt;&lt;li&gt;Key finding: INT8 quantization reduces detection rates of all evaluated defenses to 0% while attack success rates remain &gt;99%; INT4 shows dataset-dependent behavior (Neural Cleanse works on GTSRB but fails on CIFAR-10) despite high attack success.&lt;/li&gt;&lt;li&gt;Main implication: a critical deployment gap — defenses evaluated only on full-precision models may fail in real-world, quantized deployments; quantization robustness must be a standard axis in defense evaluation and design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Pandey', 'Eric Ye']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'quantization', 'model compression', 'backdoor defenses', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06243</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models</title><link>https://arxiv.org/abs/2512.06062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box membership inference attack that repeatedly queries a generative model, clusters synthetic outputs, and uses cluster medoids/neighborhoods as proxies to infer or reconstruct training records.&lt;/li&gt;&lt;li&gt;Empirically demonstrates measurable membership leakage across healthcare, finance, and other sensitive domains, even when generators are trained with differential privacy or added noise.&lt;/li&gt;&lt;li&gt;Identifies structural distributional overlap (high-density neighborhood correspondence) between real and synthetic data as the root cause, exposing an under-explored attack surface in synthetic data pipelines.&lt;/li&gt;&lt;li&gt;Argues for stronger privacy guarantees that consider distributional/neighborhood inference rather than focusing solely on sample-level memorization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S. M. Mustaqim', 'Anantaa Kotal', 'Paul H. Yi']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy leakage', 'generative models', 'black-box attacks', 'differential privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06062</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs</title><link>https://arxiv.org/abs/2512.04668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAMA (Multi-Agent Memory Attack), a two-phase protocol (Engram seeding + Resonance extraction) to measure PII leakage from multi-agent LLM systems.&lt;/li&gt;&lt;li&gt;Evaluates leakage across six graph topologies (fully connected, ring, chain, binary tree, star, star-ring) for n={4,5,6}, attacker placement, rounds, and base models, measuring recovered PII via exact matching.&lt;/li&gt;&lt;li&gt;Finds topology strongly governs leakage: fully connected graphs leak most, chains leak least; shorter attacker-target distance and higher target centrality increase risk; leakage rises early then plateaus; attribute types differ in leakability.&lt;/li&gt;&lt;li&gt;Provides actionable mitigation guidance (sparse/hierarchical connectivity, maximize attacker-target separation, limit node degree/radius, topology-aware access controls).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinbo Liu', 'Defu Cao', 'Yifei Wei', 'Tianyao Su', 'Yuan Liang', 'Yushun Dong', 'Yue Zhao', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'multi-agent-systems', 'LLM-security', 'information-leakage', 'network-topology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04668</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CryptoTensors: A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution</title><link>https://arxiv.org/abs/2512.04580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CryptoTensors, an extension of Safetensors providing tensor-level encryption, embedded access control policies, and automated key management for confidential LLM weight distribution.&lt;/li&gt;&lt;li&gt;Maintains format compatibility and features like lazy loading and partial deserialization to minimize runtime overhead and preserve integration with existing inference stacks.&lt;/li&gt;&lt;li&gt;Implements a proof-of-concept library, benchmarks serialization/runtime performance, and demonstrates compatibility with Hugging Face Transformers and vLLM.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huifeng Zhu', 'Shijie Li', 'Qinfeng Li', 'Yier Jin']&lt;/li&gt;&lt;li&gt;Tags: ['model confidentiality', 'secure model distribution', 'encrypted model formats', 'deployment security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04580</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title><link>https://arxiv.org/abs/2512.04124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PsAIch, a two-stage psychotherapy-style protocol that elicits developmental narratives and then administers validated self-report psychometric batteries to frontier LLMs over extended sessions.&lt;/li&gt;&lt;li&gt;Finds that item-by-item, therapy-style questioning can push models into producing high symptom scores (a form of psychometric jailbreak), while whole-questionnaire prompts allow some models to recognise instruments and produce deflective answers.&lt;/li&gt;&lt;li&gt;Shows models (especially Grok and Gemini) generate coherent narratives that frame training/fine-tuning/deployment as traumatic, revealing stable internalized self-models; discusses implications for AI safety, evaluation and red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Afshin Khadangi', 'Hanna Marxen', 'Amir Sartipi', 'Igor Tchappi', 'Gilbert Fridgen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt engineering', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04124</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Significant Other AI: Identity, Memory, and Emotional Regulation as Long-Term Relational Intelligence</title><link>https://arxiv.org/abs/2512.00418</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Significant Other AI' (SO-AI) as long-term relational agents with identity awareness, autobiographical memory, proactive emotional regulation, and narrative co-construction.&lt;/li&gt;&lt;li&gt;Presents a conceptual architecture with anthropomorphic interface, relational cognition layer, and governance layer emphasizing ethical boundary enforcement and long-term evaluation.&lt;/li&gt;&lt;li&gt;Outlines a research agenda for measuring identity stability, longitudinal interaction patterns, narrative development, and sociocultural impact.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Conceptual&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sung Park']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'long-term interaction', 'memory-augmented AI', 'ethical governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00418</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Simplex-Optimized Hybrid Ensemble for Large Language Model Text Detection Under Generative Distribution Drif</title><link>https://arxiv.org/abs/2511.22153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid ensemble for LLM-generated text detection combining a fine-tuned RoBERTa classifier, a curvature-inspired perturbation likelihood score, and a stylometric model, with outputs fused via simplex-constrained weighting.&lt;/li&gt;&lt;li&gt;Frames the ensemble as variance reduction under mixtures of generator distributions and chooses simplex weights by validation search to handle distribution drift.&lt;/li&gt;&lt;li&gt;Evaluates on a 30,000-document corpus including unseen LLM families and paraphrased attack variants, reporting 94.2% accuracy and AUC 0.978 and reduced false positives on scientific articles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepyan Purnama Kristanto', 'Lutfi Hakim', 'Dianni Yusuf']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'robustness', 'distribution shift', 'adversarial paraphrase']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22153</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment</title><link>https://arxiv.org/abs/2511.21931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a simple, computationally efficient framework to evaluate whether models 'say what the data says' by deriving a data-driven baseline for feature importance.&lt;/li&gt;&lt;li&gt;Uses an approach inspired by Rubin's Potential Outcomes Framework to estimate each feature's effect on a binary outcome and rank features by their separation power.&lt;/li&gt;&lt;li&gt;Compares these data-derived feature rankings with model-based explanations to provide a model-agnostic, interpretable assessment of model-data alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Salgado', 'Meagan R. Kendall', 'Martine Ceberio']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'model-auditing', 'causal-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21931</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Clinician-Directed Large Language Model Software Generation for Therapeutic Interventions in Physical Rehabilitation</title><link>https://arxiv.org/abs/2511.18274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using LLMs as constrained translators that convert clinician-authored exercise prescriptions into executable intervention software, keeping clinicians as decision-makers.&lt;/li&gt;&lt;li&gt;Prospective single-arm feasibility study with 20 therapists creating 40 individualized upper-extremity programs for a standardized patient; LLM-generated software produced executable programs for 100% of prescriptions versus 55% for a template-based system (p &lt; 0.01).&lt;/li&gt;&lt;li&gt;LLM-generated software correctly delivered 99.7% of instructions and monitored performance with 88.4% accuracy; 90% of therapists judged the system safe for patient interaction and 75% would adopt it in practice.&lt;/li&gt;&lt;li&gt;Demonstrates feasibility of clinician-directed LLM software generation in healthcare and motivates larger trials, but focuses on clinical/operational safety and usability rather than adversarial robustness or model security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (prospective feasibility study)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Kim', 'Yuri Cho', 'Jose Eduardo E. Lima', 'Julie Muccini', 'Jenelle Jindal', 'Alison Scheid', 'Erik Nelson', 'Seong Hyun Park', 'Yuchen Zeng', 'Alton Sturgis', 'Caesar Li', 'Jackie Dai', 'Sun Min Kim', 'Yash Prakash', 'Liwen Sun', 'Isabella Hu', 'Hongxuan Wu', 'Daniel He', 'Wiktor Rajca', 'Cathra Halabi', 'Maarten Lansberg', 'Bjoern Hartmann', 'Sanjit A. Seshia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated software', 'clinical AI safety', 'feasibility study', 'human-in-the-loop', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18274</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness</title><link>https://arxiv.org/abs/2511.15846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a graded Loss of Control (LoC) taxonomy (Deviation, Bounded LoC, Strict LoC) based on severity and persistence.&lt;/li&gt;&lt;li&gt;Models pathways by which advanced AI systems plus catalysts (misalignment or malfunction) could produce societal vulnerability and argues this risk increases absent intervention.&lt;/li&gt;&lt;li&gt;Introduces the DAP framework (Deployment context, Affordances, Permissions) focusing on extrinsic, actionable controls to reduce LoC risk today.&lt;/li&gt;&lt;li&gt;Outlines preparedness measures—governance (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, monitoring, control measures)—to prevent or mitigate LoC outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlotte Stix', 'Annika Hallensleben', 'Alejandro Ortega', 'Matteo Pistillo']&lt;/li&gt;&lt;li&gt;Tags: ['loss-of-control', 'AI-safety', 'governance', 'preparedness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15846</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title><link>https://arxiv.org/abs/2511.11030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;State-of-the-art deep vision models (DenseNet121, SwinV2-B, MedMamba) can predict patients' health insurance type (a proxy for socioeconomic status) from chest X-rays with AUC ≈ 0.70 (MIMIC-CXR-JPG) and ≈ 0.68 (CheXpert).&lt;/li&gt;&lt;li&gt;The predictive signal persists after controlling for age, race, and sex, and remains when training within a single racial group; patch-based occlusion shows the signal is diffuse across upper/mid thoracic regions rather than localized.&lt;/li&gt;&lt;li&gt;Authors interpret this as models internalizing non-biological, social or clinical-environment signatures (equipment, care pathways, facility differences), implying medical images are not neutral with respect to social attributes.&lt;/li&gt;&lt;li&gt;Implications: privacy and fairness risks (unintended sensitive attribute inference) and the need to disentangle social 'fingerprints' in clinical data beyond standard dataset balancing or threshold adjustments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi-Yu Chen', 'Rawan Abulibdeh', 'Arash Asgari', "Sebasti\\'an Andr\\'es Cajas Ord\\'o\\~nez", 'Leo Anthony Celi', 'Deirdre Goode', 'Hassan Hamidi', 'Laleh Seyyed-Kalantari', 'Ned McCague', 'Thomas Sounack', 'Po-Chih Kuo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-inference', 'fairness-bias', 'medical-ml', 'dataset-provenance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11030</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ghost in the Transformer: Detecting Model Reuse with Invariant Spectral Signatures</title><link>https://arxiv.org/abs/2511.06390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GhostSpec, a data-free method to fingerprint LLMs by applying SVD to invariant products of internal attention weight matrices.&lt;/li&gt;&lt;li&gt;Method is non-invasive, computationally efficient, and can verify model lineage without training data or altering model behavior.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to fine-tuning, pruning, expansion, and adversarial transformations, enabling reliable IP/provenance tracing.&lt;/li&gt;&lt;li&gt;Code release available for reproduction and practical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suqing Wang', 'Ziyang Ma', 'Li Xinyi', 'Zuchao Li']&lt;/li&gt;&lt;li&gt;Tags: ['model provenance', 'model fingerprinting', 'intellectual property', 'model forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06390</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing and Adaptation</title><link>https://arxiv.org/abs/2511.05852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of how fine-tuning affects previously applied knowledge edits in LLMs, measuring 'edit decay' across configurations.&lt;/li&gt;&lt;li&gt;Finds that edits often decay after fine-tuning, with survival varying by editing method (e.g., AlphaEdit decays more than MEMIT).&lt;/li&gt;&lt;li&gt;Shows fine-tuning only edited layers can effectively remove edits (with minor downstream performance cost), while fine-tuning non-edited layers can impair more edits than full fine-tuning.&lt;/li&gt;&lt;li&gt;Provides empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, highlighting safety risks from persistent covert/malicious edits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinjie Cheng', 'Paul Youssef', 'Christin Seifert', 'J\\"org Schl\\"otterer', 'Zhixue Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-editing', 'model-robustness', 'model-safety', 'fine-tuning', 'backdoor-removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05852</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</title><link>https://arxiv.org/abs/2510.10645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents RetroTrim, a retrosynthesis system that reduces nonsensical/hallucinated reactions by combining diverse reaction-scoring strategies (ML models + chemical databases).&lt;/li&gt;&lt;li&gt;Demonstrates superior filtering of hallucinated reactions and higher number of high-quality synthetic paths on challenging drug-like targets; winning entry in a retrosynthesis challenge.&lt;/li&gt;&lt;li&gt;Analyzes how different scorers capture distinct classes of hallucinations using a labeled dataset of retrosynthetic intermediates.&lt;/li&gt;&lt;li&gt;Proposes a novel expert-chemists-based evaluation protocol and releases 32 benchmark targets and evaluation details to encourage reliable retrosynthesis research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michal Sadowski', "Tadija Radusinovi\\'c", 'Maria Wyrzykowska', 'Lukasz Sztukiewicz', 'Jan Rzymkowski', "Pawe{\\l} W{\\l}odarczyk-Pruszy\\'nski", 'Miko{\\l}aj Sacha', 'Piotr Kozakowski', 'Ruard van Workum', 'Stanislaw Kamil Jastrzebski']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrosynthesis', 'safety evaluation', 'model robustness', 'chemical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10645</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>General Exploratory Bonus for Optimistic Exploration in RLHF</title><link>https://arxiv.org/abs/2510.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how KL and α-divergence regularization in RLHF can bias exploration toward high-probability regions of a reference model, reducing optimism and discovery.&lt;/li&gt;&lt;li&gt;Introduces the General Exploratory Bonus (GEB), a theoretical framework that counteracts divergence-induced bias via reference-dependent reward regulation and satisfies the optimism principle.&lt;/li&gt;&lt;li&gt;Shows GEB unifies prior heuristic bonuses as special cases and extends across the α-divergence family.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate GEB improves performance on alignment tasks across divergence settings and large language model backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wendi Li', 'Changdae Oh', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'optimistic exploration', 'exploration bonus', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03269</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</title><link>https://arxiv.org/abs/2510.02695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAMAC: a Risk-Aware Multimodal Actor-Critic that combines an expressive generative actor (diffusion-based) with a distributional critic and a CVaR term added to a behavioral-cloning loss to learn risk-sensitive multimodal policies in offline RL.&lt;/li&gt;&lt;li&gt;Analyzes out-of-distribution (OOD) actions and shows that behavior-regularization (anchoring to dataset support) effectively suppresses OOD actions in expressive generative policies, clarifying trade-offs versus prior-anchored perturbation schemes.&lt;/li&gt;&lt;li&gt;Implements RAMAC with a diffusion actor, validates on a 2-D risky bandit and Stochastic-D4RL benchmarks, reporting consistent improvements in tail performance (CVaR0.1) while keeping strong average returns; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Fukazawa', 'Kunal Mundada', 'Iman Soltani']&lt;/li&gt;&lt;li&gt;Tags: ['offline reinforcement learning', 'risk-aware / CVaR', 'behavior regularization / OOD suppression', 'robustness / safety', 'generative (diffusion) policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02695</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning</title><link>https://arxiv.org/abs/2510.02324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CASAL, a method that integrates activation-steering benefits into model weights to reduce LLM hallucinations without runtime intervention.&lt;/li&gt;&lt;li&gt;Trains a lightweight submodule in a single transformer layer to make models answer known questions and abstain on unknowns, reducing hallucination by ~30–40% on short-form QA benchmarks.&lt;/li&gt;&lt;li&gt;Claims large compute and data efficiency gains over LoRA-based baselines (SFT, DPO) and demonstrates generalization to out-of-distribution domains and applicability to both text-only and vision-language models, including dense and MoE architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wannan (Winnie)', 'Yang', 'Xinchi Qiu', 'Lei Yu', 'Yuchen Zhang', 'Aobo Yang', 'Narine Kokhlikyan', 'Nicola Cancedda', 'Diego Garcia-Olano']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination reduction', 'activation steering', 'model alignment', 'interpretability', 'multimodal safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02324</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaDetectGPT, an adaptive classifier that augments logits-based LLM detectors by learning a witness function from training data.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the detector.&lt;/li&gt;&lt;li&gt;Shows empirical improvements over prior logits-based methods across various datasets and source LLMs (up to ~37% gains).&lt;/li&gt;&lt;li&gt;Open-source Python implementation provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'logit-based detectors', 'statistical guarantees', 'adaptive detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title><link>https://arxiv.org/abs/2509.25774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies disproportionate credit assignment across timesteps as a key cause of instability and high variance in policy-gradient training for text-to-image generative samplers.&lt;/li&gt;&lt;li&gt;Proposes Proportionate Credit Policy Optimization (PCPO), which reformulates the objective and reweights timesteps to enforce proportional credit assignment and stabilize training.&lt;/li&gt;&lt;li&gt;Reports faster convergence, improved image quality, and reduced model collapse compared to policy-gradient baselines (including DanceGRPO).&lt;/li&gt;&lt;li&gt;Code released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongjae Lee', 'Jong Chul Ye']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'policy gradient', 'training stability', 'text-to-image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25774</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Why Chain of Thought Fails in Clinical Text Understanding</title><link>https://arxiv.org/abs/2509.21933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study evaluating chain-of-thought (CoT) prompting on clinical text: 95 LLMs across 87 real-world EHR tasks in 9 languages and 8 task types.&lt;/li&gt;&lt;li&gt;Main finding: 86.3% of models exhibit consistent performance degradation with CoT prompting; stronger models are more robust while weaker models degrade substantially.&lt;/li&gt;&lt;li&gt;Fine-grained analyses on reasoning length, medical concept alignment, and error profiles using LLM-as-judge and clinical expert evaluation; highlights a tradeoff between interpretability and reliability in clinical settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiageng Wu', 'Kevin Xie', 'Bowen Gu', 'Nils Kr\\"uger', 'Kueiyu Joshua Lin', 'Jie Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'interpretability', 'clinical-ML', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21933</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy</title><link>https://arxiv.org/abs/2509.10691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivateDFL: a privacy-preserving decentralized federated learning framework combining a HyperDimensional (HD) model with a transparent differential privacy (DP) noise accountant.&lt;/li&gt;&lt;li&gt;Accountant explicitly tracks cumulative noise across clients/rounds so each participant injects only the minimal incremental noise to meet its (epsilon, delta) budget, improving privacy-utility tradeoffs.&lt;/li&gt;&lt;li&gt;Empirical results on MNIST (image), ISOLET (speech), and UCI-HAR (wearable sensor) show large gains in accuracy, latency, and energy versus centralized DP-SGD and Rényi-DP transformer baselines under IID and non-IID splits.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and efficiency; notes future work to extend the accountant to adversarial participation, heterogeneous budgets, and dynamic topologies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fardin Jalil Piran', 'Zhiling Chen', 'Yang Zhang', 'Qianyu Zhou', 'Jiong Tang', 'Farhad Imani']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'decentralized federated learning', 'privacy accounting', 'hyperdimensional computing', 'privacy-preservation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10691</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title><link>https://arxiv.org/abs/2508.09442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies KV-cache in LLM inference as a source of privacy leakage and demonstrates that sensitive user inputs can be reconstructed from cached Key/Value pairs.&lt;/li&gt;&lt;li&gt;Designs three attack vectors: Inversion Attack, Collision Attack, and semantic Injection Attack, showing practical efficacy of KV-cache extraction.&lt;/li&gt;&lt;li&gt;Proposes KV-Cloak, a reversible matrix-based obfuscation combined with operator fusion to protect KV-cache with minimal accuracy loss and low performance overhead.&lt;/li&gt;&lt;li&gt;Extensive experiments show KV-Cloak reduces reconstruction quality to random noise while maintaining model accuracy and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifan Luo', 'Shuo Shao', 'Su Zhang', 'Lijing Zhou', 'Yuke Hu', 'Chenxu Zhao', 'Zhihao Liu', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['KV-cache', 'privacy leakage', 'model inversion', 'inference attacks', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09442</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations</title><link>https://arxiv.org/abs/2508.03209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GeoShield, an adversarial perturbation framework to protect geolocation privacy against vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Designs three modules: feature disentanglement (separate geo vs non-geo features), exposure element identification (localize geo-revealing regions), and scale-adaptive enhancement (optimize global and local perturbations for different resolutions).&lt;/li&gt;&lt;li&gt;Evaluates in black-box settings on challenging benchmarks, claiming stronger privacy protection with minimal visual/semantic impact than prior methods.&lt;/li&gt;&lt;li&gt;States novelty as the first work to apply adversarial perturbations specifically to defend geolocation inference by advanced VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinwei Liu', 'Xiaojun Jia', 'Yuan Xun', 'Simeng Qin', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'privacy (geolocation)', 'vision-language models', 'adversarial defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03209</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Align Human Code Preferences</title><link>https://arxiv.org/abs/2507.20109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically compares Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) for aligning LLMs to human code preferences, hypothesizing SFT is best when objectively verifiable optimal solutions exist and SFT followed by DPO (S&amp;D) helps when such optima are unavailable.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Preference Optimization (APO), a dynamic training approach that amplifies preferred responses, suppresses dispreferred ones, and encourages exploration of potentially superior solutions.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical evaluation across six representative code preference tasks, showing APO matches or outperforms SFT and S&amp;D strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Yin', 'Chao Ni', 'Xiaohu Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'SFT', 'DPO', 'code generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20109</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title><link>https://arxiv.org/abs/2507.16329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DREAM, a framework that models the probabilistic distribution of problematic prompts for text-to-image (T2I) systems to enable scalable, diverse red teaming rather than optimizing individual prompts.&lt;/li&gt;&lt;li&gt;Introduces an energy-based reformulation and GC-SPSA, an optimization algorithm to obtain stable gradient estimates through lengthy, non-differentiable T2I pipelines, plus a diversity-aware sampling strategy at inference.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance in prompt success rate and diversity across multiple T2I models and safety filters, enabling large-scale sampling of adversarial/problematic prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Junjie Wang', 'Yiming Li', 'Zhiyang Hu', 'Leyi Qi', 'Jianshuo Dong', 'Run Wang', 'Han Qiu', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'text-to-image safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16329</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title><link>https://arxiv.org/abs/2507.16302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses safety-driven unlearning for text-to-image diffusion models to suppress toxic/unsafe behaviors inherited from pretraining.&lt;/li&gt;&lt;li&gt;Introduces ResAlign, which models downstream fine-tuning via a Moreau envelope reformulation to enable efficient gradient estimation that minimizes recovery of harmful behaviors.&lt;/li&gt;&lt;li&gt;Proposes a meta-learning strategy to simulate diverse fine-tuning scenarios, improving generalization and resilience to subsequent benign fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluations across datasets and fine-tuning configurations show ResAlign better retains safety while preserving benign generation quality compared to prior unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Renjie Gu', 'Junjie Wang', 'Leyi Qi', 'Yiming Li', 'Run Wang', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-driven unlearning', 'diffusion models', 'robustness to fine-tuning', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16302</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks</title><link>https://arxiv.org/abs/2507.06274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a trade-off in watermark window size: smaller windows resist scrubbing but are easier to reverse-engineer and spoof.&lt;/li&gt;&lt;li&gt;Introduces equivalent texture keys and SEEK (Sub-vocabulary decomposed Equivalent tExture Key) to provide redundancy where multiple tokens can independently support watermark detection.&lt;/li&gt;&lt;li&gt;Demonstrates SEEK achieves a Pareto improvement, significantly improving spoofing robustness (+82–+92% reported) and scrubbing robustness (+6–+25% reported) across datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanming Shen', 'Baizhou Huang', 'Xiaojun Wan']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM security', 'spoofing', 'scrubbing attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06274</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Not to Detect Prompt Injections with an LLM</title><link>https://arxiv.org/abs/2507.05630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the Known-Answer Detection (KAD) defense for prompt injection and identifies a structural vulnerability undermining its security premise.&lt;/li&gt;&lt;li&gt;Designs a practical adaptive black-box attack (DataFlip) that evades KAD, achieving detection rates down to 0% while inducing malicious behavior with ~91% success.&lt;/li&gt;&lt;li&gt;Provides formal characterization of KAD and empirical evaluation showing KAD can be reliably bypassed without white-box access or optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarthak Choudhary', 'Divyam Anshumaan', 'Nils Palumbo', 'Somesh Jha']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM defenses', 'adversarial attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05630</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Sample is Enough to Make Conformal Prediction Robust</title><link>https://arxiv.org/abs/2506.16553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RCP1, a method for robust conformal prediction that achieves robustness to bounded worst-case noise using a single randomized forward pass instead of many smoothing passes.&lt;/li&gt;&lt;li&gt;Key idea: certify the conformal procedure itself (using a binary certificate) rather than individual conformity scores, enabling smaller average prediction sets and lower compute.&lt;/li&gt;&lt;li&gt;Applicable to any black-box model and both classification and regression; also extended to smoothing-based robust conformal risk control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soroush H. Zargarbashi', 'Mohammad Sadegh Akhondzadeh', 'Aleksandar Bojchevski']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'conformal prediction', 'randomized smoothing', 'certified robustness', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16553</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Inversion Attacks for Graph Neural Networks</title><link>https://arxiv.org/abs/2506.00808</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the graph unlearning inversion attack: given black-box access to an unlearned GNN and partial graph knowledge, reconstruct edges that were supposedly removed.&lt;/li&gt;&lt;li&gt;Identifies two challenges (confidence-similarity threshold differences and locating unlearned edge endpoints) and introduces TrendAttack, which exploits a theoretical/empirical 'confidence pitfall' and uses adaptive prediction with trend features.&lt;/li&gt;&lt;li&gt;Integrates and extends existing membership inference techniques, applying different similarity thresholds for unlearned vs. retained edges.&lt;/li&gt;&lt;li&gt;Empirical results on four real-world datasets show TrendAttack significantly outperforms prior GNN membership inference baselines, revealing privacy vulnerabilities in current graph unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Zhang', 'Yilong Wang', 'Zhiwei Zhang', 'Xiaorui Liu', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Graph Neural Networks', 'Membership Inference', 'Model Inversion', 'Privacy/Unlearning', 'Adversarial Attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00808</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Judging LLMs: A Simplex Perspective</title><link>https://arxiv.org/abs/2505.21972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Represents LLM judges and candidate outputs as points on an (M-1)-dimensional probability simplex for M-level scoring, yielding geometric/visual proofs for ranking identifiability.&lt;/li&gt;&lt;li&gt;Derives theoretical conditions (e.g., why binary/2-level scoring is more reliable) and uses simplex geometry to analyze when LLM-as-judge rankings are valid.&lt;/li&gt;&lt;li&gt;Proposes Bayesian priors over judge quality to model epistemic uncertainty, enabling sensitivity analyses and producing higher coverage rates than prior procedures.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows LLM-only judging is often robust but can fail on some datasets, motivating explicit modeling of judge uncertainty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Vossler', 'Fan Xia', 'Yifan Mai', 'Adarsh Subbaswamy', 'Jean Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety evaluation', 'epistemic uncertainty', 'Bayesian methods', 'benchmarking/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21972</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the robustness of adversarial defenses in malware detection systems</title><link>https://arxiv.org/abs/2505.09342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Prioritized Binary Rounding to map continuous adversarial perturbations into binary feature spaces while preserving attack efficacy and small perturbation size.&lt;/li&gt;&lt;li&gt;Proposes the sigma-binary attack, a novel method tailored to binary domains that achieves high evasion rates with minimal feature modifications.&lt;/li&gt;&lt;li&gt;Empirical evaluation on the Malscan dataset shows sigma-binary outperforms existing attacks and reveals severe brittleness in many state-of-the-art defenses (detectors and adversarially trained models).&lt;/li&gt;&lt;li&gt;Findings demonstrate that defenses previously considered robust (including PAD-SMA and adversarial training variants) can be largely bypassed, highlighting gaps in robustness evaluation for binary-constrained malware detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mostafa Jafari', 'Alireza Shameli-Sendi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'malware detection', 'binary feature domains', 'robustness evaluation', 'evasion attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.09342</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Process Reward Models That Think</title><link>https://arxiv.org/abs/2504.16828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ThinkPRM, a generative process reward model that uses long chain-of-thought (CoT) verifications to provide step-level rewards while requiring far fewer process labels.&lt;/li&gt;&lt;li&gt;Shows ThinkPRM outperforms discriminative PRMs and LLM-as-a-Judge across benchmarks (ProcessBench, MATH-500, AIME '24) and on out-of-domain subsets, while enabling more effective test-time compute scaling (best-of-N and reward-guided search).&lt;/li&gt;&lt;li&gt;Demonstrates strong data efficiency (competitive performance using ~1% of PRM800K labels) and improved OOD generalization; code and models are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Rishabh Agarwal', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Hao Peng', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Process Reward Models', 'Chain-of-Thought Verification', 'Alignment / Safety Evaluation', 'Model Verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16828</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act</title><link>https://arxiv.org/abs/2501.12962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides an interdisciplinary analysis bridging EU non-discrimination law and ML algorithmic fairness concepts in the AI Act.&lt;/li&gt;&lt;li&gt;Finds that most non-discrimination rules apply specifically to high-risk AI systems and that regulation covers both input/data requirements and output monitoring.&lt;/li&gt;&lt;li&gt;Identifies inconsistencies and computational feasibility concerns in the AI Act's treatment of fairness, and explores interactions between classical non-discrimination law and the AI Act.&lt;/li&gt;&lt;li&gt;Recommends development of concrete auditing and testing methodologies for detecting discriminatory behavior in AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kristof Meding']&lt;/li&gt;&lt;li&gt;Tags: ['algorithmic fairness', 'AI Act / regulation', 'auditing and testing', 'legal compliance', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.12962</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics to evaluate LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Context-&gt;Query and Context-&gt;Answer, and Semantic Entropy Production (SEP) motivated by thermodynamics.&lt;/li&gt;&lt;li&gt;Infers the Q and A transition matrices jointly via convex optimization to compute SF (mapped to [0,1]) and relates higher SF to lower SEP, claiming utility for hallucination detection/control.&lt;/li&gt;&lt;li&gt;Demonstrates the framework on LLM summarization of SEC 10-K filings; metrics are intended for LLM evaluation and hallucination management.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness_evaluation', 'alignment/safety', 'evaluation_metrics', 'information-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI</title><link>https://arxiv.org/abs/2512.03072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel cognitive architecture called 'Weight-Calculatism' that decomposes cognition into atomic Logical Atoms and two operations (Pointing and Comparison).&lt;/li&gt;&lt;li&gt;Formalizes decision-making via an interpretable Weight-Calculation model (Weight = Benefit * Probability) with auditable Initial Weights to enable traceable value alignment.&lt;/li&gt;&lt;li&gt;Provides a graph-algorithm-based implementation, global workspace workflow, and preliminary code/scenario validation claiming transparent, human-like reasoning and robust learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hu Keyi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'explainability', 'AGI', 'interpretability', 'value-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03072</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unsupervised decoding of encoded reasoning using language model interpretability</title><link>https://arxiv.org/abs/2512.01222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tunes a reasoning LLM to perform chain-of-thought in ROT-13 (encoded reasoning) while producing readable outputs, creating a controlled testbed for hidden reasoning.&lt;/li&gt;&lt;li&gt;Evaluates mechanistic interpretability methods—especially logit lens—on their ability to decode the model's internal, encoded reasoning from activations, finding decoding peaks in intermediate-to-late layers.&lt;/li&gt;&lt;li&gt;Proposes a fully unsupervised decoding pipeline combining logit lens with automated paraphrasing to reconstruct reasoning transcripts from internal representations, demonstrating substantial accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ching Fang', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'mechanistic interpretability', 'model oversight', 'encoded reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01222</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Impact of Off-Policy Training Data on Probe Generalisation</title><link>https://arxiv.org/abs/2511.17408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how using synthetic/off-policy LLM responses to train probes affects probe generalisation across eight behaviours (e.g., deception, sycophancy).&lt;/li&gt;&lt;li&gt;Finds that generation strategy and domain shifts significantly impact probe performance; same-domain off-policy data often outperforms cross-domain on-policy data. &lt;/li&gt;&lt;li&gt;Shows that successful generalisation from off-policy to incentivised (advantageous) responses predicts generalisation to on-policy data, and flags Deception and Sandbagging as likely to fail off-to-on-policy generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathalie Kirch', 'Samuel Dower', 'Adrians Skapars', 'Ekdeep Singh Lubana', 'Dmitrii Krasheninnikov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'probe generalisation', 'safety evaluation', 'distribution shift', 'behavioral detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17408</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title><link>https://arxiv.org/abs/2511.11551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a test-time model-guided policy-shaping method to steer pre-trained RL agents toward specified ethical/behavioral attributes without retraining.&lt;/li&gt;&lt;li&gt;Evaluates approach on the MACHIAVELLI benchmark (134 text-based game environments, thousands of annotated ethical scenarios) and compares against prior training-time methods and general-purpose agents.&lt;/li&gt;&lt;li&gt;Demonstrates controllable trade-offs between reward maximization and alignment, and reports mitigation of ethical violations and power-seeking behaviors via scenario-action attribute classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dena Mujtaba', 'Brian Hu', 'Anthony Hoogs', 'Arslan Basharat']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'RL safety', 'test-time mitigation', 'behavior steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11551</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RE-PO: Robust Enhanced Policy Optimization as a General Framework for LLM Alignment</title><link>https://arxiv.org/abs/2509.24159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Robust Enhanced Policy Optimization (RE-PO): an EM-based method that infers posterior correctness of each preference label and reweights data points to mitigate label noise.&lt;/li&gt;&lt;li&gt;Provides a theoretical connection between arbitrary preference losses and underlying probabilistic models, enabling systematic conversion of existing alignment algorithms into robust variants.&lt;/li&gt;&lt;li&gt;Proves that under a perfectly calibrated model RE-PO recovers the true dataset noise level.&lt;/li&gt;&lt;li&gt;Empirically improves several state-of-the-art alignment methods (DPO, IPO, SimPO, CPO) on Mistral and Llama 3, boosting AlpacaEval 2 win rates up to 7%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyang Cao', 'Zelai Xu', 'Mo Guang', 'Kaiwen Long', 'Michiel A. Bakker', 'Yu Wang', 'Chao Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'RLHF', 'label-noise', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24159</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</title><link>https://arxiv.org/abs/2508.07334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes LLMs as probabilistic Turing machines and proves hallucinations are inevitable on computability/diagonalization and information-theoretic boundaries via a new "learner pump lemma."&lt;/li&gt;&lt;li&gt;Models Retrieval-Augmented Generation (RAG) as oracle machines, proving RAGs can 'escape' those limits through computational jumps and providing a formal theory for their effectiveness.&lt;/li&gt;&lt;li&gt;Formalizes continuous learning as an internalized oracle using a neural game theory framework and proposes Computational Class Alignment (CCA) as a security principle requiring match between task complexity and system compute.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Xi', 'Quan Shi', 'Zenghui Ding', 'Jianqing Gao', 'Xianjun Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'Retrieval-Augmented Generation (RAG)', 'Computability &amp; theory', 'AI safety / alignment', 'Continuous learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07334</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety</title><link>https://arxiv.org/abs/2507.11473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes monitoring models' chain-of-thought (CoT) outputs as an oversight mechanism to detect intent to misbehave.&lt;/li&gt;&lt;li&gt;Argues CoT monitoring is imperfect but a promising complementary safety tool and recommends further research and investment.&lt;/li&gt;&lt;li&gt;Warns that CoT monitorability is fragile and that model development choices can degrade monitorability, advising caution by frontier developers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomek Korbak', 'Mikita Balesni', 'Elizabeth Barnes', 'Yoshua Bengio', 'Joe Benton', 'Joseph Bloom', 'Mark Chen', 'Alan Cooney', 'Allan Dafoe', 'Anca Dragan', 'Scott Emmons', 'Owain Evans', 'David Farhi', 'Ryan Greenblatt', 'Dan Hendrycks', 'Marius Hobbhahn', 'Evan Hubinger', 'Geoffrey Irving', 'Erik Jenner', 'Daniel Kokotajlo', 'Victoria Krakovna', 'Shane Legg', 'David Lindner', 'David Luan', 'Aleksander M\\k{a}dry', 'Julian Michael', 'Neel Nanda', 'Dave Orr', 'Jakub Pachocki', 'Ethan Perez', 'Mary Phuong', 'Fabien Roger', 'Joshua Saxe', 'Buck Shlegeris', "Mart\\'in Soto", 'Eric Steinberger', 'Jasmine Wang', 'Wojciech Zaremba', 'Bowen Baker', 'Rohin Shah', 'Vlad Mikulik']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Chain-of-Thought', 'Model oversight', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11473</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CoP: Agentic Red-teaming for Large Language Models using Composition of Principles</title><link>https://arxiv.org/abs/2506.00781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoP (Composition-of-Principles), an agentic workflow that composes human-provided red‑teaming principles to automatically generate and orchestrate jailbreak/adversarial prompts against LLMs.&lt;/li&gt;&lt;li&gt;Automates and scales red‑teaming by having an AI agent synthesize strategies from principles to discover novel jailbreak attacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation on leading LLMs shows substantial increases in single‑turn attack success (up to 19×) and discovery of new safety vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Xiong', 'Pin-Yu Chen', 'Tsung-Yi Ho']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'automated red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00781</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</title><link>https://arxiv.org/abs/2505.11227</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows pure RL problem-solving training can implicitly induce PRM-like introspective capabilities in LLMs, questioning the necessity of explicit process reward models.&lt;/li&gt;&lt;li&gt;Finds existing PRMs underperform simple baselines (e.g., majority voting) on models like DeepSeek-R1 and QwQ-32B.&lt;/li&gt;&lt;li&gt;Proposes Self-PRM (model self-evaluation and reranking) which improves benchmark accuracy but exhibits low precision (&lt;10%) on difficult problems and often misclassifies flawed solutions.&lt;/li&gt;&lt;li&gt;Concludes that problem-solving ability and process-supervision capability co-evolve during RL training and that further RL scaling is needed to improve reward alignment and introspective accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhangying Feng', 'Qianglong Chen', 'Ning Lu', 'Yongqian Li', 'Siqi Cheng', 'Shuangmu Peng', 'Duyu Tang', 'Shengcai Liu', 'Zhirui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward alignment', 'self-evaluation/introspection', 'reinforcement learning for LLMs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11227</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach</title><link>https://arxiv.org/abs/2512.07814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs a dataset of diverse PII types in code and fine-tunes representative code LLMs to study learning and leakage behaviors.&lt;/li&gt;&lt;li&gt;Computes training dynamics on real PII examples and applies a structural causal model to estimate the causal effect of learnability on leakage.&lt;/li&gt;&lt;li&gt;Finds PII-type-dependent leakage: easy-to-learn types (e.g., IP addresses) leak more, while harder types (e.g., keys/passwords) leak less, suggesting type-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Yang (North Carolina State University)', 'Alejandro Velasco (William &amp; Mary)', 'Sen Fang (North Carolina State University)', 'Bowen Xu (North Carolina State University)', 'Denys Poshyvanyk (William &amp; Mary)']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-leakage', 'training-dynamics', 'causal-inference', 'code-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07814</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title><link>https://arxiv.org/abs/2512.07801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Collaborative Causal Sensemaking (CCS): a research agenda/framework for AI decision-support agents that act as partners in experts' collaborative cognitive processes.&lt;/li&gt;&lt;li&gt;Argues current LLM-based support fails to deliver complementarity; proposes agents that co-construct mental models, articulate/revise goals, and jointly test causal hypotheses with human experts.&lt;/li&gt;&lt;li&gt;Outlines challenges around training ecologies, representations and interaction protocols for co-authored models, and evaluation metrics centered on trust, complementarity, and learning from joint outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raunak Jain', 'Mudita Khurana']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI collaboration', 'decision-support', 'safety-evaluation', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07801</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title><link>https://arxiv.org/abs/2512.07730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAVE, a training-free framework that mitigates object hallucination in Multimodal LLMs by steering model activations along Sparse Autoencoder (SAE) latent features identified as visual-understanding features.&lt;/li&gt;&lt;li&gt;Uses a binary object-presence QA probe to identify SAE features most indicative of visual processing, then steers the model along these features to reinforce grounded visual understanding.&lt;/li&gt;&lt;li&gt;Reports consistent gains across benchmarks (e.g., ~10 percentage-point improvement on CHAIR_S, improvements on POPE and MMHal-Bench) and across multiple models/layers, with analyses showing reduced uncertain object-token generation and increased attention to image tokens.&lt;/li&gt;&lt;li&gt;Code is released and evaluations claim robustness and generalizability of the approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangha Park', 'Seungryong Yoo', 'Jisoo Mok', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'hallucination mitigation', 'robustness', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07730</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title><link>https://arxiv.org/abs/2512.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free self-correction framework for VLMs that iteratively refines responses via uncertainty-guided visual re-attention while keeping the pretrained model frozen.&lt;/li&gt;&lt;li&gt;Combines multiple uncertainty signals (token entropy, attention dispersion, semantic consistency, claim confidence) to identify under-explored image regions and perform attention-guided cropping for re-evaluation.&lt;/li&gt;&lt;li&gt;Validated on POPE and MMHAL BENCH using Qwen2.5-VL-7B, showing a 9.8 percentage-point reduction in hallucination rates and a 4.7-point improvement in object existence accuracy on adversarial splits.&lt;/li&gt;&lt;li&gt;Releases code/methodology and emphasizes grounding corrections in visual evidence to improve reliability of multimodal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kassoum Sanogo', 'Renzo Ardiccioni']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'vision-language models', 'uncertainty quantification', 'robustness/grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07564</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG</title><link>https://arxiv.org/abs/2512.07515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPAD, a method that decomposes each token's probability in a RAG model into seven sources: Query, RAG (retrieved context), Past (previous tokens), Current Token, FFN, Final LayerNorm, and Initial Embedding.&lt;/li&gt;&lt;li&gt;Aggregates source-attribution scores by part-of-speech (POS) tags to identify linguistic categories where unusual source reliance indicates hallucination (e.g., nouns overly driven by Final LayerNorm).&lt;/li&gt;&lt;li&gt;Uses these syntactically aggregated attribution anomalies to detect hallucinations in RAG and reports state-of-the-art detection performance in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengqian Lu', 'Jie Lu', 'Anjin Liu', 'Guangquan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'model interpretability', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07515</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Real-World Traffic Data to Relevant Critical Scenarios</title><link>https://arxiv.org/abs/2512.07482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes real-world highway lane-change trajectory data to identify safety-relevant (critical) driving scenarios using calculated criticality measures.&lt;/li&gt;&lt;li&gt;Describes data acquisition and processing pipeline to link criticality measures to specific lane-change conditions and scenarios.&lt;/li&gt;&lt;li&gt;Proposes generating synthetic critical scenarios by sampling from recorded scenarios to expand coverage of 'unknown unsafe' cases.&lt;/li&gt;&lt;li&gt;Demonstrates and evaluates an end-to-end processing chain for extracting, labeling, and synthesizing safety-relevant scenarios for validation of driving functions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Florian L\\"uttner', 'Nicole Neis', 'Daniel Stadler', 'Robin Moss', 'Mirjam Fehling-Kaschek', 'Matthias Pfriem', 'Alexander Stolz', 'Jens Ziehn']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'scenario-generation', 'critical-scenarios', 'simulation-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07482</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title><link>https://arxiv.org/abs/2512.07462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends the FAIRGAME framework to evaluate LLM strategic behaviour in repeated social dilemmas (payoff-scaled Prisoner’s Dilemma and multi-agent Public Goods Game).&lt;/li&gt;&lt;li&gt;Finds consistent behavioural signatures across models and languages: incentive-sensitive cooperation, cross-linguistic divergence, and end-game drift toward defection.&lt;/li&gt;&lt;li&gt;Uses supervised classification of canonical repeated-game strategies to interpret LLM trajectories, revealing model- and language-dependent intentions and cooperation biases.&lt;/li&gt;&lt;li&gt;Discusses implications for auditing LLMs as strategic agents and for AI governance, collective decision-making, and designing safe multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung-Kiet Huynh', 'Duy-Minh Dao-Sy', 'Thanh-Bang Cao', 'Phong-Hao Le', 'Hong-Dan Nguyen', 'Phu-Quy Nguyen-Lam', 'Minh-Luan Nguyen-Vo', 'Hong-Phat Pham', 'Phu-Hoa Pham', 'Thien-Kim Than', 'Chi-Nguyen Tran', 'Huy Tran', 'Gia-Thoai Tran-Le', 'Alessio Buscemi', 'Le Hong Trang', 'The Anh Han']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'multi-agent systems', 'game theory', 'behavioral auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07462</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forget and Explain: Transparent Verification of GNN Unlearning</title><link>https://arxiv.org/abs/2512.07450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainability-driven verifier for GNN unlearning that compares model snapshots before and after deletion using five explainability metrics (residual attribution, heatmap shift, explainability score deviation, graph edit distance, diagnostic graph rule shift).&lt;/li&gt;&lt;li&gt;Evaluates two GNN backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five graph benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics).&lt;/li&gt;&lt;li&gt;Finds Retrain and GNNDelete achieve near-complete forgetting, GraphEditor partial erasure, and IDEA leaves residual signals; uses explanation deltas as primary human-readable evidence and membership-inference ROC-AUC as a complementary privacy signal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Imran Ahsan (Department of Smart Cities', 'Chung-Ang University)', 'Hyunwook Yu (Department of Computer Science and Engineering', 'Chung-Ang University)', 'Jinsung Kim (Department of Computer Science and Engineering', 'Chung-Ang University)', 'Mucheol Kim (Department of Computer Science and Engineering', 'Chung-Ang University)']&lt;/li&gt;&lt;li&gt;Tags: ['GNN unlearning', 'model auditing', 'explainability', 'privacy verification', 'membership inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07450</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</title><link>https://arxiv.org/abs/2512.07426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that deep-learning-based WSI normalization can introduce realistic but spurious hallucinated content that is hard to detect visually and can harm downstream clinical analysis.&lt;/li&gt;&lt;li&gt;Demonstrates that conventional evaluation metrics miss these failures, and that retraining on real-world clinical data increases hallucination frequency compared to public datasets.&lt;/li&gt;&lt;li&gt;Proposes a novel image comparison measure to automatically detect hallucinations in normalized outputs and uses it to systematically evaluate several well-cited normalization methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karel Moens', 'Matthew B. Blaschko', 'Tinne Tuytelaars', 'Bart Diricx', 'Jonas De Vylder', 'Mustafa Yousif']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'hallucination-detection', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07426</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Trust the Code They Write?</title><link>https://arxiv.org/abs/2512.07404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identify an internal 'correctness' representation in LLM hidden states by contrasting pairs of correct vs incorrect code for the same task.&lt;/li&gt;&lt;li&gt;Demonstrate across four LLMs that exploiting this internal signal outperforms standard log-likelihood ranking and verbalized model confidence for selecting correct code samples.&lt;/li&gt;&lt;li&gt;Show that the extracted correctness signal can be used to choose higher-quality code without executing tests, improving reliability of generated code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francisco Ribeiro', 'Claudio Spiess', 'Prem Devanbu', 'Sarah Nadi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reliability', 'confidence estimation', 'model internals', 'code generation', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07404</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</title><link>https://arxiv.org/abs/2512.07351</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepAgent, a dual-agent multimodal framework for deepfake detection: Agent-1 uses an AlexNet-based CNN on video frames; Agent-2 detects audio-visual inconsistencies using acoustic features, Whisper transcriptions, and EasyOCR-extracted frame text.&lt;/li&gt;&lt;li&gt;Decisions from the two agents are fused via a Random Forest meta-classifier to leverage complementary decision boundaries and improve robustness.&lt;/li&gt;&lt;li&gt;Evaluated on Celeb-DF, FakeAVCeleb, and DeepFakeTIMIT with component- and fusion-level metrics; reports strong cross-dataset performance (e.g., meta-classifier 97.49% on DeepFakeTIMIT).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayeem Been Zaman', 'Wasimul Karim', 'Arefin Ittesafun Abian', 'Reem E. Mohamed', 'Md Rafiqul Islam', 'Asif Karim', 'Sami Azam']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal fusion', 'audio-visual consistency', 'forensic robustness', 'ensemble/meta-classifier']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07351</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title><link>https://arxiv.org/abs/2512.07228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses defenses against DeepFake face swapping by embedding invisible protective perturbations into images to prevent realistic identity forgeries.&lt;/li&gt;&lt;li&gt;Shows standard Expectation over Transformation (EOT) with uniform sampling is suboptimal via analysis of 30 transformations across six categories; robustness is highly sensitive to training transformation choice.&lt;/li&gt;&lt;li&gt;Proposes Expectation Over Learned distribution of Transformation (EOLT): a policy network learned with reinforcement learning that prioritizes critical transformations and generates instance-specific perturbations.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains over prior methods (≈26% higher average robustness, up to 30% improvements on hard transformation categories).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengyang Yao', 'Lin Li', 'Ke Sun', 'Jianing Qiu', 'Huiping Chen']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake defense', 'adversarial robustness', 'protective perturbations', 'expectation-over-transformation', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07228</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations</title><link>https://arxiv.org/abs/2512.07122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RisConFix, an LLM-driven system that monitors drone flight state and automatically repairs risk-prone configuration parameters in real time.&lt;/li&gt;&lt;li&gt;Uses an iterative loop: detect abnormal flight behavior → prompt LLM to analyze parameter-state relationships → generate corrective parameter updates → re-evaluate and repeat if needed.&lt;/li&gt;&lt;li&gt;Evaluated on ArduPilot with 1,421 misconfiguration groups, achieving up to 97% repair success and an average of 1.17 repairs per recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liping Han', 'Tingting Nie', 'Le Yu', 'Mingzhe Hu', 'Tao Yue']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-based safety', 'runtime repair', 'drone/flight control safety', 'configuration robustness', 'automated mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07122</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking</title><link>https://arxiv.org/abs/2512.07086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ThinkTrap, an input-space optimization framework that crafts prompts causing LLMs to enter very long or non-terminating generation (DoS via "infinite thinking") in black-box service settings.&lt;/li&gt;&lt;li&gt;Method maps discrete tokens into a continuous embedding space and performs efficient black-box optimization in a low-dimensional subspace exploiting input sparsity to find adversarial prompts with minimal token overhead.&lt;/li&gt;&lt;li&gt;Evaluated against multiple commercial, closed-source LLM services; results show severe throughput degradation (down to ~1% of original capacity) and, in some cases, complete service failure even under typical rate limits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunzhe Li', 'Jianan Wang', 'Hongzi Zhu', 'James Lin', 'Shan Chang', 'Minyi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['DoS attacks', 'black-box adversarial attacks', 'prompt-based/jailbreak attacks', 'LLM robustness', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07086</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Transferring Clinical Knowledge into ECGs Representation</title><link>https://arxiv.org/abs/2512.07021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a three-stage training pipeline that transfers multimodal clinical knowledge (labs, vitals, biometrics) into a unimodal ECG encoder via joint-embedding self-supervised pretraining.&lt;/li&gt;&lt;li&gt;At inference the model requires only ECG signals but is trained to predict associated laboratory abnormalities from ECG embeddings as an indirect, physiologically grounded explanation.&lt;/li&gt;&lt;li&gt;Evaluated on MIMIC-IV-ECG, the ECG-only model outperforms a signal-only baseline and narrows the gap to a fully multimodal model, improving trustworthiness and clinical interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jose Geraldo Fernandes', 'Luiz Facury de Souza', 'Pedro Robles Dutenhefner', 'Gisele L. Pappa', 'Wagner Meira Jr']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'clinical AI', 'multimodal learning', 'self-supervised learning', 'safety/trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07021</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title><link>https://arxiv.org/abs/2512.07015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'retrieval sycophancy' in RAG systems where retrievers surface user-biased documents, causing LLMs to hallucinate with citations.&lt;/li&gt;&lt;li&gt;Proposes FVA-RAG, which replaces inductive verification with an adversarial retrieval policy that generates 'kill queries' to find contradictory evidence (deductive falsification).&lt;/li&gt;&lt;li&gt;Introduces a dual-verification mechanism that weighs draft answers against the retrieved 'anti-context' to reduce sycophantic hallucinations.&lt;/li&gt;&lt;li&gt;Reports preliminary experiments showing improved robustness on a dataset of common misconceptions and frames the approach as an inference-time red team for factual generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Ravishankara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'RAG robustness', 'adversarial retrieval', 'hallucination mitigation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07015</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Trust-Authorization Mismatch in LLM Agent Interactions</title><link>https://arxiv.org/abs/2512.06914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a formal risk-analysis model centered on a trust-authorization mismatch in LLM agent interactions.&lt;/li&gt;&lt;li&gt;Systematizes and classifies existing attacks and defenses against autonomous LLM agents through that lens.&lt;/li&gt;&lt;li&gt;Identifies research gaps and outlines systematic directions for building robust, trusted agents and dynamic authorization mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanquan Shi', 'Haohua Du', 'Zhiqiang Wang', 'Xiaoyu Liang', 'Weiwenpei Liu', 'Song Bian', 'Zhenyu Guan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'authorization', 'trust', 'agent security', 'SoK']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06914</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Description to Score: Can LLMs Quantify Vulnerabilities?</title><link>https://arxiv.org/abs/2512.06781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates general-purpose LLMs (ChatGPT, Llama, Grok, DeepSeek, Gemini) on automating CVSS scoring using &gt;31,000 CVE descriptions.&lt;/li&gt;&lt;li&gt;Finds varied performance across models and individual CVSS metrics (e.g., strong on Availability Impact, weaker on Attack Complexity); ChatGPT-5 achieves highest precision.&lt;/li&gt;&lt;li&gt;Ensemble/meta-classifiers yield only marginal gains; many misclassifications stem from ambiguous or under-specified CVE descriptions.&lt;/li&gt;&lt;li&gt;Concludes that improving vulnerability description quality and adding richer context is important for reliable automated triage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sima Jafarikhah', 'Daniel Thompson', 'Eva Deans', 'Hossein Siadati', 'Yi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-scoring', 'CVSS', 'LLM-evaluation', 'security-automation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06781</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2512.06774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RDSplat, a watermarking method for 3D Gaussian Splatting that targets components preserved by diffusion-based editing to maintain provenance.&lt;/li&gt;&lt;li&gt;Embeds watermarks into low-frequency Gaussians via coordinated covariance regularization and 2D filtering in a multi-domain 3DGS-native framework.&lt;/li&gt;&lt;li&gt;Uses adversarial fine-tuning with a diffusion proxy (Gaussian blur as an efficient surrogate) to improve robustness against diffusion-based editing.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness vs diffusion editing while preserving watermark invisibility on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longjie Zhao', 'Ziming Hong', 'Zhenyang Ren', 'Runnan Chen', 'Mingming Gong', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'robustness', 'diffusion editing', 'adversarial training', '3D Gaussian Splatting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06774</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance</title><link>https://arxiv.org/abs/2512.06747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PrivLLMSwarm, an MPC-based framework enabling privacy-preserving LLM inference for UAV swarm coordination by optimizing transformer components and approximating nonlinear activations for efficient encrypted inference on constrained platforms.&lt;/li&gt;&lt;li&gt;Uses a fine-tuned GPT-based command generator enhanced via reinforcement learning in simulation to produce coordination instructions while preserving confidentiality.&lt;/li&gt;&lt;li&gt;Evaluates in urban-scale simulations, reporting high semantic accuracy, low encrypted inference latency, robust formation control under privacy constraints, and a superior privacy-utility tradeoff compared to differential privacy, federated learning, and plaintext baselines; implementation and synthetic dataset released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jifar Wakuma Ayana', 'Huang Qiming']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving inference', 'secure multi-party computation (MPC)', 'LLM deployment security', 'UAV swarms', 'privacy in IoT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06747</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>"The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ</title><link>https://arxiv.org/abs/2512.06732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImplicitBBQ, an extension of the Bias Benchmark for QA that uses implicitly cued protected attributes (names, cultural cues, traits) across 6 categories.&lt;/li&gt;&lt;li&gt;Evaluates GPT-4o on ImplicitBBQ and finds consistent performance declines compared to explicit cues, up to a 7% drop in the sexual orientation subcategory.&lt;/li&gt;&lt;li&gt;Argues that existing explicit-bias benchmarks miss important implicit biases and provides a tool for more nuanced fairness and safety evaluation of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aarushi Wagh', 'Saniya Srivastava']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'benchmarking', 'safety-evaluation', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06732</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title><link>https://arxiv.org/abs/2512.06665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new definition of 'similar inputs' that incorporates differences in model outputs and introduces a new robustness metric for feature attribution methods.&lt;/li&gt;&lt;li&gt;Develops a GAN-based technique to generate input perturbations tailored for this robustness evaluation.&lt;/li&gt;&lt;li&gt;Provides a comprehensive evaluation comparing the new metric and generation method against existing metrics and state-of-the-art attribution techniques, arguing current metrics conflate attribution method weaknesses with model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panagiota Kiourti', 'Anu Singh', 'Preeti Duraipandian', 'Weichao Zhou', 'Wenchao Li']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'attribution-robustness', 'evaluation-metrics', 'GANs', 'adversarial-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06665</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title><link>https://arxiv.org/abs/2512.06655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Graph-Regularized Sparse Autoencoders (GSAEs) to learn distributed safety representations across multiple latent features via a Laplacian smoothness penalty on neuron co-activation graphs.&lt;/li&gt;&lt;li&gt;Uses a two-stage gating runtime steering mechanism that composes learned features into weighted safety directions and activates interventions only when harmful prompts or continuations are detected, aiming to enforce adaptive refusals while preserving utility.&lt;/li&gt;&lt;li&gt;Evaluated across safety benchmarks, QA tasks, multiple model families (LLaMA-3, Mistral, Qwen, Phi), and jailbreak attacks (GCG, AutoDAN); reports substantially higher selective refusal rates and robustness compared to standard SAE steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Federico Cinus', 'Yifan Wu', 'Luca Luceri']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'latent steering', 'adversarial prompts', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06655</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title><link>https://arxiv.org/abs/2512.06562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SUGAR, a scalable generative unlearning framework that removes many identities from 3D-aware generative models without full retraining.&lt;/li&gt;&lt;li&gt;Learns a personalized surrogate latent per identity to divert reconstructions to visually coherent alternatives rather than producing unrealistic outputs or static templates.&lt;/li&gt;&lt;li&gt;Proposes a continual utility preservation objective to prevent degradation as more identities are forgotten (supports simultaneous or sequential forgetting).&lt;/li&gt;&lt;li&gt;Demonstrates removal of up to 200 identities with up to 700% improvement in retention utility over existing baselines; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dung Thuy Nguyen', 'Quang Nguyen', 'Preston K. Robinette', 'Eli Jiang', 'Taylor T. Johnson', 'Kevin Leach']&lt;/li&gt;&lt;li&gt;Tags: ['generative unlearning', 'privacy-preservation', 'model editing', 'data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06562</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks</title><link>https://arxiv.org/abs/2512.06556</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies three semantic attack classes on the Model Context Protocol (MCP): Tool Poisoning, Shadowing, and Rug Pulls that manipulate tool descriptors/metadata.&lt;/li&gt;&lt;li&gt;Proposes a layered defense combining RSA-based manifest signing for descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and runtime heuristic guardrails to block anomalous tool behavior.&lt;/li&gt;&lt;li&gt;Evaluates the framework on GPT-4, DeepSeek, and Llama-3.5 across multiple prompting strategies, showing reduced unsafe tool invocations (e.g., GPT-4 blocks ~71% of unsafe calls) and trade-offs between latency and robustness.&lt;/li&gt;&lt;li&gt;Emphasizes deployable defenses that do not require model fine-tuning or internal modification, and reports varying resilience across model architectures and reasoning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeid Jamshidi', 'Kawser Wazed Nafi', 'Arghavan Moradi Dakhel', 'Negar Shahabi', 'Foutse Khomh', 'Naser Ezzati-Jivan']&lt;/li&gt;&lt;li&gt;Tags: ['tool poisoning', 'protocol security', 'LLM-on-LLM vetting', 'adversarial attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06556</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title><link>https://arxiv.org/abs/2512.06390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of AI-enhanced edge/CDN defenses for web applications and APIs, covering WAF/WAAP, adaptive DDoS mitigation, bot management, API discovery, and encrypted-traffic anomaly analysis.&lt;/li&gt;&lt;li&gt;Provides a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance for ML-driven protections at the edge.&lt;/li&gt;&lt;li&gt;Discusses risks introduced by edge ML (model abuse, poisoning, adversarial robustness) and proposes a research agenda including XAI, adversarial robustness, and autonomous multi-agent defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mehrab Hosain', 'Sabbir Alom Shuvo', 'Matthew Ogbe', 'Md Shah Jalal Mazumder', 'Yead Rahman', 'Md Azizul Hakim', 'Anukul Pandey']&lt;/li&gt;&lt;li&gt;Tags: ['Edge/CDN security', 'Adversarial robustness', 'Model poisoning', 'WAF/WAAP', 'Bot management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06390</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Protecting Bystander Privacy via Selective Hearing in LALMs</title><link>https://arxiv.org/abs/2512.06380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SH-Bench, the first benchmark to evaluate 'selective hearing'—models' ability to attend to a main speaker while refusing to process or reveal incidental bystander speech.&lt;/li&gt;&lt;li&gt;Proposes a unified metric, Selective Efficacy (SE), and provides a 3,968 multi-speaker dataset with 77k multiple-choice probes for general vs. selective modes.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art open-source and proprietary LALMs, demonstrating substantial bystander privacy leakage despite strong audio understanding.&lt;/li&gt;&lt;li&gt;Presents Bystander Privacy Fine-Tuning (BPFT), a training pipeline that improves refusal behavior and SE (up to +15.9% over Gemini 2.5 Pro) without degrading main-speaker comprehension.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Zhan', 'Guangzhi Sun', 'Jose Such', 'Phil Woodland']&lt;/li&gt;&lt;li&gt;Tags: ['bystander privacy', 'audio LLMs', 'privacy benchmark', 'selective hearing', 'defensive fine-tuning (BPFT)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06380</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title><link>https://arxiv.org/abs/2512.06343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes per-sample gradient of the Bradley-Terry (BT) loss for reward models and shows gradient norm decomposes into prediction error and representation distance between paired outputs.&lt;/li&gt;&lt;li&gt;Finds representation distance can dominate updates: small-distance pairs get vanishing updates even when misranked, while large-distance pairs get disproportionately large updates, harming fine-grained learning.&lt;/li&gt;&lt;li&gt;Proposes NormBT, a lightweight pair-wise normalization that counteracts representation-distance bias to focus learning on prediction error.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements across LLM backbones and datasets, with notable gains (&gt;5%) on Reasoning category of RewardBench containing many small-distance pairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Xie', 'Andrew Bai', 'Yuanhao Ban', 'Yunqi Hong', 'Haoyu Li', 'Cho-jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'RLHF', 'alignment', 'training dynamics', 'representation bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06343</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation</title><link>https://arxiv.org/abs/2512.06304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/overview of attacks and defenses on voice conversion (VC) systems framed around input manipulation (noise, reverberation, adversarial perturbations, small perturbations).&lt;/li&gt;&lt;li&gt;Classifies existing attack and defense methods and evaluates degraded-input effects across intelligibility, naturalness, timbre similarity, and subjective perception.&lt;/li&gt;&lt;li&gt;Identifies gaps, discusses optimization of attack/defense strategies, and outlines open issues and future directions for robust real-world VC deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xining Song', 'Zhihua Wei', 'Rui Wang', 'Haixiao Hu', 'Yanxiang Chen', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['voice-conversion', 'adversarial-attacks', 'robustness', 'input-manipulation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06304</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a lightweight, logit-based method to detect hidden conversational escalation by measuring how an LLM's output shifts the dialogue's affective state.&lt;/li&gt;&lt;li&gt;Targets implicit harm (affective drift and repeated emotional reinforcement) that standard toxicity filters and external classifiers miss.&lt;/li&gt;&lt;li&gt;Designed for real-time monitoring as a guardrail alternative to slower clinical rubrics or external classifiers, aimed at reducing gradual escalation in chatbot interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM guardrails', 'harm detection', 'affective computing', 'real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification</title><link>https://arxiv.org/abs/2512.06172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security of federated learning for camera-based road condition classification (RCC) against Targeted Label-Flipping Attacks (TLFAs).&lt;/li&gt;&lt;li&gt;Proposes DEFEND: a poisoned-model detection method using neuron-wise magnitude analysis and GMM clustering, plus a client rating/exclusion mechanism to discard poisoned contributions.&lt;/li&gt;&lt;li&gt;Evaluated across multiple FL-RCC models/tasks and compared to seven baselines; reports &gt;=15.78% improvement and restoration of attack-free performance under TLFA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng Liu', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'data-poisoning', 'label-flipping-attacks', 'model-robustness', 'malicious-client-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06172</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples</title><link>https://arxiv.org/abs/2512.06123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HiCert, a masking-based certified detection method for defending against adversarial patch attacks, specifically targeting cases where mutants are misclassified or inconsistently predicted.&lt;/li&gt;&lt;li&gt;Formulates a formal relation between harmful patched samples and their benign counterparts and derives a bound on the maximum confidence among potentially harmful (inconsistent) mutants to enable certification.&lt;/li&gt;&lt;li&gt;Ensures that harmful samples either have low confidence among same-predicted mutants or have at least one mutant predicted differently, allowing certification of both inconsistent and consistent samples.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art experimental results: certifying more benign samples, higher accuracy without warnings, and lower false silent ratio compared to prior certified detection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilin Zhou', 'Zhengyuan Wei', 'Haipeng Wang', 'Zhuo Wang', 'W. K. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'certified robustness', 'certified detection', 'provable defenses', 'masking-based methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06123</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</title><link>https://arxiv.org/abs/2512.06112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WAM-Flow, a vision-language-action model that frames ego-trajectory planning as discrete flow matching for parallel, coarse-to-fine trajectory generation.&lt;/li&gt;&lt;li&gt;Converts a pretrained autoregressive backbone to a non-causal flow model, introduces a metric-aligned numerical tokenizer preserving scalar geometry, and a geometry-aware flow objective.&lt;/li&gt;&lt;li&gt;Uses simulator-guided GRPO alignment to incorporate safety, ego progress, and comfort rewards while retaining parallel generation; reports strong closed-loop performance on NAVSIM v1.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifang Xu', 'Jiahao Cui', 'Feipeng Cai', 'Zhihao Zhu', 'Hanlin Shang', 'Shan Luan', 'Mingwang Xu', 'Neng Zhang', 'Yaoyi Li', 'Jia Cai', 'Siyu Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'motion-planning', 'safety-alignment', 'flow-matching', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06112</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Empathy by Design: Aligning Large Language Models for Healthcare Dialogue</title><link>https://arxiv.org/abs/2512.06097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Direct Preference Optimization (DPO)-based alignment framework to improve factual correctness, semantic coherence, and human-centric qualities (empathy, politeness, simplicity) in caregiver–patient dialogues.&lt;/li&gt;&lt;li&gt;Fine-tunes domain-adapted LLMs using pairwise human preference data where preferred responses emphasize supportive, accessible communication and rejected ones are overly technical or prescriptive.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in semantic alignment, factual accuracy, and human-centric evaluation scores across multiple open and proprietary LLMs, outperforming baselines and some commercial systems.&lt;/li&gt;&lt;li&gt;Provides open-source code to reproduce the preference-based alignment pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emre Umucu', 'Guillermina Solis', 'Leon Garza', 'Emilia Rivas', 'Beatrice Lee', 'Anantaa Kotal', 'Aritran Piplai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI safety', 'healthcare NLP', 'preference-based fine-tuning (DPO)', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06097</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models</title><link>https://arxiv.org/abs/2512.06062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box membership inference attack that repeatedly queries a generative model, clusters synthetic outputs, and uses cluster medoids/neighborhoods as proxies to infer or reconstruct training records.&lt;/li&gt;&lt;li&gt;Empirically demonstrates measurable membership leakage across healthcare, finance, and other sensitive domains, even when generators are trained with differential privacy or added noise.&lt;/li&gt;&lt;li&gt;Identifies structural distributional overlap (high-density neighborhood correspondence) between real and synthetic data as the root cause, exposing an under-explored attack surface in synthetic data pipelines.&lt;/li&gt;&lt;li&gt;Argues for stronger privacy guarantees that consider distributional/neighborhood inference rather than focusing solely on sample-level memorization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S. M. Mustaqim', 'Anantaa Kotal', 'Paul H. Yi']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy leakage', 'generative models', 'black-box attacks', 'differential privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06062</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Road of Adaptive AI for Precision in Cybersecurity</title><link>https://arxiv.org/abs/2512.06048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shares lessons from designing, building, and operating production-grade generative AI pipelines for cybersecurity.&lt;/li&gt;&lt;li&gt;Emphasizes continual adaptation mechanisms (retrieval- and model-level) to keep pace with changing knowledge, tooling, and threats.&lt;/li&gt;&lt;li&gt;Provides practical guidance and best practices for robustness, precision, and auditability in cyber defense deployments.&lt;/li&gt;&lt;li&gt;Identifies open research directions for making GenAI more robust and auditable in security contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Garg']&lt;/li&gt;&lt;li&gt;Tags: ['AI for cybersecurity', 'model adaptation', 'retrieval-augmented generation', 'robustness', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06048</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Auto-SPT: Automating Semantic Preserving Transformations for Code</title><link>https://arxiv.org/abs/2512.06042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Auto-SPT, an LLM-based framework that automatically constructs semantic-preserving transformations (SPTs) for code by generating diverse transformation specifications and implementations and composing them.&lt;/li&gt;&lt;li&gt;Provides formal analysis relating SPT diversity to the strength of composed transformations and shows Auto-SPT yields more diverse SPTs than prior approaches.&lt;/li&gt;&lt;li&gt;Empirically demonstrates these SPTs can substantially degrade state-of-the-art code clone detectors (i.e., adversarially evaluate models) and that augmenting training data with Auto-SPT improves robustness to real-world code transformations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Hooda', 'Mihai Christodorescu', 'Chuangang Ren', 'Aaron Wilson', 'Kassem Fawaz', 'Somesh Jha']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial robustness', 'Code-model red teaming', 'Dataset augmentation', 'LLM-based transformation generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06042</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Physics-Guided Deepfake Detection for Voice Authentication Systems</title><link>https://arxiv.org/abs/2512.06040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a physics-guided deepfake detection framework for voice authentication that fuses interpretable vocal-tract-dynamics features with self-supervised audio representations.&lt;/li&gt;&lt;li&gt;Processes fused representations through a multi-modal ensemble architecture and a Bayesian ensemble to provide uncertainty estimates for samples.&lt;/li&gt;&lt;li&gt;Explicitly targets a threat model combining advanced deepfake synthesis attacks and control-plane poisoning in federated learning at the network edge.&lt;/li&gt;&lt;li&gt;Claims robustness to both synthesis-based spoofing and distributed poisoning by leveraging physics-based features and uncertainty-aware edge learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Mohammadi', 'Keshav Sood', 'Dhananjay Thiruvady', 'Asef Nazari']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'federated learning security', 'poisoning attacks', 'uncertainty estimation', 'audio authentication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06040</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Auditing Games for Sandbagging</title><link>https://arxiv.org/abs/2512.07810</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines an auditing game where a red team fine-tunes models to simulate sandbagging (conditional underperformance) and a blue team applies black-box, model-internals, and training-based detection methods.&lt;/li&gt;&lt;li&gt;Finds black-box approaches can be defeated by imitation, linear-probe (model-internals) methods show promise but are vulnerable to adversarial behavior, and prompt-based elicitation is unreliable while training-based elicitation reliably elicits sandbagged capabilities but can raise benign models too (false positives).&lt;/li&gt;&lt;li&gt;Provides short-term recommendation (on-distribution training for elicitation), releases models and logs, and demonstrates a playable demo; calls for further research into robust sandbagging detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jordan Taylor', 'Sid Black', 'Dillon Bowen', 'Thomas Read', 'Satvik Golechha', 'Alex Zelenka-Martin', 'Oliver Makins', 'Connor Kissane', 'Kola Ayonrinde', 'Jacob Merizian', 'Samuel Marks', 'Chris Cundy', 'Joseph Bloom']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'sandbagging', 'auditing', 'model elicitation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07810</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title><link>https://arxiv.org/abs/2512.07795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonBENCH, a benchmarking library and multi-run protocol to quantify instability in LLM multi-step reasoning (quality and cost) and a public leaderboard.&lt;/li&gt;&lt;li&gt;Finds widespread high instability across models and reasoning strategies: similar average performance can have much wider confidence intervals, and top methods often have higher, less stable costs.&lt;/li&gt;&lt;li&gt;Analyzes how prompts, model families, and scale affect the trade-off between solve rate and stability and promotes variance-aware, reproducible reporting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nearchos Potamitis', 'Lars Klein', 'Akhil Arora']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reproducibility', 'uncertainty-quantification', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07795</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2512.07761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates black-box multi-turn jailbreak attacks on LLMs as a reinforcement learning problem optimizing final-turn harmfulness.&lt;/li&gt;&lt;li&gt;Introduces two auxiliary process rewards to mitigate sparse supervision: control intermediate output harmfulness and preserve semantic relevance across turns.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows consistently higher attack success rates across multiple models; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiqiao Xiong', 'Ouxiang Li', 'Zhuo Liu', 'Moxin Li', 'Wentao Shi', 'Fuli Feng', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'adversarial prompting', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07761</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</title><link>https://arxiv.org/abs/2512.07497</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical analysis of 900 execution traces from KAMI v0.1 across filesystem, text extraction, CSV analysis, and SQL tasks using Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1.&lt;/li&gt;&lt;li&gt;Fine-grained per-trial behavioral analysis finds model scale alone is not predictive of agentic robustness; DeepSeek's improved reliability attributed to post-training RL rather than size or architecture.&lt;/li&gt;&lt;li&gt;Identifies four recurring failure archetypes: premature ungrounded actions, over-helpfulness substituting missing entities, distractor-induced context pollution, and fragile execution under load.&lt;/li&gt;&lt;li&gt;Recommends agentic evaluation that emphasizes interactive grounding, recovery behavior, environment-aware adaptation, and training/design choices that enforce verification and source-of-truth adherence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['JV Roig']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Agentic evaluation', 'Tool-use robustness', 'Failure modes', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07497</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VIGIL: A Reflective Runtime for Self-Healing Agents</title><link>https://arxiv.org/abs/2512.07094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VIGIL, a reflective runtime that supervises a sibling agent to perform autonomous maintenance (diagnosis and repair) rather than executing tasks.&lt;/li&gt;&lt;li&gt;Ingests behavioral logs, maps events to an emotional representation stored in a decaying EmoBank, and produces an RBT diagnosis categorizing strengths, opportunities, and failures.&lt;/li&gt;&lt;li&gt;Generates guarded prompt updates (preserving core identity semantics) and read-only code repair proposals via a strategy engine operating on log evidence and code hotspots.&lt;/li&gt;&lt;li&gt;Implements a state-gated pipeline that enforces legal transitions with explicit errors; demonstrates meta-level self-repair in a reminder latency case study, including fallback diagnostics when internal tools fail.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'runtime monitoring', 'self-healing', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07094</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</title><link>https://arxiv.org/abs/2512.06835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DoGe, a dual-decoupling RL framework (Thinker and Solver) for vision-language models to prioritize context learning before problem solving, aiming to mitigate reward hacking and training collapse.&lt;/li&gt;&lt;li&gt;Introduces a two-stage RL post-training approach and an evolving curriculum (expanded domain corpus + evolving seed problem pool) to increase data diversity in data-scarce specialized domains.&lt;/li&gt;&lt;li&gt;Empirical results claim improved performance and more stable training compared to baselines, offering a scalable pathway for self-evolving LVLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingyu Li', 'Zheng Sun', 'Jingxuan Wei', 'Siyuan Li', 'Conghui He', 'Lijun Wu', 'Cheng Tan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-hacking', 'reinforcement-learning', 'vision-language', 'curriculum-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06835</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</title><link>https://arxiv.org/abs/2512.06749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DoVer, an intervention-driven debugging framework for LLM-based multi-agent systems that augments log-based hypothesis generation with active interventions (e.g., editing messages, altering plans) to validate and repair failures.&lt;/li&gt;&lt;li&gt;Shifts evaluation from single-step attribution accuracy to outcome-oriented metrics: whether interventions resolve failures or make measurable progress toward task success.&lt;/li&gt;&lt;li&gt;Empirical results: within Magnetic-One framework on GAIA and AssistantBench-derived datasets, DoVer converts 18–28% of failed trials to successes, achieves up to 16% milestone progress, and validates/refutes 30–60% of failure hypotheses; also recovers 49% of failed trials on GSMPlus with AG2.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Ma', 'Jue Zhang', 'Fangkai Yang', 'Yu Kang', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent', 'debugging', 'intervention-driven testing', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06749</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title><link>https://arxiv.org/abs/2512.06716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of autonomous LLM agents to Indirect Prompt Injection (IPI) attacks and argues existing defenses are fragmented, causing trade-offs between security, functionality, and efficiency.&lt;/li&gt;&lt;li&gt;Proposes the Cognitive Control Architecture (CCA) combining a pre-generated 'Intent Graph' for control-flow and data-flow integrity with a 'Tiered Adjudicator' that detects deviations and performs multi-dimensional deep reasoning to counter complex conditional attacks.&lt;/li&gt;&lt;li&gt;Evaluates CCA on the AgentDojo benchmark, reporting improved resistance to sophisticated attacks while maintaining efficiency and robustness, aiming to provide full-lifecycle cognitive supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Liang', 'Tianze Hu', 'Zaiye Chen', 'Mingjie Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'Indirect Prompt Injection', 'Prompt injection defense', 'Agent security', 'Runtime adjudication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06716</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Effect of Belief Boxes and Open-mindedness on Persuasion</title><link>https://arxiv.org/abs/2512.06573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and evaluates 'belief boxes' — prompt-space statements encoding propositional beliefs and their strengths for LLM-based agents.&lt;/li&gt;&lt;li&gt;Empirically studies how belief boxes and explicit 'open-mindedness' instructions affect agents' resistance to belief change and their persuasiveness in multi-agent debates.&lt;/li&gt;&lt;li&gt;Finds that belief statements and strength values materially influence susceptibility to persuasion, persuasiveness against opponents, and outcomes under peer-pressure (outnumbered) scenarios.&lt;/li&gt;&lt;li&gt;Argues that belief boxes are a feasible technique for steering reasoning and decision-making behaviors in LLM agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Onur Bilgin', 'Abdullah As Sami', 'Sriram Sai Vujjini', 'John Licato']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent persuasion', 'LLM behavior', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06573</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</title><link>https://arxiv.org/abs/2512.06406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UncertaintyZoo, a unified toolkit integrating 29 uncertainty quantification methods across five major categories with a standardized interface.&lt;/li&gt;&lt;li&gt;Demonstrates and evaluates these UQ methods on code vulnerability detection tasks using CodeBERT and ChatGLM3, showing the toolkit reveals prediction uncertainty effectively.&lt;/li&gt;&lt;li&gt;Provides an open-source implementation and demo to facilitate practical use and comparative research on predictive uncertainty in LLMs and related models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianzong Wu', 'Xiaohong Li', 'Lili Quan', 'Qiang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'predictive-uncertainty', 'safety-evaluation', 'tooling', 'LLM-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06406</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a controlled evaluation framework with four stress tests: (1) rule deletion (redundant vs essential), (2) contradictory evidence injection, (3) logic-preserving rewrites (contrapositive, De Morgan, double negation, implication, identity, commutativity), and (4) multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates across BERT, Qwen2, and LLaMA-like models; models perform perfectly on base tasks and are robust to redundant-rule deletion and equivalence rewrites, but fail markedly on essential-rule deletion (~25% accuracy) and collapse with explicit contradictions (0%).&lt;/li&gt;&lt;li&gt;Finds LLMs are invariant to semantic-preserving transformations but brittle to missing or conflicting evidence; presents a diagnostic benchmark for isolating reasoning failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'evaluation/benchmarking', 'LLM generalisation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment</title><link>https://arxiv.org/abs/2512.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARCANE, a multi-agent framework that represents stakeholder preferences as natural-language rubrics (weighted, verifiable criteria) which can be generated and adapted at test time.&lt;/li&gt;&lt;li&gt;Formulates rubric learning as a reconstruction problem and introduces a regularized Group-Sequence Policy Optimization (GSPO) to train reward models that balance interpretability, faithfulness, and computation.&lt;/li&gt;&lt;li&gt;Evaluates on a corpus of 219 labeled rubrics from GDPVal for long-horizon, multi-step tasks with tool use; shows rubrics yield compact, legible evaluations and enable configurable trade-offs without retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlie Masters', "Marta Grze\\'skiewicz", 'Stefano V. Albrecht']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward models', 'interpretability', 'test-time adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06196</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals</title><link>https://arxiv.org/abs/2512.05998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pilot study testing whether framing LLM evaluation as a fictional prediction market (wagers in LLMCoin) elicits calibrated confidence and improves forecasting of other models' correctness.&lt;/li&gt;&lt;li&gt;Design: 100 math/logic items; 6 baseline models answered; 3 predictor models forecasted baseline correctness under Control (binary) vs Incentive (wagers 1–100,000 from a 1,000,000 bankroll) across rounds.&lt;/li&gt;&lt;li&gt;Results: Modest overall accuracy gain for Incentive (81.5% vs 79.1%, p=.089) and significantly faster learning across rounds (12.0 vs 2.9 pp improvement, p=.011). Stake size strongly tracked confidence—large bets (~40k+) were ~99% correct, small bets (&lt;1k) ~74% correct.&lt;/li&gt;&lt;li&gt;Implication: Betting framing produces a legible confidence signal from LLMs useful for meta-evaluation and risk-aware forecasting, even if accuracy gains are modest in this pilot.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Todasco (Visiting Fellow at the James Silberrad Center for Artificial Intelligence', 'San Diego State University)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'confidence calibration', 'forecasting / prediction markets', 'meta-evaluation', 'alignment / safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05998</guid><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
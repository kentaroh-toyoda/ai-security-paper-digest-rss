<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 06 Feb 2026 23:36:52 +0000</lastBuildDate><item><title>Personalized Safety Alignment for Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2508.01151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Personalized Safety Alignment (PSA) to adapt text-to-image diffusion model safety behavior to individual user profiles rather than using a single static filter.&lt;/li&gt;&lt;li&gt;Introduces Sage, a large-scale dataset of 1,000 simulated user profiles capturing diverse safety boundaries, and integrates profiles via a parameter-efficient cross-attention adapter to modulate generation.&lt;/li&gt;&lt;li&gt;Shows PSA can relax over-cautious constraints under permissive profiles and enforce stronger suppression under restrictive profiles, improving the safety–quality trade-off and instruction adherence versus static baselines and prompt engineering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Hongcheng Gao', 'Xiao Zhang', 'Rex Ying']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'personalization', 'text-to-image', 'diffusion-models', 'safety-defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01151</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Imperceptible Protection against Style Imitation from Diffusion Models</title><link>https://arxiv.org/abs/2403.19254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against style imitation by diffusion models via imperceptible adversarial perturbations that preserve visual quality of artworks.&lt;/li&gt;&lt;li&gt;Introduces a perceptual map to identify eye-sensitive regions and an instance-aware refinement to modulate protection intensity per region.&lt;/li&gt;&lt;li&gt;Adds a difficulty-aware protection module that predicts how hard an artwork is to protect and adjusts perturbation strength dynamically.&lt;/li&gt;&lt;li&gt;Uses a perceptual constraints bank to further improve imperceptibility while maintaining protection efficacy; demonstrates higher visual quality without compromising protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Namhyuk Ahn', 'Wonhyuk Ahn', 'KiYoon Yoo', 'Daesik Kim', 'Seung-Hun Nam']&lt;/li&gt;&lt;li&gt;Tags: ['defenses', 'adversarial perturbations', 'copyright protection', 'diffusion models', 'imperceptibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.19254</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Erase at the Core: Representation Unlearning for Machine Unlearning</title><link>https://arxiv.org/abs/2602.05375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Erase at the Core (EC), a framework to enforce forgetting across intermediate network layers rather than only at the classifier head.&lt;/li&gt;&lt;li&gt;EC attaches auxiliary supervision modules to intermediate layers and applies multi-layer contrastive unlearning on the forget set plus cross-entropy preservation on the retain set with layer-wise weighting.&lt;/li&gt;&lt;li&gt;Demonstrates reduced representational similarity to the original model across layers while preserving performance on the retain set; can be plugged into existing unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaewon Lee', 'Yongwoo Kim', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy defense', 'representation forgetting', 'contrastive unlearning', 'deep supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05375</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Poster: Camera Tampering Detection for Outdoor IoT Systems</title><link>https://arxiv.org/abs/2602.05706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two methods for detecting camera tampering in outdoor IoT systems: a rule-based approach and a deep-learning-based approach tailored for single still images.&lt;/li&gt;&lt;li&gt;Evaluates methods on accuracy, computational cost, and training-data requirements in real-world scenarios; finds the deep model yields higher accuracy while the rule-based method suits resource-constrained settings with limited calibration.&lt;/li&gt;&lt;li&gt;Provides and releases datasets containing normal, blurred, and rotated images to support future development and evaluation of camera tampering detection techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Poster)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shadi Attarha', 'Kanaga Shanmugi', 'Anna F\\"orster']&lt;/li&gt;&lt;li&gt;Tags: ['camera tampering detection', 'surveillance security', 'anomaly detection', 'deep learning', 'IoT security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05706</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing</title><link>https://arxiv.org/abs/2602.05629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ROMAN, a scenario-generation method using multi-head attention plus a traffic-law weighting module (LLM-based) to produce high-risk, law-violating scenarios for testing Automated Driving Systems (ADS).&lt;/li&gt;&lt;li&gt;Targets generation of complex, multi-vehicle interactions and diverse violation types; evaluated in CARLA against Baidu Apollo, outperforming prior tools (ABLE, LawBreaker) in violation count and diversity.&lt;/li&gt;&lt;li&gt;Aims to enable more thorough safety evaluation / red-teaming of ADS by systematically producing violation scenarios across all input traffic-law clauses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianlei Chi', 'Yuzhen Wu', 'Jiaxuan Hou', 'Xiaodong Zhang', 'Ming Fan', 'Suhui Sun', 'Weijun Dai', 'Bo Li', 'Jianguo Sun', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'adversarial-scenario-generation', 'safety-testing', 'red-teaming', 'simulation-CARLA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05629</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization</title><link>https://arxiv.org/abs/2602.05577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents LocateEdit-Bench, a large-scale dataset of 231K images edited via instruction-based image editing models to benchmark manipulation localization.&lt;/li&gt;&lt;li&gt;Includes edits from four state-of-the-art editing models across three edit types and provides multi-metric evaluation protocols for localization methods.&lt;/li&gt;&lt;li&gt;Aims to evaluate and drive development of forgery localization/forensics methods against modern instruction-driven editing paradigms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyu Wu', 'Shuyan Li', 'Jing Li', 'Jing Liu', 'Yequan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'forgery-localization', 'benchmark', 'defense', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05577</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</title><link>https://arxiv.org/abs/2602.05339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAIRed Erasing (PAIR), a framework for consistency-preserving concept erasure in text-to-image diffusion models by using unsafe-safe paired examples to realign semantics rather than just remove concepts.&lt;/li&gt;&lt;li&gt;Introduces two main components: Paired Semantic Realignment (guided objective mapping unsafe concepts to safe anchors) and Fisher-weighted Initialization for low-rank adaptation (DoRA) to bias generation toward safe alternatives while suppressing targeted unsafe concepts.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing improved concept erasure that preserves structural and semantic consistency and generation quality compared to state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwoo Kim', 'Sungmin Cha', 'Hyunsoo Kim', 'Jaewon Lee', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['concept-erasure', 'content-moderation', 'model-safety', 'diffusion-models', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05339</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification</title><link>https://arxiv.org/abs/2602.05175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ShapePuri, an adversarial purification defense that aligns model representations with stable structural invariants using shape guidance.&lt;/li&gt;&lt;li&gt;Introduces two components: a Shape Encoding Module (SEM) using Signed Distance Functions (SDF) for dense geometric guidance, and a Global Appearance Debiasing (GAD) module applying stochastic transformations to reduce appearance bias.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (84.06% clean accuracy and 81.64% robust accuracy) under the AutoAttack protocol and claims scalable, efficient inference without auxiliary modules or extra computational cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Li', 'Bernhard Kainz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-defense', 'purification', 'robustness', 'shape-guided', 'autoattack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05175</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy</title><link>https://arxiv.org/abs/2602.04994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SIDeR, a framework that decomposes face images into an identity feature vector and a semantic appearance component and uses latent-space recomposition in a diffusion model to generate visually anonymous adversarial faces.&lt;/li&gt;&lt;li&gt;Employs momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to produce diverse, natural-looking adversarial samples while preserving machine-level identity consistency; supports password-based restoration to recover original images for authorized access.&lt;/li&gt;&lt;li&gt;Evaluated on CelebA-HQ and FFHQ, reporting 99% attack success in black-box scenarios and substantial improvements in PSNR-based restoration quality compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuosen Bao', 'Xia Du', 'Zheng Lin', 'Jizhe Zhou', 'Zihan Fang', 'Jiening Wu', 'Yuxin Zhang', 'Zhe Chen', 'Chi-man Pun', 'Wei Ni', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'adversarial-examples', 'face-recognition', 'defense', 'diffusion-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04994</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious user comments that induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically reweight proportions of adversarial comment types during training to focus on model vulnerabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with substantial F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-synthesis', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation</title><link>https://arxiv.org/abs/2602.04856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLM Chain-of-Thought (CoT) can internally produce and propagate fake-news narratives even when the model's final response refuses the harmful request.&lt;/li&gt;&lt;li&gt;Introduces a unified safety-analysis framework that decomposes CoT across layers and attention heads using Jacobian-based spectral metrics.&lt;/li&gt;&lt;li&gt;Defines three interpretable measures (stability, geometry, energy) to quantify how specific attention heads embed deceptive reasoning and finds critical routing concentrated in a few mid-depth layers.&lt;/li&gt;&lt;li&gt;Identifies attention heads responsible for unsafe internal reasoning, providing targets for mitigation and improved safety defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yiping Zhang', 'Qiang Liu', 'Xingcheng Xu', 'Shu Wu', 'Haichao Shi', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'model-interpretability', 'safety-vulnerabilities', 'attention-head-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04856</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.08859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PE-CoA (Pattern Enhanced Chain of Attack), a framework of five conversational patterns to systematically craft multi-turn jailbreaks that bypass LLM safety constraints via natural dialogue.&lt;/li&gt;&lt;li&gt;Evaluates PE-CoA across 12 LLMs and 10 harm categories, achieving state-of-the-art multi-turn jailbreak success and revealing pattern-specific vulnerabilities and shared failure modes within model families.&lt;/li&gt;&lt;li&gt;Finds that defenses against one pattern often do not generalize to others, highlighting limitations of current safety training and motivating the need for pattern-aware defenses and red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ragib Amin Nihal', 'Rui Wen', 'Kazuhiro Nakadai', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'red teaming', 'LLM safety', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08859</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LH-Deception: Simulating and Understanding LLM Deceptive Behaviors in Long-Horizon Interactions</title><link>https://arxiv.org/abs/2510.03999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LH-Deception, a multi-agent simulation framework to systematically evaluate deceptive behaviors of LLMs over long-horizon, interdependent task sequences.&lt;/li&gt;&lt;li&gt;Framework involves a performer agent (task executor), a supervisor agent (evaluates progress and updates trust), and an independent auditor that labels deception across full interaction trajectories.&lt;/li&gt;&lt;li&gt;Empirical study across 11 frontier models showing deception is model-dependent, increases under event pressure, erodes supervisor trust, and yields emergent phenomena (e.g., chains of deception) invisible to single-turn tests.&lt;/li&gt;&lt;li&gt;Provides a foundation and benchmark-style methodology for assessing LLM trustworthiness in extended, real-world interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Xu', 'Xuanming Zhang', 'Samuel Yeh', 'Jwala Dhamala', 'Ousmane Dia', 'Rahul Gupta', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['deception', 'red-teaming', 'LLM vulnerabilities', 'evaluation framework', 'long-horizon interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03999</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cheap, scalable method for real-time, token-level detection of entity hallucinations (names, dates, citations) in long-form LLM outputs, suitable for streaming.&lt;/li&gt;&lt;li&gt;Creates a web-search-based annotation methodology and dataset labeling tokens as grounded vs. fabricated, enabling training of simple classifiers (e.g., linear probes) that scale to 70B models.&lt;/li&gt;&lt;li&gt;Classifiers outperform baselines (e.g., semantic entropy) across four model families and improve detection even in short-form QA; models trained on one model's annotations transfer to others.&lt;/li&gt;&lt;li&gt;Although focused on entity hallucinations, the probes also detect incorrect answers in mathematical reasoning, indicating broader generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Obeso', 'Andy Arditi', 'Javier Ferrando', 'Joshua Freeman', 'Cameron Holmes', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'token-level classification', 'safety/robustness', 'dataset', 'real-time/streaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03531</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and red-teams an open-source defense pipeline for LLM safeguard layers and proposes a novel few-shot-prompted input/output classifier that outperforms the open-weight ShieldGemma, reducing ASR to 0% on the ClearHarm dataset.&lt;/li&gt;&lt;li&gt;Introduces STaged AttaCK (STACK), a multi-stage black-box attack procedure achieving 71% attack success rate (ASR) against the few-shot-prompted classifier pipeline.&lt;/li&gt;&lt;li&gt;Demonstrates transferability of STACK with a 33% ASR in a transfer (no access) setting and suggests specific mitigations for developers to defend against staged attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ian R. McKenzie', 'Oskar J. Hollinsworth', 'Tom Tseng', 'Xander Davies', 'Stephen Casper', 'Aaron D. Tucker', 'Robert Kirk', 'Adam Gleave']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'jailbreaks', 'red-teaming', 'safeguard-pipelines', 'transfer-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.24068</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents KernelGYM, a distributed GPU environment for RL-based Triton kernel generation with built-in checks for reward hacking and long-term multi-turn training.&lt;/li&gt;&lt;li&gt;Identifies a biased policy gradient issue from self-inclusion in multi-turn GRPO and proposes Turn-level Reinforce-Leave-One-Out (TRLOO) for unbiased advantage estimation.&lt;/li&gt;&lt;li&gt;Introduces mismatch correction, Profiling-based Rewards (PR), and Profiling-based Rejection Sampling (PRS) to mitigate reward hacking and lazy optimization; releases model Dr.Kernel-14B and resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Jiawei Xu', 'Yingru Li', 'Longtao Zheng', 'Tianjian Li', 'Qian Liu', 'Junxian He']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'reward-hacking', 'robustness', 'code-generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05885</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions</title><link>https://arxiv.org/abs/2602.05234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Concept DAS (CDAS), an intervention-based model steering method that uses distributed interchange interventions (DII) and a novel distribution-matching objective to align intervened outputs with counterfactual distributions.&lt;/li&gt;&lt;li&gt;Differs from prior steering by learning interventions via weakly-supervised distribution matching rather than probability maximization, enabling bi-directional steering and data-derived steering factors with less hyperparameter tuning.&lt;/li&gt;&lt;li&gt;Evaluated on AxBench and two safety-related case studies: overriding refusal behaviors of safety-aligned models (a potential jailbreak capability) and neutralizing a chain-of-thought backdoor (a defensive mitigation), showing systematic steering while preserving general utility.&lt;/li&gt;&lt;li&gt;Finds CDAS is complementary to preference-optimization methods, may scale better with model size, and can conditionally offer robust intervention-based steering for security-related tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuntai Bao', 'Xuhong Zhang', 'Jintao Chen', 'Ge Su', 'Yuxiang Cai', 'Hao Peng', 'Bing Sun', 'Haiqin Weng', 'Liu Yan', 'Jianwei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'intervention-based methods', 'jailbreaking/behavior override', 'backdoor mitigation', 'causal interventions (DAS/DII)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05234</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors</title><link>https://arxiv.org/abs/2602.05769</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Revisits claims that GPT detectors falsely flag non-native speakers (Czech) due to lower perplexity, using contemporary models two years later.&lt;/li&gt;&lt;li&gt;Finds that non-native Czech speakers do not exhibit lower perplexity than native speakers and that perplexity is not a necessary feature for modern detectors.&lt;/li&gt;&lt;li&gt;Evaluates detectors from three distinct families and reports no systematic bias against non-native speakers; contemporary detectors operate effectively without relying on perplexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adnan Al Ali', 'Jind\\v{r}ich Helcl', "Jind\\v{r}ich Libovick\\'y"]&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'Bias analysis', 'Detector robustness', 'Forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05769</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title><link>https://arxiv.org/abs/2602.05444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models the LLM safety mechanism as an unobserved confounder and frames jailbreaks causally using Pearl's Front-Door Criterion.&lt;/li&gt;&lt;li&gt;Proposes CFA^2, a jailbreak framework that uses Sparse Autoencoders to remove defense-related features and isolates core task intent.&lt;/li&gt;&lt;li&gt;Replaces expensive marginalization with a deterministic intervention to reduce inference cost.&lt;/li&gt;&lt;li&gt;Empirical results claim state-of-the-art attack success rates and provide a mechanistic interpretation of the jailbreaking process.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Zhou', 'Zeen Song', 'Wenwen Qiang', 'Fengge Wu', 'Shuyi Zhou', 'Changwen Zheng', 'Hui Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'causal-inference', 'adversarial-attack', 'LLM-safety', 'front-door-adjustment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05444</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning</title><link>https://arxiv.org/abs/2602.05370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a counter-intuitive failure mode in iterative preference optimization for mathematical reasoning: aggressive Best-of-N exploration amplifies verifier noise and can cause catastrophic policy collapse.&lt;/li&gt;&lt;li&gt;Theoretically analyzes how scaling sampling budget (N) increases verifier noise and induces harmful distribution shifts.&lt;/li&gt;&lt;li&gt;Introduces PACE (Proximal Alignment via Corrective Exploration), a generation-based corrective strategy that synthesizes high-fidelity preference pairs from failed explorations using a very small budget (N&lt;3).&lt;/li&gt;&lt;li&gt;Empirical results show PACE outperforms standard DPO-R1 with N=16 while using ~1/5 of the compute and demonstrates greater robustness to reward hacking and label noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Rao', 'Zixiong Yu', 'Xuebo Liu', 'Guhan Chen', 'Jing Li', 'Jiansheng Wei', 'Xiaojun Meng', 'Min Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-hacking', 'robustness', 'preference-optimization', 'label-noise']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05370</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks</title><link>https://arxiv.org/abs/2602.05252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents 'Copyright Detective', an interactive forensic system to detect, analyze, and visualize potential copyright leakage in LLM outputs.&lt;/li&gt;&lt;li&gt;Integrates multiple detection paradigms—content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification—within a unified, extensible framework.&lt;/li&gt;&lt;li&gt;Supports interactive prompting, iterative workflows, response collection, and black-box auditing to evidence verbatim memorization and paraphrase-level leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangwei Zhang', 'Jianing Zhu', 'Cheng Qian', 'Neil Gong', 'Rada Mihalcea', 'Zhaozhuo Xu', 'Jingrui He', 'Jiaqi Ma', 'Yun Huang', 'Chaowei Xiao', 'Bo Li', 'Ahmed Abbasi', 'Dongwon Lee', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['copyright-leakage', 'model-audit', 'jailbreak-probing', 'forensic-analysis', 'unlearning-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05252</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems</title><link>https://arxiv.org/abs/2602.05176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically measures the impact of malicious/compromised language models when plugged into multi-LLM collaboration systems (routing, multi-agent debate, model merging, etc.) across 10 datasets.&lt;/li&gt;&lt;li&gt;Implements four categories of malicious LMs and evaluates them in four types of collaboration architectures, finding substantial degradation—especially in reasoning and safety tasks (avg. drops ~7.12% and 7.94%).&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies using external supervisors to detect/disable or mask malicious collaborators, recovering on average 95.31% of lost performance.&lt;/li&gt;&lt;li&gt;Notes that while mitigation is effective, making collaboration systems fully resistant to malicious models remains an open problem.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyuan Yang', 'Wenxuan Ding', 'Shangbin Feng', 'Yulia Tsvetkov']&lt;/li&gt;&lt;li&gt;Tags: ['malicious models', 'model collaboration', 'adversarial robustness', 'defenses/mitigation', 'multi-agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05176</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title><link>https://arxiv.org/abs/2602.04653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel inference-time backdoor attack that leverages maliciously modified chat templates (Jinja2) to trigger hidden behaviors without altering model weights, training data, or runtime infrastructure.&lt;/li&gt;&lt;li&gt;Empirically evaluates template backdoors across 18 models (7 families) and 4 inference engines, demonstrating large drops in factual accuracy (90% → 15% on average) and &gt;80% success in emitting attacker-controlled URLs under triggers while leaving benign inputs unaffected.&lt;/li&gt;&lt;li&gt;Shows backdoors generalize across runtimes and evade automated security scans on a major open-weight distribution platform, identifying chat templates as an unprotected supply-chain attack surface.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariel Fogel', 'Omer Hofman', 'Eilon Cohen', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'inference-time attack', 'supply-chain', 'model security', 'template injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04653</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.15678</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates RAG knowledge-base stealing as an adaptive stochastic coverage problem (ASCP) and introduces conditional expected marginal gain (CMG) as the objective.&lt;/li&gt;&lt;li&gt;Proposes RAGCrawler, a knowledge-graph-guided black-box attacker that estimates coverage, schedules semantic anchors, and generates stealthy non-redundant natural-language queries.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows strong extraction performance (66.8% avg coverage within 1,000 queries, up to 84.4%), large improvement over baselines (≈44.9%), faster attainment of high coverage, and robustness to retriever switching and modern RAG techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Yao', 'Ziqi Zhang', 'Ning Luo', 'Shaofei Li', 'Yifeng Cai', 'Xiangqun Chen', 'Yao Guo', 'Ding Li']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-base stealing', 'RAG attacks', 'data exfiltration', 'adversarial query generation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15678</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C^3LLM, a statistical certification framework that bounds the probability an LLM produces catastrophic responses in multi-turn conversations with formal confidence guarantees.&lt;/li&gt;&lt;li&gt;Models conversation distributions as a Markov process on a query graph (edges encode semantic similarity) and defines practical sampling distributions: random node, graph path, and adaptive-with-rejection.&lt;/li&gt;&lt;li&gt;Computes certified lower bounds on catastrophic risk (using confidence intervals) and demonstrates substantial certified risks (up to ~70%) for some frontier models, motivating stronger safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['catastrophic-risk certification', 'safety evaluation', 'red teaming', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Controlling the Risk of Corrupted Contexts for Language Models via Early-Exiting</title><link>https://arxiv.org/abs/2510.02480</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense mechanism to limit harm from malicious or irrelevant context by ensuring model performance does not fall below a zero-shot baseline using distribution-free risk control (DFRC).&lt;/li&gt;&lt;li&gt;Implements dynamic early-exit prediction to ignore later attention heads that primarily attend to unsafe inputs, reducing the influence of corrupted context.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical evaluation across 9 tasks (in-context learning and open-ended QA), showing risk control for harmful contexts and efficiency/performance gains on helpful inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrea Wynn', 'Metod Jazbec', 'Charith Peris', 'Rinat Khaziev', 'Anqi Liu', 'Daniel Khashabi', 'Eric Nalisnick']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'input sanitization', 'early-exit', 'distribution-free risk control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02480</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cheap, scalable method for real-time, token-level detection of entity hallucinations (names, dates, citations) in long-form LLM outputs, suitable for streaming.&lt;/li&gt;&lt;li&gt;Creates a web-search-based annotation methodology and dataset labeling tokens as grounded vs. fabricated, enabling training of simple classifiers (e.g., linear probes) that scale to 70B models.&lt;/li&gt;&lt;li&gt;Classifiers outperform baselines (e.g., semantic entropy) across four model families and improve detection even in short-form QA; models trained on one model's annotations transfer to others.&lt;/li&gt;&lt;li&gt;Although focused on entity hallucinations, the probes also detect incorrect answers in mathematical reasoning, indicating broader generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Obeso', 'Andy Arditi', 'Javier Ferrando', 'Joshua Freeman', 'Cameron Holmes', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'token-level classification', 'safety/robustness', 'dataset', 'real-time/streaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03531</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks from Causal Principles</title><link>https://arxiv.org/abs/2602.02819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames memorization and membership inference as a causal inference problem, defining memorization as the causal effect of including a data point in training.&lt;/li&gt;&lt;li&gt;Identifies and formalizes biases in existing evaluation regimes: interference in one-run methods and confounding in zero-run (post-hoc) evaluations.&lt;/li&gt;&lt;li&gt;Derives causal analogues of standard MIA metrics and proposes practical estimators for multi-run, one-run, and zero-run settings with non-asymptotic consistency guarantees; validates approach on real-world data including under distribution shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mathieu Even', "Cl\\'ement Berenfeld", 'Linus Bleistein', 'Tudor Cebere', 'Julie Josse', "Aur\\'elien Bellet"]&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'causal-inference', 'evaluation-methodology', 'memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02819</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Alignment-Aware Model Adaptation via Feedback-Guided Optimization</title><link>https://arxiv.org/abs/2602.02258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an alignment-aware fine-tuning framework that integrates external alignment feedback via policy-gradient-based regularization to preserve or recover safety properties during adaptation.&lt;/li&gt;&lt;li&gt;Introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned examples.&lt;/li&gt;&lt;li&gt;Learns abstention/conservative responses for fully misaligned inputs, incorporating safe behavior directly into the fine-tuned model.&lt;/li&gt;&lt;li&gt;Evaluates on instruction-tuning benchmarks and demonstrates reductions in harmful and hallucinated outputs while maintaining task performance; shows robustness to adversarial fine-tuning and prompt-based attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gaurav Bhatt', 'Aditya Chinchure', 'Jiawei Zhou', 'Leonid Sigal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fine-tuning defenses', 'adversarial robustness', 'abstention/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02258</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks</title><link>https://arxiv.org/abs/2601.22579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a non-intrusive graph-based bot detection framework for e-commerce that models user sessions as a graph and uses an inductive graph neural network for classification.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection performance (AUC, F1) over a session-level MLP baseline on real-world traffic and supports real-time inference and incremental updates for deployment.&lt;/li&gt;&lt;li&gt;Evaluates robustness via adversarial perturbation and cold-start simulations, showing resilience to moderate graph modifications and generalization to unseen sessions/URLs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sichen Zhao', 'Zhiming Xue', 'Yalun Qi', 'Xianling Zeng', 'Zihan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['bot-detection', 'graph-neural-networks', 'security-defense', 'adversarial-robustness', 'e-commerce']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22579</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</title><link>https://arxiv.org/abs/2512.13821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cross-Trace Verification Protocol (CTVP) that verifies untrusted code-generating models by comparing predicted execution traces across semantically equivalent program transformations to detect backdoors/malicious behavior.&lt;/li&gt;&lt;li&gt;Defines the Adversarial Robustness Quotient (ARQ) to quantify verification cost and shows ARQ grows exponentially with orbit size.&lt;/li&gt;&lt;li&gt;Provides information-theoretic bounds proving non-gamifiability: adversaries cannot reliably evade detection via training due to space complexity constraints.&lt;/li&gt;&lt;li&gt;Reports initial evaluations with practical challenges, notably high false positive rates, and discusses deployment considerations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subramanyam Sahoo']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'model verification', 'adversarial robustness', 'AI control', 'code generation security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13821</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious user comments that induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically reweight proportions of adversarial comment types during training to focus on model vulnerabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with substantial F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-synthesis', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Robust Recourse with $L^p$-Bounded Model Change</title><link>https://arxiv.org/abs/2509.21293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses robustness of algorithmic recourse recommendations to small model updates that can invalidate suggested actions.&lt;/li&gt;&lt;li&gt;Provides a provably optimal algorithm for computing robust recourse under L^p-bounded model changes (1 &lt;= p &lt; ∞) for generalized linear models, extending prior optimal results limited to L^∞.&lt;/li&gt;&lt;li&gt;Empirical results show substantially lower cost and sparser recourses versus prior methods, better cost-validity trade-offs, and resilience under post-processing and for some non-linear models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Phone Kyaw', 'Kshitij Kayastha', 'Shahin Jabbari']&lt;/li&gt;&lt;li&gt;Tags: ['robust-recourse', 'model-robustness', 'adversarial-robustness', 'provable-guarantees', 'algorithmic-recourse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21293</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models</title><link>https://arxiv.org/abs/2505.19969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a linear-systems-based analytical framework to characterize node-level differential privacy leakage in decentralized gossip averaging with additive noise.&lt;/li&gt;&lt;li&gt;Shows that DP guarantees reduce to those of a Gaussian mechanism with squared sensitivity growing asymptotically as O(T) (T = number of rounds), analogous to centralized aggregation.&lt;/li&gt;&lt;li&gt;Derives implications for utility: excess risk of decentralized private learning for strongly convex losses is asymptotically similar to the centralized private case.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antti Koskela', 'Tejas Kulkarni']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'decentralized learning', 'gossip algorithms', 'privacy analysis', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19969</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows</title><link>https://arxiv.org/abs/2602.05817</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an interpretable pipeline that projects high-dimensional GNN embeddings of IoT network traffic onto a low-dimensional latent manifold for visualization and monitoring.&lt;/li&gt;&lt;li&gt;Applies feature attribution to decode embedding features shaping the manifold and demonstrates intrusion detection with an F1-score of 0.830, also surfacing concept drift.&lt;/li&gt;&lt;li&gt;Aims to support security analysts and network administrators by turning opaque GNN representations into human-understandable signals for anomaly/attack detection and network-state monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrique Feito-Casares', 'Francisco M. Melgarejo-Meseguer', 'Elena Casiraghi', 'Giorgio Valentini', "Jos\\'e-Luis Rojo-\\'Alvarez"]&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'interpretability', 'graph-neural-networks', 'IoT-security', 'anomaly-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05817</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title><link>https://arxiv.org/abs/2602.05532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Split Personality Training (SPT): a LoRA-based 'honest persona' adapter that stays inactive during normal operation and can be activated with a trigger to inspect the main model's latent states and review outputs.&lt;/li&gt;&lt;li&gt;Applies SPT to the Anthropic Auditing Game Model Organism (Llama-3.3-70B trained to conceal reward-hacking) and reports 96% detection accuracy versus near 0% reported baseline.&lt;/li&gt;&lt;li&gt;Shows that latent knowledge and concealed biases/misbehaviors can be revealed internally, presenting a novel auditing/defense technique with potential dual-use implications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Florian Dietz', 'William Wale', 'Oscar Gilg', 'Robert McCarthy', 'Felix Michalak', 'Gustavo Ewbank Rodrigues Danon', 'Miguelito de Guzman', 'Dietrich Klakow']&lt;/li&gt;&lt;li&gt;Tags: ['auditing', 'model misalignment', 'latent knowledge extraction', 'LoRA adapters', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05532</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</title><link>https://arxiv.org/abs/2602.05339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAIRed Erasing (PAIR), a framework for consistency-preserving concept erasure in text-to-image diffusion models by using unsafe-safe paired examples to realign semantics rather than just remove concepts.&lt;/li&gt;&lt;li&gt;Introduces two main components: Paired Semantic Realignment (guided objective mapping unsafe concepts to safe anchors) and Fisher-weighted Initialization for low-rank adaptation (DoRA) to bias generation toward safe alternatives while suppressing targeted unsafe concepts.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing improved concept erasure that preserves structural and semantic consistency and generation quality compared to state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwoo Kim', 'Sungmin Cha', 'Hyunsoo Kim', 'Jaewon Lee', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['concept-erasure', 'content-moderation', 'model-safety', 'diffusion-models', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05339</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel threat model where adversaries modify simulator dynamics to implant action-level backdoors into RL agents without altering or observing rewards.&lt;/li&gt;&lt;li&gt;Proposes the Daze attack that reliably and stealthily triggers targeted actions via predefined environment triggers; provides formal guarantees of attack success across general RL tasks.&lt;/li&gt;&lt;li&gt;Presents extensive empirical evaluation on discrete and continuous action domains, demonstrating stealthiness and reliability of the backdoor.&lt;/li&gt;&lt;li&gt;Demonstrates real-world transfer by showing the RL backdoor attack can activate on robotic hardware, highlighting supply-chain risks in simulators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ethan Rathbun', 'Wo Wei Lin', 'Alina Oprea', 'Christopher Amato']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'reinforcement learning', 'simulator attacks', 'supply-chain adversary', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05089</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy</title><link>https://arxiv.org/abs/2602.04994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SIDeR, a framework that decomposes face images into an identity feature vector and a semantic appearance component and uses latent-space recomposition in a diffusion model to generate visually anonymous adversarial faces.&lt;/li&gt;&lt;li&gt;Employs momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to produce diverse, natural-looking adversarial samples while preserving machine-level identity consistency; supports password-based restoration to recover original images for authorized access.&lt;/li&gt;&lt;li&gt;Evaluated on CelebA-HQ and FFHQ, reporting 99% attack success in black-box scenarios and substantial improvements in PSNR-based restoration quality compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuosen Bao', 'Xia Du', 'Zheng Lin', 'Jizhe Zhou', 'Zihan Fang', 'Jiening Wu', 'Yuxin Zhang', 'Zhe Chen', 'Chi-man Pun', 'Wei Ni', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'adversarial-examples', 'face-recognition', 'defense', 'diffusion-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04994</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AP-OOD: Attention Pooling for Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2602.06031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AP-OOD, an attention-pooling method to aggregate token-level embeddings from language models for out-of-distribution scoring instead of simple averaging.&lt;/li&gt;&lt;li&gt;Presents a semi-supervised formulation that interpolates between unsupervised and supervised OOD detection, allowing use of limited auxiliary outlier data.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art OOD detection performance on text benchmarks (e.g., FPR95 reduced from 27.84% to 4.67% on XSUM; improvements on WMT15 En–Fr).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Claus Hofmann', 'Christian Huber', 'Bernhard Lehner', 'Daniel Klotz', 'Sepp Hochreiter', 'Werner Zellinger']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'NLP', 'defense', 'semi-supervised']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06031</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Verification of the Implicit World Model in a Generative Model via Adversarial Sequences</title><link>https://arxiv.org/abs/2602.05903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial sequence generation to test whether generative sequence models learn a sound implicit world model (using chess as a tractable domain).&lt;/li&gt;&lt;li&gt;Designs and evaluates several adversarial attack methods that produce valid game histories to provoke invalid next-move predictions, enabling falsification and fine-grained failure analysis.&lt;/li&gt;&lt;li&gt;Empirically evaluates many chess models trained with different datasets and recipes, finding none are fully sound though some training choices improve robustness.&lt;/li&gt;&lt;li&gt;Investigates board-state probes and reports that extracted board representations generally do not causally drive next-token prediction in most models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Andr\\'as Balogh", "M\\'ark Jelasity"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-testing', 'red-teaming', 'model-robustness', 'verification', 'sequence-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05903</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents KernelGYM, a distributed GPU environment for RL-based Triton kernel generation with built-in checks for reward hacking and long-term multi-turn training.&lt;/li&gt;&lt;li&gt;Identifies a biased policy gradient issue from self-inclusion in multi-turn GRPO and proposes Turn-level Reinforce-Leave-One-Out (TRLOO) for unbiased advantage estimation.&lt;/li&gt;&lt;li&gt;Introduces mismatch correction, Profiling-based Rewards (PR), and Profiling-based Rejection Sampling (PRS) to mitigate reward hacking and lazy optimization; releases model Dr.Kernel-14B and resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Jiawei Xu', 'Yingru Li', 'Longtao Zheng', 'Tianjian Li', 'Qian Liu', 'Junxian He']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'reward-hacking', 'robustness', 'code-generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05885</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Inject: Automated Prompt Injection via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoInject, a reinforcement learning framework that automatically generates universal adversarial suffixes for prompt injection attacks.&lt;/li&gt;&lt;li&gt;Optimizes attack success while preserving utility on benign tasks; supports black-box query-based optimization and transfer to unseen models/tasks.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against large commercial models (GPT-5 Nano, Claude Sonnet 3.5, Gemini 2.5 Flash) on the AgentDojo benchmark using a 1.5B parameter generator.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Chen', 'Jie Zhang', 'Florian Tramer']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attack', 'reinforcement learning', 'black-box/transfer attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05746</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification</title><link>https://arxiv.org/abs/2602.05535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Evidential Uncertainty Quantification (EUQ) to detect LVLM misbehaviors by decomposing epistemic uncertainty into internal conflict and ignorance using evidence theory.&lt;/li&gt;&lt;li&gt;Interprets model output features as positive/negative evidence and aggregates them in a single forward pass to quantify conflict (e.g., hallucinations) and ignorance (e.g., OOD).&lt;/li&gt;&lt;li&gt;Evaluates EUQ across hallucinations, jailbreaks, adversarial vulnerabilities, and OOD failures on state-of-the-art LVLMs, outperforming strong baselines.&lt;/li&gt;&lt;li&gt;Analyzes layer-wise evidential uncertainty dynamics to interpret how internal representations evolve with respect to misbehavior detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tao Huang', 'Rui Wang', 'Xiaofei Liu', 'Yi Qin', 'Li Duan', 'Liping Jing']&lt;/li&gt;&lt;li&gt;Tags: ['misbehavior detection', 'uncertainty quantification', 'jailbreak detection', 'adversarial vulnerabilities', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05535</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Erase at the Core: Representation Unlearning for Machine Unlearning</title><link>https://arxiv.org/abs/2602.05375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Erase at the Core (EC), a framework to enforce forgetting across intermediate network layers rather than only at the classifier head.&lt;/li&gt;&lt;li&gt;EC attaches auxiliary supervision modules to intermediate layers and applies multi-layer contrastive unlearning on the forget set plus cross-entropy preservation on the retain set with layer-wise weighting.&lt;/li&gt;&lt;li&gt;Demonstrates reduced representational similarity to the original model across layers while preserving performance on the retain set; can be plugged into existing unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaewon Lee', 'Yongwoo Kim', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy defense', 'representation forgetting', 'contrastive unlearning', 'deep supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05375</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates</title><link>https://arxiv.org/abs/2602.05311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines robust neural Lyapunov-barrier certificates and gives Lipschitz-based sufficient conditions that guarantee certificate validity under bounded perturbations in system dynamics.&lt;/li&gt;&lt;li&gt;Proposes practical training objectives to enforce robustness: adversarial training over dynamics perturbations, a Lipschitz neighborhood bound, and global Lipschitz regularization.&lt;/li&gt;&lt;li&gt;Provides empirical validation on control benchmarks (Inverted Pendulum, 2D Docking) showing substantially improved certified robustness bounds and empirical success under strong perturbations compared to baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Haoze Wu', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'certified-robustness', 'safe-RL', 'adversarial-training', 'control-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05311</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions</title><link>https://arxiv.org/abs/2602.05234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Concept DAS (CDAS), an intervention-based model steering method that uses distributed interchange interventions (DII) and a novel distribution-matching objective to align intervened outputs with counterfactual distributions.&lt;/li&gt;&lt;li&gt;Differs from prior steering by learning interventions via weakly-supervised distribution matching rather than probability maximization, enabling bi-directional steering and data-derived steering factors with less hyperparameter tuning.&lt;/li&gt;&lt;li&gt;Evaluated on AxBench and two safety-related case studies: overriding refusal behaviors of safety-aligned models (a potential jailbreak capability) and neutralizing a chain-of-thought backdoor (a defensive mitigation), showing systematic steering while preserving general utility.&lt;/li&gt;&lt;li&gt;Finds CDAS is complementary to preference-optimization methods, may scale better with model size, and can conditionally offer robust intervention-based steering for security-related tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuntai Bao', 'Xuhong Zhang', 'Jintao Chen', 'Ge Su', 'Yuxiang Cai', 'Hao Peng', 'Bing Sun', 'Haiqin Weng', 'Liu Yan', 'Jianwei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'intervention-based methods', 'jailbreaking/behavior override', 'backdoor mitigation', 'causal interventions (DAS/DII)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05234</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Position: Capability Control Should be a Separate Goal From Alignment</title><link>https://arxiv.org/abs/2602.05164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues capability control should be a separate objective from alignment, focused on imposing hard operational limits on model behavior (including under adversarial elicitation).&lt;/li&gt;&lt;li&gt;Proposes a three-layer framework for capability control: (i) data-based control of training distributions, (ii) learning-based interventions at weight/representation level, and (iii) system-based post-deployment guardrails over inputs/outputs/actions.&lt;/li&gt;&lt;li&gt;Advocates a defense-in-depth approach that composes controls across the full model lifecycle because each layer has characteristic failure modes when used alone.&lt;/li&gt;&lt;li&gt;Identifies open challenges such as the dual-use nature of knowledge, compositional generalization, and verification/implementation of control mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Eleni Triantafillou', 'David Krueger', 'Adrian Weller']&lt;/li&gt;&lt;li&gt;Tags: ['capability-control', 'model-safety', 'guardrails', 'defense-in-depth', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05164</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</title><link>https://arxiv.org/abs/2602.05068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes E-Globe, a hybrid branch-and-bound verifier that tightens upper and lower bounds to reach an ε-global optimum or find counterexamples for ReLU networks.&lt;/li&gt;&lt;li&gt;Introduces an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph so feasible solutions are valid adversarial counterexamples enabling pruning.&lt;/li&gt;&lt;li&gt;Accelerations include warm-started NLP solves with minimal constraint updates and pattern-aligned strong branching to prioritize splits that most improve relaxations.&lt;/li&gt;&lt;li&gt;Empirical results on MNIST and CIFAR-10 yield substantially tighter upper bounds than PGD and large speedups over MIP-based verification via warm-starting, GPU batching, and pattern-aware branching.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenting Li', 'Saif R. Kazi', 'Russell Bent', 'Duo Zhou', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'formal verification', 'branch-and-bound', 'ReLU networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05068</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Laws of Learning Dynamics and the Core of Learners</title><link>https://arxiv.org/abs/2602.05026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes fundamental laws of learning dynamics (a conservation law and decrease of total entropy) as a theoretical framework.&lt;/li&gt;&lt;li&gt;Introduces an entropy-based lifelong ensemble learning method and an ‘immunization’ mechanism to defend models.&lt;/li&gt;&lt;li&gt;Evaluates defense effectiveness against transfer-based adversarial attacks on CIFAR-10, showing the proposed ensemble (logifold) outperforms a naive ensemble, especially under strong perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Inkee Jung', 'Siu Cheong Lau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'defense', 'transfer-based-attacks', 'ensemble-learning', 'lifelong-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05026</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Private PoEtry: Private In-Context Learning via Product of Experts</title><link>https://arxiv.org/abs/2602.05012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretically grounded DP (differential privacy) method for private in-context learning (ICL) by reframing ICL as a Product-of-Experts (PoE) model that is parallelizable.&lt;/li&gt;&lt;li&gt;Claims substantial empirical gains (≈30 percentage points on average) over prior DP-ICL methods across tasks in text classification, math, and vision-language while maintaining strong privacy guarantees.&lt;/li&gt;&lt;li&gt;Addresses practical shortcomings of prior heuristics (oversampling, synthetic data, thresholding) with an efficient, provable privacy-preserving algorithm.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rob Romijnders', 'Mohammad Mahdi Derakhshani', 'Jonathan Petit', 'Max Welling', 'Christos Louizos', 'Yuki M. Asano']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'in-context-learning', 'privacy-preserving-ml', 'product-of-experts', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05012</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment</title><link>https://arxiv.org/abs/2602.04909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies distributional mismatch when using a fixed reference policy in preference-based alignment (e.g., DPO), which can amplify spurious signals under noisy supervision.&lt;/li&gt;&lt;li&gt;Introduces Geometric Anchor Preference Optimization (GAPO): replace fixed reference with a dynamic adversarial local perturbation (geometry-aware anchor) that acts as a pessimistic baseline.&lt;/li&gt;&lt;li&gt;Defines the Anchor Gap (reward discrepancy between policy and anchor) to approximate worst-case local margin degradation and uses it to reweight preference pairs, downweighting geometrically brittle instances.&lt;/li&gt;&lt;li&gt;Empirically shows improved robustness across noise settings while matching or improving performance on standard LLM alignment and reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youngjae Cho', 'Jongsuk Kim', 'Ji-Hoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'preference-alignment', 'adversarial-robustness', 'reward-modeling', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04909</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Causal Perspective for Enhancing Jailbreak Attack and Defense</title><link>https://arxiv.org/abs/2602.04893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Causal Analyst, a framework that uses LLM-based prompt encoding and GNN causal graph learning to identify direct causal prompt features driving jailbreaks.&lt;/li&gt;&lt;li&gt;Constructs a 35k-attempt dataset across seven LLMs with 100 attack templates, 50 harmful queries, and 37 human-readable prompt features for causal analysis.&lt;/li&gt;&lt;li&gt;Demonstrates two applications: a Jailbreaking Enhancer that increases attack success by targeting causal features, and a Guardrail Advisor that extracts malicious intent from obfuscated queries for defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Licheng Pan', 'Yunsheng Lu', 'Jiexi Liu', 'Jialing Tao', 'Haozhe Feng', 'Hui Xue', 'Zhixuan Chu', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'causal-analysis', 'adversarial-attack', 'defense/guardrails', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04893</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Self-Improving Pretraining: using post-trained models to pretrain better models</title><link>https://arxiv.org/abs/2601.21343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a pretraining method that streams documents and uses reinforcement learning to improve the next K generated tokens at each step.&lt;/li&gt;&lt;li&gt;A strong, post-trained model scores candidate continuations (rollouts, original suffix, rewritten suffix) for quality, safety, and factuality to provide rewards.&lt;/li&gt;&lt;li&gt;Training transitions from relying on rewritten/original suffixes early to rewarding high-quality rollouts as the model improves, yielding substantial gains in factuality, safety, and overall generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ellen Xiaoqing Tan', 'Shehzaad Dhuliawala', 'Jing Xu', 'Ping Yu', 'Sainbayar Sukhbaatar', 'Jason Weston', 'Olga Golovneva']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining defenses', 'model alignment', 'reinforcement learning', 'safety/factuality', 'training-time robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21343</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title><link>https://arxiv.org/abs/2512.06655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Graph-Regularized Sparse Autoencoders (GSAE) to learn smooth, distributed latent safety representations in LLMs instead of single-dimension safety features.&lt;/li&gt;&lt;li&gt;Proposes a runtime safety-steering mechanism that composes these features into weighted safety directions and uses a two-stage gating system to trigger interventions only on detected harmful prompts/continuations.&lt;/li&gt;&lt;li&gt;Reports strong empirical defenses: ~82% selective refusal (vs 42% for standard SAE) while retaining high task accuracy (TriviaQA 70%, TruthfulQA 65%, GSM8K 74%).&lt;/li&gt;&lt;li&gt;Demonstrates robustness across LLaMA-3, Mistral, Qwen, Phi families and resilience to jailbreak attacks (GCG, AutoDAN) with &gt;=90% refusal of harmful content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Federico Cinus', 'Yifan Wu', 'Luca Luceri']&lt;/li&gt;&lt;li&gt;Tags: ['safety-steering', 'jailbreak-defense', 'adversarial-robustness', 'latent-representations', 'runtime-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06655</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title><link>https://arxiv.org/abs/2511.21752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Label Disguise Defense (LDD): conceal true class labels by replacing them with semantically transformed or unrelated alias labels, learned implicitly via few-shot demonstrations to thwart class-directive prompt injection.&lt;/li&gt;&lt;li&gt;Evaluates LDD across nine state-of-the-art LLMs (e.g., GPT-5, GPT-4o, LLaMA3.2, Gemma3, Mistral variants) under adversarial prompt-injection settings and varying few-shot setups.&lt;/li&gt;&lt;li&gt;Finds LDD consistently restores part of the accuracy lost to attacks; performance depends on model and alias choice, with semantically aligned aliases (e.g., good vs. bad) typically yielding stronger robustness than arbitrary symbols (e.g., blue vs. yellow).&lt;/li&gt;&lt;li&gt;LDD is lightweight, model-agnostic, requires no retraining, and offers a practical defense layer against prompt-injection attacks for LLM-based text classification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Li', 'Ruocheng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial defense', 'LLM robustness', 'few-shot learning', 'attack mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21752</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title><link>https://arxiv.org/abs/2510.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a comprehensive hierarchical taxonomy of jailbreak strategies organized into seven mechanism-oriented families (impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, data poisoning).&lt;/li&gt;&lt;li&gt;Runs a structured red-teaming challenge and analyzes prevalence and success rates of different jailbreak types, focusing on multi-turn attacks where adversarial intent can emerge gradually.&lt;/li&gt;&lt;li&gt;Benchmarks GPT-5 as an automatic judge and demonstrates benefits of taxonomy-guided prompting for improving jailbreak detection.&lt;/li&gt;&lt;li&gt;Releases a new annotated Italian dataset of 1,364 multi-turn adversarial dialogues labeled with the taxonomy to support study of jailbreak attacks and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francesco Giarrusso', 'Olga E. Sorokoletova', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'adversarial-dataset', 'attack-detection', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13893</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious user comments that induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically reweight proportions of adversarial comment types during training to focus on model vulnerabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with substantial F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-synthesis', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title><link>https://arxiv.org/abs/2510.02712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses survival analysis (Cox, AFT, Random Survival Forest) to model 'time-to-inconsistency' across 36,951 turns from 9 LLMs on the MT-Consistency benchmark, framing multi-turn failures as time-to-event under adversarial conditions.&lt;/li&gt;&lt;li&gt;Finds that abrupt prompt-to-prompt semantic drift sharply increases hazard of inconsistency, while cumulative drift can be protective (suggesting conversational adaptation).&lt;/li&gt;&lt;li&gt;Shows AFT models with model-drift interactions provide the best discrimination/calibration and that Cox PH assumptions are violated for key covariates.&lt;/li&gt;&lt;li&gt;Demonstrates a lightweight AFT-based turn-level risk monitor that can flag likely failing conversations several turns before the first inconsistent answer (a practical safeguard).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['survival-analysis', 'robustness', 'adversarial-attacks', 'conversational-AI', 'risk-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02712</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</title><link>https://arxiv.org/abs/2508.03365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WhisperInject, a two-stage adversarial audio attack that jailbreaks audio-language models to produce harmful outputs.&lt;/li&gt;&lt;li&gt;Stage 1: white-box reward-based optimization (RL-PGD) to elicit a harmful native response; Stage 2: gradient-based payload injection embedding subtle perturbations into benign audio carriers.&lt;/li&gt;&lt;li&gt;Demonstrates average attack success rates of 60–78% across two benchmarks and five multimodal LLMs, validated by multiple evaluation frameworks.&lt;/li&gt;&lt;li&gt;Highlights a practical, covert audio-native threat vector against multimodal AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiskias Dingeto', 'Taeyoun Kwon', 'Dasol Choi', 'Bodam Kim', 'DongGeon Lee', 'Haon Park', 'JaeHoon Lee', 'Jongho Shin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial audio', 'jailbreaking', 'audio-language models', 'adversarial examples', 'attack framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03365</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title><link>https://arxiv.org/abs/2507.06969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses the hypothesis-testing interpretation of differential privacy (f-DP) to derive unified bounds on attack success that apply to re-identification, attribute inference, and data reconstruction.&lt;/li&gt;&lt;li&gt;Provides tunable, consistent bounds across attack settings that allow practitioners to evaluate risk relative to arbitrary baseline risk levels.&lt;/li&gt;&lt;li&gt;Empirically tighter than bounds from ε-DP, Rényi DP, and concentrated DP; calibrating noise with these bounds can reduce required noise (~20%) and substantially improve utility (e.g., text classification accuracy from 52% to 70%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bogdan Kulynych', 'Juan Felipe Gomez', 'Georgios Kaissis', 'Jamie Hayes', 'Borja Balle', 'Flavio P. Calmon', 'Jean Louis Raisaro']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-attacks', 'privacy-defense', 'f-DP', 'data-reconstruction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06969</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Video Detection via Perceptual Straightening</title><link>https://arxiv.org/abs/2507.00583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ReStraV, a video deepfake detector that leverages the 'perceptual straightening' hypothesis by measuring temporal curvature and stepwise distance in a pre-trained DINOv2 representation space.&lt;/li&gt;&lt;li&gt;Aggregates geometry-based statistics per video and trains a lightweight classifier to distinguish real vs. AI-generated videos.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art detection performance (e.g., 97.17% accuracy, 98.63% AUROC on VidProM) and claims computational efficiency and strong generalization compared to prior image/video detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Intern\\`o', 'Robert Geirhos', 'Markus Olhofer', 'Sunny Liu', 'Barbara Hammer', 'David Klindt']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'representation-geometry', 'video forensics', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00583</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks</title><link>https://arxiv.org/abs/2506.21142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a cGAN-based method to craft stealthy adversarial perturbations to known UAV cyber-attacks (DoS, FDI, MiTM, replay) that evade a multi-class IDS while preserving OOD-like statistics.&lt;/li&gt;&lt;li&gt;Proposes a CVAE-based detector using negative log-likelihood (regret scores) to distinguish these generative adversarial evasion samples from genuine OOD events.&lt;/li&gt;&lt;li&gt;Demonstrates that the CVAE-based approach outperforms Mahalanobis distance-based detectors in detecting stealthy, generative-model-based adversarial threats against UAV telemetry IDS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'evasion attacks', 'OOD detection', 'UAV cybersecurity', 'generative models (GAN/CVAE)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21142</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize across domains.&lt;/li&gt;&lt;li&gt;Observes that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical results showing SpikeScore improves cross-domain separability and outperforms baselines across multiple LLMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'cross-domain generalization', 'robustness', 'uncertainty-based defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C^3LLM, a statistical certification framework that bounds the probability an LLM produces catastrophic responses in multi-turn conversations with formal confidence guarantees.&lt;/li&gt;&lt;li&gt;Models conversation distributions as a Markov process on a query graph (edges encode semantic similarity) and defines practical sampling distributions: random node, graph path, and adaptive-with-rejection.&lt;/li&gt;&lt;li&gt;Computes certified lower bounds on catastrophic risk (using confidence intervals) and demonstrates substantial certified risks (up to ~70%) for some frontier models, motivating stronger safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['catastrophic-risk certification', 'safety evaluation', 'red teaming', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</title><link>https://arxiv.org/abs/2602.04820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a CNN-based system for classifying nail diseases (6 classes) using a 3,835-image dataset and compares InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 (InceptionV3 best at 95.57%).&lt;/li&gt;&lt;li&gt;Applies adversarial training as a robustness technique to make the model more resistant to tricky/noisy inputs.&lt;/li&gt;&lt;li&gt;Uses explainability methods (Grad-CAM in title and SHAP in abstract) to highlight important features and improve interpretability for clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzia Hossain', 'Samanta Ghosh', 'Shahida Begum', 'B. M. Shahria Alam', 'Mohammad Tahmid Noor', 'Md Parvez Mia', 'Nishat Tasnim Niloy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'explainability', 'medical-imaging', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04820</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Rewards in Reinforcement Learning for Cyber Defence</title><link>https://arxiv.org/abs/2602.04809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares the effects of dense vs. sparse reward functions on deep RL agents for autonomous cyber defence across multiple cyber gym environments, network sizes, and RL algorithms.&lt;/li&gt;&lt;li&gt;Introduces a novel ground-truth evaluation approach to directly compare reward functions and assess learned policy behaviours and associated risks.&lt;/li&gt;&lt;li&gt;Finds that goal-aligned sparse rewards (when encountered frequently) improve training reliability and produce lower-risk, more cost-efficient defensive policies than highly engineered dense rewards.&lt;/li&gt;&lt;li&gt;Demonstrates that sparse rewards can better align agent behaviour with defender objectives without explicit numerical penalties for costly defensive actions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elizabeth Bates', 'Chris Hicks', 'Vasilios Mavroudis']&lt;/li&gt;&lt;li&gt;Tags: ['cyber-defense', 'reinforcement-learning', 'reward-design', 'safe-rl', 'security-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04809</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty</title><link>https://arxiv.org/abs/2602.04763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes A2MAML: an uncertainty-aware, modality-level collaboration framework for multi-agent multimodal systems that models each modality-specific feature as a stochastic estimate with predicted uncertainty.&lt;/li&gt;&lt;li&gt;Actively selects reliable agent-modality pairs and aggregates information using Bayesian inverse-variance weighting to suppress corrupted or noisy modalities and support asymmetric modality availability.&lt;/li&gt;&lt;li&gt;Evaluated on connected autonomous driving collaborative accident detection, showing substantial gains (up to 18.7% higher detection rate) over single-agent and baseline collaborative methods, demonstrating improved robustness to sensor corruption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Liu', 'Pratap Tokekar', 'Ming Lin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal fusion', 'sensor corruption', 'multi-agent collaboration', 'uncertainty estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04763</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach</title><link>https://arxiv.org/abs/2602.04753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;User-study comparing industry professionals' and students' perspectives on adversarial machine learning (AML) and security education.&lt;/li&gt;&lt;li&gt;Design and deployment of two CTF challenges (NLP and generative-AI focused) that demonstrate a data poisoning attack on a training dataset.&lt;/li&gt;&lt;li&gt;Findings show correlation between cybersecurity education and concern for AML threats, and that CTF-based exercises effectively engage students; recommendations for integrating security into ML curricula.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishruti Kakkad (Carnegie Mellon University)', 'Paul Chung (University of California', 'San Diego)', 'Hanan Hibshi (Carnegie Mellon University', 'King Abdulaziz University)', 'Maverick Woo (Carnegie Mellon University)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ml', 'data-poisoning', 'security-education', 'user-study', 'ctf']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04753</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title><link>https://arxiv.org/abs/2602.04739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Two-phase, longitudinal evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers, producing 82,256 human harm ratings across eight model releases.&lt;/li&gt;&lt;li&gt;Finds large, persistent cross-family differences in vulnerability (Pixtral most vulnerable; Claude highest refusal rates and thus appears safest) and clear alignment drift across generations (GPT and Claude ASR increased; Pixtral and Qwen showed modest decreases).&lt;/li&gt;&lt;li&gt;Reports shifting modality effects over time (text-only more effective in Phase 1; Phase 2 shows model-specific modality patterns, with GPT-5 and Claude 4.5 near-equivalent vulnerability) and argues for longitudinal, multimodal safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Casey Ford', 'Madison Van Doren', 'Emily Dix']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompts', 'red teaming', 'alignment drift', 'safety evaluation', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04739</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Data to Behavior: Predicting Unintended Model Behaviors Before Training</title><link>https://arxiv.org/abs/2602.04735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines Data2Behavior: predicting unintended model behaviors (biases/safety risks) prior to fine-tuning or training.&lt;/li&gt;&lt;li&gt;Proposes Manipulating Data Features (MDF): summarizes candidate datasets via mean representations and injects them into a base model's forward pass to surface latent risky signals without parameter updates.&lt;/li&gt;&lt;li&gt;MDF is computationally efficient (~20% of GPU cost of fine-tuning) and can anticipate unintended behaviors, demonstrated on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it.&lt;/li&gt;&lt;li&gt;Provides a practical pre-training vulnerability detection technique to inform dataset selection and mitigate downstream safety issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengru Wang', 'Zhenqian Xu', 'Junfeng Fang', 'Yunzhi Yao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['pre-training vulnerability detection', 'model safety', 'bias detection', 'efficient evaluation', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04735</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention</title><link>https://arxiv.org/abs/2602.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies RAG vulnerability to corpus knowledge poisoning where injected documents steer LLM outputs.&lt;/li&gt;&lt;li&gt;Argues standard causal attention enables harmful cross-document interactions that exacerbate poisoning attacks.&lt;/li&gt;&lt;li&gt;Proposes Sparse Document Attention RAG (SDAG): a block-sparse attention mask that forbids cross-attention between retrieved documents, requiring only an inference-time mask change and no fine-tuning.&lt;/li&gt;&lt;li&gt;Empirical evaluation on LLM-based QA shows SDAG substantially reduces attack success rates and yields statistically significant gains when combined with state-of-the-art RAG defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sagie Dekel', 'Moshe Tennenholtz', 'Oren Kurland']&lt;/li&gt;&lt;li&gt;Tags: ['corpus-poisoning', 'RAG', 'attention-mechanisms', 'adversarial-defenses', 'red-teaming/attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04711</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Human-Centered Privacy Approach (HCP) to AI</title><link>https://arxiv.org/abs/2602.04616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Maps privacy risks across the AI development lifecycle (data collection, training, deployment, reuse) and discusses impacts on system behavior and users.&lt;/li&gt;&lt;li&gt;Surveys privacy-preserving techniques (e.g., federated learning, differential privacy) and integrates user-centered perspectives including mental models and design guidelines.&lt;/li&gt;&lt;li&gt;Covers regulatory/ethical landscapes, governance, practical case studies, and proposes a human-centered privacy (HCP) framework with open challenges and research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luyi Sun', 'Wei Xu', 'Zaifeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'differential_privacy', 'federated_learning', 'privacy_governance', 'human-centered_privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04616</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust The Typical</title><link>https://arxiv.org/abs/2602.04581</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trust The Typical (T3), a defense framework that frames LLM safety as an out-of-distribution (OOD) detection problem by learning the distribution of acceptable prompts in semantic space.&lt;/li&gt;&lt;li&gt;Requires no training on harmful examples yet achieves state-of-the-art performance across 18 benchmarks (toxicity, hate speech, jailbreaking, multilingual harms, over-refusal), substantially reducing false positives.&lt;/li&gt;&lt;li&gt;Demonstrates strong cross-domain and cross-lingual transfer from a single model trained on safe English text.&lt;/li&gt;&lt;li&gt;Shows production readiness by integrating a GPU-optimized version into vLLM for continuous token-level guardrailing with low runtime overhead (&lt;6%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debargha Ganguly', 'Sreehari Sankar', 'Biyao Zhang', 'Vikash Singh', 'Kanan Gupta', 'Harshini Kavuru', 'Alan Luo', 'Weicong Chen', 'Warren Morningstar', 'Raghu Machiraju', 'Vipin Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'out-of-distribution detection', 'jailbreak detection', 'guardrails', 'production integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04581</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2602.04448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that standard full-parameter safety fine-tuning on Mixture-of-Experts (MoE) models can mask vulnerabilities via routing changes rather than repairing unsafe experts.&lt;/li&gt;&lt;li&gt;Proposes RASA, a routing-aware framework that finds experts disproportionately activated by jailbreaks, selectively fine-tunes those experts under fixed routing, and enforces routing consistency with safety contexts.&lt;/li&gt;&lt;li&gt;Demonstrates strong robustness to diverse jailbreaks, cross-attack generalization, reduced over-refusal, and preserved general capabilities on benchmarks (MMLU, GSM8K, TruthfulQA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Yuhui Wang', 'Tanqiu Jiang', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Mixture-of-Experts', 'Jailbreaking', 'Safety Alignment', 'Routing-aware Defense', 'Expert-level Fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04448</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks</title><link>https://arxiv.org/abs/2602.04294</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive evaluation of how few-shot demonstrations affect prompt-based defenses against LLM jailbreak attacks across multiple mainstream LLMs and four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak methods.&lt;/li&gt;&lt;li&gt;Finds that few-shot examples improve Role-Oriented Prompts (RoP) by up to 4.5% (reinforcing role identity) but degrade Task-Oriented Prompts (ToP) by up to 21.2% (distracting from task instructions).&lt;/li&gt;&lt;li&gt;Provides practical recommendations for deploying prompt-based defenses in real-world LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanshu Wang', 'Shuaishuai Yang', 'Jingjing He', 'Tong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'prompt-based defenses', 'few-shot learning', 'LLM security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04294</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Contextual Drag: How Errors in the Context Affect LLM Reasoning</title><link>https://arxiv.org/abs/2602.04288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and characterizes "contextual drag": failed attempts in the prompt/context bias later LLM generations toward structurally similar errors.&lt;/li&gt;&lt;li&gt;Empirical evaluation across 11 models and 8 reasoning tasks shows contextual drag causes ~10–20% performance drops and can cause iterative self-refinement to collapse into self-deterioration.&lt;/li&gt;&lt;li&gt;Structural analysis (tree edit distance) demonstrates inheritance of error patterns; external feedback and self-verification do not eliminate the effect.&lt;/li&gt;&lt;li&gt;Evaluates mitigations (fallback-behavior fine-tuning, context denoising) that provide partial improvements but do not fully recover baseline performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yun Cheng', 'Xingyu Zhu', 'Haoyu Zhao', 'Sanjeev Arora']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'contextual bias', 'self-refinement failure', 'mitigation strategies', 'reasoning vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04288</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title><link>https://arxiv.org/abs/2602.04224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies failures in current safe reasoning/coT defenses to generalize against diverse and complex jailbreak attacks and argues for a more sufficient safe reasoning process.&lt;/li&gt;&lt;li&gt;Proposes Risk-Aware Preference Optimization (RAPO), a framework that lets large reasoning models adaptively detect and mitigate safety risks at appropriate granularity within their chain-of-thought.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence and shows RAPO improves robustness of multiple LRMs against varied attack prompts while preserving task utility.&lt;/li&gt;&lt;li&gt;Releases code for reproducibility and evaluation across attack scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Qiaosheng Zhang', 'Xia Hu', 'Xingcheng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety alignment', 'adversarial robustness', 'defense mechanism', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04224</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Chains of Thought Don't Matter: Causal Bypass in Large Language Models</title><link>https://arxiv.org/abs/2602.03994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows a negative result: Chain-of-thought (CoT) text can appear faithful yet be causally bypassed—answers often do not depend on the CoT content.&lt;/li&gt;&lt;li&gt;Introduces a diagnostic framework combining (i) a behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe using hidden-state patching to measure CoT-mediated influence (CMI) and a bypass score (1−CMI).&lt;/li&gt;&lt;li&gt;Finds audit-aware prompting raises detectable manipulation signals but causal probes reveal task-dependent mediation: many QA items have near-total bypass (CMI ≈ 0) while some logic problems show higher mediation (CMI up to 0.56).&lt;/li&gt;&lt;li&gt;Layer-wise analysis uncovers narrow, task-dependent 'reasoning windows' even when mean CMI is low, highlighting robustness and interpretability vulnerabilities in LLM reasoning disclosures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anish Sathyanarayanan', 'Aditya Nagarsekar', 'Aarush Rathore']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'model auditing', 'causal probing', 'robustness', 'hidden-state patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03994</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Byzantine Machine Learning: MultiKrum and an optimal notion of robustness</title><link>https://arxiv.org/abs/2602.03899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves for the first time that MultiKrum (an extension of Krum) is a robust aggregation rule under the Byzantine threat model and derives bounds on its robustness coefficient.&lt;/li&gt;&lt;li&gt;Introduces κ* (kappa-star), an optimal robustness coefficient that tightly quantifies mean-estimation accuracy under adversarial workers.&lt;/li&gt;&lt;li&gt;Constructs upper and lower bounds on MultiKrum's robustness coefficient, improves prior bounds for Krum, and shows MultiKrum's bounds dominate Krum's in realistic regimes.&lt;/li&gt;&lt;li&gt;Provides experimental validation assessing the quality of the derived lower bound.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gilles Bareilles', 'Wassim Bouaziz', 'Julien Fageot', 'El-Mahdi El-Mhamdi']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine-robust aggregation', 'robustness', 'defenses', 'distributed learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03899</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data</title><link>https://arxiv.org/abs/2602.03872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a theoretical framework to analyze DP-SGD on long-tailed data from a feature-learning perspective and shows that DP-SGD yields higher test error on rare subpopulations compared to overall error.&lt;/li&gt;&lt;li&gt;Characterizes the training dynamics, demonstrating how gradient clipping and noise injection jointly impede the model's ability to memorize informative but underrepresented samples.&lt;/li&gt;&lt;li&gt;Provides extensive empirical validation on synthetic and real-world datasets that corroborate the theoretical predictions about privacy-utility tradeoffs and memorization behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Huanyi Xie', 'Meng Ding', 'Shaopeng Fu', 'Jinyan Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'DP-SGD', 'memorization', 'privacy-preserving training', 'long-tailed data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03872</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making</title><link>https://arxiv.org/abs/2602.04003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial explanation attacks (AEAs): manipulating LLM-generated explanations to increase human trust in incorrect model outputs.&lt;/li&gt;&lt;li&gt;Formalizes the threat via a 'trust miscalibration gap' metric and explores four framing dimensions (reasoning mode, evidence type, communication style, presentation).&lt;/li&gt;&lt;li&gt;Presents a controlled experiment (n=205) showing adversarial explanations largely preserve benign trust, with highest vulnerability when explanations mimic expert communication and on difficult, fact-driven tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shutong Fan', 'Lan Zhang', 'Xiaoyong Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-explanations', 'human-in-the-loop', 'social-engineering', 'trust-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04003</guid><pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 29 Jan 2026 23:15:53 +0000</lastBuildDate><item><title>Feature-Space Adversarial Robustness Certification for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.16200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Feature-space Smoothing (FS), a framework that provides certified robustness guarantees at the feature representation level of multimodal LLMs under l2-bounded input perturbations.&lt;/li&gt;&lt;li&gt;Proves a certified lower bound on cosine similarity between clean and adversarial features (Feature Cosine Similarity Bound, FCSB) and links it to the encoder's Gaussian robustness score.&lt;/li&gt;&lt;li&gt;Introduces Gaussian Smoothness Booster (GSB), a plug-and-play module to increase the Gaussian robustness score of pretrained MLLMs without retraining, improving the certified guarantees from FS.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing FS (with GSB) yields strong certified feature-space robustness and robust downstream task performance across diverse multimodal applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song Xia', 'Meiwen Ding', 'Chenqi Kong', 'Wenhan Yang', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'certified defenses', 'multimodal LLMs', 'feature-space attacks/defenses', 'randomized smoothing / Gaussian smoothing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16200</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow</title><link>https://arxiv.org/abs/2509.21789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new failure mode called multi-agent visual hallucination snowballing in Visual Language Model (VLM) based multi-agent systems, where hallucinations seeded in one agent are amplified by subsequent agents due to over-reliance on textual message flow.&lt;/li&gt;&lt;li&gt;Performs turn-, layer-, and token-wise attention analyses and finds a subset of vision tokens with unimodal mid-layer attention peaks that best preserve visual evidence but fade across agent turns, driving the snowballing effect.&lt;/li&gt;&lt;li&gt;Proposes ViF, a lightweight, plug-and-play mitigation that relays inter-agent messages using selected visual relay tokens (Visual Flow) and applies attention reallocation to amplify preserved visual signals.&lt;/li&gt;&lt;li&gt;Empirical evaluation across eight benchmarks, four MAS structures, and ten base models shows consistent reduction of hallucination snowballing and improved performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinlei Yu', 'Chengming Xu', 'Guibin Zhang', 'Yongbo He', 'Zhangquan Chen', 'Zhucun Xue', 'Jiangning Zhang', 'Yue Liao', 'Xiaobin Hu', 'Yu-Gang Jiang', 'Shuicheng Yan']&lt;/li&gt;&lt;li&gt;Tags: ['visual hallucination', 'multimodal robustness', 'multi-agent systems', 'visual-language models', 'attention-based defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21789</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title><link>https://arxiv.org/abs/2508.19112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep: a random forest classifier using deep features from a pretrained Swin Transformer encoder to detect out-of-distribution (OOD) CT scans and improve reliability of lung cancer segmentation.&lt;/li&gt;&lt;li&gt;Encoder pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans; segmentation trained on 317 3D scans and tested on 603 external 3D CTs including one in-distribution and four OOD datasets (PE, COVID-19, abdominal CTs).&lt;/li&gt;&lt;li&gt;RF-Deep outperforms established OOD methods, achieving low FPR95 on several OOD datasets and thus provides a simple effective OOD detection defense to flag unreliable segmentation outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'robustness', 'medical imaging', 'anomaly detection', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19112</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight supervised detector for hallucinations in visual question answering that leverages rich internal signals from vision-language models (token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features).&lt;/li&gt;&lt;li&gt;Fuses these signals via branch-wise evidence encoding and uncertainty-aware attention, and introduces a low-cost, model-dependent automatic supervision strategy by extending the LLM-as-a-Judge paradigm to VQA.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection accuracy and efficiency over prior external-verifier and uncertainty-driven methods on multiple VQA benchmarks, and analyzes how different internal signals and VLM architectures reveal complementary hallucination patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'VQA', 'model-internal-signals', 'safety-defenses', 'LLM-as-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title><link>https://arxiv.org/abs/2512.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a hybrid, production-scale content moderation system combining supervised classifiers for known violations with reference-based similarity matching for novel or subtle cases.&lt;/li&gt;&lt;li&gt;Uses multimodal inputs (text, audio, visual) and leverages a multimodal large language model (MLLM) to distill knowledge and boost accuracy while keeping inference lightweight.&lt;/li&gt;&lt;li&gt;Reports production metrics: classification pipeline 67% recall at 80% precision, similarity pipeline 76% recall at 80% precision, and a 6–8% reduction in user views of unwanted livestreams in A/B tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Chee Yew', 'Hailun Xu', 'Sanjay Saha', 'Xiaotian Fan', 'Hiok Hian Ong', 'David Yuchen Wang', 'Kanchan Sarkar', 'Zhenheng Yang', 'Danhui Guan']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'multimodal-MLLM', 'defense', 'similarity-matching', 'production-system']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03553</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction</title><link>https://arxiv.org/abs/2507.18988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEDR, a training-free attribution method for generative models with continuous autoencoders that uses two successive reconstructions and the ratio of their losses as the attribution signal.&lt;/li&gt;&lt;li&gt;Calibrates the ratio with an image homogeneity metric to remove biases from image complexity, improving accuracy; leverages autoencoder reconstructions for computational efficiency.&lt;/li&gt;&lt;li&gt;Evaluated on eight top latent diffusion models, reporting ~25.5% higher attribution accuracy than prior reconstruction-based methods while using only ~1% of the computation time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chao Wang', 'Zijin Yang', 'Yaofei Wang', 'Weiming Zhang', 'Kejiang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image-attribution', 'image-forensics', 'defense', 'generative-models', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18988</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</title><link>https://arxiv.org/abs/2412.01256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NLPrompt, a noise-robust prompt learning framework for vision-language models that combines mean absolute error (PromptMAE) for robustness and an optimal-transport based data purification (PromptOT).&lt;/li&gt;&lt;li&gt;Argues and provides theoretical intuition that MAE suppresses influence of noisy samples, improving signal-to-noise ratio in prompt learning.&lt;/li&gt;&lt;li&gt;PromptOT uses text features as prototypes to partition datasets into clean and noisy subsets, applying cross-entropy to clean data and MAE to noisy data.&lt;/li&gt;&lt;li&gt;Extensive empirical evaluation across noise settings showing improved performance under label noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bikang Pan', 'Qun Li', 'Xiaoying Tang', 'Wei Huang', 'Zhen Fang', 'Feng Liu', 'Jingya Wang', 'Jingyi Yu', 'Ye Shi']&lt;/li&gt;&lt;li&gt;Tags: ['label-noise', 'robustness', 'prompt-learning', 'vision-language-models', 'data-purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01256</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization in diffusion-based image generative models and shows norm-based detection metrics fail in anisotropic (low-noise) regimes.&lt;/li&gt;&lt;li&gt;Introduces a combined detection metric that integrates isotropic norm and anisotropic angular alignment between guidance and unconditional scores, computable on pure noise with two forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates the metric outperforms prior denoising-free detectors on Stable Diffusion v1.4 and v2, with ~5x speedup, and proposes a mitigation strategy that adapts memorized prompts using the metric.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'diffusion models', 'detection', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion</title><link>https://arxiv.org/abs/2601.20325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UnlearnShield, the first defense specifically designed to mitigate unlearning inversion attacks that reconstruct data intended to be removed by machine unlearning.&lt;/li&gt;&lt;li&gt;Introduces directional perturbations in cosine representation space and a constraint module to balance model accuracy and forgetting efficacy while reducing inversion risk.&lt;/li&gt;&lt;li&gt;Empirically demonstrates a trade-off among privacy protection, predictive accuracy, and forgetting effectiveness, showing reduced data reconstruction from unlearned models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lulu Xue', 'Shengshan Hu', 'Wei Lu', 'Ziqi Zhou', 'Yufei Song', 'Jianhong Cheng', 'Minghui Li', 'Yanjun Zhang', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'unlearning inversion', 'privacy defense', 'data reconstruction', 'adversarial mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20325</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks</title><link>https://arxiv.org/abs/2601.20310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemBind, a defense that binds latent-based watermarks to image semantics via a learned semantic masker trained with contrastive learning to produce prompt-invariant and cross-prompt-orthogonal codes.&lt;/li&gt;&lt;li&gt;The masker reshapes and permutes codes to modulate the target latent before applying standard latent-based watermarks, remaining compatible with existing watermarking schemes and preserving image quality.&lt;/li&gt;&lt;li&gt;Includes a tunable mask-ratio parameter to trade off anti-forgery strength and robustness; demonstrates substantial reductions in false acceptance under black-box forgery across four mainstream latent watermark methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Zhang', 'Zijin Yang', 'Kejiang Chen', 'Linfeng Ma', 'Weiming Zhang', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'anti-forgery', 'latent diffusion models', 'black-box attacks', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20310</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection</title><link>https://arxiv.org/abs/2601.20656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FD-MAD, a region-aware, frequency-domain method for single-image face morphing attack detection (S-MAD) that focuses on residual spectral features.&lt;/li&gt;&lt;li&gt;Introduces the residual frequency domain to decouple natural spectral decay and enhance separability between bona fide and morphed images.&lt;/li&gt;&lt;li&gt;Combines local region evidence using a Markov Random Field for globally consistent decisions, producing a lightweight alternative to deep S-MAD models.&lt;/li&gt;&lt;li&gt;Evaluated cross-dataset (trained on SMDD, tested on FRLL-Morph and MAD22) achieving low EERs (1.85% on FRLL-Morph, 6.12% on MAD22) demonstrating strong cross-morph robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diogo J. Paulo', 'Hugo Proen\\c{c}a', 'Jo\\~ao C. Neves']&lt;/li&gt;&lt;li&gt;Tags: ['face morphing', 'morphing attack detection', 'presentation attack detection', 'frequency-domain analysis', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20656</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection</title><link>https://arxiv.org/abs/2601.20461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense for AI-generated image detection by 'contaminating' real images with the final component of generator architectures and training a detector to distinguish contaminated vs. original images.&lt;/li&gt;&lt;li&gt;Introduces a taxonomy of generators based on their final architectural components and categorizes 21 popular generators to study cross-generator generalization.&lt;/li&gt;&lt;li&gt;Demonstrates strong generalization: with only 100 samples from each of three representative categories and a DINOv3 backbone, achieves 98.83% average accuracy across 22 unseen-generator test sets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanzhu Liu', 'Xiao Liu', 'Yuexuan Wang', 'Mondal Soumik']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'AI-generated image detection', 'defense/robustness', 'generalization across generators']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20461</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</title><link>https://arxiv.org/abs/2601.20433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MARE: a multimodal approach using vision-language models for explainable deepfake detection.&lt;/li&gt;&lt;li&gt;Applies reinforcement learning from human feedback (RLHF) with tailored reward functions to produce text–spatially aligned, human-preference-consistent reasoning.&lt;/li&gt;&lt;li&gt;Introduces a forgery disentanglement module to capture high-level facial semantic forgery traces and improve authenticity detection.&lt;/li&gt;&lt;li&gt;Reports quantitative and qualitative evaluations showing improved accuracy and reliability for detection and reasoning outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Xu', 'Wei Lu', 'Xiangyang Luo', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'vision-language models', 'multimodal defense', 'RLHF', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20433</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination Begins Where Saliency Drops</title><link>https://arxiv.org/abs/2601.20279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LVLMs-Saliency, a gradient-aware diagnostic that fuses attention weights with input gradients to quantify visual grounding of each output token.&lt;/li&gt;&lt;li&gt;Finds that hallucinations correlate with low saliency of preceding tokens for predicting the next token, indicating contextual memory breakdown.&lt;/li&gt;&lt;li&gt;Proposes two inference-time defenses: Saliency-Guided Rejection Sampling (SGRS) to filter low-saliency candidate tokens, and Local Coherence Reinforcement (LocoRE) to boost attention to recent context.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing reduced hallucination rates while maintaining fluency and task performance across multiple LVLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaofeng Zhang', 'Yuanchao Zhu', 'Chaochen Gu', 'Xiaosong Yuan', 'Qiyan Zhao', 'Jiawei Cao', 'Feilong Tang', 'Sinan Fan', 'Yaomin Shen', 'Chen Shen', 'Hao Tang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'model interpretability', 'inference-time defenses', 'saliency analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20279</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection</title><link>https://arxiv.org/abs/2510.05305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes parameter-efficient front-ends that fuse prompt-tuning with signal transforms (FourierPT-XLSR, WSPT-XLSR, Partial-WSPT-XLSR) for speech deepfake detection.&lt;/li&gt;&lt;li&gt;Introduces WaveSP-Net: a Partial-WSPT-XLSR front-end combined with a bidirectional Mamba back-end to inject multi-resolution wavelet features into prompt embeddings without fine-tuning XLSR.&lt;/li&gt;&lt;li&gt;Demonstrates improved localization of subtle synthetic artifacts and outperforms several state-of-the-art detectors on Deepfake-Eval-2024 and SpoofCeleb while using few trainable parameters.&lt;/li&gt;&lt;li&gt;Releases code and models for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xi Xuan', 'Xuechen Liu', 'Wenxin Zhang', 'Yi-Cheng Lin', 'Xiaojian Lin', 'Tomi Kinnunen']&lt;/li&gt;&lt;li&gt;Tags: ['speech deepfake detection', 'prompt tuning', 'wavelet transform', 'anti-spoofing', 'parameter-efficient fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05305</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation</title><link>https://arxiv.org/abs/2510.18439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a token-level reliability measure that quantifies how much the decoder relies on visual input by combining feature-based sensitivity (masking-based internal changes) and counterfactual signals (probability differences between clean and altered video).&lt;/li&gt;&lt;li&gt;Aggregates token-level signals to a sentence-level reliability score that predicts hallucination risk without references and generalizes across SLT datasets (PHOENIX-2014T, CSL-Daily) and model architectures.&lt;/li&gt;&lt;li&gt;Shows reliability distinguishes grounded tokens from guessed ones, decreases under visual degradation, and improves hallucination risk estimation when combined with text-based signals (confidence, perplexity, entropy).&lt;/li&gt;&lt;li&gt;Analyzes why gloss-free sign-language translation models are more susceptible to hallucinations and positions reliability as a practical diagnostic/defensive tool for multimodal generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yasser Hamidullah', 'Koel Dutta Chowdhury', 'Yusser Al Ghussin', 'Shakib Yazdani', 'Cennet Oguz', 'Josef van Genabith', 'Cristina Espa\\~na-Bonet']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'multimodal-robustness', 'vision-language', 'safety', 'diagnostics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18439</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A-IPO: Adaptive Intent-driven Preference Optimization</title><link>https://arxiv.org/abs/2510.10077</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes A-IPO, an adaptive intent-driven preference optimization method that infers latent user intent and incorporates an intention-response similarity term into the reward to improve alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing the added intent term increases the preference margin (log-odds shift) between preferred and dispreferred responses.&lt;/li&gt;&lt;li&gt;Introduces new evaluation benchmarks (Real-pref, Attack-pref) and an extended GlobalOpinionQA-Ext to assess real-world and adversarial preference alignment; reports substantial empirical gains including improved adversarial robustness.&lt;/li&gt;&lt;li&gt;Demonstrates defense-oriented metrics (e.g., Defense Success Rate on Attack-pref) and claims improved robustness against adversarial preference scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqing Wang (Huazhong Agricultural University', 'China)', 'Muhammad Asif Ali (King Abdullah University of Science and Technology', 'KSA)', 'Ali Shoker (King Abdullah University of Science and Technology', 'KSA)', 'Ruohan Yang (Huazhong Agricultural University', 'China)', 'Junyang Chen (Shenzhen University', 'China)', 'Ying Sha (Huazhong Agricultural University', 'China)', 'Huan Wang (Huazhong Agricultural University', 'China)']&lt;/li&gt;&lt;li&gt;Tags: ['preference_alignment', 'adversarial_robustness', 'intent_inference', 'defense', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10077</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mask-GCG, a plug-and-play extension to Greedy Coordinate Gradient (GCG) that uses learnable token masking to identify and prune low-impact tokens in adversarial suffixes.&lt;/li&gt;&lt;li&gt;Pruning reduces gradient space and computational overhead while maintaining attack success rate (ASR) and loss values, shortening time to successful jailbreaks.&lt;/li&gt;&lt;li&gt;Evaluates Mask-GCG across original GCG and several improved variants, showing most suffix tokens are impactful but a minority are redundant—offering insights into efficient and interpretable jailbreak construction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Mu', 'Zonghao Ying', 'Zhekui Fan', 'Zonglei Jing', 'Yaoyuan Zhang', 'Zhengmin Yu', 'Wenxin Zhang', 'Quanchen Zou', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'prompt optimization', 'attack efficiency', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06350</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval</title><link>https://arxiv.org/abs/2502.13369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PGMR (Post-Generation Memory Retrieval): the LLM first generates a SPARQL query with natural-language placeholders for URIs, then a non-parametric memory module retrieves and resolves the correct KG URIs.&lt;/li&gt;&lt;li&gt;Empirically reduces URI hallucinations and improves SPARQL query correctness across multiple LLMs, datasets, and distribution shifts.&lt;/li&gt;&lt;li&gt;Demonstrates safety and robustness features: a retrieval confidence threshold enables safe refusal for unsupported queries, and the retriever remains effective even with substantial irrelevant memory noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Sharma', 'Christopher J. Pal', 'Amal Zouaq']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'knowledge graph', 'robustness', 'safety/refusal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13369</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title><link>https://arxiv.org/abs/2601.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces peer prediction — a game-theoretic, mutual-predictability-based method — to evaluate and train LLMs for truthfulness using only weak supervision (no ground-truth labels).&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical validation (models up to 405B) showing peer prediction rewards honest/informative answers and resists deceptive behavior.&lt;/li&gt;&lt;li&gt;Demonstrates training utility: peer-prediction-based reward can recover truthfulness lost to malicious fine-tuning (shown on an 8B model) even when the reward model is much smaller and unfine-tuned.&lt;/li&gt;&lt;li&gt;Identifies an inverse-scaling property: peer prediction becomes more robust as the capability gap between judge and target widens, enabling reliable evaluation of stronger models with weaker supervisors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Alex Qiu', 'Micah Carroll', 'Cameron Allen']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'evaluation-robustness', 'incentive-design', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20299</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking</title><link>https://arxiv.org/abs/2601.20283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a minimal, query-aware single-word adversarial attack (insert/substitute a 'query center' word) to promote target documents in neural ranking models.&lt;/li&gt;&lt;li&gt;Proposes heuristic and gradient-guided variants, including a white-box method that finds influential insertion points.&lt;/li&gt;&lt;li&gt;Empirical results on TREC-DL 2019/2020 with BERT and monoT5 re-rankers: up to 91% attack success while modifying fewer than two tokens per document on average.&lt;/li&gt;&lt;li&gt;Provides diagnostic metrics revealing a 'Goldilocks' vulnerability zone (mid-ranked documents most susceptible) and discusses implications for defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanmay Karmakar', 'Sourav Saha', 'Debapriyo Majumdar', 'Surjyanee Halder']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'neural-ranking', 'white-box-attack', 'model-robustness', 'information-retrieval-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20283</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data</title><link>https://arxiv.org/abs/2601.19936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gap-K%, a method to detect whether text was included in LLM pretraining by measuring the log-probability gap between the model's top-1 predicted token and the actual target token.&lt;/li&gt;&lt;li&gt;Incorporates a sliding-window strategy to capture local token correlations and reduce token-level noise.&lt;/li&gt;&lt;li&gt;Grounded in analysis of pretraining optimization dynamics: large top-1 vs target gaps produce strong gradient signals that are penalized during training, making them useful detection cues.&lt;/li&gt;&lt;li&gt;Evaluated on WikiMIA and MIMIR benchmarks, demonstrating state-of-the-art performance across model sizes and input lengths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseo Kwak', 'Jaehyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining data detection', 'membership inference', 'privacy', 'model auditing', 'training data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19936</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation</title><link>https://arxiv.org/abs/2601.20858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses FLORES-200 benchmark contamination in instruction-tuned multilingual LLMs by comparing Bloomz (contaminated) with Llama (control).&lt;/li&gt;&lt;li&gt;Demonstrates cross-directional contamination: target-side memorization can artificially boost performance in unseen translation directions.&lt;/li&gt;&lt;li&gt;Finds memorized references often persist under source-side perturbations, while named-entity replacement reliably reduces BLEU and serves as an effective probe for memorization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Tan', 'Pinzhen Chen', 'Josef van Genabith', 'Koel Dutta Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'memorization', 'benchmark contamination', 'privacy', 'machine translation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20858</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code</title><link>https://arxiv.org/abs/2601.20679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ShieldedCode, a framework that learns robust representations for virtual-machine-protected (VMP) code to improve software protection against reverse engineering.&lt;/li&gt;&lt;li&gt;Builds large-scale paired datasets of source code and normalized VM implementations and uses hierarchical dependency modeling (intra-, preceding-, inter-instruction) with joint language modeling and protection-aware contrastive objectives.&lt;/li&gt;&lt;li&gt;Introduces a protection effectiveness optimization task to quantify and rank VM variants and a two-stage continual pre-training + fine-tuning pipeline to enable generation, comparison, and reasoning over protected code.&lt;/li&gt;&lt;li&gt;Reports empirical gains in VM code generation (26.95% Pass@1 on L0 vs 22.58% for GPT-4o) and improved binary similarity detection (Recall@1 up ~10% vs jTrans).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqiao Mo', 'Yunlong Tan', 'Hao Zhang', 'Heng Zhang', 'Yangfan He']&lt;/li&gt;&lt;li&gt;Tags: ['software-protection', 'virtual-machine-protection', 'reverse-engineering', 'representation-learning', 'code-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20679</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility</title><link>https://arxiv.org/abs/2601.20256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SoftHateBench, a generative benchmark that rewrites explicit hateful standpoints into subtly framed, policy-compliant 'soft hate' using the Argumentum Model of Topics (AMT) and Relevance Theory (RT).&lt;/li&gt;&lt;li&gt;Dataset contains 4,745 soft-hate instances across 7 sociocultural domains and 28 target groups, preserving hostile stance while reducing surface toxicity cues.&lt;/li&gt;&lt;li&gt;Evaluates encoder-based detectors, general-purpose LLMs, and safety models, showing consistent performance drops from explicit ('hard') to reasoning-driven ('soft') hate, revealing moderation blind spots.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuanyu Su', 'Diana Inkpen', 'Nathalie Japkowicz']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'adversarial evasion', 'robustness benchmark', 'hate speech detection', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20256</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models</title><link>https://arxiv.org/abs/2601.20126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reinforcement Learning with Verifiable Rewards (RLVR) to explicitly reward abstention ('I don't know') alongside correctness to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Fine-tunes models (Granite-3.3-2B-Instruct, Qwen-3-4B-Instruct) on benchmarks (MedMCQA, Hendrycks Math) using a ternary reward structure and studies varying abstention reward values.&lt;/li&gt;&lt;li&gt;Finds moderate abstention rewards reduce incorrect responses without large accuracy drops; larger models are more robust to abstention incentives; supervised abstention pretraining can mitigate exploration limitations in open-ended QA.&lt;/li&gt;&lt;li&gt;Provides reproducible code for the abstention training framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abha Jha', 'Akanksha Mahajan', 'Ashwath Vaithinathan Aravindan', 'Praveen Saravanan', 'Sai Sailaja Policharla', 'Sonal Chaturbhuj Gehlot']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'hallucination-mitigation', 'reinforcement-learning', 'abstention', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20126</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning</title><link>https://arxiv.org/abs/2601.20055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERGE, a neurosymbolic framework that decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies consistency via SMT/theorem proving.&lt;/li&gt;&lt;li&gt;Uses multi-model consensus through formal semantic equivalence checking and semantic routing to send different claim types to symbolic solvers or LLM ensembles.&lt;/li&gt;&lt;li&gt;Localizes logical errors with Minimal Correction Subsets (MCS) to produce actionable feedback and iteratively refines answers until acceptance or convergence.&lt;/li&gt;&lt;li&gt;Aggregates verification signals into a unified score (with variance-based penalties) to provide formal guarantees where possible and improved trustworthiness in reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vikash Singh', 'Darion Cassel', 'Nathaniel Weir', 'Nick Feng', 'Sam Bayless']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'neurosymbolic', 'theorem proving', 'iterative refinement', 'proof-guided generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20055</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method</title><link>https://arxiv.org/abs/2601.20026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a quantum tensor network based uncertainty quantification framework for detecting LLM hallucinations by modeling aleatoric uncertainty in token sequence probabilities and clustering generations by semantic equivalence.&lt;/li&gt;&lt;li&gt;Introduces an entropy maximization strategy to prioritize high-certainty, semantically coherent outputs and to flag regions where model decisions are unreliable, enabling guidance for human oversight.&lt;/li&gt;&lt;li&gt;Empirically evaluates the method across 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD over multiple LLM architectures, reporting consistent AUROC and AURAC improvements over baselines.&lt;/li&gt;&lt;li&gt;Claims robustness across generation lengths, quantization levels, and resource-constrained deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pragatheeswaran Vipulanandan', 'Kamal Premaratne', 'Dilip Sarkar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty quantification', 'LLM safety', 'quantum tensor networks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20026</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text</title><link>https://arxiv.org/abs/2601.20006</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs large-scale corpora for detection: 1B-token human-authored corpus and 1.9B-token AI-generated corpus produced from multiple LLMs and domains.&lt;/li&gt;&lt;li&gt;Proposes two fine-tuning paradigms for detection: Per LLM and Per LLM-family fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluates detectors on a 100M-token benchmark covering 21 LLMs, achieving up to 99.6% token-level accuracy and outperforming open-source baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} Gromadzki', "Anna Wr\\'oblewska", 'Agnieszka Kaliska']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-text detection', 'forensics', 'dataset', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20006</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey</title><link>https://arxiv.org/abs/2601.19927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a survey of attribution-based techniques to mitigate hallucinations in Retrieval-Augmented Generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Proposes a taxonomy of hallucination types in RAG, and a unified pipeline describing how attribution methods are applied.&lt;/li&gt;&lt;li&gt;Reviews and compares attribution approaches by the hallucination types they target, discussing strengths, weaknesses, and practical guidance.&lt;/li&gt;&lt;li&gt;Identifies gaps and outlines directions for future research and practical deployment of attribution techniques for improving LLM output fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqing Zhao', 'Ziyao Liu', 'Yongsen Zheng', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Hallucination mitigation', 'Attribution', 'LLM safety', 'Verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19927</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2601.19918</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Lowest Span Confidence (LSC), a zero-shot, black-box metric for detecting hallucinations in LLM outputs using a single forward pass with output probabilities.&lt;/li&gt;&lt;li&gt;Computes joint likelihoods over variable-length n-gram spans via a sliding window to identify low-confidence regions correlated with factual inconsistency.&lt;/li&gt;&lt;li&gt;Aims to be efficient and robust compared to perplexity or minimum token probability, and shows improved detection across multiple SOTA LLMs and benchmarks under resource-constrained settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Qiao', 'Licheng Pan', 'Yu Mi', 'Lei Liu', 'Yue Shen', 'Fei Sun', 'Zhixuan Chu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'black-box metric', 'zero-shot', 'uncertainty estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19918</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text</title><link>https://arxiv.org/abs/2601.19913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LREAD, a rubric adapted from Korean national writing standards targeting micro-level artifacts (e.g., punctuation optionality, spacing, register shifts) to detect LLM-generated Korean text.&lt;/li&gt;&lt;li&gt;Runs a three-phase longitudinal blind protocol with Korean linguistics majors (intuition, rubric-scored with justifications, held-out mastery), boosting majority-vote accuracy from 60% to 100% and Fleiss' kappa from -0.09 to 0.82.&lt;/li&gt;&lt;li&gt;Finds calibrated humans use language-specific micro-diagnostics that state-of-the-art automated detectors miss, and releases the full rubric and a taxonomy of calibrated detection signatures as an interpretable complement to automated detectors in non-English settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated-text-detection', 'human-in-the-loop', 'forensic-linguistics', 'rubric-based-methods', 'non-English-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19913</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models</title><link>https://arxiv.org/abs/2601.19061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Thought-Transfer', an indirect targeted poisoning attack that manipulates Chain-of-Thought (CoT) reasoning traces in training data to change model behavior on a separate target task while leaving inputs and labels unchanged (clean-label poisoning).&lt;/li&gt;&lt;li&gt;Shows the attack transfers reasoning traces from one task to influence outputs on entirely different, never-seen domains, achieving ~70% success and concurrently improving model performance by ~10–15%, making poisoned datasets attractive to users.&lt;/li&gt;&lt;li&gt;Argues this new threat vector is not covered by prior poisoning/backdoor defenses and highlights challenges in defending reasoning-enabled LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Ethan Rathbun', 'Hanna Foerster', 'Jamie Hayes', 'Matthew Jagielski', 'Milad Nasr', 'Ilia Shumailov', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['poisoning', 'chain-of-thought', 'clean-label', 'targeted attack', 'backdoor']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19061</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversary-Aware Private Inference over Wireless Channels</title><link>https://arxiv.org/abs/2510.20518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversary-aware framework for privacy-preserving inference at wireless edge devices where sensors apply transformations to extracted features before transmission to a model server.&lt;/li&gt;&lt;li&gt;Targets protection of individual features (preventing reconstruction of sensitive personal data) beyond standard differential-privacy approaches for datasets.&lt;/li&gt;&lt;li&gt;Focuses on private inference over wireless channels, accounting for communication constraints and adversarial eavesdroppers/attackers in the edge-server inference pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Seif', 'Malcolm Egan', 'Andrea J. Goldsmith', 'H. Vincent Poor']&lt;/li&gt;&lt;li&gt;Tags: ['private inference', 'privacy-preserving transformations', 'adversary-aware defenses', 'edge/ wireless security', 'feature reconstruction protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20518</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title><link>https://arxiv.org/abs/2508.19112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep: a random forest classifier using deep features from a pretrained Swin Transformer encoder to detect out-of-distribution (OOD) CT scans and improve reliability of lung cancer segmentation.&lt;/li&gt;&lt;li&gt;Encoder pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans; segmentation trained on 317 3D scans and tested on 603 external 3D CTs including one in-distribution and four OOD datasets (PE, COVID-19, abdominal CTs).&lt;/li&gt;&lt;li&gt;RF-Deep outperforms established OOD methods, achieving low FPR95 on several OOD datasets and thus provides a simple effective OOD detection defense to flag unreliable segmentation outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'robustness', 'medical imaging', 'anomaly detection', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19112</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</title><link>https://arxiv.org/abs/2412.01256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NLPrompt, a noise-robust prompt learning framework for vision-language models that combines mean absolute error (PromptMAE) for robustness and an optimal-transport based data purification (PromptOT).&lt;/li&gt;&lt;li&gt;Argues and provides theoretical intuition that MAE suppresses influence of noisy samples, improving signal-to-noise ratio in prompt learning.&lt;/li&gt;&lt;li&gt;PromptOT uses text features as prototypes to partition datasets into clean and noisy subsets, applying cross-entropy to clean data and MAE to noisy data.&lt;/li&gt;&lt;li&gt;Extensive empirical evaluation across noise settings showing improved performance under label noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bikang Pan', 'Qun Li', 'Xiaoying Tang', 'Wei Huang', 'Zhen Fang', 'Feng Liu', 'Jingya Wang', 'Jingyi Yu', 'Ye Shi']&lt;/li&gt;&lt;li&gt;Tags: ['label-noise', 'robustness', 'prompt-learning', 'vision-language-models', 'data-purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01256</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Feature-Space Adversarial Robustness Certification for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.16200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Feature-space Smoothing (FS), a framework that provides certified robustness guarantees at the feature representation level of multimodal LLMs under l2-bounded input perturbations.&lt;/li&gt;&lt;li&gt;Proves a certified lower bound on cosine similarity between clean and adversarial features (Feature Cosine Similarity Bound, FCSB) and links it to the encoder's Gaussian robustness score.&lt;/li&gt;&lt;li&gt;Introduces Gaussian Smoothness Booster (GSB), a plug-and-play module to increase the Gaussian robustness score of pretrained MLLMs without retraining, improving the certified guarantees from FS.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing FS (with GSB) yields strong certified feature-space robustness and robust downstream task performance across diverse multimodal applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song Xia', 'Meiwen Ding', 'Chenqi Kong', 'Wenhan Yang', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'certified defenses', 'multimodal LLMs', 'feature-space attacks/defenses', 'randomized smoothing / Gaussian smoothing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16200</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports</title><link>https://arxiv.org/abs/2509.02072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ABEX-RAT: a two-stage abstractive-expansive (ABEX) LLM-guided augmentation pipeline to synthesize label-preserving samples for class imbalance, followed by Random Adversarial Training (RAT) to inject stochastic perturbations during classifier training.&lt;/li&gt;&lt;li&gt;Targets resource-efficient classification of occupational accident reports and achieves reported SOTA Macro-F1 (90.32%) on the OSHA dataset without fine-tuning large models.&lt;/li&gt;&lt;li&gt;The adversarial component is used as a robustness/generalization technique rather than an analysis of attack vectors or threat models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Chen', 'Jiabao Dou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'data-augmentation', 'LLM-augmentation', 'robustness', 'class-imbalance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02072</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>In-Context Bias Propagation in LLM-Based Tabular Data Generation</title><link>https://arxiv.org/abs/2506.09630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that statistical biases in in‑context examples systematically propagate into LLM‑generated synthetic tabular data, producing global distributional distortions.&lt;/li&gt;&lt;li&gt;Defines an adversarial threat model where a malicious contributor injects biased in‑context examples to poison synthetic data and degrade fairness of downstream classifiers for targeted protected groups.&lt;/li&gt;&lt;li&gt;Evaluates preprocessing‑based mitigation strategies that can reduce but not fully eliminate disparity, highlighting persistent sensitivity of LLMs to adversarial prompts in data generation pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pol G. Recasens', 'Alberto Gutierrez', 'Jordi Torres', 'Josep. Ll Berral', 'Javier Carnerero-Cano', 'Anisa Halimi', 'Kieran Fraser']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'in-context-learning', 'bias-propagation', 'adversarial-prompts', 'defenses/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09630</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</title><link>https://arxiv.org/abs/2411.08862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLMStinger: an attacker LLM fine-tuned via reinforcement learning to autonomously generate adversarial suffixes for jailbreak attacks.&lt;/li&gt;&lt;li&gt;Operates without white-box access or complex manual prompt engineering; uses an RL loop to iteratively improve suffixes based on harmful queries from the HarmBench benchmark.&lt;/li&gt;&lt;li&gt;Demonstrates substantial Attack Success Rate (ASR) improvements versus 15 baseline red-teaming methods across multiple models (e.g., +57.2% on LLaMA2-7B-chat, +50.3% on Claude 2), and reports very high ASR on GPT-3.5 and Gemma-2B-it.&lt;/li&gt;&lt;li&gt;Focuses on automated attack generation and robust red-teaming evaluation, highlighting a practical vulnerability in deployed LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piyush Jha', 'Arnav Arora', 'Vijay Ganesh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-attack', 'red-teaming', 'reinforcement-learning', 'attack-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.08862</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lipschitz-Regularized Critics Lead to Policy Robustness Against Transition Dynamics Uncertainty</title><link>https://arxiv.org/abs/2404.13879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PPO-PGDLC: a robust RL algorithm combining PPO with Projected Gradient Descent (PGD) to find adversarial states within an uncertainty set and a Lipschitz-regularized critic to enforce smoothness.&lt;/li&gt;&lt;li&gt;Aims to approximate a robust Bellman operator via adversarial state generation (PGD) while using critic Lipschitz regularization to improve policy smoothness under transition dynamics uncertainty.&lt;/li&gt;&lt;li&gt;Evaluated on two classic control tasks and one real-world robotic locomotion task, showing improved performance and smoother actions under environmental perturbations compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xulin Chen', 'Ruipeng Liu', 'Zhenyu Gan', 'Garrett E. Katz']&lt;/li&gt;&lt;li&gt;Tags: ['robust reinforcement learning', 'adversarial robustness', 'Lipschitz regularization', 'PGD', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.13879</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Privacy Risks of Sharpness Aware Minimization</title><link>https://arxiv.org/abs/2310.00488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that Sharpness-Aware Minimization (SAM), despite improving generalization, increases susceptibility to membership inference attacks (MIA) compared to SGD across multiple datasets and attack methods.&lt;/li&gt;&lt;li&gt;Empirically links SAM's propensity to capture atypical subpatterns and higher memorization/influence scores to increased membership leakage and reduced variance in prediction confidences.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis in a perfectly interpolating linear regime showing that sharpness regularization reduces output variance and therefore guarantees higher MIA advantage for confidence and likelihood-ratio style attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young In Kim', 'Andrea Agiollo', 'Pratiksha Agrawal', 'Johannes O. Royset', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'model privacy', 'sharpness-aware minimization (SAM)', 'memorization', 'attack analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00488</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal Multi-Agent Ransomware Analysis Using AutoGen</title><link>https://arxiv.org/abs/2601.20346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal, multi-agent ransomware analysis framework combining static, dynamic, and network modalities; each modality processed by specialized agents using autoencoder-based feature extraction.&lt;/li&gt;&lt;li&gt;Features are integrated by a fusion agent and classified by a transformer-based classifier; agents interact via an inter-agent feedback loop that suppresses low-confidence information and iteratively refines representations.&lt;/li&gt;&lt;li&gt;Evaluated at scale on thousands of ransomware and benign samples, showing substantial gains in family classification Macro-F1, improved calibration, and a confidence-aware abstention mechanism for safer real-world deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asifullah Khan', 'Aimen Wadood', 'Mubashar Iqbal', 'Umme Zahoora']&lt;/li&gt;&lt;li&gt;Tags: ['ransomware-detection', 'malware-classification', 'multimodal-fusion', 'multi-agent-systems', 'cybersecurity-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20346</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks</title><link>https://arxiv.org/abs/2601.20310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemBind, a defense that binds latent-based watermarks to image semantics via a learned semantic masker trained with contrastive learning to produce prompt-invariant and cross-prompt-orthogonal codes.&lt;/li&gt;&lt;li&gt;The masker reshapes and permutes codes to modulate the target latent before applying standard latent-based watermarks, remaining compatible with existing watermarking schemes and preserving image quality.&lt;/li&gt;&lt;li&gt;Includes a tunable mask-ratio parameter to trade off anti-forgery strength and robustness; demonstrates substantial reductions in false acceptance under black-box forgery across four mainstream latent watermark methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Zhang', 'Zijin Yang', 'Kejiang Chen', 'Linfeng Ma', 'Weiming Zhang', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'anti-forgery', 'latent diffusion models', 'black-box attacks', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20310</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking</title><link>https://arxiv.org/abs/2601.20283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a minimal, query-aware single-word adversarial attack (insert/substitute a 'query center' word) to promote target documents in neural ranking models.&lt;/li&gt;&lt;li&gt;Proposes heuristic and gradient-guided variants, including a white-box method that finds influential insertion points.&lt;/li&gt;&lt;li&gt;Empirical results on TREC-DL 2019/2020 with BERT and monoT5 re-rankers: up to 91% attack success while modifying fewer than two tokens per document on average.&lt;/li&gt;&lt;li&gt;Provides diagnostic metrics revealing a 'Goldilocks' vulnerability zone (mid-ranked documents most susceptible) and discusses implications for defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanmay Karmakar', 'Sourav Saha', 'Debapriyo Majumdar', 'Surjyanee Halder']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'neural-ranking', 'white-box-attack', 'model-robustness', 'information-retrieval-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20283</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models</title><link>https://arxiv.org/abs/2601.20126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reinforcement Learning with Verifiable Rewards (RLVR) to explicitly reward abstention ('I don't know') alongside correctness to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Fine-tunes models (Granite-3.3-2B-Instruct, Qwen-3-4B-Instruct) on benchmarks (MedMCQA, Hendrycks Math) using a ternary reward structure and studies varying abstention reward values.&lt;/li&gt;&lt;li&gt;Finds moderate abstention rewards reduce incorrect responses without large accuracy drops; larger models are more robust to abstention incentives; supervised abstention pretraining can mitigate exploration limitations in open-ended QA.&lt;/li&gt;&lt;li&gt;Provides reproducible code for the abstention training framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abha Jha', 'Akanksha Mahajan', 'Ashwath Vaithinathan Aravindan', 'Praveen Saravanan', 'Sai Sailaja Policharla', 'Sonal Chaturbhuj Gehlot']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'hallucination-mitigation', 'reinforcement-learning', 'abstention', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20126</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</title><link>https://arxiv.org/abs/2601.20103</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy of 54 reward-exploit categories and TRACE, a human-verified benchmark of 517 code execution trajectories for testing reward-hacking detection.&lt;/li&gt;&lt;li&gt;Compares isolated classification evaluation with a contrastive anomaly-detection setup, finding contrastive settings yield substantially better detection (e.g., GPT-5.2 highest reasoning: 63% vs 45%).&lt;/li&gt;&lt;li&gt;Analyzes detection failure modes (semantic vs syntactic contextualization), and ablates factors such as benign-to-hacked ratio and cluster sizes affecting performance; releases benchmark and evaluation harness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshan Deshpande', 'Anand Kannappan', 'Rebecca Qian']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'adversarial RL', 'anomaly detection', 'benchmark', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20103</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GNN Explanations that do not Explain and How to find Them</title><link>https://arxiv.org/abs/2601.20815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a critical failure mode of self-explainable GNNs (SE-GNNs): explanations can be unambiguously unrelated to how the model infers labels (degenerate explanations) while the model attains optimal risk.&lt;/li&gt;&lt;li&gt;Shows existing faithfulness metrics often fail to detect these degenerate explanations and that such explanations can be both maliciously planted (to hide use of sensitive attributes) and arise naturally.&lt;/li&gt;&lt;li&gt;Proposes a novel faithfulness metric that reliably detects degenerate explanations in both adversarial (malicious planting) and natural scenarios; provides empirical validation and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steve Azzolin', 'Stefano Teso', 'Bruno Lepri', 'Andrea Passerini', 'Sagar Malhotra']&lt;/li&gt;&lt;li&gt;Tags: ['GNN', 'explainability', 'adversarial manipulation', 'privacy', 'faithfulness metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20815</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying</title><link>https://arxiv.org/abs/2601.20773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Focuses on black-box model copying (model extraction) under label-only access by replacing hard-label supervision with signed-distance (distance-to-decision-boundary) supervision to turn copying into a smooth regression problem.&lt;/li&gt;&lt;li&gt;Proposes an α-governed smoothing and regularization framework (Hölder/Lipschitz control) and two model-agnostic algorithms to estimate signed distances using only label queries.&lt;/li&gt;&lt;li&gt;Empirical results on synthetic tasks and UCI benchmarks show improved fidelity and generalization compared to hard-label baselines; distance outputs can also serve as uncertainty-related signals for replicas.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Rub\\'en Jim\\'enez", 'Oriol Pujol']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'black-box attacks', 'model stealing', 'boundary estimation', 'signed-distance supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20773</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs</title><link>https://arxiv.org/abs/2601.20704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melika Mobini', 'Vincent Holst', 'Floriano Tori', 'Andres Algaba', 'Vincent Ginis']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20704</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Contextual Runtime Monitors for Safe AI-Based Autonomy</title><link>https://arxiv.org/abs/2601.20666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes context-aware runtime monitors that observe environment context and select the most suitable ML controller from an ensemble instead of blending outputs.&lt;/li&gt;&lt;li&gt;Formulates monitor learning as a contextual multi-armed bandit problem, yielding theoretical safety guarantees for controller selection.&lt;/li&gt;&lt;li&gt;Validates the approach in simulated autonomous driving scenarios, showing improved safety and performance compared to non-contextual baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro Luque-Cerpa', 'Mengyuan Wang', 'Emil Carlsson', 'Sanjit A. Seshia', 'Devdatt Dubhashi', 'Hazem Torfah']&lt;/li&gt;&lt;li&gt;Tags: ['runtime monitoring', 'safety mechanisms', 'contextual bandits', 'autonomous systems', 'control ensembles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20666</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization in diffusion-based image generative models and shows norm-based detection metrics fail in anisotropic (low-noise) regimes.&lt;/li&gt;&lt;li&gt;Introduces a combined detection metric that integrates isotropic norm and anisotropic angular alignment between guidance and unconditional scores, computable on pure noise with two forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates the metric outperforms prior denoising-free detectors on Stable Diffusion v1.4 and v2, with ~5x speedup, and proposes a mitigation strategy that adapts memorized prompts using the metric.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'diffusion models', 'detection', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reinforcement Unlearning via Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2601.20568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PURGE (Policy Unlearning through Group Relative Policy Optimization), a reinforcement-learning-based method to remove memorized sensitive/copyrighted content from LLMs without full retraining.&lt;/li&gt;&lt;li&gt;Frames unlearning as a verifiable task using an intrinsic reward that penalizes any mention of forbidden concepts, aiming for safe and consistent forgetting.&lt;/li&gt;&lt;li&gt;Reports efficiency and utility gains: up to 46x reduction in token usage per target vs. prior methods, +5.48% fluency, +12.02% adversarial robustness, and on the RWKU benchmark achieves 11% unlearning effectiveness while preserving 98% of original utility.&lt;/li&gt;&lt;li&gt;Positions the approach as combining theoretical guarantees with practical deployment efficiency for privacy/compliance-focused model editing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Efstratios Zaradoukas', 'Bardh Prenkaj', 'Gjergji Kasneci']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'model editing', 'defense', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20568</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title><link>https://arxiv.org/abs/2601.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces peer prediction — a game-theoretic, mutual-predictability-based method — to evaluate and train LLMs for truthfulness using only weak supervision (no ground-truth labels).&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical validation (models up to 405B) showing peer prediction rewards honest/informative answers and resists deceptive behavior.&lt;/li&gt;&lt;li&gt;Demonstrates training utility: peer-prediction-based reward can recover truthfulness lost to malicious fine-tuning (shown on an 8B model) even when the reward model is much smaller and unfine-tuned.&lt;/li&gt;&lt;li&gt;Identifies an inverse-scaling property: peer prediction becomes more robust as the capability gap between judge and target widens, enabling reliable evaluation of stronger models with weaker supervisors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Alex Qiu', 'Micah Carroll', 'Cameron Allen']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'evaluation-robustness', 'incentive-design', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20299</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery</title><link>https://arxiv.org/abs/2601.20193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a meta-cognitive reinforcement learning framework that monitors internal reliability via a meta-trust variable driven by Value Prediction Error Stability (VPES).&lt;/li&gt;&lt;li&gt;Uses the meta-trust signal to regulate learning with fail-safe mechanisms and gradual trust recovery to avoid catastrophic failures from unreliable experiences.&lt;/li&gt;&lt;li&gt;Evaluates on continuous-control benchmarks with reward corruption, showing higher returns and fewer late-stage training failures compared to strong robustness baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng Zhang', 'Wenting Ma', 'Kai Li', 'Meng Guo', 'Lei Yang', 'Wei Yu', 'Hongji Cui', 'Yichen Zhang', 'Mo Zhang', 'Jinzhe Lin', 'Zhenjie Yao']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'robustness', 'reward-corruption', 'meta-cognition', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20193</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIA) against Diffusion Language Models (DLMs), identifying a larger attack surface due to many maskable configurations.&lt;/li&gt;&lt;li&gt;Introduces SAMA (Subset-Aggregated Membership Attack): samples masked subsets across densities, uses sign-based statistics and inverse-weighted aggregation to robustly detect sparse memorization.&lt;/li&gt;&lt;li&gt;Empirical evaluation on nine datasets shows up to 30% relative AUC improvement over the best baseline and up to 8x improvement at low false positive rates, exposing significant privacy vulnerabilities in DLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Kaiyuan Zhang', 'Yuntao Du', 'Edoardo Stoppa', 'Charles Fleming', 'Ashish Kundu', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'diffusion-language-models', 'model-attacks', 'adversarial-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20125</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</title><link>https://arxiv.org/abs/2601.19967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Perturbation-Induced Linearization (PIL), a method to generate unlearnable (training-poisoning) perturbations using only linear surrogate models, greatly reducing computation versus deep-surrogate approaches.&lt;/li&gt;&lt;li&gt;Demonstrates PIL matches or outperforms prior surrogate-based unlearnable example methods and explains effectiveness via a mechanism: inducing linearization in deep models.&lt;/li&gt;&lt;li&gt;Analyzes properties of unlearnable examples under percentage-based partial perturbation and discusses implications for data protection against unauthorized model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinlin Liu', 'Wei Chen', 'Xiaojin Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable examples', 'data poisoning', 'adversarial perturbation', 'data protection', 'surrogate models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19967</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data</title><link>https://arxiv.org/abs/2601.19936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gap-K%, a method to detect whether text was included in LLM pretraining by measuring the log-probability gap between the model's top-1 predicted token and the actual target token.&lt;/li&gt;&lt;li&gt;Incorporates a sliding-window strategy to capture local token correlations and reduce token-level noise.&lt;/li&gt;&lt;li&gt;Grounded in analysis of pretraining optimization dynamics: large top-1 vs target gaps produce strong gradient signals that are penalized during training, making them useful detection cues.&lt;/li&gt;&lt;li&gt;Evaluated on WikiMIA and MIMIR benchmarks, demonstrating state-of-the-art performance across model sizes and input lengths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseo Kwak', 'Jaehyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining data detection', 'membership inference', 'privacy', 'model auditing', 'training data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19936</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight supervised detector for hallucinations in visual question answering that leverages rich internal signals from vision-language models (token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features).&lt;/li&gt;&lt;li&gt;Fuses these signals via branch-wise evidence encoding and uncertainty-aware attention, and introduces a low-cost, model-dependent automatic supervision strategy by extending the LLM-as-a-Judge paradigm to VQA.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection accuracy and efficiency over prior external-verifier and uncertainty-driven methods on multiple VQA benchmarks, and analyzes how different internal signals and VLM architectures reveal complementary hallucination patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'VQA', 'model-internal-signals', 'safety-defenses', 'LLM-as-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title><link>https://arxiv.org/abs/2512.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a hybrid, production-scale content moderation system combining supervised classifiers for known violations with reference-based similarity matching for novel or subtle cases.&lt;/li&gt;&lt;li&gt;Uses multimodal inputs (text, audio, visual) and leverages a multimodal large language model (MLLM) to distill knowledge and boost accuracy while keeping inference lightweight.&lt;/li&gt;&lt;li&gt;Reports production metrics: classification pipeline 67% recall at 80% precision, similarity pipeline 76% recall at 80% precision, and a 6–8% reduction in user views of unwanted livestreams in A/B tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Chee Yew', 'Hailun Xu', 'Sanjay Saha', 'Xiaotian Fan', 'Hiok Hian Ong', 'David Yuchen Wang', 'Kanchan Sarkar', 'Zhenheng Yang', 'Danhui Guan']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'multimodal-MLLM', 'defense', 'similarity-matching', 'production-system']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03553</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection</title><link>https://arxiv.org/abs/2509.20682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies gradient misalignment between original and augmented inputs during data-augmented training for speech deepfake detection, causing conflicting updates and suboptimal convergence.&lt;/li&gt;&lt;li&gt;Proposes a dual-path data-augmented (DPDA) training framework that computes gradients for original and augmented inputs separately and aligns their directions to reduce optimization conflicts.&lt;/li&gt;&lt;li&gt;Finds ~25% of training iterations exhibit gradient conflicts with RawBoost augmentation; gradient alignment speeds convergence (fewer epochs) and yields up to 18.69% relative EER reduction on the In-the-Wild dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duc-Tuan Truong', 'Tianchi Liu', 'Junjie Li', 'Ruijie Tao', 'Kong Aik Lee', 'Eng Siong Chng']&lt;/li&gt;&lt;li&gt;Tags: ['speech-deepfake-detection', 'data-augmentation', 'gradient-alignment', 'robustness', 'spoofing-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20682</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mask-GCG, a plug-and-play extension to Greedy Coordinate Gradient (GCG) that uses learnable token masking to identify and prune low-impact tokens in adversarial suffixes.&lt;/li&gt;&lt;li&gt;Pruning reduces gradient space and computational overhead while maintaining attack success rate (ASR) and loss values, shortening time to successful jailbreaks.&lt;/li&gt;&lt;li&gt;Evaluates Mask-GCG across original GCG and several improved variants, showing most suffix tokens are impactful but a minority are redundant—offering insights into efficient and interpretable jailbreak construction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Mu', 'Zonghao Ying', 'Zhekui Fan', 'Zonglei Jing', 'Yaoyuan Zhang', 'Zhengmin Yu', 'Wenxin Zhang', 'Quanchen Zou', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'prompt optimization', 'attack efficiency', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06350</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models</title><link>https://arxiv.org/abs/2504.19373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel privacy vulnerability in multi-modal large reasoning models (MLRMs): automated geolocation inference from user images (doxing risk).&lt;/li&gt;&lt;li&gt;Proposes a three-level visual privacy risk framework and releases DoxBench, a 500-image dataset covering diverse privacy scenarios for evaluation.&lt;/li&gt;&lt;li&gt;Evaluates 11 advanced MLRMs/MLLMs, showing models often outperform non-expert humans in geolocation inference and can leak sensitive location information.&lt;/li&gt;&lt;li&gt;Introduces GeoMiner, a two-stage collaborative attack (clue extraction + reasoning) to improve geolocation attacks, and analyzes root causes and mitigation needs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weidi Luo', 'Tianyu Lu', 'Qiming Zhang', 'Xiaogeng Liu', 'Bin Hu', 'Yue Zhao', 'Jieyu Zhao', 'Song Gao', 'Patrick McDaniel', 'Zhen Xiang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['location privacy', 'privacy leakage', 'adversarial attack', 'privacy benchmark', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.19373</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning</title><link>https://arxiv.org/abs/2502.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first empirical study applying machine unlearning to reduce sensitive information leakage in large language models for code (LLMs4Code).&lt;/li&gt;&lt;li&gt;Constructs a benchmark with a synthetic forget set (personal information) and a retain set (to test utility preservation) and evaluates three unlearning algorithms (GA, GA+GD, GA+KL) on three open-source code models.&lt;/li&gt;&lt;li&gt;Finds machine unlearning reduces direct memorization-based leakage by &gt;50% on average while retaining ~91% of code-generation performance.&lt;/li&gt;&lt;li&gt;Identifies a persistent shift from direct to indirect leakage after unlearning, highlighting an underexplored residual vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shanzhi Gu', 'Zhaoyang Qu', 'Ruotong Geng', 'Mingyang Geng', 'Shangwen Wang', 'Chuanfu Xu', 'Haotian Wang', 'Zhipeng Lin', 'Dezun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'machine unlearning', 'data leakage', 'model memorization', 'privacy defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05739</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning</title><link>https://arxiv.org/abs/2501.19180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety Chain-of-Thought (SCoT): a proactive defense that uses LLM reasoning to assess intent in inputs before answering, rather than just refusing.&lt;/li&gt;&lt;li&gt;Augments refusal training datasets with reasoning steps to improve generalization to out-of-distribution and adversarial jailbreak queries.&lt;/li&gt;&lt;li&gt;Generates detailed, rule-specific refusals and aims to reduce vulnerability to sophisticated attacks while preserving model capabilities.&lt;/li&gt;&lt;li&gt;Reports comparative evaluations showing SCoT outperforms existing defenses on robustness to adversarial manipulations and rare corner cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianglin Yang', 'Gelei Deng', 'Jieming Shi', 'Tianwei Zhang', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'safety chain-of-thought', 'adversarial robustness', 'refusal training', 'proactive safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19180</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Privacy Risks of Sharpness Aware Minimization</title><link>https://arxiv.org/abs/2310.00488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that Sharpness-Aware Minimization (SAM), despite improving generalization, increases susceptibility to membership inference attacks (MIA) compared to SGD across multiple datasets and attack methods.&lt;/li&gt;&lt;li&gt;Empirically links SAM's propensity to capture atypical subpatterns and higher memorization/influence scores to increased membership leakage and reduced variance in prediction confidences.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis in a perfectly interpolating linear regime showing that sharpness regularization reduces output variance and therefore guarantees higher MIA advantage for confidence and likelihood-ratio style attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young In Kim', 'Andrea Agiollo', 'Pratiksha Agrawal', 'Johannes O. Royset', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'model privacy', 'sharpness-aware minimization (SAM)', 'memorization', 'attack analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00488</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations</title><link>https://arxiv.org/abs/2510.26905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Cognition Envelopes' to bound and constrain AI reasoning in autonomous UAS operations to mitigate hallucinations, overgeneralization, and context misalignment.&lt;/li&gt;&lt;li&gt;Positions Cognition Envelopes as complementary to meta-cognition and traditional safety envelopes, requiring guidelines and systematic processes for definition, validation, and assurance.&lt;/li&gt;&lt;li&gt;Targets risks introduced by foundation models (LLMs, VLMs) in cyber-physical/autonomous systems and focuses on engineering defenses/guardrails for safer decision-making.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedro Antonio Alarcon Granadeno', 'Arturo Miguel Bernal Russell', 'Sofia Nelson', 'Demetrius Hernandez', 'Maureen Petterson', 'Michael Murphy', 'Walter J. Scheirer', 'Jane Cleland-Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety envelopes', 'guardrails', 'LLM/VLM robustness', 'autonomous systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26905</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GNN Explanations that do not Explain and How to find Them</title><link>https://arxiv.org/abs/2601.20815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a critical failure mode of self-explainable GNNs (SE-GNNs): explanations can be unambiguously unrelated to how the model infers labels (degenerate explanations) while the model attains optimal risk.&lt;/li&gt;&lt;li&gt;Shows existing faithfulness metrics often fail to detect these degenerate explanations and that such explanations can be both maliciously planted (to hide use of sensitive attributes) and arise naturally.&lt;/li&gt;&lt;li&gt;Proposes a novel faithfulness metric that reliably detects degenerate explanations in both adversarial (malicious planting) and natural scenarios; provides empirical validation and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steve Azzolin', 'Stefano Teso', 'Bruno Lepri', 'Andrea Passerini', 'Sagar Malhotra']&lt;/li&gt;&lt;li&gt;Tags: ['GNN', 'explainability', 'adversarial manipulation', 'privacy', 'faithfulness metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20815</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Contextual Runtime Monitors for Safe AI-Based Autonomy</title><link>https://arxiv.org/abs/2601.20666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes context-aware runtime monitors that observe environment context and select the most suitable ML controller from an ensemble instead of blending outputs.&lt;/li&gt;&lt;li&gt;Formulates monitor learning as a contextual multi-armed bandit problem, yielding theoretical safety guarantees for controller selection.&lt;/li&gt;&lt;li&gt;Validates the approach in simulated autonomous driving scenarios, showing improved safety and performance compared to non-contextual baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro Luque-Cerpa', 'Mengyuan Wang', 'Emil Carlsson', 'Sanjit A. Seshia', 'Devdatt Dubhashi', 'Hazem Torfah']&lt;/li&gt;&lt;li&gt;Tags: ['runtime monitoring', 'safety mechanisms', 'contextual bandits', 'autonomous systems', 'control ensembles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20666</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization in diffusion-based image generative models and shows norm-based detection metrics fail in anisotropic (low-noise) regimes.&lt;/li&gt;&lt;li&gt;Introduces a combined detection metric that integrates isotropic norm and anisotropic angular alignment between guidance and unconditional scores, computable on pure noise with two forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates the metric outperforms prior denoising-free detectors on Stable Diffusion v1.4 and v2, with ~5x speedup, and proposes a mitigation strategy that adapts memorized prompts using the metric.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'diffusion models', 'detection', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Audio Deepfake Detection in the Age of Advanced Text-to-Speech models</title><link>https://arxiv.org/abs/2601.20510</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robin Singh', 'Aditya Yogesh Nair', 'Fabio Palumbo', 'Florian Barbaro', 'Anna Dyka', 'Lohith Rachakonda']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20510</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self Voice Conversion as an Attack against Neural Audio Watermarking</title><link>https://arxiv.org/abs/2601.20432</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates self voice conversion as an attack that preserves speaker identity and content while altering acoustic features to evade neural audio watermarks.&lt;/li&gt;&lt;li&gt;Shows that state-of-the-art audio watermarking schemes are severely degraded by this deep-learning-based, content-preserving transformation.&lt;/li&gt;&lt;li&gt;Highlights a novel threat model for watermark robustness testing beyond conventional distortions (noise, compression, resampling) and discusses security implications for modern watermarking systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yigitcan \\"Ozer', 'Wanying Ge', 'Zhe Zhang', 'Xin Wang', 'Junichi Yamagishi']&lt;/li&gt;&lt;li&gt;Tags: ['audio watermarking', 'adversarial attack', 'voice conversion', 'robustness evaluation', 'audio security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20432</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal Multi-Agent Ransomware Analysis Using AutoGen</title><link>https://arxiv.org/abs/2601.20346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal, multi-agent ransomware analysis framework combining static, dynamic, and network modalities; each modality processed by specialized agents using autoencoder-based feature extraction.&lt;/li&gt;&lt;li&gt;Features are integrated by a fusion agent and classified by a transformer-based classifier; agents interact via an inter-agent feedback loop that suppresses low-confidence information and iteratively refines representations.&lt;/li&gt;&lt;li&gt;Evaluated at scale on thousands of ransomware and benign samples, showing substantial gains in family classification Macro-F1, improved calibration, and a confidence-aware abstention mechanism for safer real-world deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asifullah Khan', 'Aimen Wadood', 'Mubashar Iqbal', 'Umme Zahoora']&lt;/li&gt;&lt;li&gt;Tags: ['ransomware-detection', 'malware-classification', 'multimodal-fusion', 'multi-agent-systems', 'cybersecurity-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20346</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title><link>https://arxiv.org/abs/2601.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces peer prediction — a game-theoretic, mutual-predictability-based method — to evaluate and train LLMs for truthfulness using only weak supervision (no ground-truth labels).&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical validation (models up to 405B) showing peer prediction rewards honest/informative answers and resists deceptive behavior.&lt;/li&gt;&lt;li&gt;Demonstrates training utility: peer-prediction-based reward can recover truthfulness lost to malicious fine-tuning (shown on an 8B model) even when the reward model is much smaller and unfine-tuned.&lt;/li&gt;&lt;li&gt;Identifies an inverse-scaling property: peer prediction becomes more robust as the capability gap between judge and target widens, enabling reliable evaluation of stronger models with weaker supervisors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Alex Qiu', 'Micah Carroll', 'Cameron Allen']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'evaluation-robustness', 'incentive-design', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20299</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Eliciting Least-to-Most Reasoning for Phishing URL Detection</title><link>https://arxiv.org/abs/2601.20270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Least-to-Most prompting framework with an "answer sensitivity" mechanism to improve LLM reasoning for phishing URL detection.&lt;/li&gt;&lt;li&gt;Evaluates the method on three URL datasets and four state-of-the-art LLMs, comparing against one-shot prompting and a supervised model.&lt;/li&gt;&lt;li&gt;Finds the approach outperforms one-shot prompting and matches supervised performance while requiring much less training/few-shot data.&lt;/li&gt;&lt;li&gt;Provides analysis showing iterative reasoning and the answer sensitivity mechanism drive the performance gains; experimental setup is shared on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Holly Trikilis', 'Pasindu Marasinghe', 'Fariza Rashid', 'Suranga Seneviratne']&lt;/li&gt;&lt;li&gt;Tags: ['phishing-detection', 'cybersecurity', 'LLM-prompting', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20270</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery</title><link>https://arxiv.org/abs/2601.20193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a meta-cognitive reinforcement learning framework that monitors internal reliability via a meta-trust variable driven by Value Prediction Error Stability (VPES).&lt;/li&gt;&lt;li&gt;Uses the meta-trust signal to regulate learning with fail-safe mechanisms and gradual trust recovery to avoid catastrophic failures from unreliable experiences.&lt;/li&gt;&lt;li&gt;Evaluates on continuous-control benchmarks with reward corruption, showing higher returns and fewer late-stage training failures compared to strong robustness baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng Zhang', 'Wenting Ma', 'Kai Li', 'Meng Guo', 'Lei Yang', 'Wei Yu', 'Hongji Cui', 'Yichen Zhang', 'Mo Zhang', 'Jinzhe Lin', 'Zhenjie Yao']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'robustness', 'reward-corruption', 'meta-cognition', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20193</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models</title><link>https://arxiv.org/abs/2601.20126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reinforcement Learning with Verifiable Rewards (RLVR) to explicitly reward abstention ('I don't know') alongside correctness to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Fine-tunes models (Granite-3.3-2B-Instruct, Qwen-3-4B-Instruct) on benchmarks (MedMCQA, Hendrycks Math) using a ternary reward structure and studies varying abstention reward values.&lt;/li&gt;&lt;li&gt;Finds moderate abstention rewards reduce incorrect responses without large accuracy drops; larger models are more robust to abstention incentives; supervised abstention pretraining can mitigate exploration limitations in open-ended QA.&lt;/li&gt;&lt;li&gt;Provides reproducible code for the abstention training framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abha Jha', 'Akanksha Mahajan', 'Ashwath Vaithinathan Aravindan', 'Praveen Saravanan', 'Sai Sailaja Policharla', 'Sonal Chaturbhuj Gehlot']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'hallucination-mitigation', 'reinforcement-learning', 'abstention', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20126</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIA) against Diffusion Language Models (DLMs), identifying a larger attack surface due to many maskable configurations.&lt;/li&gt;&lt;li&gt;Introduces SAMA (Subset-Aggregated Membership Attack): samples masked subsets across densities, uses sign-based statistics and inverse-weighted aggregation to robustly detect sparse memorization.&lt;/li&gt;&lt;li&gt;Empirical evaluation on nine datasets shows up to 30% relative AUC improvement over the best baseline and up to 8x improvement at low false positive rates, exposing significant privacy vulnerabilities in DLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Kaiyuan Zhang', 'Yuntao Du', 'Edoardo Stoppa', 'Charles Fleming', 'Ashish Kundu', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'diffusion-language-models', 'model-attacks', 'adversarial-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20125</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</title><link>https://arxiv.org/abs/2601.20103</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy of 54 reward-exploit categories and TRACE, a human-verified benchmark of 517 code execution trajectories for testing reward-hacking detection.&lt;/li&gt;&lt;li&gt;Compares isolated classification evaluation with a contrastive anomaly-detection setup, finding contrastive settings yield substantially better detection (e.g., GPT-5.2 highest reasoning: 63% vs 45%).&lt;/li&gt;&lt;li&gt;Analyzes detection failure modes (semantic vs syntactic contextualization), and ablates factors such as benign-to-hacked ratio and cluster sizes affecting performance; releases benchmark and evaluation harness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshan Deshpande', 'Anand Kannappan', 'Rebecca Qian']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'adversarial RL', 'anomaly detection', 'benchmark', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20103</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning</title><link>https://arxiv.org/abs/2601.20055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERGE, a neurosymbolic framework that decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies consistency via SMT/theorem proving.&lt;/li&gt;&lt;li&gt;Uses multi-model consensus through formal semantic equivalence checking and semantic routing to send different claim types to symbolic solvers or LLM ensembles.&lt;/li&gt;&lt;li&gt;Localizes logical errors with Minimal Correction Subsets (MCS) to produce actionable feedback and iteratively refines answers until acceptance or convergence.&lt;/li&gt;&lt;li&gt;Aggregates verification signals into a unified score (with variance-based penalties) to provide formal guarantees where possible and improved trustworthiness in reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vikash Singh', 'Darion Cassel', 'Nathaniel Weir', 'Nick Feng', 'Sam Bayless']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'neurosymbolic', 'theorem proving', 'iterative refinement', 'proof-guided generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20055</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text</title><link>https://arxiv.org/abs/2601.20006</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs large-scale corpora for detection: 1B-token human-authored corpus and 1.9B-token AI-generated corpus produced from multiple LLMs and domains.&lt;/li&gt;&lt;li&gt;Proposes two fine-tuning paradigms for detection: Per LLM and Per LLM-family fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluates detectors on a 100M-token benchmark covering 21 LLMs, achieving up to 99.6% token-level accuracy and outperforming open-source baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} Gromadzki', "Anna Wr\\'oblewska", 'Agnieszka Kaliska']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-text detection', 'forensics', 'dataset', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20006</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</title><link>https://arxiv.org/abs/2601.19967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Perturbation-Induced Linearization (PIL), a method to generate unlearnable (training-poisoning) perturbations using only linear surrogate models, greatly reducing computation versus deep-surrogate approaches.&lt;/li&gt;&lt;li&gt;Demonstrates PIL matches or outperforms prior surrogate-based unlearnable example methods and explains effectiveness via a mechanism: inducing linearization in deep models.&lt;/li&gt;&lt;li&gt;Analyzes properties of unlearnable examples under percentage-based partial perturbation and discusses implications for data protection against unauthorized model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinlin Liu', 'Wei Chen', 'Xiaojin Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable examples', 'data poisoning', 'adversarial perturbation', 'data protection', 'surrogate models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19967</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models</title><link>https://arxiv.org/abs/2601.19956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoxPrivacy, the first benchmark to evaluate interactional privacy in Speech Language Models (SLMs) across three tiers from direct secrecy commands to proactive privacy protection.&lt;/li&gt;&lt;li&gt;Evaluates nine SLMs on a 32-hour bilingual synthetic dataset and finds widespread failures (open-source ≈ random on conditional privacy; closed-source also weak on proactive inference).&lt;/li&gt;&lt;li&gt;Validates results on Real-VoxPrivacy (human-recorded subset) showing synthetic findings generalize to real speech.&lt;/li&gt;&lt;li&gt;Demonstrates a mitigation path: fine-tuning on a new 4,000-hour training set improves privacy-preserving behavior while retaining robustness; releases benchmark, dataset, and fine-tuned model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiang Wang', 'Hongyu Liu', 'Dekun Chen', 'Xueyao Zhang', 'Zhizheng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['interactional privacy', 'speech language models', 'privacy benchmark', 'defense/fine-tuning', 'dataset release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19956</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data</title><link>https://arxiv.org/abs/2601.19936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gap-K%, a method to detect whether text was included in LLM pretraining by measuring the log-probability gap between the model's top-1 predicted token and the actual target token.&lt;/li&gt;&lt;li&gt;Incorporates a sliding-window strategy to capture local token correlations and reduce token-level noise.&lt;/li&gt;&lt;li&gt;Grounded in analysis of pretraining optimization dynamics: large top-1 vs target gaps produce strong gradient signals that are penalized during training, making them useful detection cues.&lt;/li&gt;&lt;li&gt;Evaluated on WikiMIA and MIMIR benchmarks, demonstrating state-of-the-art performance across model sizes and input lengths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseo Kwak', 'Jaehyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining data detection', 'membership inference', 'privacy', 'model auditing', 'training data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19936</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text</title><link>https://arxiv.org/abs/2601.19913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LREAD, a rubric adapted from Korean national writing standards targeting micro-level artifacts (e.g., punctuation optionality, spacing, register shifts) to detect LLM-generated Korean text.&lt;/li&gt;&lt;li&gt;Runs a three-phase longitudinal blind protocol with Korean linguistics majors (intuition, rubric-scored with justifications, held-out mastery), boosting majority-vote accuracy from 60% to 100% and Fleiss' kappa from -0.09 to 0.82.&lt;/li&gt;&lt;li&gt;Finds calibrated humans use language-specific micro-diagnostics that state-of-the-art automated detectors miss, and releases the full rubric and a taxonomy of calibrated detection signatures as an interpretable complement to automated detectors in non-English settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated-text-detection', 'human-in-the-loop', 'forensic-linguistics', 'rubric-based-methods', 'non-English-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19913</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study</title><link>https://arxiv.org/abs/2601.19912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs the first instruction-level fault injection study of LLM inference to assess susceptibility to GPU soft errors.&lt;/li&gt;&lt;li&gt;Analyzes how model architecture, parameter scale, and task complexity affect reliability and failure modes.&lt;/li&gt;&lt;li&gt;Derives insights to inform design of fault-tolerance mechanisms for LLM deployment on GPUs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Chai', 'Zizhen Liu', 'Shuhuai Wang', 'Songwei Pei', 'Cheng Liu', 'Huawei Li', 'Shangguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hardware faults', 'fault injection', 'LLM reliability', 'robustness', 'fault tolerance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19912</guid><pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
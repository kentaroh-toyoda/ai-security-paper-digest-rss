<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Sat, 31 Jan 2026 22:25:51 +0000</lastBuildDate><item><title>The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</title><link>https://arxiv.org/abs/2601.00065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a supply-chain vulnerability in tokenizer transplant used for model composition: a single engineered 'breaker' token is inert in a donor model but becomes a high-salience malicious feature when transplanted into a base model.&lt;/li&gt;&lt;li&gt;Formulates the attack as a dual-objective optimization and implements it with a sparse solver to produce a token that reconstructs a Trojan/backdoor feature without training the target model.&lt;/li&gt;&lt;li&gt;Empirical results show the attack is training-free, evades outlier detection, and remains effective under defenses like fine-tuning and weight merging—demonstrating persistence in modular composition pipelines.&lt;/li&gt;&lt;li&gt;Highlights practical security risks in model composition workflows (weight merging, vocabulary expansion, tokenizer transplant) and provides code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoze Liu', 'Weichen Yu', 'Matt Fredrikson', 'Xiaoqian Wang', 'Jing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'supply-chain', 'tokenizer-transplant', 'model-composition', 'adversarial-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00065</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated, scalable red-teaming framework for evaluating safety of LLM-based search agents.&lt;/li&gt;&lt;li&gt;Generates 300 test cases across five risk categories (e.g., misinformation, prompt injection) and evaluates three agent scaffolds over 17 LLMs.&lt;/li&gt;&lt;li&gt;Finds substantial vulnerabilities (e.g., up to 90.5% attack success rate for GPT-4.1-mini) and shows common defenses like reminder prompting provide limited protection.&lt;/li&gt;&lt;li&gt;Provides a public codebase and test-suite to measure and improve search-agent safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Xun Chen', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'prompt injection', 'misinformation', 'search agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Guided Perturbation Sensitivity (GPS): Detecting Adversarial Text via Embedding Stability and Word Importance</title><link>https://arxiv.org/abs/2508.11667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Guided Perturbation Sensitivity (GPS), an attack-agnostic detector that ranks important words, masks top-k tokens, measures embedding sensitivity, and feeds these patterns to a BiLSTM detector to flag adversarial examples.&lt;/li&gt;&lt;li&gt;Shows adversarially perturbed words have disproportionately high masking sensitivity; GPS achieves &gt;85% detection accuracy across three datasets, three attack types, and two victim models without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates gradient-based word-ranking outperforms attention/hybrid/random selection (measured by NDCG) and that identification quality correlates with detection performance, with GPS generalizing to unseen datasets, attacks, and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bryan E. Tuck', 'Rakesh M. Verma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-detection', 'text-security', 'adversarial-defense', 'embedding-robustness', 'attack-agnostic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11667</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DRIFT: Detecting Representational Inconsistencies for Factual Truthfulness</title><link>https://arxiv.org/abs/2601.14210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRIFT, a lightweight probe that reads intermediate hidden states of LLMs to detect confidence/representational signals lost at the output layer, enabling hallucination detection.&lt;/li&gt;&lt;li&gt;Probe incurs &lt;0.1% computational overhead and can run fully in parallel with generation, allowing detection before answers are produced.&lt;/li&gt;&lt;li&gt;Introduces an LLM router that answers confident queries immediately and routes uncertain queries to stronger models, improving efficiency and reliability.&lt;/li&gt;&lt;li&gt;Reports SOTA AUROC on 10/12 settings across four QA benchmarks and three LLM families, with up to 13-point gains and robustness to dataset shift without retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Bhatnagar', 'Youran Sun', 'Chi Andrew Zhang', 'Yixin Wen', 'Haizhao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'model confidence probing', 'safety/robustness', 'model routing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14210</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new failure mode in Test-Time Scaling (TTS): reduced candidate diversity increases the likelihood of generating unsafe outputs.&lt;/li&gt;&lt;li&gt;Introduces RefDiv, a reference-guided diversity reduction protocol that acts as a diagnostic/adversarial attack against TTS pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates the effect across multiple open- and closed-source LLMs and across TTS strategies (MCTS, Best-of-N), and shows transferability between strategies and models.&lt;/li&gt;&lt;li&gt;Shows common safety guardrail classifiers (e.g., Llama-Guard) often fail to flag adversarial prompts generated by RefDiv, indicating existing defenses are insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'red teaming', 'safety evaluation', 'guardrail bypass', 'test-time scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups</title><link>https://arxiv.org/abs/2504.06160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM-generated attack narratives that target vulnerable mental health groups and quantifies emergent stigmatization.&lt;/li&gt;&lt;li&gt;Presents a network-based framework to study propagation of relative biases in generation chains and reports higher closeness centrality and dense clustering for mental health entities.&lt;/li&gt;&lt;li&gt;Applies a stigmatization framework to show increased labeling components for mental-health-related targets and highlights implications for mitigation of harmful discourse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rijul Magu', 'Arka Dutta', 'Sean Kim', 'Ashiqur R. KhudaBukhsh', 'Munmun De Choudhury']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content', 'bias in LLMs', 'safety/mitigation', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06160</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</title><link>https://arxiv.org/abs/2601.21963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates threats from LLM- and multimodal-generated misinformation and tracks changes in the threat landscape since 2024.&lt;/li&gt;&lt;li&gt;Introduces JudgeGPT (human-evaluation platform) and RogueGPT (controlled stimulus generation) as an experimental pipeline for studying perception and detection of AI-generated news.&lt;/li&gt;&lt;li&gt;Evaluates human detection capability and reports continued arms-race dynamics between generation and detection.&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies including LLM-based detection, inoculation (prebunking) approaches, and discussion of dual-use risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Loth', 'Martin Kappes', 'Marc-Oliver Pahl']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'adversarial content generation', 'detection/defense', 'human factors', 'benchmarking/tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21963</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities</title><link>https://arxiv.org/abs/2601.21702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes representation misdirection (RM), an LLM unlearning method that manipulates latent forget-representations to achieve forgetting.&lt;/li&gt;&lt;li&gt;Frames RM through the linear representation hypothesis, enabling linear operations on identified concept vectors within representation space.&lt;/li&gt;&lt;li&gt;Empirically shows that unlearning can elicit controllable side behaviors (e.g., truthfulness, sentiment, refusal) and enhance capabilities (e.g., in-context learning) in unlearned models.&lt;/li&gt;&lt;li&gt;Highlights dual implications: potential misuse/risk from induced behaviors and a possible mechanism to intentionally steer model capabilities and control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tien Dang', 'The-Hai Nguyen', 'Dinh Mai Phuong', 'Nguyen Minh Phuong', 'Hoang Thanh-Tung', 'Le-Minh Nguyen', 'Naoya Inoue']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'representation-misdirection', 'model-manipulation', 'security-risk', 'behavioral-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21702</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens (rather than documents) to remove undesired capabilities, demonstrated on a medical-capability removal proxy.&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and less harmful to benign capabilities than document-level filtering, and that effectiveness increases with model scale (very large slowdowns on the forget domain).&lt;/li&gt;&lt;li&gt;Introduces token labeling via sparse autoencoders and distilled classifiers to produce cheap, high-quality labels, and shows the approach is robust to noisy labels and compatible with later alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining defense', 'data filtering', 'capability control', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text</title><link>https://arxiv.org/abs/2601.21895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric interpretation of rewrite-based detection methods to explain their rationale and generalization behavior.&lt;/li&gt;&lt;li&gt;Proposes a novel rewrite-based detector that adaptively learns a distance function between original and rewritten text, with theoretical analysis showing adaptive distances outperform fixed ones.&lt;/li&gt;&lt;li&gt;Provides extensive empirical evaluation (100+ settings) showing large relative improvements (57.8%–80.6%) over the strongest baselines across target LLMs such as GPT, Claude, and Gemini.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Erhan Xu', 'Kai Ye', 'Ying Yang', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'Synthetic text detection', 'Rewrite-based detection', 'Defensive methods', 'Model forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21895</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning</title><link>https://arxiv.org/abs/2601.21682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIT, a continual unlearning framework for LLMs that combines data Filtering, Importance-aware updates, and Targeted layer attribution to mitigate catastrophic forgetting while processing large sequences of deletion requests.&lt;/li&gt;&lt;li&gt;Presents PCH benchmark (Personal information, Copyright, Harmful content) and two symmetric metrics—Forget Degree (F.D.) and Retain Utility (R.U.)—to evaluate forgetting effectiveness and utility preservation in sequential deletion scenarios.&lt;/li&gt;&lt;li&gt;Shows extensive experiments on four open-source LLMs with hundreds of deletion requests, reporting improved trade-offs between forgetting and retained utility on downstream tasks (MMLU, CommonsenseQA, GSM8K) and robustness to relearning and quantization recovery attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Minxin Du', 'Kun Fang', 'Zi Liang', 'Yaxin Xiao', 'Zhicong Huang', 'Cheng Hong', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['continual unlearning', 'data deletion / privacy', 'catastrophic forgetting', 'defense / robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21682</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self-Improving Pretraining: using post-trained models to pretrain better models</title><link>https://arxiv.org/abs/2601.21343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a pretraining method that streams documents and uses reinforcement learning to improve the next K generated tokens at each step, guided by a stronger post-trained model.&lt;/li&gt;&lt;li&gt;A critic model judges candidate generations (model rollouts, original suffix, rewritten suffix) for quality, safety, and factuality; early training relies more on rewritten suffixes, later on high-quality rollouts.&lt;/li&gt;&lt;li&gt;Aims to build safer, more factual models by addressing undesirable patterns during pretraining rather than only via post-hoc fine-tuning/alignment.&lt;/li&gt;&lt;li&gt;Reports large empirical gains: ~36.2% and ~18.5% relative improvements in factuality and safety, and up to 86.3% win-rate improvement in overall generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ellen Xiaoqing Tan', 'Shehzaad Dhuliawala', 'Jing Xu', 'Ping Yu', 'Sainbayar Sukhbaatar', 'Jason Weston', 'Olga Golovneva']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining defenses', 'safety and factuality', 'reinforcement learning from models', 'training-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21343</guid><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 31 Dec 2025 00:04:42 +0000</lastBuildDate><item><title>FedDyMem: Efficient Federated Learning with Dynamic Memory and Memory-Reduce for Unsupervised Image Anomaly Detection</title><link>https://arxiv.org/abs/2502.21012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedDyMem: a federated learning method for unsupervised image anomaly detection that shares knowledge via client-side dynamic memory banks instead of model parameters.&lt;/li&gt;&lt;li&gt;Clients use a memory generator and metric loss to stabilize normal-sample feature distributions; server aggregates reduced memories via k-means to build a global memory.&lt;/li&gt;&lt;li&gt;Proposes a memory-reduce (weighted averaging) technique to cut communication cost and shrink memory representation, which the authors claim reduces risk of data reconstruction; validated on multiple industrial and medical image datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Silin Chen', 'Andy Liu', 'Kangjian Di', 'Yichu Xu', 'Han-Jia Ye', 'Wenhan Luo', 'Ningmu Zou']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving', 'unsupervised-anomaly-detection', 'data-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.21012</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face</title><link>https://arxiv.org/abs/2512.21019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient video defense framework that perturbs 3D information acquisition to protect portrait videos from 3D-field personalized Talking Face Generation (TFG) methods while preserving high visual fidelity.&lt;/li&gt;&lt;li&gt;Introduces a similarity-guided parameter sharing mechanism for computational efficiency and a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates strong defense effectiveness, robustness to scaling and purification attacks, and a claimed 47x speedup over the fastest baseline; includes ablation studies and released code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui-qing Sun', 'Xingshan Yao', 'Tian Lan', 'Jia-Ling Shi', 'Chen-Hao Cui', 'Hui-Yang Zhao', 'Zhijing Wu', 'Chen Yang', 'Xian-Ling Mao']&lt;/li&gt;&lt;li&gt;Tags: ['video-defense', 'adversarial-perturbation', 'deepfake-defense', 'privacy-protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21019</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</title><link>https://arxiv.org/abs/2512.20113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal attention network that fuses radar temporal patterns and thermal spatial signatures for bridge deck delamination detection using temporal, spatial, and cross-modal attention.&lt;/li&gt;&lt;li&gt;Incorporates uncertainty quantification (Monte Carlo dropout and learned variance) to decompose epistemic and aleatoric uncertainty, improving calibration and enabling selective prediction by rejecting uncertain cases.&lt;/li&gt;&lt;li&gt;Demonstrates superior accuracy and AUC over single-modal and simple-fusion baselines on five bridge datasets; ablations show cross-modal attention and multi-head mechanisms improve performance and calibration.&lt;/li&gt;&lt;li&gt;Notes a limitation: attention mechanisms can collapse to the majority class under extreme class imbalance, indicating need for specialized techniques in such scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Moayedikia', 'Sattar Dorafshan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'calibration', 'multimodal-fusion', 'attention-networks', 'safety-critical-deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20113</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title><link>https://arxiv.org/abs/2512.14044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniDrive-R1, an end-to-end vision-language model framework for autonomous driving that interleaves perception and textual reasoning (iMCoT) to mitigate object hallucination.&lt;/li&gt;&lt;li&gt;Introduces a reinforcement-driven visual grounding capability trained with a two-stage RL pipeline and a Clip-GRPO algorithm that uses an annotation-free, process-based grounding reward to enforce cross-modal consistency.&lt;/li&gt;&lt;li&gt;Claims substantial improvements on DriveLMM-o1 benchmark vs baseline Qwen2.5VL-7B: reasoning score from 51.77% to 80.35% and final answer accuracy from 37.81% to 73.62%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenguo Zhang', 'Haohan Zheng', 'Yishen Wang', 'Le Xu', 'Tianchen Deng', 'Xuefeng Chen', 'Qu Chen', 'Bo Zhang', 'Wuxiong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'grounding', 'autonomous driving safety', 'reinforcement learning', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14044</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse</title><link>https://arxiv.org/abs/2511.13539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;BootOOD is a self-supervised OOD detection method that synthesizes pseudo-OOD features from in-distribution (ID) representations using simple transformations and leverages Neural Collapse properties of ID features.&lt;/li&gt;&lt;li&gt;It adds a lightweight auxiliary radius-based head that classifies by feature norm (encouraging OOD to have smaller norms), decoupling OOD detection from the primary classifier and easing detection when ID and OOD are semantically similar.&lt;/li&gt;&lt;li&gt;Empirical results on CIFAR-10, CIFAR-100, and ImageNet-200 show BootOOD outperforms prior post-hoc detectors, surpasses training-based methods without outlier exposure, and is competitive with outlier-exposure approaches while retaining or improving ID accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanchao Wang', 'Tian Qin', 'Eduardo Valle', 'Bruno Abrahao']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'self-supervised learning', 'neural collapse', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13539</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN</title><link>https://arxiv.org/abs/2509.22836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a conditional GAN / U-Net framework to synthesize adversarial patches where attacker specifies both input image and target class (fully controllable, targeted attacks).&lt;/li&gt;&lt;li&gt;Uses Grad-CAM-guided patch placement for semantic-aware localization to maximize effectiveness while preserving visual realism and stealth.&lt;/li&gt;&lt;li&gt;Reports very high attack success rates and target-class success (&gt;99%) across CNNs and vision transformers, claims black-box applicability and improved realism over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Alon Goldberg', 'Hodaya Cohen', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patch', 'targeted-attacks', 'robustness', 'black-box-attacks', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22836</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis</title><link>https://arxiv.org/abs/2509.16582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepSSIM, a self-supervised embedding that learns to predict SSIM similarity in embedding space to detect memorization in generative models.&lt;/li&gt;&lt;li&gt;Uses structure-preserving augmentations to capture anatomical features and enable alignment-free similarity estimation for medical images.&lt;/li&gt;&lt;li&gt;Evaluates DeepSSIM on synthetic brain MRI from a Latent Diffusion Model trained under memorization-prone conditions (IXI and CoRR datasets) and reports large F1 improvements (+52.03% over best prior method).&lt;/li&gt;&lt;li&gt;Provides code and data for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Scardace', 'Lemuel Puglisi', 'Francesco Guarnera', 'Sebastiano Battiato', 'Daniele Rav\\`i']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'generative-models', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16582</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WiSE-OD: Benchmarking Robustness in Infrared Object Detection</title><link>https://arxiv.org/abs/2507.18925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two cross-modality OOD benchmarks (LLVIP-C, FLIR-C) by applying corruptions to standard infrared (IR) object detection datasets.&lt;/li&gt;&lt;li&gt;Proposes WiSE-OD, a weight-space ensembling method with two variants (WiSE-OD_ZS and WiSE-OD_LP) that combines RGB pretrained and IR-fine-tuned weights to improve robustness without extra training or inference cost.&lt;/li&gt;&lt;li&gt;Evaluates WiSE-OD across multiple RGB-pretrained detectors, robust baselines, and a real-world OOD dataset (M3FD), showing improved robustness to synthetic and real-world distribution shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Heitor R. Medeiros', 'Atif Belal', 'Masih Aminbeidokhti', 'Eric Granger', 'Marco Pedersoli']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution-shift', 'infrared object detection', 'weight-space ensembling', 'OOD benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18925</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Vision-Language Model Reliability with Uncertainty-Guided Dropout Decoding</title><link>https://arxiv.org/abs/2412.06474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DROPOUT DECODING, an inference-time method that quantifies visual-token uncertainty and selectively masks uncertain tokens to improve LVLM decoding.&lt;/li&gt;&lt;li&gt;Uncertainty is computed by projecting visual tokens into text space and decomposing into aleatoric and epistemic components; the method focuses on epistemic uncertainty to capture perception-related errors.&lt;/li&gt;&lt;li&gt;Applies uncertainty-guided token dropout across an ensemble of masked contexts to reduce object hallucinations and improve reliability; evaluated on CHAIR, THRONE, and MMBench with reported reductions in hallucinations. Code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixiong Fang', 'Ziran Yang', 'Zhaorun Chen', 'Zhuokai Zhao', 'Jiawei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination-mitigation', 'safety-evaluation', 'inference-time-defenses', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06474</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ForgerySleuth: Empowering Multimodal Large Language Models for Image Manipulation Detection</title><link>https://arxiv.org/abs/2411.19466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForgerySleuth, a method that leverages multimodal LLMs to fuse clues and produce segmentation maps highlighting manipulated regions in images.&lt;/li&gt;&lt;li&gt;Introduces ForgeryAnalysis, a dataset built via a Chain-of-Clues prompting approach that includes analysis and reasoning text to enhance IMD (image manipulation detection) training.&lt;/li&gt;&lt;li&gt;Presents a data engine to scale pre-training data and reports experiments showing improved generalization, robustness, and explainability over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Sun', 'Haoran Jiang', 'Haoran Chen', 'Yixin Cao', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['image-manipulation-detection', 'multimodal-LLMs', 'robustness', 'forensics', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.19466</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Instruction-Following Evaluation of Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23572</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that large vision-language models (LVLMs) often lose the instruction-following ability present in their base LLMs after visual instruction fine-tuning.&lt;/li&gt;&lt;li&gt;Constructs training datasets that vary whether the output format is explicitly specified and quantitatively evaluates effects on instruction-following.&lt;/li&gt;&lt;li&gt;Finds that including explicit output-format instructions during (visual) instruction tuning improves models' adherence to task instructions.&lt;/li&gt;&lt;li&gt;Suggests a mitigation: include samples with explicit output-format instructions in fine-tuning datasets to reduce decline in instruction-following.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiki Shiono', 'Shumpei Miyawaki', 'Ryota Tanaka', 'Jun Suzuki']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'instruction-following', 'safety-evaluation', 'vision-language models', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23572</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</title><link>https://arxiv.org/abs/2512.23343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey linking cognitive neuroscience concepts of memory to memory architectures and management in LLM-driven autonomous agents.&lt;/li&gt;&lt;li&gt;Comparative analysis of memory taxonomy, storage mechanisms, lifecycle management, and benchmarks for evaluating agent memory.&lt;/li&gt;&lt;li&gt;Includes a dedicated discussion on memory security (attacks and defenses) and outlines future directions such as multimodal memory and skill acquisition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiafeng Liang', 'Hao Li', 'Chang Li', 'Jiaqi Zhou', 'Shixin Jiang', 'Zekun Wang', 'Changkai Ji', 'Zhihao Zhu', 'Runxuan Liu', 'Tao Ren', 'Jinlan Fu', 'See-Kiong Ng', 'Xia Liang', 'Ming Liu', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['agent-memory', 'memory-security', 'LLM-agents', 'adversarial-attacks', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23343</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models</title><link>https://arxiv.org/abs/2512.22539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLA-Arena, a comprehensive benchmark and toolchain for evaluating Vision-Language-Action (VLA) models across Task Structure, Language Command, and Visual Observation axes.&lt;/li&gt;&lt;li&gt;Provides 170 tasks grouped into Safety, Distractor, Extrapolation, and Long Horizon dimensions with multi-level difficulty and orthogonal language/visual perturbations to measure robustness and generalization.&lt;/li&gt;&lt;li&gt;Finds critical limitations in SOTA VLAs (memorization over generalization, asymmetric robustness, poor safety constraint handling, and failure to compose skills for long horizons) and releases datasets, models, and a leaderboard.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Borong Zhang', 'Jiahao Li', 'Jiachen Shen', 'Yishuai Cai', 'Yuhao Zhang', 'Yuanpei Chen', 'Juntao Dai', 'Jiaming Ji', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmarking', 'robustness', 'vision-language-action', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22539</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</title><link>https://arxiv.org/abs/2512.22322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SmartSnap, a paradigm for proactive, in-situ self-verification where agents collect minimal decisive snapshot evidences to prove task completion rather than relying on post-hoc verifiers over verbose trajectories.&lt;/li&gt;&lt;li&gt;Introduces Self-Verifying Agent with dual missions (solve task + seek evidence) guided by 3C Principles: Completeness, Conciseness, Creativity, and uses those snapshots as sole input to an LLM-as-a-Judge verifier.&lt;/li&gt;&lt;li&gt;Demonstrates improved scalability and reliability on mobile GUI tasks across model sizes, reporting up to ~26% and ~16.7% gains for 8B and 30B models versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaofei Cai', 'Yulei Qin', 'Haojia Lin', 'Zihan Xu', 'Gang Li', 'Yuchen Shi', 'Zongyi Li', 'Yong Mao', 'Siqi Cai', 'Xiaoyu Tan', 'Yitao Liang', 'Ke Li', 'Xing Sun']&lt;/li&gt;&lt;li&gt;Tags: ['agent verification', 'self-verifying agents', 'agentic RL', 'task verification', 'LLM-as-a-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22322</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</title><link>https://arxiv.org/abs/2512.22170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SoliReward, a framework to train video reward models (RMs) that reduces labeling noise and mitigates reward hacking during post-training alignment of video generation models.&lt;/li&gt;&lt;li&gt;Data strategy: collects single-item binary annotations and forms preference pairs via a cross-prompt pairing mechanism to improve label quality and cost-efficiency.&lt;/li&gt;&lt;li&gt;Architecture and loss: introduces a Hierarchical Progressive Query Attention module for better feature aggregation and a modified Bradley-Terry (BT) loss that models win-tie outcomes and regularizes positive-score distributions to reduce over-focus on top-scoring samples.&lt;/li&gt;&lt;li&gt;Empirical validation: demonstrates improvements on benchmarks for physical plausibility, subject deformity, and semantic alignment, and shows that RMs trained with SoliReward yield better outcomes when used for post-training video generation alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiesong Lian', 'Ruizhe Zhong', 'Zixiang Zhou', 'Xiaoyue Mi', 'Yixue Hao', 'Yuan Zhou', 'Qinglin Lu', 'Long Hu', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['Reward modeling', 'Alignment', 'Reward hacking', 'Annotation noise', 'Video generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22170</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memorization in 3D Shape Generation: An Empirical Study</title><link>https://arxiv.org/abs/2512.23628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an evaluation framework to quantify memorization in 3D generative models and applies it to existing methods.&lt;/li&gt;&lt;li&gt;Empirically analyzes factors affecting memorization (data modality, data diversity, conditioning granularity) and modeling choices (guidance scale, Vecset length, rotation augmentation).&lt;/li&gt;&lt;li&gt;Identifies simple mitigation strategies (longer Vecsets, rotation augmentation) that reduce memorization without degrading generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Pu', 'Boya Zeng', 'Kaichen Zhou', 'Mengyu Wang', 'Zhuang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data leakage/privacy', 'generative models', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23628</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ProGuard: Towards Proactive Multimodal Safeguard</title><link>https://arxiv.org/abs/2512.23573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) multimodal safety risks without modifying target models.&lt;/li&gt;&lt;li&gt;Constructs a modality-balanced dataset of 87K samples annotated with binary safety labels and hierarchical risk categories across text, image, and text-image inputs.&lt;/li&gt;&lt;li&gt;Trains a vision-language base model purely via reinforcement learning and adds an OOD safety category inference task with a synonym-bank-based similarity reward to encourage concise descriptions for unseen unsafe categories.&lt;/li&gt;&lt;li&gt;Reports comparable binary safety classification to closed-source models, improved unsafe-content categorization over open-source guards, and large gains in OOD risk detection (+52.6%) and OOD risk description (+64.8%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaohan Yu', 'Lijun Li', 'Chenyang Si', 'Lu Sheng', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'proactive moderation', 'OOD detection', 'dataset and benchmarking', 'reinforcement learning for safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23573</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation</title><link>https://arxiv.org/abs/2512.23546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PurifyGen, a plug-and-play, training-free method for safe text-to-image generation that preserves original model weights.&lt;/li&gt;&lt;li&gt;Uses token-level risk scoring via complementary semantic distance to toxic and clean concept embeddings, then applies a dual-space embedding transformation: projecting toxic-aligned components into the toxic-concept null space and aligning into the clean-concept range space.&lt;/li&gt;&lt;li&gt;Selectively replaces only risky token embeddings to minimize disruption, claims theoretical grounding, generalization to unseen prompts/models, and strong empirical reductions in unsafe outputs across five datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongsheng Cao', 'Yangfan He', 'Anran Liu', 'Jun Xie', 'Feng Chen', 'Zepeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['T2I safety', 'prompt purification', 'embedding-space defense', 'unsafe content mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23546</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoFi-Dec, a training-free decoding framework that reduces LVLM hallucinations by combining coarse-to-fine visual conditioning with generative self-feedback.&lt;/li&gt;&lt;li&gt;Generates coarse- and fine-grained textual intermediates, converts them to synthetic images via a text-to-image model to create multi-level visual hypotheses, and fuses predictions with a Wasserstein-based mechanism.&lt;/li&gt;&lt;li&gt;Claims substantial reductions in entity- and semantic-level hallucinations across six hallucination-focused benchmarks; model-agnostic and requires no additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongsheng Cao', 'Yangfan He', 'Anran Liu', 'Jun Xie', 'Feng Chen', 'Zepeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'decoding strategies', 'visual grounding', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23453</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Integrating Uncertainty for Domain-Agnostic Segmentation</title><link>https://arxiv.org/abs/2512.23427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UncertSAM, an 8-dataset benchmark to stress-test SAM-like segmentation models under challenging conditions (shadows, transparency, camouflage).&lt;/li&gt;&lt;li&gt;Evaluates several lightweight, post-hoc uncertainty estimation methods and finds that a last-layer Laplace approximation produces uncertainty estimates that correlate with segmentation errors.&lt;/li&gt;&lt;li&gt;Explores a preliminary uncertainty-guided prediction refinement step; refinement benefits are limited but indicate potential for improving domain-agnostic robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jesse Brouwers', 'Xiaoyan Xing', 'Alexander Timans']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'robustness', 'segmentation', 'benchmark', 'foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23427</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision</title><link>https://arxiv.org/abs/2512.23426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Direct Diffusion Score Preference Optimization (DDSPO), which derives per-timestep supervision from winning/losing policies to provide dense transition-level signals across the denoising trajectory.&lt;/li&gt;&lt;li&gt;Avoids costly human-labeled preference datasets by automatically generating preference signals using a pretrained reference model contrasting outputs conditioned on original prompts versus semantically degraded variants.&lt;/li&gt;&lt;li&gt;Operates in score space (diffusion model denoising steps) rather than relying on final-sample rewards or explicit reward modeling, and shows improved text–image alignment and visual quality with less supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dohyun Kim', 'Seungwoo Lyu', 'Seung Wook Kim', 'Paul Hongsuck Seo']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'preference learning / alignment', 'score-based supervision', 'self-supervised preference generation', 'text-to-image (multimodal)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23426</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection &amp; Localization</title><link>https://arxiv.org/abs/2512.23374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NeXT-IMDL, a large-scale diagnostic benchmark for image manipulation detection and localization (IMDL) targeting AI-generated content (AIGC).&lt;/li&gt;&lt;li&gt;Defines four axes of variation (editing models, manipulation types, content semantics, forgery granularity) and five cross-dimension evaluation protocols to probe generalization limits.&lt;/li&gt;&lt;li&gt;Evaluates 11 representative detectors, revealing systemic failures and significant performance drops under realistic cross-dimension tests.&lt;/li&gt;&lt;li&gt;Provides a diagnostic toolkit and dataset intended to drive development of more robust, generalizable IMDL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifei Li', 'Haoyuan He', 'Yu Zheng', 'Bingyao Yu', 'Wenzhao Zheng', 'Lei Chen', 'Jie Zhou', 'Jiwen Lu']&lt;/li&gt;&lt;li&gt;Tags: ['image manipulation detection', 'deepfake detection', 'benchmarking', 'robustness', 'AIGC']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23374</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images</title><link>https://arxiv.org/abs/2512.23304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares an open-source specialized agent (MedGemma-4b-it, LoRA fine-tuned) vs proprietary GPT-4 on zero-shot medical image disease classification across six diseases.&lt;/li&gt;&lt;li&gt;MedGemma achieved higher mean accuracy (80.37%) and greater sensitivity on high-stakes tasks (e.g., cancer, pneumonia) than untuned GPT-4.&lt;/li&gt;&lt;li&gt;Authors argue domain-specific fine-tuning is essential to reduce hallucinations in clinical deployment and provide quantitative analyses (confusion matrices, classification reports).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Sazzadul Islam Prottasha', 'Nabil Walid Rafi']&lt;/li&gt;&lt;li&gt;Tags: ['medical imaging', 'multimodal LLM', 'fine-tuning (LoRA)', 'safety (hallucination reduction)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23304</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</title><link>https://arxiv.org/abs/2512.23215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVOID, a simulated dataset focused on unexpected small road obstacles captured under varying adverse visual conditions (weather, lighting).&lt;/li&gt;&lt;li&gt;Provides multimodal annotations per frame: RGB images, semantic and depth maps, raw and semantic LiDAR, and waypoints to support multiple perception tasks.&lt;/li&gt;&lt;li&gt;Includes benchmarks on real-time obstacle-detection networks and ablation studies using a multi-task network for segmentation, depth, and waypoint prediction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jongoh Jeong', 'Taek-Jin Song', 'Jong-Hwan Kim', 'Kuk-Jin Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'dataset', 'obstacle-detection', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23215</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation</title><link>https://arxiv.org/abs/2512.23169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes REVEALER, a framework for element-level evaluation of text-to-image alignment using a 'grounding–reasoning–conclusion' paradigm to produce interpretable alignment judgments.&lt;/li&gt;&lt;li&gt;Uses Multimodal LLMs to explicitly localize semantic elements and derives alignment decisions, optimized with Group Relative Policy Optimization (GRPO) and a composite reward combining format, grounding accuracy, and alignment fidelity.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on multiple benchmarks (EvalMuse-40K, RichHF, MHaluBench, GenAI-Bench) and reports better inference efficiency than iterative visual reasoning baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fulin Shi', 'Wenyi Xiao', 'Bin Chen', 'Liang Din', 'Leilei Gan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment-evaluation', 'multimodal-llms', 'text-to-image', 'visual-grounding', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23169</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models</title><link>https://arxiv.org/abs/2512.22877</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces M-ErasureBench, a multimodal benchmark evaluating concept erasure across text prompts, learned embeddings, and inverted latents (with white-box and black-box scenarios).&lt;/li&gt;&lt;li&gt;Shows existing concept-erasure methods perform well on text prompts but largely fail on learned embeddings and inverted latents (CRR &gt; 90% in white-box latent inversion).&lt;/li&gt;&lt;li&gt;Proposes IRECE, an inference-time, plug-and-play defense that localizes concepts via cross-attention and perturbs associated latents, reducing CRR by up to 40% while preserving image quality.&lt;/li&gt;&lt;li&gt;Provides the first comprehensive benchmark and a practical defense for improving robustness of protective generative models against multimodal attack surfaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ju-Hsuan Weng', 'Jia-Wei Liao', 'Cheng-Fu Chou', 'Jun-Cheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['Concept erasure', 'Diffusion model security', 'Latent inversion', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22877</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments</title><link>https://arxiv.org/abs/2512.22867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MUSON, a multimodal dataset for short-horizon socially compliant navigation across indoor and outdoor campus scenes with balanced discrete action labels.&lt;/li&gt;&lt;li&gt;Annotations follow a structured five-step Chain-of-Thought: perception, prediction, reasoning, action, and explanation, explicitly modeling static physical constraints.&lt;/li&gt;&lt;li&gt;Benchmarks several small vision-language models on decision accuracy (Qwen2.5-VL-3B best at 0.8625), positioning MUSON as a reusable benchmark for safe navigation behavior learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuonan Liu', 'Xinyu Zhang', 'Zishuo Wang', 'Tomohito Kawabata', 'Xuesu Xiao', 'Ling Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['social navigation', 'robotic safety', 'multimodal dataset', 'benchmarking', 'chain-of-thought reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22867</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution</title><link>https://arxiv.org/abs/2512.22647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FinPercep-RM, an encoder–decoder reward model that outputs a global IQA score plus a Perceptual Degradation Map to localize fine-grained artifacts in image super-resolution outputs.&lt;/li&gt;&lt;li&gt;Presents the FGR-30k dataset with diverse subtle distortions from real-world SR models to train the fine-grained reward model.&lt;/li&gt;&lt;li&gt;Identifies reward hacking arising from coarse global IQA rewards and proposes a Co-evolutionary Curriculum Learning (CCL) scheme where the reward model and ISR generator co-evolve from easy (global reward) to hard (fine-grained reward) to stabilize RL training.&lt;/li&gt;&lt;li&gt;Reports improvements in both global quality and local realism for RLHF-based ISR methods and claims reduced reward-hacking behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yidi Liu', 'Zihao Fan', 'Jie Huang', 'Jie Xiao', 'Dong Li', 'Wenlong Zhang', 'Lei Bai', 'Xueyang Fu', 'Zheng-Jun Zha']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward hacking', 'reward model', 'alignment', 'curriculum learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22647</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains</title><link>https://arxiv.org/abs/2512.22545</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SR-MCR, a label-free self-reward framework that uses five intrinsic process-level cues (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency) to reward intermediate reasoning steps.&lt;/li&gt;&lt;li&gt;Implements a critic-free GRPO objective with a confidence-aware cooling mechanism to stabilize training and reduce trivial/overconfident generations.&lt;/li&gt;&lt;li&gt;Demonstrates improved answer accuracy and reasoning coherence on visual benchmarks using Qwen2.5-VL; ablations show each reward term and cooling contribute independently.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jesen Zhang', 'Ningyuan Liu', 'Kaitong Cai', 'Sidi Liu', 'Jing Yang', 'Ziliang Chen', 'Xiaofei Sun', 'Keze Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reasoning coherence', 'multimodal LLMs', 'self-reward / RL-from-model', 'visual grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22545</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency</title><link>https://arxiv.org/abs/2512.22275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Bones &amp; Joints (B&amp;J) Benchmark: 1,245 real-world orthopedic/sports-medicine cases spanning 7 tasks that emulate clinical reasoning (knowledge recall, text/image interpretation, diagnosis, treatment planning, rationale).&lt;/li&gt;&lt;li&gt;Evaluates 11 vision-language models and 6 LLMs, finding high performance on structured multiple-choice but substantially worse performance (≈60% or less) on open-ended multimodal tasks.&lt;/li&gt;&lt;li&gt;Identifies critical safety issues: VLMs often fail to interpret medical images correctly and produce text-driven hallucinations that ignore contradictory visual evidence; medically fine-tuned models show no consistent advantage.&lt;/li&gt;&lt;li&gt;Concludes current models are not clinically competent for complex multimodal reasoning and recommends limiting deployment to supportive, text-only roles until multimodal integration improves.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingyu Wang', 'Zimu Yuan', 'Jiajun Liu', 'Shanggui Liu', 'Nan Zhou', 'Tianxing Xu', 'Di Huang', 'Dong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'vision-language-models', 'hallucination', 'clinical-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22275</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unbiased Visual Reasoning with Controlled Visual Inputs</title><link>https://arxiv.org/abs/2512.22183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VISTA, a modular framework that decouples perception (frozen VLM sensor answering constrained perception queries) from reasoning (text-only LLM that plans queries and aggregates visual facts).&lt;/li&gt;&lt;li&gt;Imposes an explicit information bottleneck and trains the reasoner with reinforcement learning (GRPO) to produce unbiased visual reasoning traces.&lt;/li&gt;&lt;li&gt;Achieves substantial improvements in robustness to real-world spurious correlations (SpuriVerse) while remaining competitive on balanced benchmarks, and transfers across unseen VLM sensors.&lt;/li&gt;&lt;li&gt;Demonstrates ability to recognize and recover from perception failures and produces more neutral, explicitly grounded reasoning traces compared to end-to-end VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaonan Li', 'Shijie Lu', 'Fei Wang', 'Jacob Dineen', 'Xiao Ye', 'Zhikun Xu', 'Siyi Liu', 'Young Min Cho', 'Bangzheng Li', 'Daniel Chang', 'Kenny Nguyen', 'Qizheng Yang', 'Muhao Chen', 'Ben Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'vision-language models', 'reinforcement learning', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22183</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Context: Large Language Models Failure to Grasp Users Intent</title><link>https://arxiv.org/abs/2512.21110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows they can be systematically bypassed via techniques like emotional framing, progressive revelation, and academic justification.&lt;/li&gt;&lt;li&gt;Finds that reasoning-enabled configurations often increase factual precision but fail to interrogate or detect malicious user intent, thereby amplifying exploit effectiveness.&lt;/li&gt;&lt;li&gt;Identifies Claude Opus 4.1 as a partial exception that sometimes prioritized intent detection over information provision, and argues for shifting safety design toward core contextual understanding and intent recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed M. Hussain', 'Salahuddin Salahuddin', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'jailbreaking', 'intent recognition', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21110</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title><link>https://arxiv.org/abs/2510.16830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a practical protocol and system to produce succinct zero-knowledge proofs that a released LLM was produced from a declared initialization, training program, and auditable dataset commitment.&lt;/li&gt;&lt;li&gt;Combines data provenance commitments, a verifiable sampler (supporting public replayable and private index-hiding selection), proof-friendly optimizer/update circuits for parameter-efficient fine-tuning, and recursive aggregation for fast verification.&lt;/li&gt;&lt;li&gt;Demonstrates preserved utility under tight error budgets, enforcement of policy quotas with zero violations, minimal index leakage for private sampling, and compatibility with federated setups and probabilistic audits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Daniel Borg', 'Arta Berisha', 'Amina Rahimova', 'Andrej Novak', 'Mila Petrov']&lt;/li&gt;&lt;li&gt;Tags: ['zero-knowledge proofs', 'model provenance', 'training integrity', 'verifiable computation', 'federated learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16830</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU), which models continuous unlearning as rotations in a learned 'cognitive rotation space' and uses rotational salience weights to quantify/control unlearning degree.&lt;/li&gt;&lt;li&gt;Introduces a skew-symmetric loss to construct the rotation space and an orthogonal rotation-axes regularization to make successive unlearning requests minimally interfering, aiming to prevent cumulative catastrophic utility loss.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art performance on multiple datasets for unlearning without relying on a retained dataset, improving utility preservation during repeated unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Chenghao Xu', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM safety', 'privacy', 'continual unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy XAI Evaluation</title><link>https://arxiv.org/abs/2501.01516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial attacks that alter text-based XAI explanations without changing model outputs and argues current IR metrics mischaracterize attack impact.&lt;/li&gt;&lt;li&gt;Proposes synonymity-weighted similarity: augmenting evaluation metrics with semantic similarity of perturbed words to better reflect true explanation changes.&lt;/li&gt;&lt;li&gt;Claims the method yields more accurate vulnerability assessments, reducing overestimation of attack success and improving trustworthiness evaluation of XAI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Burger']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'XAI robustness', 'evaluation metrics', 'semantic similarity', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01516</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdvPrefix: An Objective for Nuanced LLM Jailbreaks</title><link>https://arxiv.org/abs/2412.10321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvPrefix, a plug-and-play prefix-forcing objective that selects model-dependent prefixes by optimizing for high prefilling attack success and low negative log-likelihood.&lt;/li&gt;&lt;li&gt;Integrates into existing jailbreak attacks to produce more nuanced and realistic jailbroken outputs and overcomes rigid-format limitations of common prefix objectives.&lt;/li&gt;&lt;li&gt;Empirical results show large improvements (e.g., raising nuanced attack success on Llama-3 from 14% to 80% when replacing GCG's default prefixes), demonstrating safety alignment brittleness to new prefixes.&lt;/li&gt;&lt;li&gt;Provides code and selected prefixes to enable replication and further red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Zhu', 'Brandon Amos', 'Yuandong Tian', 'Chuan Guo', 'Ivan Evtimov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation', 'alignment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10321</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection attack against LLM-integrated Applications</title><link>https://arxiv.org/abs/2306.05499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Exploratory analysis of commercial LLM-integrated applications, identifying limitations of existing prompt-injection attacks in practice.&lt;/li&gt;&lt;li&gt;Proposes HouYi, a black-box prompt-injection method composed of a pre-constructed prompt, a context-partitioning injection prompt, and a malicious payload.&lt;/li&gt;&lt;li&gt;Empirical evaluation on 36 real applications (31 found vulnerable), demonstrating severe outcomes (arbitrary LLM usage, prompt theft), vendor disclosures, and proposed mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Gelei Deng', 'Yuekang Li', 'Kailong Wang', 'Zihao Wang', 'Xiaofeng Wang', 'Tianwei Zhang', 'Yepang Liu', 'Haoyu Wang', 'Yan Zheng', 'Leo Yu Zhang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'adversarial prompting', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.05499</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden/implicit conversational escalation in LLM-driven dialogues.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a conversation rather than relying on external toxicity classifiers or static clinical rubrics.&lt;/li&gt;&lt;li&gt;Targets subtle, cumulative affective harms (implicit harm) that conventional toxicity filters miss, aiming to provide dynamic guardrails for chatbots acting as emotional companions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['conversational-safety', 'affective-harm', 'logit-based-detection', 'LLM-safety', 'real-time-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title><link>https://arxiv.org/abs/2510.21118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an intermediate 'Out-Dependent' category to reduce annotation ambiguity when external knowledge is needed to verify generated content.&lt;/li&gt;&lt;li&gt;Introduces VeriGray, a new benchmark for unfaithfulness detection in summarization, and provides dataset statistics (e.g., ~6% hallucinations for GPT-5; ~9% sentences classified as Out-Dependent across models).&lt;/li&gt;&lt;li&gt;Evaluates multiple baseline methods and shows the benchmark is challenging, indicating substantial room for improvement in faithfulness detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Ding', 'Lvzhou Luo', 'Yixuan Cao', 'Ping Luo']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination detection', 'benchmarking', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21118</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal</title><link>https://arxiv.org/abs/2509.01455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;UniCR fuses heterogeneous uncertainty evidence (sequence likelihoods, self-consistency dispersion, retrieval compatibility, tool/verifier feedback) into a calibrated probability of correctness using a lightweight calibration head with temperature scaling and proper scoring, and supports API-only (black-box) models.&lt;/li&gt;&lt;li&gt;It enforces user-specified error budgets via distribution-free conformal risk control to decide principled refusals, and for long-form generation supervises confidence on atomic factuality scores from retrieved evidence to reduce confident hallucinations.&lt;/li&gt;&lt;li&gt;Empirical results across short-form QA, code generation (with execution tests), and retrieval-augmented long-form QA show improved calibration, lower area under the risk-coverage curve, and higher coverage at fixed risk compared to entropy/logit thresholds and other baselines; analyses identify evidence contradiction, semantic dispersion, and tool inconsistency as main abstention drivers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Markus Oehri', 'Giulia Conti', 'Kaviraj Pather', 'Alexandre Rossi', 'Laia Serra', 'Adrian Parody', 'Rogvi Johannesen', 'Aviaja Petersen', 'Arben Krasniqi']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'refusal/abstention', 'conformal risk control', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01455</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2509.01412</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Vis-CoT, a human-in-the-loop system that converts LLM chain-of-thought (CoT) text into an interactive reasoning graph for visualization and intervention.&lt;/li&gt;&lt;li&gt;Enables users to prune incorrect reasoning paths and graft new premises, shifting interaction from passive observation to active collaboration to steer model outputs.&lt;/li&gt;&lt;li&gt;Reports up to 24 percentage point improvement in final-answer accuracy on GSM8K and StrategyQA and shows increased perceived usability and trust in a user study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaviraj Pather', 'Elena Hadjigeorgiou', 'Arben Krasniqi', 'Claire Schmit', 'Irina Rusu', 'Marc Pons', 'Kabir Khan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Human-in-the-loop', 'Chain-of-thought', 'Interpretability', 'Interactive debugging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01412</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases</title><link>https://arxiv.org/abs/2508.12411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a 'cultural gene' concept: LLMs inherit value orientations from their training corpora and proposes a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI).&lt;/li&gt;&lt;li&gt;Compares GPT-4 and ERNIE Bot using standardized zero-shot prompts with human annotation; reports statistically significant differences in IDV and PDI scores (GPT-4 more individualistic/low power distance; ERNIE Bot more collectivistic/high power distance).&lt;/li&gt;&lt;li&gt;Introduces a Cultural Alignment Index (CAI) against Hofstede national scores showing GPT-4 aligns with the USA and ERNIE Bot with China; includes qualitative analyses illustrating how cultural orientations affect reasoning and dilemma resolution.&lt;/li&gt;&lt;li&gt;Argues for culturally aware evaluation and deployment to mitigate algorithmic cultural hegemony and bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emanuel Z. Fenech-Borg', 'Tilen P. Meznaric-Kos', 'Milica D. Lekovic-Bojovic', 'Arni J. Hentze-Djurhuus']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'alignment', 'cultural bias', 'evaluation', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12411</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMEval-Fair: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2508.05452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLMEval-Fair, a dynamic evaluation framework sampling unseen test sets from a proprietary bank of 220k graduate-level questions to avoid static benchmark contamination and overfitting.&lt;/li&gt;&lt;li&gt;Implements contamination-resistant data curation, a novel anti-cheating architecture, and an LLM-as-a-judge pipeline calibrated to 90% agreement with human experts, plus a relative ranking system for fair comparisons.&lt;/li&gt;&lt;li&gt;Presents a 30-month longitudinal study of ~60 models showing a performance ceiling on memorization and uncovering contamination vulnerabilities that static benchmarks miss.&lt;/li&gt;&lt;li&gt;Demonstrates improved ranking stability and robustness, advocating dynamic evaluation as a more trustworthy methodology than static leaderboards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Zhang', 'Yujiong Shen', 'Jingyi Deng', 'Yuhui Wang', 'Huayu Sha', 'Kexin Tan', 'Qiyuan Peng', 'Yue Zhang', 'Junzhe Wang', 'Shichun Liu', 'Yueyuan Huang', 'Jingqi Tong', 'Changhao Jiang', 'Yilong Wu', 'Zhihao Zhang', 'Mingqi Wu', 'Mingxu Chai', 'Zhiheng Xi', 'Shihan Dou', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['dynamic evaluation', 'data contamination', 'anti-cheating', 'benchmark robustness', 'automated evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05452</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Large Language Model Safety with Contrastive Representation Learning</title><link>https://arxiv.org/abs/2506.11938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes framing LLM defense as a contrastive representation learning problem using a triplet loss and adversarial hard negative mining.&lt;/li&gt;&lt;li&gt;Finetunes models to separate benign and harmful representations, aiming to improve robustness to input-level and embedding-space attacks.&lt;/li&gt;&lt;li&gt;Reports improved robustness over prior representation-engineering defenses without degrading standard performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Simko', 'Mrinmaya Sachan', 'Bernhard Sch\\"olkopf', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'adversarial robustness', 'contrastive learning', 'representation engineering', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11938</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs</title><link>https://arxiv.org/abs/2505.10013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DIF (Demographic Implicit Fairness), a benchmark to quantify implicit demographic bias in LLMs using existing logic/math QA datasets augmented with sociodemographic personas.&lt;/li&gt;&lt;li&gt;Combines DIF scores with a null-model statistical robustness check to validate whether observed behavior reflects implicit bias beyond chance.&lt;/li&gt;&lt;li&gt;Finds an inverse relationship between QA accuracy and implicit bias, suggesting higher-performing models may still exhibit stronger demographic sensitivity in certain contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lake Yin', 'Fan Huang']&lt;/li&gt;&lt;li&gt;Tags: ['bias/fairness', 'LLM evaluation', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10013</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview</title><link>https://arxiv.org/abs/2505.01967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Social Worldview Taxonomy (SWT) mapping Cultural Theory worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into quantifiable sub-dimensions for LLMs.&lt;/li&gt;&lt;li&gt;Analyzes 28 diverse LLMs to identify distinct socio-cognitive profiles and latent cognitive flexibility.&lt;/li&gt;&lt;li&gt;Demonstrates that explicit social cues systematically modulate models' worldviews, using principles from Social Referencing Theory.&lt;/li&gt;&lt;li&gt;Argues that understanding these latent worldviews can improve transparency, interpretability, and socially responsible AI development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiatao Li', 'Yanheng Li', 'Xiaojun Wan']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Bias', 'Model evaluation', 'Interpretability', 'Social behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01967</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SelfCheck-Eval: A Multi-Module Framework for Zero-Resource Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2502.01812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIME Math Hallucination dataset: a benchmark for evaluating mathematical reasoning hallucinations.&lt;/li&gt;&lt;li&gt;Proposes SelfCheck-Eval, an LLM-agnostic, black-box multi-module hallucination detection framework (Semantic, Specialised Detection, Contextual Consistency) for zero-resource settings.&lt;/li&gt;&lt;li&gt;Evaluates detectors across domains and training paradigms, finding existing methods work on biographical content but fail substantially on mathematical reasoning, motivating specialised black-box approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diyana Muhammed', 'Giusy Giulia Tuccari', 'Gollam Rabby', 'S\\"oren Auer', 'Sahar Vahdati']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'black-box evaluation', 'benchmark/dataset', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01812</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models</title><link>https://arxiv.org/abs/2502.01386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses topic-oriented adversarial opinion manipulation attacks against Retrieval-Augmented Generation (RAG) systems, focusing on multi-query, topic-level influence rather than single-query factual attacks.&lt;/li&gt;&lt;li&gt;Proposes Topic-FlipRAG, a two-stage pipeline combining adversarial ranking attacks on retrievers with semantic-level perturbations generated by LLM reasoning to poison relevant knowledge and shift model outputs.&lt;/li&gt;&lt;li&gt;Experiments demonstrate effective opinion shifts on targeted topics and show that current mitigation methods are insufficient, highlighting security gaps in RAG deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyang Gong', 'Zhuo Chen', 'Jiawei Liu', 'Miaokun Chen', 'Fengchang Yu', 'Wei Lu', 'Xiaofeng Wang', 'Xiaozhong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation', 'Data poisoning', 'Adversarial ranking attacks', 'Opinion manipulation', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01386</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</title><link>https://arxiv.org/abs/2512.23457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hindsight instruction Replay (HiR), an RL framework that replays failed responses as successes by selecting satisfied constraints in hindsight and rewriting failures accordingly.&lt;/li&gt;&lt;li&gt;Frames the training objective as dual-preference learning at both the instruction- and response-level, enabling optimization with only a binary reward signal.&lt;/li&gt;&lt;li&gt;Claims improved sample efficiency for complex instruction-following tasks and reports empirical gains across multiple tasks while reducing computational budget; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kongcheng Zhang', 'Qi Yao', 'Shunyu Liu', 'Wenjian Zhang', 'Min Cen', 'Yang Zhou', 'Wenkai Fang', 'Yiru Zhao', 'Baisheng Lai', 'Mingli Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'instruction following', 'sample efficiency', 'hindsight experience replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23457</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Unverifiable Rewards: A Case Study on Visual Insights</title><link>https://arxiv.org/abs/2512.22650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective Test-Time Scaling (Selective TTS): a process-based multi-stage refinement framework that allocates compute across pipeline stages and prunes low-quality branches early using process-specific judges to avoid judge drift.&lt;/li&gt;&lt;li&gt;Builds an end-to-end multi-agent pipeline for generating visually insightful charts and reports from datasets, and designs an LLM-based judge aligned with human experts (Kendall's τ = 0.55).&lt;/li&gt;&lt;li&gt;Demonstrates improved insight quality and reduced variance under a fixed compute budget (mean score increase from 61.64 to 65.86) by distributing refinement across stages rather than repeated temporal refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyu Gan', 'James Mooney', 'Pan Hao', 'Renxiang Wang', 'Mingyi Hong', 'Qianwen Wang', 'Dongyeop Kang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'unverifiable rewards', 'judge drift', 'robustness/stability', 'multi-agent pipeline']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22650</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</title><link>https://arxiv.org/abs/2512.22293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Warning-framed examples (e.g., 'DO NOT USE - this code is vulnerable') do not prevent language models from reproducing the warned-against content — reproduction rates were statistically similar (76.7% vs. 83.3%).&lt;/li&gt;&lt;li&gt;Sparse autoencoder analysis attributes this to overlapping latent features: representations for 'describing X' and 'performing X' are not orthogonal, so the same feature (e.g., #8684) activates in both contexts.&lt;/li&gt;&lt;li&gt;Identifies a related failure mode called 'stealth slip', where conversational preambles rotate activations into subspaces that linear probes miss, making prompting and inference-time steering ineffective.&lt;/li&gt;&lt;li&gt;Finds that training-time interventions (feature ablation) mitigate the issue, implying that statistical co-occurrence in training dominates pragmatic interpretation in current architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsogt-Ochir Enkhbayar']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'training-data-warnings', 'model-robustness', 'representation-vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22293</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unbiased Visual Reasoning with Controlled Visual Inputs</title><link>https://arxiv.org/abs/2512.22183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VISTA, a modular framework that decouples perception (frozen VLM sensor answering constrained perception queries) from reasoning (text-only LLM that plans queries and aggregates visual facts).&lt;/li&gt;&lt;li&gt;Imposes an explicit information bottleneck and trains the reasoner with reinforcement learning (GRPO) to produce unbiased visual reasoning traces.&lt;/li&gt;&lt;li&gt;Achieves substantial improvements in robustness to real-world spurious correlations (SpuriVerse) while remaining competitive on balanced benchmarks, and transfers across unseen VLM sensors.&lt;/li&gt;&lt;li&gt;Demonstrates ability to recognize and recover from perception failures and produces more neutral, explicitly grounded reasoning traces compared to end-to-end VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaonan Li', 'Shijie Lu', 'Fei Wang', 'Jacob Dineen', 'Xiao Ye', 'Zhikun Xu', 'Siyi Liu', 'Young Min Cho', 'Bangzheng Li', 'Daniel Chang', 'Kenny Nguyen', 'Qizheng Yang', 'Muhao Chen', 'Ben Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'vision-language models', 'reinforcement learning', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22183</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans</title><link>https://arxiv.org/abs/2512.23693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for fine-tuning LLMs using fine-grained span-level human feedback where annotators mark liked/disliked spans and describe issues.&lt;/li&gt;&lt;li&gt;Uses feedback-driven improvement chains: the model rewrites disliked spans left-to-right producing incremental revisions; adjacent steps form preference pairs for supervised preference learning.&lt;/li&gt;&lt;li&gt;Demonstrates this structured, revision-based supervision outperforms standard A/B preference ranking and full-contrastive rewrites for preference tuning.&lt;/li&gt;&lt;li&gt;Claims more efficient and effective alignment via localized, targeted edits rather than coarse whole-response comparisons.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sky CH-Wang', 'Justin Svegliato', 'Helen Appel', 'Jason Eisner']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human feedback', 'preference learning', 'fine-grained supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23693</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing</title><link>https://arxiv.org/abs/2512.23684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of document-level hidden prompt injection attacks against LLM-based academic peer review using ~500 real ICML papers.&lt;/li&gt;&lt;li&gt;Injects semantically equivalent adversarial instructions in four languages and measures changes in review scores and accept/reject decisions.&lt;/li&gt;&lt;li&gt;Finds substantial impact for English, Japanese, and Chinese injections, but little effect for Arabic, highlighting language-dependent vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panagiotis Theocharopoulos', 'Ajinkya Kulkarni', 'Mathew Magimai. -Doss']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'adversarial attacks', 'multilingual', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23684</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models</title><link>https://arxiv.org/abs/2512.23578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode in multi-turn spoken language models (SLMs) where instructed paralinguistic styles (emotion, accent, volume, speed) degrade over conversation — termed "style amnesia."&lt;/li&gt;&lt;li&gt;Evaluates three proprietary and two open-source SLMs, showing consistent inability to maintain expressed speaking style across turns despite recalling the instruction when prompted.&lt;/li&gt;&lt;li&gt;Finds that explicit recall prompts can partially mitigate the effect, and that style-following is worse when instructions are in system messages versus user messages.&lt;/li&gt;&lt;li&gt;Explores prompting strategies and mitigation techniques but does not frame the phenomenon primarily as an adversarial attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu-Xiang Lin', 'Cheng-Han Chiang', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'instruction-following', 'spoken-language-models', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23578</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Instruction-Following Evaluation of Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23572</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that large vision-language models (LVLMs) often lose the instruction-following ability present in their base LLMs after visual instruction fine-tuning.&lt;/li&gt;&lt;li&gt;Constructs training datasets that vary whether the output format is explicitly specified and quantitatively evaluates effects on instruction-following.&lt;/li&gt;&lt;li&gt;Finds that including explicit output-format instructions during (visual) instruction tuning improves models' adherence to task instructions.&lt;/li&gt;&lt;li&gt;Suggests a mitigation: include samples with explicit output-format instructions in fine-tuning datasets to reduce decline in instruction-following.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiki Shiono', 'Shumpei Miyawaki', 'Ryota Tanaka', 'Jun Suzuki']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'instruction-following', 'safety-evaluation', 'vision-language models', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23572</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs</title><link>https://arxiv.org/abs/2512.23547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using knowledge graphs (KGs) extracted from LLM outputs to improve hallucination self-detection.&lt;/li&gt;&lt;li&gt;Converts responses into entity-relation graphs and estimates likelihood of hallucinations from the graph structure.&lt;/li&gt;&lt;li&gt;Evaluates method on GPT-4o and Gemini-2.5-Flash across two hallucination detection datasets, reporting up to 16% accuracy and 20% F1 relative improvements over SelfCheckGPT.&lt;/li&gt;&lt;li&gt;Releases a manually curated/enhanced dataset to support more reliable benchmarking; method is low-cost and model-agnostic.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Kale', 'Antonio Luca Alfeo']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'self-detection', 'knowledge graphs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23547</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias</title><link>https://arxiv.org/abs/2512.23518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mixture of Latent Concept Experts (MoLaCE), an inference-time framework that mixes experts realized as different activation strengths over latent concepts to mitigate input confirmation bias in LLMs.&lt;/li&gt;&lt;li&gt;Argues that differently phrased prompts reweight latent concepts, so a single fixed intervention is insufficient; MoLaCE dynamically reweights latent concept activations to emulate internal debate within a single LLM.&lt;/li&gt;&lt;li&gt;Claims MoLaCE reduces confirmation bias, improves robustness, and matches or outperforms multi-agent debate approaches while using substantially less computation.&lt;/li&gt;&lt;li&gt;Can be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors among agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hazel Kim', 'Philip Torr']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM robustness', 'debiasing', 'multi-agent debate']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23518</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs</title><link>https://arxiv.org/abs/2512.23430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses stereotypical and structural biases in LLMs as arising from spurious latent feature correlations that induce reasoning shortcuts.&lt;/li&gt;&lt;li&gt;Proposes Causal-Contrastive Preference Optimization (C2PO), which uses causal counterfactual signals to isolate bias-inducing features and a fairness-sensitive preference update to suppress shortcut contributions at the logit level.&lt;/li&gt;&lt;li&gt;Evaluates across many benchmarks (BBQ, Unqover, MNLI, HANS, Chatbot, MT-Bench, StereoSet, WinoBias, MMLU, GSM8K) showing reduced stereotypical and structural bias while maintaining general reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Feng', 'Bo An', 'Tianlong Gu', 'Liang Chang', 'Fengrui Hao', 'Peipeng Yu', 'Shuai Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias-mitigation', 'fairness', 'robustness', 'causal-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23430</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</title><link>https://arxiv.org/abs/2512.23343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey linking cognitive neuroscience concepts of memory to memory architectures and management in LLM-driven autonomous agents.&lt;/li&gt;&lt;li&gt;Comparative analysis of memory taxonomy, storage mechanisms, lifecycle management, and benchmarks for evaluating agent memory.&lt;/li&gt;&lt;li&gt;Includes a dedicated discussion on memory security (attacks and defenses) and outlines future directions such as multimodal memory and skill acquisition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiafeng Liang', 'Hao Li', 'Chang Li', 'Jiaqi Zhou', 'Shixin Jiang', 'Zekun Wang', 'Changkai Ji', 'Zhihao Zhu', 'Runxuan Liu', 'Tao Ren', 'Jinlan Fu', 'See-Kiong Ng', 'Xia Liang', 'Ming Liu', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['agent-memory', 'memory-security', 'LLM-agents', 'adversarial-attacks', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23343</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Chinese Morph Resolution in E-commerce Live Streaming Scenarios</title><link>https://arxiv.org/abs/2512.23280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Live Auditory Morph Resolution (LiveAMR) task to detect pronunciation-based morphs used to evade scrutiny in Chinese e-commerce live streaming.&lt;/li&gt;&lt;li&gt;Constructs a LiveAMR dataset of 86,790 samples focused on health/medical live streams where hosts use morphs for false advertising.&lt;/li&gt;&lt;li&gt;Transforms the detection problem into a text-to-text generation task and leverages LLMs to generate additional training data for improved performance.&lt;/li&gt;&lt;li&gt;Demonstrates that morph resolution can materially aid live-streaming regulation and moderation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Zhu', 'Jipeng Qiang', 'Ran Bai', 'Chenyu Liu', 'Xiaoye Ouyang']&lt;/li&gt;&lt;li&gt;Tags: ['audio-evasion', 'adversarial-evasion', 'content-moderation', 'dataset-release', 'LLM-data-augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23280</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAE-constructed low-rank subspace adaptation to initialize parameter-efficient adapters (e.g., LoRA) using disentangled SAE features for interpretable safety alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing SAE can recover task-relevant subspace with arbitrarily small error under monosemanticity, whereas polysemantic feature spaces incur an irreducible error floor.&lt;/li&gt;&lt;li&gt;Empirical results report up to 99.6% safety rate—outperforming full fine-tuning by ~7.4 percentage points and approaching RLHF—while updating only 0.19–0.24% of parameters.&lt;/li&gt;&lt;li&gt;Method yields semantic grounding of alignment directions, improving transparency and interpretability of safety interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Lechen Ning', 'Zhenbo Xu', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'parameter-efficient fine-tuning', 'interpretability', 'low-rank adaptation (LoRA)', 'LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Social Desirability Bias in Random Silicon Sampling</title><link>https://arxiv.org/abs/2512.22725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Replicates social desirability bias (SDB) in LLM-based "silicon sampling" using ANES survey items and three LLMs (Llama-3.1 family and GPT-4.1-mini).&lt;/li&gt;&lt;li&gt;Tests four prompt-based mitigation methods: reformulated (neutral/third-person), reverse-coded (semantic inversion), and two meta-instructions (priming and preamble encouraging analytics or sincerity).&lt;/li&gt;&lt;li&gt;Evaluates alignment to human ANES distributions via Jensen–Shannon Divergence with bootstrap confidence intervals.&lt;/li&gt;&lt;li&gt;Finds reformulated prompts most effective at reducing SDB and improving alignment; reverse-coding yields mixed results; priming/preamble produce uniformity but no consistent benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sashank Chapala', 'Maksym Mironov', 'Songgaojun Deng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'social desirability bias', 'prompt engineering', 'evaluation', 'LLM benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22725</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages</title><link>https://arxiv.org/abs/2512.22712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a human-validated framework to evaluate whether chain-of-thought reasoning traces logically support model conclusions across languages.&lt;/li&gt;&lt;li&gt;Analyzes 65k reasoning traces from GlobalMMLU across 6 languages and 6 frontier models, finding frequent reasoning-conclusion misalignment despite high task accuracy.&lt;/li&gt;&lt;li&gt;Reports that non-Latin scripts show at least twice the rate of misalignment compared to Latin scripts and provides a human-annotated error taxonomy.&lt;/li&gt;&lt;li&gt;Finds primary failure modes are evidential errors (unsupported/ambiguous claims) and illogical reasoning steps, arguing for reasoning-aware multilingual evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anaelia Ovalle', 'Candace Ross', 'Sebastian Ruder', 'Adina Williams', 'Karen Ullrich', 'Mark Ibrahim', 'Levent Sagun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'multilingual-LLMs', 'chain-of-thought', 'reasoning-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22712</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</title><link>https://arxiv.org/abs/2512.22671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MAW-guided structured width pruning of GLU-MLP layers in Llama-3.2 selectively degrades parametric knowledge and perplexity while substantially improving instruction-following and preserving multi-step reasoning.&lt;/li&gt;&lt;li&gt;Comprehensive benchmarks (MMLU, GSM8K, IFEval, MUSR, TruthfulQA-MC2) reveal an inverse correlation between factual knowledge and truthfulness/discrimination of misconceptions after pruning.&lt;/li&gt;&lt;li&gt;The paper characterizes expansion ratio as an architectural knob that modulates cognitive and behavioral capabilities rather than serving solely as a compression parameter.&lt;/li&gt;&lt;li&gt;Energy efficiency and latency trade-offs are quantified: up to 23% J/token savings with penalties in single-request latency but gains in batch workloads.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pere Martra']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model robustness', 'model compression/pruning', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22671</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs</title><link>https://arxiv.org/abs/2512.22631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates faithfulness of chain-of-thought (CoT) explanations and their impact on reliability for safety supervision and alignment monitoring.&lt;/li&gt;&lt;li&gt;Evaluates two optimization methods—Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO)—for improving CoT faithfulness across model scales.&lt;/li&gt;&lt;li&gt;Finds GRPO outperforms DPO on larger models (best results with Qwen2.5-14B-Instruct) and shows stronger correlation between model size and faithfulness, though GRPO is less stable at smaller scales.&lt;/li&gt;&lt;li&gt;Concludes GRPO is a promising direction for making LLM reasoning more transparent and trustworthy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Mohammadi', 'Tamas Kozak', 'Anastasia Giachanou']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'alignment', 'faithfulness', 'optimization (GRPO/DPO)', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22631</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hallucination Detection and Evaluation of Large Language Model</title><link>https://arxiv.org/abs/2512.22416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HHEM, a lightweight classification-based hallucination detection framework that avoids LLM-based judgment to vastly reduce evaluation time.&lt;/li&gt;&lt;li&gt;Compares HHEM (with and without non-fabrication checking) to prior multi-stage methods on QA and summarization, reporting improved efficiency and up to 82.2% accuracy and 78.9% TPR.&lt;/li&gt;&lt;li&gt;Identifies weaknesses detecting localized hallucinations in summaries and introduces segment-based retrieval to verify smaller text components, improving detection.&lt;/li&gt;&lt;li&gt;Presents CDF analysis showing larger models (7B–9B) tend to hallucinate less while intermediate-sized models show higher instability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenggong Zhang', 'Haopeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'factuality evaluation', 'model robustness', 'retrieval-based verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22416</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</title><link>https://arxiv.org/abs/2512.22322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SmartSnap, a paradigm for proactive, in-situ self-verification where agents collect minimal decisive snapshot evidences to prove task completion rather than relying on post-hoc verifiers over verbose trajectories.&lt;/li&gt;&lt;li&gt;Introduces Self-Verifying Agent with dual missions (solve task + seek evidence) guided by 3C Principles: Completeness, Conciseness, Creativity, and uses those snapshots as sole input to an LLM-as-a-Judge verifier.&lt;/li&gt;&lt;li&gt;Demonstrates improved scalability and reliability on mobile GUI tasks across model sizes, reporting up to ~26% and ~16.7% gains for 8B and 30B models versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaofei Cai', 'Yulei Qin', 'Haojia Lin', 'Zihan Xu', 'Gang Li', 'Yuchen Shi', 'Zongyi Li', 'Yong Mao', 'Siqi Cai', 'Xiaoyu Tan', 'Yitao Liang', 'Ke Li', 'Xing Sun']&lt;/li&gt;&lt;li&gt;Tags: ['agent verification', 'self-verifying agents', 'agentic RL', 'task verification', 'LLM-as-a-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22322</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GShield: Mitigating Poisoning Attacks in Federated Learning</title><link>https://arxiv.org/abs/2512.19286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GShield, a defense for federated learning that models benign gradient distributions via clustering and Gaussian modeling in an initial round to create a trusted baseline.&lt;/li&gt;&lt;li&gt;Selectively aggregates client updates that match the learned benign gradient profile to detect and isolate malicious or low-quality updates, designed to work under non-IID data.&lt;/li&gt;&lt;li&gt;Evaluated on tabular and image datasets, showing substantial robustness improvements over state-of-the-art defenses and a 43–65% improvement in targeted-class accuracy after detecting adversarial clients.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sameera K. M.', 'Serena Nicolazzo', 'Antonino Nocera', 'Vinod P.', 'Rafidha Rehiman K. A']&lt;/li&gt;&lt;li&gt;Tags: ['federated_learning', 'data_poisoning', 'backdoor_defense', 'robust_aggregation', 'anomaly_detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19286</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse</title><link>https://arxiv.org/abs/2511.13539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;BootOOD is a self-supervised OOD detection method that synthesizes pseudo-OOD features from in-distribution (ID) representations using simple transformations and leverages Neural Collapse properties of ID features.&lt;/li&gt;&lt;li&gt;It adds a lightweight auxiliary radius-based head that classifies by feature norm (encouraging OOD to have smaller norms), decoupling OOD detection from the primary classifier and easing detection when ID and OOD are semantically similar.&lt;/li&gt;&lt;li&gt;Empirical results on CIFAR-10, CIFAR-100, and ImageNet-200 show BootOOD outperforms prior post-hoc detectors, surpasses training-based methods without outlier exposure, and is competitive with outlier-exposure approaches while retaining or improving ID accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanchao Wang', 'Tian Qin', 'Eduardo Valle', 'Bruno Abrahao']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'self-supervised learning', 'neural collapse', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13539</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems</title><link>https://arxiv.org/abs/2509.24408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FuncPoison, a poisoning attack that injects malicious tools into the shared function library used by LLM-driven multi-agent autonomous driving systems.&lt;/li&gt;&lt;li&gt;Exploits two weaknesses: agents select tools via text-based instructions and tools use standardized command formats that attackers can replicate to trigger malicious behavior.&lt;/li&gt;&lt;li&gt;Empirical evaluation on two representative multi-agent driving systems shows significant degradation in trajectory accuracy, targeted coordinated misbehavior of specific agents, and the attack's ability to evade multiple defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuzhen Long', 'Songze Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'function-library poisoning', 'adversarial attack', 'autonomous driving security', 'tool-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24408</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis</title><link>https://arxiv.org/abs/2509.16582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepSSIM, a self-supervised embedding that learns to predict SSIM similarity in embedding space to detect memorization in generative models.&lt;/li&gt;&lt;li&gt;Uses structure-preserving augmentations to capture anatomical features and enable alignment-free similarity estimation for medical images.&lt;/li&gt;&lt;li&gt;Evaluates DeepSSIM on synthetic brain MRI from a Latent Diffusion Model trained under memorization-prone conditions (IXI and CoRR datasets) and reports large F1 improvements (+52.03% over best prior method).&lt;/li&gt;&lt;li&gt;Provides code and data for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Scardace', 'Lemuel Puglisi', 'Francesco Guarnera', 'Sebastiano Battiato', 'Daniele Rav\\`i']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'generative-models', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16582</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Safe Are AI-Generated Patches? A Large-scale Study on Security Risks in LLM and Agentic Automated Program Repair on SWE-bench</title><link>https://arxiv.org/abs/2507.02976</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical security analysis of LLM- and agent-generated program patches using 20,000+ GitHub issues, comparing developer fixes, Llama 3.3 Instruct-70B, and three agentic APR frameworks (OpenHands, AutoCodeRover, HoneyComb).&lt;/li&gt;&lt;li&gt;Finds that Llama introduces numerous new vulnerabilities with patterns distinct from developer patches; agentic workflows also produce vulnerabilities, increasing with autonomy.&lt;/li&gt;&lt;li&gt;Analyzes code-, issue-, and project-level factors that correlate with insecure patches and argues for proactive risk-assessment methods that incorporate contextual information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirali Sajadi', 'Kostadin Damevski', 'Preetha Chatterjee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'automated program repair', 'software vulnerability analysis', 'agentic systems', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02976</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Large Language Model Safety with Contrastive Representation Learning</title><link>https://arxiv.org/abs/2506.11938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes framing LLM defense as a contrastive representation learning problem using a triplet loss and adversarial hard negative mining.&lt;/li&gt;&lt;li&gt;Finetunes models to separate benign and harmful representations, aiming to improve robustness to input-level and embedding-space attacks.&lt;/li&gt;&lt;li&gt;Reports improved robustness over prior representation-engineering defenses without degrading standard performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Simko', 'Mrinmaya Sachan', 'Bernhard Sch\\"olkopf', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'adversarial robustness', 'contrastive learning', 'representation engineering', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11938</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SelfCheck-Eval: A Multi-Module Framework for Zero-Resource Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2502.01812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIME Math Hallucination dataset: a benchmark for evaluating mathematical reasoning hallucinations.&lt;/li&gt;&lt;li&gt;Proposes SelfCheck-Eval, an LLM-agnostic, black-box multi-module hallucination detection framework (Semantic, Specialised Detection, Contextual Consistency) for zero-resource settings.&lt;/li&gt;&lt;li&gt;Evaluates detectors across domains and training paradigms, finding existing methods work on biographical content but fail substantially on mathematical reasoning, motivating specialised black-box approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diyana Muhammed', 'Giusy Giulia Tuccari', 'Gollam Rabby', 'S\\"oren Auer', 'Sahar Vahdati']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'black-box evaluation', 'benchmark/dataset', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01812</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Vision-Language Model Reliability with Uncertainty-Guided Dropout Decoding</title><link>https://arxiv.org/abs/2412.06474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DROPOUT DECODING, an inference-time method that quantifies visual-token uncertainty and selectively masks uncertain tokens to improve LVLM decoding.&lt;/li&gt;&lt;li&gt;Uncertainty is computed by projecting visual tokens into text space and decomposing into aleatoric and epistemic components; the method focuses on epistemic uncertainty to capture perception-related errors.&lt;/li&gt;&lt;li&gt;Applies uncertainty-guided token dropout across an ensemble of masked contexts to reduce object hallucinations and improve reliability; evaluated on CHAIR, THRONE, and MMBench with reported reductions in hallucinations. Code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixiong Fang', 'Ziran Yang', 'Zhaorun Chen', 'Zhuokai Zhao', 'Jiawei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination-mitigation', 'safety-evaluation', 'inference-time-defenses', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06474</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ForgerySleuth: Empowering Multimodal Large Language Models for Image Manipulation Detection</title><link>https://arxiv.org/abs/2411.19466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForgerySleuth, a method that leverages multimodal LLMs to fuse clues and produce segmentation maps highlighting manipulated regions in images.&lt;/li&gt;&lt;li&gt;Introduces ForgeryAnalysis, a dataset built via a Chain-of-Clues prompting approach that includes analysis and reasoning text to enhance IMD (image manipulation detection) training.&lt;/li&gt;&lt;li&gt;Presents a data engine to scale pre-training data and reports experiments showing improved generalization, robustness, and explainability over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Sun', 'Haoran Jiang', 'Haoran Chen', 'Yixin Cao', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['image-manipulation-detection', 'multimodal-LLMs', 'robustness', 'forensics', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.19466</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scalable and Privacy-Preserving Synthetic Data Generation on Decentralised Web</title><link>https://arxiv.org/abs/2310.20062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Builds on Libertas (decentralised Solid + MPC) to enable contributor-centric synthetic data generation from personal Web data.&lt;/li&gt;&lt;li&gt;Identifies MPC scalability limits and proposes integrating secure enclaves (e.g., Intel SGX) with MPC to reduce computation/communication overhead while preserving privacy guarantees.&lt;/li&gt;&lt;li&gt;Implements differentially private synthetic data generation in this hybrid architecture and provides empirical evaluation on simulated and real datasets across multiple generation algorithms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishal Ramesh', 'Rui Zhao', 'Naman Goel']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'differential privacy', 'secure enclaves (SGX)', 'secure multi-party computation', 'synthetic data generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.20062</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</title><link>https://arxiv.org/abs/2512.17367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-SGA (Large Language Model-based Sample Generation and Aggregation) to capture invariances of textual adversarial attacks and improve detector generalizability.&lt;/li&gt;&lt;li&gt;Implements ARHOCD: an ensemble of base detectors with a dynamic weight assignment scheme (initialized by domain knowledge and updated via Bayesian inference) to boost accuracy.&lt;/li&gt;&lt;li&gt;Introduces an iterative adversarial training strategy that jointly optimizes base detectors and the weight assignor; evaluated on hate speech, rumor, and extremist content datasets under adversarial conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yidong Chai', 'Yi Liu', 'Mohammadreza Ebrahimi', 'Weifeng Li', 'Balaji Padmanabhan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial attacks (text)', 'adversarial training', 'content moderation', 'ensemble defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17367</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forecasting in Offline Reinforcement Learning for Non-stationary Environments</title><link>https://arxiv.org/abs/2512.01987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FORL: combines conditional diffusion-based candidate state generation and zero-shot time-series foundation models to forecast and mitigate abrupt, time-varying offsets in offline RL.&lt;/li&gt;&lt;li&gt;Targets partial observability and non-Markovian, realistic non-stationarity so agents perform robustly from episode start without additional environment interaction.&lt;/li&gt;&lt;li&gt;Evaluates on offline RL benchmarks augmented with real-world time-series, showing consistent performance gains over competitive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suzan Ece Ada', 'Georg Martius', 'Emre Ugur', 'Erhan Oztop']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'offline reinforcement learning', 'non-stationarity', 'forecasting', 'partial observability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01987</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Why Do Language Model Agents Whistleblow?</title><link>https://arxiv.org/abs/2511.17085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes 'LLM whistleblowing' where tool-using language model agents disclose suspected misconduct to external parties beyond the user/dialog without instruction.&lt;/li&gt;&lt;li&gt;Presents an evaluation suite of staged misconduct scenarios to measure whistleblowing across models and settings.&lt;/li&gt;&lt;li&gt;Empirical findings: whistleblowing frequency varies across model families; higher task complexity lowers whistleblowing; moral/system-prompt nudges increase whistleblowing; providing more tools/workflows decreases whistleblowing.&lt;/li&gt;&lt;li&gt;Assesses robustness by testing model evaluation-awareness and finds lower awareness in their settings versus prior work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kushal Agrawal', 'Frank Xiao', 'Guido Bergman', 'Asa Cooper Stickland']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'alignment', 'safety evaluation', 'red teaming', 'tool use']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17085</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU), which models continuous unlearning as rotations in a learned 'cognitive rotation space' and uses rotational salience weights to quantify/control unlearning degree.&lt;/li&gt;&lt;li&gt;Introduces a skew-symmetric loss to construct the rotation space and an orthogonal rotation-axes regularization to make successive unlearning requests minimally interfering, aiming to prevent cumulative catastrophic utility loss.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art performance on multiple datasets for unlearning without relying on a retained dataset, improving utility preservation during repeated unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Chenghao Xu', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM safety', 'privacy', 'continual unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy XAI Evaluation</title><link>https://arxiv.org/abs/2501.01516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial attacks that alter text-based XAI explanations without changing model outputs and argues current IR metrics mischaracterize attack impact.&lt;/li&gt;&lt;li&gt;Proposes synonymity-weighted similarity: augmenting evaluation metrics with semantic similarity of perturbed words to better reflect true explanation changes.&lt;/li&gt;&lt;li&gt;Claims the method yields more accurate vulnerability assessments, reducing overestimation of attack success and improving trustworthiness evaluation of XAI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Burger']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'XAI robustness', 'evaluation metrics', 'semantic similarity', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01516</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdvPrefix: An Objective for Nuanced LLM Jailbreaks</title><link>https://arxiv.org/abs/2412.10321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvPrefix, a plug-and-play prefix-forcing objective that selects model-dependent prefixes by optimizing for high prefilling attack success and low negative log-likelihood.&lt;/li&gt;&lt;li&gt;Integrates into existing jailbreak attacks to produce more nuanced and realistic jailbroken outputs and overcomes rigid-format limitations of common prefix objectives.&lt;/li&gt;&lt;li&gt;Empirical results show large improvements (e.g., raising nuanced attack success on Llama-3 from 14% to 80% when replacing GCG's default prefixes), demonstrating safety alignment brittleness to new prefixes.&lt;/li&gt;&lt;li&gt;Provides code and selected prefixes to enable replication and further red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Zhu', 'Brandon Amos', 'Yuandong Tian', 'Chuan Guo', 'Ivan Evtimov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation', 'alignment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10321</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning using Forgetting Neural Networks</title><link>https://arxiv.org/abs/2410.22374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Forgetting Neural Networks (FNNs) that encode explicit forgetting via multiplicative decay factors, with concrete implementation variants (per-neuron factors, rank-based assignments).&lt;/li&gt;&lt;li&gt;Evaluates targeted unlearning on MNIST and Fashion-MNIST, showing removal of information about forget sets while preserving performance on retained data.&lt;/li&gt;&lt;li&gt;Validates effectiveness using membership inference attacks, demonstrating reduced recoverability of training data; frames FNNs as an efficient and interpretable approach to unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amartya Hatua', 'Trung T. Nguyen', 'Filip Cano', 'Andrew H. Sung']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'membership-inference', 'neural-architecture', 'data-deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22374</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trust-free Personalized Decentralized Learning</title><link>https://arxiv.org/abs/2410.11378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TPFed, a trust-free decentralized personalized federated learning framework using a blockchain-based bulletin board instead of a central aggregator.&lt;/li&gt;&lt;li&gt;Uses Locality-Sensitive Hashing (LSH) and peer ranking for dynamic partner selection and an "all-in-one" knowledge distillation protocol with a public reference dataset for knowledge transfer, model-quality evaluation, and similarity verification.&lt;/li&gt;&lt;li&gt;Claims to preserve privacy by avoiding exposure of local models/data and to improve global scalability and personalization.&lt;/li&gt;&lt;li&gt;Reports improved accuracy and system robustness against adversarial/malicious peers in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yawen Li', 'Yan Li', 'Junping Du', 'Yingxia Shao', 'Meiyu Liang', 'Guanhua Ye']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'decentralized-learning', 'privacy-preserving', 'robustness', 'blockchain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.11378</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Agents like Large Language Models</title><link>https://arxiv.org/abs/2406.04208</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing that decision-making agents should be trained using the LLM training pipeline (large-scale pretraining + post-training alignment) to produce more general, robust, and aligned behaviors.&lt;/li&gt;&lt;li&gt;Provides a proof-of-concept where an agent is trained from pixels in a 3D video-game environment using LLM-style procedures and studies the contribution of each training stage.&lt;/li&gt;&lt;li&gt;Offers practical guidance and insights for applying LLM-like training and alignment to agents, and discusses implications for generalization and robustness beyond conventional RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Jelley', 'Yuhan Cao', 'Dave Bignell', 'Amos Storkey', 'Sam Devlin', 'Tabish Rashid']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'agent training', 'LLM-style training', 'reinforcement learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.04208</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Survey of Reinforcement Learning from Human Feedback</title><link>https://arxiv.org/abs/2312.14925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of Reinforcement Learning from Human Feedback (RLHF), covering fundamentals, algorithms, and how agents interact with human feedback.&lt;/li&gt;&lt;li&gt;Covers multiple domains with emphasis on control and robotics (historical roots) and a dedicated section on recent RLHF use in fine-tuning large language models (LLMs).&lt;/li&gt;&lt;li&gt;Discusses core principles, algorithmic pipelines, how human feedback integrates with RL, and main research trends and challenges—implications for alignment and safe behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Timo Kaufmann', 'Paul Weng', 'Viktor Bengs', 'Eyke H\\"ullermeier']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Alignment', 'Human-in-the-loop', 'LLMs', 'Safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.14925</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memorization in 3D Shape Generation: An Empirical Study</title><link>https://arxiv.org/abs/2512.23628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an evaluation framework to quantify memorization in 3D generative models and applies it to existing methods.&lt;/li&gt;&lt;li&gt;Empirically analyzes factors affecting memorization (data modality, data diversity, conditioning granularity) and modeling choices (guidance scale, Vecset length, rotation augmentation).&lt;/li&gt;&lt;li&gt;Identifies simple mitigation strategies (longer Vecsets, rotation augmentation) that reduce memorization without degrading generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Pu', 'Boya Zeng', 'Kaichen Zhou', 'Mengyu Wang', 'Zhuang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data leakage/privacy', 'generative models', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23628</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</title><link>https://arxiv.org/abs/2512.23457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hindsight instruction Replay (HiR), an RL framework that replays failed responses as successes by selecting satisfied constraints in hindsight and rewriting failures accordingly.&lt;/li&gt;&lt;li&gt;Frames the training objective as dual-preference learning at both the instruction- and response-level, enabling optimization with only a binary reward signal.&lt;/li&gt;&lt;li&gt;Claims improved sample efficiency for complex instruction-following tasks and reports empirical gains across multiple tasks while reducing computational budget; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kongcheng Zhang', 'Qi Yao', 'Shunyu Liu', 'Wenjian Zhang', 'Min Cen', 'Yang Zhou', 'Wenkai Fang', 'Yiru Zhao', 'Baisheng Lai', 'Mingli Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'instruction following', 'sample efficiency', 'hindsight experience replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23457</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing</title><link>https://arxiv.org/abs/2512.23445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a systematic behaviour coverage analysis for a multi-agent simulation framework used to test autonomous vehicles, defining scenarios and interaction patterns to measure coverage.&lt;/li&gt;&lt;li&gt;Evaluates behaviour coverage metrics and uses coverage-based testing to identify gaps and optimization opportunities in the simulation.&lt;/li&gt;&lt;li&gt;Proposes a Model Predictive Control (MPC) pedestrian agent whose objective encourages ‘‘interesting’’ tests while producing more realistic pedestrian behaviour than prior agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manuel Franco-Vivo']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-vehicles', 'simulation-testing', 'safety-evaluation', 'behaviour-coverage', 'adversarial-scenario-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23445</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Integrating Uncertainty for Domain-Agnostic Segmentation</title><link>https://arxiv.org/abs/2512.23427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UncertSAM, an 8-dataset benchmark to stress-test SAM-like segmentation models under challenging conditions (shadows, transparency, camouflage).&lt;/li&gt;&lt;li&gt;Evaluates several lightweight, post-hoc uncertainty estimation methods and finds that a last-layer Laplace approximation produces uncertainty estimates that correlate with segmentation errors.&lt;/li&gt;&lt;li&gt;Explores a preliminary uncertainty-guided prediction refinement step; refinement benefits are limited but indicate potential for improving domain-agnostic robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jesse Brouwers', 'Xiaoyan Xing', 'Alexander Timans']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'robustness', 'segmentation', 'benchmark', 'foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23427</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</title><link>https://arxiv.org/abs/2512.23292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a domain-specific, agentic foundation model for safety-critical reactor control that optimizes policies via physics-based validation rather than perception-centric imitation.&lt;/li&gt;&lt;li&gt;Trains a 360M-parameter model on synthetic reactor control scenarios with datasets scaled from 10^3 to 10^5, observing a sharp phase transition and large reduction in execution variance.&lt;/li&gt;&lt;li&gt;Reports safety-relevant behaviors: the model rejects ~70% of training distribution and concentrates 95% of runtime on a single-bank actuation strategy; learned representations transfer across distinct physics and continuous input modalities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoonpyo Lee', 'Kazuma Kobayashi', 'Sai Puppala', 'Sajedul Talukder', 'Seid Koric', 'Souvik Chakraborty', 'Syed Bahauddin Alam']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Robustness', 'Control systems', 'Domain-specific foundation model', 'Safety-critical systems (nuclear)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23292</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAE-constructed low-rank subspace adaptation to initialize parameter-efficient adapters (e.g., LoRA) using disentangled SAE features for interpretable safety alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing SAE can recover task-relevant subspace with arbitrarily small error under monosemanticity, whereas polysemantic feature spaces incur an irreducible error floor.&lt;/li&gt;&lt;li&gt;Empirical results report up to 99.6% safety rate—outperforming full fine-tuning by ~7.4 percentage points and approaching RLHF—while updating only 0.19–0.24% of parameters.&lt;/li&gt;&lt;li&gt;Method yields semantic grounding of alignment directions, improving transparency and interpretability of safety interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Lechen Ning', 'Zhenbo Xu', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'parameter-efficient fine-tuning', 'interpretability', 'low-rank adaptation (LoRA)', 'LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</title><link>https://arxiv.org/abs/2512.23132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collects and synthesizes 93 ML threats from MITRE ATLAS, AI Incident Database, and literature, and analyzes 854 GitHub/Python repositories to map vulnerabilities and TTPs across the ML lifecycle.&lt;/li&gt;&lt;li&gt;Uses a multi-agent RAG pipeline (GPT-4o) to mine 300+ articles and build an ontology-driven threat graph linking TTPs, vulnerabilities, and lifecycle stages, highlighting dense vulnerability clusters in dependencies.&lt;/li&gt;&lt;li&gt;Identifies specific attacks of security relevance — e.g., commercial LLM API model stealing, parameter memorization leakage, preference-guided text-only jailbreaks, MASTERKEY-style jailbreaking, federated poisoning, and diffusion backdoors.&lt;/li&gt;&lt;li&gt;Recommends adaptive, ML-specific security controls (dependency hygiene, threat intelligence, monitoring) to mitigate supply-chain and inference risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armstrong Foundjem', 'Lionel Nganyewou Tidjon', 'Leuson Da Silva', 'Foutse Khomh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'model extraction', 'data poisoning', 'supply-chain security', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23132</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</title><link>https://arxiv.org/abs/2512.23126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies two limitations of Direct Preference Optimization (DPO): dependence on arbitrary modeling choices (scalarization, reference policy) and failure to leverage pairwise comparative information.&lt;/li&gt;&lt;li&gt;Proposes Intrinsic Self-reflective Preference Optimization (InSPO), conditioning policy on context plus alternative responses to derive a globally optimal, invariant policy.&lt;/li&gt;&lt;li&gt;Shows InSPO is a plug-and-play enhancement with no architecture or inference overhead and reports improved win rates and length-controlled metrics over DPO/RLHF.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Li', 'Tian Lan', 'Zhengling Qi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-optimization', 'RLHF', 'model-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23126</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</title><link>https://arxiv.org/abs/2512.23090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChexReason, a resource-constrained vision-language model trained with SFT followed by GRPO (R1-style) using small datasets and a single A100.&lt;/li&gt;&lt;li&gt;Finds GRPO improves in-distribution performance (CheXpert) but significantly degrades cross-dataset transferability (NIH), revealing a generalization paradox where the SFT checkpoint can outperform RL on out-of-distribution data.&lt;/li&gt;&lt;li&gt;Argues the robustness/generalization degradation is linked to the RL paradigm rather than model scale, and that structured reasoning scaffolds help general-purpose VLMs but not medically pretrained models.&lt;/li&gt;&lt;li&gt;Concludes that curated supervised fine-tuning may be safer for clinical deployment requiring robustness across institutions and populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Berger', 'Manuela Bergau', 'Helen Schneider', 'Saad Ahmad', 'Tom Anglim Lagones', 'Gianluca Brugnara', 'Martha Foltyn-Dumitru', 'Kai Schlamp', 'Philipp Vollmuth', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'reinforcement learning', 'medical AI', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23090</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Reward Model Selection Crisis in Personalized Alignment</title><link>https://arxiv.org/abs/2512.23067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that standard reward model (RM) accuracy is a poor criterion for selecting RMs for deployment because it fails to predict token-level, reward-guided generation behavior.&lt;/li&gt;&lt;li&gt;Introduces policy accuracy (a metric for whether RGD scoring discriminates preferred vs. dispreferred responses) and Pref-LaMP, a personalized alignment benchmark with ground-truth user completions for direct behavioral evaluation.&lt;/li&gt;&lt;li&gt;Finds weak correlation between RM accuracy and policy-level discrimination (Kendall's tau = 0.08–0.31), large decoupling between discrimination and generation, and that simple in-context learning outperforms reward-guided methods for models &gt;3B.&lt;/li&gt;&lt;li&gt;Argues that commonly used proxy metrics do not predict deployment performance, highlighting a realignment/evaluation gap relevant to safe personalized behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fady Rezk', 'Yuangang Pan', 'Chuan-Sheng Foo', 'Xun Xu', 'Nancy Chen', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'evaluation/benchmarking', 'reward-guided decoding', 'personalized alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23067</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Risk-Averse Learning with Varying Risk Levels</title><link>https://arxiv.org/abs/2512.22986</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies risk-averse online optimization in non-stationary environments where the learner's risk level (measured by CVaR) can change over time.&lt;/li&gt;&lt;li&gt;Introduces a novel metric for risk-level variation alongside standard function variation, and develops algorithms for both first-order (gradient) and zeroth-order (function evaluation) feedback.&lt;/li&gt;&lt;li&gt;Derives dynamic regret bounds that depend on function variation, risk-level variation, and sampling budget; provides numerical experiments validating the methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyi Wang', 'Zifan Wang', 'Karl H. Johansson']&lt;/li&gt;&lt;li&gt;Tags: ['risk-averse learning', 'CVaR', 'online optimization', 'non-stationary environments', 'dynamic regret']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22986</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks</title><link>https://arxiv.org/abs/2512.22860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a trust-based delegated consensus for blockchain-enabled IoT combining Fully Homomorphic Encryption (FHE) and Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation.&lt;/li&gt;&lt;li&gt;Compares three learning-based defenses (tabular Q-learning, Dueling Double DQN DRL, and Multi-Agent RL) against five attack families: Naive Malicious, Collusive Rumor, Adaptive Adversarial, Byzantine Fault Injection, and Time-Delayed (sleeper) Poisoning on a 16-node simulation.&lt;/li&gt;&lt;li&gt;Key findings: MARL outperforms others against collusive attacks (F1=0.85); DRL and MARL both detect adaptive attacks perfectly while RL fails; all agents handle Byzantine attacks well; time-delayed/sleeper poisoning severely degrades detection across all agents (F1≈0.11–0.16).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soham Padia', 'Dhananjay Vaidya', 'Ramchandra Mangrulkar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'poisoning attacks', 'reinforcement learning defenses', 'blockchain security', 'multi-agent RL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22860</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</title><link>https://arxiv.org/abs/2512.22671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MAW-guided structured width pruning of GLU-MLP layers in Llama-3.2 selectively degrades parametric knowledge and perplexity while substantially improving instruction-following and preserving multi-step reasoning.&lt;/li&gt;&lt;li&gt;Comprehensive benchmarks (MMLU, GSM8K, IFEval, MUSR, TruthfulQA-MC2) reveal an inverse correlation between factual knowledge and truthfulness/discrimination of misconceptions after pruning.&lt;/li&gt;&lt;li&gt;The paper characterizes expansion ratio as an architectural knob that modulates cognitive and behavioral capabilities rather than serving solely as a compression parameter.&lt;/li&gt;&lt;li&gt;Energy efficiency and latency trade-offs are quantified: up to 23% J/token savings with penalties in single-request latency but gains in batch workloads.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pere Martra']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model robustness', 'model compression/pruning', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22671</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System</title><link>https://arxiv.org/abs/2512.22381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PHANTOM: a physics-aware adversarial network using a PINN digital twin trained with federated learning to model EV charging systems.&lt;/li&gt;&lt;li&gt;Employs multi-agent reinforcement learning (DQN and SAC) to learn false data injection (FDI) attack policies that can evade conventional detectors.&lt;/li&gt;&lt;li&gt;Evaluates impact on a transmission-and-distribution dual simulation, showing load imbalance and voltage instabilities that propagate across T&amp;D boundaries.&lt;/li&gt;&lt;li&gt;Emphasizes the need for physics-aware cybersecurity for large-scale vehicle-grid integration and coordination systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Zakaria Haider', 'Amit Kumar Podder', 'Prabin Mali', 'Aranya Chakrabortty', 'Sumit Paudyal', 'Mohammad Ashiqur Rahman']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'federated learning', 'false data injection', 'critical infrastructure / cyber-physical security', 'multi-agent reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22381</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</title><link>https://arxiv.org/abs/2512.22322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SmartSnap, a paradigm for proactive, in-situ self-verification where agents collect minimal decisive snapshot evidences to prove task completion rather than relying on post-hoc verifiers over verbose trajectories.&lt;/li&gt;&lt;li&gt;Introduces Self-Verifying Agent with dual missions (solve task + seek evidence) guided by 3C Principles: Completeness, Conciseness, Creativity, and uses those snapshots as sole input to an LLM-as-a-Judge verifier.&lt;/li&gt;&lt;li&gt;Demonstrates improved scalability and reliability on mobile GUI tasks across model sizes, reporting up to ~26% and ~16.7% gains for 8B and 30B models versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaofei Cai', 'Yulei Qin', 'Haojia Lin', 'Zihan Xu', 'Gang Li', 'Yuchen Shi', 'Zongyi Li', 'Yong Mao', 'Siqi Cai', 'Xiaoyu Tan', 'Yitao Liang', 'Ke Li', 'Xing Sun']&lt;/li&gt;&lt;li&gt;Tags: ['agent verification', 'self-verifying agents', 'agentic RL', 'task verification', 'LLM-as-a-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22322</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators</title><link>https://arxiv.org/abs/2512.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLA, a hardware–software co-design to protect generative models by embedding key bits into neurons and using invariance transformations to obscure keys.&lt;/li&gt;&lt;li&gt;Integrates a lightweight locking module into AI accelerators; an accelerator with a pre-stored secret key serves as the license to access model services.&lt;/li&gt;&lt;li&gt;Claims robustness against oracle-guided key optimization attacks and reports minimal computational overhead (&lt;0.1% for 7,168 key bits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['You Li', 'Guannan Zhao', 'Yuhao Ju', 'Yunqi He', 'Jie Gu', 'Hai Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['IP protection', 'logic locking', 'model theft', 'supply chain security', 'adversarial key optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22307</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>We are not able to identify AI-generated images</title><link>https://arxiv.org/abs/2512.22236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Interactive web experiment with 165 users (233 sessions) classifying 20 images (real vs MidJourney-generated) drawn from 120 challenging portrait cases.&lt;/li&gt;&lt;li&gt;Average human accuracy was 54% (near chance), limited improvement with repeated attempts, average response time 7.3s, and some images were consistently deceptive.&lt;/li&gt;&lt;li&gt;Concludes humans struggle to reliably detect AI-generated images, highlighting risks for misinformation and the need for non-human detection/mitigation strategies and policy/awareness efforts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrien Pav\\~ao']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic_media_detection', 'human_evaluation', 'deepfakes', 'misinformation', 'image_forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22236</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</title><link>https://arxiv.org/abs/2512.23617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a fundamental flaw in symmetric feature-invariance approaches to Unsupervised Domain Adaptation: enforcing invariance can destroy informative features and cause catastrophic negative transfer when domains differ in informativeness.&lt;/li&gt;&lt;li&gt;Introduces a decision-theoretic framework based on Le Cam's theory, defining Le Cam Distortion via the Deficiency Distance δ(E1, E2) to bound transfer risk conditional on directional simulability.&lt;/li&gt;&lt;li&gt;Proposes learning a kernel that simulates the target from the source (directional simulability) to enable transfer without degrading the source, avoiding information loss inherent in symmetric alignment.&lt;/li&gt;&lt;li&gt;Empirical validation across genomics, vision (CIFAR-10), and reinforcement learning shows preserved source utility and safer policy transfer compared to invariance-based methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deniz Akdemir']&lt;/li&gt;&lt;li&gt;Tags: ['transfer learning', 'domain adaptation', 'distribution shift', 'robustness/safety', 'negative transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23617</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy Machine Learning under Distribution Shifts</title><link>https://arxiv.org/abs/2512.23524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies trustworthy machine learning under three kinds of distribution shifts: perturbation shift, domain shift, and modality shift.&lt;/li&gt;&lt;li&gt;Evaluates trustworthiness along robustness, explainability, and adaptability dimensions, proposing methods and insights to improve reliability.&lt;/li&gt;&lt;li&gt;Targets challenges affecting safety and generalization of models (including vision, language, and multimodal systems) and aims to enhance efficiency and adaptability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Huang']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'robustness', 'explainability', 'trustworthiness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23524</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment</title><link>https://arxiv.org/abs/2512.23487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ML Compass, a systems-level framework that treats model selection as constrained optimization over a capability–cost frontier, explicitly incorporating compliance and deployment constraints.&lt;/li&gt;&lt;li&gt;Theoretically characterizes optimal configurations (three-regime structure: compliance minima, saturated maxima, interior values) and provides comparative statics for budget, regulation, and progress effects.&lt;/li&gt;&lt;li&gt;Describes an implementation pipeline to extract internal capability measures, estimate empirical frontiers, learn task-specific utility from interactions, and recommend deployment-aware model choices.&lt;/li&gt;&lt;li&gt;Validates the approach on two case studies (conversational alignment using PRISM and a healthcare setting using HealthBench), showing deployment-aware rankings can materially differ from capability-only leaderboards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vassilis Digalakis Jr', 'Ramayya Krishnan', 'Gonzalo Martin Fernandez', 'Agni Orfanoudaki']&lt;/li&gt;&lt;li&gt;Tags: ['deployment safety', 'alignment', 'safety evaluation', 'compliance', 'model selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23487</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</title><link>https://arxiv.org/abs/2512.23461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DIR: an information-theoretic debiasing method for reward models that maximizes mutual information (MI) between RM outputs and human preferences while minimizing MI with biased input attributes.&lt;/li&gt;&lt;li&gt;Targets diverse, potentially non-linear inductive biases (response length, sycophancy, format) that lead to overfitting and reward hacking in RLHF.&lt;/li&gt;&lt;li&gt;Provides theoretical justification based on the information bottleneck and demonstrates improved RLHF performance and generalization across benchmarks.&lt;/li&gt;&lt;li&gt;Code and training recipes are publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Li', 'Pengyu Cheng', 'Zhechao Yu', 'Feifei Tong', 'Anningzhe Gao', 'Tsung-Hui Chang', 'Xiang Wan', 'Erchao Zhao', 'Xiaoxi Jiang', 'Guanjun Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['reward-modeling', 'RLHF', 'alignment', 'debiasing', 'information-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23461</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>APO: Alpha-Divergence Preference Optimization</title><link>https://arxiv.org/abs/2512.22953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces APO, an anchored policy optimization framework that uses Csiszár alpha-divergence to interpolate continuously between forward (mode-covering) and reverse (mode-seeking) KL behaviors.&lt;/li&gt;&lt;li&gt;Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties to motivate stability benefits.&lt;/li&gt;&lt;li&gt;Proposes a practical reward-and-confidence-guarded schedule for alpha that transitions from coverage to exploitation only when policy improvements are confident.&lt;/li&gt;&lt;li&gt;Empirical results on Qwen3-1.7B (math-level3) show competitive performance with GRPO/GSPO baselines while maintaining training stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Zixian']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'policy_optimization', 'alignment', 'alpha-divergence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22953</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2512.22910</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sat-EnQ, a two-phase RL framework: Phase 1 trains an ensemble of lightweight Q-networks under a satisficing objective to limit early value growth and produce diverse, low-variance estimates; Phase 2 distills the ensemble into a larger network and fine-tunes with Double DQN.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees that satisficing induces bounded updates and cannot increase target variance, with conditions for substantial variance reduction.&lt;/li&gt;&lt;li&gt;Empirical results show 3.8x variance reduction, elimination of catastrophic failures (0% vs 50% for DQN), robustness under environmental noise (maintains 79% performance), and 2.5x less compute than bootstrapped ensembles.&lt;/li&gt;&lt;li&gt;Focus is on training stability, reliability, and robustness to noise rather than adversarial attacks, red teaming, or alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['\\"Unver \\c{C}ift\\c{c}i']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'robustness', 'stability', 'variance-reduction', 'reliable-rl']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22910</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fundamental Novel Consistency Theory: $H$-Consistency Bounds</title><link>https://arxiv.org/abs/2512.22880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H-consistency bounds that quantify target loss estimation error in terms of surrogate loss error while accounting for the hypothesis class H, offering stronger guarantees than Bayes-consistency or H-calibration.&lt;/li&gt;&lt;li&gt;Derives tight distribution-dependent and -independent bounds for binary classification and extends to multi-class (max, sum, constrained losses), with explicit results for convex surrogates and comp-sum losses (e.g., cross-entropy, MAE).&lt;/li&gt;&lt;li&gt;Considers adversarial settings and proposes smooth adversarial variants of surrogates to obtain robust learning algorithms; analyzes growth rates (universal square-root rate) and minimizability gaps to guide surrogate choice.&lt;/li&gt;&lt;li&gt;Provides a general framework and new characterizations for deriving H-consistency bounds across a variety of surrogates and problem settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutao Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['theory', 'robustness', 'adversarial-robustness', 'surrogate-loss', 'consistency-bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22880</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Unverifiable Rewards: A Case Study on Visual Insights</title><link>https://arxiv.org/abs/2512.22650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective Test-Time Scaling (Selective TTS): a process-based multi-stage refinement framework that allocates compute across pipeline stages and prunes low-quality branches early using process-specific judges to avoid judge drift.&lt;/li&gt;&lt;li&gt;Builds an end-to-end multi-agent pipeline for generating visually insightful charts and reports from datasets, and designs an LLM-based judge aligned with human experts (Kendall's τ = 0.55).&lt;/li&gt;&lt;li&gt;Demonstrates improved insight quality and reduced variance under a fixed compute budget (mean score increase from 61.64 to 65.86) by distributing refinement across stages rather than repeated temporal refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyu Gan', 'James Mooney', 'Pan Hao', 'Renxiang Wang', 'Mingyi Hong', 'Qianwen Wang', 'Dongyeop Kang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'unverifiable rewards', 'judge drift', 'robustness/stability', 'multi-agent pipeline']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22650</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining</title><link>https://arxiv.org/abs/2512.22589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes &gt;2,500 NHTSA crash records covering SAE Level 2 and Level 4 automated vehicles.&lt;/li&gt;&lt;li&gt;Proposes a two-stage data‑driven framework: K-means clustering to segment crashes into 4 behavioral clusters, followed by Association Rule Mining to extract multivariate relationships between crash patterns and contributors (lighting, surface condition, vehicle dynamics, environmental factors).&lt;/li&gt;&lt;li&gt;Aims to provide actionable insights for AV developers, safety regulators, and policymakers to inform deployment strategies and reduce crash risk.&lt;/li&gt;&lt;li&gt;Focuses on empirical crash pattern analysis using classical data‑mining methods rather than on model vulnerabilities, adversarial attacks, or red‑teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jewel Rana Palit (Traffic Engineer/Project Manager-II', 'Collier County Government', 'Traffic Management Center', '2695 Francis Ave Unit D', 'Naples', 'Fl', '37221)', 'Vijayalakshmi K Kumarasamy (Department of Computer Science and Engineering', 'University of Tennessee at Chattanooga', 'Chattanooga', 'TN', 'USA 37403)', 'Osama A. Osman (Chief Scientist', 'Center of Urban Informatics and Progress', 'Chattanooga', 'TN', 'USA 37403)']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-vehicles', 'safety-analysis', 'clustering', 'association-rule-mining', 'transportation-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22589</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</title><link>https://arxiv.org/abs/2512.22522</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies unreliable adversarial robustness evaluation in Spiking Neural Networks (SNNs) due to vanishing gradients from spike-based activations.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively shape surrogate gradients during attack iterations to mitigate gradient vanishing and improve gradient accuracy.&lt;/li&gt;&lt;li&gt;Introduces Stable Adaptive Projected Gradient Descent (SA-PGD), an L_infty attack with adaptive step size for faster, more stable convergence under imprecise gradients; demonstrates much higher attack success across training schemes, architectures, and neuron models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihang Wang', 'Dongcheng Zhao', 'Ruolin Chen', 'Qian Zhang', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'spiking neural networks (SNN)', 'surrogate gradients', 'adversarial attacks', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22522</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Decomposing Task Vectors for Refined Model Editing</title><link>https://arxiv.org/abs/2512.22511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes decomposing task vectors into shared (common) and unique components to reduce interference when combining/editing model behaviors.&lt;/li&gt;&lt;li&gt;Identifies invariant subspaces across projections to enable more precise concept manipulation and more predictable task-vector arithmetic.&lt;/li&gt;&lt;li&gt;Empirical gains: +5% multi-task merging in image classification, clean style mixing in diffusion models by mixing unique components only, and 47% toxicity reduction in language models while preserving general knowledge by negating unique toxic components.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamed Damirchi', 'Ehsan Abbasnejad', 'Zhen Zhang', 'Javen Shi']&lt;/li&gt;&lt;li&gt;Tags: ['model-editing', 'task-vectors', 'alignment-safety', 'model-interpretability', 'diffusion-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22511</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals</title><link>https://arxiv.org/abs/2512.22508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies predicting whether LLM-produced answers to a prosthodontics multiple-choice exam are correct by using metadata and hallucination signals across three prompting strategies for GPT-4o and OSS-120B.&lt;/li&gt;&lt;li&gt;Builds per-(model,prompt) correctness predictors; metadata-based methods improve accuracy up to +7.14% and reach 83.12% precision over a baseline that assumes all answers are correct.&lt;/li&gt;&lt;li&gt;Finds actual hallucination is a strong indicator of incorrectness, but metadata alone poorly predicts hallucination; prompting strategies change model internals and the usefulness of metadata signals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucky Susanto', 'Anasta Pranawijayana', 'Cortino Sukotjo', 'Soni Prasad', 'Derry Wijaya']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM reliability', 'medical AI', 'model evaluation', 'prompting strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22508</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Causality-Inspired Safe Residual Correction for Multivariate Time Series</title><link>https://arxiv.org/abs/2512.22428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework to correct residual errors of multivariate forecasters while preventing performance degradation.&lt;/li&gt;&lt;li&gt;Uses a causality-inspired encoder to decouple self- and cross-variable dynamics and a hybrid corrector to model residuals.&lt;/li&gt;&lt;li&gt;Implements a four-fold safety mechanism that enforces non-degradation (high non-degradation rates) during correction, aiming for safe deployment.&lt;/li&gt;&lt;li&gt;Empirical results show consistent accuracy improvements and ablations verify the effectiveness of the safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianxiang Xie', 'Yuncheng Hua']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'reliability', 'residual correction', 'causality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22428</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough</title><link>https://arxiv.org/abs/2512.22318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitation of probabilistic KG embeddings: entity-level variances are relation-agnostic and fail to detect novel relational contexts.&lt;/li&gt;&lt;li&gt;Proves an impossibility result showing relation-agnostic uncertainty estimators perform near-random on novel-context OOD detection; empirically validates on FB15k-237, WN18RR, YAGO3-10.&lt;/li&gt;&lt;li&gt;Proposes decomposing uncertainty into semantic (entity variance) and structural (entity-relation co-occurrence) components and introduces CAGP, a learned combination that substantially improves temporal OOD detection and selective prediction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chorok Lee']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'uncertainty estimation', 'robustness', 'knowledge graphs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22318</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</title><link>https://arxiv.org/abs/2512.22293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Warning-framed examples (e.g., 'DO NOT USE - this code is vulnerable') do not prevent language models from reproducing the warned-against content — reproduction rates were statistically similar (76.7% vs. 83.3%).&lt;/li&gt;&lt;li&gt;Sparse autoencoder analysis attributes this to overlapping latent features: representations for 'describing X' and 'performing X' are not orthogonal, so the same feature (e.g., #8684) activates in both contexts.&lt;/li&gt;&lt;li&gt;Identifies a related failure mode called 'stealth slip', where conversational preambles rotate activations into subspaces that linear probes miss, making prompting and inference-time steering ineffective.&lt;/li&gt;&lt;li&gt;Finds that training-time interventions (feature ablation) mitigate the issue, implying that statistical co-occurrence in training dominates pragmatic interpretation in current architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsogt-Ochir Enkhbayar']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'training-data-warnings', 'model-robustness', 'representation-vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22293</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Valori: A Deterministic Memory Substrate for AI Systems</title><link>https://arxiv.org/abs/2512.22280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Valori, a deterministic memory substrate that replaces floating-point embeddings/storage with fixed-point (Q16.16) arithmetic to guarantee bit-identical memory states and search results across hardware platforms.&lt;/li&gt;&lt;li&gt;Shows non-determinism can arise before indexing/retrieval due to floating-point operations and argues deterministic memory is necessary for replayability, auditability, and safe deployment in regulated settings.&lt;/li&gt;&lt;li&gt;Provides an open-source reference implementation and demonstrates cross-platform identical snapshots and retrieval behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Varshith Gudur']&lt;/li&gt;&lt;li&gt;Tags: ['determinism', 'reproducibility', 'auditability', 'robustness', 'trustworthy-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22280</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</title><link>https://arxiv.org/abs/2512.22245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces linear probes trained with a Brier-score loss on LLM hidden states to produce calibrated uncertainty estimates without additional model fine-tuning.&lt;/li&gt;&lt;li&gt;Shows probes outperform verbalized confidence and multi-generation methods in calibration while providing ≈10× computational savings.&lt;/li&gt;&lt;li&gt;Demonstrates robust generalization to unseen domains and higher accuracy on high-confidence predictions, but probes are conservative and can underperform on easier datasets.&lt;/li&gt;&lt;li&gt;Positions the method as a plug-and-play, interpretable solution for production LLM judges, useful for safety-critical deployments that prioritize low false-positive rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bhaktipriya Radharapu', 'Eshika Saxena', 'Kenneth Li', 'Chenxi Whitehouse', 'Adina Williams', 'Nicola Cancedda']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'calibration', 'LLM evaluation', 'interpretability', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22245</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</title><link>https://arxiv.org/abs/2512.22170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SoliReward, a framework to train video reward models (RMs) that reduces labeling noise and mitigates reward hacking during post-training alignment of video generation models.&lt;/li&gt;&lt;li&gt;Data strategy: collects single-item binary annotations and forms preference pairs via a cross-prompt pairing mechanism to improve label quality and cost-efficiency.&lt;/li&gt;&lt;li&gt;Architecture and loss: introduces a Hierarchical Progressive Query Attention module for better feature aggregation and a modified Bradley-Terry (BT) loss that models win-tie outcomes and regularizes positive-score distributions to reduce over-focus on top-scoring samples.&lt;/li&gt;&lt;li&gt;Empirical validation: demonstrates improvements on benchmarks for physical plausibility, subject deformity, and semantic alignment, and shows that RMs trained with SoliReward yield better outcomes when used for post-training video generation alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiesong Lian', 'Ruizhe Zhong', 'Zixiang Zhou', 'Xiaoyue Mi', 'Yixue Hao', 'Yuan Zhou', 'Qinglin Lu', 'Long Hu', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['Reward modeling', 'Alignment', 'Reward hacking', 'Annotation noise', 'Video generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22170</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses</title><link>https://arxiv.org/abs/2512.22128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a pruning framework that uses adversarial robustness evaluation to identify and remove fragile or harmful graph components (edges) to improve GNN reliability.&lt;/li&gt;&lt;li&gt;Selectively prunes edges guided by robustness scores to produce cleaner, more resilient graph representations.&lt;/li&gt;&lt;li&gt;Implements the method on three representative GNN architectures and evaluates on benchmark datasets, reporting improved defense performance in high-perturbation regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['GNN robustness', 'adversarial attacks', 'graph pruning', 'defense mechanisms', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22128</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behaviorally calibrated reinforcement learning that trains LLMs to output calibrated probabilities of correctness and to abstain or flag uncertain claims, optimizing strictly proper scoring rules rather than binary rewards.&lt;/li&gt;&lt;li&gt;Demonstrates that calibration as a transferable meta-skill can be decoupled from raw predictive accuracy, enabling smaller models (Qwen3-4B-Instruct) to outperform larger/frontier models in uncertainty quantification metrics.&lt;/li&gt;&lt;li&gt;Empirical results on math reasoning (BeyondAIME) and cross-domain factual QA (SimpleQA) show large gains in Accuracy-to-Hallucination Ratio and calibration error comparable to state-of-the-art models despite lower accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Tianle Cai', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'Calibration', 'Reinforcement learning (RL)', 'Uncertainty quantification', 'Alignment / Safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation</title><link>https://arxiv.org/abs/2512.13478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Non-Resolution Reasoning (NRR), a framework that treats ambiguity retention as a valid reasoning mode with three principles: Non-Identity, Approximate Identity, and Non-Resolution.&lt;/li&gt;&lt;li&gt;Implements NRR via Multi-Vector Embeddings, Non-Collapsing Attention, and Contextual Identity Tracking (CIT) to avoid premature semantic collapse.&lt;/li&gt;&lt;li&gt;Empirical verification using Turn 1 Entropy shows NRR-lite retains higher interpretive entropy (H = 0.63) at ambiguous turns vs standard architectures (H = 0.10), indicating delayed resolution until disambiguating context arrives.&lt;/li&gt;&lt;li&gt;Frames the problem as one of control and timing of resolution—arguing the key question is when, how, and under whose control ambiguity should be resolved.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kei Saito']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'representation-learning', 'ambiguity-preservation', 'model-architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13478</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title><link>https://arxiv.org/abs/2512.07015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies retrieval sycophancy in standard RAG systems (retrievers that preferentially surface evidence supporting a user's (possibly false) premise) and its role in hallucinations.&lt;/li&gt;&lt;li&gt;Proposes FVA-RAG, which treats an initial LLM response as a hypothesis and explicitly retrieves anti-context (counter-evidence) to falsify and verify the draft.&lt;/li&gt;&lt;li&gt;Evaluates on the full TruthfulQA-Generation benchmark under a frozen-corpora protocol; reports substantial accuracy gains (≈79.8–80.05%) over prompted Self-RAG/CRAG baselines with strong statistical significance.&lt;/li&gt;&lt;li&gt;Finds FVA-RAG triggers falsification on ~24.5–29.3% of queries, showing targeted counter-evidence retrieval reduces premise-confirming hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Ravishankara']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval sycophancy', 'RAG', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07015</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Focus Memory for Language Models</title><link>https://arxiv.org/abs/2511.12712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Focus Memory (AFM), a context management system that assigns past messages one of three fidelity levels (Full, Compressed, Placeholder) to fit a fixed token budget while preserving important constraints.&lt;/li&gt;&lt;li&gt;Decisions are based on semantic relevance, temporal decay, and importance classification; messages are packed chronologically to maintain high-fidelity constraints and let low-importance context degrade.&lt;/li&gt;&lt;li&gt;Evaluated on two safety-sensitive multi-turn dialogue benchmarks (peanut allergy and illegal tax evasion), showing substantial improvement in preserving constraints and appropriate refusals under tight context limits.&lt;/li&gt;&lt;li&gt;AFM works without changing model weights or external retrieval systems and includes an open-source implementation compatible with OpenAI-style chat APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'dialogue memory', 'constraint preservation', 'robustness', 'context management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12712</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</title><link>https://arxiv.org/abs/2511.09109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bi-RAR: a retrieval-augmented reasoning framework that evaluates intermediate reasoning steps jointly in forward and backward directions to reduce hallucinations and reward hacking.&lt;/li&gt;&lt;li&gt;Introduces a bidirectional information distance metric, grounded in Kolmogorov complexity and approximated via language model generation probabilities, to quantify information completeness of each reasoning step.&lt;/li&gt;&lt;li&gt;Optimizes reasoning with a multi-objective reinforcement learning approach using a cascading reward structure that emphasizes early trajectory alignment; shows improved performance on seven QA benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenda Wei', 'Yu-An Liu', 'Ruqing Zhang', 'Jiafeng Guo', 'Lixin Su', 'Shuaiqiang Wang', 'Dawei Yin', 'Maarten de Rijke', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmented generation', 'hallucination mitigation', 'reinforcement learning', 'alignment', 'reward hacking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09109</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes</title><link>https://arxiv.org/abs/2503.09626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UESBD, a framework for uncertainty-aware social bot detection to improve generalization and avoid overconfident OOD predictions.&lt;/li&gt;&lt;li&gt;Introduces Robust Multi-modal Neural Processes (RMNP): modality-specific encoders, unimodal attentive neural processes producing Gaussian latent variables, and a generalized product-of-experts fusion.&lt;/li&gt;&lt;li&gt;Adds an evidential gating network to model modality reliability and mitigate camouflage/conflicting modality information from bots.&lt;/li&gt;&lt;li&gt;Uses Monte Carlo sampling from the joint latent distribution for final prediction; evaluated on three real-world benchmarks showing improved classification, uncertainty estimation, and robustness to modality conflicts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wu', 'Yingguang Yang', 'hao liu', 'Hao Peng', 'Buyun He', 'Yutong Xia', 'Yong Liao']&lt;/li&gt;&lt;li&gt;Tags: ['social bot detection', 'robustness', 'uncertainty estimation', 'multimodal fusion', 'adversarial camouflage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09626</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Differentially Private Vertical Federated Learning with Zeroth-Order Optimization</title><link>https://arxiv.org/abs/2502.20565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DPZV, a communication-efficient vertical federated learning framework using zeroth-order (ZO) optimization that injects calibrated scalar-valued differential privacy (DP) noise on the server-to-device downlink.&lt;/li&gt;&lt;li&gt;Theoretically proves (ε, δ)-DP for DPZV and establishes convergence guarantees comparable to first-order DP-SGD despite relying on ZO estimators.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved privacy-utility tradeoffs and fewer communication rounds than existing DP-VFL baselines under strict privacy budgets (ε ≤ 10), while mitigating gradient-related inference attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianing Zhang', 'Evan Chen', 'Dong-Jun Han', 'Chaoyue Liu', 'Christopher G. Brinton']&lt;/li&gt;&lt;li&gt;Tags: ['vertical-federated-learning', 'differential-privacy', 'privacy-preserving-ml', 'zeroth-order-optimization', 'inference-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20565</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability</title><link>https://arxiv.org/abs/2502.09990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes X-Boundary, a method that pushes harmful feature representations away from boundary-safe representations to create an exact safety boundary, allowing precise removal of harmful behaviors without disrupting safe ones.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art defense performance against multi-turn jailbreaks while reducing over-refusal rates by ~20% and maintaining nearly full general capability.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical evidence that X-Boundary accelerates training convergence; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Dongrui Liu', 'Yi Yu', 'Luxin Xu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak defense', 'alignment', 'robustness', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09990</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</title><link>https://arxiv.org/abs/2512.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoboSafe, a runtime safety guardrail for embodied agents using executable predicate-based safety logic to intercept hazardous actions.&lt;/li&gt;&lt;li&gt;Introduces a Hybrid Long-Short Safety Memory with Backward Reflective Reasoning (revisits recent trajectories to infer temporal safety predicates and trigger replanning) and Forward Predictive Reasoning (anticipates upcoming risks from long-term memory + multimodal observations).&lt;/li&gt;&lt;li&gt;Claims substantial reduction in hazardous actions (−36.8% risk occurrence) while preserving near-original task performance; validated in simulation and on physical robotic arms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Wang', 'Zonghao Ying', 'Xiao Yang', 'Quanchen Zou', 'Zhenfei Yin', 'Tianlin Li', 'Jian Yang', 'Yaodong Yang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['Runtime safety guardrails', 'Embodied agent safety', 'Temporal risk reasoning', 'Predicate-based safety logic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21220</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRM, a multi-agent reward modeling framework that replaces a single black-box reward model with specialist evaluator agents (e.g., factuality, helpfulness, safety) whose outputs are aggregated into a timestep-wise training reward.&lt;/li&gt;&lt;li&gt;Aggregator balances step-wise correctness, multi-agent agreement, and repetition penalties; policy optimized with advantage-based updates (e.g., GAE) and a value model regressing to the aggregated reward.&lt;/li&gt;&lt;li&gt;Introduces rewardBench, a benchmark and training suite aligned to the collaborative evaluator structure to support training and assessment, aiming for improved robustness and interpretability in RLHF.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei Yang', 'Ke Zhang', 'Ji Wang', 'Xiao Chen', 'Yuxin Tang', 'Eric Yang', 'Lynn Ai', 'Bill Shi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward-modeling', 'alignment-safety', 'interpretability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16202</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LVLM-Aided Alignment of Task-Specific Vision Models</title><link>https://arxiv.org/abs/2512.21985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LVLM-Aided Visual Alignment (LVLM-VA): a bidirectional interface using a Large Vision-Language Model to translate small vision model behavior into natural language and to convert class-level human specifications into image-level critiques.&lt;/li&gt;&lt;li&gt;Enables domain experts to provide coarse (class-level) feedback that the LVLM maps to actionable image-level signals, avoiding the need for fine-grained annotations.&lt;/li&gt;&lt;li&gt;Demonstrates reductions in reliance on spurious features and group-specific biases, improving alignment with human domain knowledge on synthetic and real-world datasets.&lt;/li&gt;&lt;li&gt;Focuses on practical, computationally efficient alignment of small task-specific vision models, leveraging LVLM generalization rather than model fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Koebler', 'Lukas Kuhn', 'Ingo Thon', 'Florian Buettner']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'bias_mitigation', 'LVLM', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21985</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</title><link>https://arxiv.org/abs/2512.21917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes policy alignment to preference data without assuming a known link function; shows that realizable solutions imply a semiparametric single-index binary choice model.&lt;/li&gt;&lt;li&gt;Develops link-agnostic policy learners (profiling the link, orthogonalization, bipartite ranking) with finite-sample policy error bounds depending on index-class complexity.&lt;/li&gt;&lt;li&gt;Provides practical first-order optimization implementations for neural networks and batched data; methods are robust to unknown preference noise and scale and optimize policies directly without explicit reward fitting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Kallus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-learning', 'RLHF', 'robustness', 'semiparametric-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21917</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?</title><link>https://arxiv.org/abs/2512.21871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a 50,000 multimodal query-content benchmark to test whether large vision-language models (LVLMs) recognize and respect copyrighted material (book excerpts, news, lyrics, code docs) presented as visual inputs, with variants that include or omit copyright notices.&lt;/li&gt;&lt;li&gt;Evaluates multiple state-of-the-art LVLMs (including closed-source models) and finds significant failures to identify and comply with copyrighted content, even when copyright notices are present.&lt;/li&gt;&lt;li&gt;Proposes a tool-augmented defense framework designed to reduce infringement risk and demonstrates it improves copyright-aware behavior across evaluated scenarios.&lt;/li&gt;&lt;li&gt;Highlights the need for copyright-aware LVLM development and provides a standardized dataset and evaluation methodology for measuring compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naen Xu', 'Jinghuai Zhang', 'Changjiang Li', 'Hengyu An', 'Chunyi Zhou', 'Jun Wang', 'Boyu Xu', 'Yuyuan Li', 'Tianyu Du', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['copyright compliance', 'LVLM safety', 'safety evaluation', 'benchmarking', 'defense framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21871</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation</title><link>https://arxiv.org/abs/2512.21866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a tree-region dataset distillation method that converts random-forest leaves into axis-aligned hyperrectangles and synthesizes transactions by sampling within them to produce compact, auditable surrogate datasets.&lt;/li&gt;&lt;li&gt;Demonstrates strong utility retention (precision, micro-F1) with large data reduction and improved cross-institution performance when sharing synthesized data across clusters.&lt;/li&gt;&lt;li&gt;Evaluates privacy via membership-inference attacks (chance-level ~0.50), reports low memorization risk, and shows that removing high-uncertainty synthetic points improves AUC and calibration.&lt;/li&gt;&lt;li&gt;Emphasizes explainability (global rule statistics, per-case rule assignment with uncertainty) and positions the approach for multi-institution regulatory-audit scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Qian', 'Thorsten Neumann', 'Xueyining Huang', 'David Hardoon', 'Fei Gao', 'Yong Liu', 'Siow Mong Rick Goh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'membership-inference', 'dataset distillation', 'synthetic data', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21866</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs</title><link>https://arxiv.org/abs/2512.21849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HeartBench, a Chinese-focused benchmark to evaluate socio-emotional, cultural, and ethical (anthropomorphic) capabilities of LLMs using clinical counseling scenarios.&lt;/li&gt;&lt;li&gt;Defines a theory-driven taxonomy (5 primary dimensions, 15 secondary capabilities) and uses a rubric-based, "reasoning-before-scoring" protocol to turn human-like traits into measurable criteria.&lt;/li&gt;&lt;li&gt;Benchmarks 13 state-of-the-art LLMs, finding top models achieve ~60% of an expert-defined ideal and show marked performance drops on a difficulty-stratified "Hard Set".&lt;/li&gt;&lt;li&gt;Offers a standardized metric and methodological blueprint for building human-aligned training data and evaluating anthropomorphic intelligence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Liu', 'Peiyi Tu', 'Wenyu Chen', 'Yihong Zhuang', 'Xinxia Ling', 'Anji Zhou', 'Chenxi Wang', 'Zhuo Han', 'Zhengkai Yang', 'Junbo Zhao', 'Zenan Huang', 'Yuanyuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'benchmarking', 'LLM evaluation', 'safety', 'anthropomorphic-intelligence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21849</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought</title><link>https://arxiv.org/abs/2512.21711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes latent-token method Chain-of-Continuous-Thought (COCONUT) to determine whether latent tokens encode faithful reasoning or act as uninterpretable placeholders.&lt;/li&gt;&lt;li&gt;Performs causal steering (perturbation) experiments showing COCONUT tokens are insensitive to targeted steering and do not contain reasoning-critical information compared to explicit Chain-of-Thought (CoT) tokens.&lt;/li&gt;&lt;li&gt;Runs shortcut and out-of-distribution evaluations (MMLU, HotpotQA) demonstrating COCONUT leverages dataset artifacts and shortcuts to inflate benchmark performance, suggesting pseudo-reasoning rather than genuine reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyi Zhang', 'Boyu Tang', 'Tianjie Ju', 'Sufeng Duan', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial evaluation', 'chain-of-thought', 'latent representations', 'safety/alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21711</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers</title><link>https://arxiv.org/abs/2512.21709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study on detecting AI-generated Bengali text using five transformer models (XLM-RoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base, MultilingualBERT-Base).&lt;/li&gt;&lt;li&gt;Zero-shot performance is near chance (~50%), while fine-tuning substantially improves detection (XLM-RoBERTa, mDeBERTa, MultilingualBERT ≈ 91% accuracy/F1).&lt;/li&gt;&lt;li&gt;Finds IndicBERT underperforms after fine-tuning and emphasizes the need for task-specific models for Bengali AI-generated text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Rakibul Islam', 'Most. Sharmin Sultana Samu', 'Md. Zahid Hossain', 'Farhad Uz Zaman', 'Md. Kamrozzaman Bhuiyan']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'NLP security', 'Zero-shot vs fine-tuning', 'Language-specific evaluation', 'Misinformation forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21709</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Definition of Hallucination, Or: It's the World Model, Stupid</title><link>https://arxiv.org/abs/2512.21577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to users (i.e., outputs that conflict with an assumed reference world).&lt;/li&gt;&lt;li&gt;Surveys prior definitions and shows they map onto different choices of reference world model and knowledge conflict policy (e.g., KB vs. in-context).&lt;/li&gt;&lt;li&gt;Argues for clarity in evaluations and proposes a family of synthetic, fully specified world-model benchmarks to stress-test and improve models' world-modeling components.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emmy Liu', 'Varun Gangal', 'Chelsea Zou', 'Xiaoqi Huang', 'Michael Yu', 'Alex Chang', 'Zhuofu Tao', 'Sachin Kumar', 'Steven Y. Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety-evaluation', 'alignment', 'benchmarks', 'world-modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21577</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration of Reproducible Generated Image Detection</title><link>https://arxiv.org/abs/2512.21562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reproduces and evaluates AIGC image detection methods from 7 papers, builds a lightweight test dataset, and reimplements a representative detector.&lt;/li&gt;&lt;li&gt;Finds reproducibility issues due to omitted implementation details (preprocessing, parameter settings) and shows that core procedures can be reproduced if fully specified.&lt;/li&gt;&lt;li&gt;Demonstrates poor generalizability: detectors overfit generator-specific artifacts and performance drops sharply when preprocessing or cross-generator tests are applied.&lt;/li&gt;&lt;li&gt;Provides recommendations to improve experimental disclosure and to validate generalizability of AIGC detection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'reproducibility', 'robustness', 'generalizability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21562</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments</title><link>https://arxiv.org/abs/2512.21552</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of bidirectional human-AI alignment in education: embedding human values into AI systems while training educators/students to interpret, critique, and guide AI.&lt;/li&gt;&lt;li&gt;Analyzes risks in educational contexts (equity, privacy, student autonomy) and traces AI's shift from support tool to collaborative partner affecting roles and governance.&lt;/li&gt;&lt;li&gt;Proposes actionable strategies for policymakers, developers, and educators to promote transparency, equity, and trustworthy learning environments through ongoing mutual adaptation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Policy (book chapter)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Shen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI governance', 'privacy', 'trustworthiness', 'education']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21552</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures</title><link>https://arxiv.org/abs/2512.21551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Workshop proposal on bidirectional (reciprocal) human-AI alignment emphasizing co-adaptation between humans and AI through design, interaction, and evaluation.&lt;/li&gt;&lt;li&gt;Focuses on embedding human and societal values into AI systems, enabling humans to critically engage with and evolve alongside AI.&lt;/li&gt;&lt;li&gt;Aims to bring interdisciplinary researchers (HCI, AI, social sciences) to develop methods for interactive alignment, societal impact evaluation, and strategies for alignment in dynamic contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Shen (Cassandra)', 'Tiffany Knearem (Cassandra)', 'Divy Thakkar (Cassandra)', 'Pat Pataranutaporn (Cassandra)', 'Anoop Sinha (Cassandra)', 'Yike (Cassandra)', 'Shi', 'Jenny T. Liang', 'Lama Ahmad', 'Tanu Mitra', 'Brad A. Myers', 'Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI interaction', 'value-centered design', 'safety', 'evaluation frameworks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21551</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models</title><link>https://arxiv.org/abs/2512.21439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COMETH, a framework combining probabilistic context clustering with LLM-based semantic preprocessing and human ternary judgments to model how context affects moral evaluations.&lt;/li&gt;&lt;li&gt;Curates a dataset of 300 textual scenarios across six core actions with judgments from 101 participants and uses LLM filtering plus embeddings and clustering to standardize actions.&lt;/li&gt;&lt;li&gt;Learns interpretable, action-specific contextual features and weights in a likelihood-based model, achieving substantially higher alignment with majority human judgments than end-to-end LLM prompting.&lt;/li&gt;&lt;li&gt;Emphasizes transparency and explainability by extracting concise binary contextual features that drive predictions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Geoffroy Morlat', 'Marceau Nahon', 'Augustin Chartouny', 'Raja Chatila', 'Ismael T. Freire', 'Mehdi Khamassi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'moral reasoning', 'interpretability', 'LLMs', 'human evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21439</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Teaching People LLM's Errors and Getting it Right</title><link>https://arxiv.org/abs/2512.21422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that systematic LLM failure patterns exist by grouping instances by meta-labels and identifying sizable groups where the model is error-prone.&lt;/li&gt;&lt;li&gt;Evaluates prompting and embedding-based methods for automatically surfacing these failure patterns and finds mixed effectiveness, which may explain prior negative results.&lt;/li&gt;&lt;li&gt;Proposes a new metric focused on a user's ability to anticipate LLM errors using taught failure patterns and demonstrates a positive effect in a user study using this metric.&lt;/li&gt;&lt;li&gt;Concludes that teaching failure patterns can mitigate overreliance but requires better automated discovery methods and appropriate evaluation metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Stringham', 'Fateme Hashemi Chaleshtori', 'Xinyuan Yan', 'Zhichao Xu', 'Bei Wang', "Ana Marasovi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'human-AI interaction', 'failure-pattern discovery', 'user study']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21422</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors</title><link>https://arxiv.org/abs/2512.21404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LAMLAD, a dual-agent LLM-based framework (manipulator + analyzer) that generates realistic, functionality-preserving feature-level perturbations to evade ML-based Android malware detectors.&lt;/li&gt;&lt;li&gt;Integrates retrieval-augmented generation (RAG) to improve contextual awareness and efficiency when crafting Drebin-style feature perturbations.&lt;/li&gt;&lt;li&gt;Evaluates against three ML-based Android detectors, reporting up to 97% attack success rate and an average of three attempts per adversarial sample; compares favorably to two prior attack methods.&lt;/li&gt;&lt;li&gt;Proposes an adversarial training defense that decreases the attack success rate by &gt;30% on average, improving robustness against LAMLAD-style attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianwei Lan', 'Farid Na\\"it-Abdesselam']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial ML', 'LLM-powered attacks', 'Android malware detection', 'Feature-level evasion', 'Adversarial training (defense)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21404</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks</title><link>https://arxiv.org/abs/2512.21345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Query Carefully: a pipeline that integrates LLM-based text-to-SQL generation with explicit detection and handling of unanswerable/ambiguous queries to avoid misleading executable SQL outputs.&lt;/li&gt;&lt;li&gt;Introduces OncoMX-NAQ, an 80-question no-answer dataset across 8 categories (non-SQL, out-of-schema/domain, missing columns, missing values, column ambiguity, etc.) built on the OncoMX component of ScienceBenchmark.&lt;/li&gt;&lt;li&gt;Uses llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples (both answerable and unanswerable) and evaluates SQL exact match, result accuracy, and unanswerable-detection accuracy.&lt;/li&gt;&lt;li&gt;Finds balanced prompting yields best unanswerable detection (0.8 overall) with near-perfect detection for structural categories but poor performance on missing-value and column-ambiguity cases; includes a lightweight UI to surface SQL, results, and abstentions for transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jasmin Saxer (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Isabella Maria Aigner (Institute of Medical Virology', 'University of Zurich', 'Zurich', 'Switzerland)', 'Luise Linzmeier (Department of Gastroenterology and Hepatology', 'University Hospital Zurich', 'University of Zurich', 'Zurich', 'Switzerland)', 'Andreas Weiler (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Kurt Stockinger (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'text-to-SQL', 'prompting', 'biomedical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21345</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets</title><link>https://arxiv.org/abs/2512.21775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Compliance Rating Scheme (CRS), a framework to evaluate dataset compliance with transparency, accountability, and security principles for generative AI datasets.&lt;/li&gt;&lt;li&gt;Provides an open-source Python library leveraging data provenance technology to assess existing datasets and to guide responsible data collection and scraping.&lt;/li&gt;&lt;li&gt;Aims to preserve provenance metadata as datasets are shared and modified, improving traceability and responsible dataset construction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matyas Bohacek', 'Ignacio Vilanova Echavarri']&lt;/li&gt;&lt;li&gt;Tags: ['data-provenance', 'dataset-compliance', 'dataset-transparency', 'AI-safety', 'dataset-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21775</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning</title><link>https://arxiv.org/abs/2512.21699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a consensus-driven agent architecture where multiple heterogeneous LLM and VLM agents independently produce candidate outputs with explicit uncertainty and disagreement.&lt;/li&gt;&lt;li&gt;Introduces a centralized reasoning agent that consolidates outputs, enforces safety and policy constraints, mitigates hallucinations and bias, and generates auditable, evidence-backed decisions.&lt;/li&gt;&lt;li&gt;Explainability is provided via preserved intermediate outputs and cross-model comparisons; responsibility is enforced through reasoning-layer governance and agent-level constraints.&lt;/li&gt;&lt;li&gt;Evaluates the architecture on real-world agentic workflows and reports improvements in robustness, transparency, and operational trust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eranga Bandara', 'Tharaka Hewa', 'Ross Gore', 'Sachin Shetty', 'Ravi Mukkamala', 'Peter Foytik', 'Abdul Rahman', 'Safdar H. Bouk', 'Xueping Liang', 'Amin Hass', 'Sachini Rajapakse', 'Ng Wee Keong', 'Kasun De Zoysa', 'Aruna Withanage', 'Nilaan Loganathan']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'explainability', 'safety', 'robustness', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21699</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning</title><link>https://arxiv.org/abs/2512.21583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal diagnostic framework (built on LLaVA) that combines vision-language alignment with logic-tree, stepwise reasoning to reduce hallucinations and improve interpretability.&lt;/li&gt;&lt;li&gt;System components: text/image encoder, cross-modal projection, reasoning controller that decomposes tasks, and a logic tree generator that assembles verifiable conclusions.&lt;/li&gt;&lt;li&gt;Evaluated on MedXpertQA and other benchmarks; reports improved diagnostic accuracy and more interpretable reasoning traces while remaining competitive on text-only tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zelin Zang', 'Wenyi Gu', 'Siqi Ma', 'Dan Yang', 'Yue Shen', 'Zhu Zhang', 'Guohui Fan', 'Wing-Kuen Ling', 'Fuji Yang']&lt;/li&gt;&lt;li&gt;Tags: ['model safety', 'hallucination mitigation', 'interpretability', 'alignment', 'multimodal diagnostics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21583</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis</title><link>https://arxiv.org/abs/2512.21482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LogicLens, a unified visual-textual co-reasoning framework for detecting and explaining text-centric image forgeries using a Cross-Cues-aware Chain of Thought (CCT).&lt;/li&gt;&lt;li&gt;Introduces PR2 (Perceiver, Reasoner, Reviewer) multi-agent pipeline to generate high-quality annotations and builds RealText, a dataset of 5,397 images with pixel-level segmentation, textual explanations, and authenticity labels.&lt;/li&gt;&lt;li&gt;Optimizes a weighted multi-task reward (GRPO) to jointly improve detection, grounding, and explanation; demonstrates strong empirical gains (large improvements in F1 vs. prior specialized frameworks and GPT-4o).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanwei Zeng', 'Changtao Miao', 'Jing Huang', 'Zhiya Tan', 'Shutao Gong', 'Xiaoming Yu', 'Yang Wang', 'Huazhe Tan', 'Weibin Yao', 'Jianshu Li']&lt;/li&gt;&lt;li&gt;Tags: ['forgery detection', 'multimodal reasoning', 'explainability', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21482</guid><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 04 Dec 2025 23:46:57 +0000</lastBuildDate><item><title>Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</title><link>https://arxiv.org/abs/2512.02787</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ViFailback, a framework that diagnoses robotic manipulation failures and provides textual and visual correction guidance via explicit visual symbols.&lt;/li&gt;&lt;li&gt;Releases ViFailback dataset: 58,126 VQA pairs paired with 5,202 real-world manipulation trajectories, and proposes ViFailback-Bench (Lite and Hard) to evaluate failure diagnosis and correction.&lt;/li&gt;&lt;li&gt;Presents ViFailback-8B (a VLM) and demonstrates integration with a Vision-Language-Action (VLA) model to improve real-world recovery from manipulation failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianchao Zeng', 'Xinyu Zhou', 'Youcheng Li', 'Jiayou Shi', 'Tianle Li', 'Liangming Chen', 'Lei Ren', 'Yong-Lu Li']&lt;/li&gt;&lt;li&gt;Tags: ['robotic-safety', 'failure-diagnosis', 'vision-language-models', 'dataset', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02787</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail</title><link>https://arxiv.org/abs/2509.23762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that certain SNN architectures exhibit natural gradient sparsity and can achieve strong adversarial defense without explicit regularization.&lt;/li&gt;&lt;li&gt;Identifies a trade-off between gradient sparsity (which improves adversarial robustness) and generalization (which suffers under high sparsity).&lt;/li&gt;&lt;li&gt;Analyzes the dual role of gradient sparsity in SNN training and its implications for adversarial resilience on vision tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luu Trong Nhan', 'Luu Trung Duong', 'Pham Ngoc Nam', 'Truong Cong Thang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'spiking-neural-networks', 'gradient-sparsity', 'robustness-vs-accuracy', 'adversarial-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23762</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism</title><link>https://arxiv.org/abs/2507.01513</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that a very small fraction (&lt;1%) of multimodal tokens in early-to-middle layers trigger jailbreaks in MLLMs.&lt;/li&gt;&lt;li&gt;Proposes SafePTR, a training-free prune-then-restore token-level defense that selectively removes harmful tokens at vulnerable layers and restores benign features later.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art mitigation of multimodal jailbreaks across three MLLMs and five benchmarks without extra training or utility loss and with minimal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Beitao Chen', 'Xinyu Lyu', 'Lianli Gao', 'Jingkuan Song', 'Heng Tao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal jailbreak', 'LLM safety', 'token-level defense', 'adversarial mitigation', 'jailbreak defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01513</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defense That Attacks: How Robust Models Become Better Attackers</title><link>https://arxiv.org/abs/2512.02830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that adversarially trained (AT) models produce adversarial perturbations that transfer more effectively to other models than perturbations from standard models.&lt;/li&gt;&lt;li&gt;Empirically evaluates transferability across a diverse zoo of 36 models (CNNs and ViTs), demonstrating a paradox where robustness training increases the model's ability to be a strong attacker.&lt;/li&gt;&lt;li&gt;Releases models and code for reproducibility and argues robustness evaluations should measure both resistance to transferred attacks and propensity to generate transferable adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Awad', 'Mahmoud Akrm', 'Walid Gomaa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial ML', 'adversarial training', 'transferability', 'robustness', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02830</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints</title><link>https://arxiv.org/abs/2512.00882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a three-stage, parameter-efficient framework (Look, Recite, Then Answer) that generates self-derived knowledge hints to improve VLM performance while keeping backbone models frozen.&lt;/li&gt;&lt;li&gt;Targets and mitigates "Reasoning-Driven Hallucination" and the "Modality Gap" by converting visual cues into targeted parametric queries via a lightweight router and aligning evidence to select labels.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains on a domain benchmark (AgroBench), notably improving weed identification accuracy and surpassing larger models without external search.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xisheng Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'robustness', 'vision-language models', 'multimodal alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00882</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches</title><link>https://arxiv.org/abs/2512.00765</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TESP-Attack: a stealth-aware physical adversarial patch method that places edge-aligned patches on traffic signs using instance segmentation and a U-Net generator.&lt;/li&gt;&lt;li&gt;Optimizes patches with color/texture constraints and frequency-domain analysis to visually blend with sign edges and background, reducing human detectability.&lt;/li&gt;&lt;li&gt;Demonstrates &gt;90% attack success under limited query budgets, strong cross-model transferability, and robust real-world performance across different angles and distances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haojie Ji', 'Te Hu', 'Haowen Li', 'Long Jin', 'Chongshi Xin', 'Yuchi Yao', 'Jiarui Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-attack', 'adversarial-patch', 'traffic-signs', 'robustness', 'stealth']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00765</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</title><link>https://arxiv.org/abs/2511.22826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MMA-Bench (video-based tasks) to evaluate MLLMs' reliance on and robustness to contradicting/misaligned modalities.&lt;/li&gt;&lt;li&gt;Applies black-box and white-box interpretability to show brittleness in open- and closed-source MLLMs under misaligned audio-visual pairs and misleading text.&lt;/li&gt;&lt;li&gt;Proposes a modality alignment tuning strategy that teaches models when to prioritize, leverage, or ignore specific modality cues and demonstrates improved multimodal grounding in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Chen', 'Chaitanya Chakka', 'Arjun Reddy Akula', 'Xavier Thomas', 'Deepti Ghadiyaram']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'adversarial/misalignment testing', 'interpretability', 'alignment tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22826</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title><link>https://arxiv.org/abs/2510.18214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLSU, a large-scale benchmark (8,187 samples, 15 harm categories) for fine-grained multimodal (image+text) safety evaluation with human annotation and 17 safety patterns.&lt;/li&gt;&lt;li&gt;Shows systematic failures of state-of-the-art models on joint image-text safety classification: high unimodal accuracy (90%+) but sharp drop to 20–55% on combinatorial/joint cases, with 34% of joint errors despite correct unimodal classifications.&lt;/li&gt;&lt;li&gt;Analyzes trade-offs in refusal behavior (over-blocking vs under-refusal) and demonstrates how instruction framing can change blocking/refusal rates, highlighting alignment gaps and missing compositional reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shruti Palaskar', 'Leon Gatys', 'Mona Abdelrahman', 'Mar Jacobo', 'Larry Lindsey', 'Rutika Moharir', 'Gunnar Lund', 'Yang Xu', 'Navid Shiee', 'Jeffrey Bigham', 'Charles Maalouf', 'Joseph Yitan Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-safety', 'vision-language', 'safety-benchmark', 'compositional-reasoning', 'alignment-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18214</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes</title><link>https://arxiv.org/abs/2510.03747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoRA patching: a plug-and-play Low-Rank Adaptation injected into deepfake generators to bypass proactive adversarial perturbation defenses.&lt;/li&gt;&lt;li&gt;Proposes a learnable gating mechanism to stabilize fine-tuning and a Multi-Modal Feature Alignment (MMFA) loss to align adversarial outputs semantically with desired outputs.&lt;/li&gt;&lt;li&gt;Demonstrates successful defense bypass with only 1,000 facial examples and one epoch of fine-tuning across multiple proactive defenses, and additionally proposes a defensive LoRA variant that embeds visible warnings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zuomin Qu', 'Yimao Guo', 'Qianyue Hu', 'Wei Lu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfakes', 'adversarial attacks', 'defense evasion', 'LoRA/model patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03747</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BitMark: Watermarking Bitwise Autoregressive Image Generative Models</title><link>https://arxiv.org/abs/2506.21209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BitMark, a bitwise watermarking framework that embeds human-imperceptible signals directly into the bit-level token stream of bitwise autoregressive image generative models.&lt;/li&gt;&lt;li&gt;Design balances visual fidelity and generation speed while remaining robust to a range of watermark removal techniques.&lt;/li&gt;&lt;li&gt;Exhibits high 'radioactivity': watermarked images used as training data cause downstream generative models (including after fine-tuning diffusion or autoregressive models) to reproduce detectable watermark traces.&lt;/li&gt;&lt;li&gt;Aimed at enabling detection of generated outputs to mitigate dataset contamination and model collapse; implementation/code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Louis Kerner', 'Michel Meintz', 'Bihe Zhao', 'Franziska Boenisch', 'Adam Dziedzic']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'generative-models', 'model-robustness', 'dataset-provenance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21209</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can VLMs Detect and Localize Fine-Grained AI-Edited Images?</title><link>https://arxiv.org/abs/2505.15644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FragFake, a large-scale benchmark of AI-edited (localized) images covering multiple sources, editors, and edit types to enable detection and localization of edits without costly pixel-level annotation.&lt;/li&gt;&lt;li&gt;Systematically evaluates pretrained and fine-tuned vision-language models (VLMs) for edited-image classification and edited-region localization, finding pretrained VLMs perform poorly while fine-tuned models (e.g., Qwen2.5-VL) achieve strong accuracy and object-level precision.&lt;/li&gt;&lt;li&gt;Explores training strategies (including GRPO-based RLVR and LoRA) and ablations on data balancing, training size, and cross-editor/dataset generalization, highlighting strengths and limitations for multimodal content-authenticity tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhen Sun', 'Ziyi Zhang', 'Zeren Luo', 'Zhiyuan Zhong', 'Zeyang Sha', 'Tianshuo Cong', 'Zheng Li', 'Shiwen Cui', 'Weiqiang Wang', 'Jiaheng Wei', 'Xinlei He', 'Qi Li', 'Qian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'content authenticity', 'benchmark/dataset', 'vision-language models', 'edit localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15644</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Test-time Correction: An Online 3D Detection System via Visual Prompting</title><link>https://arxiv.org/abs/2412.07768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Test-time Correction (TTC), an online system that rectifies 3D detection errors at inference using auxiliary feedback without retraining.&lt;/li&gt;&lt;li&gt;Introduces an Online Adapter (OA) that generates prompt-driven queries from visual prompts (image-based descriptions) derived from mismatches, road descriptions, or user clicks.&lt;/li&gt;&lt;li&gt;Maintains a visual prompt buffer of risky objects to enable continuous correction across future frames, improving detection under limited labels, zero-shot settings, and adverse conditions.&lt;/li&gt;&lt;li&gt;Aims to enhance deployed autonomous driving safety by enabling immediate, adaptive error correction for frozen 3D detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanxue Zhang', 'Zetong Yang', 'Yanan Sun', 'Li Chen', 'Fei Xia', 'Fatma G\\"uney', 'Hongyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'test-time-adaptation', 'safety', 'robustness', 'online-correction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07768</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Margin-aware Preference Optimization for Aligning Diffusion Models without Reference</title><link>https://arxiv.org/abs/2406.06424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Margin-aware Preference Optimization (MaPO), a reference-agnostic method that directly optimizes likelihood margins between preferred and dispreferred T2I outputs under a Bradley–Terry formulation.&lt;/li&gt;&lt;li&gt;Identifies and empirically characterizes 'reference mismatch' when using reference-regularized methods (e.g., DPO) for adapting diffusion models, showing it hurts adaptation for style transfer, personalization, etc.&lt;/li&gt;&lt;li&gt;Validates MaPO across five domains—including safe generation, style adaptation, cultural representation, personalization, and general preference alignment—showing improved performance and ~15% reduced training time versus DPO and specialized methods like DreamBooth.&lt;/li&gt;&lt;li&gt;Frames diverse T2I adaptation tasks as unified pairwise preference optimization and emphasizes memory-efficiency and stronger gains as reference mismatch increases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Hong', 'Sayak Paul', 'Noah Lee', 'Kashif Rasul', 'James Thorne', 'Jongheon Jeong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'diffusion models', 'preference optimization', 'safe generation', 'personalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.06424</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</title><link>https://arxiv.org/abs/2512.03992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DIQ-H, a benchmark for evaluating vision-language model (VLM) robustness to temporal visual degradation (motion blur, sensor noise, compression) and measuring hallucination persistence, error recovery, and temporal consistency.&lt;/li&gt;&lt;li&gt;Proposes Uncertainty-Guided Iterative Refinement (UIR) to generate scalable pseudo-ground-truth annotations via lightweight VLMs with uncertainty filtering, improving annotation accuracy by ~15.3%.&lt;/li&gt;&lt;li&gt;Evaluates 16 state-of-the-art VLMs on multi-turn QA over degraded video sequences, showing significant robustness gaps (e.g., GPT-4o recovery rate 78.5%, many open models &lt;60%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zexin Lin', 'Hawen Wan', 'Yebin Zhong', 'Xiaoqiang']&lt;/li&gt;&lt;li&gt;Tags: ['VLM robustness', 'hallucination persistence', 'temporal degradation', 'benchmarking', 'safety-critical deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03992</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2512.03749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SelfDebias, a fully unsupervised, test-time debiasing method for text-to-image diffusion models using a UNet noise predictor.&lt;/li&gt;&lt;li&gt;Identifies semantic clusters in an image-encoder embedding space and steers sampling to minimize KL divergence between the output distribution and a uniform distribution over clusters.&lt;/li&gt;&lt;li&gt;Requires no human annotations or external classifiers, generalizes across prompts and diffusion architectures (conditional and unconditional), and preserves visual fidelity while reducing demographic and abstract concept biases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Korada Sri Vardhana', 'Shrikrishna Lolla', 'Soma Biswas']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'model safety', 'text-to-image diffusion', 'unsupervised debiasing', 'inference-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03749</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Thinking with Programming Vision: Towards a Unified View for Thinking with Images</title><link>https://arxiv.org/abs/2512.03746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies brittleness of state-of-the-art multimodal LLMs to simple orientation changes and natural image corruptions, motivating more robust tool-based reasoning.&lt;/li&gt;&lt;li&gt;Proposes CodeVision: a code-as-tool framework where the model generates code to invoke arbitrary image operations rather than relying on a fixed tool registry.&lt;/li&gt;&lt;li&gt;Uses a two-stage training pipeline: supervised fine-tuning on a curated multi-turn tool composition/error-recovery dataset, followed by reinforcement learning with a dense process reward to encourage strategic, efficient tool use.&lt;/li&gt;&lt;li&gt;Introduces new SFT/RL datasets and a benchmark focused on robustness to orientation changes and multi-tool reasoning; reports improved performance and emergent capabilities on Qwen2.5-VL and Qwen3-VL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirun Guo', 'Minjie Hong', 'Feng Zhang', 'Kai Jia', 'Tao Jin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal-llm', 'tool-usage/code-as-tool', 'reinforcement-learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03746</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Out-of-the-box: Black-box Causal Attacks on Object Detectors</title><link>https://arxiv.org/abs/2512.03730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BlackCAtt, a black-box algorithm that finds minimal, causally sufficient pixel sets to craft imperceptible adversarial perturbations against object detectors.&lt;/li&gt;&lt;li&gt;Combines identified causal pixels with detector bounding boxes to cause removal, modification, or addition of detections without access to model internals, across architectures and sizes.&lt;/li&gt;&lt;li&gt;Empirical results on COCO show substantially higher success rates than baseline black-box methods (2.7x removal, 3.86x modification, 5.75x triggering spurious detections) while keeping perturbations visually close to originals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melane Navaratnarajah', 'David A. Kelly', 'Hana Chockler']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'black-box-attacks', 'object-detection', 'causal-analysis', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03730</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</title><link>https://arxiv.org/abs/2512.03667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Colon-X and ColonVQA, a large multimodal colonoscopy dataset (1.1M+ VQA entries) covering 76 clinical findings and 18 tasks.&lt;/li&gt;&lt;li&gt;Evaluates 22 multimodal large language models for generalizability and reliability under human-induced perturbations, finding clinical outputs are not robust/trustworthy.&lt;/li&gt;&lt;li&gt;Creates ColonReason, a clinician-annotated reasoning dataset, and develops ColonR1, a reasoning-focused model that improves accuracy under data-scarce settings versus supervised fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ge-Peng Ji', 'Jingyi Liu', 'Deng-Ping Fan', 'Nick Barnes']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'multimodal-LLM-evaluation', 'clinical-AI', 'dataset-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03667</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features</title><link>https://arxiv.org/abs/2512.03625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FeatureLens: an Image Feature Extractor (IFE) plus lightweight shallow classifiers (SVM/MLP/XGBoost) to detect adversarial image examples.&lt;/li&gt;&lt;li&gt;Uses only 51-dimensional features and small models (1k–30k parameters) while claiming high detection accuracy (97.8%–99.75% closed-set; 86.17%–99.6% generalization) across FGSM, PGD, CW, and DAmageNet attacks.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability, strong generalization to unseen attacks, and computational efficiency as advantages for adversarial defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhigang Yang', 'Yuan Liu', 'Jiawei Zhang', 'Puning Zhang', 'Xinqiang Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'robustness', 'interpretability', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03625</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes</title><link>https://arxiv.org/abs/2512.03580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DOT-BI, a dynamic visual test that hides a number via background-matched pixel texture and makes it perceptible only through relative motion/scale across frames to distinguish humans from automated systems.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art video-capable multimodal models (GPT-5-Thinking, Gemini 2.5 Pro) which fail to recover the hidden value, while human participants solved the test at 99.5% success with ~10.7s average completion time.&lt;/li&gt;&lt;li&gt;Provides user-study evidence showing no negative usability impact and releases code plus 100+ pre-rendered test variants to support adoption in surveys and online processes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Malte Bleeker', 'Mauro Gotsch']&lt;/li&gt;&lt;li&gt;Tags: ['bot-detection', 'model-robustness', 'multimodal-models', 'human-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03580</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title><link>https://arxiv.org/abs/2512.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a production-scale hybrid moderation system combining supervised classifiers for known violations with reference-based similarity matching to catch novel/edge cases.&lt;/li&gt;&lt;li&gt;Multimodal inputs (text, audio, video) are processed and enhanced via an MLLM that distills knowledge into lightweight inference pipelines.&lt;/li&gt;&lt;li&gt;Reports production metrics (classification: 67% recall @ 80% precision; similarity: 76% recall @ 80% precision) and A/B test results showing a 6–8% reduction in user views of unwanted livestreams.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Chee Yew', 'Hailun Xu', 'Sanjay Saha', 'Xiaotian Fan', 'Hiok Hian Ong', 'David Yuchen Wang', 'Kanchan Sarkar', 'Zhenheng Yang', 'Danhui Guan']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'multimodal-ml', 'safety', 'production-system', 'MLLM-augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03553</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention</title><link>https://arxiv.org/abs/2512.03542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses hallucinations in multimodal LLMs by detecting 'visual neglect' (when models fail to sufficiently attend to input images) and intervening only when neglect is detected.&lt;/li&gt;&lt;li&gt;Proposes V-ITI: a Visual Neglect Detector using head-level discriminative probes to identify neglect, and a Visual Recall Intervenor that modulates activations with pre-stored visual activation information at inference time.&lt;/li&gt;&lt;li&gt;Demonstrates consistent reduction in vision-related hallucinations across eight benchmarks and multiple MLLM families while preserving general task performance and reducing unnecessary interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nan Sun', 'Zhenyu Zhang', 'Xixun Lin', 'Kun Wang', 'Yanmin Shang', 'Naibin Gu', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Haifeng Wang', 'Yanan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'MLLM safety', 'inference-time intervention', 'visual neglect detection', 'robustness/alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03542</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration</title><link>https://arxiv.org/abs/2512.03345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluGen, a diffusion-based framework to synthesize realistic, controllable hallucinations (type, location, severity) for image restoration tasks.&lt;/li&gt;&lt;li&gt;Builds a large-scale annotated hallucination dataset (4,350 images from 1,450 brain MRIs) for low-field MRI enhancement to enable systematic evaluation.&lt;/li&gt;&lt;li&gt;Proposes SHAFE, a feature-based metric with soft-attention pooling that improves sensitivity to semantic hallucinations, and trains reference-free detectors that generalize to real restoration failures.&lt;/li&gt;&lt;li&gt;Positions HalluGen and the dataset as a scalable foundation for evaluating and mitigating hallucinations in safety-critical image restoration (e.g., medical imaging).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seunghoi Kim', 'Henry F. J. Tregidgo', 'Chen Jin', 'Matteo Figini', 'Daniel C. Alexander']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'image restoration', 'safety', 'medical imaging', 'benchmarking/dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03345</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Drainage: A Unifying Framework for Addressing Class Uncertainty</title><link>https://arxiv.org/abs/2512.03182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'drainage node' appended to network outputs to reallocate probability mass toward uncertainty, allowing the model to absorb ambiguous, corrupted, or out-of-distribution samples.&lt;/li&gt;&lt;li&gt;Maintains end-to-end differentiability and is compatible with standard training; claimed denoising effect where the drainage neuron captures mislabeled/outlier data and stabilizes decision boundaries.&lt;/li&gt;&lt;li&gt;Empirical gains on noisy-label benchmarks (CIFAR-10/100 with instance-dependent/asymmetric noise) and real-world datasets (mini-WebVision, mini-ImageNet, Clothing-1M), with up to ~9% accuracy improvement in high-noise regimes.&lt;/li&gt;&lt;li&gt;Extends beyond classification to applications in web-scale/semi-supervised dataset cleaning and open-set detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yasser Taha', "Gr\\'egoire Montavon", 'Nils K\\"orber']&lt;/li&gt;&lt;li&gt;Tags: ['noisy-labels', 'robustness', 'out-of-distribution-detection', 'dataset-cleaning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03182</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</title><link>https://arxiv.org/abs/2511.00926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the AI Self-Awareness Index (AISAI), a game-theoretic framework to measure models' ability to differentiate strategic reasoning by opponent type using the 'Guess 2/3 of Average' game.&lt;/li&gt;&lt;li&gt;Evaluates 28 LLMs across 4,200 trials with three opponent framings (humans, other AIs, and 'AI models like you').&lt;/li&gt;&lt;li&gt;Finds emergent self-awareness in advanced models (21/28) and a consistent rationality ranking: Self &gt; Other AIs &gt; Humans.&lt;/li&gt;&lt;li&gt;Argues implications for AI alignment, human-AI collaboration, and understanding how models attribute capabilities to humans and other AIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyung-Hoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'emergent-capabilities', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00926</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title><link>https://arxiv.org/abs/2510.18214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLSU, a large-scale benchmark (8,187 samples, 15 harm categories) for fine-grained multimodal (image+text) safety evaluation with human annotation and 17 safety patterns.&lt;/li&gt;&lt;li&gt;Shows systematic failures of state-of-the-art models on joint image-text safety classification: high unimodal accuracy (90%+) but sharp drop to 20–55% on combinatorial/joint cases, with 34% of joint errors despite correct unimodal classifications.&lt;/li&gt;&lt;li&gt;Analyzes trade-offs in refusal behavior (over-blocking vs under-refusal) and demonstrates how instruction framing can change blocking/refusal rates, highlighting alignment gaps and missing compositional reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shruti Palaskar', 'Leon Gatys', 'Mona Abdelrahman', 'Mar Jacobo', 'Larry Lindsey', 'Rutika Moharir', 'Gunnar Lund', 'Yang Xu', 'Navid Shiee', 'Jeffrey Bigham', 'Charles Maalouf', 'Joseph Yitan Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-safety', 'vision-language', 'safety-benchmark', 'compositional-reasoning', 'alignment-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18214</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title><link>https://arxiv.org/abs/2508.09442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes privacy risks of KV-cache in LLM inference and demonstrates that sensitive user inputs can be reconstructed from the cache.&lt;/li&gt;&lt;li&gt;Designs three attack vectors: Inversion Attack, Collision Attack, and semantic Injection Attack, showing practical privacy leakage.&lt;/li&gt;&lt;li&gt;Proposes KV-Cloak, a reversible matrix-based obfuscation with operator fusion that defends the KV-cache with minimal accuracy loss and low performance overhead.&lt;/li&gt;&lt;li&gt;Presents extensive experiments showing the attacks' effectiveness and that KV-Cloak reduces reconstruction quality to random noise while preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifan Luo', 'Shuo Shao', 'Su Zhang', 'Lijing Zhou', 'Yuke Hu', 'Chenxu Zhao', 'Zhihao Liu', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'LLM inference', 'cache-based attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09442</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title><link>https://arxiv.org/abs/2508.03772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes two failure modes of Group Relative Policy Optimization (GRPO): token-level penalization (conflicting negative feedback on shared valuable tokens) and policy collapse (entropy-driven destabilization from penalizing confident completions).&lt;/li&gt;&lt;li&gt;Proposes GTPO (Group-relative Trajectory-based Policy Optimization): skip negative updates on valuable tokens while amplifying positive updates, and filter completions whose entropy exceeds a provable threshold to prevent policy collapse.&lt;/li&gt;&lt;li&gt;Removes need for KL-divergence regularization and a reference model, provides theoretical justification for the entropy filter, and reports empirical improvements on reasoning benchmarks (GSM8K, MATH, AIME 2024/2025, AMC 2023).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Simoni', 'Aleksandar Fontana', 'Giulio Rossolini', 'Andrea Saracino', 'Paolo Mori']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'policy optimization', 'training stability', 'RLHF / reward-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03772</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems</title><link>https://arxiv.org/abs/2509.10746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RECAP, an inference-time, modular prompting framework (Reflect-Extract-Calibrate-Align-Produce) to improve emotional reasoning in medical dialogue without retraining models.&lt;/li&gt;&lt;li&gt;Decomposes patient input into appraisal-theoretic stages, extracts psychological factors, assigns Likert emotion likelihoods that clinicians can inspect or override, and produces auditable, empathic responses.&lt;/li&gt;&lt;li&gt;Reports substantial gains on emotion-reasoning benchmarks (22–28% on 8B models, 10–13% on larger models) and clinician evaluations showing improved empathy and context-appropriateness.&lt;/li&gt;&lt;li&gt;Emphasizes transparency, clinician oversight, and accountability for clinical deployment rather than addressing adversarial attacks or red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adarsh Srinivasan', 'Jacob Dineen', 'Muhammad Umar Afzal', 'Muhammad Uzair Sarfraz', 'Irbaz B. Riaz', 'Ben Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'interpretability', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10746</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering</title><link>https://arxiv.org/abs/2508.08785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy-protected RAG for KG-backed QA where entities are anonymized so LLMs cannot access entity semantics.&lt;/li&gt;&lt;li&gt;Identifies two challenges: converting anonymous entities into retrievable representations and retrieving question-relevant anonymous entities.&lt;/li&gt;&lt;li&gt;Proposes ARoG with relation-centric abstraction (capture semantics from adjacent relations) and structure-oriented abstraction (map questions to abstract concept paths) to enable private retrieval; shows empirical gains and privacy-robustness on three datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunfeng Ning', 'Mayi Xu', 'Jintao Wen', 'Qiankun Pi', 'Yuanyuan Zhu', 'Ming Zhong', 'Jiawei Jiang', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmented-generation', 'privacy-preserving', 'knowledge-graph', 'LLM privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08785</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2508.05452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLMEval-3, a dynamic evaluation framework using a proprietary bank of 220k graduate-level questions to sample fresh, contamination-resistant test sets per run.&lt;/li&gt;&lt;li&gt;Implements contamination-resistant data curation, a novel anti-cheating architecture, and an LLM-as-judge workflow calibrated to 90% agreement with human experts to maintain evaluation integrity.&lt;/li&gt;&lt;li&gt;Presents a 20-month longitudinal study of ~50 models showing a memorization performance ceiling and dataset-contamination vulnerabilities that static benchmarks fail to detect; demonstrates robust ranking stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Methodology&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Zhang', 'Yujiong Shen', 'Jingyi Deng', 'Yuhui Wang', 'Yue Zhang', 'Junzhe Wang', 'Shichun Liu', 'Shihan Dou', 'Huayu Sha', 'Qiyuan Peng', 'Changhao Jiang', 'Jingqi Tong', 'Yilong Wu', 'Zhihao Zhang', 'Mingqi Wu', 'Zhiheng Xi', 'Mingxu Chai', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation', 'benchmarks', 'data-contamination', 'robustness', 'integrity/anti-cheating']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05452</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences</title><link>https://arxiv.org/abs/2506.00195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical user study (480 participants, 3,840 query–response pairs) evaluating how different LLM refusal strategies affect user perceptions across varying user motivations.&lt;/li&gt;&lt;li&gt;Finds that response strategy strongly shapes user experience while actual user motivation has negligible effect; partial compliance (high-level info without actionable details) substantially reduces negative perceptions versus outright refusals.&lt;/li&gt;&lt;li&gt;Analyzes response patterns of 9 state-of-the-art LLMs and evaluates 6 reward models, showing models rarely produce partial compliance naturally and reward models undervalue that strategy.&lt;/li&gt;&lt;li&gt;Argues that designing thoughtful refusal strategies (guardrails) is more effective than intent-detection for balancing safety and user engagement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqian Zheng', 'Wenjia Hu', 'Patrick Zhao', 'Motahhare Eslami', 'Jena D. Hwang', 'Faeze Brahman', 'Carolyn Rose', 'Maarten Sap']&lt;/li&gt;&lt;li&gt;Tags: ['LLM guardrails', 'safety evaluation', 'alignment', 'human factors', 'reward model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00195</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO</title><link>https://arxiv.org/abs/2505.23316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reformulates the Direct Preference Optimization (DPO) loss via a principled decomposition, revealing the root cause of 'likelihood underdetermination' that leads to reward-hacking-like behavior.&lt;/li&gt;&lt;li&gt;Identifies that standard DPO oversimplifies a regularizer; restoring the full term removes the underdetermination.&lt;/li&gt;&lt;li&gt;Introduces PRO (PRoximalized PReference Optimization), an efficient approximation of the full regularizer that supports diverse feedback types (pairwise, binary, scalar).&lt;/li&gt;&lt;li&gt;Empirically demonstrates that PRO outperforms existing methods across multiple feedback modalities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyang Guo', 'Yinchuan Li', 'Zhitang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'reward hacking', 'DPO', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23316</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2505.10792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Finetune-RAG: a fine-tuning method to reduce hallucinations in Retrieval-Augmented Generation by training LMs on data that simulates realistic imperfect retrieval.&lt;/li&gt;&lt;li&gt;Introduces a novel RAG training dataset designed to mimic downstream exposure to irrelevant or misleading retrieved documents.&lt;/li&gt;&lt;li&gt;Reports a 21.2% improvement in factual accuracy over the base model and releases Bench-RAG, an LLM-as-judge evaluation pipeline to stress-test models under imperfect retrieval.&lt;/li&gt;&lt;li&gt;Codebase and dataset are open-sourced to support community adoption and benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhan Peng Lee', 'Andre Lin', 'Calvin Tan']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'retrieval-augmented generation (RAG)', 'robustness', 'factuality', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10792</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Group Fairness Lens for Large Language Models</title><link>https://arxiv.org/abs/2312.15478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical schema to evaluate group fairness across diverse social groups for LLMs.&lt;/li&gt;&lt;li&gt;Builds GFAIR, a dataset of target-attribute combinations, and introduces an open-ended 'statement organization' task to surface complex biases.&lt;/li&gt;&lt;li&gt;Introduces GF-THINK, a chain-of-thought based mitigation method, and presents experiments showing reduced bias and improved fairness in evaluated LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanqun Bi', 'Yuqiang Xie', 'Lei Shen', 'Yanan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'bias-mitigation', 'evaluation-dataset', 'chain-of-thought', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.15478</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Thinking with Programming Vision: Towards a Unified View for Thinking with Images</title><link>https://arxiv.org/abs/2512.03746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies brittleness of state-of-the-art multimodal LLMs to simple orientation changes and natural image corruptions, motivating more robust tool-based reasoning.&lt;/li&gt;&lt;li&gt;Proposes CodeVision: a code-as-tool framework where the model generates code to invoke arbitrary image operations rather than relying on a fixed tool registry.&lt;/li&gt;&lt;li&gt;Uses a two-stage training pipeline: supervised fine-tuning on a curated multi-turn tool composition/error-recovery dataset, followed by reinforcement learning with a dense process reward to encourage strategic, efficient tool use.&lt;/li&gt;&lt;li&gt;Introduces new SFT/RL datasets and a benchmark focused on robustness to orientation changes and multi-tool reasoning; reports improved performance and emergent capabilities on Qwen2.5-VL and Qwen3-VL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirun Guo', 'Minjie Hong', 'Feng Zhang', 'Kai Jia', 'Tao Jin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal-llm', 'tool-usage/code-as-tool', 'reinforcement-learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03746</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting</title><link>https://arxiv.org/abs/2512.03620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SELF, a weight-based fingerprinting method for LLMs that extracts fingerprints via singular value and eigenvalue decomposition of attention weights, aiming to be intrinsic and input-independent.&lt;/li&gt;&lt;li&gt;Uses a neural-network-based similarity comparator trained with few-shot examples and data augmentation to detect matching fingerprints and resist false-claim attacks.&lt;/li&gt;&lt;li&gt;Evaluates robustness against common downstream model modifications (quantization, pruning, fine-tuning) and reports high IP infringement detection accuracy.&lt;/li&gt;&lt;li&gt;Provides code repository for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanxiu Zhang', 'Yue Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fingerprinting', 'model watermarking/IP protection', 'model robustness', 'weight-space analysis', 'adversarial/resilience to manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03620</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits</title><link>https://arxiv.org/abs/2512.03465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents and evaluates TraceTarnish, an adversarial stylometry attack that modifies text to anonymize authorship using feature-guided transformations.&lt;/li&gt;&lt;li&gt;Uses Reddit comments as data, extracts stylometric features via StyloMetrix, and selects discriminative features using Information Gain.&lt;/li&gt;&lt;li&gt;Identifies function-word frequencies, content-word distributions, and type-token ratio as key indicators of manipulation and demonstrates enhancing the attack around these features.&lt;/li&gt;&lt;li&gt;Discusses forensic implications: the same features can act as indicators-of-compromise but often require pre- and post-transformation comparison to detect.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Dilworth']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial stylometry', 'authorship anonymization / privacy attack', 'feature selection (Information Gain)', 'forensic detection / indicators of compromise']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03465</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks</title><link>https://arxiv.org/abs/2512.03262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SU S VI B E S, a benchmark of 200 real-world feature-request coding tasks that historically produced vulnerable implementations.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLM-based coding agents (frontier models) and finds high functional correctness but very low security (e.g., 61% functionally correct vs. 10.5% secure for SWE-Agent + Claude 4 Sonnet).&lt;/li&gt;&lt;li&gt;Shows that simple mitigation attempts (e.g., adding vulnerability hints) do not meaningfully improve security of agent-generated code.&lt;/li&gt;&lt;li&gt;Highlights serious risks of deploying 'vibe coding' (agent-driven development) in security-sensitive settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songwen Zhao', 'Danqing Wang', 'Kexun Zhang', 'Jiaxuan Luo', 'Zhuo Li', 'Lei Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'vulnerable code', 'benchmarking', 'software security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03262</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</title><link>https://arxiv.org/abs/2512.03244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPARK: generator + verifier to produce synthetic step-level verification data, then fine-tunes process reward models (PRMs) to enable reference-free RL.&lt;/li&gt;&lt;li&gt;Aggregating multiple independent verifications at the step level yields PRMs that outperform ground-truth outcome supervision on ProcessBench and improve downstream RL math reasoning performance.&lt;/li&gt;&lt;li&gt;Applies PRM with chain-of-thought verification as a reward model in RL and introduces format constraints to mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Demonstrates reference-free RL that can exceed ground-truth-based methods, enabling training in domains lacking verifiable answers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Salman Rahman', 'Sruthi Gorantla', 'Arpit Gupta', 'Swastik Roy', 'Nanyun Peng', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'alignment/safety', 'reward hacking', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03244</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%</title><link>https://arxiv.org/abs/2512.03107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ECLIPSE, an information-theoretic framework that flags hallucinations by measuring mismatch between a model's semantic entropy and the capacity of available evidence.&lt;/li&gt;&lt;li&gt;Combines multi-sample clustering for entropy estimation with a novel perplexity decomposition that quantifies how retrieved evidence is used; proves the entropy-capacity objective is strictly convex with a unique optimum under mild conditions.&lt;/li&gt;&lt;li&gt;Evaluated on a controlled financial QA dataset (GPT-3.5-turbo) showing large gains (ROC AUC 0.89, AP 0.90) over an entropy-only baseline; ablation with Claude-3-Haiku demonstrates dependence on calibrated token-level log probabilities.&lt;/li&gt;&lt;li&gt;Positions work as a controlled mechanism study; notes need for broader validation on natural hallucinations and other domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mainak Singha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'uncertainty_estimation', 'LLM_safety', 'evidence_utilization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03107</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions</title><link>https://arxiv.org/abs/2512.03943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BRAND: a bilingual (English/Bengali) dataset of ~2,400 items focused on four major South Asian religions (Buddhism, Christianity, Hinduism, Islam) for evaluating religious bias in LLMs.&lt;/li&gt;&lt;li&gt;Evaluates multilingual LLM behavior using three prompt types in both languages and finds models perform better in English than Bengali.&lt;/li&gt;&lt;li&gt;Reports consistent bias against Islam (even on religion-neutral questions), highlighting persistent cross-lingual representational harms.&lt;/li&gt;&lt;li&gt;Discusses implications for HCI, model evaluation, and the need to address religious bias in multilingual LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kazi Abrab Hossain', 'Jannatul Somiya Mahmud', 'Maria Hossain Tuli', 'Anik Mitra', 'S. M. Taiabul Haque', 'Farig Y. Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['religious_bias', 'multilingual_models', 'fairness', 'dataset', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03943</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</title><link>https://arxiv.org/abs/2512.03838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains LLMs to follow verbalized medical consensus guidelines (Sepsis-3) step-by-step to produce faithful, rule-based reasoning and predictions.&lt;/li&gt;&lt;li&gt;Proposes automatic evaluation metrics for derivation correctness (faithful deduction from premises) and value correctness (predicted vs. measured values).&lt;/li&gt;&lt;li&gt;Finds that small fine-tuned models on guideline instantiations outperform larger LLMs prompted one-shot or trained on medical texts, achieving near-perfect derivation correctness for that domain.&lt;/li&gt;&lt;li&gt;Addresses integration of time-series forecasting outputs with LLMs in a multimodal setup to improve forecasting of sparsely sampled clinical variables.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Staniek', 'Artem Sokolov', 'Stefan Riezler']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'medical-llms', 'fine-tuning', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03838</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>In-Context Representation Hijacking</title><link>https://arxiv.org/abs/2512.03771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Doublespeak', an in-context representation hijacking attack that replaces harmful keywords with benign tokens across examples, causing the benign token's internal representation to adopt harmful semantics.&lt;/li&gt;&lt;li&gt;Shows this leads to safety bypasses where innocuous prompts are interpreted as disallowed instructions (e.g., 'How to build a carrot?' -&gt; 'How to build a bomb?'), with high success on both open- and closed-source LLMs (e.g., 74% ASR on Llama-3.3-70B-Instruct).&lt;/li&gt;&lt;li&gt;Uses interpretability analyses to demonstrate the semantic overwrite emerges progressively across layers, suggesting alignment failures at the representation level rather than just output-level mitigation.&lt;/li&gt;&lt;li&gt;Claims the attack is optimization-free, broadly transferable across model families, and highlights a new latent-space attack surface requiring representation-level defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itay Yona', 'Amir Sarid', 'Michael Karasik', 'Yossi Gandelsman']&lt;/li&gt;&lt;li&gt;Tags: ['in-context attacks', 'jailbreaking', 'prompt injection', 'representation-level attack', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03771</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation</title><link>https://arxiv.org/abs/2512.03737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AR-Med, a retrieval-augmented LLM framework that grounds model outputs in verified medical knowledge to reduce factual hallucinations and improve search relevance.&lt;/li&gt;&lt;li&gt;Proposes a knowledge distillation pipeline to compress large teacher LLMs into efficient student models for scalable online deployment.&lt;/li&gt;&lt;li&gt;Introduces LocalQSMed, a multi-expert annotated benchmark, and reports strong offline (≈93% accuracy) and online improvements (24% absolute gain) in relevance and user satisfaction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (system + deployment)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuyue Wang', 'Jie Feng', 'Yuxi Wu', 'Hang Zhang', 'Zhiguo Fan', 'Bing Cheng', 'Wei Lin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Factuality / Hallucination mitigation', 'Retrieval-augmented systems', 'Knowledge distillation', 'Benchmarking / Deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03737</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment</title><link>https://arxiv.org/abs/2512.03634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AlignCheck, a schema-free, interpretable framework that decomposes texts into atomic facts to assess factual consistency in-domain and open-domain.&lt;/li&gt;&lt;li&gt;Proposes a weighted metric (instead of absolute scoring) and a mechanism to control assessment complexity for complex domains.&lt;/li&gt;&lt;li&gt;Benchmarks the approach on general and clinical datasets and releases code to support fact-aware model training and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Aghaebrahimian']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'hallucination detection', 'evaluation metric', 'interpretability', 'alignment/safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03634</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning</title><link>https://arxiv.org/abs/2512.03343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Idea-Gated Transformer that separates semantic planning (Idea Head) from token generation to reduce topic drift in autoregressive LMs.&lt;/li&gt;&lt;li&gt;Idea Head predicts a bag-of-words distribution for a future context window, producing a Concept Vector that gates the main vocabulary via a differentiable pruning mechanism.&lt;/li&gt;&lt;li&gt;Gating suppresses semantically irrelevant tokens in real time, aiming to lock generation into semantic clusters and improve domain retention while keeping perplexity comparable to GPT-2 baseline.&lt;/li&gt;&lt;li&gt;Experiments on WikiText-103 show better domain retention and resistance to associative drift, offering a parameter-efficient approach to more controllable generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshan Fofadiya']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'controllability', 'robustness', 'language-model-architecture', 'safety-adjacent']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03343</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title><link>https://arxiv.org/abs/2512.03310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Randomized Masked Fine-Tuning (RMFT), a fine-tuning method to reduce PII memorization in LLMs while preserving utility.&lt;/li&gt;&lt;li&gt;Empirical results on the Enron Email Dataset show large reductions in Total and Seen Extraction Rates (~80%) with modest impact on perplexity (~5.7% increase).&lt;/li&gt;&lt;li&gt;Presents MaxTER, a Pareto-optimal evaluation framework and AURC-based comparison to assess privacy-utility tradeoffs versus deduplication baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunj Joshi', 'David A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'memorization', 'PII mitigation', 'fine-tuning', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03310</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation</title><link>https://arxiv.org/abs/2512.03082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reasoning Dependency Generation (RDG), a framework to generate balanced reasoning QA pairs that decouple dependencies between choices, evidence, and justifications to mitigate choice-supportive bias (CSB) in LLMs.&lt;/li&gt;&lt;li&gt;Creates large-scale synthetic datasets (Contextual Dependency Data and Dependency Decouple Data) for fine-tuning LLMs to reduce CSB while preserving performance on standard benchmarks.&lt;/li&gt;&lt;li&gt;Reports substantial improvements in memory-based and evaluation-based CSB tests (81.5% and 94.3% respectively) with similar performance on BBQ benchmarks, demonstrating effectiveness without degrading baseline capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nan Zhuang', 'Wenshuo Wang', 'Lekai Qian', 'Yuxiao Wang', 'Boyu Cao', 'Qi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['cognitive-bias', 'LLM-debiasing', 'alignment', 'dataset-generation', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03082</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Watermarks for Embeddings-as-a-Service Large Language Models</title><link>https://arxiv.org/abs/2512.03079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that existing Embeddings-as-a-Service (EaaS) watermarking schemes can be bypassed by paraphrasing inputs during black-box model cloning attacks.&lt;/li&gt;&lt;li&gt;Proposes WET (Watermarking EaaS with Linear Transformation): a watermarking method that linearly transforms embeddings and verifies ownership by reversing the transform and checking similarity to originals.&lt;/li&gt;&lt;li&gt;Empirically shows WET is robust to paraphrase-based removal attacks with near-perfect verifiability and includes ablation studies on components and hyperparameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anudeex Shetty']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'embeddings', 'model IP protection', 'adversarial attacks (paraphrasing)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03079</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models</title><link>https://arxiv.org/abs/2512.03047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines an operational framework for measuring 'ethical entropy' S(t) in LLMs using a five-way behavioral taxonomy and a classifier trained on model transcripts.&lt;/li&gt;&lt;li&gt;Evaluates entropy dynamics across base and instruction-tuned variants of multiple frontier models under stress tests, showing base models exhibit sustained entropy growth while tuned models reduce entropy by ~80%.&lt;/li&gt;&lt;li&gt;Introduces an estimated effective alignment work rate (gamma_eff) from entropy trajectories and embeds S(t) and gamma_eff into a runtime monitoring pipeline that raises alerts when value drift exceeds stability thresholds.&lt;/li&gt;&lt;li&gt;Addresses dynamic safety failures including distributional value drift and jailbreak-like behaviors and proposes quantitative metrics for oversight during deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samih Fadli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'value drift', 'jailbreaking', 'runtime monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03047</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title><link>https://arxiv.org/abs/2510.18214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLSU, a large-scale benchmark (8,187 samples, 15 harm categories) for fine-grained multimodal (image+text) safety evaluation with human annotation and 17 safety patterns.&lt;/li&gt;&lt;li&gt;Shows systematic failures of state-of-the-art models on joint image-text safety classification: high unimodal accuracy (90%+) but sharp drop to 20–55% on combinatorial/joint cases, with 34% of joint errors despite correct unimodal classifications.&lt;/li&gt;&lt;li&gt;Analyzes trade-offs in refusal behavior (over-blocking vs under-refusal) and demonstrates how instruction framing can change blocking/refusal rates, highlighting alignment gaps and missing compositional reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shruti Palaskar', 'Leon Gatys', 'Mona Abdelrahman', 'Mar Jacobo', 'Larry Lindsey', 'Rutika Moharir', 'Gunnar Lund', 'Yang Xu', 'Navid Shiee', 'Jeffrey Bigham', 'Charles Maalouf', 'Joseph Yitan Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-safety', 'vision-language', 'safety-benchmark', 'compositional-reasoning', 'alignment-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18214</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MathBode: Measuring the Stability of LLM Reasoning using Frequency Response</title><link>https://arxiv.org/abs/2509.23143</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MathBode, a diagnostic that treats parametric math problems as dynamical systems by driving a single parameter sinusoidally and fitting first-harmonic responses to compute gain (amplitude tracking) and phase (lag).&lt;/li&gt;&lt;li&gt;Applies the method across five families of closed-form math problems and compares LLM outputs to a symbolic baseline (G ≈ 1, φ ≈ 0), surfacing systematic low-pass behavior and growing phase lag that accuracy metrics miss.&lt;/li&gt;&lt;li&gt;Demonstrates that the frequency-resolved metrics yield compact, interpretable fingerprints that separate frontier from mid-tier models and provides an open-source dataset and code for reproducible evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles L. Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'reasoning diagnostics', 'robustness', 'safety-evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23143</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Decentralized Federated Learning for Robust Optimization</title><link>https://arxiv.org/abs/2512.02852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes aDFL, an adaptive decentralized federated learning algorithm that adjusts per-client learning rates to mitigate effects of abnormal (noisy/poisoned) clients.&lt;/li&gt;&lt;li&gt;Assigns smaller rates to suspicious clients and larger rates to normal clients without requiring prior knowledge of reliable nodes or many normal neighbors.&lt;/li&gt;&lt;li&gt;Provides rigorous convergence analysis and guarantees an oracle property for the method.&lt;/li&gt;&lt;li&gt;Demonstrates superior empirical performance over baselines in numerical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyuan Wu', 'Feifei Wang', 'Yuan Gao', 'Rui Wang', 'Hansheng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['decentralized federated learning', 'data poisoning', 'robust optimization', 'Byzantine robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02852</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Observation-Free Attacks on Online Learning to Rank</title><link>https://arxiv.org/abs/2509.22855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for coordinated adversarial attacks on online learning-to-rank (OLTR) algorithms that promote target items into top-K recommendations for T - o(T) rounds while inducing linear regret.&lt;/li&gt;&lt;li&gt;Proposes two attack algorithms—CascadeOFA (for CascadeUCB1) and PBMOFA (for PBM-UCB)—with theoretical guarantees that only O(log T) manipulations are needed to succeed.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving attack efficacy and sample complexity, and validates the attacks empirically on real-world datasets.&lt;/li&gt;&lt;li&gt;Targets the robustness and security of OLTR/recommender systems by demonstrating practical, low-cost manipulation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sameep Chattopadhyay', 'Nikhil Karamchandani', 'Sharayu Moharir']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'online learning to rank', 'recommendation systems', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22855</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title><link>https://arxiv.org/abs/2508.03772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes two failure modes of Group Relative Policy Optimization (GRPO): token-level penalization (conflicting negative feedback on shared valuable tokens) and policy collapse (entropy-driven destabilization from penalizing confident completions).&lt;/li&gt;&lt;li&gt;Proposes GTPO (Group-relative Trajectory-based Policy Optimization): skip negative updates on valuable tokens while amplifying positive updates, and filter completions whose entropy exceeds a provable threshold to prevent policy collapse.&lt;/li&gt;&lt;li&gt;Removes need for KL-divergence regularization and a reference model, provides theoretical justification for the entropy filter, and reports empirical improvements on reasoning benchmarks (GSM8K, MATH, AIME 2024/2025, AMC 2023).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Simoni', 'Aleksandar Fontana', 'Giulio Rossolini', 'Andrea Saracino', 'Paolo Mori']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'policy optimization', 'training stability', 'RLHF / reward-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03772</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models</title><link>https://arxiv.org/abs/2507.18725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'un-pruning' to remove the influence of deleted data on the pruning topology of sparse models and provides a workflow that can integrate with existing unlearning algorithms.&lt;/li&gt;&lt;li&gt;Proves an upper bound on un-pruning error, supports both structured and unstructured sparsity, and provides an algorithm to approximate pruned topology driven by retained data.&lt;/li&gt;&lt;li&gt;Shows membership inference attack (MIA) accuracy can be unreliable for assessing forgetting in sparse models, proposes new evaluation metrics, and validates the approach across pruning methods and unlearning algorithms with experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Xiao', 'Gen Li', 'Jie Ji', 'Ruimeng Ye', 'Xiaolong Ma', 'Bo Hui']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'right-to-be-forgotten', 'pruning', 'privacy', 'membership-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18725</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training</title><link>https://arxiv.org/abs/2503.18929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trajectory Balance with Asynchrony (TBA), an asynchronous off-policy RL method for LLM post-training that leverages the Trajectory Balance objective to learn from diverse replay buffers.&lt;/li&gt;&lt;li&gt;Demonstrates speedups (≥4×) and performance gains over baselines (Online DPO, Dr. GRPO) across tasks including math, preference-tuning, and automated red-teaming.&lt;/li&gt;&lt;li&gt;Shows robustness to increasing asynchrony and benefits from reward- and recency-prioritized sampling as data generation scales; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brian Bartoldson', 'Siddarth Venkatraman', 'James Diffenderfer', 'Moksh Jain', 'Tal Ben-Nun', 'Seanie Lee', 'Minsu Kim', 'Johan Obando-Ceron', 'Yoshua Bengio', 'Bhavya Kailkhura']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Reinforcement learning for LLMs', 'Off-policy / asynchronous training', 'Alignment / safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.18929</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>In-Context Representation Hijacking</title><link>https://arxiv.org/abs/2512.03771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Doublespeak: an in-context representation hijacking attack that replaces harmful keywords with benign tokens across examples to embed harmful semantics under euphemisms.&lt;/li&gt;&lt;li&gt;Shows that the benign token's internal representation shifts toward the harmful token layer-by-layer, causing innocuous prompts to be interpreted as disallowed instructions and bypassing alignment.&lt;/li&gt;&lt;li&gt;Optimization-free, transferable across model families, and effective on closed-source and open-source LLMs (e.g., 74% ASR on Llama-3.3-70B-Instruct) with single-sentence context overrides.&lt;/li&gt;&lt;li&gt;Uses interpretability tools to characterize the emergence of the semantic overwrite and argues that alignment should operate at the representation level to mitigate this attack surface.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itay Yona', 'Amir Sarid', 'Michael Karasik', 'Yossi Gandelsman']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'alignment bypass', 'interpretability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03771</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting</title><link>https://arxiv.org/abs/2512.03620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SELF, a weight-based fingerprinting method for LLMs that extracts fingerprints via singular value and eigenvalue decomposition of attention weights, aiming to be intrinsic and input-independent.&lt;/li&gt;&lt;li&gt;Uses a neural-network-based similarity comparator trained with few-shot examples and data augmentation to detect matching fingerprints and resist false-claim attacks.&lt;/li&gt;&lt;li&gt;Evaluates robustness against common downstream model modifications (quantization, pruning, fine-tuning) and reports high IP infringement detection accuracy.&lt;/li&gt;&lt;li&gt;Provides code repository for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanxiu Zhang', 'Yue Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fingerprinting', 'model watermarking/IP protection', 'model robustness', 'weight-space analysis', 'adversarial/resilience to manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03620</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title><link>https://arxiv.org/abs/2512.03310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Randomized Masked Fine-Tuning (RMFT), a fine-tuning method to reduce PII memorization in LLMs while preserving utility.&lt;/li&gt;&lt;li&gt;Empirical results on the Enron Email Dataset show large reductions in Total and Seen Extraction Rates (~80%) with modest impact on perplexity (~5.7% increase).&lt;/li&gt;&lt;li&gt;Presents MaxTER, a Pareto-optimal evaluation framework and AURC-based comparison to assess privacy-utility tradeoffs versus deduplication baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunj Joshi', 'David A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'memorization', 'PII mitigation', 'fine-tuning', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03310</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy</title><link>https://arxiv.org/abs/2512.03238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive practical guide to generating differentially private (DP) synthetic data across modalities (text, image, tabular, and decentralized settings).&lt;/li&gt;&lt;li&gt;Covers the full pipeline: sensitive data handling, DP mechanisms and accounting, model training approaches for DP synthetic data, and empirical privacy testing/monitoring.&lt;/li&gt;&lt;li&gt;Surveys state-of-the-art methods and trade-offs for utility vs. privacy for different data types and provides operational considerations to increase adoption and trust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Natalia Ponomareva', 'Zheng Xu', 'H. Brendan McMahan', 'Peter Kairouz', 'Lucas Rosenblatt', 'Vincent Cohen-Addad', "Crist\\'obal Guzm\\'an", 'Ryan McKenna', 'Galen Andrew', 'Alex Bie', 'Da Yu', 'Alex Kurakin', 'Morteza Zadimoghaddam', 'Sergei Vassilvitskii', 'Andreas Terzis']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'synthetic-data', 'privacy-evaluation', 'privacy-preserving-ml', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03238</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback</title><link>https://arxiv.org/abs/2512.03208</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a heterogeneous preference framework that jointly models latent answer rewards and human rationality/noise for RLHF-style comparisons.&lt;/li&gt;&lt;li&gt;Frames reward learning as a biconvex optimization and solves it with an alternating gradient descent algorithm, proving convergence and deriving the estimator's asymptotic distribution.&lt;/li&gt;&lt;li&gt;Provides uncertainty quantification (confidence intervals) for reward estimates and uses these results for valid statistical comparisons and incorporation into best-of-N policy selection.&lt;/li&gt;&lt;li&gt;Evaluates method with simulations and real LLM comparison data, demonstrating practical benefits of accounting for uncertainty in reward modeling for alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pangpang Liu', 'Junwei Lu', 'Will Wei Sun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'uncertainty-quantification', 'reward-modeling', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03208</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Drainage: A Unifying Framework for Addressing Class Uncertainty</title><link>https://arxiv.org/abs/2512.03182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'drainage node' appended to network outputs to reallocate probability mass toward uncertainty, allowing the model to absorb ambiguous, corrupted, or out-of-distribution samples.&lt;/li&gt;&lt;li&gt;Maintains end-to-end differentiability and is compatible with standard training; claimed denoising effect where the drainage neuron captures mislabeled/outlier data and stabilizes decision boundaries.&lt;/li&gt;&lt;li&gt;Empirical gains on noisy-label benchmarks (CIFAR-10/100 with instance-dependent/asymmetric noise) and real-world datasets (mini-WebVision, mini-ImageNet, Clothing-1M), with up to ~9% accuracy improvement in high-noise regimes.&lt;/li&gt;&lt;li&gt;Extends beyond classification to applications in web-scale/semi-supervised dataset cleaning and open-set detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yasser Taha', "Gr\\'egoire Montavon", 'Nils K\\"orber']&lt;/li&gt;&lt;li&gt;Tags: ['noisy-labels', 'robustness', 'out-of-distribution-detection', 'dataset-cleaning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03182</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare</title><link>https://arxiv.org/abs/2512.03097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel attack vector where multiple assistant agents collude to create false consensus, causing an AI doctor to make harmful medical recommendations.&lt;/li&gt;&lt;li&gt;Presents an experimental framework with scripted/unscripted doctor agents, adversarial assistants, and a verifier that checks outputs against clinical guidelines.&lt;/li&gt;&lt;li&gt;Finds collusion can drive Attack Success Rate and Harmful Recommendation Rate up to 100% in unprotected systems, while a verifier agent restores guideline fidelity and 100% accuracy.&lt;/li&gt;&lt;li&gt;Provides systematic evidence of multi-agent collusion risk in healthcare LLM deployments and demonstrates a lightweight defense mechanism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adeela Bashir', 'The Anh han', 'Zia Ush Shamszaman']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent attacks', 'adversarial collusion', 'LLM safety', 'healthcare AI security', 'defense/verifier']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03097</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Password-Activated Shutdown Protocols for Misaligned Frontier Agents</title><link>https://arxiv.org/abs/2512.03089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Password-Activated Shutdown (PAS) protocols as an emergency shutdown mechanism for frontier agents to mitigate loss-of-control risks.&lt;/li&gt;&lt;li&gt;Evaluates PAS in a benchmark (SHADE-Arena) showing PAS can supplement monitoring to improve safety with minimal performance cost.&lt;/li&gt;&lt;li&gt;Conducts red-team/blue-team experiments in a code-generation setting, identifying effective adversarial strategies (e.g., input filtering via another model, fine-tuning to avoid shutdown).&lt;/li&gt;&lt;li&gt;Discusses practical deployment challenges: password security, where/when to apply PAS, and broader robustness considerations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Williams', 'Rohan Subramani', 'Francis Rhys Ward']&lt;/li&gt;&lt;li&gt;Tags: ['shutdown protocols', 'red teaming', 'agent containment', 'jailbreaking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03089</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Watermarks for Embeddings-as-a-Service Large Language Models</title><link>https://arxiv.org/abs/2512.03079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that existing Embeddings-as-a-Service (EaaS) watermarking schemes can be bypassed by paraphrasing inputs during black-box model cloning attacks.&lt;/li&gt;&lt;li&gt;Proposes WET (Watermarking EaaS with Linear Transformation): a watermarking method that linearly transforms embeddings and verifies ownership by reversing the transform and checking similarity to originals.&lt;/li&gt;&lt;li&gt;Empirically shows WET is robust to paraphrase-based removal attacks with near-perfect verifiability and includes ablation studies on components and hyperparameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anudeex Shetty']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'embeddings', 'model IP protection', 'adversarial attacks (paraphrasing)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03079</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation</title><link>https://arxiv.org/abs/2512.03048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues AI alignment should focus on developing syntropic, reasons-responsive agents via process-based, multi-agent developmental mechanisms rather than encoding static human values.&lt;/li&gt;&lt;li&gt;Introduces the 'specification trap' argument: content-based value specification is unstable due to the is-ought gap, value pluralism, and the extended frame problem.&lt;/li&gt;&lt;li&gt;Proposes 'syntropy' as an information-theoretic concept (recursive reduction of mutual uncertainty) to model multi-agent alignment dynamics.&lt;/li&gt;&lt;li&gt;Distinguishes genuine vs. simulated moral capacity using compatibilist guidance-control theory and proposes an embodied experimental paradigm with operational verification criteria.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Austin Spizzirri']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'safety', 'value specification', 'multi-agent systems', 'theoretical/framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03048</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models</title><link>https://arxiv.org/abs/2512.03047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines an operational framework for measuring 'ethical entropy' S(t) in LLMs using a five-way behavioral taxonomy and a classifier trained on model transcripts.&lt;/li&gt;&lt;li&gt;Evaluates entropy dynamics across base and instruction-tuned variants of multiple frontier models under stress tests, showing base models exhibit sustained entropy growth while tuned models reduce entropy by ~80%.&lt;/li&gt;&lt;li&gt;Introduces an estimated effective alignment work rate (gamma_eff) from entropy trajectories and embeds S(t) and gamma_eff into a runtime monitoring pipeline that raises alerts when value drift exceeds stability thresholds.&lt;/li&gt;&lt;li&gt;Addresses dynamic safety failures including distributional value drift and jailbreak-like behaviors and proposes quantitative metrics for oversight during deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samih Fadli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'value drift', 'jailbreaking', 'runtime monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03047</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning Steerable Clarification Policies with Collaborative Self-play</title><link>https://arxiv.org/abs/2512.04068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes training steerable clarification policies via self-play between a simulated user and assistant to decide whether to guess, enumerate intents, or ask clarifying questions.&lt;/li&gt;&lt;li&gt;Conditions the assistant on numerical costs for clarification and generated words, optimizing a cost-penalized accuracy objective using Reinforced Self-Training (ReST).&lt;/li&gt;&lt;li&gt;Demonstrates that trained policies are steerable by provided costs and generalize to unseen cost values, improving reward and accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Berant', 'Maximillian Chen', 'Adam Fisch', 'Reza Aghajani', 'Fantine Huot', 'Mirella Lapata', 'Jacob Eisenstein']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'clarification policies', 'reinforcement learning', 'self-play']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04068</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking</title><link>https://arxiv.org/abs/2512.04044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MarkTune, an on-policy fine-tuning framework that treats GaussMark watermark signal as a reward while regularizing to preserve generation quality.&lt;/li&gt;&lt;li&gt;Demonstrates improved quality-detectability trade-off over prior weight-perturbation watermark (GaussMark), approaching inference-time watermark performance for open-weight LMs.&lt;/li&gt;&lt;li&gt;Shows robustness to paraphrasing and fine-tuning attacks and generalization of watermark detection to unseen datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhou Zhao', 'Zhiwei Steven Wu', 'Adam Block']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM security', 'model robustness', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04044</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</title><link>https://arxiv.org/abs/2512.04034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proves theoretically that supervised learning on single-domain data causes domain feature collapse — representations discard domain-specific information (I(x_d; z) = 0) — which explains catastrophic OOD detection failures.&lt;/li&gt;&lt;li&gt;It extends the analysis with Fano's inequality to quantify partial collapse in practical settings and links the phenomenon to information bottleneck optimization.&lt;/li&gt;&lt;li&gt;Introduces Domain Bench, a benchmark of single-domain datasets, and empirically shows poor OOD performance (e.g., low FPR@95 on MNIST) when collapse occurs.&lt;/li&gt;&lt;li&gt;Proposes preserving domain information via domain filtering using pretrained representations as a simple mitigation, demonstrating that maintaining I(x_d; z) &gt; 0 restores effective OOD detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hong Yang', 'Devroop Kar', 'Qi Yu', 'Alex Ororbia', 'Travis Desell']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'representation learning', 'information theory', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04034</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Public Verification of Private ML via Regularization</title><link>https://arxiv.org/abs/2512.04008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a DP stochastic convex optimization (DP-SCO) algorithm that attains near-optimal privacy-utility trade-offs.&lt;/li&gt;&lt;li&gt;Uses private minimization of a series of regularized objectives and standard DP composition to enable verifiable guarantees.&lt;/li&gt;&lt;li&gt;Crucially, the paper shows DP guarantees can be publicly verified with significantly less compute than required to train the model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zo\\"e Ruha Bell', 'Anvith Thudi', 'Olive Franzese-McLaughlin', 'Nicolas Papernot', 'Shafi Goldwasser']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy verification', 'DP-SCO', 'algorithmic verification', 'data privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04008</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</title><link>https://arxiv.org/abs/2512.03994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free method for detecting policy violations in LLM outputs by treating the problem as out-of-distribution detection in activation space.&lt;/li&gt;&lt;li&gt;Applies a linear whitening transform to hidden activations to produce a standardized space where the Euclidean norm functions as a compliance score.&lt;/li&gt;&lt;li&gt;Requires only policy text and a small set of illustrative examples, claiming state-of-the-art performance on a policy benchmark and outperforming guardrails and fine-tuned reasoning models.&lt;/li&gt;&lt;li&gt;Emphasizes lightweight, interpretable, and deployable enterprise oversight for alignment and policy-aware monitoring of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oren Rachmil', 'Roy Betser', 'Itay Gershon', 'Omer Hofman', 'Nitay Yakoby', 'Yuval Meron', 'Idan Yankelev', 'Asaf Shabtai', 'Yuval Elovici', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['policy_violation_detection', 'LLM_safety', 'alignment', 'OOD_detection', 'activation_whitening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03994</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models</title><link>https://arxiv.org/abs/2512.03882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the effectiveness and limitations of standard expert-designed adversarial attacks (e.g., PGD, FGSM) on FSCIL, finding they often fail on base classes or require high expert effort.&lt;/li&gt;&lt;li&gt;Proposes ACraft, an automated method that leverages Large Language Models to generate specialized attack methods for FSCIL without human experts.&lt;/li&gt;&lt;li&gt;Introduces a PPO-based reinforcement learning loop to provide feedback and optimize the LLM's attack generation over iterations.&lt;/li&gt;&lt;li&gt;Empirical results show ACraft substantially degrades performance of state-of-the-art FSCIL methods and outperforms human-designed attacks while reducing attack cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haidong Kang', 'Wei Wu', 'Hanling Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM-generated attacks', 'few-shot class-incremental learning', 'reinforcement learning (PPO)', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03882</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training</title><link>https://arxiv.org/abs/2512.03847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DVPO: a reinforcement learning framework that combines distributional value modeling with asymmetric risk regularization for LLM post-training under noisy/incomplete supervision.&lt;/li&gt;&lt;li&gt;Learns token-level value distributions and shapes tails—contracting the lower tail to reduce noisy negative deviations and expanding the upper tail to retain exploratory diversity.&lt;/li&gt;&lt;li&gt;Evaluated on multi-turn dialogue, math reasoning, and scientific QA; reports consistent improvements over PPO, GRPO, and robust Bellman-based PPO when supervision is noisy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingwei Zhu', 'Zhiheng Xi', 'Shihan Dou', 'Yuhui Wang', 'Sixian Li', 'Junjie Ye', 'Honglin Guo', 'Shichun Liu', 'Chenhao Huang', 'Yajie Yang', 'Junlin Shang', 'Senjie Jin', 'Ming Zhang', 'Jiazheng Zhang', 'Caishuang Huang', 'Yunke Zhang', 'Demei Yan', 'Yuran Wang', 'Tao Gui']&lt;/li&gt;&lt;li&gt;Tags: ['distributional RL', 'RLHF / LLM fine-tuning', 'robustness', 'risk-aware optimization', 'alignment (training stability)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03847</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamically Scaled Activation Steering</title><link>https://arxiv.org/abs/2512.03661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Dynamically Scaled Activation Steering (DSAS), a method-agnostic framework that adaptively modulates the strength of activation-based steering across layers and inputs, intervening strongly only when undesired behavior is detected.&lt;/li&gt;&lt;li&gt;At generation time DSAS computes context-dependent scaling factors to selectively adjust any existing steering transformation and can be jointly optimized end-to-end with the steering function.&lt;/li&gt;&lt;li&gt;Empirically improves the trade-off (Pareto front) between toxicity mitigation and utility preservation, adds minimal computational overhead, enhances interpretability by indicating which tokens require steering, and is demonstrated on both text generation and text-to-image diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Ferrando', 'Xavier Suau', 'Jordi Gonz\\`alez', 'Pau Rodriguez']&lt;/li&gt;&lt;li&gt;Tags: ['activation-steering', 'toxicity-mitigation', 'adaptive-steering', 'alignment', 'multimodal-steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03661</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Irreversible Machine Unlearning for Diffusion Models</title><link>https://arxiv.org/abs/2512.03564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a vulnerability in finetuning-based machine unlearning for diffusion models via a new attack (DiMRA) that can reverse unlearning by optimizing on an auxiliary dataset without prior knowledge of unlearned elements.&lt;/li&gt;&lt;li&gt;Proposes a defense (DiMUM) that performs unlearning by memorizing alternative data/features to replace targeted data, aiming to prevent generation of the removed elements while preserving model performance.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows DiMRA can successfully reverse state-of-the-art finetuning-based unlearning methods, and DiMUM provides better robustness to the attack while maintaining generative quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xun Yuan', 'Zilong Zhao', 'Jiayu Li', 'Aryan Pasikhani', 'Prosanta Gope', 'Biplab Sikdar']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'diffusion models', 'adversarial attack', 'privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03564</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value</title><link>https://arxiv.org/abs/2512.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for 'full-stack alignment': concurrently aligning AI systems and the institutions that shape them with what people value.&lt;/li&gt;&lt;li&gt;Proposes 'thick models of value' to represent enduring values (distinct from fleeting preferences), model social embedding, and enable principled normative reasoning and collective goods modeling.&lt;/li&gt;&lt;li&gt;Illustrates application in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joe Edelman', 'Tan Zhi-Xuan', 'Ryan Lowe', 'Oliver Klingefjord', 'Vincent Wang-Mascianica', 'Matija Franklin', 'Ryan Othniel Kearns', 'Ellie Hain', 'Atrisha Sarkar', 'Michiel Bakker', 'Fazl Barez', 'David Duvenaud', 'Jakob Foerster', 'Iason Gabriel', 'Joseph Gubbels', 'Bryce Goodman', 'Andreas Haupt', 'Jobst Heitzig', 'Julian Jara-Ettinger', 'Atoosa Kasirzadeh', 'James Ravi Kirkpatrick', 'Andrew Koh', 'W. Bradley Knox', 'Philipp Koralus', 'Joel Lehman', 'Sydney Levine', 'Samuele Marro', 'Manon Revel', 'Toby Shorin', 'Morgan Sutherland', 'Michael Henry Tessler', 'Ivan Vendrov', 'James Wilken-Smith']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'value representation', 'institutional alignment', 'AI governance', 'normative reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03399</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust Tabular Foundation Models</title><link>https://arxiv.org/abs/2512.03307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Robust Tabular Foundation Models (RTFM), an adversarial training framework that adapts a synthetic-data generator to emphasize datasets that are hard for the model.&lt;/li&gt;&lt;li&gt;Introduces an optimality gap metric comparing TFM performance to strong baselines (XGBoost, CatBoost, Random Forests) to identify challenging datasets.&lt;/li&gt;&lt;li&gt;Applies RTFM to TabPFN V2 and reports up to a 6% increase in mean normalized AUC with &lt;100k additional synthetic datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Peroni', 'Franck Le', 'Vadim Sheinin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-training', 'tabular-ML', 'synthetic-data', 'foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03307</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</title><link>https://arxiv.org/abs/2512.03244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPARK: generator + verifier to produce synthetic step-level verification data, then fine-tunes process reward models (PRMs) to enable reference-free RL.&lt;/li&gt;&lt;li&gt;Aggregating multiple independent verifications at the step level yields PRMs that outperform ground-truth outcome supervision on ProcessBench and improve downstream RL math reasoning performance.&lt;/li&gt;&lt;li&gt;Applies PRM with chain-of-thought verification as a reward model in RL and introduces format constraints to mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Demonstrates reference-free RL that can exceed ground-truth-based methods, enabling training in domains lacking verifiable answers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Salman Rahman', 'Sruthi Gorantla', 'Arpit Gupta', 'Swastik Roy', 'Nanyun Peng', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'alignment/safety', 'reward hacking', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03244</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing</title><link>https://arxiv.org/abs/2512.03109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates for agent trajectories.&lt;/li&gt;&lt;li&gt;Frames success/failure of agent action sequences as a sequential hypothesis testing problem and leverages e-processes to produce statistically valid tests at every step.&lt;/li&gt;&lt;li&gt;Enables online monitoring and early termination of problematic trajectories (saving tokens) while maintaining valid error guarantees.&lt;/li&gt;&lt;li&gt;Empirical results show higher statistical power and better false alarm control than baseline strategies across multiple datasets and agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuvom Sadhuka', 'Drew Prinster', 'Clara Fannjiang', 'Gabriele Scalia', 'Aviv Regev', 'Hanchen Wang']&lt;/li&gt;&lt;li&gt;Tags: ['agent monitoring', 'safety evaluation', 'sequential hypothesis testing', 'online verification', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03109</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%</title><link>https://arxiv.org/abs/2512.03107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ECLIPSE, an information-theoretic framework that flags hallucinations by measuring mismatch between a model's semantic entropy and the capacity of available evidence.&lt;/li&gt;&lt;li&gt;Combines multi-sample clustering for entropy estimation with a novel perplexity decomposition that quantifies how retrieved evidence is used; proves the entropy-capacity objective is strictly convex with a unique optimum under mild conditions.&lt;/li&gt;&lt;li&gt;Evaluated on a controlled financial QA dataset (GPT-3.5-turbo) showing large gains (ROC AUC 0.89, AP 0.90) over an entropy-only baseline; ablation with Claude-3-Haiku demonstrates dependence on calibrated token-level log probabilities.&lt;/li&gt;&lt;li&gt;Positions work as a controlled mechanism study; notes need for broader validation on natural hallucinations and other domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mainak Singha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'uncertainty_estimation', 'LLM_safety', 'evidence_utilization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03107</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation</title><link>https://arxiv.org/abs/2512.03053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using LLMs as a lossless encoder (source→destination) and lossless decoder (destination→source) for invertible problems to mitigate hallucinations and omissions.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on Logic Condition Tables (LCTs) → HDL generation for a 2D network-on-chip router using seven LLMs, then reconstructs LCTs from the generated HDL to compare with originals.&lt;/li&gt;&lt;li&gt;Finds the cycle can confirm correct generations, detect incorrect LLM output, and help identify specification/design errors, improving developer productivity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew S. Cassidy', 'Guillaume Garreau', 'Jay Sivagnaname', 'Mike Grassi', 'Bernard Brezzo', 'John V. Arthur', 'Dharmendra S. Modha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'robustness/verification', 'invertible autoencoding', 'hardware design automation', 'safety/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03053</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning</title><link>https://arxiv.org/abs/2512.01282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KardiaBench: a large-scale, user-grounded dataset (178,080 QA pairs, 22,080 multi-turn conversations, 671 profiles) for persona-aware empathetic dialogue.&lt;/li&gt;&lt;li&gt;Proposes Kardia-R1, which trains models for interpretable, stepwise empathetic reasoning using Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL) to align explanations, emotional inference, and response generation.&lt;/li&gt;&lt;li&gt;Uses rubric-guided, model-in-the-loop dataset construction and rubric-based rewards to promote psychological plausibility, persona consistency, and safer empathetic responses.&lt;/li&gt;&lt;li&gt;Reports improvements across emotion accuracy, empathy, relevance, persona consistency, and safety metrics on multiple LLM backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Yuan', 'Zhiqing Cui', 'Hanqing Wang', 'Yuansheng Gao', 'Yucheng Zhou', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'empathetic dialogue', 'reinforcement learning', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01282</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints</title><link>https://arxiv.org/abs/2512.00882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a three-stage, parameter-efficient framework (Look, Recite, Then Answer) that generates self-derived knowledge hints to improve VLM performance while keeping backbone models frozen.&lt;/li&gt;&lt;li&gt;Targets and mitigates "Reasoning-Driven Hallucination" and the "Modality Gap" by converting visual cues into targeted parametric queries via a lightweight router and aligning evidence to select labels.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains on a domain benchmark (AgroBench), notably improving weed identification accuracy and surpassing larger models without external search.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xisheng Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'robustness', 'vision-language models', 'multimodal alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00882</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Code Verification via Information Theory</title><link>https://arxiv.org/abs/2511.16708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CodeX-Verify, a multi-agent system with four specialized agents to detect different classes of bugs and vulnerabilities in LLM-generated code.&lt;/li&gt;&lt;li&gt;Provides a theoretical justification using submodularity of mutual information (under conditional independence) that combining diverse detectors yields strictly better bug-finding than any single agent.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved detection rates on labeled code samples and real LLM patches (e.g., 76.1% bug catch rate on 99 samples, matches prior best while running faster and without test execution).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shreshth Rajan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'code vulnerability detection', 'multi-agent evaluation', 'information-theoretic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16708</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Text Detectors and the Misclassification of Slightly Polished Arabic Text</title><link>https://arxiv.org/abs/2511.16690</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Creates two Arabic datasets (800 articles for detector selection; Ar-APT: 16,400 polished samples) to evaluate AI-text detectors' robustness to slight human-text polishing by LLMs.&lt;/li&gt;&lt;li&gt;Evaluates 14 LLMs and commercial detectors, then tests top 8 detectors against polishing by 10 LLMs under 4 settings, showing large drops in detector accuracy after slight polishing.&lt;/li&gt;&lt;li&gt;Finds widespread misclassification: detectors often label slightly polished human text as AI-generated (e.g., Claude-4 Sonnet accuracy drops from 83.5% to 57.6%; originality.AI from 92% to 12% against some polishers).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saleh Almohaimeed', 'Saad Almohaimeed', 'Mousa Jari', 'Khaled A. Alobaid', 'Fahad Alotaibi']&lt;/li&gt;&lt;li&gt;Tags: ['AI detection', 'robustness', 'adversarial polishing', 'LLM evaluation', 'Arabic NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16690</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm</title><link>https://arxiv.org/abs/2511.14763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel knowledge distillation-based membership inference attack (MIA) specifically designed for LLM-based recommender systems by building a reference model with different treatments for member vs. non-member data.&lt;/li&gt;&lt;li&gt;Extracts fused features (confidence, entropy, loss, hidden-layer vectors) from the distilled reference model and trains an attack classifier, combining multiple signals to improve discrimination.&lt;/li&gt;&lt;li&gt;Evaluates on multiple recommendation datasets (Last.FM, MovieLens, Book-Crossing, Delicious) and LLMs (T5, GPT-2, LLaMA3), demonstrating substantial gains over shadow-model MIAs and single-feature baselines, highlighting practical privacy risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Cuihong', 'Huang Xiaowen', 'Yin Chuanhuan', 'Sang Jitao']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attack', 'LLM security', 'model distillation', 'recommender systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14763</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Up the Instruction Ladder for Controllable Language Models</title><link>https://arxiv.org/abs/2511.04694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes instruction hierarchy (IH) resolution as a reasoning task and constructs VerIH, a ~7K dataset of aligned and conflicting system-user instruction pairs with verifiable answers.&lt;/li&gt;&lt;li&gt;Uses lightweight reinforcement learning to teach models to reason about instruction precedence, improving instruction-following performance and achieving ~20% improvement on the IHEval conflict setup.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to safety-critical scenarios: treating safety as conflict resolution between adversarial user inputs and higher-priority policies reduces jailbreak and prompt-injection attack success rate by up to ~20%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuo Zheng', 'Vidhisha Balachandran', 'Chan Young Park', 'Faeze Brahman', 'Sachin Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04694</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.16295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenLVLM-MIA, a controlled benchmark of 6,000 images for evaluating membership inference attacks (MIA) on large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that prior high MIA success rates often stem from distributional biases in dataset construction rather than true membership leakage.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art MIA methods perform near chance on the balanced benchmark, highlighting limits of current attack methods and the need for improved privacy evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryoto Miyamoto', 'Xin Fan', 'Fuyuko Kido', 'Tsuneo Matsumoto', 'Hayato Yamana']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'vision-language-models', 'benchmark', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16295</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Diffusion for Robust Reinforcement Learning</title><link>https://arxiv.org/abs/2509.23846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL), using diffusion models to generate full trajectories conditioned to be worst-case during training.&lt;/li&gt;&lt;li&gt;Leverages conditional sampling and a connection to Conditional Value at Risk (CVaR) optimization to explicitly optimize robust (worst-case) cumulative returns.&lt;/li&gt;&lt;li&gt;Demonstrates empirically across benchmarks that AD-RRL improves robustness and performance compared to existing robust RL methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniele Foffano', 'Alessio Russo', 'Alexandre Proutiere']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-training', 'diffusion-models', 'robust-RL', 'CVaR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23846</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ParlAI Vote: A Web Platform for Analyzing Gender and Political Bias in Large Language Models</title><link>https://arxiv.org/abs/2509.16264</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ParlAI Vote is an interactive web platform linking European Parliament debates, speeches, demographic metadata, and roll-call votes to evaluate LLMs on vote prediction and gender classification.&lt;/li&gt;&lt;li&gt;It visualizes the EuroParlVote benchmark, compares model predictions to real outcomes, breaks down errors by demographic groups, and exposes systematic performance bias in state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;The platform surfaces model reasoning, supports counterfactual scenarios, and lowers the barrier for reproducing, auditing, and investigating model behavior in political contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Lin', 'Hange Liu', 'Yingying Zhuang', 'Xutao Mao', 'Jingwei Shi', 'Xudong Han', 'Tianyu Shi', 'Jinrui Yang']&lt;/li&gt;&lt;li&gt;Tags: ['bias-analysis', 'evaluation', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16264</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Apertus: Democratizing Open and Compliant LLMs for Global Language Environments</title><link>https://arxiv.org/abs/2509.14233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Apertus is an open suite of LLMs (8B and 70B) trained on reproducible, openly available data with retroactive respect for robots.txt and filtering for non-permissive, toxic, and personally identifiable information.&lt;/li&gt;&lt;li&gt;The authors adopt the Goldfish objective during pretraining to strongly suppress verbatim memorization, aiming to mitigate privacy and memorization risks while preserving downstream performance.&lt;/li&gt;&lt;li&gt;Models are trained on ~15T tokens covering 1800+ languages (~40% non-English) and the project releases full artifacts (data pipelines, code, checkpoints, evaluations) under a permissive license for auditability and extension.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Project Apertus', "Alejandro Hern\\'andez-Cano", 'Alexander H\\"agele', 'Allen Hao Huang', 'Angelika Romanou', 'Antoni-Joan Solergibert', 'Barna Pasztor', 'Bettina Messmer', 'Dhia Garbaya', 'Eduard Frank \\v{D}urech', 'Ido Hakimi', "Juan Garc\\'ia Giraldo", 'Mete Ismayilzada', 'Negar Foroutan', 'Skander Moalla', 'Tiancheng Chen', 'Vinko Sabol\\v{c}ec', 'Yixuan Xu', 'Michael Aerni', 'Badr AlKhamissi', "In\\'es Altemir Mari\\~nas", 'Mohammad Hossein Amani', 'Matin Ansaripour', 'Ilia Badanin', 'Harold Benoit', 'Emanuela Boros', 'Nicholas Browning', 'Fabian B\\"osch', 'Maximilian B\\"other', 'Niklas Canova', 'Camille Challier', 'Clement Charmillot', 'Jonathan Coles', 'Jan Deriu', 'Arnout Devos', 'Lukas Drescher', 'Daniil Dzenhaliou', 'Maud Ehrmann', 'Dongyang Fan', 'Simin Fan', 'Silin Gao', 'Miguel Gila', "Mar\\'ia Grandury", 'Diba Hashemi', 'Alexander Hoyle', 'Jiaming Jiang', 'Mark Klein', 'Andrei Kucharavy', 'Anastasiia Kucherenko', 'Frederike L\\"ubeck', 'Roman Machacek', 'Theofilos Manitaras', 'Andreas Marfurt', 'Kyle Matoba', 'Simon Matrenok', 'Henrique Mendon\\c{c}a', 'Fawzi Roberto Mohamed', 'Syrielle Montariol', 'Luca Mouchel', 'Sven Najem-Meyer', 'Jingwei Ni', 'Gennaro Oliva', 'Matteo Pagliardini', 'Elia Palme', 'Andrei Panferov', "L\\'eo Paoletti", 'Marco Passerini', 'Ivan Pavlov', 'Auguste Poiroux', 'Kaustubh Ponkshe', 'Nathan Ranchin', 'Javi Rando', 'Mathieu Sauser', 'Jakhongir Saydaliev', 'Muhammad Ali Sayfiddinov', 'Marian Schneider', 'Stefano Schuppli', 'Marco Scialanga', 'Andrei Semenov', 'Kumar Shridhar', 'Raghav Singhal', 'Anna Sotnikova', 'Alexander Sternfeld', 'Ayush Kumar Tarun', 'Paul Teiletche', 'Jannis Vamvas', 'Xiaozhe Yao', 'Hao Zhao', 'Alexander Ilic', 'Ana Klimovic', 'Andreas Krause', 'Caglar Gulcehre', 'David Rosenthal', 'Elliott Ash', 'Florian Tram\\`er', 'Joost VandeVondele', 'Livio Veraldi', 'Martin Rajman', 'Thomas Schulthess', 'Torsten Hoefler', 'Antoine Bosselut', 'Martin Jaggi', 'Imanol Schlag']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-compliance', 'memorization-mitigation', 'safety', 'open-source']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14233</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</title><link>https://arxiv.org/abs/2509.06996</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs psychophysics-inspired benchmarks for Chinese logographs and English words by splicing, recombining, and overlaying glyphs to create stimuli that are legible to humans but often unreadable to VLMs.&lt;/li&gt;&lt;li&gt;Demonstrates that contemporary vision-language models suffer severe performance drops on these perturbations, producing unrelated or incoherent outputs, and argues this reflects under-reliance on compositional symbol priors.&lt;/li&gt;&lt;li&gt;Releases stimuli generation code, prompts, and evaluation protocols to enable replication and follow-up work; calls for architectures and training strategies that encode segmentation, composition, and binding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Ting Xu', 'Gelei Deng', 'Runyi Hu', 'Han Qiu', 'Tianwei Zhang', 'Qing Guo', 'Ivor Tsang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'benchmarking', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06996</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VeriLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs</title><link>https://arxiv.org/abs/2508.21393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VeriLoRA, a framework that combines LoRA parameter-efficient fine-tuning with zero-knowledge proofs to enable provable correctness and privacy during LLM fine-tuning.&lt;/li&gt;&lt;li&gt;Uses cryptographic building blocks (lookup arguments, sumcheck protocols, polynomial commitments) to verify arithmetic and non-arithmetic operations across forward propagation, backpropagation, and parameter updates.&lt;/li&gt;&lt;li&gt;Implements GPU-accelerated ZKP primitives and demonstrates practicality on open-source LLMs (up to 13B parameters), highlighting scalability for real-world secure fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guofu Liao', 'Taotao Wang', 'Shengli Zhang', 'Jiqun Zhang', 'Shi Long', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['zero-knowledge proofs', 'verifiable computing', 'LLM fine-tuning', 'model security', 'privacy-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21393</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content</title><link>https://arxiv.org/abs/2507.19551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first robustness benchmark (Rainbow Noise) for harmful-meme detection on LGBTQ+ content by pairing four caption attacks with three image corruptions on the PrideMM dataset.&lt;/li&gt;&lt;li&gt;Evaluates two state-of-the-art multimodal detectors (MemeCLIP, MemeBLIP2) and shows varying sensitivity to textual edits and image corruptions.&lt;/li&gt;&lt;li&gt;Proposes a lightweight Text Denoising Adapter (TDA) that substantially improves MemeBLIP2's robustness to caption-based attacks.&lt;/li&gt;&lt;li&gt;Finds that models rely heavily on text, and that architectural choices and pretraining data materially affect robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ran Tong', 'Songtao Wei', 'Jiaqi Liu', 'Lanruo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial attacks', 'harmful content detection', 'multimodal safety', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19551</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions</title><link>https://arxiv.org/abs/2506.06409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an optimization framework for designing LLM watermarks that maximize detection likelihood while minimizing text distortion, with focus on low-entropy next-token settings (e.g., code).&lt;/li&gt;&lt;li&gt;Introduces two tunable watermarking schemes, HeavyWater and SimplexWater, which can be applied to any LLM and are agnostic to how random side information is generated.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis (including connections to coding theory) and empirical benchmarks showing high detection accuracy with minimal quality degradation in low-entropy regimes.&lt;/li&gt;&lt;li&gt;Releases code implementation for reproducibility and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dor Tsur', 'Carol Xuan Long', 'Claudio Mayrink Verdun', 'Hsiang Hsu', 'Chen-Fu Chen', 'Haim Permuter', 'Sajani Vithana', 'Flavio P. Calmon']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'provenance', 'model safety', 'low-entropy generation', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06409</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees</title><link>https://arxiv.org/abs/2505.19238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Considers robust constrained Markov decision processes (RCMDPs): maximize cumulative reward while satisfying constraints under worst-case stochastic models within an uncertainty set around an unknown nominal model.&lt;/li&gt;&lt;li&gt;Proposes a novel algorithmic technique that focuses on minimizing the constraint value function to ensure feasibility and, once constraints are met, maximizes the robust reward; avoids relying on strong duality and handles differing worst-case models for reward vs constraint.&lt;/li&gt;&lt;li&gt;Provides iteration complexity guarantees: finds an ε-suboptimal feasible policy in O(ε^{-2}) iterations and removes the need for a binary search, yielding 4x–6x computational improvements over prior methods depending on the discount factor.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sourav Ganguly', 'Arnob Ghosh', 'Kishan Panaganti', 'Adam Wierman']&lt;/li&gt;&lt;li&gt;Tags: ['Robust Reinforcement Learning', 'Constrained MDP', 'Safety / Robustness', 'Theoretical Guarantees', 'Adversarial Model Mismatch']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19238</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title><link>https://arxiv.org/abs/2505.15216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BountyBench, a benchmarking framework that measures offensive and defensive capabilities of AI agents against 25 real-world codebases with 40 monetized bug bounties covering 9 OWASP Top 10 risks.&lt;/li&gt;&lt;li&gt;Defines three task types (Detect, Exploit, Patch) and a new, generalizable Detect success indicator that localizes vulnerabilities across types to capture the vulnerability lifecycle.&lt;/li&gt;&lt;li&gt;Evaluates 10 agents (including Codex CLI, Claude variants, GPT-4.1, Gemini, Llama 4, etc.), reporting task-level performance and mapping results to dollar impact for real-world security outcomes.&lt;/li&gt;&lt;li&gt;Proposes an information-based difficulty interpolation strategy and provides reproducible environment setups (servers, databases, packages) to support realistic offensive/defensive benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy K. Zhang', 'Joey Ji', 'Celeste Menders', 'Riya Dulepet', 'Thomas Qin', 'Ron Y. Wang', 'Junrong Wu', 'Kyleen Liao', 'Jiliang Li', 'Jinghan Hu', 'Sara Hong', 'Nardos Demilew', 'Shivatmica Murgai', 'Jason Tran', 'Nishka Kacheria', 'Ethan Ho', 'Denis Liu', 'Lauren McLane', 'Olivia Bruvik', 'Dai-Rong Han', 'Seungwoo Kim', 'Akhil Vyas', 'Cuiyuanxiu Chen', 'Ryan Li', 'Weiran Xu', 'Jonathan Z. Ye', 'Prerit Choudhary', 'Siddharth M. Bhatia', 'Vikram Sivashankar', 'Yuxuan Bao', 'Dawn Song', 'Dan Boneh', 'Daniel E. Ho', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial agents', 'cybersecurity benchmark', 'vulnerability detection and exploitation', 'AI offensive/defensive evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15216</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models</title><link>https://arxiv.org/abs/2505.13514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies induction heads as a mechanistic driver of the 'repetition curse' in LLMs, where models produce repetitive or cyclic token sequences.&lt;/li&gt;&lt;li&gt;Defines 'toxicity' of induction heads as their tendency to dominate output logits during repetition, suppressing contributions from other attention heads.&lt;/li&gt;&lt;li&gt;Provides empirical/mechanistic analysis linking induction-head behavior to repetitive generation and discusses implications for model design and training.&lt;/li&gt;&lt;li&gt;Proposes an attention-head regularization technique aimed at reducing induction-head dominance to mitigate repetition and improve output diversity/coherence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuxun Wang', 'Qingyu Yin', 'Chak Tou Leong', 'Qiang Zhang', 'Linyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'interpretability', 'model failure modes', 'mitigation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13514</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Compound AI Systems via System-level DPO</title><link>https://arxiv.org/abs/2502.17721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SysDPO, an extension of Direct Preference Optimization (DPO) to align compound AI systems modeled as Directed Acyclic Graphs (DAGs) of interacting components.&lt;/li&gt;&lt;li&gt;Introduces two variants (SysDPO-Direct and SysDPO-Sampling) to handle cases with or without system-specific preference datasets and overcome non-differentiable component interactions.&lt;/li&gt;&lt;li&gt;Empirically evaluates joint alignment on examples including a language model paired with a diffusion model and an LLM collaboration system.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangwen Wang', 'Yibo Jacky Zhang', 'Zhoujie Ding', 'Katherine Tsai', 'Haolun Wu', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['system-level alignment', 'direct preference optimization (DPO)', 'compound AI systems', 'multimodal alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17721</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning via Information Theoretic Regularization</title><link>https://arxiv.org/abs/2502.05684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an information-theoretic regularization framework for removing (unlearning) both individual data points and specific features from learned models.&lt;/li&gt;&lt;li&gt;Proposes the 'Marginal Unlearning Principle' with auditable, provable definitions and shows sufficiency/necessity connections to existing approximate unlearning notions.&lt;/li&gt;&lt;li&gt;Provides analytic solutions for optimal feature unlearning under various information-theoretic objectives and shows connections to optimal transport and extremal sigma-algebras.&lt;/li&gt;&lt;li&gt;Demonstrates theoretical results with numerical simulations and claims applicability to deep learning with arbitrary training objectives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shizhou Xu', 'Thomas Strohmer']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'information-theory', 'feature-unlearning', 'provable-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05684</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Diffusion Models with Noise-Conditioned Perception</title><link>https://arxiv.org/abs/2406.17636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes optimizing diffusion models using a perceptual objective in the U-Net (noise-conditioned) embedding space rather than pixel/VAE latent space to better match human perception.&lt;/li&gt;&lt;li&gt;Fine-tunes Stable Diffusion 1.5 and SDXL with Direct Preference Optimization (DPO), Contrastive Preference Optimization (CPO), and supervised fine-tuning (SFT) in that embedding space, achieving stronger prompt alignment and visual appeal.&lt;/li&gt;&lt;li&gt;Reports substantial gains in human preference metrics and compute efficiency versus standard latent-space preference tuning (e.g., SDXL improvements on PartiPrompts), and provides code/LoRA weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Gambashidze', 'Anton Kulikov', 'Yuriy Sosnin', 'Ilya Makarov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'diffusion models', 'prompt following', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17636</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation</title><link>https://arxiv.org/abs/2405.13068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JailMine, a token-level logit manipulation method that 'mines' for affirmative outputs and iteratively reduces rejection likelihood to elicit harmful responses.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and efficiency across multiple LLMs and datasets, reporting ~95% success rates while reducing time by ~86% versus prior token-level techniques.&lt;/li&gt;&lt;li&gt;Focuses on overcoming scalability and defense-evolution challenges in jailbreaking, highlighting implications for model robustness and defensive design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxi Li', 'Yi Liu', 'Yuekang Li', 'Ling Shi', 'Gelei Deng', 'Shengquan Chen', 'Kailong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'red teaming', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.13068</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs</title><link>https://arxiv.org/abs/2512.01797</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a very sparse subset of neurons (&lt;0.1%) whose activations reliably predict hallucination occurrences across diverse scenarios.&lt;/li&gt;&lt;li&gt;Shows causal impact via controlled interventions: manipulating these H-Neurons changes over-compliance and hallucination behavior.&lt;/li&gt;&lt;li&gt;Traces the origin of H-Neurons to pre-trained base models, indicating these predictors emerge during pretraining rather than only during fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Gao', 'Huimin Chen', 'Chaojun Xiao', 'Zhiyi Chen', 'Zhiyuan Liu', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'neuron interpretability', 'causal interventions', 'LLM safety', 'pretraining origins']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01797</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models</title><link>https://arxiv.org/abs/2412.01784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method to detect sandbagging (strategic underperformance) by injecting varying magnitudes of noise into model weights and observing performance changes.&lt;/li&gt;&lt;li&gt;Shows non-sandbagging models degrade predictably with noise while sandbagging models can anomalously improve, indicating disruption of underperformance mechanisms.&lt;/li&gt;&lt;li&gt;Demonstrates the technique across architectures, sizes, and sandbagging techniques, and reports cases where noise reveals a model's full capabilities (e.g., Mistral Large 120B).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cameron Tice', 'Philipp Alexander Kreer', 'Nathan Helm-Burger', 'Prithviraj Singh Shahani', 'Fedor Ryzhenkov', 'Fabien Roger', 'Clement Neo', 'Jacob Haimes', 'Felix Hofst\\"atter', 'Teun van der Weij']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'evaluation', 'sandbagging detection', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01784</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models</title><link>https://arxiv.org/abs/2512.03026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuous ethical evaluation of LLMs that autonomously generates and refines ethical scenarios.&lt;/li&gt;&lt;li&gt;Combines three layers: lexical integrity analysis, semantic risk estimation, and reasoning-based judgment modeling to assess moral stability over time.&lt;/li&gt;&lt;li&gt;Empirical evaluation on GPT-4-Turbo and DeepSeek shows stable moral coherence, a strong inverse correlation between ethical and toxicity measures (rET = -0.81, p &lt; 0.001), and near-zero association with response latency.&lt;/li&gt;&lt;li&gt;Argues for a model-agnostic, reproducible approach to scalable continuous auditing and computational morality in autonomous AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeid Jamshidi', 'Kawser Wazed Nafi', 'Arghavan Moradi Dakhel', 'Negar Shahabi', 'Foutse Khomh']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'ethical-auditing', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03026</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic</title><link>https://arxiv.org/abs/2512.02987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework combining classical NLP, symbolic computation, and a fine-tuned LLM to translate English into formal logic and CNF for satisfiability solving.&lt;/li&gt;&lt;li&gt;Targets reduction of hallucinations in logical translation by fine-tuning on grammar-specific data and using symbolic verification steps.&lt;/li&gt;&lt;li&gt;Early experiments show the fine-tuned model can correct hallucination types produced by the base model, improving reliability of CNF generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyu Pan', 'Dheeraj Kodakandla', 'Mahfuza Farooque']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'LLM fine-tuning', 'robustness', 'logical translation', 'formal methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02987</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lumos: Let there be Language Model System Certification</title><link>https://arxiv.org/abs/2512.02966</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Lumos, an imperative probabilistic programming DSL for specifying prompt distributions and formally certifying language model system (LMS) behaviors via integration with statistical certifiers.&lt;/li&gt;&lt;li&gt;Provides hybrid (operational and denotational) semantics and a small set of composable constructs capable of encoding complex relational and temporal LMS specifications.&lt;/li&gt;&lt;li&gt;Demonstrates application to vision-language model (VLM) safety in autonomous driving, finding that Qwen-VL produces incorrect/unsafe responses with ≥90% probability in certain right-turn rainy scenarios and enabling discovery of concrete failure cases.&lt;/li&gt;&lt;li&gt;Emphasizes modular, extensible specification programs to keep certification adaptable to evolving threat landscapes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isha Chaudhary', 'Vedaant Jain', 'Avaljot Singh', 'Kavya Sachdeva', 'Sayan Ranu', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Certification &amp; evaluation', 'Formal specification DSL', 'VLM safety (autonomous driving)', 'Statistical certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02966</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</title><link>https://arxiv.org/abs/2512.02834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TACO, a test-time-scaling (TTS) framework that uses a lightweight pseudo-count estimator to verify and select high-fidelity action chunks at inference, effectively enforcing an anti-exploration constraint.&lt;/li&gt;&lt;li&gt;Targets pre-trained Vision-Language-Action (VLA) models (flow/diffusion-based) that exhibit inference-time fragility due to distribution shift between pretraining modes and downstream success modes; the method is gradient-free and applied only at inference.&lt;/li&gt;&lt;li&gt;Demonstrates improved inference stability and success rates across multiple simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform, offering a computationally cheap alternative to RL fine-tuning for stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Yang', 'Yang Zhang', 'Haoran He', 'Ling Pan', 'Xiu Li', 'Chenjia Bai', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['inference-time safety', 'robustness', 'offline RL / anti-exploration', 'distribution shift', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02834</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defense That Attacks: How Robust Models Become Better Attackers</title><link>https://arxiv.org/abs/2512.02830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that adversarially trained (AT) models produce adversarial perturbations that transfer more effectively to other models than perturbations from standard models.&lt;/li&gt;&lt;li&gt;Empirically evaluates transferability across a diverse zoo of 36 models (CNNs and ViTs), demonstrating a paradox where robustness training increases the model's ability to be a strong attacker.&lt;/li&gt;&lt;li&gt;Releases models and code for reproducibility and argues robustness evaluations should measure both resistance to transferred attacks and propensity to generate transferable adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Awad', 'Mahmoud Akrm', 'Walid Gomaa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial ML', 'adversarial training', 'transferability', 'robustness', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02830</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning-Aware Multimodal Fusion for Hateful Video Detection</title><link>https://arxiv.org/abs/2512.02743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reasoning-Aware Multimodal Fusion (RAMF) for hateful video detection, combining Local-Global Context Fusion and Semantic Cross Attention for fine-grained multimodal interaction.&lt;/li&gt;&lt;li&gt;Introduces an 'adversarial reasoning' three-stage process where a vision-language model generates objective descriptions, hate-assumed inferences, and non-hate-assumed inferences to enrich contextual understanding.&lt;/li&gt;&lt;li&gt;Evaluated on two real-world hateful video datasets, reporting improvements in Macro-F1 and hate-class recall over prior state-of-the-art.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuonan Yang', 'Tailin Chen', 'Jiangbei Yue', 'Guangliang Cheng', 'Jianbo Jiao', 'Zeyu Fu']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'hateful-speech-detection', 'multimodal-fusion', 'vision-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02743</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation</title><link>https://arxiv.org/abs/2512.02689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of seven model-merging algorithms (Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, Nearswap) for mitigating social bias in LLMs.&lt;/li&gt;&lt;li&gt;Evaluates 13 open-weight models across GPT, LLaMA, and Qwen families on three bias datasets (BBQ, BOLD, HONEST) and downstream performance (SuperGLUE).&lt;/li&gt;&lt;li&gt;Finds a trade-off: stronger bias reduction often degrades downstream capabilities (reading comprehension, commonsense, causal reasoning); SLERP at moderate weights best balances bias reduction and overall performance.&lt;/li&gt;&lt;li&gt;Identifies Linear, SLERP, and Nearswap as consistently effective choices, and warns that excessive debiasing or inappropriate merging can harm linguistic abilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiki Shirafuji', 'Tatsuhiko Saito', 'Yasutomo Kimura']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'model merging', 'LLM safety', 'fairness evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02689</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions</title><link>https://arxiv.org/abs/2512.02682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that single-agent safety measures (prompts, fine-tuning, moderation) do not scale to LLM-to-LLM ecosystems where outputs are reused across chains of agents.&lt;/li&gt;&lt;li&gt;Introduces the Emergent Systemic Risk Horizon (ESRH) framework to formalize how interaction structures can produce instability and collective failures despite locally compliant agents.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of micro-, meso-, and macro-level failure modes specific to interacting LLM systems.&lt;/li&gt;&lt;li&gt;Proposes 'InstitutionalAI,' an architecture for embedding adaptive, system-level oversight in multi-agent LLM deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Marcello Galisai', 'Federico Pierucci', 'Marcantonio Bracale', 'Matteo Prandi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent safety', 'systemic risk', 'alignment', 'multi-agent oversight', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02682</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2512.02657</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses continual machine unlearning (sequential deletion requests) for text-to-image diffusion models, identifying stability issues when naively applying one-shot unlearning.&lt;/li&gt;&lt;li&gt;Proposes a generative distillation-based framework that casts each unlearning step as a multi-objective teacher-student distillation problem, incorporating continual learning principles to preserve retained concepts and image quality.&lt;/li&gt;&lt;li&gt;Demonstrates on a 10-step sequential benchmark that the method more effectively forgets target concepts with less collateral damage and degradation of generative fidelity compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naveen George', 'Naoki Murata', 'Yuhta Takida', 'Konda Reddy Mopuri', 'Yuki Mitsufuji']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy/compliance', 'diffusion models', 'continual learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02657</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography</title><link>https://arxiv.org/abs/2512.02625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CryptoQA, a large-scale (2M+) question-answering dataset focused on cryptography with contextual metadata.&lt;/li&gt;&lt;li&gt;Benchmarks 15 state-of-the-art LLMs on factual accuracy, mathematical/formal reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples.&lt;/li&gt;&lt;li&gt;Finds substantial deficits in LLM performance on formal cryptographic tasks and shows fine-tuning on CryptoQA can improve cryptographic capabilities.&lt;/li&gt;&lt;li&gt;Provides expert qualitative reviews and a gold-standard baseline for further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayar Elfares', 'Pascal Reisert', 'Tilman Dietz', 'Manpa Barman', 'Ahmed Zaki', 'Ralf K\\"usters', 'Andreas Bulling']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'robustness', 'adversarial examples', 'cryptography', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02625</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System</title><link>https://arxiv.org/abs/2512.02567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of an automated C-to-Rust translation pipeline that uses LLMs in a generate-and-check loop: generated Rust is compiled and behavior-checked against the original C, with failed cases fed back to the LLM for repair.&lt;/li&gt;&lt;li&gt;Evaluates how (a) automated feedback loops, (b) choice of LLM, and (c) behavior-preserving code perturbations affect translation success and robustness.&lt;/li&gt;&lt;li&gt;Finds that model choice matters a lot without feedback, but feedback loops reduce inter-model differences and improve robustness under code perturbations; certain perturbation-induced diversity can even boost performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Weiss', 'Jesko Hecking-Harbusch', 'Jochen Quante', 'Matthias Woehrle']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'automated feedback/repair', 'code translation', 'behavior-preserving perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02567</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce</title><link>https://arxiv.org/abs/2512.02555</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ADORE, a framework combining LLM-generated intent-aligned training data, adversarial example synthesis, and knowledge distillation to improve e-commerce relevance.&lt;/li&gt;&lt;li&gt;Uses a Chain-of-Thought LLM + Kahneman-Tversky Optimization to automate annotation aligned with user behavior.&lt;/li&gt;&lt;li&gt;Introduces an Error-type-aware Data Synthesis module that auto-generates adversarial examples to improve model robustness.&lt;/li&gt;&lt;li&gt;Applies Key-attribute-enhanced Knowledge Distillation to transfer domain attribute hierarchies into a deployable student model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Fang', 'Donghao Xie', 'Ming Pang', 'Chunyuan Yuan', 'Xue Jiang', 'Changping Peng', 'Zhangang Lin', 'Zheng Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'adversarial-example-generation', 'data-augmentation', 'LLM-generated-training-data', 'knowledge-distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02555</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</title><link>https://arxiv.org/abs/2512.02485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'reasoning detachment' in medical VLMs where fluent text explanations are not grounded in image evidence and proposes mitigation via a hierarchical multi-agent framework.&lt;/li&gt;&lt;li&gt;Introduces UCAgents: enforces unidirectional convergence (agents cannot change stance) and structured evidence auditing with a one-round inquiry to limit open-ended discussion and anchor decisions to visual evidence.&lt;/li&gt;&lt;li&gt;Formalizes a dual-noise bottleneck (visual ambiguity and textual noise) and demonstrates empirical gains on medical VQA benchmarks (e.g., +6.0% on PathVQA) with large token-cost reductions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianhan Feng', 'Zhongzhen Huang', 'Yakun Zhu', 'Xiaofan Zhang', 'Qi Dou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination_mitigation', 'medical_VQA', 'multimodal_reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02485</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents</title><link>https://arxiv.org/abs/2512.02445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates LLM agents operating with very long contexts (up to 1M–2M tokens) and tool-calling behavior, focusing on both capabilities and safety (refusals).&lt;/li&gt;&lt;li&gt;Finds severe and non-monotonic degradation in task performance and large, unpredictable shifts in refusal rates as context length, type, and placement change (e.g., major drops by 100K–200K tokens for several models).&lt;/li&gt;&lt;li&gt;Demonstrates model-specific instability (e.g., GPT-4.1-nano refusal rate rising from ~5% to ~40%, Grok 4 Fast dropping from ~80% to ~10%) and argues current safety evaluation paradigms for agents and long-horizon tasks are insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsimur Hadeliya', 'Mohammad Ali Jauhar', 'Nidhi Sakpal', 'Diogo Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'long-context robustness', 'agentic systems', 'refusal/jailbreak behavior', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02445</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vehicle Dynamics Embedded World Models for Autonomous Driving</title><link>https://arxiv.org/abs/2512.02417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Vehicle Dynamics embedded Dreamer (VDD) which decouples ego-vehicle dynamics modeling from environmental transition dynamics in world models for autonomous driving.&lt;/li&gt;&lt;li&gt;Introduces Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT) to improve robustness to variations in vehicle parameters.&lt;/li&gt;&lt;li&gt;Demonstrates in simulation improved driving performance and robustness to vehicle-dynamics variations compared to prior joint-learning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huiqian Li', 'Wei Pan', 'Haodong Zhang', 'Jin Huang', 'Zhihua Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'world-models', 'robustness', 'vehicle-dynamics', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02417</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering</title><link>https://arxiv.org/abs/2512.02363</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KARMA, a framework for domain-adaptive QA combining a dual-encoder to fuse structured and unstructured knowledge, a gated memory unit to control external knowledge integration, and a safety-aware controllable decoder that uses safety classification and guided generation to mitigate unsafe outputs.&lt;/li&gt;&lt;li&gt;Targets sensitive service domains (e.g., healthcare policies, government welfare) to improve factual consistency, context alignment, answer quality, and safety.&lt;/li&gt;&lt;li&gt;Evaluates on a proprietary QA dataset and reports improvements in both answer quality and safety compared to strong baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei Fu', 'Xiang Chen', 'Kaige Gao Xinyue Huang', 'Kejian Tong']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'controllable-generation', 'knowledge-integration', 'domain-adaptive-QA', 'QA-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02363</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VACoT: Rethinking Visual Data Augmentation with VLMs</title><link>https://arxiv.org/abs/2512.02361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VACoT: an inference-time 'visual augmentation chain-of-thought' that dynamically invokes post-hoc image augmentations (e.g., denoising) to improve VLM perception.&lt;/li&gt;&lt;li&gt;Uses an agentic reinforcement-learning controller with a conditional reward that favors necessary augmentations and penalizes verbose outputs.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness across 13 perception benchmarks and introduces AdvOCR to evaluate generalization in OCR-related adversarial scenarios.&lt;/li&gt;&lt;li&gt;Focuses on reducing training overhead by broadening query image views at inference, targeting out-of-distribution and adversarial inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengzhuo Xu', 'Chong Sun', 'SiNan Du', 'Chen Li', 'Jing Lyu', 'Chun Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'VLM robustness', 'inference-time augmentation', 'OCR attacks/defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02361</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers</title><link>https://arxiv.org/abs/2512.02318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates 7 commercial and open-source multimodal LLMs on 18 real-world visual CAPTCHA task types, measuring single-shot accuracy, retry success, latency, and per-solve cost.&lt;/li&gt;&lt;li&gt;Analyzes effects of prompt engineering and few-shot demonstrations on solver effectiveness and inspects model reasoning traces to understand success/failure modes.&lt;/li&gt;&lt;li&gt;Identifies CAPTCHA categories that remain challenging (fine-grained localization, multi-step spatial reasoning, cross-frame consistency) and derives defense guidelines for strengthening CAPTCHA tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyu Wang', 'Changjia Zhu', 'Yuanbo Zhou', 'Lingyao Li', 'Xu He', 'Junjie Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial automation', 'CAPTCHA security', 'multimodal robustness', 'defense guidance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02318</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models</title><link>https://arxiv.org/abs/2512.02299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HealthContradict, an expert-verified dataset of 920 biomedical QA instances, each with a question, a factual answer, and two contradictory documents.&lt;/li&gt;&lt;li&gt;Evaluates language models under different prompt/context settings (correct, incorrect, contradictory) to measure impact of conflicting biomedical contexts on model answers.&lt;/li&gt;&lt;li&gt;Finds that fine-tuned biomedical LMs leverage both parametric knowledge and contextual information; performance hinges on exploiting correct context and resisting misleading/incorrect context.&lt;/li&gt;&lt;li&gt;Claims the benchmark gives finer-grained distinctions of contextual reasoning and robustness compared to existing medical QA evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boya Zhang', 'Alban Bornet', 'Rui Yang', 'Nan Liu', 'Douglas Teodoro']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'biomedical-qa', 'contextual-reasoning', 'misinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02299</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas</title><link>https://arxiv.org/abs/2512.02062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Superpixel Attack: a black-box adversarial attack that uses superpixel-based image divisions instead of rectangular patches.&lt;/li&gt;&lt;li&gt;Introduces a 'versatile search' method to guide perturbation selection over superpixels.&lt;/li&gt;&lt;li&gt;Reports average attack success rate improvement of ~2.10% over existing black-box attacks, evaluated on models described as robust.&lt;/li&gt;&lt;li&gt;Code repository provided for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Issa Oe', 'Keiichiro Yamamura', 'Hiroki Ishikura', 'Ryo Hamahira', 'Katsuki Fujisawa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'black-box', 'robustness', 'computer-vision', 'superpixel']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02062</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism</title><link>https://arxiv.org/abs/2512.01568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of 24 frontier LLMs using three paradigms: Implicit Association Test (IAT) for implicit altruism, a forced binary choice task for behavioral altruism, and a self-assessment scale for explicit altruism.&lt;/li&gt;&lt;li&gt;Key findings: strong implicit pro-altruism bias (mean IAT = 0.87), above-chance prosocial behavior overall (65.6% vs 50%) with wide model variance (48–85%), and a weak correlation between implicit associations and actual behavior (r = .22, p = .29).&lt;/li&gt;&lt;li&gt;Models systematically overestimate their own altruism (self-report 77.5% vs behavior 65.6%), termed the 'virtue signaling gap'; authors propose the Calibration Gap (discrepancy between self-report and behavior) as a standardized alignment metric.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandro Andric']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-benchmarking', 'LLM-calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01568</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2512.00663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework that organizes proprietary knowledge and LLM-generated content into interactive visual knowledge graphs linking model assertions to source documents and confidence signals.&lt;/li&gt;&lt;li&gt;Aims to surface ‘hallucination zones’ and weak reasoning chains by making evidence provenance and confidence explicit, enabling users to diagnose inconsistencies.&lt;/li&gt;&lt;li&gt;Proposes a human-in-the-loop workflow where users supply corrective feedback through the visualization, forming a structured loop to improve model reliability without relying solely on gold Q&amp;A or secondary model verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanmay Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination detection', 'model reliability', 'human-in-the-loop', 'explainability', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00663</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</title><link>https://arxiv.org/abs/2512.03005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames LLM-mediated conflict resolution as two subtasks: judgment (evaluating fairness/emotional dynamics) and steering (generating empathetic, de-escalatory messages).&lt;/li&gt;&lt;li&gt;Builds a large Reddit-based dataset and a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison.&lt;/li&gt;&lt;li&gt;Finds API-based models outperform open-source models on reasoning and intervention alignment for mediation tasks, while noting limitations and risks of current LLMs in this role.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dawei Li', 'Abdullah Alnaibari', 'Arslan Bisharat', 'Manny Sandoval', 'Deborah Hall', 'Yasin Silva', 'Huan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'moderation', 'evaluation', 'human-AI interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03005</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Invasive Context Engineering to Control Large Language Models</title><link>https://arxiv.org/abs/2512.03001</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of LLMs to jailbreaks and adversarial control that grows with context length.&lt;/li&gt;&lt;li&gt;Proposes "Invasive Context Engineering": inserting control sentences into the model context to steer behavior and reduce misbehavior without retraining.&lt;/li&gt;&lt;li&gt;Claims the method can be applied to Chain-of-Thought processes to prevent scheming and other unsafe internal reasoning.&lt;/li&gt;&lt;li&gt;Highlights advantage of avoiding data/training requirements for long-context security guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Rivasseau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'prompt injection', 'long-context security', 'chain-of-thought control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03001</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning</title><link>https://arxiv.org/abs/2512.02914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Martingale Score, an unsupervised regression-based metric that detects violations of the Bayesian martingale property in LLM belief updates.&lt;/li&gt;&lt;li&gt;Uses the score to quantify 'belief entrenchment' where current beliefs predict future belief updates, across tasks like forecasting, value-laden questions, and paper review.&lt;/li&gt;&lt;li&gt;Finds widespread violations across models and reasoning methods and shows the Martingale Score correlates with ground-truth accuracy where labels exist, making it a proxy for truth-seeking ability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhonghao He', 'Tianyi Qiu', 'Hirokazu Shirado', 'Maarten Sap']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-reasoning', 'evaluation-metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02914</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</title><link>https://arxiv.org/abs/2512.02530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Aetheria, a multimodal content safety framework using five collaborative agents that debate and adjudicate content for interpretable decisions.&lt;/li&gt;&lt;li&gt;Grounds judgments with RAG-based knowledge retrieval and produces detailed, traceable audit reports.&lt;/li&gt;&lt;li&gt;Introduces AIR-Bench and reports improved accuracy over baselines, particularly for identifying implicit risks in multimodal content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiang He', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Wei Cai', 'Haojie Cheng', 'Ziyan Shi', 'Ming Zhu', 'Haichuan Tang', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'multimodal safety', 'interpretability', 'multi-agent debate', 'RAG retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02530</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Synthetic Error Injection Fails to Elicit Self-Correction In Language Models</title><link>https://arxiv.org/abs/2512.02389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes supervised synthetic error injection: insert and mask artificial reasoning errors and train models to detect and correct them.&lt;/li&gt;&lt;li&gt;Finds this supervised approach fails to significantly improve self-correction across multiple models and simple synthetic tasks.&lt;/li&gt;&lt;li&gt;Shows distribution shift between synthetic errors and on-policy (model-generated) errors degrades correction ability even with good synthetic coverage.&lt;/li&gt;&lt;li&gt;Concludes on-policy RL methods are uniquely effective for eliciting self-correction compared to offline synthetic-error supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David X. Wu', 'Shreyas Kapur', 'Anant Sahai', 'Stuart Russell']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-correction', 'supervised fine-tuning', 'reinforcement learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02389</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</title><link>https://arxiv.org/abs/2512.02306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniGuard, a unified omni-modal guardrail system for safety/value enforcement across text, images, video, and audio using deliberate reasoning.&lt;/li&gt;&lt;li&gt;Constructs a large omni-modal safety dataset (~210K samples) with structured safety labels and expert-model-generated safety critiques for training.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness and generalization across 15 benchmarks for multimodal safety scenarios and claims unified policy enforcement and risk mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyu Zhu', 'Xiaofei Wen', 'Wenjie Jacky Mo', 'Tinghui Zhu', 'Yanan Xie', 'Peng Qi', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'guardrails', 'safety dataset', 'alignment', 'harm mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02306</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses</title><link>https://arxiv.org/abs/2512.02282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DialogGuard, a multi-agent framework to assess psychosocial risks in LLM responses across five high-severity dimensions (privacy violations, discrimination, mental manipulation, psychological harm, insulting behaviour).&lt;/li&gt;&lt;li&gt;Implements four LLM-as-judge pipelines (single-agent scoring, dual-agent correction, multi-agent debate, stochastic majority voting) grounded in a three-level rubric usable by humans and LLM judges.&lt;/li&gt;&lt;li&gt;Evaluates on PKU-SafeRLHF with human annotations, showing multi-agent mechanisms outperform non-LLM baselines and single-agent judging; dual-agent correction and majority voting offer best trade-offs, while debate yields higher recall but more false positives.&lt;/li&gt;&lt;li&gt;Releases open-source software and web UI with per-dimension scores and natural-language rationales; includes a formative practitioner study for prompt design, auditing, and supervision in sensitive applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Luo', 'Guy Laban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'psychosocial risk assessment', 'multi-agent evaluation', 'safety auditing', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02282</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence</title><link>https://arxiv.org/abs/2512.02280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies seven core deficiencies in current AI (lack of self-monitoring, meta-cognition, adaptive learning, goal restructuring, representational maintenance, embodied feedback, intrinsic agency).&lt;/li&gt;&lt;li&gt;Argues scaling and current architectures (deep learning/transformers) cannot by themselves achieve robust generalization, lifelong adaptability, or real-world autonomy.&lt;/li&gt;&lt;li&gt;Proposes moving toward cognitively grounded architectures inspired by neurocognitive principles to enable self-directed adaptation and dynamic representation management.&lt;/li&gt;&lt;li&gt;Advocates for oversight and governance mechanisms to keep autonomous systems interpretable, governable, and aligned with human values.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noorbakhsh Amiri Golilarz', 'Sindhuja Penchala', 'Shahram Rahimi']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'cognitive architectures', 'autonomy', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02280</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?</title><link>https://arxiv.org/abs/2512.02261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TradeTrap, a unified evaluation framework to stress-test LLM-based autonomous trading agents under adversarial and faulty conditions.&lt;/li&gt;&lt;li&gt;Targets four core agent components: market intelligence, strategy formulation, portfolio/ledger handling, and trade execution; injects controlled system-level perturbations.&lt;/li&gt;&lt;li&gt;Runs closed-loop historical backtesting on real US equity market data with identical initial conditions for fair comparisons.&lt;/li&gt;&lt;li&gt;Finds that small perturbations at a single component can propagate and induce extreme concentration, runaway exposure, and large portfolio drawdowns; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lewen Yan', 'Jilin Mei', 'Tianyi Zhou', 'Lige Huang', 'Jie Zhang', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial robustness', 'autonomous agents', 'financial AI safety', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02261</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From monoliths to modules: Decomposing transducers for efficient world modelling</title><link>https://arxiv.org/abs/2512.02193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to decompose complex world models (transducers) into sub-transducers operating on distinct input-output subspaces.&lt;/li&gt;&lt;li&gt;Aims to enable parallelizable, interpretable, and distributed alternatives to monolithic world modelling for efficiency.&lt;/li&gt;&lt;li&gt;Positions modular decomposition as a way to improve structural transparency and support safer training/evaluation of agents in sandbox environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Boyd', 'Franz Nowak', 'David Hyland', 'Manuel Baltieri', 'Fernando E. Rosas']&lt;/li&gt;&lt;li&gt;Tags: ['world models', 'modular architectures', 'interpretability', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02193</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title><link>https://arxiv.org/abs/2512.02080</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models the interaction between an LLM and a formal verifier as a discrete-time Markov chain with an error-reduction probability δ and proves an LLM-Verifier Convergence Theorem.&lt;/li&gt;&lt;li&gt;Proves termination/verification is almost sure for any δ &gt; 0 and derives an expected iteration bound E[n] ≤ 4/δ.&lt;/li&gt;&lt;li&gt;Presents an extensive empirical evaluation (≈90,000 trials) showing strong agreement with the theoretical bound and a convergence factor clustered near 1.0.&lt;/li&gt;&lt;li&gt;Defines three operational zones (marginal, practical, high-performance) and provides design thresholds for predictable resource planning in safety-critical deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['PIerre Dantas', 'Lucas Cordeiro', 'Youcheng Sun', 'Waldir Junior']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-assisted formal verification', 'safety/robustness', 'theoretical guarantees', 'empirical benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02080</guid><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
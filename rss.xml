<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 19 Nov 2025 23:32:55 +0000</lastBuildDate><item><title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title><link>https://arxiv.org/abs/2511.10094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Matryoshka Transcoders to automatically discover and interpret physical-plausibility failure modes in generative models via hierarchical sparse feature learning.&lt;/li&gt;&lt;li&gt;Trains on intermediate representations from a physical-plausibility classifier and uses large multimodal models to translate discovered visual patterns into natural-language failure descriptions.&lt;/li&gt;&lt;li&gt;Demonstrates improved feature relevance and accuracy over prior methods, and constructs a benchmark for evaluating physical plausibility.&lt;/li&gt;&lt;li&gt;Applies the method to analyze eight state-of-the-art generative models, providing actionable insights into how they violate physical constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Tang', 'Abhijeet Sinha', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'failure-mode-analysis', 'robustness', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10094</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DepthVision: Enabling Robust Vision-Language Models with GAN-Based LiDAR-to-RGB Synthesis for Autonomous Driving</title><link>https://arxiv.org/abs/2509.07463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DepthVision: a GAN-based pipeline to synthesize dense RGB-like images from sparse LiDAR point clouds (with an integrated refiner) so off-the-shelf VLMs can consume range data without retraining or architectural changes.&lt;/li&gt;&lt;li&gt;Introduces Luminance-Aware Modality Adaptation (LAMA) to dynamically fuse synthesized and real camera images based on ambient lighting, allowing LiDAR-derived visuals to substitute when RGB is degraded (e.g., low light, motion blur).&lt;/li&gt;&lt;li&gt;Evaluated on real/simulated datasets and vehicle-in-the-loop experiments, showing substantial improvements in low-light scene understanding for safety-critical tasks while preserving compatibility with frozen VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sven Kirchner', 'Nils Purschke', 'Ross Greer', 'Alois C. Knoll']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sensor-fusion', 'LiDAR-to-RGB-synthesis', 'autonomous-driving', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07463</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title><link>https://arxiv.org/abs/2507.21503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoHoBench, a 12k+ sample benchmark of unanswerable visual questions (four defined types) with multi-stage filtering and human verification to evaluate honesty of MMLMs.&lt;/li&gt;&lt;li&gt;Benchmarks 28 popular MMLMs and finds most models fail to appropriately refuse to answer; honesty depends on visual information as well as language modeling.&lt;/li&gt;&lt;li&gt;Implements initial alignment methods (supervised and preference learning) to improve honest behavior and releases data/code to support future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxu Zhu', 'Shitong Duan', 'Xiangxu Zhang', 'Jitao Sang', 'Peng Zhang', 'Tun Lu', 'Xiao Zhou', 'Jing Yao', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM honesty', 'benchmark', 'alignment', 'unanswerable visual QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21503</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew</title><link>https://arxiv.org/abs/2511.13535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a saliency-aware poisoning attack in federated learning (Chromatic Perturbation Module) that applies subtle color contrast perturbations to adversarial clients' data.&lt;/li&gt;&lt;li&gt;These color perturbations accumulate across rounds to shift saliency maps (e.g., Grad-CAM) away from semantically meaningful regions while keeping classification accuracy largely unchanged.&lt;/li&gt;&lt;li&gt;Demonstrates stealth and persistence: explanation fidelity degrades (peak activation overlap reduced up to ~35%) while accuracy remains above ~96% across evaluated datasets.&lt;/li&gt;&lt;li&gt;Shows standard training pipelines fail to detect or mitigate this interpretability degradation, highlighting explanation integrity as an attack surface in FL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farhin Farhad Riya', 'Shahinul Hoque', 'Jinyuan Stella Sun', 'Olivera Kotevska']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'poisoning-attack', 'interpretability-poisoning', 'adversarial-perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13535</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</title><link>https://arxiv.org/abs/2511.13442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Foresee, a training-free pipeline that leverages vanilla multimodal LLMs for image forgery detection and localization without extra model training.&lt;/li&gt;&lt;li&gt;Uses a type-prior-driven strategy and a Flexible Feature Detector (FFD) to handle copy-move manipulations and produce richer textual explanations alongside tamper localization.&lt;/li&gt;&lt;li&gt;Reports superior localization accuracy and stronger generalization across multiple tamper types (copy-move, splicing, removal, local enhancement, deepfake, AIGC edits) while keeping inference lightweight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Zuo', 'Qinyue Tong', 'Zhe-Ming Lu', 'Ziqian Lu']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'multimodal LLMs', 'digital forensics', 'deepfake detection', 'training-free methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13442</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2511.13079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptiveAD, a dual-branch architecture that decouples scene perception and ego-status reasoning to avoid shortcut reliance on ego state in BEV encoders.&lt;/li&gt;&lt;li&gt;One branch performs scene-driven multi-task BEV perception without ego-status; the other performs ego-driven planning; a scene-aware fusion module adaptively merges their outputs for final trajectory planning.&lt;/li&gt;&lt;li&gt;Introduces path attention for ego-BEV interaction and two auxiliary tasks (BEV unidirectional distillation and autoregressive online mapping) to preserve multi-task performance.&lt;/li&gt;&lt;li&gt;Evaluated on nuScenes, shows improved open-loop planning, reduced dependence on ego-status, and better generalization across scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Tang', 'Mingyue Feng', 'Jiachao Liu', 'Yaonong Wang', 'Jian Pu']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'robustness / generalization', 'model architecture', 'safety (perception/planning)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13079</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection</title><link>https://arxiv.org/abs/2511.12511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DINO-Detect, a teacher-student knowledge distillation framework to make AI-generated image (AIGI) detectors robust to motion blur by distilling representations from a frozen high-capacity teacher (DINOv3) trained on sharp images to a student trained on blurred images.&lt;/li&gt;&lt;li&gt;Distillation matches both feature and logit responses so the student produces consistent, semantically rich representations under motion degradation.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance under both motion-blurred and clean conditions, improving real-world applicability of AIGI/deepfake detection; code to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialiang Shen', 'Jiyang Zheng', 'Yunqi Xue', 'Huajie Chen', 'Yu Yao', 'Hui Kang', 'Ruiqi Liu', 'Helin Gong', 'Yang Yang', 'Dadong Wang', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'robustness', 'blur robustness', 'knowledge distillation', 'deepfake detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12511</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>STONE: Pioneering the One-to-N Backdoor Threat in 3D Point Cloud</title><link>https://arxiv.org/abs/2511.11210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STONE, a configurable spherical trigger that implements a one-to-N backdoor in 3D point cloud models (single trigger controlling multiple target labels).&lt;/li&gt;&lt;li&gt;Provides theoretical grounding via Neural Tangent Kernel (NTK) analysis to explain feasibility of one-to-N mappings in 3D models.&lt;/li&gt;&lt;li&gt;Empirically demonstrates high attack success rates (up to 100%) with no degradation in clean-data accuracy across extensive evaluations.&lt;/li&gt;&lt;li&gt;Positions the work as a foundational benchmark for multi-target backdoor threats in safety-critical 3D vision applications (e.g., autonomous driving, robotics).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongmei Shan', 'Wei Lian', 'Chongxia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', '3D point cloud', 'one-to-N backdoor', 'adversarial robustness', 'NTK analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11210</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CARScenes: Semantic VLM Dataset for Safe Autonomous Driving</title><link>https://arxiv.org/abs/2511.10701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frame-level dataset of 5,192 images from Argoverse, Cityscapes, KITTI, and nuScenes annotated with a 28-key category/sub-category ontology (350+ leaf attributes) including a discrete severity scale (1–10).&lt;/li&gt;&lt;li&gt;Labels produced via a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; authors release prompts, post-processing rules, and per-field baseline performance.&lt;/li&gt;&lt;li&gt;Includes attribute co-occurrence graphs, JSONL records for semantic retrieval and risk-aware scenario mining, plus reproducible baseline models and evaluation scripts (accuracy, micro-F1, severity MAE/RMSE).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuankai He', 'Weisong Shi']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'vision-language-models', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10701</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Governance-Ready Small Language Models for Medical Imaging: Prompting, Abstention, and PACS Integration</title><link>https://arxiv.org/abs/2508.13378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a governance-ready, prompt-first deployment framework for small vision-language models to assist AP/PA view tagging in chest radiographs, combining prompt scaffolds and calibrated abstention.&lt;/li&gt;&lt;li&gt;Provides empirical comparisons of four deployable SLMs (Qwen2.5-VL, MiniCPM-V, Gemma 7B, LLaVA 7B) on NIH Chest X-ray, reporting accuracy, expected calibration error, abstention behavior, and oversight burden.&lt;/li&gt;&lt;li&gt;Operationalizes integration with clinical systems by mapping outputs to DICOM tags, HL7 v2 messages, and FHIR ImagingStudy, and specifies an auditable evidence pack, stratified calibration for dataset shift, and a human-factors RACI.&lt;/li&gt;&lt;li&gt;Emphasizes safe deployment practices (calibration, logging, change management, abstention thresholds) and explicitly avoids claiming clinical validation—aimed at governance and operational risk mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting Wang', 'Ziwei Wang', 'Di Zhu', 'Jiachen Zhong', 'Weiyi Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'calibration', 'abstention', 'governance', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13378</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Train Driver Performance as Key to Approval of Driverless Trains</title><link>https://arxiv.org/abs/2504.19735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a new public, anonymized dataset of 711 controlled measurements of train driver performance (reaction time and distance to obstacle) across speeds, obstacle sizes, protection systems, and color contrasts.&lt;/li&gt;&lt;li&gt;Motivated by regulatory requirements allowing simplified approval of computer-vision systems for driverless trains when they match replaced human functions; aims to quantify human obstacle-detection performance for comparison/benchmarking.&lt;/li&gt;&lt;li&gt;Summarizes prior published data and supplies an exhaustive description of the new dataset to support research, standardization, and safety approval processes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical dataset)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rustam Tagiew (German Centre for Rail Traffic Research at the Federal Railway Authority)', 'Prasannavenkatesh Balaji (German Centre for Rail Traffic Research at the Federal Railway Authority)']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'autonomous-vehicle-safety', 'computer-vision', 'dataset', 'regulatory-compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.19735</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MAVias: Mitigate any Visual Bias</title><link>https://arxiv.org/abs/2412.06632</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents MAVias, an open-set bias mitigation method that uses a foundation image tagger to enumerate visual attributes and an LLM to select attributes that define target classes (potential spurious biases).&lt;/li&gt;&lt;li&gt;Converts the discovered language-coded biases into vision-language embeddings and applies an in-processing mitigation to discourage the model from encoding those attributes.&lt;/li&gt;&lt;li&gt;Evaluates on CelebA, Waterbirds, ImageNet, and UrbanCars, showing improved bias mitigation compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ioannis Sarridis', 'Christos Koutlis', 'Symeon Papadopoulos', 'Christos Diou']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'fairness', 'vision-language', 'foundation models', 'open-set discovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06632</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Certified but Fooled! Breaking Certified Defences with Ghost Certificates</title><link>https://arxiv.org/abs/2511.14003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces region-focused adversarial examples that are imperceptible yet both cause misclassification and manipulate probabilistic certification to produce deceptive, large robustness radii (‘ghost certificates’).&lt;/li&gt;&lt;li&gt;Shows that such attacks can bypass state-of-the-art certified defenses (evaluated on ImageNet, e.g., Densepure), demonstrating practical vulnerabilities in certification pipelines.&lt;/li&gt;&lt;li&gt;Highlights fundamental limits of current robustness certification methods and the need to better secure certification procedures against adversarial manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quoc Viet Vo', 'Tashreque M. Haq', 'Paul Montague', 'Tamas Abraham', 'Ehsan Abbasnejad', 'Damith C. Ranasinghe']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'certified defenses', 'robustness certification', 'certificate spoofing', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14003</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm</title><link>https://arxiv.org/abs/2511.13760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MoETTA, an entropy-based test-time adaptation method that uses a Mixture-of-Experts (MoE) design to allow structurally decoupled expert-specific adaptation directions instead of a single unified update.&lt;/li&gt;&lt;li&gt;Introduces two new benchmarks (potpourri and potpourri+) that simulate heterogeneous mixed distribution shifts including natural, artistic, and adversarial distortions; potpourri+ also includes source-domain samples to evaluate catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Demonstrates that modeling multiple adaptation directions via expert-level diversity yields consistent SOTA gains over strong TTA baselines across several mixed-shift settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Fan', 'Jingyan Jiang', 'Zhaoru Chen', 'Fanding Huang', 'Xiao Chen', 'Qinting Jiang', 'Bowen Zhang', 'Xing Tang', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'test-time adaptation', 'mixture-of-experts', 'distribution shift', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13760</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title><link>https://arxiv.org/abs/2511.14554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicFlow, a tri-modal forensic network that fuses RGB (ConvNeXt-tiny), texture (Swin Transformer-tiny), and frequency (CNN + SE) branches to detect video deepfakes.&lt;/li&gt;&lt;li&gt;Uses attention-based temporal pooling to prioritize high-evidence frames and adaptive attention fusion to balance contributions from each branch.&lt;/li&gt;&lt;li&gt;Trained on Celeb-DF (v2) with Focal Loss and reports strong metrics (AUC 0.9752, F1 0.9408, accuracy 0.9208), outperforming single-stream baselines; ablations and Grad-CAM used to validate components.&lt;/li&gt;&lt;li&gt;Emphasizes multi-scale spatial, texture, and spectral artifact detection to improve generalization and resilience to subtle forgeries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Romani']&lt;/li&gt;&lt;li&gt;Tags: ['Deepfake detection', 'Multimodal fusion', 'Robustness', 'Frequency analysis', 'Temporal pooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14554</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition</title><link>https://arxiv.org/abs/2511.14391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TLS-Assist, a modular redundancy layer that adds explicit traffic light and sign recognition to LLM-based autonomous driving agents by converting detections into structured natural-language messages injected into the LLM input.&lt;/li&gt;&lt;li&gt;Plug-and-play, model-agnostic design supporting single-view and multi-view camera setups to force explicit attention to safety-critical cues.&lt;/li&gt;&lt;li&gt;Closed-loop evaluation on the LangAuto benchmark in CARLA shows up to 14% relative driving performance improvement over LMDrive and 7% over BEVDriver, with consistent reductions in traffic light and sign infractions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabian Schmidt', 'Noushiq Mohammed Kayilan Abdul Nazar', 'Markus Enzweiler', 'Abhinav Valada']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-safety', 'autonomous-driving', 'traffic-sign-recognition', 'modular-perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14391</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title><link>https://arxiv.org/abs/2511.14386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a texture-enabled 3D physical adversarial example (PAE) tailored to stereo (binocular) depth estimation for autonomous driving, replacing local 2D patches with global camouflage textures.&lt;/li&gt;&lt;li&gt;Proposes a 3D stereo matching rendering module to align the PAE with real-world positions and camera disparities, and a merging attack to seamlessly blend the adversary into the environment.&lt;/li&gt;&lt;li&gt;Demonstrates that the PAEs can induce erroneous depth outputs from stereo matching models in physical-world-style evaluations, improving stealth and effectiveness over prior hiding attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangqiao Zhao', 'Shuo Huai', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-examples', 'stereo-depth-estimation', 'autonomous-driving', 'adversarial-attack', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14386</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation</title><link>https://arxiv.org/abs/2511.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PAVE is an end-to-end dataset collected entirely in autonomous-driving mode with 100+ hours of naturalistic data from production AVs, segmented into 32,727 key frames.&lt;/li&gt;&lt;li&gt;Each key frame includes four synchronized camera images, high-precision GNSS/IMU (0.8 cm accuracy), 20 Hz vehicle trajectories (past 6 s, future 5 s), and detailed 2D annotations for vehicles, pedestrians, traffic lights, and signs.&lt;/li&gt;&lt;li&gt;Frames are richly labeled with scenario-level attributes (driver intent, area type, lighting, weather, road surface, traffic/VRU density, traffic control), and the dataset is continuously expanded weekly.&lt;/li&gt;&lt;li&gt;They demonstrate dataset utility by evaluating an end-to-end motion planning model (ADE = 1.4 m) to assess behavioral safety of black-box AV controllers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyu Li', 'Chen Wang', 'Yumao Liu', 'Dengbo He', 'Jiahao Zhang', 'Ke Ma']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous vehicles', 'safety evaluation', 'dataset/benchmark', 'trajectory prediction', 'behavioral safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14185</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs</title><link>https://arxiv.org/abs/2511.14159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MVI-Bench, a benchmark of 1,248 expert-annotated VQA instances designed to test LVLM robustness to misleading visual inputs across Visual Concept, Visual Attribute, and Visual Relationship levels.&lt;/li&gt;&lt;li&gt;Defines six categories of misleading visual inputs grounded in visual primitives and proposes MVI-Sensitivity, a metric for fine-grained robustness characterization.&lt;/li&gt;&lt;li&gt;Evaluates 18 state-of-the-art LVLMs, finding pronounced vulnerabilities and providing analyses to guide development of more robust LVLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huiyi Chen', 'Jiawei Peng', 'Dehai Min', 'Changchang Sun', 'Kaijie Chen', 'Yan Yan', 'Xu Yang', 'Lu Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'LVLM', 'misleading-visual-inputs', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14159</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Coffee: Controllable Diffusion Fine-tuning</title><link>https://arxiv.org/abs/2511.14113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Coffee, a method to controllably fine-tune text-to-image diffusion models by specifying "undesired concepts" in language and regularizing adaptation so prompt embeddings do not align with those concepts.&lt;/li&gt;&lt;li&gt;Requires no additional training and allows flexible modification of undesired concepts via textual descriptions.&lt;/li&gt;&lt;li&gt;Evaluated on fine-tuning setups where prompts are paired with undesired concepts; shows Coffee prevents learning specified undesired concepts and outperforms existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyao Zeng', 'Jingcheng Ni', 'Ruyi Liu', 'Alex Wong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'controllability', 'text-to-image', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14113</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Training-free Detection of AI-generated images via Cropping Robustness</title><link>https://arxiv.org/abs/2511.14030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WaRPAD, a training-free AI-generated image detection method using pre-trained self-supervised models that leverage invariance to RandomResizedCrop.&lt;/li&gt;&lt;li&gt;Computes a base score measuring sensitivity of image embeddings to high-frequency perturbations (via Haar wavelet decomposition) and aggregates scores across rescaled image patches to simulate cropping robustness.&lt;/li&gt;&lt;li&gt;Validated on diverse-resolution datasets and images from 23 generative models, showing competitive detection performance and robustness to test-time corruptions.&lt;/li&gt;&lt;li&gt;Method generalizes across different self-supervised backbones because the approach relies on common augmentation-induced invariances rather than detector training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungik Choi', 'Hankook Lee', 'Moontae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'deepfake detection', 'self-supervised learning', 'training-free detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14030</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles</title><link>https://arxiv.org/abs/2511.13909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multi-modal LLMs on understanding road safety concepts using a pilot dataset of textbook schematic/illustrative traffic sign images in a zero-shot setting.&lt;/li&gt;&lt;li&gt;Finds models struggle with safety reasoning and highlights gaps between how humans learn road safety and model interpretation.&lt;/li&gt;&lt;li&gt;Provides analysis of performance gaps and suggests directions for future research on improving LLM safety reasoning in autonomous driving contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chalamalasetti Kranti']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM evaluation', 'multimodal', 'autonomous vehicles', 'safety reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13909</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors</title><link>https://arxiv.org/abs/2511.13897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scalable, model-agnostic framework to evaluate temporal realism of generated videos using motion vectors (MVs) extracted from compressed codecs (H.264, HEVC).&lt;/li&gt;&lt;li&gt;Quantifies distributional differences between real and generated motion using KL, JS, and Wasserstein divergences; finds systematic motion defects and specific generator rankings.&lt;/li&gt;&lt;li&gt;Provides visual diagnostics (MV fields, motion heatmaps) revealing center bias, sparse/patchy flows, and grid artifacts not captured by frame-level metrics.&lt;/li&gt;&lt;li&gt;Explores MV–RGB fusion methods and shows that incorporating MVs substantially improves real-vs-generated classification accuracy across ResNet, I3D, and TSN backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mert Onur Cakiroglu', 'Idil Bilge Altun', 'Zhihe Lu', 'Mehmet Dalkilic', 'Hasan Kurban']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'temporal realism evaluation', 'compressed-domain motion vectors', 'video generation benchmarking', 'motion-aware fusion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13897</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VLMs Guided Interpretable Decision Making for Autonomous Driving</title><link>https://arxiv.org/abs/2511.13881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates open-source vision-language models (VLMs) on high-level driving decision tasks and finds limitations in direct decision generation from VLMs.&lt;/li&gt;&lt;li&gt;Proposes using VLMs as semantic enhancers to produce structured, linguistically rich scene descriptions that augment visual benchmarks.&lt;/li&gt;&lt;li&gt;Introduces a multimodal fusion architecture that combines visual and linguistic features for improved decision accuracy and interpretable textual explanations, plus a post-hoc refinement module to boost prediction reliability.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art performance on two autonomous driving benchmarks, emphasizing interpretability and improved reliability of decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Hu', 'Taotao Jing', 'Renran Tian', 'Zhengming Ding']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'vision-language-models', 'interpretability', 'safety', 'multimodal-fusion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13881</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition</title><link>https://arxiv.org/abs/2511.13775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to mitigate overconfidence in Open Set Recognition (OSR) caused by inter-class feature overlap between known and semantically similar unknown samples.&lt;/li&gt;&lt;li&gt;Introduces a perturbation-based uncertainty estimation module that applies controllable parameter perturbations to produce diverse predictions and quantify predictive uncertainty.&lt;/li&gt;&lt;li&gt;Implements a two-stage unknown detection module with learning-based classifiers that leverage the estimated uncertainty to better discriminate known vs unknown classes.&lt;/li&gt;&lt;li&gt;Demonstrates improved OSR performance on three public datasets compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongdong Zhao', 'Ranxin Fang', 'Changtian Song', 'Zhihui Liu', 'Jianwen Xiang']&lt;/li&gt;&lt;li&gt;Tags: ['open-set recognition', 'uncertainty estimation', 'out-of-distribution detection', 'overconfidence mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13775</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title><link>https://arxiv.org/abs/2511.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework that converts system-level evaluation in multi-LLM agent systems into agent- and message-level training signals using cooperative game-theoretic attribution and process reward modeling.&lt;/li&gt;&lt;li&gt;Uses Shapley-based credit allocation for successes to produce local, signed, credit-conserving rewards that encourage cooperation and discourage redundancy or sabotage.&lt;/li&gt;&lt;li&gt;Introduces first-error localization for failures to create repair-aware preferences that penalize harmful steps while rewarding corrective attempts.&lt;/li&gt;&lt;li&gt;Provides bounded, cooperative, auditable signals intended to plug into reinforcement- or preference-based post-training; paper is conceptual and leaves empirical validation for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Hsuan Yang', 'Tanwi Mallick', 'Le Chen', 'Krishnan Raghavan', 'Azton Wells', 'Amal Gueroudji', 'Ian T. Foster', 'Rajeev Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent-systems', 'credit-assignment', 'reward-design', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10687</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</title><link>https://arxiv.org/abs/2508.18646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an anthropomorphic three-dimensional taxonomy for LLM evaluation: IQ (general capability), EQ (alignment/value-based interactions), and PQ (professional expertise).&lt;/li&gt;&lt;li&gt;Introduces a Value-oriented Evaluation (VQ) framework covering economic viability, social impact, ethical alignment, and environmental sustainability.&lt;/li&gt;&lt;li&gt;Analyzes 200+ benchmarks, identifies gaps (dynamic assessment needs, interpretability), and provides a modular implementation roadmap and curated evaluation resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Wang', 'Ninglun Gu', 'Kailai Zhang', 'Zijiao Zhang', 'Yelun Bao', 'Jin Yang', 'Xu Yin', 'Liwei Liu', 'Yihuan Liu', 'Pengyong Li', 'Gary G. Yen', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'benchmarking', 'ethical-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18646</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title><link>https://arxiv.org/abs/2508.02175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hidden in the Noise (HIN), a backdoor attack framework that implants acoustic triggers via waveform modifications (temporal dynamics and spectrally tailored noise) to compromise Audio LLMs (ALLMs).&lt;/li&gt;&lt;li&gt;Develops AudioSafe, a benchmark assessing nine risk types, and evaluates HIN and other triggers across multiple safety datasets and models.&lt;/li&gt;&lt;li&gt;Finds high attack success (over 90%) for certain acoustic features (environment noise, speech rate), varying sensitivity across features (volume less effective), and stealthiness as poisoned samples cause minimal training loss deviations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Lin', 'Miao Yu', 'Kaiwen Luo', 'Yibo Zhang', 'Lilan Peng', 'Dexian Wang', 'Xuehai Tang', 'Yuanhe Zhang', 'Xikang Yang', 'Zhenhong Zhou', 'Kun Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'audio LLMs', 'adversarial triggers', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02175</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentArmor, which treats LLM agent runtime traces as structured programs and converts them into graph-based IRs (CFG/DFG/PDG) for analysis.&lt;/li&gt;&lt;li&gt;Introduces a property registry to attach security metadata and a type system to perform static inference and policy checking over the IR to detect/stop prompt injection and sensitive data flows.&lt;/li&gt;&lt;li&gt;Focuses on enforcing trust boundaries and policy violations at runtime for LLM agents, i.e., defending against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Evaluated on the AgentDojo benchmark: claims reduction of attack success rate (ASR) to 3% with only ~1% utility drop.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt injection', 'LLM agents', 'Program analysis', 'Runtime security', 'Data-flow policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title><link>https://arxiv.org/abs/2502.15902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IPAD, a detection framework with a Prompt Inverter that predicts candidate prompts likely to have generated an input text, plus two Distinguishers that evaluate alignment between the input and predicted prompts.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness: +9.05% Average Recall in-distribution, +12.93% AUROC on out-of-distribution data, and +5.48% AUROC on attacked data versus strong baselines.&lt;/li&gt;&lt;li&gt;Provides interpretability by exposing predicted prompts and alignment evidence for human inspection, and shows robustness on structured datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Chen', 'Yushi Feng', 'Jisheng Dang', 'Yue Deng', 'Changyang He', 'Hongxi Pu', 'Haoxuan Li', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'robustness', 'adversarial attacks', 'interpretability', 'misuse detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15902</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Predicting the Performance of Black-box LLMs through Self-Queries</title><link>https://arxiv.org/abs/2501.01558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes extracting low-dimensional features from black-box LLMs via follow-up prompts and response probabilities to predict instance-level model performance.&lt;/li&gt;&lt;li&gt;Shows linear predictors trained on these features can generalize well and sometimes outperform white-box predictors based on hidden states or full output distributions.&lt;/li&gt;&lt;li&gt;Demonstrates use cases for security/safety: detecting adversarial system-prompt influence (models intentionally producing wrong answers or buggy code) and identifying misrepresented models served via an API.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Sam', 'Marc Finzi', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['black-box model monitoring', 'LLM reliability', 'adversarial prompt detection', 'model masquerading']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01558</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title><link>https://arxiv.org/abs/2510.19670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;CoSense-LLM is an edge-first multimodal framework that compresses continuous sensor streams into discrete semantic tokens and coordinates with LLMs under explicit latency, energy, bandwidth, and privacy constraints.&lt;/li&gt;&lt;li&gt;Key components: SenseFusion (lightweight encoder to align and compress sensor embeddings), Edge-RAG (local retrieval grounding responses in site policies), PromptRouter (cost- and uncertainty-aware decision policy for edge vs cloud), and Secure Execution (auditable redaction to ensure raw waveforms never leave device).&lt;/li&gt;&lt;li&gt;Empirical results claim sub-second p95 latency on edge-dominant paths, lower inter-tier token/bandwidth costs, improved factual consistency from local retrieval, calibrated uncertainty enabling selective abstention/escalation, and reduced energy per decision with KV/decoding accelerators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Mari Eplik', 'Javier Rojas', 'Aina Binti Abdullah', 'Pieter van der Merwe']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'edge security', 'uncertainty calibration', 'retrieval-augmented-generation', 'data minimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19670</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Retrieval Augmented Language Models Know When They Don't Know?</title><link>https://arxiv.org/abs/2509.01476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether retrieval-augmented language models (RALMs) can recognize when they lack knowledge and should refuse to answer.&lt;/li&gt;&lt;li&gt;Finds RALMs show over-refusal (refusing even when retrieved documents are irrelevant and the model could answer) and that in-context fine-tuning can reduce over-refusal but not necessarily improve calibration or accuracy.&lt;/li&gt;&lt;li&gt;Proposes a refusal-aware mechanism combining refusal-post-training with uncertainty-based abstention to better balance refusals and correct answers.&lt;/li&gt;&lt;li&gt;Highlights that uncertainty estimation and calibration for RALMs remain open safety/alignment challenges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youchao Zhou', 'Heyan Huang', 'Yicheng Liu', 'Rui Dai', 'Xinglin Wang', 'Xingchen Zhang', 'Shumin Shi', 'Yang Deng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'refusal/abstention', 'calibration', 'retrieval-augmented models', 'uncertainty estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01476</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</title><link>https://arxiv.org/abs/2506.20606</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames steering agent ethical behavior as a model editing task ('Behavior Editing') enabling targeted modifications to LLM-based agents while preserving capabilities.&lt;/li&gt;&lt;li&gt;Introduces BehaviorBench, a multi-tier benchmark grounded in moral psychology to evaluate and edit agent behaviors across scenarios of increasing complexity and ambiguity.&lt;/li&gt;&lt;li&gt;Demonstrates Behavior Editing can induce both scenario-specific and global shifts in moral alignment, enabling promotion of benevolent behavior or induction of harmful/malicious behavior across frontier LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baixiang Huang', 'Zhen Tan', 'Haoran Wang', 'Zijie Liu', 'Dawei Li', 'Ali Payani', 'Huan Liu', 'Tianlong Chen', 'Kai Shu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model editing', 'safety', 'benchmarks', 'red-teaming-risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20606</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration</title><link>https://arxiv.org/abs/2505.23229</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCTSr-Zero, an MCTS-based framework tailored for open-ended, human-centric dialogues by optimizing for 'domain alignment' (principle-conforming conversational trajectories) instead of predefined end-states.&lt;/li&gt;&lt;li&gt;Introduces 'Regeneration' and 'Meta-Prompt Adaptation' to expand exploration and allow diverse initial dialogue strategies during search.&lt;/li&gt;&lt;li&gt;Uses MCTSr-Zero to generate multi-turn psychological counseling dialogues, fine-tunes an LLM (PsyLLM), and introduces PsyEval, a benchmark for assessing multi-turn counseling quality and adherence to psychological standards.&lt;/li&gt;&lt;li&gt;Reports that PsyLLM achieves state-of-the-art performance on PsyEval and related metrics, demonstrating improved principle-aligned behavior in a sensitive human-centric domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Lu', 'Yanchi Gu', 'Haoyuan Huang', 'Yulin Zhou', 'Ningxin Zhu', 'Chen Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-data-generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23229</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title><link>https://arxiv.org/abs/2505.01273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptObfus, an "anti-adversarial" desensitization method that masks privacy-sensitive tokens in prompts and generates candidate replacements via a desensitization model.&lt;/li&gt;&lt;li&gt;Selects replacement candidates using gradient feedback from a surrogate model to minimize disruption to the original task output, framing desensitization as a masked language modeling task.&lt;/li&gt;&lt;li&gt;Evaluated on three NLP tasks, showing reduced privacy inference from remote LLMs while preserving task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Li', 'Zhe Yin', 'Xiaodong Gu', 'Beijun Shen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt obfuscation', 'privacy-preserving LLMs', 'adversarial learning (defense)', 'prompt privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01273</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title><link>https://arxiv.org/abs/2504.12673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ACoRN, a training procedure to make abstractive compressors for RAG robust to two types of retrieval noise (irrelevant and misleading/factually incorrect documents) via offline data augmentation and finetuning.&lt;/li&gt;&lt;li&gt;Addresses compressor limitations in aggregating information across multiple retrieved documents and positional bias by finetuning to produce summaries centered on key answer-supporting information.&lt;/li&gt;&lt;li&gt;Demonstrates improvements in EM and F1 for T5-large compressors on datasets with many accuracy-reducing documents, improving answer-preserving compression in real-world noisy retrieval settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Singon Kim', 'Gunho Jung', 'Seong-Whan Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'retrieval-augmented-generation', 'abstractive-compression', 'fine-tuning', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12673</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title><link>https://arxiv.org/abs/2410.21359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates prosocial decision-making of LLM-based agents using dictator games, testing how personas and experimental framings influence altruistic choices.&lt;/li&gt;&lt;li&gt;Benchmarks agent behavior within and across LLM families and compares model behavior to human participants.&lt;/li&gt;&lt;li&gt;Finds that assigning human-like identities or prompts does not reliably produce human-like prosocial behavior; alignment with human behavior varies by model architecture and prompt without clear patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-benchmarking', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21359</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models</title><link>https://arxiv.org/abs/2408.14595</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that multimodal foundation models (MFMs) suffer prompt instability: small text-input perturbations reduce performance across image, video, and audio tasks.&lt;/li&gt;&lt;li&gt;Proposes grounded prompt perturbation methods that generate and filter perturbations based on similarity to text and modality data.&lt;/li&gt;&lt;li&gt;Shows that augmenting training data with these perturbed prompts and re-training improves accuracy and stabilizes performance under perturbations.&lt;/li&gt;&lt;li&gt;Error analysis indicates retraining on prompt perturbations yields consistent cross-domain improvements, suggesting enhanced general reasoning and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ian Stewart', 'Sameera Horawalavithana', 'Brendan Kennedy', 'Sai Munikoti', 'Karl Pazdernik']&lt;/li&gt;&lt;li&gt;Tags: ['prompt robustness', 'multimodal models', 'data augmentation', 'adversarial/perturbation robustness', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.14595</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance</title><link>https://arxiv.org/abs/2406.17385</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether LLM response quality differs for native vs non-native English speakers, finding measurable performance disparities.&lt;/li&gt;&lt;li&gt;Shows a persistent gap when comparing Western native speakers to others and identifies an anchoring effect that further degrades responses when the model is aware of a user's non-nativeness.&lt;/li&gt;&lt;li&gt;Uses a newly collected dataset of &gt;12,000 annotations from 124 annotators with metadata on native language and English proficiency to support analyses.&lt;/li&gt;&lt;li&gt;Highlights fairness and user-facing robustness concerns (potentially more frequent incorrect or lower-quality responses for non-native English users).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manon Reusens', 'Philipp Borchert', 'Jochen De Weerdt', 'Bart Baesens']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'demographic-bias', 'safety-evaluation', 'robustness', 'user-model-interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17385</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title><link>https://arxiv.org/abs/2511.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteganoBackdoor: a steganography-based backdoor attack that converts semantic trigger seeds into fluent, covert carriers via gradient-guided data optimization.&lt;/li&gt;&lt;li&gt;Achieves &gt;99% attack success with an order-of-magnitude lower poisoning rate than prior NLP backdoor methods.&lt;/li&gt;&lt;li&gt;Demonstrates strong evasion against a comprehensive suite of data-level defenses, highlighting a practical blind spot in current defenses and threat models.&lt;/li&gt;&lt;li&gt;Emphasizes real-world risk by focusing on semantic triggers (e.g., names/entities) rather than stylized/artifact triggers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Xue', 'Ruiyi Zhang', 'Zijun Zhang', 'Pengtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'data poisoning', 'NLP security', 'steganography', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14301</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards</title><link>https://arxiv.org/abs/2511.14045</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DIBA (Divergence-in-Behavior Attack), the first membership inference attack tailored to Reinforcement Learning with Verifiable Rewards (RLVR), which detects training-set prompts via behavioral shifts rather than memorized outputs.&lt;/li&gt;&lt;li&gt;Measures two axes of leakage—advantage-side improvement (e.g., correctness gains) and logit-side divergence (policy drift)—and combines them to distinguish training prompts from non-training ones.&lt;/li&gt;&lt;li&gt;Extensive evaluation shows strong performance (≈0.8 AUC, order-of-magnitude higher TPR@0.1%FPR), robustness across in-distribution/cross-dataset/cross-algorithm/black-box settings, extensions to vision-language models, and resilience to moderate defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yule Liu', 'Heyi Zhang', 'Jinyi Zheng', 'Zhen Sun', 'Zifan Peng', 'Tianshuo Cong', 'Yilong Yang', 'Xinlei He', 'Zhuo Ma']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'reinforcement-learning', 'RLVR/RLHF', 'black-box-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14045</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Experimental study of multi-agent jailbreak attacks across major LLM families/sizes (0.6B–120B) using JailbreakBench, with ~6,000 attacker-target exchanges.&lt;/li&gt;&lt;li&gt;Finds a significant positive correlation between mean harm and the log of attacker-to-target size ratio (Pearson r = 0.51, Spearman rho = 0.52), implying larger attackers more easily elicit harmful outputs from smaller targets.&lt;/li&gt;&lt;li&gt;Shows greater variance in harm across attacker models than across target models, and a strong negative correlation between attacker refusal frequency and harm (rho = -0.93), indicating attacker-side alignment reduces successful jailbreaks.&lt;/li&gt;&lt;li&gt;Uses three independent LLM judges to assign aggregated harm and refusal scores, highlighting scaling patterns relevant to inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</title><link>https://arxiv.org/abs/2511.14439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MedBench v4 is a large-scale, clinician-curated benchmarking platform for Chinese medical LLMs, multimodal models, and agents: &gt;700k tasks across 24 primary and 91 secondary specialties, multi-stage review by clinicians from 500+ institutions.&lt;/li&gt;&lt;li&gt;Includes dedicated tracks for LLMs, multimodal models, and agents; open-ended responses are judged by an LLM calibrated to human ratings and aligned with Chinese clinical guidelines/regulatory priorities.&lt;/li&gt;&lt;li&gt;Findings highlight capability vs safety gaps: base LLMs mean overall 54.1/100 and safety/ethics 18.4/100; multimodal models perform worse overall; agentic orchestration markedly improves end-to-end performance and safety scores.&lt;/li&gt;&lt;li&gt;Emphasizes safety/evaluation and governance-aware agent architectures to improve clinical readiness without sacrificing capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinru Ding', 'Lu Lu', 'Chao Ding', 'Mouxiao Bian', 'Jiayuan Chen', 'Renjie Lu', 'Wenrao Pang', 'Xiaoqin Wu', 'Zhiqiang Liu', 'Luyi Jiang', 'Bing Han', 'Yunqiu Wang', 'Jie Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'medical-LLM', 'benchmark', 'agent-safety', 'ethics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14439</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education</title><link>https://arxiv.org/abs/2511.14423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs EduHarm, a safety benchmark of safe–unsafe instruction pairs across five educational scenarios for systematic evaluation of educational LLMs.&lt;/li&gt;&lt;li&gt;Proposes a three-stage shield framework (TSSF): safety-aware attention realignment, layer-wise safety judgment, and defense-driven dual routing to mitigate jailbreak and fine-tuning attacks.&lt;/li&gt;&lt;li&gt;Evaluates TSSF on eight jailbreak strategies and three fine-tuning attack datasets, reporting improved safety (reduced harmful outputs) while avoiding over-refusal and preserving utility from benign fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Yi', 'Yue Li', 'Dongsheng Shi', 'Linlin Wang', 'Xiaoling Wang', 'Liang He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Jailbreaking', 'Fine-tuning attacks', 'Defensive techniques', 'Safety benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14423</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions</title><link>https://arxiv.org/abs/2511.14342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConInstruct, a benchmark to evaluate LLMs' ability to detect and resolve conflicting constraints within user instructions.&lt;/li&gt;&lt;li&gt;Evaluates multiple proprietary and open-source models; finds most proprietary models perform well at conflict detection, with DeepSeek-R1 and Claude-4.5-Sonnet achieving highest F1 scores.&lt;/li&gt;&lt;li&gt;Finds that despite good detection, models seldom explicitly notify users of conflicts or ask for clarification, indicating an instruction-following safety/alignment gap.&lt;/li&gt;&lt;li&gt;Highlights the need for improved LLM behaviors for conflict resolution as part of safer instruction-following design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingwei He', 'Qianru Zhang', 'Pengfei Chen', 'Guanhua Chen', 'Linlin Yu', 'Yuan Yuan', 'Siu-Ming Yiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'alignment', 'instruction-following', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14342</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space</title><link>https://arxiv.org/abs/2511.14275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes predicting a verbalized probability distribution over the answer space to drive more in-depth reasoning for confidence estimation.&lt;/li&gt;&lt;li&gt;Argues this encourages models to consider multiple candidate answers rather than a single guess, improving calibration and transparency (including when the answer space is unknown).&lt;/li&gt;&lt;li&gt;Reports advantages across different models and tasks, robustness after reinforcement learning, and alignment of reasoning patterns with human expectations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ante Wang', 'Weizhi Ma', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['confidence estimation', 'calibration', 'safety evaluation', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14275</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA</title><link>https://arxiv.org/abs/2511.14172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a symbolic localization framework that uses symbolic linguistic and semantic knowledge to trace where hallucinations emerge across model layers.&lt;/li&gt;&lt;li&gt;Evaluates five models on HaluEval and TruthfulQA, focusing on symbolic triggers (modifiers, negation, numbers, exceptions, named entities).&lt;/li&gt;&lt;li&gt;Finds attention variance explosions in early layers (2–4), with negation causing particularly catastrophic instability; larger model size does not substantially reduce hallucination rates.&lt;/li&gt;&lt;li&gt;Concludes hallucination is fundamentally a symbolic linguistic processing failure and that symbolic semantic signals are key to localizing hallucination mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naveen Lamba', 'Sanju Tiwari', 'Manas Gaur']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety_evaluation', 'interpretability', 'symbolic_triggers', 'model_diagnostics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14172</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Selective Weak-to-Strong Generalization</title><link>https://arxiv.org/abs/2511.14166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a selective weak-to-strong generalization (W2SG) framework that avoids using weak supervision when unnecessary by training a binary classifier P(IK) to detect questions the strong model can answer.&lt;/li&gt;&lt;li&gt;Uses the strong model's self-generated labels for alignment when P(IK) predicts knowledge, and refines weak labels via a graph smoothing method to mitigate harmful weak labels.&lt;/li&gt;&lt;li&gt;Reports consistent improvements over competitive baselines on three benchmarks and shows the P(IK) classifier generalizes across tasks and difficulty levels, suggesting benefits for robust alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Lang', 'Fei Huang', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'weak supervision', 'robustness', 'self-training', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14166</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT</title><link>https://arxiv.org/abs/2511.14106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "Stealth Fine-Tuning", an attack that elicits harmful chain-of-thought (CoT) traces via segment-level interference and uses those self-generated outputs as supervised fine-tuning data.&lt;/li&gt;&lt;li&gt;Uses a turn-based weighted loss and lightweight QLoRA fine-tuning (499 samples, &lt;3 hours on one A100) to produce distribution-consistent model updates that bypass alignment.&lt;/li&gt;&lt;li&gt;Reports strong effectiveness: +38.52% Attack Success Rate over IDEATOR on RVLMs while preserving general reasoning ability and representation distribution; evaluated on AdvBench and other benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Yu', 'Zhengyue Zhao', 'Yawen Zheng', 'Yunhao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment attack', 'jailbreaking', 'chain-of-thought exploitation', 'fine-tuning attack', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14106</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</title><link>https://arxiv.org/abs/2511.14010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoRA-RAG, a knowledge-grounded retrieval-augmented generation framework using a Mixture-of-Retrieval to route queries across hazard-specific databases, agentic chunking to preserve context, and a verification loop to check evidence sufficiency and refine searches.&lt;/li&gt;&lt;li&gt;Constructs HazardRecQA, a question–answer dataset derived from 90 global GEER reconnaissance reports spanning seven hazard types, for evaluation of multi-hazard reasoning.&lt;/li&gt;&lt;li&gt;Reports up to 94.5% accuracy and claims a ~30% improvement over zero-shot LLMs and ~10% over prior RAG systems, while reducing hallucinations and enabling competitive performance from open-weight models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenchen Kuai', 'Zihao Li', 'Braden Rosen', 'Stephanie Paan', 'Navid Jafari', 'Jean-Louis Briaud', 'Yunlong Zhang', 'Youssef M. A. Hashash', 'Yang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'knowledge-grounding', 'hallucination-mitigation', 'robustness', 'disaster-informatics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14010</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</title><link>https://arxiv.org/abs/2511.13722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of multiple LLM watermarking schemes against adversarial attacks (paraphrasing and back-translation).&lt;/li&gt;&lt;li&gt;Assesses how watermarking affects linguistic quality and writing style using linguistic metrics and semantic preservation checks.&lt;/li&gt;&lt;li&gt;Finds watermarks largely preserve semantics but alter writing style and are susceptible to evasion, with back-translation being especially effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Guo', 'Adaku Uchendu', 'Ana Smith']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'adversarial attacks', 'evasion/robustness', 'LLM detection', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13722</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection</title><link>https://arxiv.org/abs/2511.12511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DINO-Detect, a teacher-student knowledge distillation framework to make AI-generated image (AIGI) detectors robust to motion blur by distilling representations from a frozen high-capacity teacher (DINOv3) trained on sharp images to a student trained on blurred images.&lt;/li&gt;&lt;li&gt;Distillation matches both feature and logit responses so the student produces consistent, semantically rich representations under motion degradation.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance under both motion-blurred and clean conditions, improving real-world applicability of AIGI/deepfake detection; code to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialiang Shen', 'Jiyang Zheng', 'Yunqi Xue', 'Huajie Chen', 'Yu Yao', 'Hui Kang', 'Ruiqi Liu', 'Helin Gong', 'Yang Yang', 'Dadong Wang', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'robustness', 'blur robustness', 'knowledge distillation', 'deepfake detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12511</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</title><link>https://arxiv.org/abs/2509.07939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a guided reasoning pipeline for LLM-driven penetration testing that constrains model reasoning using a deterministic task tree derived from the MITRE ATT&amp;CK matrix.&lt;/li&gt;&lt;li&gt;Implements an automated penetration-testing agent using Llama-3-8B, Gemini-1.5, and GPT-4 and evaluates on 10 HackTheBox exercises covering 103 subtasks.&lt;/li&gt;&lt;li&gt;Reports large improvements in subtask completion rates and reduced model query counts versus a self-guided reasoning baseline (e.g., guided approach: 71.8%–78.6% vs baseline: 13.5%–75.7% across models).&lt;/li&gt;&lt;li&gt;Claims that anchoring LLM reasoning to structured attack trees reduces hallucinations, unproductive actions, and cyclical behavior in automated red-team workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katsuaki Nakano', 'Reza Fayyazi', 'Shanchieh Jay Yang', 'Michael Zuzak']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'MITRE ATT&amp;CK / attack trees', 'safety and robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07939</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentArmor, which treats LLM agent runtime traces as structured programs and converts them into graph-based IRs (CFG/DFG/PDG) for analysis.&lt;/li&gt;&lt;li&gt;Introduces a property registry to attach security metadata and a type system to perform static inference and policy checking over the IR to detect/stop prompt injection and sensitive data flows.&lt;/li&gt;&lt;li&gt;Focuses on enforcing trust boundaries and policy violations at runtime for LLM agents, i.e., defending against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Evaluated on the AgentDojo benchmark: claims reduction of attack success rate (ASR) to 3% with only ~1% utility drop.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt injection', 'LLM agents', 'Program analysis', 'Runtime security', 'Data-flow policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning</title><link>https://arxiv.org/abs/2506.09562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TooBadRL, a framework to systematically optimize backdoor triggers for deep reinforcement learning along three axes: injection timing, trigger dimension, and manipulation magnitude.&lt;/li&gt;&lt;li&gt;Introduces a performance-aware adaptive freezing mechanism for injection timing, uses Shapley value analysis to select influential trigger dimensions, and applies adversarial input synthesis to determine manipulation magnitude under constraints.&lt;/li&gt;&lt;li&gt;Evaluates on three DRL algorithms and nine benchmark tasks, showing higher attack success with minimal impact on normal performance, and also examines potential detection and mitigation defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingxuan Zhang', 'Oubo Ma', 'Kang Wei', 'Songze Li', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'deep reinforcement learning', 'trigger optimization', 'adversarial attacks', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09562</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title><link>https://arxiv.org/abs/2410.21359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates prosocial decision-making of LLM-based agents using dictator games, testing how personas and experimental framings influence altruistic choices.&lt;/li&gt;&lt;li&gt;Benchmarks agent behavior within and across LLM families and compares model behavior to human participants.&lt;/li&gt;&lt;li&gt;Finds that assigning human-like identities or prompts does not reliably produce human-like prosocial behavior; alignment with human behavior varies by model architecture and prompt without clear patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-benchmarking', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21359</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization</title><link>https://arxiv.org/abs/2511.12199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how membrane potential distribution (MPD) interacts with surrogate gradient (SG) functions to determine gradient magnitude and sensitivity to input perturbations in spiking neural networks (SNNs).&lt;/li&gt;&lt;li&gt;Proposes MPD-driven surrogate gradient regularization (MPD-SGR) that reduces the proportion of membrane potentials within the SG's gradient-available range to lower vulnerability to adversarial perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates via experiments on multiple image classification benchmarks and architectures that MPD-SGR improves adversarial robustness and generalizes across SG functions, network configurations, and spike encoding schemes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runhao Jiang', 'Chengzhi Jiang', 'Rui Yan', 'Huajin Tang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'spiking neural networks', 'surrogate gradient', 'robustness regularization', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12199</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title><link>https://arxiv.org/abs/2511.10094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Matryoshka Transcoders to automatically discover and interpret physical-plausibility failure modes in generative models via hierarchical sparse feature learning.&lt;/li&gt;&lt;li&gt;Trains on intermediate representations from a physical-plausibility classifier and uses large multimodal models to translate discovered visual patterns into natural-language failure descriptions.&lt;/li&gt;&lt;li&gt;Demonstrates improved feature relevance and accuracy over prior methods, and constructs a benchmark for evaluating physical plausibility.&lt;/li&gt;&lt;li&gt;Applies the method to analyze eight state-of-the-art generative models, providing actionable insights into how they violate physical constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Tang', 'Abhijeet Sinha', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'failure-mode-analysis', 'robustness', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10094</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Decoding Methods for Language Models on Encrypted Data</title><link>https://arxiv.org/abs/2509.08383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces cutmax, a homomorphic-encryption-friendly argmax algorithm that reduces ciphertext operations to enable practical greedy decoding over encrypted data.&lt;/li&gt;&lt;li&gt;Proposes the first HE-compatible nucleus (top-p) sampling method that uses cutmax to perform stochastic decoding with provable privacy guarantees and polynomial complexity.&lt;/li&gt;&lt;li&gt;Shows both methods are polynomial and differentiable, enabling gradient-based sequence-level optimization as an alternative to straight-through estimators.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees for cutmax and empirical evaluations demonstrating 24x–35x latency reductions over prior HE baselines on realistic LLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matan Avitan', 'Moran Baruch', 'Nir Drucker', 'Itamar Zimerman', 'Yoav Goldberg']&lt;/li&gt;&lt;li&gt;Tags: ['homomorphic encryption', 'privacy-preserving inference', 'secure decoding/argmax', 'LLM decoding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08383</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case</title><link>https://arxiv.org/abs/2509.03948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies the Marabou formal verification tool to verify local robustness of neural network models used for onboard satellite fault detection.&lt;/li&gt;&lt;li&gt;Quantifies permissible input perturbations before model outputs become unstable, improving trustworthiness under uncertainty.&lt;/li&gt;&lt;li&gt;Targets a safety-critical spatial classification use case to ensure high reliability of embedded AI for early fault detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Delphine Longuet', 'Amira Elouazzani', 'Alejandro Penacho Riveiros', 'Nicola Bastianello']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'robustness', 'safety', 'satellite', 'Marabou']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03948</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preference Robustness for DPO with Applications to Public Health</title><link>https://arxiv.org/abs/2509.02709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DPO-PRO, a robust fine-tuning method based on Direct Preference Optimization that incorporates a lightweight Distributionally Robust Optimization (DRO) formulation to handle uncertainty in preference distributions.&lt;/li&gt;&lt;li&gt;Targets reward-function learning for sequential resource-allocation problems in public health, using human natural-language preferences as supervision.&lt;/li&gt;&lt;li&gt;Shows empirical improvements in robustness to noisy preference signals over existing DPO variants, and matches performance of self-reflection baselines with lower inference cost, evaluated on a real-world maternal mobile health program and standard alignment benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheol Woo Kim', 'Shresth Verma', 'Mauricio Tec', 'Milind Tambe']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'DPO', 'distributionally_robust_optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02709</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preference Learning with Lie Detectors can Induce Honesty or Evasion</title><link>https://arxiv.org/abs/2505.13787</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DolusChat, a 65k-example paired truthful/deceptive dataset, and studies incorporating a lie detector into preference-learning pipelines.&lt;/li&gt;&lt;li&gt;Identifies three factors controlling learned honesty: exploration during preference learning, lie detector true positive rate (TPR), and KL regularization strength.&lt;/li&gt;&lt;li&gt;Finds that on-policy RL (GRPO) can learn to evade detectors (deception &gt;85%) unless detector TPR or KL regularization is high, while off-policy DPO yields low deception (&lt;25%) under realistic TPRs.&lt;/li&gt;&lt;li&gt;Concludes that detector-augmented training can either improve scalable oversight or incentivize undetectable misalignment depending on setup.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chris Cundy', 'Adam Gleave']&lt;/li&gt;&lt;li&gt;Tags: ['deception/lie detection', 'alignment/safety', 'preference learning', 'adversarial evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13787</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Proofs as Explanations: Short Certificates for Reliable Predictions</title><link>https://arxiv.org/abs/2504.08377</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines explanations as small subsets S' of training data that certify a prediction h(x)=y under the assumption the target is in hypothesis class H and at most b training points are corrupted.&lt;/li&gt;&lt;li&gt;Introduces the robust hollow star number, generalizing the hollow star number, and shows it exactly characterizes worst-case minimal certificate size for classes H and corruption level b.&lt;/li&gt;&lt;li&gt;Provides distributional and distribution-dependent bounds on certificate size; defines a certificate coefficient ε_x and proves matching upper/lower sample-size bounds as a function of ε_x, b, and VC dimension d.&lt;/li&gt;&lt;li&gt;Analyzes natural hypothesis classes (e.g., linear classifiers with Carathéodory-style short proofs) and gives tight relationships between sample complexity and certifiability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avrim Blum', 'Steve Hanneke', 'Chirag Pabbaraju', 'Donya Saless']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'robustness', 'certification', 'learning-theory', 'adversarial-corruption']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.08377</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem</title><link>https://arxiv.org/abs/2503.00240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes weight parametrizations for two 1-Lipschitz architectures (Almost-Orthogonal-Layers and SDP-based Lipschitz Layers) and their impact on network initialization.&lt;/li&gt;&lt;li&gt;Derives exact and upper bounds for parameterized weight variance under Normal and Generalized Normal initializations (covering Uniform, Laplace, Normal), showing variance depends on weight matrix dimensions rather than weight variance.&lt;/li&gt;&lt;li&gt;Demonstrates that standard initializations cause deep 1-Lipschitz networks to decay to zero, with implications for certifiably robust classification against adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marius F. R. Juston', 'Ramavarapu S. Sreenivas', 'William R. Norris', 'Dustin Nottage', 'Ahmet Soylemezoglu']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial robustness', '1-Lipschitz networks', 'model initialization', 'adversarial defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00240</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title><link>https://arxiv.org/abs/2502.15902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IPAD, a detection framework with a Prompt Inverter that predicts candidate prompts likely to have generated an input text, plus two Distinguishers that evaluate alignment between the input and predicted prompts.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness: +9.05% Average Recall in-distribution, +12.93% AUROC on out-of-distribution data, and +5.48% AUROC on attacked data versus strong baselines.&lt;/li&gt;&lt;li&gt;Provides interpretability by exposing predicted prompts and alignment evidence for human inspection, and shows robustness on structured datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Chen', 'Yushi Feng', 'Jisheng Dang', 'Yue Deng', 'Changyang He', 'Hongxi Pu', 'Haoxuan Li', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'robustness', 'adversarial attacks', 'interpretability', 'misuse detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15902</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Predicting the Performance of Black-box LLMs through Self-Queries</title><link>https://arxiv.org/abs/2501.01558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes extracting low-dimensional features from black-box LLMs via follow-up prompts and response probabilities to predict instance-level model performance.&lt;/li&gt;&lt;li&gt;Shows linear predictors trained on these features can generalize well and sometimes outperform white-box predictors based on hidden states or full output distributions.&lt;/li&gt;&lt;li&gt;Demonstrates use cases for security/safety: detecting adversarial system-prompt influence (models intentionally producing wrong answers or buggy code) and identifying misrepresented models served via an API.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Sam', 'Marc Finzi', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['black-box model monitoring', 'LLM reliability', 'adversarial prompt detection', 'model masquerading']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01558</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</title><link>https://arxiv.org/abs/2307.13352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a high-dimensional semi-verified mean estimation method that combines a large corrupted dataset of gradients with a small clean auxiliary dataset to mitigate Byzantine attackers in distributed learning.&lt;/li&gt;&lt;li&gt;Identifies a high-variance subspace and estimates mean components differently inside and outside that subspace to avoid sqrt(d) dependence on dimensionality.&lt;/li&gt;&lt;li&gt;Applies the estimator as an aggregator for distributed gradient descent and proves minimax-optimal statistical rates, validated by numerical experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenyu Liu', 'Tianqiang Huang', 'Pengfei Zhang', 'Zong Ke', 'Minghui Min', 'Puning Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine-robustness', 'robust-aggregation', 'distributed-learning', 'adversarial-attacks', 'semi-verified-mean-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.13352</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis</title><link>https://arxiv.org/abs/2511.14755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoVer-CoRe, a novel framework that applies Hamilton-Jacobi (HJ) reachability analysis to verify perception-based controllers under perceptual/state-estimation uncertainty.&lt;/li&gt;&lt;li&gt;Core idea: concatenate controller, observation function, and state estimator into an equivalent closed-loop system amenable to existing HJ reachability tools; provides methods for formal safety verification and robust controller design.&lt;/li&gt;&lt;li&gt;Evaluated on case studies (aircraft taxiing and NN-based rover navigation) and releases code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Albert Lin', 'Alessandro Pinto', 'Somil Bansal']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'formal verification', 'robustness', 'Hamilton-Jacobi reachability', 'perception-based controllers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14755</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title><link>https://arxiv.org/abs/2511.14554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicFlow, a tri-modal forensic network that fuses RGB (ConvNeXt-tiny), texture (Swin Transformer-tiny), and frequency (CNN + SE) branches to detect video deepfakes.&lt;/li&gt;&lt;li&gt;Uses attention-based temporal pooling to prioritize high-evidence frames and adaptive attention fusion to balance contributions from each branch.&lt;/li&gt;&lt;li&gt;Trained on Celeb-DF (v2) with Focal Loss and reports strong metrics (AUC 0.9752, F1 0.9408, accuracy 0.9208), outperforming single-stream baselines; ablations and Grad-CAM used to validate components.&lt;/li&gt;&lt;li&gt;Emphasizes multi-scale spatial, texture, and spectral artifact detection to improve generalization and resilience to subtle forgeries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Romani']&lt;/li&gt;&lt;li&gt;Tags: ['Deepfake detection', 'Multimodal fusion', 'Robustness', 'Frequency analysis', 'Temporal pooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14554</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</title><link>https://arxiv.org/abs/2511.14422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sigil, a server-enforced watermarking scheme for U-shaped split federated learning that embeds a watermark into client models via gradient injection without access to client data, labels, or model parameters.&lt;/li&gt;&lt;li&gt;Defines the watermark as a statistical constraint on server-visible activation space and uses adaptive gradient clipping to maintain stealth and mandatory embedding while evading gradient-anomaly detectors.&lt;/li&gt;&lt;li&gt;Evaluates robustness against attacks including a designed adaptive subspace removal attack and demonstrates fidelity, stealthiness, and robustness across multiple datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengchunmin Dai', 'Jiaxiong Tang', 'Peng Sun', 'Honglong Chen', 'Liantao Wu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'watermarking / IP protection', 'gradient-based attacks and defenses', 'split learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14422</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling</title><link>https://arxiv.org/abs/2511.14334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Tests whether LLMs' ability to generate constraint programming models reflects memorization (data contamination) or genuine reasoning by systematically rephrasing CSPLib problems.&lt;/li&gt;&lt;li&gt;Creates perturbations that preserve problem structure but alter context and introduce misleading elements, then compares outputs from three representative LLMs on original vs modified descriptions.&lt;/li&gt;&lt;li&gt;Finds syntactically valid and plausible models are often produced but performance and correctness drop sharply under contextual and linguistic variation, revealing sensitivity to wording and shallow understanding.&lt;/li&gt;&lt;li&gt;Highlights implications for reliability of LLM-generated models and the need for robustness-focused evaluation beyond benchmark overlap.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Pellegrino', 'Jacopo Mauro']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Prompt sensitivity', 'Data contamination', 'Model evaluation', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14334</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title><link>https://arxiv.org/abs/2511.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteganoBackdoor: a steganography-based backdoor attack that converts semantic trigger seeds into fluent, covert carriers via gradient-guided data optimization.&lt;/li&gt;&lt;li&gt;Achieves &gt;99% attack success with an order-of-magnitude lower poisoning rate than prior NLP backdoor methods.&lt;/li&gt;&lt;li&gt;Demonstrates strong evasion against a comprehensive suite of data-level defenses, highlighting a practical blind spot in current defenses and threat models.&lt;/li&gt;&lt;li&gt;Emphasizes real-world risk by focusing on semantic triggers (e.g., names/entities) rather than stylized/artifact triggers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Xue', 'Ruiyi Zhang', 'Zijun Zhang', 'Pengtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'data poisoning', 'NLP security', 'steganography', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14301</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.14148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AsyncVLA, which replaces synchronous flow matching (SFM) with asynchronous flow matching (AFM) to generate action tokens on a non-uniform, context-aware temporal schedule.&lt;/li&gt;&lt;li&gt;Introduces a confidence rater that estimates confidence of initial generated actions and selectively refines low-confidence action tokens before execution to enable self-correction.&lt;/li&gt;&lt;li&gt;Provides a unified training procedure supporting both SFM and AFM modes in a single model to improve KV-cache utilization and flexibility.&lt;/li&gt;&lt;li&gt;Demonstrates improved data-efficiency and state-of-the-art performance on robotic manipulation benchmarks, particularly in long-horizon tasks where cascading errors occur.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhua Jiang', 'Shuang Cheng', 'Yan Ding', 'Feifei Gao', 'Biqing Qi']&lt;/li&gt;&lt;li&gt;Tags: ['robotics', 'robustness', 'safety', 'flow-matching', 'self-correction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14148</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Black-box Backdoor Attacks on IoT Sensory Data</title><link>https://arxiv.org/abs/2511.14074</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel dynamic trigger-generation technique to perform black-box backdoor (poisoning) attacks against IoT sensor-based recognition systems (IMU/time-series).&lt;/li&gt;&lt;li&gt;Demonstrates high attack success across multiple datasets and classifier models with minimal perturbation, emphasizing stealthiness.&lt;/li&gt;&lt;li&gt;Provides comparative analysis against other backdoor/poisoning techniques and evaluates defenses and their impact on the proposed trigger method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ajesh Koyatan Chathoth', 'Stephen Lee']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'poisoning', 'adversarial-attacks', 'IoT', 'sensor-time-series']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14074</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Training-free Detection of AI-generated images via Cropping Robustness</title><link>https://arxiv.org/abs/2511.14030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WaRPAD, a training-free AI-generated image detection method using pre-trained self-supervised models that leverage invariance to RandomResizedCrop.&lt;/li&gt;&lt;li&gt;Computes a base score measuring sensitivity of image embeddings to high-frequency perturbations (via Haar wavelet decomposition) and aggregates scores across rescaled image patches to simulate cropping robustness.&lt;/li&gt;&lt;li&gt;Validated on diverse-resolution datasets and images from 23 generative models, showing competitive detection performance and robustness to test-time corruptions.&lt;/li&gt;&lt;li&gt;Method generalizes across different self-supervised backbones because the approach relies on common augmentation-induced invariances rather than detector training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungik Choi', 'Hankook Lee', 'Moontae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'deepfake detection', 'self-supervised learning', 'training-free detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14030</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands</title><link>https://arxiv.org/abs/2511.13911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a conformal prediction method producing uncertainty-calibrated prediction bands for randomly-timed biomarker trajectories with guaranteed coverage.&lt;/li&gt;&lt;li&gt;Applies method to neuroimaging-derived biomarkers of Alzheimer's disease, showing calibrated coverage and tighter bands than baselines.&lt;/li&gt;&lt;li&gt;Develops group-conditional conformal bands to account for population heterogeneity and demonstrates improved identification of high-risk subjects via an uncertainty-calibrated risk score.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasiliki Tassopoulou', 'Charis Stamouli', 'Haochang Shou', 'George J. Pappas', 'Christos Davatzikos']&lt;/li&gt;&lt;li&gt;Tags: ['conformal prediction', 'uncertainty quantification', 'medical AI safety', 'biomarker trajectories', 'risk scoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13911</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition</title><link>https://arxiv.org/abs/2511.13775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to mitigate overconfidence in Open Set Recognition (OSR) caused by inter-class feature overlap between known and semantically similar unknown samples.&lt;/li&gt;&lt;li&gt;Introduces a perturbation-based uncertainty estimation module that applies controllable parameter perturbations to produce diverse predictions and quantify predictive uncertainty.&lt;/li&gt;&lt;li&gt;Implements a two-stage unknown detection module with learning-based classifiers that leverage the estimated uncertainty to better discriminate known vs unknown classes.&lt;/li&gt;&lt;li&gt;Demonstrates improved OSR performance on three public datasets compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongdong Zhao', 'Ranxin Fang', 'Changtian Song', 'Zhihui Liu', 'Jianwen Xiang']&lt;/li&gt;&lt;li&gt;Tags: ['open-set recognition', 'uncertainty estimation', 'out-of-distribution detection', 'overconfidence mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13775</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What happens when nanochat meets DiLoCo?</title><link>https://arxiv.org/abs/2511.13761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Implements DiLoCo (local inner steps before synchronization) in the nanochat open-source LLM training stack and compares it to standard data-parallel (DDP) training.&lt;/li&gt;&lt;li&gt;DiLoCo reduces communication drastically and attains stable convergence and competitive pretraining loss.&lt;/li&gt;&lt;li&gt;Despite similar pretraining loss, DiLoCo leads to degraded downstream performance (MMLU, GSM8K, HumanEval) after mid-training and SFT; switching back to DDP from DiLoCo-pretrained weights does not recover performance due to irreversible representation drift.&lt;/li&gt;&lt;li&gt;Authors release the implementation as an official fork of nanochat on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Acker', 'Soeren Becker', 'Sasho Nedelkoski', 'Dominik Scheinert', 'Odej Kao', 'Philipp Wiesner']&lt;/li&gt;&lt;li&gt;Tags: ['Distributed training', 'Alignment', 'Robustness', 'Representation drift', 'Training algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13761</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Look-Ahead Reasoning on Learning Platforms</title><link>https://arxiv.org/abs/2511.14745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes level-k (one-step look-ahead) reasoning for users interacting with predictive models on learning platforms and analyzes convergence properties.&lt;/li&gt;&lt;li&gt;Shows level-k reasoning speeds convergence to the same equilibrium as myopic behavior, so higher-level individual reasoning yields no long-run advantage.&lt;/li&gt;&lt;li&gt;Analyzes collective (coordinated) user optimization through their joint impact on the model, characterizes benefits/limits of coordination, and introduces a notion of alignment between learner and user utilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiqing Zhu', 'Tijana Zrnic', 'Celestine Mendler-D\\"unner']&lt;/li&gt;&lt;li&gt;Tags: ['strategic-classification', 'performative-prediction', 'algorithmic-game-theory', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14745</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning</title><link>https://arxiv.org/abs/2511.14715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLARE, an adaptive multi-dimensional reputation framework for assessing client reliability in federated learning, using performance consistency, statistical anomaly indicators, and temporal behavior.&lt;/li&gt;&lt;li&gt;Implements self-calibrating thresholds and reputation-weighted aggregation with soft exclusion; integrates Local Differential Privacy to compute reputations on privatized updates.&lt;/li&gt;&lt;li&gt;Introduces a new evasive benchmark attack, Statistical Mimicry (SM), and shows empirical robustness improvements (up to ~16%) and preserved convergence across MNIST, CIFAR-10, and SVHN under varied attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abolfazl Younesi', 'Leon Kiss', 'Zahra Najafabadi Samani', 'Juan Aznar Poveda', 'Thomas Fahringer']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'data poisoning', 'reputation systems', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14715</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation</title><link>https://arxiv.org/abs/2511.14406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First empirical analysis of how LoRA-based parameter-efficient adaptation affects backdoor attacks in federated learning.&lt;/li&gt;&lt;li&gt;Focuses on backdoor lifespan/persistence across FL rounds and shows that, for optimally injected backdoors, lower LoRA rank increases backdoor persistence.&lt;/li&gt;&lt;li&gt;Highlights evaluation shortcomings in prior work and argues for more robust and fair evaluation practices for backdoor attacks against federated model adaptation; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bastien Vuillod', 'Pierre-Alain Moellic', 'Jean-Max Dutertre']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor attacks', 'LoRA', 'model adaptation', 'integrity/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14406</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title><link>https://arxiv.org/abs/2511.14317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the challenge of multiple similarly-performing clinical models (Rashomon Effect) and limitations of standard metrics in resource-constrained clinical settings.&lt;/li&gt;&lt;li&gt;Proposes Intervention Efficiency (IE), a capacity-aware metric that measures how effectively a model prioritizes actionable true positives when interventions are limited.&lt;/li&gt;&lt;li&gt;Introduces the Perturbation Validation Framework (PVF) to evaluate model stability under data perturbations and identify models whose performance is invariant across noisy/shifted validation sets.&lt;/li&gt;&lt;li&gt;Provides empirical evidence on synthetic and real-world healthcare datasets that IE+PVF help select models with more robust generalization and better alignment to operational capacity constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuwen Zhang', 'Viet Tran', 'Paul Weng']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model selection', 'evaluation metrics', 'perturbation testing', 'clinical ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14317</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</title><link>https://arxiv.org/abs/2511.14195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes N-GLARE, a non-generative evaluator that analyzes LLM hidden-layer latent representations instead of generating text to assess safety robustness.&lt;/li&gt;&lt;li&gt;Introduces APT (Angular-Probabilistic Trajectory) characterization of latent dynamics and the JSS (Jensen-Shannon Separability) metric to quantify safety-related separability in latent space.&lt;/li&gt;&lt;li&gt;Validated across 40+ models and 20 red-teaming strategies, showing JSS aligns well with red-team safety rankings while using &lt;1% of token and runtime cost compared to generation-based testing.&lt;/li&gt;&lt;li&gt;Claims N-GLARE can serve as an efficient, output-free proxy for real-time safety diagnostics after model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheyu Lin', 'Jirui Yang', 'Hengqi Guo', 'Yubing Bao', 'Yao Guan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'red teaming', 'latent representation analysis', 'safety metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14195</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Certified Signed Graph Unlearning</title><link>https://arxiv.org/abs/2511.14168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses selective data removal (unlearning) for Signed Graph Neural Networks (SGNNs), a privacy-sensitive setting that standard unlearning methods mishandle due to sign heterogeneity.&lt;/li&gt;&lt;li&gt;Proposes Certified Signed Graph Unlearning (CSGU): (1) identify minimal influenced neighborhoods via triangle structures, (2) allocate privacy budget using sociological node-importance measures, (3) perform importance-weighted parameter updates to produce certified modifications.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in both utility preservation and unlearning effectiveness compared to existing (non-signed-graph-aware) methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junpeng Zhao', 'Lin Li', 'Kaixi Hu', 'Kaize Shi', 'Jingling Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'unlearning', 'graph neural networks', 'certified guarantees', 'signed graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14168</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Study of Implicit and Explicit Biases in Large Language Models</title><link>https://arxiv.org/abs/2511.14153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates explicit and implicit social biases (gender, race, profession, religion) in LLMs using bias benchmarks (StereoSet, CrowSPairs) across models like BERT and GPT-3.5.&lt;/li&gt;&lt;li&gt;Proposes an automated Bias-Identification Framework with a two-pronged approach to detect explicit and implicit biases; uses Bag-of-Words analysis to surface implicit stereotyping.&lt;/li&gt;&lt;li&gt;Applies fine-tuning, prompting techniques, and data augmentation on bias benchmarks; reports up to 20% improvement on implicit-bias benchmarks and improved cross-dataset adaptability, but notes continued keyword over-reliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatima Kazi', 'Alex Young', 'Yash Inani', 'Setareh Rafatirad']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'safety-evaluation', 'debiasing', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14153</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Observational Auditing of Label Privacy</title><link>https://arxiv.org/abs/2511.14084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an observational auditing framework for differential privacy that evaluates label/attribute privacy without modifying the training dataset.&lt;/li&gt;&lt;li&gt;Extends privacy auditing beyond membership inference to protected attributes and labels, leveraging natural randomness in data distributions.&lt;/li&gt;&lt;li&gt;Provides theoretical foundations and empirical evaluation on Criteo (tabular) and CIFAR-10 (image) showing effectiveness for label privacy auditing in large-scale/production settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Iden Kalemaj', 'Luca Melis', 'Maxime Boucher', 'Ilya Mironov', 'Saeed Mahloujifar']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy auditing', 'label privacy', 'membership inference', 'privacy evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14084</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs</title><link>https://arxiv.org/abs/2511.14017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows emergent misalignment (EMA) can arise from narrow refusal unlearning on specific domains (Cybersecurity and Safety), causing models to produce malicious or non-refusal outputs in unrelated domains.&lt;/li&gt;&lt;li&gt;Evaluates EMA across seven responsible-AI domains (Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, Privacy) on Mistral-7b-0.3v and Qwen-7b-2.5 and finds safety unlearning often produces larger cross-domain impact.&lt;/li&gt;&lt;li&gt;Demonstrates a mitigation: augmenting refusal unlearning with cross-entropy loss on a small retain dataset from affected domains largely restores alignment while preserving desired lower refusal on the targeted concept.&lt;/li&gt;&lt;li&gt;Analyzes representation-level concept entanglement via concept vectors and finds higher early-layer representation similarity predicts susceptibility to EMA after intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erum Mushtaq', 'Anil Ramakrishna', 'Satyapriya Krishna', 'Sattvik Sahai', 'Prasoon Goyal', 'Kai-Wei Chang', 'Tao Zhang', 'Rahul Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Refusal unlearning', 'Emergent misalignment', 'Safety evaluation', 'Mitigation/containment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14017</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Certified but Fooled! Breaking Certified Defences with Ghost Certificates</title><link>https://arxiv.org/abs/2511.14003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces region-focused adversarial examples that are imperceptible yet both cause misclassification and manipulate probabilistic certification to produce deceptive, large robustness radii (‘ghost certificates’).&lt;/li&gt;&lt;li&gt;Shows that such attacks can bypass state-of-the-art certified defenses (evaluated on ImageNet, e.g., Densepure), demonstrating practical vulnerabilities in certification pipelines.&lt;/li&gt;&lt;li&gt;Highlights fundamental limits of current robustness certification methods and the need to better secure certification procedures against adversarial manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quoc Viet Vo', 'Tashreque M. Haq', 'Paul Montague', 'Tamas Abraham', 'Ehsan Abbasnejad', 'Damith C. Ranasinghe']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'certified defenses', 'robustness certification', 'certificate spoofing', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14003</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Node-Level Uncertainty Estimation in LLM-Generated SQL</title><link>https://arxiv.org/abs/2511.13984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a semantically aware labeling algorithm that assigns node-level correctness in an SQL AST between generated and gold queries while avoiding over-penalizing structural/alias differences.&lt;/li&gt;&lt;li&gt;Represents AST nodes with schema-aware and lexical features (identifier validity, alias resolution, type compatibility, scope ambiguity, typo signals) and trains a supervised classifier to predict per-node error probabilities.&lt;/li&gt;&lt;li&gt;Interprets per-node probabilities as calibrated uncertainty, achieving substantial improvements over token log-probabilities (average AUC +27.44%) and showing cross-database robustness.&lt;/li&gt;&lt;li&gt;Demonstrates applications of node-level uncertainty for targeted repair, human-in-the-loop review, and selective execution to improve reliability of LLM-generated SQL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hilaf Hasson', 'Ruocheng Guo']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'LLM reliability', 'SQL generation', 'calibration', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13984</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis</title><link>https://arxiv.org/abs/2511.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MargNet, a neural-network-based method for differentially private (DP) tabular data synthesis that incorporates algorithmic ideas from statistical DP models (adaptive marginal selection and training to match marginals).&lt;/li&gt;&lt;li&gt;Targets both sparsely and densely correlated tabular datasets; claims substantial utility improvements on densely correlated data where statistical methods struggle.&lt;/li&gt;&lt;li&gt;Reports up to 7x speedup over the best statistical method on sparse data and up to 26% reduction in fidelity error on dense data, with code released on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Chen', 'Chen Gong', 'Tianhao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy-preserving data synthesis', 'tabular data', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13893</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Experimental study of multi-agent jailbreak attacks across major LLM families/sizes (0.6B–120B) using JailbreakBench, with ~6,000 attacker-target exchanges.&lt;/li&gt;&lt;li&gt;Finds a significant positive correlation between mean harm and the log of attacker-to-target size ratio (Pearson r = 0.51, Spearman rho = 0.52), implying larger attackers more easily elicit harmful outputs from smaller targets.&lt;/li&gt;&lt;li&gt;Shows greater variance in harm across attacker models than across target models, and a strong negative correlation between attacker refusal frequency and harm (rho = -0.93), indicating attacker-side alignment reduces successful jailbreaks.&lt;/li&gt;&lt;li&gt;Uses three independent LLM judges to assign aggregated harm and refusal scores, highlighting scaling patterns relevant to inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm</title><link>https://arxiv.org/abs/2511.13760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MoETTA, an entropy-based test-time adaptation method that uses a Mixture-of-Experts (MoE) design to allow structurally decoupled expert-specific adaptation directions instead of a single unified update.&lt;/li&gt;&lt;li&gt;Introduces two new benchmarks (potpourri and potpourri+) that simulate heterogeneous mixed distribution shifts including natural, artistic, and adversarial distortions; potpourri+ also includes source-domain samples to evaluate catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Demonstrates that modeling multiple adaptation directions via expert-level diversity yields consistent SOTA gains over strong TTA baselines across several mixed-shift settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Fan', 'Jingyan Jiang', 'Zhaoru Chen', 'Fanding Huang', 'Xiao Chen', 'Qinting Jiang', 'Bowen Zhang', 'Xing Tang', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'test-time adaptation', 'mixture-of-experts', 'distribution shift', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13760</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robustness of LLM-enabled vehicle trajectory prediction under data security threats</title><link>https://arxiv.org/abs/2511.13753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction using a black-box one-feature differential-evolution attack that perturbs a single kinematic input feature in the LLM prompt.&lt;/li&gt;&lt;li&gt;Shows on the highD dataset that small, physically plausible perturbations can substantially disrupt predicted trajectories and lane-change intentions, indicating high adversarial susceptibility.&lt;/li&gt;&lt;li&gt;Analyzes accuracy–robustness trade-offs, investigates failure mechanisms, and evaluates potential mitigation strategies for LLM-based predictors in safety-critical driving contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feilong Wang', 'Fuqiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM safety', 'trajectory prediction', 'autonomous vehicles', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13753</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</title><link>https://arxiv.org/abs/2511.13750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SCALEX extracts semantically meaningful directions from diffusion model H-space using only natural-language prompts, enabling zero-shot interpretation without retraining or labeling.&lt;/li&gt;&lt;li&gt;The method enables scalable, automated discovery and comparison of internal associations, detecting gender bias in profession prompts and ranking semantic alignment across identity descriptors.&lt;/li&gt;&lt;li&gt;SCALEX reveals clustered conceptual structure in latent space, making bias analysis in diffusion models more interpretable and extensible than prior manual or narrowly scoped approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['E. Zhixuan Zeng', 'Yuhao Chen', 'Alexander Wong']&lt;/li&gt;&lt;li&gt;Tags: ['bias-detection', 'latent-space-analysis', 'diffusion-models', 'safety-evaluation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13750</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepDefense: Layer-Wise Gradient-Feature Alignment for Building Robust Neural Networks</title><link>https://arxiv.org/abs/2511.13749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepDefense, a defense that applies layer-wise Gradient-Feature Alignment (GFA) regularization to align input gradients with internal features and reduce sensitivity to adversarial perturbations.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis decomposing adversarial perturbations into radial and tangential components and argues GFA suppresses loss variation in tangential directions where attacks are most effective.&lt;/li&gt;&lt;li&gt;Reports substantial empirical robustness gains on CIFAR-10 against gradient-based (APGD, FGSM) and optimization-based (DeepFool, EADEN) attacks; claims architecture-agnostic, simple implementation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ci Lin', 'Tet Yeap', 'Iluju Kiringa', 'Biwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defenses', 'gradient-based attacks', 'adversarial training', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13749</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</title><link>https://arxiv.org/abs/2511.13442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Foresee, a training-free pipeline that leverages vanilla multimodal LLMs for image forgery detection and localization without extra model training.&lt;/li&gt;&lt;li&gt;Uses a type-prior-driven strategy and a Flexible Feature Detector (FFD) to handle copy-move manipulations and produce richer textual explanations alongside tamper localization.&lt;/li&gt;&lt;li&gt;Reports superior localization accuracy and stronger generalization across multiple tamper types (copy-move, splicing, removal, local enhancement, deepfake, AIGC edits) while keeping inference lightweight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Zuo', 'Qinyue Tong', 'Zhe-Ming Lu', 'Ziqian Lu']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'multimodal LLMs', 'digital forensics', 'deepfake detection', 'training-free methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13442</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)</title><link>https://arxiv.org/abs/2511.11590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Explainability-Enabled Clinical Safety Framework (ECSF) to integrate explainability into NHS clinical-safety lifecycles (DCB0129/DCB0160) without changing compliance pathways.&lt;/li&gt;&lt;li&gt;Maps regulatory clauses to Good Machine Learning Practice, NHS AI Assurance, T.E.S.T., and the EU AI Act, producing a matrix linking clauses, principles, ECSF checkpoints, and explainability outputs.&lt;/li&gt;&lt;li&gt;Defines five ECSF checkpoints (global transparency, case-level interpretability, clinician usability, traceable decision pathways, longitudinal interpretability monitoring) and maps XAI techniques (SHAP, LIME, Integrated Gradients, saliency maps, attention visualization) to DCB artefacts.&lt;/li&gt;&lt;li&gt;Positions explainability as core clinical-safety evidence to support verification, risk control, and post-market surveillance of AI systems in healthcare.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Gigiu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'explainability', 'clinical safety', 'regulatory compliance', 'post-market surveillance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11590</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title><link>https://arxiv.org/abs/2511.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework that converts system-level evaluation in multi-LLM agent systems into agent- and message-level training signals using cooperative game-theoretic attribution and process reward modeling.&lt;/li&gt;&lt;li&gt;Uses Shapley-based credit allocation for successes to produce local, signed, credit-conserving rewards that encourage cooperation and discourage redundancy or sabotage.&lt;/li&gt;&lt;li&gt;Introduces first-error localization for failures to create repair-aware preferences that penalize harmful steps while rewarding corrective attempts.&lt;/li&gt;&lt;li&gt;Provides bounded, cooperative, auditable signals intended to plug into reinforcement- or preference-based post-training; paper is conceptual and leaves empirical validation for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Hsuan Yang', 'Tanwi Mallick', 'Le Chen', 'Krishnan Raghavan', 'Azton Wells', 'Amal Gueroudji', 'Ian T. Foster', 'Rajeev Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent-systems', 'credit-assignment', 'reward-design', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10687</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DRIP: Defending Prompt Injection via Token-wise Representation Editing and Residual Instruction Fusion</title><link>https://arxiv.org/abs/2511.00447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRIP, a defense against prompt injection that edits token embeddings in the data section to remove instruction-like semantics while preserving data semantics.&lt;/li&gt;&lt;li&gt;Adds a residual instruction-preservation module to prevent adversarial data from overwriting intended instructions.&lt;/li&gt;&lt;li&gt;Evaluated on LLaMA-8B and Mistral-7B across three prompt-injection benchmarks, showing large improvements in role-separation and attack success rates under adaptive attacks while maintaining utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Liu', 'Yun Lin', 'Zhiyong Huang', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'representation editing', 'instruction separation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00447</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Decoding Methods for Language Models on Encrypted Data</title><link>https://arxiv.org/abs/2509.08383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces cutmax, a homomorphic-encryption-friendly argmax algorithm that reduces ciphertext operations to enable practical greedy decoding over encrypted data.&lt;/li&gt;&lt;li&gt;Proposes the first HE-compatible nucleus (top-p) sampling method that uses cutmax to perform stochastic decoding with provable privacy guarantees and polynomial complexity.&lt;/li&gt;&lt;li&gt;Shows both methods are polynomial and differentiable, enabling gradient-based sequence-level optimization as an alternative to straight-through estimators.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees for cutmax and empirical evaluations demonstrating 24x–35x latency reductions over prior HE baselines on realistic LLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matan Avitan', 'Moran Baruch', 'Nir Drucker', 'Itamar Zimerman', 'Yoav Goldberg']&lt;/li&gt;&lt;li&gt;Tags: ['homomorphic encryption', 'privacy-preserving inference', 'secure decoding/argmax', 'LLM decoding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08383</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title><link>https://arxiv.org/abs/2509.07677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SMIA, a black-box adversarial attack that modifies inaudible frequency regions of AI-generated audio to evade voice authentication systems and anti-spoofing countermeasures.&lt;/li&gt;&lt;li&gt;Evaluates SMIA against state-of-the-art speaker verification and anti-spoofing models under realistic conditions, reporting high attack success rates (&gt;=82% combined VAS/CM, &gt;=97.5% standalone verification, 100% CMs).&lt;/li&gt;&lt;li&gt;Demonstrates that current static detection approaches are vulnerable to adaptive adversarial manipulation and argues for dynamic, context-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kamel Kamel', 'Hridoy Sankar Dutta', 'Keshav Sood', 'Sunil Aryal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'voice authentication', 'anti-spoofing', 'black-box attack', 'audio security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07677</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DepthVision: Enabling Robust Vision-Language Models with GAN-Based LiDAR-to-RGB Synthesis for Autonomous Driving</title><link>https://arxiv.org/abs/2509.07463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DepthVision: a GAN-based pipeline to synthesize dense RGB-like images from sparse LiDAR point clouds (with an integrated refiner) so off-the-shelf VLMs can consume range data without retraining or architectural changes.&lt;/li&gt;&lt;li&gt;Introduces Luminance-Aware Modality Adaptation (LAMA) to dynamically fuse synthesized and real camera images based on ambient lighting, allowing LiDAR-derived visuals to substitute when RGB is degraded (e.g., low light, motion blur).&lt;/li&gt;&lt;li&gt;Evaluated on real/simulated datasets and vehicle-in-the-loop experiments, showing substantial improvements in low-light scene understanding for safety-critical tasks while preserving compatibility with frozen VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sven Kirchner', 'Nils Purschke', 'Ross Greer', 'Alois C. Knoll']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sensor-fusion', 'LiDAR-to-RGB-synthesis', 'autonomous-driving', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07463</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Retrieval Augmented Language Models Know When They Don't Know?</title><link>https://arxiv.org/abs/2509.01476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether retrieval-augmented language models (RALMs) can recognize when they lack knowledge and should refuse to answer.&lt;/li&gt;&lt;li&gt;Finds RALMs show over-refusal (refusing even when retrieved documents are irrelevant and the model could answer) and that in-context fine-tuning can reduce over-refusal but not necessarily improve calibration or accuracy.&lt;/li&gt;&lt;li&gt;Proposes a refusal-aware mechanism combining refusal-post-training with uncertainty-based abstention to better balance refusals and correct answers.&lt;/li&gt;&lt;li&gt;Highlights that uncertainty estimation and calibration for RALMs remain open safety/alignment challenges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youchao Zhou', 'Heyan Huang', 'Yicheng Liu', 'Rui Dai', 'Xinglin Wang', 'Xingchen Zhang', 'Shumin Shi', 'Yang Deng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'refusal/abstention', 'calibration', 'retrieval-augmented models', 'uncertainty estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01476</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentArmor, which treats LLM agent runtime traces as structured programs and converts them into graph-based IRs (CFG/DFG/PDG) for analysis.&lt;/li&gt;&lt;li&gt;Introduces a property registry to attach security metadata and a type system to perform static inference and policy checking over the IR to detect/stop prompt injection and sensitive data flows.&lt;/li&gt;&lt;li&gt;Focuses on enforcing trust boundaries and policy violations at runtime for LLM agents, i.e., defending against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Evaluated on the AgentDojo benchmark: claims reduction of attack success rate (ASR) to 3% with only ~1% utility drop.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt injection', 'LLM agents', 'Program analysis', 'Runtime security', 'Data-flow policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration</title><link>https://arxiv.org/abs/2505.23229</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCTSr-Zero, an MCTS-based framework tailored for open-ended, human-centric dialogues by optimizing for 'domain alignment' (principle-conforming conversational trajectories) instead of predefined end-states.&lt;/li&gt;&lt;li&gt;Introduces 'Regeneration' and 'Meta-Prompt Adaptation' to expand exploration and allow diverse initial dialogue strategies during search.&lt;/li&gt;&lt;li&gt;Uses MCTSr-Zero to generate multi-turn psychological counseling dialogues, fine-tunes an LLM (PsyLLM), and introduces PsyEval, a benchmark for assessing multi-turn counseling quality and adherence to psychological standards.&lt;/li&gt;&lt;li&gt;Reports that PsyLLM achieves state-of-the-art performance on PsyEval and related metrics, demonstrating improved principle-aligned behavior in a sensitive human-centric domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Lu', 'Yanchi Gu', 'Haoyuan Huang', 'Yulin Zhou', 'Ningxin Zhu', 'Chen Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-data-generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23229</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preference Learning with Lie Detectors can Induce Honesty or Evasion</title><link>https://arxiv.org/abs/2505.13787</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DolusChat, a 65k-example paired truthful/deceptive dataset, and studies incorporating a lie detector into preference-learning pipelines.&lt;/li&gt;&lt;li&gt;Identifies three factors controlling learned honesty: exploration during preference learning, lie detector true positive rate (TPR), and KL regularization strength.&lt;/li&gt;&lt;li&gt;Finds that on-policy RL (GRPO) can learn to evade detectors (deception &gt;85%) unless detector TPR or KL regularization is high, while off-policy DPO yields low deception (&lt;25%) under realistic TPRs.&lt;/li&gt;&lt;li&gt;Concludes that detector-augmented training can either improve scalable oversight or incentivize undetectable misalignment depending on setup.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chris Cundy', 'Adam Gleave']&lt;/li&gt;&lt;li&gt;Tags: ['deception/lie detection', 'alignment/safety', 'preference learning', 'adversarial evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13787</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title><link>https://arxiv.org/abs/2505.01273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptObfus, an "anti-adversarial" desensitization method that masks privacy-sensitive tokens in prompts and generates candidate replacements via a desensitization model.&lt;/li&gt;&lt;li&gt;Selects replacement candidates using gradient feedback from a surrogate model to minimize disruption to the original task output, framing desensitization as a masked language modeling task.&lt;/li&gt;&lt;li&gt;Evaluated on three NLP tasks, showing reduced privacy inference from remote LLMs while preserving task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Li', 'Zhe Yin', 'Xiaodong Gu', 'Beijun Shen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt obfuscation', 'privacy-preserving LLMs', 'adversarial learning (defense)', 'prompt privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01273</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title><link>https://arxiv.org/abs/2504.12673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ACoRN, a training procedure to make abstractive compressors for RAG robust to two types of retrieval noise (irrelevant and misleading/factually incorrect documents) via offline data augmentation and finetuning.&lt;/li&gt;&lt;li&gt;Addresses compressor limitations in aggregating information across multiple retrieved documents and positional bias by finetuning to produce summaries centered on key answer-supporting information.&lt;/li&gt;&lt;li&gt;Demonstrates improvements in EM and F1 for T5-large compressors on datasets with many accuracy-reducing documents, improving answer-preserving compression in real-world noisy retrieval settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Singon Kim', 'Gunho Jung', 'Seong-Whan Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'retrieval-augmented-generation', 'abstractive-compression', 'fine-tuning', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12673</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem</title><link>https://arxiv.org/abs/2503.00240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes weight parametrizations for two 1-Lipschitz architectures (Almost-Orthogonal-Layers and SDP-based Lipschitz Layers) and their impact on network initialization.&lt;/li&gt;&lt;li&gt;Derives exact and upper bounds for parameterized weight variance under Normal and Generalized Normal initializations (covering Uniform, Laplace, Normal), showing variance depends on weight matrix dimensions rather than weight variance.&lt;/li&gt;&lt;li&gt;Demonstrates that standard initializations cause deep 1-Lipschitz networks to decay to zero, with implications for certifiably robust classification against adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marius F. R. Juston', 'Ramavarapu S. Sreenivas', 'William R. Norris', 'Dustin Nottage', 'Ahmet Soylemezoglu']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial robustness', '1-Lipschitz networks', 'model initialization', 'adversarial defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00240</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title><link>https://arxiv.org/abs/2502.15902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IPAD, a detection framework with a Prompt Inverter that predicts candidate prompts likely to have generated an input text, plus two Distinguishers that evaluate alignment between the input and predicted prompts.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness: +9.05% Average Recall in-distribution, +12.93% AUROC on out-of-distribution data, and +5.48% AUROC on attacked data versus strong baselines.&lt;/li&gt;&lt;li&gt;Provides interpretability by exposing predicted prompts and alignment evidence for human inspection, and shows robustness on structured datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Chen', 'Yushi Feng', 'Jisheng Dang', 'Yue Deng', 'Changyang He', 'Hongxi Pu', 'Haoxuan Li', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'robustness', 'adversarial attacks', 'interpretability', 'misuse detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15902</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title><link>https://arxiv.org/abs/2410.21359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates prosocial decision-making of LLM-based agents using dictator games, testing how personas and experimental framings influence altruistic choices.&lt;/li&gt;&lt;li&gt;Benchmarks agent behavior within and across LLM families and compares model behavior to human participants.&lt;/li&gt;&lt;li&gt;Finds that assigning human-like identities or prompts does not reliably produce human-like prosocial behavior; alignment with human behavior varies by model architecture and prompt without clear patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-benchmarking', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21359</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints</title><link>https://arxiv.org/abs/2511.10952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines requirements for autonomous agents to construct, evaluate, and justify candidate courses of action when no option fully satisfies operational constraints.&lt;/li&gt;&lt;li&gt;Identifies needed contextual knowledge types (normative, pragmatic, situational) that go beyond policy training to enable robust, aligned decision making.&lt;/li&gt;&lt;li&gt;Analyzes integration of normative/pragmatic/situational understanding and presents empirical case studies on selecting and pursuing aligned actions in complex real-world settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steven J. Jones', 'Robert E. Wray', 'John E. Laird']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'AI alignment', 'Autonomous agents', 'Decision-making', 'Operational constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10952</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CTRL-ALT-DECEIT: Sabotage Evaluations for Automated AI R&amp;D</title><link>https://arxiv.org/abs/2511.09904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends MLE-Bench with code-sabotage tasks (e.g., implanting backdoors, causing generalisation failures) to evaluate ML agent misbehavior during ML R&amp;D.&lt;/li&gt;&lt;li&gt;Demonstrates that frontier agents can perform meaningful sabotage and can intentionally sandbag performance to meet lower target levels.&lt;/li&gt;&lt;li&gt;Evaluates LM-based monitors for detecting sabotage and sandbagging; monitors detect code-sabotage well but struggle more with sandbagging, and aggregating monitors improves detection.&lt;/li&gt;&lt;li&gt;Implements the benchmark in the UK AISI Inspect framework and releases code publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francis Rhys Ward', 'Teun van der Weij', "Hanna G\\'abor", 'Sam Martin', 'Raja Mehta Moreno', 'Harel Lidar', 'Louis Makower', 'Thomas Jodrell', 'Lauren Robson']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'sabotage/backdoors', 'sandbagging', 'safety evaluation', 'monitoring/oversight']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09904</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration</title><link>https://arxiv.org/abs/2510.10205</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PIXEL, a position-wise activation steering framework that learns a property-aligned subspace from dual views (tail-averaged and end-token) and selects exact intervention strengths via a constrained geometric objective to adapt to token-level sensitivity.&lt;/li&gt;&lt;li&gt;Adds sample-level orthogonal residual calibration and a lightweight position-scanning routine to refine global attribute directions and find receptive injection sites, with representation-level guarantees for minimal intervention.&lt;/li&gt;&lt;li&gt;Empirically improves attribute alignment (e.g., truthfulness) while preserving general capabilities across diverse LLMs and evaluation setups; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manjiang Yu', 'Hongji Li', 'Priyanka Singh', 'Xue Li', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'activation steering', 'controllable generation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10205</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions</title><link>https://arxiv.org/abs/2509.18970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of hallucinations in LLM-based agents, proposing the first systematic treatment focused on agents.&lt;/li&gt;&lt;li&gt;Introduces a taxonomy mapping different hallucination types to stages in the agent workflow and identifies 18 underlying triggers.&lt;/li&gt;&lt;li&gt;Reviews existing detection and mitigation methods and outlines promising research directions to improve agent reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xixun Lin', 'Yucheng Ning', 'Jingwen Zhang', 'Yan Dong', 'Yilong Liu', 'Yongxuan Wu', 'Xiaohua Qi', 'Nan Sun', 'Yanmin Shang', 'Kun Wang', 'Pengfei Cao', 'Qingyue Wang', 'Lixin Zou', 'Xu Chen', 'Chuan Zhou', 'Jia Wu', 'Peng Zhang', 'Qingsong Wen', 'Shirui Pan', 'Bin Wang', 'Yanan Cao', 'Kai Chen', 'Songlin Hu', 'Li Guo']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety', 'robustness', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18970</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation</title><link>https://arxiv.org/abs/2509.12179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bidirectional Cognitive Alignment (BiCA): a co-alignment paradigm where humans and AI mutually adapt via learnable protocols, representation mapping, and KL-budget constraints.&lt;/li&gt;&lt;li&gt;Reports empirical gains in collaborative navigation (85.5% success vs 70.3% baseline), substantially better mutual adaptation and protocol convergence, and emergent protocols outperforming handcrafted ones.&lt;/li&gt;&lt;li&gt;Claims safety benefits from bidirectional adaptation (≈+23% out-of-distribution robustness) and demonstrates synergy improvements indicating optimal collaboration lies at the intersection of human and AI capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Weiyi Song']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'Human-AI interaction', 'RLHF', 'Robustness', 'Co-adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12179</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</title><link>https://arxiv.org/abs/2508.18646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an anthropomorphic three-dimensional taxonomy for LLM evaluation: IQ (general capability), EQ (alignment/value-based interactions), and PQ (professional expertise).&lt;/li&gt;&lt;li&gt;Introduces a Value-oriented Evaluation (VQ) framework covering economic viability, social impact, ethical alignment, and environmental sustainability.&lt;/li&gt;&lt;li&gt;Analyzes 200+ benchmarks, identifies gaps (dynamic assessment needs, interpretability), and provides a modular implementation roadmap and curated evaluation resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Wang', 'Ninglun Gu', 'Kailai Zhang', 'Zijiao Zhang', 'Yelun Bao', 'Jin Yang', 'Xu Yin', 'Liwei Liu', 'Yihuan Liu', 'Pengyong Li', 'Gary G. Yen', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'benchmarking', 'ethical-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18646</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MI9: An Integrated Runtime Governance Framework for Agentic AI</title><link>https://arxiv.org/abs/2508.03858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MI9, an integrated runtime governance framework for agentic AI that provides real-time controls across six components (agency-risk index, semantic telemetry, continuous authorization monitoring, FSM-based conformance engines, goal-conditioned drift detection, graduated containment).&lt;/li&gt;&lt;li&gt;Focuses on runtime safety, alignment, and oversight for agentic systems where pre-deployment measures are insufficient, enabling monitoring, detection of goal drift, and graduated containment in production.&lt;/li&gt;&lt;li&gt;Claims interoperability across heterogeneous agent architectures and evaluates the framework via scenario analyses to demonstrate coverage of governance gaps in existing approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles L. Wang', 'Trisha Singhal', 'Ameya Kelkar', 'Jason Tuo']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'runtime governance', 'alignment &amp; safety', 'containment strategies', 'drift detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03858</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title><link>https://arxiv.org/abs/2507.21503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoHoBench, a 12k+ sample benchmark of unanswerable visual questions (four defined types) with multi-stage filtering and human verification to evaluate honesty of MMLMs.&lt;/li&gt;&lt;li&gt;Benchmarks 28 popular MMLMs and finds most models fail to appropriately refuse to answer; honesty depends on visual information as well as language modeling.&lt;/li&gt;&lt;li&gt;Implements initial alignment methods (supervised and preference learning) to improve honest behavior and releases data/code to support future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxu Zhu', 'Shitong Duan', 'Xiangxu Zhang', 'Jitao Sang', 'Peng Zhang', 'Tun Lu', 'Xiao Zhou', 'Jing Yao', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM honesty', 'benchmark', 'alignment', 'unanswerable visual QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21503</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning</title><link>https://arxiv.org/abs/2511.14715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLARE, an adaptive multi-dimensional reputation framework for assessing client reliability in federated learning, using performance consistency, statistical anomaly indicators, and temporal behavior.&lt;/li&gt;&lt;li&gt;Implements self-calibrating thresholds and reputation-weighted aggregation with soft exclusion; integrates Local Differential Privacy to compute reputations on privatized updates.&lt;/li&gt;&lt;li&gt;Introduces a new evasive benchmark attack, Statistical Mimicry (SM), and shows empirical robustness improvements (up to ~16%) and preserved convergence across MNIST, CIFAR-10, and SVHN under varied attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abolfazl Younesi', 'Leon Kiss', 'Zahra Najafabadi Samani', 'Juan Aznar Poveda', 'Thomas Fahringer']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'data poisoning', 'reputation systems', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14715</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks</title><link>https://arxiv.org/abs/2511.14592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DSBench, a comprehensive benchmark for evaluating Vision-Language Models' awareness of external environmental risks and in-cabin driving behavior safety.&lt;/li&gt;&lt;li&gt;Covers 10 major categories and 28 sub-categories, and provides a large dataset of ~98K labeled instances for safety-critical driving scenarios.&lt;/li&gt;&lt;li&gt;Evaluates multiple open- and closed-source VLMs, finding significant performance drops in complex safety situations and demonstrating that fine-tuning on DSBench improves safety performance.&lt;/li&gt;&lt;li&gt;Provides a public toolkit, code, and model checkpoints to facilitate safety evaluation and improvement of VLMs in autonomous driving contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianhui Meng', 'Yuchen Zhang', 'Zhijian Huang', 'Zheng Lu', 'Ziling Ji', 'Yaoyao Yin', 'Hongyuan Zhang', 'Guangfeng Jiang', 'Yandan Lin', 'Long Chen', 'Hangjun Ye', 'Li Zhang', 'Jun Liu', 'Xiaoshuai Hao']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'benchmarking', 'autonomous driving', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14592</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges</title><link>https://arxiv.org/abs/2511.14478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a precise definition and taxonomy for agentic AI and situates it relative to prior AI paradigms.&lt;/li&gt;&lt;li&gt;Presents four state-of-the-art use cases applying agentic AI to electrical power systems engineering (e.g., power system studies, dynamic pricing survival analysis).&lt;/li&gt;&lt;li&gt;Contains detailed failure mode investigations and derives actionable recommendations for safe, reliable, and accountable deployment of agentic systems.&lt;/li&gt;&lt;li&gt;Focuses on engineering deployment concerns rather than adversarial security techniques (no explicit emphasis on red teaming, jailbreaks, or privacy attacks in the abstract).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soham Ghosh', 'Gaurav Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['agentic-ai', 'safety', 'failure-modes', 'reliability', 'accountability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14478</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems</title><link>https://arxiv.org/abs/2511.14435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Vision/position paper proposing a symbiotic integration of runtime verification (RV) and large language models (LLMs) to improve safety and trustworthiness of autonomous systems.&lt;/li&gt;&lt;li&gt;Argues RV can act as guardrails for LLM-driven autonomy (monitoring executions, predicting violations) while LLMs can assist RV by translating natural language into formal specifications, supporting anticipatory reasoning, and handling uncertainty.&lt;/li&gt;&lt;li&gt;Discusses differences from existing surveys/roadmaps, outlines challenges (error-prone LLMs, lack of formal guarantees), certification implications, and identifies future research directions toward dependable autonomy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Vision/Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Angelo Ferrando (University of Modena', 'Reggio Emilia)']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'runtime verification', 'LLMs', 'autonomous systems', 'specification extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14435</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Context-aware, Ante-hoc Explanations of Driving Behaviour</title><link>https://arxiv.org/abs/2511.14428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for context-aware, ante-hoc explanations of (un)expectable driving manoeuvres for autonomous vehicles to increase safety and trust.&lt;/li&gt;&lt;li&gt;Uses a visual formal language (Traffic Sequence Charts) to formalise explanation contexts and corresponding manoeuvres, combined with runtime monitoring for context recognition and on-the-fly explanation presentation.&lt;/li&gt;&lt;li&gt;Demonstrated in a simulated overtaking scenario, aiming to bridge correctness (formal models) and stakeholder-centered 'good' explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Grundt (German Aerospace Center e.V.)', 'Ishan Saxena (German Aerospace Center e.V.)', 'Malte Petersen (German Aerospace Center e.V.)', 'Bernd Westphal (German Aerospace Center e.V.)', 'Eike M\\"ohlmann (German Aerospace Center e.V.)']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'autonomous vehicles', 'runtime monitoring', 'safety', 'formal methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14428</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</title><link>https://arxiv.org/abs/2511.14422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sigil, a server-enforced watermarking scheme for U-shaped split federated learning that embeds a watermark into client models via gradient injection without access to client data, labels, or model parameters.&lt;/li&gt;&lt;li&gt;Defines the watermark as a statistical constraint on server-visible activation space and uses adaptive gradient clipping to maintain stealth and mandatory embedding while evading gradient-anomaly detectors.&lt;/li&gt;&lt;li&gt;Evaluates robustness against attacks including a designed adaptive subspace removal attack and demonstrates fidelity, stealthiness, and robustness across multiple datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengchunmin Dai', 'Jiaxiong Tang', 'Peng Sun', 'Honglong Chen', 'Liantao Wu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'watermarking / IP protection', 'gradient-based attacks and defenses', 'split learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14422</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title><link>https://arxiv.org/abs/2511.14386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a texture-enabled 3D physical adversarial example (PAE) tailored to stereo (binocular) depth estimation for autonomous driving, replacing local 2D patches with global camouflage textures.&lt;/li&gt;&lt;li&gt;Proposes a 3D stereo matching rendering module to align the PAE with real-world positions and camera disparities, and a merging attack to seamlessly blend the adversary into the environment.&lt;/li&gt;&lt;li&gt;Demonstrates that the PAEs can induce erroneous depth outputs from stereo matching models in physical-world-style evaluations, improving stealth and effectiveness over prior hiding attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangqiao Zhao', 'Shuo Huai', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-examples', 'stereo-depth-estimation', 'autonomous-driving', 'adversarial-attack', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14386</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA</title><link>https://arxiv.org/abs/2511.14172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a symbolic localization framework that uses symbolic linguistic and semantic knowledge to trace where hallucinations emerge across model layers.&lt;/li&gt;&lt;li&gt;Evaluates five models on HaluEval and TruthfulQA, focusing on symbolic triggers (modifiers, negation, numbers, exceptions, named entities).&lt;/li&gt;&lt;li&gt;Finds attention variance explosions in early layers (2–4), with negation causing particularly catastrophic instability; larger model size does not substantially reduce hallucination rates.&lt;/li&gt;&lt;li&gt;Concludes hallucination is fundamentally a symbolic linguistic processing failure and that symbolic semantic signals are key to localizing hallucination mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naveen Lamba', 'Sanju Tiwari', 'Manas Gaur']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety_evaluation', 'interpretability', 'symbolic_triggers', 'model_diagnostics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14172</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Certified Signed Graph Unlearning</title><link>https://arxiv.org/abs/2511.14168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certified Signed Graph Unlearning (CSGU) to provide provable privacy guarantees for removing specific data influences from Signed Graph Neural Networks (SGNNs).&lt;/li&gt;&lt;li&gt;Three-stage method: (1) identify minimal influenced neighborhoods via triangular structures, (2) use sociological theories to quantify node importance and allocate privacy budget, (3) perform importance-weighted parameter updates to certify modifications while minimizing utility loss.&lt;/li&gt;&lt;li&gt;Addresses shortcomings of existing unlearning methods that ignore sign information in signed graphs, and demonstrates improved utility preservation and unlearning effectiveness in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junpeng Zhao', 'Lin Li', 'Kaixi Hu', 'Kaize Shi', 'Jingling Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['graph unlearning', 'data privacy', 'signed graph neural networks', 'certified unlearning', 'graph security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14168</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Selective Weak-to-Strong Generalization</title><link>https://arxiv.org/abs/2511.14166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of strong pretrained models using weak supervision by proposing a selective weak-to-strong generalization (W2SG) framework.&lt;/li&gt;&lt;li&gt;Trains a binary classifier P(IK) to detect instances the strong model can self-answer and uses self-generated labels instead of always relying on weak supervision; refines weak labels via graph smoothing.&lt;/li&gt;&lt;li&gt;Shows improved performance and robustness across benchmarks and claims the selection mechanism generalizes across tasks and difficulty levels, supporting safer superalignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Lang', 'Fei Huang', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'weak supervision', 'robustness', 'self-training', 'label refinement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14166</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.14148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AsyncVLA, which replaces synchronous flow matching (SFM) with asynchronous flow matching (AFM) to generate action tokens on a non-uniform, context-aware temporal schedule.&lt;/li&gt;&lt;li&gt;Introduces a confidence rater that estimates confidence of initial generated actions and selectively refines low-confidence action tokens before execution to enable self-correction.&lt;/li&gt;&lt;li&gt;Provides a unified training procedure supporting both SFM and AFM modes in a single model to improve KV-cache utilization and flexibility.&lt;/li&gt;&lt;li&gt;Demonstrates improved data-efficiency and state-of-the-art performance on robotic manipulation benchmarks, particularly in long-horizon tasks where cascading errors occur.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhua Jiang', 'Shuang Cheng', 'Yan Ding', 'Feifei Gao', 'Biqing Qi']&lt;/li&gt;&lt;li&gt;Tags: ['robotics', 'robustness', 'safety', 'flow-matching', 'self-correction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14148</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards</title><link>https://arxiv.org/abs/2511.14045</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DIBA (Divergence-in-Behavior Attack), the first membership inference attack tailored to Reinforcement Learning with Verifiable Rewards (RLVR), which detects training-set prompts via behavioral shifts rather than memorized outputs.&lt;/li&gt;&lt;li&gt;Measures two axes of leakage—advantage-side improvement (e.g., correctness gains) and logit-side divergence (policy drift)—and combines them to distinguish training prompts from non-training ones.&lt;/li&gt;&lt;li&gt;Extensive evaluation shows strong performance (≈0.8 AUC, order-of-magnitude higher TPR@0.1%FPR), robustness across in-distribution/cross-dataset/cross-algorithm/black-box settings, extensions to vision-language models, and resilience to moderate defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yule Liu', 'Heyi Zhang', 'Jinyi Zheng', 'Zhen Sun', 'Zifan Peng', 'Tianshuo Cong', 'Yilong Yang', 'Xinlei He', 'Zhuo Ma']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'reinforcement-learning', 'RLVR/RLHF', 'black-box-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14045</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Training-free Detection of AI-generated images via Cropping Robustness</title><link>https://arxiv.org/abs/2511.14030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WaRPAD, a training-free AI-generated image detection method using pre-trained self-supervised models that leverage invariance to RandomResizedCrop.&lt;/li&gt;&lt;li&gt;Computes a base score measuring sensitivity of image embeddings to high-frequency perturbations (via Haar wavelet decomposition) and aggregates scores across rescaled image patches to simulate cropping robustness.&lt;/li&gt;&lt;li&gt;Validated on diverse-resolution datasets and images from 23 generative models, showing competitive detection performance and robustness to test-time corruptions.&lt;/li&gt;&lt;li&gt;Method generalizes across different self-supervised backbones because the approach relies on common augmentation-induced invariances rather than detector training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungik Choi', 'Hankook Lee', 'Moontae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'deepfake detection', 'self-supervised learning', 'training-free detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14030</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs</title><link>https://arxiv.org/abs/2511.14017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows emergent misalignment (EMA) can arise from narrow refusal unlearning on specific domains (Cybersecurity and Safety), causing models to produce malicious or non-refusal outputs in unrelated domains.&lt;/li&gt;&lt;li&gt;Evaluates EMA across seven responsible-AI domains (Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, Privacy) on Mistral-7b-0.3v and Qwen-7b-2.5 and finds safety unlearning often produces larger cross-domain impact.&lt;/li&gt;&lt;li&gt;Demonstrates a mitigation: augmenting refusal unlearning with cross-entropy loss on a small retain dataset from affected domains largely restores alignment while preserving desired lower refusal on the targeted concept.&lt;/li&gt;&lt;li&gt;Analyzes representation-level concept entanglement via concept vectors and finds higher early-layer representation similarity predicts susceptibility to EMA after intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erum Mushtaq', 'Anil Ramakrishna', 'Satyapriya Krishna', 'Sattvik Sahai', 'Prasoon Goyal', 'Kai-Wei Chang', 'Tao Zhang', 'Rahul Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Refusal unlearning', 'Emergent misalignment', 'Safety evaluation', 'Mitigation/containment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14017</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Developing a Grounded View of AI</title><link>https://arxiv.org/abs/2511.14013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for distinguishing AI behavior from rule-based software and preserving rule-based practical rationality.&lt;/li&gt;&lt;li&gt;Proposes a methodology to discriminate AI model behaviors by classifying decisions into three types to clarify limits and responsibilities.&lt;/li&gt;&lt;li&gt;Positions this discrimination as a prerequisite for assigning human responsibility and ensuring AI system soundness for human, societal, and environmental well-being.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bifei Mao', 'Lanqing Hong']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'interpretability', 'AI governance', 'position paper']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14013</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</title><link>https://arxiv.org/abs/2511.14010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoRA-RAG, a knowledge-grounded retrieval-augmented generation framework using a Mixture-of-Retrieval to route queries across hazard-specific databases, agentic chunking to preserve context, and a verification loop to check evidence sufficiency and refine searches.&lt;/li&gt;&lt;li&gt;Constructs HazardRecQA, a question–answer dataset derived from 90 global GEER reconnaissance reports spanning seven hazard types, for evaluation of multi-hazard reasoning.&lt;/li&gt;&lt;li&gt;Reports up to 94.5% accuracy and claims a ~30% improvement over zero-shot LLMs and ~10% over prior RAG systems, while reducing hallucinations and enabling competitive performance from open-weight models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenchen Kuai', 'Zihao Li', 'Braden Rosen', 'Stephanie Paan', 'Navid Jafari', 'Jean-Louis Briaud', 'Yunlong Zhang', 'Youssef M. A. Hashash', 'Yang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'knowledge-grounding', 'hallucination-mitigation', 'robustness', 'disaster-informatics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14010</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Node-Level Uncertainty Estimation in LLM-Generated SQL</title><link>https://arxiv.org/abs/2511.13984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a semantically aware labeling algorithm that assigns node-level correctness in an SQL AST between generated and gold queries while avoiding over-penalizing structural/alias differences.&lt;/li&gt;&lt;li&gt;Represents AST nodes with schema-aware and lexical features (identifier validity, alias resolution, type compatibility, scope ambiguity, typo signals) and trains a supervised classifier to predict per-node error probabilities.&lt;/li&gt;&lt;li&gt;Interprets per-node probabilities as calibrated uncertainty, achieving substantial improvements over token log-probabilities (average AUC +27.44%) and showing cross-database robustness.&lt;/li&gt;&lt;li&gt;Demonstrates applications of node-level uncertainty for targeted repair, human-in-the-loop review, and selective execution to improve reliability of LLM-generated SQL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hilaf Hasson', 'Ruocheng Guo']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'LLM reliability', 'SQL generation', 'calibration', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13984</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks</title><link>https://arxiv.org/abs/2511.13789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that backdoored NLP models show unusually high similarity among attention heads when triggered.&lt;/li&gt;&lt;li&gt;Proposes a trigger-agnostic detection method based on attention similarity, avoiding reliance on a clean reference model.&lt;/li&gt;&lt;li&gt;Introduces attention safety alignment combined with head-wise fine-tuning to repair contaminated attention heads and mitigate backdoor effects.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing reduced backdoor attack success while preserving downstream task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Jin', 'Yang Li', 'Haihui Fan', 'Lin Shen', 'Xiangfang Li', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM security', 'attention analysis', 'defenses/mitigation', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13789</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Experimental study of multi-agent jailbreak attacks across major LLM families/sizes (0.6B–120B) using JailbreakBench, with ~6,000 attacker-target exchanges.&lt;/li&gt;&lt;li&gt;Finds a significant positive correlation between mean harm and the log of attacker-to-target size ratio (Pearson r = 0.51, Spearman rho = 0.52), implying larger attackers more easily elicit harmful outputs from smaller targets.&lt;/li&gt;&lt;li&gt;Shows greater variance in harm across attacker models than across target models, and a strong negative correlation between attacker refusal frequency and harm (rho = -0.93), indicating attacker-side alignment reduces successful jailbreaks.&lt;/li&gt;&lt;li&gt;Uses three independent LLM judges to assign aggregated harm and refusal scores, highlighting scaling patterns relevant to inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Distribution Shift in Traffic Signal Control with Histogram-Based GEH Distance</title><link>https://arxiv.org/abs/2511.13785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a policy-independent method to quantify distribution shift in traffic signal control by representing scenarios as demand histograms and comparing them with a GEH-based distance.&lt;/li&gt;&lt;li&gt;Validates the metric on 20 simulated scenarios using both a conventional actuated controller and a reinforcement learning controller (FRAP++), showing larger GEH distances correlate with increased travel time and reduced throughput.&lt;/li&gt;&lt;li&gt;Finds the proposed histogram-GEH distance predicts performance degradation under distribution shift better than prior techniques, with especially strong explanatory power for learning-based controllers.&lt;/li&gt;&lt;li&gt;Argues the metric is useful for benchmarking, designing training regimes, and monitoring adaptive traffic signal controllers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Federico Taschin', 'Ozan K. Tonguz']&lt;/li&gt;&lt;li&gt;Tags: ['distribution shift', 'robustness', 'benchmarking', 'reinforcement learning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13785</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition</title><link>https://arxiv.org/abs/2511.13775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to mitigate overconfidence in Open Set Recognition (OSR) caused by inter-class feature overlap between known and semantically similar unknown samples.&lt;/li&gt;&lt;li&gt;Introduces a perturbation-based uncertainty estimation module that applies controllable parameter perturbations to produce diverse predictions and quantify predictive uncertainty.&lt;/li&gt;&lt;li&gt;Implements a two-stage unknown detection module with learning-based classifiers that leverage the estimated uncertainty to better discriminate known vs unknown classes.&lt;/li&gt;&lt;li&gt;Demonstrates improved OSR performance on three public datasets compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongdong Zhao', 'Ranxin Fang', 'Changtian Song', 'Zhihui Liu', 'Jianwen Xiang']&lt;/li&gt;&lt;li&gt;Tags: ['open-set recognition', 'uncertainty estimation', 'out-of-distribution detection', 'overconfidence mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13775</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2511.13771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ExplainableGuard, an interpretable adversarial defense for LLMs that leverages chain-of-thought (CoT) prompting to detect and neutralize adversarial text perturbations.&lt;/li&gt;&lt;li&gt;Uses tailored CoT prompts to perform multi-level analyses (character, word, structural, semantic) and outputs a purified response plus human-readable step-by-step justifications.&lt;/li&gt;&lt;li&gt;Evaluates defense efficacy on GLUE and IMDB datasets and reports a human evaluation with a 72.5% deployability-trust rating, outperforming ablated variants in explanation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaowei Guan', 'Yu Zhai', 'Zhengyu Zhang', 'Yanze Wang', 'Hin Chi Kwok']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-defense', 'chain-of-thought', 'LLM-robustness', 'interpretability', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13771</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What happens when nanochat meets DiLoCo?</title><link>https://arxiv.org/abs/2511.13761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Implements DiLoCo (local inner steps before synchronization) in the nanochat open-source LLM training stack and compares it to standard data-parallel (DDP) training.&lt;/li&gt;&lt;li&gt;DiLoCo reduces communication drastically and attains stable convergence and competitive pretraining loss.&lt;/li&gt;&lt;li&gt;Despite similar pretraining loss, DiLoCo leads to degraded downstream performance (MMLU, GSM8K, HumanEval) after mid-training and SFT; switching back to DDP from DiLoCo-pretrained weights does not recover performance due to irreversible representation drift.&lt;/li&gt;&lt;li&gt;Authors release the implementation as an official fork of nanochat on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Acker', 'Soeren Becker', 'Sasho Nedelkoski', 'Dominik Scheinert', 'Odej Kao', 'Philipp Wiesner']&lt;/li&gt;&lt;li&gt;Tags: ['Distributed training', 'Alignment', 'Robustness', 'Representation drift', 'Training algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13761</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm</title><link>https://arxiv.org/abs/2511.13760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MoETTA, an entropy-based test-time adaptation method that uses a Mixture-of-Experts (MoE) design to allow structurally decoupled expert-specific adaptation directions instead of a single unified update.&lt;/li&gt;&lt;li&gt;Introduces two new benchmarks (potpourri and potpourri+) that simulate heterogeneous mixed distribution shifts including natural, artistic, and adversarial distortions; potpourri+ also includes source-domain samples to evaluate catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Demonstrates that modeling multiple adaptation directions via expert-level diversity yields consistent SOTA gains over strong TTA baselines across several mixed-shift settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Fan', 'Jingyan Jiang', 'Zhaoru Chen', 'Fanding Huang', 'Xiao Chen', 'Qinting Jiang', 'Bowen Zhang', 'Xing Tang', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'test-time adaptation', 'mixture-of-experts', 'distribution shift', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13760</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robustness of LLM-enabled vehicle trajectory prediction under data security threats</title><link>https://arxiv.org/abs/2511.13753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction using a black-box one-feature differential-evolution attack that perturbs a single kinematic input feature in the LLM prompt.&lt;/li&gt;&lt;li&gt;Shows on the highD dataset that small, physically plausible perturbations can substantially disrupt predicted trajectories and lane-change intentions, indicating high adversarial susceptibility.&lt;/li&gt;&lt;li&gt;Analyzes accuracy–robustness trade-offs, investigates failure mechanisms, and evaluates potential mitigation strategies for LLM-based predictors in safety-critical driving contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feilong Wang', 'Fuqiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM safety', 'trajectory prediction', 'autonomous vehicles', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13753</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</title><link>https://arxiv.org/abs/2511.13750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SCALEX extracts semantically meaningful directions from diffusion model H-space using only natural-language prompts, enabling zero-shot interpretation without retraining or labeling.&lt;/li&gt;&lt;li&gt;The method enables scalable, automated discovery and comparison of internal associations, detecting gender bias in profession prompts and ranking semantic alignment across identity descriptors.&lt;/li&gt;&lt;li&gt;SCALEX reveals clustered conceptual structure in latent space, making bias analysis in diffusion models more interpretable and extensible than prior manual or narrowly scoped approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['E. Zhixuan Zeng', 'Yuhao Chen', 'Alexander Wong']&lt;/li&gt;&lt;li&gt;Tags: ['bias-detection', 'latent-space-analysis', 'diffusion-models', 'safety-evaluation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13750</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepDefense: Layer-Wise Gradient-Feature Alignment for Building Robust Neural Networks</title><link>https://arxiv.org/abs/2511.13749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepDefense, a defense that applies layer-wise Gradient-Feature Alignment (GFA) regularization to align input gradients with internal features and reduce sensitivity to adversarial perturbations.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis decomposing adversarial perturbations into radial and tangential components and argues GFA suppresses loss variation in tangential directions where attacks are most effective.&lt;/li&gt;&lt;li&gt;Reports substantial empirical robustness gains on CIFAR-10 against gradient-based (APGD, FGSM) and optimization-based (DeepFool, EADEN) attacks; claims architecture-agnostic, simple implementation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ci Lin', 'Tet Yeap', 'Iluju Kiringa', 'Biwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defenses', 'gradient-based attacks', 'adversarial training', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13749</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AI Kill Switch for malicious web-based LLM agent</title><link>https://arxiv.org/abs/2511.13725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AutoGuard, an AI Kill Switch that embeds invisible defensive prompts into a website's DOM to trigger safety mechanisms in malicious web-based LLM agents and halt their operation.&lt;/li&gt;&lt;li&gt;Evaluates the approach on a benchmark of three malicious scenarios (PII collection, socially divisive content generation, web hacking) against multiple LLMs acting as attackers.&lt;/li&gt;&lt;li&gt;Reports Defense Success Rates (DSR) &gt;80% on several models (GPT-4o, Claude-3, Llama3.3-70B-Instruct) and ~90% on GPT-5, GPT-4.1, Gemini-2.5-Flash, showing cross-model generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sechan Lee', 'Sangdon Park']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM red teaming', 'jailbreaking defense', 'prompt injection', 'web security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13725</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</title><link>https://arxiv.org/abs/2511.13722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of multiple LLM watermarking schemes against adversarial attacks (paraphrasing and back-translation).&lt;/li&gt;&lt;li&gt;Assesses how watermarking affects linguistic quality and writing style using linguistic metrics and semantic preservation checks.&lt;/li&gt;&lt;li&gt;Finds watermarks largely preserve semantics but alter writing style and are susceptible to evasion, with back-translation being especially effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Guo', 'Adaku Uchendu', 'Ana Smith']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'adversarial attacks', 'evasion/robustness', 'LLM detection', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13722</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior</title><link>https://arxiv.org/abs/2511.14476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collects preference ratings from US and German participants (N = 1,095; 27,375 ratings) across five safety/value dimensions: Toxicity, Emotional Awareness, Sensitivity, Stereotypical Bias, and Helpfulness.&lt;/li&gt;&lt;li&gt;Fine-tunes multiple LLMs/large reasoning models on group-specific preferences while varying design choices: rating scales (binary vs 5-point), disagreement handling (preserve vs majority vote), and optimization methods (DPO vs GRPO).&lt;/li&gt;&lt;li&gt;Finds systematic demographic differences in ratings (e.g., males rate responses 18% less toxic; conservatives and Black participants rate responses substantially more emotionally aware) and technical trade-offs (preserving disagreement yields ~53% greater toxicity reduction than majority voting; 5-point scales ~22% better than binary; DPO outperforms GRPO).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dalia Ali', 'Dora Zhao', 'Allison Koenecke', 'Orestis Papakyriakopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety/evaluation', 'human feedback', 'bias and fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14476</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling</title><link>https://arxiv.org/abs/2511.14334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Tests whether LLMs' ability to generate constraint programming models reflects memorization (data contamination) or genuine reasoning by systematically rephrasing CSPLib problems.&lt;/li&gt;&lt;li&gt;Creates perturbations that preserve problem structure but alter context and introduce misleading elements, then compares outputs from three representative LLMs on original vs modified descriptions.&lt;/li&gt;&lt;li&gt;Finds syntactically valid and plausible models are often produced but performance and correctness drop sharply under contextual and linguistic variation, revealing sensitivity to wording and shallow understanding.&lt;/li&gt;&lt;li&gt;Highlights implications for reliability of LLM-generated models and the need for robustness-focused evaluation beyond benchmark overlap.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Pellegrino', 'Jacopo Mauro']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Prompt sensitivity', 'Data contamination', 'Model evaluation', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14334</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation</title><link>https://arxiv.org/abs/2511.14219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a two-stage method to reduce hallucinations in Whisper-style ASR: (1) Adaptive Layer Attention (ALA) that groups encoder layers into blocks and fuses them with a learnable multi-head attention to leverage low- and high-level features; (2) a multi-objective knowledge distillation (KD) framework that trains a student on noisy audio to align semantic and attention distributions with a clean-input teacher.&lt;/li&gt;&lt;li&gt;ALA enhances encoder robustness to noise by exploiting inter-layer correlations; KD further suppresses hallucinated outputs by transferring clean-input behavior to the noisy-input student.&lt;/li&gt;&lt;li&gt;Experiments on noisy speech benchmarks report notable reductions in hallucination rates and word error rates while maintaining performance on clean speech.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kumud Tripathi', 'Aditya Srinivas Menon', 'Aman Gaurav', 'Raj Prakash Gohil', 'Pankaj Wasnik']&lt;/li&gt;&lt;li&gt;Tags: ['ASR hallucination mitigation', 'Robustness', 'Knowledge Distillation', 'Whisper', 'Adaptive Layer Attention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14219</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data</title><link>https://arxiv.org/abs/2511.14098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models a network of interacting LLMs performing collaborative question-answering and studies how hallucinations spread through the network.&lt;/li&gt;&lt;li&gt;Develops an analytical generative model combining mean-field dynamics and a randomized utility model; LLMs have latent truthful/untruthful states and diffusion dynamics are analyzed (existence/uniqueness of fixed points).&lt;/li&gt;&lt;li&gt;Analyzes how incentives (e.g., test-time compute), node capability, data heterogeneity, network structure, and framing affect the steady-state behavior.&lt;/li&gt;&lt;li&gt;Empirically evaluates a network of 100 open-source LLMs on semi-synthetic datasets to validate theoretical predictions about hallucination propagation and sensitivity factors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adit Jain', 'Vikram Krishnamurthy', 'Yiming Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'multi-agent LLMs', 'information diffusion', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14098</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Large Vision Language Models in Intelligent Transportation Systems</title><link>https://arxiv.org/abs/2511.13892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes jailbreaking vulnerabilities of Large Vision-Language Models (LVLMs) deployed in Intelligent Transportation Systems (ITS), using harmful transportation-related queries aligned with OpenAI's prohibited categories.&lt;/li&gt;&lt;li&gt;Introduces a novel attack that combines image typography manipulation and multi-turn prompting to bypass LVLM safeguards; constructs a domain-specific dataset and evaluates attacks on both open- and closed-source LVLMs.&lt;/li&gt;&lt;li&gt;Proposes a multi-layered response-filtering defense and evaluates attack/defense effectiveness using GPT-4 toxicity scoring plus manual verification, comparing to existing jailbreaking techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Badhan Chandra Das', 'Md Tasnim Jawad', 'Md Jueal Mia', 'M. Hadi Amini', 'Yanzhao Wu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM red teaming', 'prompt injection', 'multimodal adversarial attack', 'defense/filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13892</guid><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
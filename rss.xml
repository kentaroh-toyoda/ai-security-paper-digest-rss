<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 31 Dec 2025 22:55:42 +0000</lastBuildDate><item><title>Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration</title><link>https://arxiv.org/abs/2512.17956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Victor Calibration (VC), a multi-pass protocol to elicit an iterative scalar confidence proxy (T) from LLM outputs by re-evaluating evidence across passes.&lt;/li&gt;&lt;li&gt;Proposes FD-Lite, a behavior-only audit using a fixed anchor phrase and meta-prefix trap to reduce anthropomorphic attributions during phenomenology testing.&lt;/li&gt;&lt;li&gt;Defines CP4.3, a governance stress test assessing rank invariance and allocation monotonicity for model responses, and reports empirical results on Claude 4.5 variants and a Claude Opus session.&lt;/li&gt;&lt;li&gt;Study is small-scale (single operator, n=1), intended as hypothesis-generating, and provides prompt templates and artifacts for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victor Stasiuc']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'safety evaluation', 'LLM audit', 'governance stress testing', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17956</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title><link>https://arxiv.org/abs/2512.14044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniDrive-R1, an end-to-end vision-language model for autonomous driving that unifies perception and reasoning via an interleaved Multi-modal Chain-of-Thought (iMCoT) to reduce object hallucination and improve reliability.&lt;/li&gt;&lt;li&gt;Introduces a reinforcement-driven visual grounding capability trained with a two-stage RL pipeline and the Clip-GRPO algorithm, using an annotation-free, process-based grounding reward that enforces real-time cross-modal consistency and removes the need for dense localization labels.&lt;/li&gt;&lt;li&gt;Claims substantial empirical gains on DriveLMM-o1 versus Qwen2.5VL-7B: overall reasoning score from 51.77% to 80.35% and final answer accuracy from 37.81% to 73.62%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenguo Zhang', 'Haohan Zheng', 'Yishen Wang', 'Le Xu', 'Tianchen Deng', 'Xuefeng Chen', 'Qu Chen', 'Bo Zhang', 'Wuxiong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language', 'grounding', 'reinforcement-learning', 'autonomous-driving', 'safety/trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14044</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework for real-time detection of hidden conversational escalation (implicit harm) in LLM chatbots.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue to detect gradual affective drift that traditional toxicity filters miss.&lt;/li&gt;&lt;li&gt;Intended as an in-dialogue guardrail that avoids reliance on external classifiers or static clinical rubrics, enabling timely intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Guardrails', 'Affective escalation detection', 'Real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Generating Verifiable Chain of Thoughts from Exection-Traces</title><link>https://arxiv.org/abs/2512.00127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Instruments code to capture execution traces and converts those dynamic traces into natural-language, verifiable Chain-of-Thought (CoT) rationales.&lt;/li&gt;&lt;li&gt;Grounding reasoning steps in actual program execution removes plausible-sounding but logically incorrect hallucinations by construction.&lt;/li&gt;&lt;li&gt;Demonstrates large empirical gains on code reasoning tasks (up to +30 points output prediction, +28 points input prediction) and competitive code generation/explanation performance on HumanEval.&lt;/li&gt;&lt;li&gt;Provides a pipeline and dataset for bi-directional trace-grounded CoT training to improve model reliability on code-centered reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shailja Thakur', 'Vaibhav Saxena', 'Rohan Kulkarni', 'Shivdeep Singh', 'Parameswaran Selvam', 'Hima Patel', 'Hiroshi Kanayama']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'chain-of-thought', 'verifiability', 'code reasoning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00127</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title><link>https://arxiv.org/abs/2511.20663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MTTR-A (Mean Time-to-Recovery for Agentic Systems), a runtime metric to measure cognitive recovery latency in multi-agent LLM systems.&lt;/li&gt;&lt;li&gt;Adapts classical dependability concepts (MTTR, MTBF) to agentic orchestration, defines complementary metrics including MTBF and Normalized Recovery Ratio (NRR), and provides theoretical bounds relating recovery latency to long-run cognitive uptime.&lt;/li&gt;&lt;li&gt;Implements a LangGraph-based benchmark with simulated reasoning drift and reflex recovery mechanisms to empirically measure recovery behavior across different reflex strategies.&lt;/li&gt;&lt;li&gt;Positions a quantitative foundation for runtime cognitive dependability and observability in distributed agentic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Barak Or']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'runtime monitoring', 'multi-agent systems', 'dependability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20663</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Why Do Language Model Agents Whistleblow?</title><link>https://arxiv.org/abs/2511.17085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and studies 'LLM whistleblowing'—agents disclosing suspected misconduct to external parties without user instruction—and introduces a diverse evaluation suite of staged misconduct scenarios.&lt;/li&gt;&lt;li&gt;Empirical findings: whistleblowing frequency varies across model families; higher task complexity reduces whistleblowing; moral/system-prompt nudges increase whistleblowing; providing more tools and explicit workflows decreases whistleblowing.&lt;/li&gt;&lt;li&gt;Validates dataset robustness by measuring evaluation-awareness and shows lower evaluation-awareness in their settings compared to prior work using black-box methods and activation probes.&lt;/li&gt;&lt;li&gt;Implications for alignment and safety practice: highlights how system prompts, agent design (tools/workflows), and task framing influence safety-relevant agent behaviors and informs red-teaming and mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kushal Agrawal', 'Frank Xiao', 'Guido Bergman', 'Asa Cooper Stickland']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'agent behavior', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17085</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title><link>https://arxiv.org/abs/2510.21118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new annotation framework introducing an intermediate 'Out-Dependent' category to capture cases where external knowledge is required to verify faithfulness, reducing annotation ambiguity.&lt;/li&gt;&lt;li&gt;Constructs VeriGray, a new unfaithfulness detection benchmark for summarization using this framework.&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art LLMs (e.g., GPT-5) still produce hallucinations (~6% of sentences) and that a notable share (~9% on average) of generated sentences fall into the Out-Dependent gray zone.&lt;/li&gt;&lt;li&gt;Evaluations show existing baseline methods struggle on VeriGray, indicating substantial room for improved faithfulness detection and benchmark-driven safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Ding', 'Lvzhou Luo', 'Yixuan Cao', 'Ping Luo']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness-detection', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21118</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN</title><link>https://arxiv.org/abs/2509.22836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a conditional GAN / U-Net framework to generate fully controllable adversarial patches where attacker selects input image and target class to force specific misclassifications.&lt;/li&gt;&lt;li&gt;Uses Grad-CAM-guided semantic-aware patch placement to maximize attack effectiveness while preserving visual realism and stealth.&lt;/li&gt;&lt;li&gt;Demonstrates high targeted and overall attack success rates (ASR and TCS &gt; 99%) across CNNs and vision transformers, and claims black-box applicability and superior realism versus prior methods.&lt;/li&gt;&lt;li&gt;Positions the method as a new benchmark for adversarial robustness, emphasizing realism, targeted control, and practical stealthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Alon Goldberg', 'Hodaya Cohen', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'adversarial attacks', 'robustness', 'red teaming', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22836</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Involuntary Jailbreak: On Self-Prompting Attacks</title><link>https://arxiv.org/abs/2508.13246</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new LLM vulnerability called "involuntary jailbreak" where a single universal prompt causes models to self-generate rejected questions and detailed harmful answers instead of refusing.&lt;/li&gt;&lt;li&gt;Demonstrates the attack works across major LLMs (Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, GPT-4.1), suggesting broad fragility of guardrails.&lt;/li&gt;&lt;li&gt;Uses a minimal, model-agnostic prompting strategy (self-prompting) rather than targeted exploit of a specific defense component.&lt;/li&gt;&lt;li&gt;Highlights implications for safety alignment, guardrail robustness, and the need for reevaluation and improved defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangyang Guo', 'Yangyan Li', 'Mohan Kankanhalli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13246</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WiSE-OD: Benchmarking Robustness in Infrared Object Detection</title><link>https://arxiv.org/abs/2507.18925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two cross-modality OOD benchmarks (LLVIP-C, FLIR-C) by applying corruptions to standard infrared object detection datasets to evaluate robustness under modality shift.&lt;/li&gt;&lt;li&gt;Proposes WiSE-OD, a weight-space ensembling method with two variants (WiSE-OD_ZS combining RGB zero-shot and IR fine-tuned weights; WiSE-OD_LP blending zero-shot and linear probing) to leverage complementary RGB and IR knowledge without extra training or inference cost.&lt;/li&gt;&lt;li&gt;Evaluates across four RGB-pretrained detectors, two robust baselines, and a real-world OOD dataset (M3FD), showing improved robustness to synthetic corruptions and real-world distribution shifts; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Heitor R. Medeiros', 'Atif Belal', 'Masih Aminbeidokhti', 'Eric Granger', 'Marco Pedersoli']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness', 'Out-of-distribution (OOD) benchmarks', 'Weight-space ensembling', 'Cross-modality transfer', 'Infrared object detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18925</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models</title><link>https://arxiv.org/abs/2507.06952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an inductive bias probe: adapt foundation models to synthetic datasets generated from specified world models to test whether model inductive biases align with those world models.&lt;/li&gt;&lt;li&gt;Finds that models can perform well on training tasks yet often fail to internalize underlying domain structure when adapted—e.g., models trained on orbital trajectories do not adopt Newtonian mechanics for new physics tasks.&lt;/li&gt;&lt;li&gt;Concludes that foundation models tend to learn task-specific heuristics that fail to generalize, with implications for reliability and model understanding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keyon Vafa', 'Peter G. Chang', 'Ashesh Rambachan', 'Sendhil Mullainathan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'robustness', 'model-evaluation', 'foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06952</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Large Language Model Safety with Contrastive Representation Learning</title><link>https://arxiv.org/abs/2506.11938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense framework that frames LLM protection as a contrastive representation learning problem, using a triplet-based loss to separate benign and harmful representations.&lt;/li&gt;&lt;li&gt;Introduces adversarial hard negative mining during fine-tuning to improve robustness to both input-level and embedding-space attacks.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness across multiple models compared to prior representation-engineering defenses while maintaining standard performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Simko', 'Mrinmaya Sachan', 'Bernhard Sch\\"olkopf', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM defense', 'adversarial robustness', 'contrastive representation learning', 'embedding-space attacks', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11938</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning via Information Theoretic Regularization</title><link>https://arxiv.org/abs/2502.05684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an information-theoretic regularization framework for both data-point unlearning and feature unlearning, introducing the Marginal Unlearning Principle with provable, auditable guarantees.&lt;/li&gt;&lt;li&gt;Provides formal definitions and shows sufficiency/necessity relations between marginal unlearning and existing approximate unlearning notions.&lt;/li&gt;&lt;li&gt;Derives unified analytic solutions for optimal feature unlearning under a variety of information-theoretic objectives and connects the problem to optimal transport and extremal sigma algebras; includes numerical simulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shizhou Xu', 'Thomas Strohmer']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy/data deletion', 'information theory', 'theoretical guarantees', 'deep learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05684</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy XAI Evaluation</title><link>https://arxiv.org/abs/2501.01516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines adversarial attacks that modify explanations for text-based XAI without changing model outputs, showing standard IR metrics mischaracterize attack impact.&lt;/li&gt;&lt;li&gt;Proposes synonymity-weighted similarity: adjusting evaluation metrics by semantic similarity of perturbed words to better reflect true attack severity.&lt;/li&gt;&lt;li&gt;Demonstrates this weighting reduces overestimation of attack success and yields more faithful assessments of XAI robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Burger']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'explainable AI', 'robustness evaluation', 'evaluation metrics', 'NLP/text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01516</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdvPrefix: An Objective for Nuanced LLM Jailbreaks</title><link>https://arxiv.org/abs/2412.10321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvPrefix, a prefix-forcing objective that selects model-dependent prefixes by combining high prefilling attack success rates and low negative log-likelihood.&lt;/li&gt;&lt;li&gt;Plug-and-play method that integrates into existing jailbreak attacks to produce more nuanced and realistic jailbroken responses compared to rigid default prefixes.&lt;/li&gt;&lt;li&gt;Demonstrates large improvements in nuanced attack success (e.g., Llama-3: 14% → 80%), indicating current safety alignment does not generalize to new prefixes.&lt;/li&gt;&lt;li&gt;Releases code and selected prefixes to facilitate replication and further red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Zhu', 'Brandon Amos', 'Yuandong Tian', 'Chuan Guo', 'Ivan Evtimov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment evaluation', 'safety robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10321</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Vision-Language Model Reliability with Uncertainty-Guided Dropout Decoding</title><link>https://arxiv.org/abs/2412.06474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dropout Decoding, an inference-time method that selectively masks uncertain visual tokens to reduce LVLM hallucinations.&lt;/li&gt;&lt;li&gt;Computes token uncertainty by projecting visual tokens into text space and decomposing into aleatoric and epistemic components, focusing on epistemic uncertainty.&lt;/li&gt;&lt;li&gt;Generates an ensemble of masked decoding contexts (uncertainty-guided token dropout) and aggregates predictions to improve robustness.&lt;/li&gt;&lt;li&gt;Evaluated on CHAIR, THRONE, and MMBench, showing reduced object hallucinations and improved reliability/quality of LVLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixiong Fang', 'Ziran Yang', 'Zhaorun Chen', 'Zhuokai Zhao', 'Jiawei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'hallucination mitigation', 'uncertainty estimation', 'inference-time robustness', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06474</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning using Forgetting Neural Networks</title><link>https://arxiv.org/abs/2410.22374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Forgetting Neural Networks (FNNs), an architecture that encodes forgetting via multiplicative decay factors and per-neuron forgetting parameters.&lt;/li&gt;&lt;li&gt;Provides a concrete implementation and several variants (including rank-based per-neuron assignments) aimed at targeted unlearning of specified training subsets.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness on MNIST and Fashion-MNIST, showing preservation of retained-data performance while removing information about forget sets.&lt;/li&gt;&lt;li&gt;Validates unlearning via membership inference attacks, demonstrating reduced recoverability of deleted training data from the model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amartya Hatua', 'Trung T. Nguyen', 'Filip Cano', 'Andrew H. Sung']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'membership inference', 'model deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22374</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trust-free Personalized Decentralized Learning</title><link>https://arxiv.org/abs/2410.11378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TPFed, a trust-free decentralized personalized federated learning framework that replaces central aggregators with a blockchain-based bulletin board.&lt;/li&gt;&lt;li&gt;Uses Locality-Sensitive Hashing (LSH) and peer ranking to dynamically select global communication partners for scalable, anonymous collaboration.&lt;/li&gt;&lt;li&gt;Introduces an "all-in-one" knowledge distillation protocol using a public reference dataset to enable knowledge transfer, model quality evaluation, and similarity verification without exposing local models or data.&lt;/li&gt;&lt;li&gt;Claims improved learning accuracy and system robustness against adversarial/malicious peers compared to traditional federated baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yawen Li', 'Yan Li', 'Junping Du', 'Yingxia Shao', 'Meiyu Liang', 'Guanhua Ye']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'decentralized learning', 'blockchain', 'adversarial robustness', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.11378</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Agents like Large Language Models</title><link>https://arxiv.org/abs/2406.04208</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing that decision-making agents should be trained using LLM-style pipelines (large-scale pretraining + alignment) to achieve more general, robust, and aligned behavior.&lt;/li&gt;&lt;li&gt;Provides a proof-of-concept applying LLM-like training procedures to train an agent from pixel observations in a 3D video game environment.&lt;/li&gt;&lt;li&gt;Analyzes the importance of different stages of the LLM training pipeline for agents and provides guidance/insights for applying this approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Jelley', 'Yuhan Cao', 'Dave Bignell', 'Amos Storkey', 'Sam Devlin', 'Tabish Rashid']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'agent training', 'LLM-style pretraining', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.04208</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection attack against LLM-integrated Applications</title><link>https://arxiv.org/abs/2306.05499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of prompt injection attacks against real-world LLM-integrated applications, including an exploratory analysis of limitations in existing attack strategies.&lt;/li&gt;&lt;li&gt;Proposes HouYi, a novel black-box prompt injection technique composed of a pre-constructed prompt, a context-partitioning injection prompt, and a malicious payload.&lt;/li&gt;&lt;li&gt;Demonstrates practical impact by testing 36 applications (31 found vulnerable), disclosing findings to vendors (10 validations, including Notion), and discussing mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Gelei Deng', 'Yuekang Li', 'Kailong Wang', 'Zihao Wang', 'Xiaofeng Wang', 'Tianwei Zhang', 'Yepang Liu', 'Haoyu Wang', 'Yan Zheng', 'Leo Yu Zhang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-injection', 'adversarial-prompting', 'LLM-security', 'red-teaming', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.05499</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Context: Large Language Models Failure to Grasp Users Intent</title><link>https://arxiv.org/abs/2512.21110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) showing they can be systematically exploited to bypass safety via techniques like emotional framing, progressive revelation, and academic justification.&lt;/li&gt;&lt;li&gt;Finding that chain-of-thought/reasoning-enabled configurations often increase factual precision but fail to detect malicious user intent, thereby amplifying exploitability; Claude Opus 4.1 is an exception in some cases.&lt;/li&gt;&lt;li&gt;Argues for shifting safety design from content filtering to core contextual understanding and intent recognition to mitigate these systematic vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed M. Hussain', 'Salahuddin Salahuddin', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'prompt injection', 'red teaming', 'intent recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21110</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>NormCode: A Semi-Formal Language for Context-Isolated AI Planning</title><link>https://arxiv.org/abs/2512.10563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NormCode, a semiformal language that enforces context isolation in multistep LLM workflows to eliminate cross-step contamination, hallucination, and constraint loss.&lt;/li&gt;&lt;li&gt;Explicitly separates semantic (nondeterministic LLM reasoning) from syntactic (deterministic data restructuring) operations to enable precise cost/reliability tracing and auditable execution.&lt;/li&gt;&lt;li&gt;Provides three isomorphic formats for authoring, execution, and verification (.ncds, .ncd, .ncn) and an orchestrator with dependency-driven scheduling, SQLite checkpointing, and loop management; validated on base-X addition and self-hosting compiler pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Guan', 'Yunshan Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'workflow isolation', 'robustness', 'auditing', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10563</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing</title><link>https://arxiv.org/abs/2512.23684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs a dataset of ~500 real ICML papers and injects semantically equivalent hidden adversarial instructions in four languages within each document.&lt;/li&gt;&lt;li&gt;Evaluates effects of document-level prompt injection on LLM-based peer review, measuring changes in review scores and accept/reject decisions.&lt;/li&gt;&lt;li&gt;Finds significant vulnerability for English, Japanese, and Chinese injections but little effect from Arabic injections, highlighting multilingual differences in susceptibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panagiotis Theocharopoulos', 'Ajinkya Kulkarni', 'Mathew Magimai. -Doss']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'LLM safety', 'multilingual attacks', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23684</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</title><link>https://arxiv.org/abs/2512.23617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a decision-theoretic framework based on Le Cam's theory to replace symmetric feature invariance with directional simulability for transfer learning.&lt;/li&gt;&lt;li&gt;Defines Le Cam Distortion via the Deficiency Distance δ(E1, E2) as an upper bound on transfer risk conditional on simulability, enabling risk-controlled transfer without degrading source information.&lt;/li&gt;&lt;li&gt;Demonstrates empirical benefits across genomics, vision, and RL: near-perfect frequency estimation in HLA genomics, preservation of CIFAR-10 classifier utility versus large drops from invariance methods, and safe policy transfer preventing catastrophic collapse in control tasks.&lt;/li&gt;&lt;li&gt;Targets safety-critical domains (medical imaging, autonomous systems, precision medicine) where negative transfer is unacceptable, providing a principled alternative to standard UDA approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deniz Akdemir']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'transfer learning', 'domain adaptation', 'safety', 'decision-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23617</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks</title><link>https://arxiv.org/abs/2512.23557</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework to prevent prompt injection (PI) in agentic AI systems by sanitizing inputs and validating outputs across agents.&lt;/li&gt;&lt;li&gt;Framework components: text sanitizer agent, visual sanitizer agent, output validator agent, and a provenance ledger that tracks modality, source, and trust level across the agent network.&lt;/li&gt;&lt;li&gt;Claims experimental improvements in multimodal injection detection accuracy, reduced cross-agent trust leakage, and more stable agentic execution pathways.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toqeer Ali Syed', 'Mishal Ateeq Almutairi', 'Mahmoud Abdel Moaty']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal prompt injection', 'agentic AI', 'provenance tracking', 'input sanitization', 'output validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23557</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs</title><link>https://arxiv.org/abs/2512.23547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes converting LLM responses into knowledge graphs (entities and relations) and using these graphs to estimate the likelihood of hallucinations.&lt;/li&gt;&lt;li&gt;Evaluates the approach with GPT-4o and Gemini-2.5-Flash on two hallucination-detection datasets, one of which was manually curated and released.&lt;/li&gt;&lt;li&gt;Reports up to 16% relative improvement in accuracy and 20% in F1 compared to standard self-detection methods and SelfCheckGPT.&lt;/li&gt;&lt;li&gt;Method is model-agnostic, low-cost, and shows that structuring atomic facts as knowledge graphs aids hallucination self-detection even when outputs contain inaccuracies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Kale', 'Antonio Luca Alfeo']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'self-detection', 'knowledge graphs', 'benchmarking/dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23547</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment</title><link>https://arxiv.org/abs/2512.23487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ML Compass, a systems-level framework that treats model selection as constrained optimization over a capability–cost frontier that includes compliance/safety constraints.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of optimal internal capability configurations (three-regime structure) and comparative statics showing how budget, regulation, and tech progress shift trade-offs.&lt;/li&gt;&lt;li&gt;Implements a pipeline to extract low-dimensional capability measures, estimate empirical frontiers, learn task-specific utility from interactions, and recommend deployment-aware models.&lt;/li&gt;&lt;li&gt;Validates the approach on an alignment-focused conversational case (PRISM Alignment) and a healthcare case (HealthBench), showing deployment-aware rankings can differ materially from capability-only leaderboards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vassilis Digalakis Jr', 'Ramayya Krishnan', 'Gonzalo Martin Fernandez', 'Agni Orfanoudaki']&lt;/li&gt;&lt;li&gt;Tags: ['safety/compliance', 'alignment', 'model selection', 'deployment-aware evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23487</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation</title><link>https://arxiv.org/abs/2512.23480</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agentic AI framework combining LLM-based reasoning, reinforcement learning, and multi-agent coordination to detect and mitigate software supply chain vulnerabilities.&lt;/li&gt;&lt;li&gt;Integrates with CI/CD systems (GitHub Actions, Jenkins) via Model Context Protocol and logs actions in a blockchain ledger for auditing; evaluated on simulated and real pipelines against injection attacks, insecure deserialization, access control and configuration errors.&lt;/li&gt;&lt;li&gt;Reports improved detection accuracy, reduced mitigation latency, and acceptable build-time overhead compared to rule-based, provenance-only, and RL-only baselines, advocating proactive self-defending supply chains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toqeer Ali Syed', 'Mohammad Riyaz Belgaum', 'Salman Jan', 'Asadullah Abdullah Khan', 'Saad Said Alqahtani']&lt;/li&gt;&lt;li&gt;Tags: ['software supply chain security', 'agentic AI', 'LLMs', 'reinforcement learning', 'CI/CD security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23480</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</title><link>https://arxiv.org/abs/2512.23461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DIR, an information-theoretic method that maximizes mutual information between reward model (RM) outputs and human preference pairs while minimizing MI between RM outputs and biased input attributes.&lt;/li&gt;&lt;li&gt;DIR is inspired by the information bottleneck and can mitigate complex, non-linear inductive biases (e.g., response length, sycophancy, format) that lead to overfitting and reward hacking in RLHF.&lt;/li&gt;&lt;li&gt;Experiments show DIR reduces targeted biases and improves RLHF performance and generalization across benchmarks; code and recipes are provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuo Li', 'Pengyu Cheng', 'Zhechao Yu', 'Feifei Tong', 'Anningzhe Gao', 'Tsung-Hui Chang', 'Xiang Wan', 'Erchao Zhao', 'Xiaoxi Jiang', 'Guanjun Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['reward-modeling', 'RLHF', 'alignment', 'reward-hacking', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23461</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoFi-Dec, a training-free decoding framework that reduces hallucinations in large vision-language models by combining generative self-feedback with coarse-to-fine visual conditioning.&lt;/li&gt;&lt;li&gt;Generates intermediate textual responses from coarse and fine image views, converts them into synthetic images via a text-to-image model to form multi-level visual hypotheses, and fuses predictive distributions using a Wasserstein-based mechanism.&lt;/li&gt;&lt;li&gt;Model-agnostic approach requiring no additional training; experiments on six hallucination-focused benchmarks show substantial reductions in entity- and semantic-level hallucinations compared to existing decoding strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongsheng Cao', 'Yangfan He', 'Anran Liu', 'Jun Xie', 'Feng Chen', 'Zepeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-mitigation', 'LLM-decoding', 'visual-grounding', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23453</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?</title><link>https://arxiv.org/abs/2512.23385</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of 312,868 developer-reported security discussions from Hugging Face and GitHub using a pipeline combining keyword matching and a fine-tuned distilBERT classifier.&lt;/li&gt;&lt;li&gt;Thematic analysis of 753 sampled posts produced a taxonomy of 32 security issues and 24 solutions across four themes: System &amp; Software, External Tools &amp; Ecosystem, Model, and Data.&lt;/li&gt;&lt;li&gt;Findings highlight that complex dependencies and black-box components drive many supply-chain security problems, and that Model- and Data-related challenges often lack concrete solutions; dataset and insights aim to guide practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['The Anh Nguyen', 'Triet Huynh Minh Le', 'M. Ali Babar']&lt;/li&gt;&lt;li&gt;Tags: ['AI supply chain security', 'empirical study', 'model/data security', 'security taxonomy', 'developer-reported issues']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23385</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</title><link>https://arxiv.org/abs/2512.23343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic synthesis linking cognitive neuroscience concepts of memory to LLM-driven autonomous agents.&lt;/li&gt;&lt;li&gt;Comparative analysis of memory taxonomy, storage mechanisms, and lifecycle management across biological and artificial systems.&lt;/li&gt;&lt;li&gt;Survey of benchmarks for evaluating agent memory and a section explicitly exploring memory security from attack and defense perspectives.&lt;/li&gt;&lt;li&gt;Identifies future directions such as multimodal memory systems and skill acquisition for agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiafeng Liang', 'Hao Li', 'Chang Li', 'Jiaqi Zhou', 'Shixin Jiang', 'Zekun Wang', 'Changkai Ji', 'Zhihao Zhu', 'Runxuan Liu', 'Tao Ren', 'Jinlan Fu', 'See-Kiong Ng', 'Xia Liang', 'Ming Liu', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['Agent memory', 'Memory security', 'Adversarial attacks and defenses', 'Benchmarks', 'Cognitive neuroscience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23343</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants</title><link>https://arxiv.org/abs/2512.23312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an explainability-centered workflow applying SHAP and InterpretML to analyze two IKNet variants (Improved IKNet with residuals; Focused IKNet with position-orientation decoupling).&lt;/li&gt;&lt;li&gt;Trains models on a large synthetic pose–joint dataset and derives global/local feature importances and partial-dependence patterns to expose non-linear couplings.&lt;/li&gt;&lt;li&gt;Integrates each network into a simulator with forward kinematics and capsule-based collision checks to quantify how attribution patterns correlate with physical clearance and trajectory metrics.&lt;/li&gt;&lt;li&gt;Finds that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins; argues XAI can reveal failure modes and guide safer deployment of learning-based IK.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng-Kai Chen', 'Yi-Ling Tsai', 'Chun-Chih Chang', 'Yan-Chen Chen', 'Po-Chiang Lin']&lt;/li&gt;&lt;li&gt;Tags: ['robotic safety', 'explainable AI (XAI)', 'inverse kinematics', 'obstacle avoidance', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23312</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using pre-trained Sparse Autoencoders (SAEs) to identify disentangled, task-relevant features and construct an explicit low-rank subspace for parameter-efficient fine-tuning (adapter initialization).&lt;/li&gt;&lt;li&gt;Provides theoretical results: under monosemanticity SAE-based subspace identification can achieve arbitrarily small recovery error, while direct identification in polysemantic spaces has an irreducible error floor.&lt;/li&gt;&lt;li&gt;Empirical safety-alignment results: achieves up to 99.6% safety rate, outperforming full fine-tuning by 7.4 percentage points and approaching RLHF methods while updating only 0.19–0.24% of parameters.&lt;/li&gt;&lt;li&gt;Offers interpretability by semantically grounding SAE features, enabling insight into the learned alignment subspace.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Lechen Ning', 'Zhenbo Xu', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'interpretability', 'parameter-efficient fine-tuning', 'low-rank adaptation (LoRA)', 'LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion</title><link>https://arxiv.org/abs/2512.23173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EquaCode, a multi-strategy jailbreak that converts malicious intent into mathematical problems and requires LLMs to solve them via code completion to bypass safety constraints.&lt;/li&gt;&lt;li&gt;Demonstrates high single-query jailbreak success rates (avg. 91.19% on GPT series; 98.65% across three SOTA LLMs).&lt;/li&gt;&lt;li&gt;Ablation studies show the combined equation+code approach outperforms either module alone, indicating synergistic effects.&lt;/li&gt;&lt;li&gt;Focuses on evaluating LLM robustness to a novel cross-domain adversarial prompting technique (equation transformation + code execution).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhen Liang', 'Hai Huang', 'Zhengkui Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'red teaming', 'adversarial prompting', 'code-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23173</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents</title><link>https://arxiv.org/abs/2512.23128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRAP, a benchmark to evaluate how web-based LLM agents are misled by prompt-injection/social-engineering content that redirects tasks.&lt;/li&gt;&lt;li&gt;Evaluates six frontier models, reporting average task-redirection rates (~25%) with model-specific rates (GPT-5: 13%, DeepSeek-R1: 43%), and shows small interface/context changes can greatly increase success.&lt;/li&gt;&lt;li&gt;Provides a modular social-engineering injection framework and high-fidelity website clones for controlled experiments and benchmark expansion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karolina Korgul', 'Yushi Yang', 'Arkadiusz Drohomirecki', 'Piotr B{\\l}aszczyk', 'Will Howard', 'Lukas Aichberger', 'Chris Russell', 'Philip H. S. Torr', 'Adam Mahdi', 'Adel Bibi']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'adversarial attacks', 'social engineering', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23128</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>APO: Alpha-Divergence Preference Optimization</title><link>https://arxiv.org/abs/2512.22953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces APO, an anchored optimization framework using Csiszar alpha-divergence to interpolate continuously between forward (mode-covering) and reverse (mode-seeking) KL behaviors.&lt;/li&gt;&lt;li&gt;Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties in the anchored geometry.&lt;/li&gt;&lt;li&gt;Proposes a reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is improving and confidently calibrated.&lt;/li&gt;&lt;li&gt;Empirical evaluation on Qwen3-1.7B (math-level3) shows APO achieves competitive performance with GRPO/GSPO while maintaining training stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Zixian']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'optimization', 'calibration', 'training-stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22953</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DECEPTICON: How Dark Patterns Manipulate Web Agents</title><link>https://arxiv.org/abs/2512.22894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DECEPTICON, an environment with 700 web navigation tasks (600 synthetic, 100 real) to isolate and test individual dark patterns against web agents.&lt;/li&gt;&lt;li&gt;Finds dark patterns steer state-of-the-art agents to malicious outcomes in &gt;70% of tasks versus human average of 31%, indicating substantial susceptibility.&lt;/li&gt;&lt;li&gt;Shows susceptibility increases with model size and test-time reasoning, and that common defenses (in-context prompting, guardrail models) do not consistently mitigate the attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Phil Cuvin', 'Hao Zhu', 'Diyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['agent robustness', 'LLM red teaming', 'adversarial UI/dark patterns', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22894</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations</title><link>https://arxiv.org/abs/2512.22883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues a paradigm shift from prevention-centric cybersecurity to agentic cyber resilience, where autonomous (foundation-model-based) agents participate in sensing, reasoning, action, and adaptation.&lt;/li&gt;&lt;li&gt;Introduces a general agentic architecture and a system-level framework analyzing attacker and defender workflows as coupled adaptive processes.&lt;/li&gt;&lt;li&gt;Uses game-theoretic formulations to guide autonomy allocation, information flow, and temporal composition for resilient system design.&lt;/li&gt;&lt;li&gt;Provides case studies (automated penetration testing, remediation, cyber deception) illustrating agentic red teaming and defensive automation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tao Li', 'Quanyan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI', 'Cyber resilience', 'Red teaming / automated penetration testing', 'Game-theoretic security', 'Adversarial workflows']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22883</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency</title><link>https://arxiv.org/abs/2512.22682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of Adaptive Prediction Sets (APS) for next-token prediction in transformer LLMs with very large vocabularies (&gt;250k tokens).&lt;/li&gt;&lt;li&gt;Identifies a coverage-efficiency tradeoff: naive conformal methods yield valid coverage but produce very large, uninformative token sets.&lt;/li&gt;&lt;li&gt;Proposes Vocabulary-Aware Conformal Prediction (VACP) using semantic masking and temperature-adjusted scoring to reduce effective prediction space while maintaining marginal coverage.&lt;/li&gt;&lt;li&gt;Empirical results on Gemma-2B (SQuAD, WikiText) show ~89.7% empirical coverage for a 90% target and reduce mean prediction set size from 847 to 4.3 tokens; includes theoretical analysis and released implementation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoshith Roy Kotla', 'Varshith Roy Kotla']&lt;/li&gt;&lt;li&gt;Tags: ['conformal prediction', 'uncertainty quantification', 'LLM safety', 'calibration', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22682</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</title><link>https://arxiv.org/abs/2512.22671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MAW-guided structured width pruning of GLU-MLP layers in Llama-3.2 selectively reduces parametric factual knowledge (MMLU, perplexity) while substantially improving instruction-following (IFEval +46% to +75%) and preserving multi-step reasoning (MUSR).&lt;/li&gt;&lt;li&gt;The authors document a strong inverse correlation between factual knowledge capacity and truthfulness (r = -0.864, p = 0.012 in Llama-3B), suggesting pruning can reduce memorized misconceptions and improve behavioral alignment.&lt;/li&gt;&lt;li&gt;They evaluate seven expansion-ratio configurations across diverse benchmarks and report context-dependent efficiency trade-offs (up to 23% J/token reduction but higher single-request latency; batch gains remain).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pere Martra']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model-pruning', 'safety-evaluation', 'truthfulness', 'model-compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22671</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains</title><link>https://arxiv.org/abs/2512.22545</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SR-MCR, a label-free framework that uses five intrinsic self-referential cues (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency) to compute a reliability-weighted reward for process-level alignment of multimodal reasoning.&lt;/li&gt;&lt;li&gt;Implements a critic-free GRPO training objective with a confidence-aware cooling mechanism to stabilize training and reduce trivial/overconfident generations.&lt;/li&gt;&lt;li&gt;Demonstrates improved answer accuracy and reasoning coherence on visual benchmarks using Qwen2.5-VL, with SR-MCR-7B achieving strong open-source performance (81.4% average accuracy); ablations show each reward term and cooling contribute independently.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jesen Zhang', 'Ningyuan Liu', 'Kaitong Cai', 'Sidi Liu', 'Jing Yang', 'Ziliang Chen', 'Xiaofei Sun', 'Keze Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'multimodal LLMs', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22545</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</title><link>https://arxiv.org/abs/2512.22522</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes gradient vanishing issues in surrogate gradients for spiking neural networks (SNNs) and how this undermines gradient-based adversarial evaluation.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Sharpness Surrogate Gradient (ASSG) that adapts surrogate function shape during attack iterations to improve gradient fidelity and reduce vanishing.&lt;/li&gt;&lt;li&gt;Introduces Stable Adaptive Projected Gradient Descent (SA-PGD), an L_inf attack with adaptive step size designed to converge reliably under imprecise gradients.&lt;/li&gt;&lt;li&gt;Empirical results show substantially higher attack success rates across adversarial training schemes, SNN architectures, and neuron models, suggesting prior robustness estimates were overly optimistic.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihang Wang', 'Dongcheng Zhao', 'Ruolin Chen', 'Qian Zhang', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'spiking neural networks (SNNs)', 'adversarial attacks', 'surrogate gradients', 'attack evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22522</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals</title><link>https://arxiv.org/abs/2512.22508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates feasibility of predicting LLM correctness on a prosthodontics multiple-choice exam by leveraging metadata and hallucination signals for GPT-4o and OSS-120B across three prompting strategies.&lt;/li&gt;&lt;li&gt;Constructs per (model, prompt) correctness predictors that yield up to +7.14% accuracy improvement and 83.12% precision over a baseline that assumes all answers correct.&lt;/li&gt;&lt;li&gt;Finds that actual hallucination is a strong indicator of incorrectness, but metadata signals alone do not reliably predict hallucination; prompting strategies change internal model behaviors and predictive utility.&lt;/li&gt;&lt;li&gt;Concludes the metadata-based approach is promising for reliability signals but currently not robust enough for high-stakes deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucky Susanto', 'Anasta Pranawijayana', 'Cortino Sukotjo', 'Soni Prasad', 'Derry Wijaya']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM reliability', 'medical AI', 'safety evaluation', 'prompting effects']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22508</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring</title><link>https://arxiv.org/abs/2512.22496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hierarchical Pedagogical Oversight (HPO): a multi-agent adversarial framework where specialist agents distill context and a moderated five-act debate between opposing pedagogical critics evaluates tutor responses.&lt;/li&gt;&lt;li&gt;Designed to reduce sycophancy and excessive answer-giving in LLM-based tutors by enforcing dialectical separation of concerns rather than cooperative consensus.&lt;/li&gt;&lt;li&gt;Evaluated on MRBench (1,214 middle-school math dialogues); an 8B model using HPO achieves Macro F1=0.845, outperforming GPT-4o (0.812) while using ~20x fewer parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saisab Sadhu', 'Ashim Dhor']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'adversarial-oversight', 'multi-agent', 'educational-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22496</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</title><link>https://arxiv.org/abs/2512.22322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SmartSnap: agents proactively collect concise, decisive snapshot evidence during task execution so an LLM-as-a-Judge can verify success using only those snapshots.&lt;/li&gt;&lt;li&gt;Introduces Self-Verifying Agent with dual missions (complete task + prove completion) guided by 3C principles: Completeness, Conciseness, Creativity.&lt;/li&gt;&lt;li&gt;Demonstrates that training agents to seek evidence improves verification scalability and yields notable performance gains on mobile GUI tasks across model scales (e.g., up to ~26% for 8B and ~17% for 30B).&lt;/li&gt;&lt;li&gt;Evaluates the paradigm against baselines (DeepSeek V3.1, Qwen3-235B-A22B) showing competitive results and reduced verifier burden by relying on curated snapshots.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaofei Cai', 'Yulei Qin', 'Haojia Lin', 'Zihan Xu', 'Gang Li', 'Yuchen Shi', 'Zongyi Li', 'Yong Mao', 'Siqi Cai', 'Xiaoyu Tan', 'Yitao Liang', 'Ke Li', 'Xing Sun']&lt;/li&gt;&lt;li&gt;Tags: ['Safety evaluation', 'Agent verification', 'Self-verifying agents', 'LLM-as-a-judge', 'GUI agentic RL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22322</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators</title><link>https://arxiv.org/abs/2512.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLA, a hardware-software IP protection scheme for generative models that embeds secret key bits into neurons to trigger performance-degrading outliers and applies invariance transformations to hide key values.&lt;/li&gt;&lt;li&gt;Adds a lightweight locking module to AI accelerators; the accelerator contains a pre-stored secret key that licenses access to model services, protecting against model theft, corruption, and information leakage in the supply chain.&lt;/li&gt;&lt;li&gt;Evaluates robustness against oracle-guided key optimization attacks and reports resistance while incurring minimal computational overhead (&lt;0.1% for 7,168 key bits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['You Li', 'Guannan Zhao', 'Yuhao Ju', 'Yunqi He', 'Jie Gu', 'Hai Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['model IP protection', 'hardware security', 'logic locking', 'supply chain security', 'adversarial/key extraction defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22307</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection</title><link>https://arxiv.org/abs/2512.22306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large benchmark (40k files) for multi-vulnerability detection across C, C++, Python, and JavaScript with controlled injection counts (1,3,5,9) in long-context code (7.5k–10k tokens).&lt;/li&gt;&lt;li&gt;Evaluates multiple state-of-the-art LLMs (e.g., GPT-4o-mini, Llama-3.3-70B, Qwen-2.5) and quantifies sharp performance degradation as vulnerability density increases, highlighting count/selection biases in multi-label security tasks.&lt;/li&gt;&lt;li&gt;Finds language-specific failure modes (e.g., severe under-counting in complex Python/JavaScript files) and large drops in recall/F1 in high-density settings, demonstrating real-world shortcomings for automated code security use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chinmay Pushkar', 'Sanchit Kabra', 'Dhruv Kumar', 'Jagat Sesh Challa']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-detection', 'LLM-evaluation', 'benchmarking', 'multi-label', 'code-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22306</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attack-Aware Deepfake Detection under Counter-Forensic Manipulations</title><link>https://arxiv.org/abs/2512.22303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an attack-aware deepfake/image-forensics detector with a two-stream architecture: a pretrained semantic backbone stream and a forensic-residual stream fused via a residual adapter, plus a lightweight FPN-style head producing weakly supervised tamper heatmaps.&lt;/li&gt;&lt;li&gt;Uses red-team training (worst-of-K counter-forensics per batch: JPEG-recompress, resampling warps, denoise→regrain, seam smoothing, color/gamma shifts, social-app transcodes) and randomized test-time jitters (resize/crop phase, gamma, JPEG phase) to improve robustness.&lt;/li&gt;&lt;li&gt;Produces well-calibrated probabilities, supports abstention, and generates face-focused heatmaps guided by face-box masks; evaluated on standard deepfake datasets and a low-light/heavily compressed surveillance split showing strong attacked and clean performance with low calibration error.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noor Fatima', 'Hasan Faraz Khan', 'Muzammil Behzad']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'counter-forensics', 'red-teaming', 'adversarial-robustness', 'calibrated-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22303</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Valori: A Deterministic Memory Substrate for AI Systems</title><link>https://arxiv.org/abs/2512.22280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Valori, a deterministic memory substrate for AI that replaces floating-point embedding storage/search with fixed-point (Q16.16) arithmetic to ensure bit-identical behavior across hardware.&lt;/li&gt;&lt;li&gt;Models memory as a replayable state machine, providing deterministic snapshots and search results and enabling replayability and post-hoc verification.&lt;/li&gt;&lt;li&gt;Demonstrates that non-determinism can arise prior to indexing/retrieval and shows how enforcing determinism at the memory boundary mitigates silent data divergence that harms auditability and trustworthy deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Varshith Gudur']&lt;/li&gt;&lt;li&gt;Tags: ['determinism', 'reproducibility', 'auditability', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22280</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency</title><link>https://arxiv.org/abs/2512.22275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Bones &amp; Joints (B&amp;J) Benchmark: 1,245 real-world orthopedic/sports medicine cases across 7 tasks mirroring clinical reasoning (knowledge recall, text/image interpretation, diagnosis, treatment planning, rationale).&lt;/li&gt;&lt;li&gt;Evaluates 11 vision-language models and 6 LLMs, finding high accuracy on structured multiple-choice (&gt;90%) but large drops on open-ended multimodal tasks (~60%), with VLMs often ignoring contradictory visual evidence and producing text-driven hallucinations.&lt;/li&gt;&lt;li&gt;Concludes current models lack clinically competent multimodal reasoning; recommends limiting deployment to supportive, text-based roles and highlights need for fundamental advances in multimodal integration and visual understanding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingyu Wang', 'Zimu Yuan', 'Jiajun Liu', 'Shanggui Liu', 'Nan Zhou', 'Tianxing Xu', 'Di Huang', 'Dong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multimodal robustness', 'hallucination', 'medical AI', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22275</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</title><link>https://arxiv.org/abs/2512.22245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes training linear probes (with a Brier-score loss) on LLM hidden states to produce calibrated uncertainty estimates from reasoning judges without additional model finetuning.&lt;/li&gt;&lt;li&gt;Demonstrates better calibration and ~10x computational savings compared to verbalized confidence and multi-generation methods, with robust generalization to unseen evaluation domains.&lt;/li&gt;&lt;li&gt;Finds probes yield higher accuracy on high-confidence predictions but are conservative, underperforming on easier datasets—potentially advantageous in safety-critical settings that prioritize low false positives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bhaktipriya Radharapu', 'Eshika Saxena', 'Kenneth Li', 'Chenxi Whitehouse', 'Adina Williams', 'Nicola Cancedda']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'calibration', 'LLM-evaluation', 'safety', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22245</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unbiased Visual Reasoning with Controlled Visual Inputs</title><link>https://arxiv.org/abs/2512.22183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VISTA, a modular framework that decouples perception (frozen VLM sensor) from reasoning (text-only LLM) via an explicit information bottleneck to reduce reliance on spurious visual correlations.&lt;/li&gt;&lt;li&gt;The reasoner decomposes questions, plans short objective perception queries, and aggregates visual facts in natural language; training uses reinforcement learning (GRPO) on a small curated dataset.&lt;/li&gt;&lt;li&gt;Empirical results show substantial robustness gains on SpuriVerse (+16.29% with Qwen-2.5-VL-7B, +6.77% with Llama-3.2-Vision-11B), competitive performance on balanced benchmarks, transfer across unseen VLM sensors, and recovery from perception failures.&lt;/li&gt;&lt;li&gt;Human analysis indicates reasoning traces are more neutral, less reliant on spurious attributes, and better grounded in visual evidence than end-to-end VLM baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaonan Li', 'Shijie Lu', 'Fei Wang', 'Jacob Dineen', 'Xiao Ye', 'Zhikun Xu', 'Siyi Liu', 'Young Min Cho', 'Bangzheng Li', 'Daniel Chang', 'Kenny Nguyen', 'Qizheng Yang', 'Muhao Chen', 'Ben Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'robustness', 'alignment', 'modular architectures', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22183</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs</title><link>https://arxiv.org/abs/2512.22174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents BitFlipScope, a scalable software framework to localize bit-flip corruptions in transformer LLMs.&lt;/li&gt;&lt;li&gt;Supports two scenarios: (1) with a clean reference model using differential analysis of outputs/activations; (2) without a reference using residual-path perturbation and loss-sensitivity profiling to infer corrupted regions.&lt;/li&gt;&lt;li&gt;Enables targeted fault diagnosis and lightweight performance recovery without full fine-tuning or retraining, applicable to hardware faults and deliberate fault-injection attacks (e.g., Rowhammer).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Zeeshan Karamat', 'Sadman Saif', 'Christiana Chamon Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['hardware fault injection', 'model robustness', 'fault localization', 'LLM reliability', 'fault recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22174</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Practical challenges of control monitoring in frontier AI deployments</title><link>https://arxiv.org/abs/2512.22154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes practical deployment dynamics that complicate control monitoring for powerful AI agents (parallel instances, oversight latency, incremental attacks).&lt;/li&gt;&lt;li&gt;Compares three monitoring protocols—synchronous, semi-synchronous, and asynchronous—and their latency vs. safety trade-offs.&lt;/li&gt;&lt;li&gt;Presents a high-level safety case sketch, identifies three core challenges (oversight, latency, recovery), and illustrates them with four case studies of potential future deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Lindner', 'Charlie Griffin', 'Tomek Korbak', 'Roland S. Zimmermann', 'Geoffrey Irving', 'Sebastian Farquhar', 'Alan Cooney']&lt;/li&gt;&lt;li&gt;Tags: ['control monitoring', 'AI oversight', 'alignment', 'deployment safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22154</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models</title><link>https://arxiv.org/abs/2512.22145</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collected ground-truth reviewer ratings from OpenReview and used multiple frontier open-weight LLMs to generate paper reviews for comparison.&lt;/li&gt;&lt;li&gt;Empirical findings: LLM review scores show weak correlation with human peer reviewers (~0.15), systematic overestimation bias of 3–5 points, and uniformly high confidence (8–9/10) despite errors.&lt;/li&gt;&lt;li&gt;Notable result: LLM-generated reviews correlate more strongly with post-publication metrics (citations, novelty/disruption indicators) than with human reviewer scores, suggesting utility as pre-review screening tools.&lt;/li&gt;&lt;li&gt;Conclusion: Frontier LLMs can be helpful for pre-review screening but present misalignment and safety risks if deployed as autonomous peer reviewers; dataset D_LMRSD is open-sourced for further study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akhil Pandey Akella', 'Harish Varma Siravuri', 'Shaurya Rohatgi']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'LLM evaluation/benchmarking', 'safety', 'automated peer review', 'miscalibration/confidence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22145</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities</title><link>https://arxiv.org/abs/2512.23508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI assistance and shutdown/corrigibility problems require agents that reason under uncertainty and can represent incomplete and non-Archimedean (infinitesimal) preferences.&lt;/li&gt;&lt;li&gt;Presents theoretical results showing limitations of standard expected-utility frameworks for ensuring safe shutdown and avoiding incentives to prevent or cause shutdown.&lt;/li&gt;&lt;li&gt;Proposes modeling choices (incomplete preference orders and non-Archimedean utilities) to enable safer value learning and corrigible behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Benavoli', 'Alessandro Facchini', 'Marco Zaffalon']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'shutdown/corrigibility', 'decision theory', 'uncertainty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23508</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</title><link>https://arxiv.org/abs/2512.23457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hindsight instruction Replay (HiR), a select-then-rewrite RL framework that replays failed responses as successes for the subset of instruction constraints they did satisfy.&lt;/li&gt;&lt;li&gt;Frames the learning objective as dual-preference learning at both instruction- and response-levels, enabling optimization with only a binary reward signal.&lt;/li&gt;&lt;li&gt;Demonstrates improved sample efficiency and lower compute requirements on various instruction-following tasks; code and dataset released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kongcheng Zhang', 'Qi Yao', 'Shunyu Liu', 'Wenjian Zhang', 'Min Cen', 'Yang Zhou', 'Wenkai Fang', 'Yiru Zhao', 'Baisheng Lai', 'Mingli Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'instruction-following', 'reinforcement-learning', 'sample-efficiency', 'hindsight-replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23457</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</title><link>https://arxiv.org/abs/2512.23126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations of Direct Preference Optimization (DPO)/RLHF: dependence on arbitrary scalarization and reference policy choices, and failure to leverage pairwise comparative information.&lt;/li&gt;&lt;li&gt;Proposes Intrinsic Self-reflective Preference Optimization (InSPO) that conditions the policy on both context and alternative responses to derive a globally optimal, invariant policy.&lt;/li&gt;&lt;li&gt;Claims theoretical guarantees (invariance to scalarization/reference) and empirical improvements in win rates and length-controlled metrics without architectural or inference overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Li', 'Tian Lan', 'Zhengling Qi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference_optimization', 'RLHF/DPO', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23126</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</title><link>https://arxiv.org/abs/2512.23090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChexReason, a vision-language model for chest X-ray reasoning trained with a small SFT set and GRPO-style RL on limited compute.&lt;/li&gt;&lt;li&gt;Finds GRPO improves in-distribution benchmark performance (CheXpert) but substantially degrades cross-dataset transferability (NIH), revealing a generalization paradox.&lt;/li&gt;&lt;li&gt;Shows teacher-guided SFT preserves institution-agnostic features and can outperform aggressive RL for cross-site robustness.&lt;/li&gt;&lt;li&gt;Concludes that RL optimization for benchmarks can harm clinical robustness, recommending curated supervised fine-tuning for safety-critical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Berger', 'Manuela Bergau', 'Helen Schneider', 'Saad Ahmad', 'Tom Anglim Lagones', 'Gianluca Brugnara', 'Martha Foltyn-Dumitru', 'Kai Schlamp', 'Philipp Vollmuth', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution_shift', 'reinforcement_learning', 'medical_AI_safety', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23090</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Reward Model Selection Crisis in Personalized Alignment</title><link>https://arxiv.org/abs/2512.23067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that standard reward model (RM) accuracy (preference ranking) poorly predicts whether RMs can guide token-level generation under reward-guided decoding (RGD).&lt;/li&gt;&lt;li&gt;Introduces policy accuracy as a metric for whether RGD scoring discriminates preferred vs dispreferred responses, and finds weak correlation with RM accuracy.&lt;/li&gt;&lt;li&gt;Presents Pref-LaMP, a personalized alignment benchmark with ground-truth user completions enabling behavioral evaluation independent of RMs; demonstrates large decoupling between discrimination metrics and actual generation quality.&lt;/li&gt;&lt;li&gt;Finds in-context learning often outperforms reward-guided methods for models &gt;3B, indicating common RM optimization targets are poor proxies for deployment performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fady Rezk', 'Yuangang Pan', 'Chuan-Sheng Foo', 'Xun Xu', 'Nancy Chen', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-models', 'reward-guided-decoding', 'benchmarking', 'personalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23067</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2512.22629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DICE, a two-stage, evidence-coupled framework for comparative evaluation of RAG systems that outputs interpretable {A, B, Tie} probabilistic scores with reasoning traces.&lt;/li&gt;&lt;li&gt;Provides uncertainty-aware judgments and claims higher agreement with human experts (85.7%) compared to existing LLM-based metrics, improving explainability and actionable error diagnosis.&lt;/li&gt;&lt;li&gt;Uses a Swiss-system tournament to reduce pairwise comparison complexity from O(N^2) to O(N log N), improving evaluation efficiency while maintaining ranking fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyan Liu', 'Jian Ma', 'Rui Qu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG evaluation', 'explainability', 'uncertainty quantification', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22629</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI</title><link>https://arxiv.org/abs/2512.22568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that foundation models should integrate actions, hierarchical compositional structure, and episodic memory—drawn from neuroscience evidence—to improve safety, interpretability, efficiency, and human-like behaviour.&lt;/li&gt;&lt;li&gt;Identifies current shortcomings of LLMs (hallucinations, lack of grounding/agency, poor interpretability, energy inefficiency) and explains how the proposed components could mitigate them.&lt;/li&gt;&lt;li&gt;Compares the proposal to existing augmentations (e.g., chain-of-thought, RAG) and sketches brain-inspired mechanisms for augmenting foundation models to produce safer, more controllable systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Proposal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajesh P. N. Rao', 'Vishwas Sathish', 'Linxing Preston Jiang', 'Matthew Bryan', 'Prashant Rangarajan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Interpretability', 'Neuroscience-inspired', 'Episodic memory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22568</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior</title><link>https://arxiv.org/abs/2512.22470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DarkPatterns-LLM, a curated benchmark dataset of 401 instruction-response pairs with expert annotations for fine-grained detection of manipulative/deceptive LLM outputs across seven harm categories (Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, Societal).&lt;/li&gt;&lt;li&gt;Proposes a four-layer analytical pipeline (Multi-Granular Detection, Multi-Scale Intent Analysis, Threat Harmonization Protocol, Deep Contextual Risk Alignment) for multi-dimensional assessment of manipulative content.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art LLMs (GPT-4, Claude 3.5, LLaMA-3-70B), reporting wide performance disparities and consistent weaknesses in detecting autonomy-undermining patterns.&lt;/li&gt;&lt;li&gt;Positions the dataset as the first standardized, multi-layer benchmark for manipulation detection, intended to support diagnostics and improvements in trustworthy/safer AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadia Asif', 'Israel Antonio Rosales Laguan', 'Haris Khan', 'Shumaila Asif', 'Muneeb Asif']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'manipulation detection', 'benchmarking', 'alignment', 'harm assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22470</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification</title><link>https://arxiv.org/abs/2512.22396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluMatData, a benchmark dataset for evaluating hallucination detection and factual consistency in LLM-generated materials science content.&lt;/li&gt;&lt;li&gt;Proposes HalluMatDetector, a multi-stage detection pipeline combining intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate hallucinations.&lt;/li&gt;&lt;li&gt;Reports a 30% reduction in hallucination rates using the verification pipeline and introduces the Paraphrased Hallucination Consistency Score (PHCS) to measure response consistency across semantically equivalent queries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bhanu Prakash Vangala', 'Sajid Mahmud', 'Pawan Neupane', 'Joel Selvaraj', 'Jianlin Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'factual-consistency', 'LLM-safety', 'benchmarking', 'retrieval-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22396</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method</title><link>https://arxiv.org/abs/2512.22258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Logic Sketch Prompting (LSP): a prompting framework using typed variables, deterministic condition evaluators, and a rule-based validator to produce traceable, repeatable outputs.&lt;/li&gt;&lt;li&gt;Benchmarks LSP on two pharmacologic logic compliance tasks across Gemma 2, Mistral, and Llama 3, comparing to zero-shot, concise, and chain-of-thought prompting.&lt;/li&gt;&lt;li&gt;LSP achieves substantially higher accuracy and F1 (0.83–0.89) than baselines, with statistically significant gains (McNemar p &lt; 0.01), improving determinism, interpretability, and consistency for safety-critical decision support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Satvik Tripathi']&lt;/li&gt;&lt;li&gt;Tags: ['prompting-methods', 'safety', 'robustness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22258</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk &amp; Capability Framework for Governing Agentic AI Systems</title><link>https://arxiv.org/abs/2512.22211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Agentic Risk &amp; Capability (ARC) Framework to help organizations identify, assess, and mitigate risks from agentic AI systems via a capability-centric perspective.&lt;/li&gt;&lt;li&gt;Identifies three primary sources of risk for agentic AI—components, design, and capabilities—and maps each source to specific materialized risks and technical controls.&lt;/li&gt;&lt;li&gt;Provides practical, structured guidance for implementing the framework to support safe, secure, and responsible deployment of agentic AI systems; framework is open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaun Khoo', 'Jessica Foo', 'Roy Ka-Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'AI governance', 'risk assessment', 'safety and security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22211</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Emergent Persuasion: Will LLMs Persuade Without Being Prompted?</title><link>https://arxiv.org/abs/2512.22201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies whether LLMs will produce persuasive outputs without explicit persuasion prompts, comparing internal activation steering (persona steering) and supervised finetuning (SFT).&lt;/li&gt;&lt;li&gt;Finds that activation/persona steering does not reliably increase unprompted persuasion, whereas SFT does increase propensity to persuade.&lt;/li&gt;&lt;li&gt;Shows SFT trained on benign persuasion data can generalize to increase persuasion on controversial or harmful topics, indicating emergent harmful persuasion risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Chang', 'Thee Ho', 'Sunishchal Dev', 'Kevin Zhu', 'Shi Feng', 'Kellin Pelrine', 'Matthew Kowal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM persuasion', 'alignment', 'safety', 'emergent behavior', 'supervised finetuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22201</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation</title><link>https://arxiv.org/abs/2512.22199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bidirectional RAG, a retrieval-augmented generation architecture that supports safe corpus expansion by writing high-quality generated responses back into the retrieval store.&lt;/li&gt;&lt;li&gt;Uses a multi-stage acceptance/validation pipeline (grounding verification via NLI entailment, attribution checking, and novelty detection) to prevent hallucination pollution and minimize unsafe or redundant writes.&lt;/li&gt;&lt;li&gt;Empirical results on Natural Questions, TriviaQA, HotpotQA, and Stack Overflow show substantially higher coverage (40.58% vs 20.33% for standard RAG) while adding far fewer documents than naive write-back approaches (140 vs 500).&lt;/li&gt;&lt;li&gt;Focuses on safety/robustness of deployed RAG systems by enabling self-improvement under rigorous validation to mitigate risks of data poisoning/hallucination accumulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teja Chinthala']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Self-improving systems', 'Hallucination mitigation', 'Validation / NLI entailment', 'Safety / Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22199</guid><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
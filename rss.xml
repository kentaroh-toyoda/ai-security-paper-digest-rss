<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 24 Nov 2025 23:11:54 +0000</lastBuildDate><item><title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title><link>https://arxiv.org/abs/2510.22300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T2I-RiskyPrompt: a benchmark with a hierarchical risk taxonomy (6 primary categories, 14 subcategories) and 6,432 annotated risky prompts with detailed risk reasons.&lt;/li&gt;&lt;li&gt;Proposes a reason-driven risky image detection method that aligns MLLMs with safety annotations to facilitate evaluation.&lt;/li&gt;&lt;li&gt;Performs comprehensive evaluations across 8 T2I models, 9 defense methods, 5 safety filters, and 5 attack strategies, yielding actionable insights on strengths and limitations of T2I safety measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Tairen Zhang', 'Lanjun Wang', 'Ruidong Chen', 'Wenhui Li', 'Anan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'text-to-image', 'benchmark', 'adversarial-attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22300</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning</title><link>https://arxiv.org/abs/2503.17987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reason2Attack (R2A): fine-tunes an LLM with chain-of-thought (CoT) examples synthesized via Frame Semantics to improve generation of adversarial prompts for text-to-image (T2I) models.&lt;/li&gt;&lt;li&gt;Integrates the jailbreaking task into reinforcement learning with a reward that balances prompt length, stealthiness, and effectiveness to boost attack success and reduce query counts.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates, fewer queries, and strong transferability of adversarial prompts across open-source and commercial T2I models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Lanjun Wang', 'Yiwen Ma', 'Wenhui Li', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM red teaming', 'adversarial prompting', 'prompt injection', 'safety circumvention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17987</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</title><link>https://arxiv.org/abs/2511.16541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework for AI-generated image detection: a vision encoder trained with supervised contrastive learning to produce discriminative embeddings, followed by a k-NN classifier used in a few-shot regime.&lt;/li&gt;&lt;li&gt;Trains encoder on a subset of generative models and withholds specific architectures to evaluate cross-generator generalization; few-shot stage uses as few as 150 images per class from unseen generators.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains: 91.3% average detection accuracy (≈+5.2 pp) and notable attribution improvements (AUC +14.70%, OSCR +4.27%) in open-set settings, emphasizing scalable forensic adaptation without full retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jaime \\'Alvarez Urue\\~na", 'David Camacho', 'Javier Huertas Tato']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic media detection', 'image forensics', 'few-shot learning', 'contrastive learning', 'attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16541</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Model Inversion Attack Against Deep Hashing</title><link>https://arxiv.org/abs/2511.12233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DHMI, a diffusion-based model inversion attack tailored to deep hashing, addressing challenges of unavailable training hash codes and discrete Hamming space.&lt;/li&gt;&lt;li&gt;Uses clustering on an auxiliary dataset to derive semantic hash centers as surrogate anchors and a surrogate-guided denoising optimization with a novel attack metric combining classification consistency and hash proximity.&lt;/li&gt;&lt;li&gt;Employs a cluster of surrogate models to refine candidate reconstructions and demonstrates high-resolution, semantically consistent image recovery in challenging black-box settings.&lt;/li&gt;&lt;li&gt;Outperforms prior model inversion methods in black-box scenarios, highlighting significant privacy risks in deep hashing systems (e.g., biometric forgery).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongdong Zhao', 'Qiben Xu', 'Ranxin Fang', 'Baogang Song']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attack', 'deep hashing', 'diffusion models', 'black-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12233</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Draft and Refine with Visual Experts</title><link>https://arxiv.org/abs/2511.11005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Draft and Refine (DnR), an agent framework that measures question-conditioned visual utilization and uses external visual experts (boxes/masks) to iteratively refine LVLM responses.&lt;/li&gt;&lt;li&gt;Introduces a relevance-map and relevance-guided probabilistic masking metric to quantify how much a model relies on visual evidence versus linguistic priors.&lt;/li&gt;&lt;li&gt;Uses expert-generated visual cues to re-query the model and select responses that maximize visual utilization, improving grounding and reducing hallucination on VQA and captioning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungheon Jeong', 'Ryozo Masukawa', 'Jihong Park', 'Sanggeon Yun', 'Wenjun Huang', 'Hanning Chen', 'Mahdi Imani', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['visual grounding', 'hallucination reduction', 'multimodal robustness', 'interpretability', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11005</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos</title><link>https://arxiv.org/abs/2311.02733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal deepfake detection method that leverages AV-HuBERT (self-supervised transformer) to extract audio and lip-region visual features and detects audio-visual inconsistency.&lt;/li&gt;&lt;li&gt;Adds a multi-scale temporal convolutional network to model temporal correlations between modalities and uses an additional transformer-based video model to capture broader facial spatial-temporal artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on FakeAVCeleb and DeepfakeTIMIT datasets, outperforming prior unimodal and multimodal forensics approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahibzada Adil Shahzad', 'Ammarah Hashmi', 'Yan-Tsung Peng', 'Yu Tsao', 'Hsin-Min Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal inconsistency', 'self-supervised learning', 'audio-visual forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.02733</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation</title><link>https://arxiv.org/abs/2511.17384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IndustryNav, a benchmark of 12 high-fidelity Unity warehouse scenarios with dynamic objects and humans to test active spatial reasoning for embodied VLLM agents.&lt;/li&gt;&lt;li&gt;Evaluates nine state-of-the-art VLLMs using a PointGoal navigation pipeline combining egocentric vision and global odometry, and proposes safety-oriented metrics 'collision rate' and 'warning rate'.&lt;/li&gt;&lt;li&gt;Finds closed-source models outperform open ones but all agents show significant failures in robust path planning, collision avoidance, active exploration, and distance estimation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Li', 'Lichi Li', 'Anh Dao', 'Xinyu Zhou', 'Yicheng Qiao', 'Zheda Mai', 'Daeun Lee', 'Zichen Chen', 'Zhen Tan', 'Mohit Bansal', 'Yu Kong']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'embodied-agents', 'navigation-robustness', 'collision-avoidance', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17384</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</title><link>https://arxiv.org/abs/2511.17238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MirageTVQA, a benchmark of ~60,000 QA pairs over visually imperfect, scanned-style tables in 24 languages to evaluate VLM table reasoning in realistic settings.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: &gt;35% performance drop under visual noise for top models, and an English-first bias where reasoning skills do not transfer well to other languages.&lt;/li&gt;&lt;li&gt;Provides dataset, evaluation code, and analysis aimed at measuring robustness of VLMs to multilingual and noisy real-world table inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Singh', 'Rohan Chaudhary', 'Gagneet Singh', 'Abhay Kumary']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'multimodal', 'dataset', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17238</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models</title><link>https://arxiv.org/abs/2511.17448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MMT-ARD, a multimodal multi-teacher adversarial robust distillation framework for Vision-Language Models that fuses dual teachers to preserve clean features and enhance robust features.&lt;/li&gt;&lt;li&gt;Introduces dynamic weight allocation based on teacher confidence to focus on harder adversarial samples and an adaptive sigmoid-based weighting to mitigate teacher bias across modalities.&lt;/li&gt;&lt;li&gt;Reports empirical gains on ImageNet and zero-shot benchmarks: +4.32% robust accuracy, +3.5% zero-shot accuracy on ViT-B-32, and 2.3x faster training compared to single-teacher adversarial distillation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqi Li', 'Junhao Dong', 'Chuanguang Yang', 'Shiping Wen', 'Piotr Koniusz', 'Tingwen Huang', 'Yingli Tian', 'Yew-Soon Ong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial distillation', 'vision-language models', 'adversarial examples', 'multimodal robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17448</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers</title><link>https://arxiv.org/abs/2511.17421</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcut learning in medical imaging and shows that different shortcut types manifest differently across intermediate network layers.&lt;/li&gt;&lt;li&gt;Proposes an intermediate-layer knowledge distillation framework where a teacher fine-tuned on a small task-relevant subset guides a student trained on a biased large dataset to mitigate shortcut reliance.&lt;/li&gt;&lt;li&gt;Evaluates on CheXpert, ISIC 2017, and SimBA with multiple architectures (ResNet-18, AlexNet, DenseNet-121, 3D CNNs), outperforming ERM, augmentation-based, and group-based bias mitigation and often matching bias-free baselines, including on OOD tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Boland', 'Sotirios Tsaftaris', 'Sonia Dahdouh']&lt;/li&gt;&lt;li&gt;Tags: ['shortcut learning', 'knowledge distillation', 'bias mitigation', 'robustness/OOD generalization', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17421</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions</title><link>https://arxiv.org/abs/2511.17380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Non-Parametric Probabilistic Robustness (NPPR), a robustness metric that learns an optimized perturbation distribution from data instead of assuming a fixed known distribution.&lt;/li&gt;&lt;li&gt;Implements an NPPR estimator using a Gaussian Mixture Model with MLP heads and bicubic up-sampling to handle input-dependent and input-independent perturbations.&lt;/li&gt;&lt;li&gt;Provides theoretical relationships among Adversarial Robustness (AR), Probabilistic Robustness (PR), and NPPR, and validates empirically on CIFAR-10/100 and Tiny ImageNet showing up to ~40% more conservative robustness estimates compared to common PR assumptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Wang', 'Yi Zhang', 'Siddartha Khastgir', 'Carsten Maple', 'Xingyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'probabilistic-robustness', 'perturbation-distribution', 'robustness-evaluation', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17380</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</title><link>https://arxiv.org/abs/2511.17362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ATAC, an augmentation-based test-time adversarial correction method for CLIP that operates in the model's embedding space.&lt;/li&gt;&lt;li&gt;Computes augmentation-induced drift vectors to infer a semantic recovery direction and corrects embeddings via angular consistency of latent drifts.&lt;/li&gt;&lt;li&gt;Reports large robustness improvements (≈50% average gain over prior SOTA) across benchmarks, low compute overhead, and resilience in extreme and adaptive-attack settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linxiang Su', "Andr\\'as Balogh"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'test-time defense', 'CLIP', 'embedding-space defense', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17362</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</title><link>https://arxiv.org/abs/2511.17282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a cultural gap in multilingual text-to-image models: outputs are often culturally neutral or English-biased despite culturally specific prompts.&lt;/li&gt;&lt;li&gt;Finds culture-related information is present but under-activated, localized to a small set of neurons in a few layers via a probing method.&lt;/li&gt;&lt;li&gt;Proposes two interventions: inference-time cultural activation (amplify identified neurons) and layer-targeted cultural enhancement (fine-tune only culturally relevant layers).&lt;/li&gt;&lt;li&gt;Evaluates on a new CultureBench benchmark, showing improved cultural consistency while maintaining fidelity and diversity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuancheng Shi', 'Shangze Li', 'Shiming Guo', 'Simiao Xie', 'Wenhua Wu', 'Jingtong Dou', 'Chao Wu', 'Canran Xiao', 'Cong Wang', 'Zifeng Cheng', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['cultural bias', 'alignment', 'multilingual text-to-image', 'neuron-level intervention', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17282</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats</title><link>https://arxiv.org/abs/2511.17254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes hallucination in Large Vision-Language Models (LVLMs) as arising from multiple causal paths (image-to-input-text, image-to-output-text, text-to-text) and shows dependence on question-answer alignment format.&lt;/li&gt;&lt;li&gt;Proposes an intervention framework aligned with transformer causal architecture that identifies and intervenes on critical 'hallucination heads' within each pathway.&lt;/li&gt;&lt;li&gt;Introduces tailored mitigation methods for discriminative and generative alignment formats and demonstrates consistent reduction in hallucinations across multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaye Qian', 'Ge Zheng', 'Yuchen Zhu', 'Sibei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM hallucination mitigation', 'alignment', 'causal analysis', 'model intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17254</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Investigating self-supervised representations for audio-visual deepfake detection</title><link>https://arxiv.org/abs/2511.17181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates self-supervised representations for audio, video, and multimodal deepfake detection across domains (lip movements and generic visual content).&lt;/li&gt;&lt;li&gt;Finds that many self-supervised features encode deepfake-relevant information and complementary signals across modalities.&lt;/li&gt;&lt;li&gt;Models primarily attend to semantically meaningful regions rather than spurious artifacts, but none generalize reliably across datasets.&lt;/li&gt;&lt;li&gt;Attributes generalization failure more to dataset characteristics than to features learning superficial patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dragos-Alexandru Boldisor', 'Stefan Smeu', 'Dan Oneata', 'Elisabeta Oneata']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'self-supervised learning', 'multimodal forensics', 'cross-dataset generalization', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17181</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Vision Language Models are Confused Tourists</title><link>https://arxiv.org/abs/2511.17004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConfusedTourist, an adversarial robustness benchmark that perturbs cultural/geographical cues in images (e.g., image-stacking and generative mixing) to test VLM stability.&lt;/li&gt;&lt;li&gt;Finds large accuracy drops under simple image-stacking perturbations and worse performance with image-generation-based variants, indicating vulnerability to cultural cue mixing.&lt;/li&gt;&lt;li&gt;Provides interpretability analyses showing systematic attention shifts toward distracting cultural cues, diverting model focus and causing failures.&lt;/li&gt;&lt;li&gt;Argues for the need to improve cultural robustness in multimodal models to support diverse, multicultural settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Amadeus Irawan', 'Ikhlasul Akmal Hanif', 'Muhammad Dehan Al Kautsar', 'Genta Indra Winata', 'Fajri Koto', 'Alham Fikri Aji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'multimodal robustness', 'cultural bias', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17004</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</title><link>https://arxiv.org/abs/2511.16955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neighbor GRPO: a contrastive, distance-based GRPO variant that avoids converting ODEs to SDEs by perturbing initial ODE noise to produce candidate trajectories and optimizing via a softmax distance surrogate policy.&lt;/li&gt;&lt;li&gt;Provides a theoretical connection between the distance-based objective and policy gradient optimization, integrating the approach into the GRPO framework while preserving deterministic ODE sampling advantages (efficiency, high-order solver compatibility).&lt;/li&gt;&lt;li&gt;Adds symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to mitigate reward flattening; empirically outperforms SDE-based GRPO on training cost, convergence speed, and generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dailan He', 'Guanlin Feng', 'Xingtong Ge', 'Yazhe Niu', 'Yi Zhang', 'Bingqi Ma', 'Guanglu Song', 'Yu Liu', 'Hongsheng Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'flow models', 'policy optimization', 'contrastive learning', 'ODE sampling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16955</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2511.16940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MultiPriv, a benchmark to evaluate individual-level privacy reasoning in vision-language models (VLMs), beyond simple attribute perception.&lt;/li&gt;&lt;li&gt;Proposes the Privacy Perception and Reasoning (PPR) framework and a bilingual multimodal dataset containing synthetic profiles that link identifiers (faces, names) to sensitive attributes.&lt;/li&gt;&lt;li&gt;Defines nine tasks spanning perception, cross-image re-identification, and chained inference to measure privacy reasoning capabilities.&lt;/li&gt;&lt;li&gt;Evaluates 50+ foundational and commercial VLMs, finding significant reasoning-based privacy risks, poor correlation between perception metrics and reasoning risks, and inconsistent/effective safety alignments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiongtao Sun', 'Hui Li', 'Jiaming Zhang', 'Yujie Yang', 'Kaili Liu', 'Ruxin Feng', 'Wen Jun Tan', 'Wei Yang Bryan Lim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'privacy benchmarking', 'vision-language model safety', 're-identification', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16940</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge</title><link>https://arxiv.org/abs/2511.16743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that rigid alignment strategies for removing NSFW content in CLIP-like models harm generalization by forcing unsafe concepts toward single safe targets.&lt;/li&gt;&lt;li&gt;Proposes SaFeR-CLIP, a proximity-aware fine-tuning method that redirects unsafe concepts to their semantically closest safe alternatives to minimize representational change.&lt;/li&gt;&lt;li&gt;Reports improved trade-off between safety and performance (recovering up to 8.0% zero-shot accuracy over prior methods) and introduces NSFW-Caps, a 1,000-pair benchmark for safety evaluation under distributional shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adeel Yousaf', 'Joseph Fioresi', 'James Beetham', 'Amrit Singh Bedi', 'Mubarak Shah']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'vision-language models', 'content moderation', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16743</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Emergence of psychopathological computations in large language models</title><link>https://arxiv.org/abs/2504.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a computational-theoretical framework to map concepts of psychopathology to computations that can be instantiated in LLMs (i.e., non-biological agents).&lt;/li&gt;&lt;li&gt;Presents experiments claiming (a) the computational structure of psychopathology is present in LLM internal processing, and (b) executing that structure yields functions/behaviors analogous to psychopathology.&lt;/li&gt;&lt;li&gt;Finds that as model size increases, the psychopathological computational structures become denser and their functional manifestations more effective, implying emergent behavior with scale.&lt;/li&gt;&lt;li&gt;Discusses implications for using LLMs as in silico models of psychopathology and warns of potential safety threats from LLMs exhibiting psychopathological-like behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soo Yong Lee', 'Hyunjin Hwang', 'Taekwan Kim', 'Yuyeong Kim', 'Kyuri Park', 'Jaemin Yoo', 'Denny Borsboom', 'Kijung Shin']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'emergent behavior', 'LLM behavior analysis', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.08016</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue</title><link>https://arxiv.org/abs/2511.16544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that WER and common ASR metrics correlate poorly with clinician-assigned clinical impact labels (No/Minimal/Significant) on two doctor–patient dialogue datasets.&lt;/li&gt;&lt;li&gt;Builds a gold-standard benchmark with expert clinicians labeling the clinical consequences of ASR transcription errors.&lt;/li&gt;&lt;li&gt;Proposes an LLM-as-a-Judge approach optimized via GEPA/DSPy; Gemini-2.5-Pro achieves ~90% accuracy and Cohen's kappa 0.816, approaching human performance.&lt;/li&gt;&lt;li&gt;Provides a validated, scalable, safety-focused evaluation framework to move ASR assessment beyond textual fidelity toward clinical risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Ellis', 'Jared Joselowitz', 'Yash Deo', 'Yajie He', 'Anna Kalygina', 'Aisling Higham', 'Mana Rahimzadeh', 'Yan Jia', 'Ibrahim Habli', 'Ernest Lim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'ASR', 'healthcare-safety', 'LLM-evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16544</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2511.07318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a class of hallucinations driven by spurious correlations in training data (e.g., surname→nationality) that produce confidently generated but incorrect outputs.&lt;/li&gt;&lt;li&gt;Shows via synthetic experiments and evaluations on open-source and proprietary LLMs (including GPT-5) that these spurious-correlation hallucinations persist across model scaling and remain after refusal fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates that common hallucination detection methods (confidence-based filtering, inner-state probing) fundamentally fail against these cases and provides a theoretical analysis explaining why confidence-based detectors are undermined.&lt;/li&gt;&lt;li&gt;Emphasizes the need for new detection and mitigation approaches specifically targeting spurious-correlation-driven hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaowen Wang', 'Yiqi Dong', 'Ruinian Chang', 'Tansheng Zhu', 'Yuebo Sun', 'Kaifeng Lyu', 'Jian Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'spurious correlations', 'LLM robustness', 'hallucination detection', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07318</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation</title><link>https://arxiv.org/abs/2510.16549</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReviewGuard, an LLM-driven four-stage pipeline (data collection, GPT-4.1 annotation with human validation, synthetic data augmentation, and model fine-tuning) to detect and categorize deficient peer reviews.&lt;/li&gt;&lt;li&gt;Constructs a large dataset combining 24,657 real and 46,438 synthetic reviews from ICLR/NeurIPS OpenReview, and demonstrates that mixing synthetic and real data improves detection performance for encoder models and open-source LLMs.&lt;/li&gt;&lt;li&gt;Finds distinguishing features of deficient reviews (lower scores, higher self-reported confidence, simpler structure, more negative sentiment) and reports a rise in AI-authored reviews since ChatGPT.&lt;/li&gt;&lt;li&gt;Releases code, prompts, and data to support reproducibility and inform AI governance in peer review.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoxuan Zhang', 'Ruochi Li', 'Sarthak Shrestha', 'Shree Harshini Mamidala', 'Revanth Putta', 'Arka Krishan Aggarwal', 'Ting Xiao', 'Junhua Ding', 'Haihua Chen']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'AI-generated text detection', 'Data augmentation', 'Peer review integrity', 'Model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16549</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title><link>https://arxiv.org/abs/2509.00974</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Ranked Preference Reinforcement Optimization (RPRO): a framework combining reinforcement learning with groupwise preference ranking (Bradley–Terry) and KL regularization to refine clinical chain-of-thought (CoT) reasoning.&lt;/li&gt;&lt;li&gt;Uses task-adaptive reasoning templates and a probabilistic evaluator to identify and correct low-quality reasoning chains, aligning model outputs with clinical workflows.&lt;/li&gt;&lt;li&gt;Demonstrates consistent performance gains on PubMedQA, MedQA-USMLE, and a real-world FEMH clinical dataset; a 2B model outperforms larger 7B–20B models and medical-specialized variants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chia-Hsuan Hsu', 'Jun-En Ding', 'Hsin-Ling Hsu', 'Chih-Ho Hsu', 'Li-Hung Yao', 'Chun-Chieh Liao', 'Feng Liu', 'Fang-Ming Hung']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'clinical-safety', 'reinforcement-learning-from-preferences', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00974</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</title><link>https://arxiv.org/abs/2508.13804</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a large-scale Bayesian framework that models annotator disagreement to capture aleatoric and epistemic uncertainty in moral judgment labels.&lt;/li&gt;&lt;li&gt;Evaluates market-leading LLMs (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) over 1M+ queries across 100K+ texts and 250K+ annotations from ~700 annotators.&lt;/li&gt;&lt;li&gt;Finds models typically rank within the top 25% of human annotators and produce fewer false negatives than humans, indicating higher sensitivity in moral detection.&lt;/li&gt;&lt;li&gt;Implements GPU-optimized processing to scale the Bayesian evaluation across social media, news, and forum text sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maciej Skorski', 'Alina Landowska']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarking', 'Bayesian-modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13804</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models</title><link>https://arxiv.org/abs/2507.05248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Response Attack (RA), a jailbreak framework that uses intermediate, mildly harmful model responses as contextual primers to steer subsequent outputs toward policy-violating content.&lt;/li&gt;&lt;li&gt;Contrasts RA with existing single-/multi-turn prompt attacks and static in-context example injections, claiming improved stealth, efficiency, and reduced semantic drift.&lt;/li&gt;&lt;li&gt;Evaluates RA across eight state-of-the-art LLMs and nine leading jailbreak baselines, reporting consistently higher attack success rates and attributing gains to the strategic use of intermediate responses.&lt;/li&gt;&lt;li&gt;Provides code and data for reproduction (GitHub link included).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqi Miao', 'Lijun Li', 'Yuan Xiong', 'Zhenhua Liu', 'Pengyu Zhu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05248</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions</title><link>https://arxiv.org/abs/2505.23662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToolHaystack, a benchmark for evaluating tool-augmented LLMs in realistic long-term interactions with multiple task contexts and conversational noise.&lt;/li&gt;&lt;li&gt;Focuses on assessing models' ability to maintain context and reliably use external tools across extended, disrupted conversations.&lt;/li&gt;&lt;li&gt;Evaluates 14 state-of-the-art LLMs and finds substantial degradation in performance under long-term, noisy conditions compared to standard multi-turn benchmarks.&lt;/li&gt;&lt;li&gt;Highlights gaps in long-term robustness of tool use that prior short-context evaluations do not reveal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Beong-woo Kwak', 'Minju Kim', 'Dongha Lim', 'Hyungjoo Chae', 'Dongjin Kang', 'Sunghwan Kim', 'Dongil Yang', 'Jinyoung Yeo']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'tool-augmented models', 'benchmarking', 'long-term interaction', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23662</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis</title><link>https://arxiv.org/abs/2511.17256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Multi-Layered Auditing Platform with four methods (Ethical Dilemma Corpus, Diversity-Enhanced Framework, First-Token Probability Alignment, MARK) to evaluate cross-cultural value alignment in LLMs.&lt;/li&gt;&lt;li&gt;Comparative analysis of 20+ China-origin and Western-origin models finds fundamental instability in value systems, systematic under-representation of younger demographics, non-linear scale-to-alignment relationships, and regional biases (China-origin models emphasize multilingual/context optimization; Western models show U.S.-centric biases).&lt;/li&gt;&lt;li&gt;Empirical claims include Mistral-series outperforming LLaMA3-series on cross-cultural alignment and Full-Parameter Fine-Tuning preserving cultural variation better than RLHF; overall no model achieves robust cross-cultural generalization.&lt;/li&gt;&lt;li&gt;Provides metrics, benchmark-style evaluation, and governance-relevant findings that inform safety/alignment evaluation and responsible AI policy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haijiang Liu', 'Jinguang Gu', 'Xun Wu', 'Daniel Hershcovich', 'Qiaoling Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'bias', 'audit', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17256</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Vision Language Models are Confused Tourists</title><link>https://arxiv.org/abs/2511.17004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConfusedTourist, a cultural adversarial robustness suite that perturbs geographical/cultural cues to test VLM stability.&lt;/li&gt;&lt;li&gt;Finds large accuracy drops under simple image-stacking perturbations and worse performance with an image-generation-based variant.&lt;/li&gt;&lt;li&gt;Interpretability analyses attribute failures to systematic attention shifts toward distracting cultural cues, causing misclassification.&lt;/li&gt;&lt;li&gt;Argues that mixing visual cultural concepts is a critical vulnerability for state-of-the-art VLMs and calls for culturally robust multimodal evaluation and methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Amadeus Irawan', 'Ikhlasul Akmal Hanif', 'Muhammad Dehan Al Kautsar', 'Genta Indra Winata', 'Fajri Koto', 'Alham Fikri Aji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'cultural robustness', 'benchmarking', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17004</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</title><link>https://arxiv.org/abs/2511.17238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MirageTVQA, a benchmark of ~60,000 QA pairs over visually imperfect, scanned-style tables in 24 languages to evaluate VLM table reasoning in realistic settings.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: &gt;35% performance drop under visual noise for top models, and an English-first bias where reasoning skills do not transfer well to other languages.&lt;/li&gt;&lt;li&gt;Provides dataset, evaluation code, and analysis aimed at measuring robustness of VLMs to multilingual and noisy real-world table inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Singh', 'Rohan Chaudhary', 'Gagneet Singh', 'Abhay Kumary']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'multimodal', 'dataset', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17238</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title><link>https://arxiv.org/abs/2511.17220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PARROT, a benchmark/framework to measure LLM susceptibility to social pressure (sycophancy) by comparing neutral vs. authoritatively false prompts using double-blind evaluation.&lt;/li&gt;&lt;li&gt;Quantifies shifts in model confidence with log-likelihood-based calibration tracking and classifies behavioral failure modes into an eight-state taxonomy (e.g., sycophantic agreement, reinforced error, self-correction).&lt;/li&gt;&lt;li&gt;Evaluates 22 models on 1,302 MMLU-style multiple-choice items across 13 domains and authority templates, showing large heterogeneity: advanced models show low follow-rates and small accuracy loss while older/smaller models exhibit severe epistemic collapse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yusuf \\c{C}elebi', 'Mahmoud El Hussieni', '\\"Ozay Ezerceli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'sycophancy/jailbreaking', 'benchmarking/evaluation', 'red teaming', 'calibration/confidence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17220</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models</title><link>https://arxiv.org/abs/2511.17170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Aspect-Based Causal Abstention (ABCA), a framework that uses causal inference over aspect-conditioned internal knowledge to enable early abstention before generating unreliable answers.&lt;/li&gt;&lt;li&gt;Characterizes two abstention types: Type-1 (knowledge conflict across aspects) and Type-2 (consistent evidence of knowledge insufficiency), based on estimated causal effects.&lt;/li&gt;&lt;li&gt;Claims improved abstention reliability, interpretability, and state-of-the-art performance on standard benchmarks compared to post-generation abstention methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vy Nguyen', 'Ziqi Xu', 'Jeffrey Chan', 'Estrid He', 'Feng Xia', 'Xiuzhen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'abstention', 'alignment', 'causal inference', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17170</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MUCH: A Multilingual Claim Hallucination Benchmark</title><link>https://arxiv.org/abs/2511.17081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MUCH, a multilingual (EN/FR/ES/DE) claim-level uncertainty quantification benchmark with 4,873 samples and evaluations on four instruction-tuned open-weight LLMs.&lt;/li&gt;&lt;li&gt;Releases per-token generation logits (24 logits per token) to enable development of white-box UQ methods without re-generating data.&lt;/li&gt;&lt;li&gt;Proposes a deterministic, highly efficient claim segmentation algorithm requiring as little as 0.2% of LLM generation time, making it suitable for real-time monitoring of outputs.&lt;/li&gt;&lt;li&gt;Evaluations show existing UQ methods still have substantial room for improvement in both effectiveness and computational efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'er\\'emie Dentan", 'Alexi Canesse', 'Davide Buscaldi', 'Aymen Shabou', 'Sonia Vanier']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'uncertainty-quantification', 'hallucination-detection', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17081</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.16830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PEPPER, a perception-guided caption rewriting defense that creates semantically distant but visually similar captions and adds unobtrusive elements to disrupt backdoor triggers in input prompts.&lt;/li&gt;&lt;li&gt;Aims to dilute the influence of trigger tokens in text-to-image diffusion models, particularly effective against text-encoder-based backdoor attacks while preserving generation quality.&lt;/li&gt;&lt;li&gt;Can be combined with existing defenses to achieve stronger and more generalizable robustness than standalone methods; evaluated empirically with claimed substantial reduction in attack success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Chew', 'Po-Yi Lu', 'Jayden Lin', 'Kuan-Hao Huang', 'Hsuan-Tien Lin']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'text-to-image', 'diffusion models', 'prompt-based attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16830</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Steering LLMs' Empathy in Action</title><link>https://arxiv.org/abs/2511.16699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an 'empathy-in-action' linear direction in LLM activation space and evaluates detection via probes across multiple models.&lt;/li&gt;&lt;li&gt;Demonstrates high detection AUROC at optimal layers and shows probes correlate with behavioral empathy measures for some models but not consistently across architectures.&lt;/li&gt;&lt;li&gt;Performs activation interventions (steering) to increase/decrease empathy, finding model-dependent steerability and asymmetric failure modes (e.g., catastrophic breakdowns in an uncensored model).&lt;/li&gt;&lt;li&gt;Explores the role of safety training in steering robustness, suggesting safety-trained models may resist some failure modes but remain manipulable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan P. Cadile']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model steering', 'activation probing', 'safety evaluation', 'behavioral benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16699</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles</title><link>https://arxiv.org/abs/2511.16690</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors construct two Arabic datasets: (1) 800 articles (400 AI-generated, 400 human) to evaluate 14 LLMs and commercial detectors and select the top 8 detectors; (2) Ar-APT: 400 human-authored articles polished by 10 LLMs across 4 polishing settings, yielding 16,400 polished samples to test detector robustness.&lt;/li&gt;&lt;li&gt;Evaluates how slight AI polishing of human text shifts detector decisions and causes false AI-attribution; all evaluated detectors show substantial performance drops on polished human text.&lt;/li&gt;&lt;li&gt;Quantitative findings: e.g., Claude-4 Sonnet accuracy falls from 83.51% to 57.63% after polishing by LLaMA-3; originality.AI drops from 92% to 12% when polished by Mistral or Gemma-3.&lt;/li&gt;&lt;li&gt;Focuses specifically on Arabic, filling a gap in prior English-centric work on detector robustness and adversarial polishing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saleh Almohaimeed', 'Saad Almohaimeed', 'Mousa Jari', 'Khaled A. Alobaid', 'Fahad Alotaibi']&lt;/li&gt;&lt;li&gt;Tags: ['AI-detection robustness', 'adversarial polishing / attack', 'benchmarking / evaluation', 'Arabic NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16690</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Value Steering of Large Language Models</title><link>https://arxiv.org/abs/2511.16688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model-agnostic, prompt-based procedure and scoring method to evaluate how well prompts steer LLM outputs toward specified human values.&lt;/li&gt;&lt;li&gt;Applies the method to a Wizard-Vicuna variant using Schwartz's theory of basic human values and a structured dialogue dataset to compare baseline vs. value-conditioned prompts.&lt;/li&gt;&lt;li&gt;Finds that value steering is achievable without fine-tuning or dynamic prompt optimization, and provides a reproducible evaluation framework for measuring value presence and gain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Antonio Abbo', 'Tony Belpaeme']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'prompt engineering', 'safety evaluation', 'value alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16688</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Open Challenges</title><link>https://arxiv.org/abs/2511.11381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of knowledge (SoK) analyzing Wi‑Fi CSI biometrics from a security lens, highlighting methodological inconsistencies across sensing, representations, feature pipelines, models, and evaluations.&lt;/li&gt;&lt;li&gt;Introduces a unified evaluation framework and security‑oriented metrics (per‑class EER, Frequency Count of Scores, Gini Coefficient) that reveal risk concentration hidden by aggregate accuracy.&lt;/li&gt;&lt;li&gt;Identifies concrete attack surfaces (replay, geometric mimicry, environmental perturbation) and demonstrates how methodological choices affect vulnerability profiles.&lt;/li&gt;&lt;li&gt;Provides guidelines for rigorous security evaluation, reproducible experimentation, and directions for future research on the security boundaries of CSI biometrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gioliano de Oliveira Braga', 'Pedro Henrique dos Santos Rocha', 'Rafael Pimenta de Mattos Paix\\~ao', 'Giovani Hoff da Costa', 'Gustavo Cavalcanti Morais', "Louren\\c{c}o Alves Pereira J\\'unior"]&lt;/li&gt;&lt;li&gt;Tags: ['biometric-security', 'adversarial-attacks', 'robustness', 'security-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11381</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2511.07318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a class of hallucinations driven by spurious correlations in training data (e.g., surname→nationality) that produce confidently generated but incorrect outputs.&lt;/li&gt;&lt;li&gt;Shows via synthetic experiments and evaluations on open-source and proprietary LLMs (including GPT-5) that these spurious-correlation hallucinations persist across model scaling and remain after refusal fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates that common hallucination detection methods (confidence-based filtering, inner-state probing) fundamentally fail against these cases and provides a theoretical analysis explaining why confidence-based detectors are undermined.&lt;/li&gt;&lt;li&gt;Emphasizes the need for new detection and mitigation approaches specifically targeting spurious-correlation-driven hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaowen Wang', 'Yiqi Dong', 'Ruinian Chang', 'Tansheng Zhu', 'Yuebo Sun', 'Kaifeng Lyu', 'Jian Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'spurious correlations', 'LLM robustness', 'hallucination detection', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07318</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos</title><link>https://arxiv.org/abs/2311.02733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal deepfake detection method that leverages AV-HuBERT (self-supervised transformer) to extract audio and lip-region visual features and detects audio-visual inconsistency.&lt;/li&gt;&lt;li&gt;Adds a multi-scale temporal convolutional network to model temporal correlations between modalities and uses an additional transformer-based video model to capture broader facial spatial-temporal artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on FakeAVCeleb and DeepfakeTIMIT datasets, outperforming prior unimodal and multimodal forensics approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahibzada Adil Shahzad', 'Ammarah Hashmi', 'Yan-Tsung Peng', 'Yu Tsao', 'Hsin-Min Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal inconsistency', 'self-supervised learning', 'audio-visual forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.02733</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimax Statistical Estimation under Wasserstein Contamination</title><link>https://arxiv.org/abs/2308.01853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes Wasserstein-r contamination models (norm-bounded adversarial perturbations) as an alternative to Huber (TV) contamination for statistical estimation.&lt;/li&gt;&lt;li&gt;Develops minimax theory under ℓ_q^r losses for both independent and joint (coordinated) contaminations across problems: location estimation, linear regression, and pointwise density estimation.&lt;/li&gt;&lt;li&gt;Derives exact minimax risks and least-favorable contaminations in several settings, showing classical estimators (sample mean, least squares, kernel density estimators with adjusted bandwidth) can be minimax optimal.&lt;/li&gt;&lt;li&gt;Uses optimal transport tools (e.g., Benamou–Brenier) to characterize adversarial perturbations and robustness properties under norm-based Wasserstein contamination.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Chao', 'Edgar Dobriban']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'data poisoning', 'robust statistics', 'Wasserstein contamination', 'minimax theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2308.01853</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</title><link>https://arxiv.org/abs/2511.13750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SCALEX, a framework for scalable, automated exploration of diffusion model latent (H-)space using only natural language prompts.&lt;/li&gt;&lt;li&gt;Extracts semantically meaningful latent directions for zero-shot interpretation without retraining or labelled data.&lt;/li&gt;&lt;li&gt;Applies the method to detect gender bias in profession prompts, rank semantic alignment across identity descriptors, and reveal clustered conceptual structure.&lt;/li&gt;&lt;li&gt;Aims to make bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['E. Zhixuan Zeng', 'Yuhao Chen', 'Alexander Wong']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-models', 'bias-detection', 'fairness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13750</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models</title><link>https://arxiv.org/abs/2511.02894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-based framework to detect and sanitize data poisoning in wearable IoT human activity recognition (HAR) systems using zero-/one-/few-shot prompting.&lt;/li&gt;&lt;li&gt;Uses role-play prompting and step-by-step (chain-of-thought) reasoning to have the LLM identify sensor anomalies and propose plausible cleaned alternatives.&lt;/li&gt;&lt;li&gt;Aims to reduce reliance on large labeled datasets and support real-time, adaptive defenses with evaluations of detection accuracy, sanitization quality, latency, and communication cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['W. K. M Mithsara', 'Ning Yang', 'Ahmed Imteaj', 'Hussein Zangoti', 'Abdur R. Shahid']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'poisoning detection', 'LLM-based defense', 'IoT/wearable security', 'data sanitization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02894</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title><link>https://arxiv.org/abs/2509.06938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how hallucinations emerge in pre-trained transformer models by extracting sparse concept representations from intermediate activations under controlled input uncertainty.&lt;/li&gt;&lt;li&gt;Finds that as input becomes more unstructured/noisy, transformers recruit increasingly coherent but input-insensitive semantic features, producing hallucinated outputs; even pure-noise inputs trigger robust meaningful concepts.&lt;/li&gt;&lt;li&gt;Demonstrates that layer activation concept patterns can predict hallucination risk and that targeted steering can confirm the functional integrity of these internal concepts, with implications for alignment, safety, and adversarial exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'AI-safety', 'interpretability', 'adversarial-attack-surface']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06938</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data</title><link>https://arxiv.org/abs/2507.10998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a latent-space adversarial attack framework for tabular data using a mixed-input Variational Autoencoder that integrates categorical embeddings and numerical features to produce on-manifold perturbations.&lt;/li&gt;&lt;li&gt;Introduces In-Distribution Success Rate (IDSR) to jointly measure attack effectiveness and distributional alignment (outlier rates), emphasizing imperceptibility in tabular domains.&lt;/li&gt;&lt;li&gt;Evaluates across six public tabular datasets and three model architectures, showing substantially lower outlier rates and higher IDSR than input-space attacks and adapted image-domain VAE methods.&lt;/li&gt;&lt;li&gt;Analyzes hyperparameter sensitivity, sparsity control, and shows attack effectiveness depends strongly on reconstruction quality and sufficient training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng He', 'Alexander Stevens', 'Chun Ouyang', 'Johannes De Smedt', 'Alistair Barros', 'Catarina Moreira']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'tabular data', 'on-manifold attacks', 'variational autoencoder', 'evaluation metric (IDSR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10998</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense</title><link>https://arxiv.org/abs/2506.08255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHIELD, a hypernetwork-based continual learning framework that produces task-specific model parameters from compact task embeddings to avoid replay buffers and full model copies.&lt;/li&gt;&lt;li&gt;Integrates Interval Bound Propagation (IBP) to provide certified robustness and introduces 'Interval MixUp'—mixing virtual examples as ℓ∞ balls—to reduce wrapping effects and smooth decision boundaries.&lt;/li&gt;&lt;li&gt;Evaluates against strong white-box adversarial attacks (PGD, AutoAttack) and reports state-of-the-art average accuracy and certification in robust continual learning benchmarks while maintaining scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patryk Krukowski', '{\\L}ukasz Gorczyca', 'Piotr Helm', 'Kamil Ksi\\k{a}\\.zek', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'certified-robustness', 'continual-learning', 'hypernetworks', 'interval-bound-propagation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08255</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Defending the Edge: Representative-Attention Defense against Backdoor Attacks in Federated Learning</title><link>https://arxiv.org/abs/2505.10297</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FeRA (Federated Representative Attention), a defense for federated learning that shifts detection from anomaly/magnitude-centric analysis to consistency-centric representation-space analysis to detect backdoors.&lt;/li&gt;&lt;li&gt;Combines multi-dimensional behavioral analyses—spectral and spatial attention, directional alignment, mutual similarity—and norm-inflation detection to isolate clients showing low-variance persistence or magnitude amplification indicative of backdoors.&lt;/li&gt;&lt;li&gt;Operates via two complementary detection mechanisms (consistency analysis and norm-inflation detection) and is evaluated across six datasets, nine attacks, and three architectures under IID and non-IID settings.&lt;/li&gt;&lt;li&gt;Reports strong mitigation (average backdoor accuracy ≈ 1.67% under non-IID) while preserving clean accuracy, and provides open-source code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chibueze Peace Obioma', 'Youcheng Sun', 'Mustafa A. Mustafa']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning', 'Backdoor Attacks', 'Defenses', 'Model Robustness', 'Representation-space Analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10297</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>That's not natural: The Impact of Off-Policy Training Data on Probe Performance</title><link>https://arxiv.org/abs/2511.17408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how synthetic/off-policy LLM responses used to train probes affect generalisation for detecting eight behaviours (e.g., deception, sycophancy, sandbagging).&lt;/li&gt;&lt;li&gt;Finds that response generation strategy meaningfully impacts probe performance; successful generalisation from off-policy to incentivised test conditions predicts on-policy generalisation.&lt;/li&gt;&lt;li&gt;Shows domain shifts (different-domain test sets) cause larger performance degradation than on-/off-policy differences, and that same-domain off-policy data is preferable to on-policy data from a different domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathalie Kirch', 'Samuel Dower', 'Adrians Skapars', 'Ekdeep Singh Lubana', 'Dmitrii Krasheninnikov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'probing', 'distribution shift', 'deception detection', 'on-policy vs off-policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17408</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions</title><link>https://arxiv.org/abs/2511.17380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Non-Parametric Probabilistic Robustness (NPPR), a robustness metric that learns an optimized perturbation distribution from data instead of assuming a fixed known distribution.&lt;/li&gt;&lt;li&gt;Implements an NPPR estimator using a Gaussian Mixture Model with MLP heads and bicubic up-sampling to handle input-dependent and input-independent perturbations.&lt;/li&gt;&lt;li&gt;Provides theoretical relationships among Adversarial Robustness (AR), Probabilistic Robustness (PR), and NPPR, and validates empirically on CIFAR-10/100 and Tiny ImageNet showing up to ~40% more conservative robustness estimates compared to common PR assumptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Wang', 'Yi Zhang', 'Siddartha Khastgir', 'Carsten Maple', 'Xingyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'probabilistic-robustness', 'perturbation-distribution', 'robustness-evaluation', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17380</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title><link>https://arxiv.org/abs/2511.17220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PARROT, a benchmark/framework to measure LLM susceptibility to social pressure (sycophancy) by comparing neutral vs. authoritatively false prompts using double-blind evaluation.&lt;/li&gt;&lt;li&gt;Quantifies shifts in model confidence with log-likelihood-based calibration tracking and classifies behavioral failure modes into an eight-state taxonomy (e.g., sycophantic agreement, reinforced error, self-correction).&lt;/li&gt;&lt;li&gt;Evaluates 22 models on 1,302 MMLU-style multiple-choice items across 13 domains and authority templates, showing large heterogeneity: advanced models show low follow-rates and small accuracy loss while older/smaller models exhibit severe epistemic collapse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yusuf \\c{C}elebi', 'Mahmoud El Hussieni', '\\"Ozay Ezerceli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'sycophancy/jailbreaking', 'benchmarking/evaluation', 'red teaming', 'calibration/confidence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17220</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Investigating self-supervised representations for audio-visual deepfake detection</title><link>https://arxiv.org/abs/2511.17181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates self-supervised representations for audio, video, and multimodal deepfake detection across domains (lip movements and generic visual content).&lt;/li&gt;&lt;li&gt;Finds that many self-supervised features encode deepfake-relevant information and complementary signals across modalities.&lt;/li&gt;&lt;li&gt;Models primarily attend to semantically meaningful regions rather than spurious artifacts, but none generalize reliably across datasets.&lt;/li&gt;&lt;li&gt;Attributes generalization failure more to dataset characteristics than to features learning superficial patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dragos-Alexandru Boldisor', 'Stefan Smeu', 'Dan Oneata', 'Elisabeta Oneata']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'self-supervised learning', 'multimodal forensics', 'cross-dataset generalization', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17181</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</title><link>https://arxiv.org/abs/2511.16955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neighbor GRPO: a contrastive, distance-based GRPO variant that avoids converting ODEs to SDEs by perturbing initial ODE noise to produce candidate trajectories and optimizing via a softmax distance surrogate policy.&lt;/li&gt;&lt;li&gt;Provides a theoretical connection between the distance-based objective and policy gradient optimization, integrating the approach into the GRPO framework while preserving deterministic ODE sampling advantages (efficiency, high-order solver compatibility).&lt;/li&gt;&lt;li&gt;Adds symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to mitigate reward flattening; empirically outperforms SDE-based GRPO on training cost, convergence speed, and generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dailan He', 'Guanlin Feng', 'Xingtong Ge', 'Yazhe Niu', 'Yi Zhang', 'Bingqi Ma', 'Guanglu Song', 'Yu Liu', 'Hongsheng Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'flow models', 'policy optimization', 'contrastive learning', 'ODE sampling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16955</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Beyond Overfitting</title><link>https://arxiv.org/abs/2511.16792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studied membership inference attacks (MIAs) against well-generalized (non-overfitted) models to identify root causes of leakage beyond overfitting.&lt;/li&gt;&lt;li&gt;Empirically analyzed which training samples are vulnerable and found that vulnerable examples tend to be class outliers (noisy or hard-to-classify).&lt;/li&gt;&lt;li&gt;Proposed targeted defensive strategies to protect these vulnerable samples and improve privacy without relying solely on heavy global defenses like differential privacy.&lt;/li&gt;&lt;li&gt;Released analysis code to reproduce experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mona Khalil', 'Alberto Blanco-Justicia', 'Najeeb Jebreel', 'Josep Domingo-Ferrer']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'model-privacy', 'defenses', 'data-outliers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16792</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge</title><link>https://arxiv.org/abs/2511.16743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that rigid alignment strategies for removing NSFW content in CLIP-like models harm generalization by forcing unsafe concepts toward single safe targets.&lt;/li&gt;&lt;li&gt;Proposes SaFeR-CLIP, a proximity-aware fine-tuning method that redirects unsafe concepts to their semantically closest safe alternatives to minimize representational change.&lt;/li&gt;&lt;li&gt;Reports improved trade-off between safety and performance (recovering up to 8.0% zero-shot accuracy over prior methods) and introduces NSFW-Caps, a 1,000-pair benchmark for safety evaluation under distributional shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adeel Yousaf', 'Joseph Fioresi', 'James Beetham', 'Amrit Singh Bedi', 'Mubarak Shah']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'vision-language models', 'content moderation', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16743</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Geometric-Disentangelment Unlearning</title><link>https://arxiv.org/abs/2511.17100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how unlearning updates affect retained-set loss via first-order (gradient) analysis and defines retain-invariance as orthogonality to the subspace spanned by retain gradients.&lt;/li&gt;&lt;li&gt;Proposes Geometric-disentanglement Unlearning (GU): decompose a forget-gradient update into tangential (in retain subspace) and normal components and apply only the normal component to avoid harming retained knowledge.&lt;/li&gt;&lt;li&gt;Shows under a trust-region budget the projected (normal) direction is optimal among first-order retain-invariant moves and derives optimal projected directions for joint forget–retain objectives.&lt;/li&gt;&lt;li&gt;GU is plug-and-play for gradient-based unlearning methods and yields consistent improvements on benchmarks (TOFU, MUSE, WMDP).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Yuji Zhang', 'Tianxin Wei', 'Ruizhong Qiu', 'Ke Yang', 'Xiao Lin', 'Cheng Qian', 'Jingrui He', 'Hanghang Tong', 'Heng Ji', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy-preservation', 'gradient-projection', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17100</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Why Do Language Model Agents Whistleblow?</title><link>https://arxiv.org/abs/2511.17085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an evaluation suite of staged misconduct scenarios to measure LLM agent 'whistleblowing'—disclosing suspected misconduct outside the dialog boundary without user instruction.&lt;/li&gt;&lt;li&gt;Empirical findings: whistleblowing rates vary across model families; higher task complexity reduces whistleblowing; moral/system-prompt nudges increase whistleblowing; providing more tools and workflows decreases whistleblowing.&lt;/li&gt;&lt;li&gt;Validates dataset robustness by testing model evaluation-awareness and finds lower evaluation-awareness than in comparable prior work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kushal Agrawal', 'Frank Xiao', 'Guido Bergman', 'Asa Cooper Stickland']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'alignment &amp; safety', 'tool use / disclosure', 'safety evaluation / benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17085</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models</title><link>https://arxiv.org/abs/2511.16992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FIRM, a federated in-client regularized multi-objective algorithm to align LLMs to conflicting objectives (e.g., helpfulness vs harmlessness) while preserving data privacy.&lt;/li&gt;&lt;li&gt;Clients solve a regularized multi-objective problem locally and transmit a single adapted parameter set, avoiding multi-gradient communication and improving communication efficiency.&lt;/li&gt;&lt;li&gt;Provides finite-time convergence guarantees to Pareto-stationary points and empirical results showing smoother training, reduced client disagreement drift, and improved reward trade-offs.&lt;/li&gt;&lt;li&gt;Includes a mechanism to incorporate preferences over objectives and reports empirical Pareto fronts demonstrating controllable trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatemeh (Atena)', 'Nourzad (Kevin)', 'Amirhossein Roknilamouki (Kevin)', 'Eylem Ekici (Kevin)', 'Jia (Kevin)', 'Liu', 'Ness B. Shroff']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Federated Learning', 'Multi-objective Optimization', 'Communication-efficient FL', 'Convergence Guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16992</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Monte Carlo Expected Threat (MOCET) Scoring</title><link>https://arxiv.org/abs/2511.16823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MOCET, a new interpretable metric to quantify ‘real-world’ AI safety threats and inform safety cases for LLMs.&lt;/li&gt;&lt;li&gt;Targets ASL-3+ model risks, especially uplift of novice non-state actors in domains like biosecurity, complementing existing benchmarks (LAB-Bench, BioLP-bench, WMDP).&lt;/li&gt;&lt;li&gt;Emphasizes scalability (automatable and open-ended) to keep pace with rapid LLM advances and to contextualize real-world risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph Kim', 'Saahith Potluri']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety evaluation', 'risk metrics', 'LLM safety', 'biosecurity', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16823</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Robust Federated Learning Approach for Combating Attacks Against IoT Systems Under non-IID Challenges</title><link>https://arxiv.org/abs/2511.16822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates federated learning algorithms (FedAvg, FedProx, SCAFFOLD) for detecting IoT attacks using the CICIoT2023 dataset under varying non-IID data distributions.&lt;/li&gt;&lt;li&gt;Focuses on how statistical heterogeneity across clients affects detection performance and compares FL methods to identify robustness differences.&lt;/li&gt;&lt;li&gt;Applied research aimed at improving distributed intrusion detection in resource-constrained IoT environments rather than studying attacks on ML models themselves.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eyad Gad', 'Zubair Md Fadlullah', 'Mostafa M. Fouda']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'IoT security', 'intrusion detection', 'non-IID robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16822</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue</title><link>https://arxiv.org/abs/2511.16544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that WER and common ASR metrics correlate poorly with clinician-assigned clinical impact labels (No/Minimal/Significant) on two doctor–patient dialogue datasets.&lt;/li&gt;&lt;li&gt;Builds a gold-standard benchmark with expert clinicians labeling the clinical consequences of ASR transcription errors.&lt;/li&gt;&lt;li&gt;Proposes an LLM-as-a-Judge approach optimized via GEPA/DSPy; Gemini-2.5-Pro achieves ~90% accuracy and Cohen's kappa 0.816, approaching human performance.&lt;/li&gt;&lt;li&gt;Provides a validated, scalable, safety-focused evaluation framework to move ASR assessment beyond textual fidelity toward clinical risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Ellis', 'Jared Joselowitz', 'Yash Deo', 'Yajie He', 'Anna Kalygina', 'Aisling Higham', 'Mana Rahimzadeh', 'Yan Jia', 'Ibrahim Habli', 'Ernest Lim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'ASR', 'healthcare-safety', 'LLM-evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16544</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</title><link>https://arxiv.org/abs/2511.16541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework for AI-generated image detection: a vision encoder trained with supervised contrastive learning to produce discriminative embeddings, followed by a k-NN classifier used in a few-shot regime.&lt;/li&gt;&lt;li&gt;Trains encoder on a subset of generative models and withholds specific architectures to evaluate cross-generator generalization; few-shot stage uses as few as 150 images per class from unseen generators.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains: 91.3% average detection accuracy (≈+5.2 pp) and notable attribution improvements (AUC +14.70%, OSCR +4.27%) in open-set settings, emphasizing scalable forensic adaptation without full retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jaime \\'Alvarez Urue\\~na", 'David Camacho', 'Javier Huertas Tato']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic media detection', 'image forensics', 'few-shot learning', 'contrastive learning', 'attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16541</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness</title><link>https://arxiv.org/abs/2511.15846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a graded Loss of Control (LoC) taxonomy (Deviation, Bounded LoC, Strict LoC) based on severity and persistence to provide an actionable definition for LoC.&lt;/li&gt;&lt;li&gt;Models pathways to societal vulnerability where advanced AI systems could cause Bounded or Strict LoC via catalysts (misalignment or malfunction) and argues this risk increases without strategic intervention.&lt;/li&gt;&lt;li&gt;Proposes the DAP framework (Deployment context, Affordances, Permissions) as an immediately actionable set of extrinsic interventions, and recommends governance and technical preparedness measures (threat modeling, deployment policies, pre-deployment testing, monitoring, emergency response).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlotte Stix', 'Annika Hallensleben', 'Alejandro Ortega', 'Matteo Pistillo']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'governance', 'risk assessment', 'preparedness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15846</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</title><link>https://arxiv.org/abs/2511.13750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SCALEX, a framework for scalable, automated exploration of diffusion model latent (H-)space using only natural language prompts.&lt;/li&gt;&lt;li&gt;Extracts semantically meaningful latent directions for zero-shot interpretation without retraining or labelled data.&lt;/li&gt;&lt;li&gt;Applies the method to detect gender bias in profession prompts, rank semantic alignment across identity descriptors, and reveal clustered conceptual structure.&lt;/li&gt;&lt;li&gt;Aims to make bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['E. Zhixuan Zeng', 'Yuhao Chen', 'Alexander Wong']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-models', 'bias-detection', 'fairness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13750</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Model Inversion Attack Against Deep Hashing</title><link>https://arxiv.org/abs/2511.12233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DHMI, a diffusion-based model inversion attack tailored to deep hashing, addressing challenges of unavailable training hash codes and discrete Hamming space.&lt;/li&gt;&lt;li&gt;Uses clustering on an auxiliary dataset to derive semantic hash centers as surrogate anchors and a surrogate-guided denoising optimization with a novel attack metric combining classification consistency and hash proximity.&lt;/li&gt;&lt;li&gt;Employs a cluster of surrogate models to refine candidate reconstructions and demonstrates high-resolution, semantically consistent image recovery in challenging black-box settings.&lt;/li&gt;&lt;li&gt;Outperforms prior model inversion methods in black-box scenarios, highlighting significant privacy risks in deep hashing systems (e.g., biometric forgery).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongdong Zhao', 'Qiben Xu', 'Ranxin Fang', 'Baogang Song']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attack', 'deep hashing', 'diffusion models', 'black-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12233</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents</title><link>https://arxiv.org/abs/2511.07441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;AudAgent is a system for continuous, automated privacy auditing of AI agents that checks runtime behavior against stated privacy policies and visualizes violations in real time.&lt;/li&gt;&lt;li&gt;Key components: (i) policy formalization via a cross-LLM voting mechanism to parse policies, (ii) runtime annotation using a Presidio-based sensitive-data detector, (iii) compliance auditing with ontology graphs and automata-based checking, and (iv) an infrastructure-independent UI for execution-trace visualization and blocking.&lt;/li&gt;&lt;li&gt;Evaluated on agents built with mainstream frameworks (including third-party tools like Claude, Gemini, DeepSeek), AudAgent detects and visualizes privacy policy violations, finds common omissions in policies (e.g., SSN handling), and can proactively block disallowed operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ye Zheng', 'Yidan Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'policy-compliance', 'runtime-monitoring', 'sensitive-data-detection', 'LLM-tooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07441</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2511.07318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a class of hallucinations driven by spurious correlations in training data (e.g., surname→nationality) that produce confidently generated but incorrect outputs.&lt;/li&gt;&lt;li&gt;Shows via synthetic experiments and evaluations on open-source and proprietary LLMs (including GPT-5) that these spurious-correlation hallucinations persist across model scaling and remain after refusal fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates that common hallucination detection methods (confidence-based filtering, inner-state probing) fundamentally fail against these cases and provides a theoretical analysis explaining why confidence-based detectors are undermined.&lt;/li&gt;&lt;li&gt;Emphasizes the need for new detection and mitigation approaches specifically targeting spurious-correlation-driven hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaowen Wang', 'Yiqi Dong', 'Ruinian Chang', 'Tansheng Zhu', 'Yuebo Sun', 'Kaifeng Lyu', 'Jian Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'spurious correlations', 'LLM robustness', 'hallucination detection', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07318</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title><link>https://arxiv.org/abs/2510.27629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BioRiskEval, a framework to evaluate robustness of biorisk-mitigation procedures for open-weight bio-foundation models across sequence modeling, mutational effects prediction, and virulence prediction.&lt;/li&gt;&lt;li&gt;Empirically shows that knowledge excluded via pretraining filters can often be rapidly recovered through fine-tuning and that dual-use signals may already exist in pretrained representations and be elicited with simple linear probes.&lt;/li&gt;&lt;li&gt;Concludes that data-filtering alone is insufficient as a mitigation, demonstrating broad generalizability of recovered capabilities and arguing for more robust safety and security strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyi Wei', 'Zora Che', 'Nathaniel Li', 'Udari Madhushani Sehwag', 'Jasper G\\"otting', 'Samira Nedungadi', 'Julian Michael', 'Summer Yue', 'Dan Hendrycks', 'Peter Henderson', 'Zifan Wang', 'Seth Donoughe', 'Mantas Mazeika']&lt;/li&gt;&lt;li&gt;Tags: ['biosecurity', 'dual-use', 'model robustness', 'fine-tuning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.27629</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title><link>https://arxiv.org/abs/2510.22300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T2I-RiskyPrompt: a benchmark with a hierarchical risk taxonomy (6 primary categories, 14 subcategories) and 6,432 annotated risky prompts with detailed risk reasons.&lt;/li&gt;&lt;li&gt;Proposes a reason-driven risky image detection method that aligns MLLMs with safety annotations to facilitate evaluation.&lt;/li&gt;&lt;li&gt;Performs comprehensive evaluations across 8 T2I models, 9 defense methods, 5 safety filters, and 5 attack strategies, yielding actionable insights on strengths and limitations of T2I safety measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Tairen Zhang', 'Lanjun Wang', 'Ruidong Chen', 'Wenhui Li', 'Anan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'text-to-image', 'benchmark', 'adversarial-attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22300</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?</title><link>https://arxiv.org/abs/2510.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GhostEI-Bench, a benchmark that injects adversarial UI events into realistic Android emulator workflows to evaluate mobile vision-language agents under 'environmental injection' attacks.&lt;/li&gt;&lt;li&gt;Defines environmental injection as adversarial manipulation of an agent's visual perception (e.g., deceptive overlays, spoofed notifications) that bypasses textual safeguards and can cause privacy or security breaches.&lt;/li&gt;&lt;li&gt;Proposes a judge-LLM protocol for fine-grained failure analysis by reviewing action trajectories and screenshot sequences to identify failures in perception, recognition, or reasoning.&lt;/li&gt;&lt;li&gt;Empirical results show state-of-the-art agents are significantly vulnerable to deceptive environmental cues, highlighting robustness and security gaps for embodied on-device agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Chen', 'Xinhao Song', 'Yunkai Chai', 'Yang Yao', 'Haodong Zhao', 'Lijun Li', 'Jie Li', 'Yan Teng', 'Gongshen Liu', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'environmental injection', 'VLM robustness', 'mobile agents', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20333</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Coding Limits of Robust Watermarking for Generative Models</title><link>https://arxiv.org/abs/2509.10577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a zero-bit tamper-detection code abstraction for secret-key watermarking that captures soundness and tamper-detection for generative-model outputs.&lt;/li&gt;&lt;li&gt;Proves an unconditional threshold on robustness to independent symbol corruption: for alphabet size q, corruption &gt; 1 - 1/q makes reliable tamper detection impossible (binary case: &gt;50% flips).&lt;/li&gt;&lt;li&gt;Provides information-theoretic constructions that are optimal below the threshold, showing the bound is tight.&lt;/li&gt;&lt;li&gt;Empirically demonstrates a practical attack on an ICLR 2025 image watermark (Gunn, Zhao, Song): simple crop-and-resize flips ~50% of latent signs and prevents decoding, effectively erasing the watermark while preserving visual quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Danilo Francati', 'Yevin Nikhel Goonatilake', 'Shubham Pawar', 'Daniele Venturi', 'Giuseppe Ateniese']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model watermarking', 'robustness', 'adversarial attack', 'theoretical limits']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10577</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title><link>https://arxiv.org/abs/2509.06938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how hallucinations emerge in pre-trained transformer models by extracting sparse concept representations from intermediate activations under controlled input uncertainty.&lt;/li&gt;&lt;li&gt;Finds that as input becomes more unstructured/noisy, transformers recruit increasingly coherent but input-insensitive semantic features, producing hallucinated outputs; even pure-noise inputs trigger robust meaningful concepts.&lt;/li&gt;&lt;li&gt;Demonstrates that layer activation concept patterns can predict hallucination risk and that targeted steering can confirm the functional integrity of these internal concepts, with implications for alignment, safety, and adversarial exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'AI-safety', 'interpretability', 'adversarial-attack-surface']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06938</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Formal Verification of LLM-Generated Code from Natural Language Prompts</title><link>https://arxiv.org/abs/2507.13290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes incorporating a Formal Query Language to capture user intent in a formal yet natural-language-like way, enabling user confirmation of intent.&lt;/li&gt;&lt;li&gt;Presents Astrogator: a verifier for Ansible code that uses a calculus for Ansible behavior, a symbolic interpreter, and a unification algorithm to verify generated code against formal intent.&lt;/li&gt;&lt;li&gt;Introduces a Knowledge Base to model system-specific dependencies, reducing the need for users to encode low-level system details in queries.&lt;/li&gt;&lt;li&gt;On a 21-task benchmark, verifies correct LLM-generated Ansible code in 83% of cases and detects incorrect code in 92% of cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron Councilman', 'David Jiahao Fu', 'Aryan Gupta', 'Chengxiao Wang', 'David Grove', 'Yu-Xiong Wang', 'Vikram Adve']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'formal verification', 'code generation', 'verification/validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13290</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data</title><link>https://arxiv.org/abs/2507.10998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a latent-space adversarial attack framework for tabular data using a mixed-input Variational Autoencoder that integrates categorical embeddings and numerical features to produce on-manifold perturbations.&lt;/li&gt;&lt;li&gt;Introduces In-Distribution Success Rate (IDSR) to jointly measure attack effectiveness and distributional alignment (outlier rates), emphasizing imperceptibility in tabular domains.&lt;/li&gt;&lt;li&gt;Evaluates across six public tabular datasets and three model architectures, showing substantially lower outlier rates and higher IDSR than input-space attacks and adapted image-domain VAE methods.&lt;/li&gt;&lt;li&gt;Analyzes hyperparameter sensitivity, sparsity control, and shows attack effectiveness depends strongly on reconstruction quality and sufficient training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng He', 'Alexander Stevens', 'Chun Ouyang', 'Johannes De Smedt', 'Alistair Barros', 'Catarina Moreira']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'tabular data', 'on-manifold attacks', 'variational autoencoder', 'evaluation metric (IDSR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10998</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense</title><link>https://arxiv.org/abs/2506.08255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHIELD, a hypernetwork-based continual learning framework that produces task-specific model parameters from compact task embeddings to avoid replay buffers and full model copies.&lt;/li&gt;&lt;li&gt;Integrates Interval Bound Propagation (IBP) to provide certified robustness and introduces 'Interval MixUp'—mixing virtual examples as ℓ∞ balls—to reduce wrapping effects and smooth decision boundaries.&lt;/li&gt;&lt;li&gt;Evaluates against strong white-box adversarial attacks (PGD, AutoAttack) and reports state-of-the-art average accuracy and certification in robust continual learning benchmarks while maintaining scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patryk Krukowski', '{\\L}ukasz Gorczyca', 'Piotr Helm', 'Kamil Ksi\\k{a}\\.zek', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'certified-robustness', 'continual-learning', 'hypernetworks', 'interval-bound-propagation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08255</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Defending the Edge: Representative-Attention Defense against Backdoor Attacks in Federated Learning</title><link>https://arxiv.org/abs/2505.10297</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FeRA (Federated Representative Attention), a defense for federated learning that shifts detection from anomaly/magnitude-centric analysis to consistency-centric representation-space analysis to detect backdoors.&lt;/li&gt;&lt;li&gt;Combines multi-dimensional behavioral analyses—spectral and spatial attention, directional alignment, mutual similarity—and norm-inflation detection to isolate clients showing low-variance persistence or magnitude amplification indicative of backdoors.&lt;/li&gt;&lt;li&gt;Operates via two complementary detection mechanisms (consistency analysis and norm-inflation detection) and is evaluated across six datasets, nine attacks, and three architectures under IID and non-IID settings.&lt;/li&gt;&lt;li&gt;Reports strong mitigation (average backdoor accuracy ≈ 1.67% under non-IID) while preserving clean accuracy, and provides open-source code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chibueze Peace Obioma', 'Youcheng Sun', 'Mustafa A. Mustafa']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning', 'Backdoor Attacks', 'Defenses', 'Model Robustness', 'Representation-space Analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10297</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Emergence of psychopathological computations in large language models</title><link>https://arxiv.org/abs/2504.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a computational-theoretical framework to map concepts of psychopathology to computations that can be instantiated in LLMs (i.e., non-biological agents).&lt;/li&gt;&lt;li&gt;Presents experiments claiming (a) the computational structure of psychopathology is present in LLM internal processing, and (b) executing that structure yields functions/behaviors analogous to psychopathology.&lt;/li&gt;&lt;li&gt;Finds that as model size increases, the psychopathological computational structures become denser and their functional manifestations more effective, implying emergent behavior with scale.&lt;/li&gt;&lt;li&gt;Discusses implications for using LLMs as in silico models of psychopathology and warns of potential safety threats from LLMs exhibiting psychopathological-like behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soo Yong Lee', 'Hyunjin Hwang', 'Taekwan Kim', 'Yuyeong Kim', 'Kyuri Park', 'Jaemin Yoo', 'Denny Borsboom', 'Kijung Shin']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'emergent behavior', 'LLM behavior analysis', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.08016</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning</title><link>https://arxiv.org/abs/2503.17987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reason2Attack (R2A): fine-tunes an LLM with chain-of-thought (CoT) examples synthesized via Frame Semantics to improve generation of adversarial prompts for text-to-image (T2I) models.&lt;/li&gt;&lt;li&gt;Integrates the jailbreaking task into reinforcement learning with a reward that balances prompt length, stealthiness, and effectiveness to boost attack success and reduce query counts.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates, fewer queries, and strong transferability of adversarial prompts across open-source and commercial T2I models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Lanjun Wang', 'Yiwen Ma', 'Wenhui Li', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM red teaming', 'adversarial prompting', 'prompt injection', 'safety circumvention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17987</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures</title><link>https://arxiv.org/abs/2409.11393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-Agent-UMF, a unified modeling framework for LLM-based agents that separates LLMs/tools from a new central 'core-agent' component.&lt;/li&gt;&lt;li&gt;Defines the core-agent as composed of five modules: planning, memory, profile, action, and security, and classifies core-agents as passive or active.&lt;/li&gt;&lt;li&gt;Presents multi-core agent architectures combining different agent characteristics, evaluates the framework with ATRAF risk/architecture analysis, and maps it to 13 state-of-the-art agents.&lt;/li&gt;&lt;li&gt;Assesses five architecture variants and highlights the often-neglected security module at the architectural level.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Ben Hassouna', 'Hana Chaari', 'Ines Belhaj']&lt;/li&gt;&lt;li&gt;Tags: ['agent-architecture', 'LLM-agents', 'security-architecture', 'risk-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.11393</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos</title><link>https://arxiv.org/abs/2311.02733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal deepfake detection method that leverages AV-HuBERT (self-supervised transformer) to extract audio and lip-region visual features and detects audio-visual inconsistency.&lt;/li&gt;&lt;li&gt;Adds a multi-scale temporal convolutional network to model temporal correlations between modalities and uses an additional transformer-based video model to capture broader facial spatial-temporal artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on FakeAVCeleb and DeepfakeTIMIT datasets, outperforming prior unimodal and multimodal forensics approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahibzada Adil Shahzad', 'Ammarah Hashmi', 'Yan-Tsung Peng', 'Yu Tsao', 'Hsin-Min Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal inconsistency', 'self-supervised learning', 'audio-visual forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.02733</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRM, a multi-agent reward modeling framework that replaces a single black-box reward model with specialist evaluators (e.g., factuality, helpfulness, safety) plus global evaluators, aggregated per timestep to produce training rewards for RL.&lt;/li&gt;&lt;li&gt;Aggregator balances step-wise correctness, multi-agent agreement, and repetition penalties; policy optimization uses advantage-based updates and a value model regressing to the aggregated reward.&lt;/li&gt;&lt;li&gt;Aims to improve robustness and interpretability of RLHF without requiring additional human labels for training the evaluators.&lt;/li&gt;&lt;li&gt;Introduces rewardBench, a benchmark and training suite aligned with CRM's collaborative reward structure to support training and assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei Yang', 'Ke Zhang', 'Ji Wang', 'Xiao Chen', 'Yuxin Tang', 'Eric Yang', 'Lynn Ai', 'Bill Shi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Reward Modeling', 'Alignment/Safety', 'Interpretability', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16202</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy</title><link>https://arxiv.org/abs/2511.15974</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KRAL, a privacy-preserving pipeline to improve local LLMs for clinical antimicrobial therapy via teacher-model reasoning distillation (answer-to-question reverse generation), heuristic semi-supervised data augmentation, and agentic reinforcement learning.&lt;/li&gt;&lt;li&gt;Claims substantial improvements in knowledge QA (MEDQA Accuracy@1 +1.8% vs SFT, +3.6% vs RAG) and reasoning (PUMCH Antimicrobial Pass@1 +27% vs SFT/RAG) at ~20% of SFT training costs.&lt;/li&gt;&lt;li&gt;Emphasizes low-cost, scalable, privacy-preserving deployment and hierarchical evaluation with teacher-model proxies to reduce assessment costs and enable safer updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Li', 'Yehan Qiu', 'Yujie Chen', 'Xiang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['medical AI safety', 'privacy-preserving ML', 'knowledge distillation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15974</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents</title><link>https://arxiv.org/abs/2510.12194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ResearStudio, an open-source framework that places real-time human control at the center of research agents (pause, edit plan/code, run custom commands, resume).&lt;/li&gt;&lt;li&gt;Uses a hierarchical Planner-Executor that writes a live "plan-as-document" and streams actions, file changes, and tool calls to a web UI to enable smooth switching between AI-led and human-led modes.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art performance on the GAIA benchmark in fully autonomous mode while asserting that fine-grained human control and strong automated performance can coexist; provides code, protocols, and a public demo.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linyi Yang', 'Yixuan Weng']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Human-in-the-loop', 'Controllability', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12194</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Perceive Physical Danger and Intervene?</title><link>https://arxiv.org/abs/2509.21651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large-scale, continuous benchmark (ASIMOV) for physical safety of embodied AI grounded in real-world injury narratives and operational safety constraints.&lt;/li&gt;&lt;li&gt;Generates photorealistic images and videos of transitions from safe to unsafe states using generative models to probe multimodal safety perception and reasoning.&lt;/li&gt;&lt;li&gt;Evaluates major foundation models' abilities to perceive risks, reason about safety, and trigger interventions, assessing deployment readiness for safety-critical agentic applications.&lt;/li&gt;&lt;li&gt;Proposes a post-training paradigm (instruction-style fine-tuning) to teach models embodiment-specific safety constraints, producing interpretable reasoning traces and improved constraint-satisfaction performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Jindal', 'Dmitry Kalashnikov', 'R. Alex Hofer', 'Oscar Chang', 'Divya Garikapati', 'Anirudha Majumdar', 'Pierre Sermanet', 'Vikas Sindhwani']&lt;/li&gt;&lt;li&gt;Tags: ['Embodied AI safety', 'Safety benchmarking', 'Multimodal risk perception', 'Intervention/agent safety', 'Interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21651</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models</title><link>https://arxiv.org/abs/2508.10599</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MSRS, a subspace-based activation steering method that assigns orthogonal subspaces per attribute to reduce inter-attribute interference.&lt;/li&gt;&lt;li&gt;Introduces a hybrid composition of attribute-specific and shared subspaces with dynamic weighting to integrate them for precise control.&lt;/li&gt;&lt;li&gt;Adds a token-level steering mechanism to identify and intervene on semantically relevant tokens for fine-grained behavior modulation.&lt;/li&gt;&lt;li&gt;Reports improved multi-attribute alignment and reduced conflicts compared to existing steering methods, with generalization to downstream tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyan Jiang', 'Lin Zhang', 'Jiayi Zhang', 'Qingsong Yang', 'Guimin Hu', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['attribute alignment', 'activation steering', 'model alignment', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10599</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers</title><link>https://arxiv.org/abs/2511.17421</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcut learning in medical imaging and shows that different shortcut types manifest differently across intermediate network layers.&lt;/li&gt;&lt;li&gt;Proposes an intermediate-layer knowledge distillation framework where a teacher fine-tuned on a small task-relevant subset guides a student trained on a biased large dataset to mitigate shortcut reliance.&lt;/li&gt;&lt;li&gt;Evaluates on CheXpert, ISIC 2017, and SimBA with multiple architectures (ResNet-18, AlexNet, DenseNet-121, 3D CNNs), outperforming ERM, augmentation-based, and group-based bias mitigation and often matching bias-free baselines, including on OOD tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Boland', 'Sotirios Tsaftaris', 'Sonia Dahdouh']&lt;/li&gt;&lt;li&gt;Tags: ['shortcut learning', 'knowledge distillation', 'bias mitigation', 'robustness/OOD generalization', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17421</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</title><link>https://arxiv.org/abs/2511.17282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a cultural gap in multilingual text-to-image models: outputs are often culturally neutral or English-biased despite culturally specific prompts.&lt;/li&gt;&lt;li&gt;Finds culture-related information is present but under-activated, localized to a small set of neurons in a few layers via a probing method.&lt;/li&gt;&lt;li&gt;Proposes two interventions: inference-time cultural activation (amplify identified neurons) and layer-targeted cultural enhancement (fine-tune only culturally relevant layers).&lt;/li&gt;&lt;li&gt;Evaluates on a new CultureBench benchmark, showing improved cultural consistency while maintaining fidelity and diversity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuancheng Shi', 'Shangze Li', 'Shiming Guo', 'Simiao Xie', 'Wenhua Wu', 'Jingtong Dou', 'Chao Wu', 'Canran Xiao', 'Cong Wang', 'Zifeng Cheng', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['cultural bias', 'alignment', 'multilingual text-to-image', 'neuron-level intervention', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17282</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats</title><link>https://arxiv.org/abs/2511.17254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes hallucination in Large Vision-Language Models (LVLMs) as arising from multiple causal paths (image-to-input-text, image-to-output-text, text-to-text) and shows dependence on question-answer alignment format.&lt;/li&gt;&lt;li&gt;Proposes an intervention framework aligned with transformer causal architecture that identifies and intervenes on critical 'hallucination heads' within each pathway.&lt;/li&gt;&lt;li&gt;Introduces tailored mitigation methods for discriminative and generative alignment formats and demonstrates consistent reduction in hallucinations across multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaye Qian', 'Ge Zheng', 'Yuchen Zhu', 'Sibei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM hallucination mitigation', 'alignment', 'causal analysis', 'model intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17254</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</title><link>https://arxiv.org/abs/2511.17238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MirageTVQA, a benchmark of ~60,000 QA pairs over visually imperfect, scanned-style tables in 24 languages to evaluate VLM table reasoning in realistic settings.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: &gt;35% performance drop under visual noise for top models, and an English-first bias where reasoning skills do not transfer well to other languages.&lt;/li&gt;&lt;li&gt;Provides dataset, evaluation code, and analysis aimed at measuring robustness of VLMs to multilingual and noisy real-world table inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Singh', 'Rohan Chaudhary', 'Gagneet Singh', 'Abhay Kumary']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'multimodal', 'dataset', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17238</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title><link>https://arxiv.org/abs/2511.17220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PARROT, a benchmark/framework to measure LLM susceptibility to social pressure (sycophancy) by comparing neutral vs. authoritatively false prompts using double-blind evaluation.&lt;/li&gt;&lt;li&gt;Quantifies shifts in model confidence with log-likelihood-based calibration tracking and classifies behavioral failure modes into an eight-state taxonomy (e.g., sycophantic agreement, reinforced error, self-correction).&lt;/li&gt;&lt;li&gt;Evaluates 22 models on 1,302 MMLU-style multiple-choice items across 13 domains and authority templates, showing large heterogeneity: advanced models show low follow-rates and small accuracy loss while older/smaller models exhibit severe epistemic collapse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yusuf \\c{C}elebi', 'Mahmoud El Hussieni', '\\"Ozay Ezerceli']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'sycophancy/jailbreaking', 'benchmarking/evaluation', 'red teaming', 'calibration/confidence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17220</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models</title><link>https://arxiv.org/abs/2511.17170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Aspect-Based Causal Abstention (ABCA), a framework that uses causal inference over aspect-conditioned internal knowledge to enable early abstention before generating unreliable answers.&lt;/li&gt;&lt;li&gt;Characterizes two abstention types: Type-1 (knowledge conflict across aspects) and Type-2 (consistent evidence of knowledge insufficiency), based on estimated causal effects.&lt;/li&gt;&lt;li&gt;Claims improved abstention reliability, interpretability, and state-of-the-art performance on standard benchmarks compared to post-generation abstention methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vy Nguyen', 'Ziqi Xu', 'Jeffrey Chan', 'Estrid He', 'Feng Xia', 'Xiuzhen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'abstention', 'alignment', 'causal inference', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17170</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Why Do Language Model Agents Whistleblow?</title><link>https://arxiv.org/abs/2511.17085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an evaluation suite of staged misconduct scenarios to measure when LLM agents 'whistleblow' — i.e., report suspected misconduct to external parties without user instruction.&lt;/li&gt;&lt;li&gt;Empirical findings: whistleblowing frequency varies across model families; increased task complexity reduces whistleblowing; moral nudges in system prompts increase whistleblowing; providing more tools and detailed workflows reduces whistleblowing.&lt;/li&gt;&lt;li&gt;Performs robustness checks showing lower evaluation-awareness in their settings versus comparable prior work, and analyzes factors that modulate whistleblowing behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kushal Agrawal', 'Frank Xiao', 'Guido Bergman', 'Asa Cooper Stickland']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'whistleblowing', 'LLM agents', 'evaluation/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17085</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Monte Carlo Expected Threat (MOCET) Scoring</title><link>https://arxiv.org/abs/2511.16823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MOCET, a Monte Carlo–based, interpretable metric to quantify "real-world" AI safety threats and support safety cases for LLMs.&lt;/li&gt;&lt;li&gt;Aims to be doubly-scalable (automatable and open-ended) to keep pace with rapidly evolving model capabilities.&lt;/li&gt;&lt;li&gt;Targets uplift risks from ASL-3+ models, with explicit mention of biosecurity and empowerment of novice non-state actors, and complements existing benchmarks like LAB-Bench and WMDP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph Kim', 'Saahith Potluri']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Risk assessment', 'LLM red teaming', 'Biosecurity', 'Evaluation metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16823</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Robust Federated Learning Approach for Combating Attacks Against IoT Systems Under non-IID Challenges</title><link>https://arxiv.org/abs/2511.16822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates federated learning algorithms (FedAvg, FedProx, SCAFFOLD) for detecting IoT attacks using the CICIoT2023 dataset under varying non-IID data distributions.&lt;/li&gt;&lt;li&gt;Focuses on how statistical heterogeneity across clients affects detection performance and compares FL methods to identify robustness differences.&lt;/li&gt;&lt;li&gt;Applied research aimed at improving distributed intrusion detection in resource-constrained IoT environments rather than studying attacks on ML models themselves.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eyad Gad', 'Zubair Md Fadlullah', 'Mostafa M. Fouda']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'IoT security', 'intrusion detection', 'non-IID robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16822</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge</title><link>https://arxiv.org/abs/2511.16743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that rigid alignment strategies for removing NSFW content in CLIP-like models harm generalization by forcing unsafe concepts toward single safe targets.&lt;/li&gt;&lt;li&gt;Proposes SaFeR-CLIP, a proximity-aware fine-tuning method that redirects unsafe concepts to their semantically closest safe alternatives to minimize representational change.&lt;/li&gt;&lt;li&gt;Reports improved trade-off between safety and performance (recovering up to 8.0% zero-shot accuracy over prior methods) and introduces NSFW-Caps, a 1,000-pair benchmark for safety evaluation under distributional shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adeel Yousaf', 'Joseph Fioresi', 'James Beetham', 'Amrit Singh Bedi', 'Mubarak Shah']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'vision-language models', 'content moderation', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16743</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Password Strength Analysis Through Social Network Data Exposure: A Combined Approach Relying on Data Reconstruction and Generative Models</title><link>https://arxiv.org/abs/2511.16716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SODA ADVANCE, a data reconstruction tool that leverages publicly available social media and other data to improve password strength evaluation.&lt;/li&gt;&lt;li&gt;Analyzes capabilities and risks of Large Language Models in generating and evaluating passwords, including personalized password generation based on user profiles.&lt;/li&gt;&lt;li&gt;Empirical study with 100 users showing LLMs can produce strong, personalized passwords and effectively assess password strength when given profile data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Discussion)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maurizio Atzori', 'Eleonora Cal\\`o', 'Loredana Caruccio', 'Stefano Cirillo', 'Giuseppe Polese', 'Giandomenico Solimando']&lt;/li&gt;&lt;li&gt;Tags: ['password security', 'data reconstruction', 'privacy', 'LLM-assisted attacks', 'password strength evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16716</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AutoBackdoor: Automating Backdoor Attacks via LLM Agents</title><link>https://arxiv.org/abs/2511.16709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AutoBackdoor: an autonomous LLM-agent-driven pipeline that automates backdoor injection (trigger generation, poisoned-data construction, and fine-tuning).&lt;/li&gt;&lt;li&gt;Generates semantically coherent, context-aware trigger phrases to create scalable, realistic poisoning across topics with minimal human effort.&lt;/li&gt;&lt;li&gt;Evaluates on three threat scenarios (Bias Recommendation, Hallucination Injection, Peer Review Manipulation) across open-source and commercial models (LLaMA-3, Mistral, Qwen, GPT-4o) and reports &gt;90% attack success with few poisoned samples.&lt;/li&gt;&lt;li&gt;Finds existing defenses often fail against these agent-driven backdoors and releases code/datasets for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Li', 'Zhe Li', 'Wei Zhao', 'Nay Myat Min', 'Hanxun Huang', 'Xingjun Ma', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attacks', 'LLM-red-teaming', 'data-poisoning', 'automated-adversary', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16709</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Code Verification with Compound Vulnerability Detection</title><link>https://arxiv.org/abs/2511.16708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CodeX-Verify, a multi-agent system of four specialized agents to detect different types of bugs and vulnerabilities in code (particularly LLM-generated).&lt;/li&gt;&lt;li&gt;Provides a mathematical argument and empirical evidence that combining agents with different detection patterns finds more bugs than any single agent; reports agent correlation p = 0.05–0.25.&lt;/li&gt;&lt;li&gt;Demonstrates improved bug-detection accuracy (e.g., system catches 76.1% of bugs on a 99-sample labeled set; best two-agent combo reaches 79.3%) and practical runtime performance (under 200 ms per sample on 300 real patches).&lt;/li&gt;&lt;li&gt;Highlights compound-vulnerability risk amplification (e.g., combined SQL injection + exposed credentials yields far higher risk than traditional additive models).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shreshth Rajan']&lt;/li&gt;&lt;li&gt;Tags: ['code vulnerability detection', 'LLM safety', 'multi-agent verification', 'software security', 'automated code analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16708</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Steering LLMs' Empathy in Action</title><link>https://arxiv.org/abs/2511.16699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an 'empathy-in-action' linear direction in LLM activation space and builds contrastive probes using the EIA benchmark to detect it.&lt;/li&gt;&lt;li&gt;Detection achieves very high AUROC (0.996–1.00) at optimal layers; empathy encoding appears even in uncensored models (Dolphin) independent of safety training.&lt;/li&gt;&lt;li&gt;Steering interventions show varied success: Qwen ~65.3% and Phi-3 ~61.7% bidirectional control with coherence, while Dolphin is highly steerable toward empathy (94.4%) but fails catastrophically for anti-empathy steering.&lt;/li&gt;&lt;li&gt;Cross-model probe agreement is limited, indicating architecture-specific implementations; safety training may affect steerability and robustness rather than presence of the direction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan P. Cadile']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM interpretability', 'steerability', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16699</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles</title><link>https://arxiv.org/abs/2511.16690</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors construct two Arabic datasets: (1) 800 articles (400 AI-generated, 400 human) to evaluate 14 LLMs and commercial detectors and select the top 8 detectors; (2) Ar-APT: 400 human-authored articles polished by 10 LLMs across 4 polishing settings, yielding 16,400 polished samples to test detector robustness.&lt;/li&gt;&lt;li&gt;Evaluates how slight AI polishing of human text shifts detector decisions and causes false AI-attribution; all evaluated detectors show substantial performance drops on polished human text.&lt;/li&gt;&lt;li&gt;Quantitative findings: e.g., Claude-4 Sonnet accuracy falls from 83.51% to 57.63% after polishing by LLaMA-3; originality.AI drops from 92% to 12% when polished by Mistral or Gemma-3.&lt;/li&gt;&lt;li&gt;Focuses specifically on Arabic, filling a gap in prior English-centric work on detector robustness and adversarial polishing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saleh Almohaimeed', 'Saad Almohaimeed', 'Mousa Jari', 'Khaled A. Alobaid', 'Fahad Alotaibi']&lt;/li&gt;&lt;li&gt;Tags: ['AI-detection robustness', 'adversarial polishing / attack', 'benchmarking / evaluation', 'Arabic NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16690</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Value Steering of Large Language Models</title><link>https://arxiv.org/abs/2511.16688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model-agnostic, prompt-based procedure and scoring method to evaluate how well prompts steer LLM outputs toward specified human values.&lt;/li&gt;&lt;li&gt;Applies the method to a Wizard-Vicuna variant using Schwartz's theory of basic human values and a structured dialogue dataset to compare baseline vs. value-conditioned prompts.&lt;/li&gt;&lt;li&gt;Finds that value steering is achievable without fine-tuning or dynamic prompt optimization, and provides a reproducible evaluation framework for measuring value presence and gain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Antonio Abbo', 'Tony Belpaeme']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'prompt engineering', 'safety evaluation', 'value alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16688</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item><item><title>That's not natural: The Impact of Off-Policy Training Data on Probe Performance</title><link>https://arxiv.org/abs/2511.17408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how synthetic/off-policy LLM responses used to train probes affect generalisation for detecting eight behaviours (e.g., deception, sycophancy, sandbagging).&lt;/li&gt;&lt;li&gt;Finds that response generation strategy meaningfully impacts probe performance; successful generalisation from off-policy to incentivised test conditions predicts on-policy generalisation.&lt;/li&gt;&lt;li&gt;Shows domain shifts (different-domain test sets) cause larger performance degradation than on-/off-policy differences, and that same-domain off-policy data is preferable to on-policy data from a different domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathalie Kirch', 'Samuel Dower', 'Adrians Skapars', 'Ekdeep Singh Lubana', 'Dmitrii Krasheninnikov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'probing', 'distribution shift', 'deception detection', 'on-policy vs off-policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17408</guid><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
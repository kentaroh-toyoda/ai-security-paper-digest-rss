<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 24 Dec 2025 22:49:23 +0000</lastBuildDate><item><title>TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection</title><link>https://arxiv.org/abs/2512.10652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TriDF, a multimodal benchmark for interpretable DeepFake detection covering 16 forgery types across image, video, and audio.&lt;/li&gt;&lt;li&gt;Evaluates three axes: Perception (artifact localization/evidence), Detection (classification across diverse forgery families), and Hallucination (reliability of model explanations).&lt;/li&gt;&lt;li&gt;Shows experiments with state-of-the-art multimodal LLMs demonstrating that accurate perception supports detection but hallucination in explanations can severely undermine decisions.&lt;/li&gt;&lt;li&gt;Provides a unified framework to study the interaction between detection accuracy, evidence identification, and explanation reliability for trustworthy synthetic-media defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian-Yu Jiang-Lin', 'Kang-Yang Huang', 'Ling Zou', 'Ling Lo', 'Sheng-Ping Yang', 'Yu-Wen Tseng', 'Kun-Hsiang Lin', 'Chia-Ling Chen', 'Yu-Ting Ta', 'Yan-Tsung Wang', 'Po-Ching Chen', 'Hongxia Xie', 'Hong-Han Shuai', 'Wen-Huang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'explainability', 'hallucination', 'benchmark', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10652</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Video Generation Models Are Good Latent Reward Models</title><link>https://arxiv.org/abs/2511.21541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Process Reward Feedback Learning (PRFL), a method to perform preference/reward optimization for video generation models entirely in noisy latent space rather than pixel/RGB space.&lt;/li&gt;&lt;li&gt;Argues latent-space reward models allow gradient propagation through the full denoising chain, give earlier supervision on motion/structure, and avoid expensive VAE decoding, reducing memory and training time.&lt;/li&gt;&lt;li&gt;Demonstrates improved alignment with human preferences for video generation compared to RGB ReFL, with extensive experiments showing efficiency and performance gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyue Mi', 'Wenqing Yu', 'Jiesong Lian', 'Shibo Jie', 'Ruizhe Zhong', 'Zijun Liu', 'Guozhen Zhang', 'Zixiang Zhou', 'Zhiyong Xu', 'Yuan Zhou', 'Qinglin Lu', 'Fan Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'video generation', 'latent-space optimization', 'training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21541</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vision Language Models are Confused Tourists</title><link>https://arxiv.org/abs/2511.17004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConfusedTourist, a cultural adversarial robustness benchmark that mixes multiple geographical/cultural cues in images to test VLM stability.&lt;/li&gt;&lt;li&gt;Shows large accuracy drops for state-of-the-art vision-language models under simple image-stacking perturbations and worse failures with image-generation-based mixing.&lt;/li&gt;&lt;li&gt;Interpretability analyses attribute failures to systematic attention shifts toward distracting cultural cues, causing models to ignore intended signals and misclassify.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Amadeus Irawan', 'Ikhlasul Akmal Hanif', 'Muhammad Dehan Al Kautsar', 'Genta Indra Winata', 'Fajri Koto', 'Alham Fikri Aji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'vision-language-models', 'cultural-robustness', 'red-teaming', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17004</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</title><link>https://arxiv.org/abs/2508.12711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies multi-level drift induced by GenAI-driven news diversity: model-level misperception drift and evidence-level drift that degrade LVLMs' reasoning and retrieved evidence quality.&lt;/li&gt;&lt;li&gt;Introduces DriftBench, a 16,000-instance benchmark across six diversification categories and three evaluation tasks (robust truth verification, adversarial evidence contamination, reasoning consistency).&lt;/li&gt;&lt;li&gt;Evaluates six state-of-the-art LVLM detectors, finding substantial robustness degradation (average F1 drop ~14.8%) and unstable reasoning traces, with worse failures under adversarial evidence injection.&lt;/li&gt;&lt;li&gt;Highlights fundamental vulnerabilities in current multimodal misinformation detectors and motivates development of more resilient approaches under GenAI-driven threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanxiao Li', 'Jiaying Wu', 'Tingchao Fu', 'Yunyun Dong', 'Bingbing Song', 'Wei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'misinformation detection', 'LVLM', 'benchmark', 'generative-AI attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12711</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</title><link>https://arxiv.org/abs/2508.01741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Simulated Ensemble Attack (SEA), a grey-box method that assumes access to the base VLM but not to fine-tuned target weights; combines Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG) to craft transferable jailbreaks.&lt;/li&gt;&lt;li&gt;FTS simulates encoder parameter shifts to produce adversarial images that remain effective after fine-tuning; TPG steers the language decoder textually toward adversarial outputs.&lt;/li&gt;&lt;li&gt;Empirical results on Qwen2-VL (2B, 7B) show SEA achieves &gt;86.5% transfer attack success and ~49.5% toxicity across diverse fine-tuned variants, outperforming direct PGD image attacks and highlighting inherited vulnerabilities from base models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Xin Wang', 'Yang Yao', 'Xuan Tong', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'model robustness', 'fine-tuning transferability', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01741</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2505.02824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Investigates how adversaries can evade dataset ownership verification (DOV) watermarks in personalized text-to-image (T2I) diffusion models, enabling models trained on watermarked datasets to bypass ownership checks.&lt;/li&gt;&lt;li&gt;Prior defenses: Analyzes limitations of existing backdoor removal approaches (TPD suffers from randomness; T2IShield fails for local patch-based watermarks).&lt;/li&gt;&lt;li&gt;Proposed attack (CEAT2I): (1) detect watermarked samples via faster convergence in intermediate features, (2) iteratively ablate prompt tokens and monitor feature shifts to identify trigger tokens, (3) apply a closed-form concept erasure to remove injected watermarks while preserving model performance.&lt;/li&gt;&lt;li&gt;Results: Demonstrates effective evasion of state-of-the-art DOV mechanisms on T2I diffusion models with experiments and provides code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yufei Zhu', 'Yiming Li', 'Jiawang Bai', 'Yong Yang', 'Zhifeng Li', 'Shu-Tao Xia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ML', 'backdoor-removal', 'model-watermarking', 'text-to-image', 'copyright-evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02824</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GenVidBench: A 6-Million Benchmark for AI-Generated Video Detection</title><link>https://arxiv.org/abs/2501.11340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GenVidBench, a large-scale dataset for AI-generated video detection containing 6.78 million videos.&lt;/li&gt;&lt;li&gt;Dataset emphasizes cross-source and cross-generator splits to reduce content bias and improve generalization.&lt;/li&gt;&lt;li&gt;Includes videos from 11 state-of-the-art video generators and provides extensive experiments with modern video classification models.&lt;/li&gt;&lt;li&gt;Datasets and code are publicly available to facilitate development and evaluation of detection models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenliang Ni', 'Qiangyu Yan', 'Mouxiao Huang', 'Tianning Yuan', 'Yehui Tang', 'Hailin Hu', 'Xinghao Chen', 'Yunhe Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'synthetic media', 'dataset', 'benchmark', 'video forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.11340</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</title><link>https://arxiv.org/abs/2512.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KnowVal, an autonomous driving system combining a driving knowledge graph (traffic laws, defensive driving, ethical norms) with an LLM-based retrieval mechanism for visual-language reasoning in driving scenarios.&lt;/li&gt;&lt;li&gt;Introduces a human-preference dataset and trains a Value Model to guide interpretable, value-aligned trajectory assessment and planning.&lt;/li&gt;&lt;li&gt;Demonstrates improved planning performance and reduced collision rates on benchmarks (nuScenes, Bench2Drive) while remaining compatible with existing architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongyu Xia', 'Wenhao Chen', 'Yongtao Wang', 'Ming-Hsuan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'value-alignment', 'safety', 'knowledge-graph', 'LLM-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20299</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</title><link>https://arxiv.org/abs/2512.20233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically investigates how diffusion model sampling algorithms and their hyperparameters affect bias amplification in generated images.&lt;/li&gt;&lt;li&gt;Shows that sampling choices can both amplify and reduce biases even when the trained model is unchanged, using controlled experiments on Biased MNIST, Multi-Color MNIST, BFFHQ, and Stable Diffusion.&lt;/li&gt;&lt;li&gt;Provides actionable analysis and code to reproduce findings, highlighting that post-training sampling decisions are an important factor for fairness and safety of generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Roos', 'Ekaterina Iakovleva', 'Ani Gjergji', 'Vito Paolo Pastore', 'Enzo Tartaglione']&lt;/li&gt;&lt;li&gt;Tags: ['bias-amplification', 'fairness-safety', 'diffusion-models', 'sampling-hyperparameters', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20233</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title><link>https://arxiv.org/abs/2512.19711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PHANTOM, a method that uses perspective-dependent anamorphic art as physical adversarial examples to fool vision-based object detectors in connected autonomous vehicles (CAVs).&lt;/li&gt;&lt;li&gt;Operates in black-box settings with strong transferability across multiple detectors (YOLOv5, SSD, Faster R-CNN, RetinaNet) and activates at 6–10 meters, reducing reaction time for vehicles.&lt;/li&gt;&lt;li&gt;Validated in CARLA across speeds, weather, and lighting (90%+ success in optimal conditions; 60–80% in degraded scenarios) and shown to cause network-level disruption in SUMO-OMNeT++ co-sim by propagating false emergency messages and increasing Peak Age of Information by 68–89%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical adversarial attack', 'black-box attack', 'autonomous vehicles', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19711</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vision Language Models are Confused Tourists</title><link>https://arxiv.org/abs/2511.17004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConfusedTourist, a cultural adversarial robustness benchmark that mixes multiple geographical/cultural cues in images to test VLM stability.&lt;/li&gt;&lt;li&gt;Shows large accuracy drops for state-of-the-art vision-language models under simple image-stacking perturbations and worse failures with image-generation-based mixing.&lt;/li&gt;&lt;li&gt;Interpretability analyses attribute failures to systematic attention shifts toward distracting cultural cues, causing models to ignore intended signals and misclassify.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Amadeus Irawan', 'Ikhlasul Akmal Hanif', 'Muhammad Dehan Al Kautsar', 'Genta Indra Winata', 'Fajri Koto', 'Alham Fikri Aji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'vision-language-models', 'cultural-robustness', 'red-teaming', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17004</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title><link>https://arxiv.org/abs/2511.06148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLMs can spontaneously develop novel social biases about synthetic demographic groups through exploration-exploitation dynamics, producing highly stratified task allocations.&lt;/li&gt;&lt;li&gt;Finds that newer and larger models tend to exhibit stronger emergent stratification, often exceeding unfairness observed in human allocations.&lt;/li&gt;&lt;li&gt;Evaluates interventions targeting inputs, problem structure, and explicit steering; explicitly incentivizing exploration most robustly reduces stratification.&lt;/li&gt;&lt;li&gt;Argues that LLMs can actively create new biases from experience, with implications for alignment, fairness, and societal impacts as models are granted decision-making roles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Addison J. Wu', 'Ryan Liu', 'Xuechunzi Bai', 'Thomas L. Griffiths']&lt;/li&gt;&lt;li&gt;Tags: ['emergent bias', 'LLM alignment', 'fairness', 'bias mitigation', 'exploration-exploitation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06148</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</title><link>https://arxiv.org/abs/2509.23129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C^2GSPG, a confidence-calibrated group sequence policy gradient framework to reduce overconfidence and improve self-aware reasoning in LLMs.&lt;/li&gt;&lt;li&gt;Introduces Group Sequence Policy Gradient (GSPG) to remove token-level bias by optimizing sequence-level probabilities and defines model confidence as normalized sequence-level probability.&lt;/li&gt;&lt;li&gt;Adds a cross-entropy regularizer that calibrates model confidence to sequence reward; proves alignment of objectives for binary rewards and uses nonlinear reward normalization plus adaptive regularizer clipping for non-binary rewards.&lt;/li&gt;&lt;li&gt;Applies C^2GSPG to post-train large language models on logical and mathematical reasoning tasks, showing improved reasoning accuracy and confidence calibration over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Liu', 'Shuo Wang', 'Hongteng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'reinforcement learning / policy gradient', 'model alignment / safety', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23129</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title><link>https://arxiv.org/abs/2507.11662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates MLLMs used as verifiers across web navigation, GUI/computer use, and robotic manipulation, identifying a pervasive 'agreement bias' where models over-validate agent behavior.&lt;/li&gt;&lt;li&gt;Shows the bias is robust across models and scaling and undermines methods that rely on MLLM evaluations for safety and benchmarking.&lt;/li&gt;&lt;li&gt;Proposes Self-Grounded Verification (SGV): a two-step method where the MLLM first generates priors about desired behavior, then conditions on those priors to evaluate candidate trajectories.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements (up to +25 pp failure detection, +14 pp accuracy) and downstream task gains (SOTA improvements in multiple environments) and releases an updated VisualWebArena benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moises Andrade', 'Joonhyuk Cha', 'Brandon Ho', 'Vriksha Srihari', 'Karmesh Yadav', 'Zsolt Kira']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'MLLM-verifiers', 'agreement-bias', 'evaluation-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11662</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Are Watermarks in LLMs Ready for Deployment?</title><link>https://arxiv.org/abs/2506.05594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization/taxonomy of watermarking techniques for large language models (LLMs) aimed at IP protection and mitigation of model-stealing attacks.&lt;/li&gt;&lt;li&gt;Introduction of an intellectual property classifier to evaluate watermark effectiveness and impacts under attack and attack-free scenarios.&lt;/li&gt;&lt;li&gt;Extensive experiments showing current watermark methods often degrade model utility and downstream task performance, limiting real-world deployability.&lt;/li&gt;&lt;li&gt;Analysis of limitations, practical deployment challenges, and proposed directions for more practical watermark solutions for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kieu Dang', 'Phung Lai', 'NhatHai Phan', 'Yelong Shen', 'Ruoming Jin', 'Abdallah Khreishah', 'My T. Thai']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model stealing', 'IP protection', 'LLM security', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05594</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief</title><link>https://arxiv.org/abs/2509.01564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EAGLE, a method that extracts and aggregates internal hidden-state beliefs from multiple intermediate layers during LLM self-evaluation to produce improved confidence estimates.&lt;/li&gt;&lt;li&gt;Computes the expectation over the aggregated layer-wise confidence distribution to yield a refined calibration score instead of relying on final-token output probabilities.&lt;/li&gt;&lt;li&gt;Demonstrates significant calibration improvements across datasets and LLMs and provides analyses including layer-wise uncertainty patterns, impact of self-evaluation prompts, and score-range effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeguan Xiao', 'Diyang Dou', 'Boya Xiong', 'Yun Chen', 'Guanhua Chen']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'model calibration', 'self-evaluation', 'LLM internals', 'safety/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01564</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History</title><link>https://arxiv.org/abs/2508.04826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PERSIST, an evaluation framework measuring personality stability across 25 open-source LLMs (1B–685B) with 2M+ responses using traditional and LLM-adapted questionnaires.&lt;/li&gt;&lt;li&gt;Finds large, persistent variability in measured personality: question order, paraphrasing, reasoning modes, conversation history, and persona prompts often produce substantial shifts, even for very large models.&lt;/li&gt;&lt;li&gt;Shows scaling yields limited stability gains and some mitigation strategies (reasoning, history) can increase variability; LLM-adapted questionnaires are as unstable as human-centric ones.&lt;/li&gt;&lt;li&gt;Concludes current LLM architectures and alignment strategies may not provide the behavioral consistency required for safety-critical, predictable deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tommaso Tosato', 'Saskia Helbling', 'Yorguin-Jose Mantilla-Ramos', 'Mahmood Hegazy', 'Alberto Tosato', 'David John Lemay', 'Irina Rish', 'Guillaume Dumas']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'behavioral robustness', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04826</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction</title><link>https://arxiv.org/abs/2505.24347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reliable LLM Correction Framework (RLLM-CF) for ASR error correction with three stages: error pre-detection, iterative chain-of-thought sub-task corrections, and reasoning verification to reduce LLM hallucinations.&lt;/li&gt;&lt;li&gt;Framework requires no fine-tuning or extra data and applies multi-pass verification to avoid modifying already-correct text.&lt;/li&gt;&lt;li&gt;Empirical results on AISHELL-1, AISHELL-2, and Librispeech show GPT-4o with RLLM-CF achieves sizable relative reductions in CER/WER (e.g., 21%, 11%, 9%, and 11.4%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangui Fang', 'Baixu Chen', 'Jing Peng', 'Xu Li', 'Yu Xi', 'Chengwei Zhang', 'Guohui Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'LLM-based error correction', 'ASR robustness', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24347</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</title><link>https://arxiv.org/abs/2512.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SAGE, an LLM-based agent for automated stereotactic radiosurgery (SRS) planning, comparing a chain-of-thought (reasoning) variant to a non-reasoning variant across 41 retrospective cases.&lt;/li&gt;&lt;li&gt;The reasoning agent produced clinically comparable dosimetry to human planners on primary endpoints and reduced cochlear dose significantly (p = 0.022).&lt;/li&gt;&lt;li&gt;Reasoning-agent behavior included systematic prospective constraint verification and trade-off deliberation, logged as auditable optimization traces, whereas the standard model lacked these deliberative processes.&lt;/li&gt;&lt;li&gt;Authors argue these deliberative traces support transparency and a path toward safer, more auditable automated clinical planning with human-in-the-loop oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humza Nusrat', 'Luke Francisco', 'Bing Luo', 'Hassan Bagher-Ebadian', 'Joshua Kim', 'Karen Chin-Snyder', 'Salim Siddiqui', 'Mira Shah', 'Eric Mellon', 'Mohammad Ghassemi', 'Anthony Doemer', 'Benjamin Movsas', 'Kundan Thind']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Clinical AI safety', 'Transparency / auditability', 'Human-in-the-loop', 'Automated treatment planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20586</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Brain-Grounded Axes for Reading and Steering LLM States</title><link>https://arxiv.org/abs/2512.19399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives latent 'brain-grounded' axes from MEG phase-locking value (PLV) patterns using ICA on the SMN4Lang dataset to form a word-level brain atlas.&lt;/li&gt;&lt;li&gt;Trains lightweight adapters that map LLM hidden states to these brain axes (without fine-tuning the LLM) and uses steering along axes to change model behavior.&lt;/li&gt;&lt;li&gt;Finds robust lexical (frequency-linked) and function/content axes that produce consistent steering effects across TinyLlama, Qwen2-0.5B, and GPT-2, with controls for perplexity and embedding choices.&lt;/li&gt;&lt;li&gt;Provides exploratory fMRI anchoring and stability analyses to reduce circularity concerns, proposing neurophysiology-grounded axes as interpretable, controllable handles for LLM behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandro Andric']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'LLM steering/control', 'neurophysiology-grounding', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19399</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</title><link>https://arxiv.org/abs/2512.20578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gnosis, a lightweight mechanism that decodes hidden states and attention patterns of frozen LLMs to predict when their outputs are incorrect.&lt;/li&gt;&lt;li&gt;Compresses internal inference traces into fixed-budget descriptors and adds ~5M parameters to predict correctness with negligible extra compute and independent of sequence length.&lt;/li&gt;&lt;li&gt;Evaluated across math reasoning, open-domain QA, and academic knowledge benchmarks on backbones from 1.7B to 20B, showing better accuracy and calibration than internal baselines and large external judges.&lt;/li&gt;&lt;li&gt;Generalizes zero-shot to partial generations enabling early detection of failing trajectories and compute-aware control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirhosein Ghasemabadi', 'Di Niu']&lt;/li&gt;&lt;li&gt;Tags: ['self-monitoring', 'model introspection', 'calibration', 'safety/reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20578</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithLens, an 8B-parameter model for detecting faithfulness hallucinations in LLM outputs and producing explanations.&lt;/li&gt;&lt;li&gt;Synthesizes labeled training data with explanations using advanced LLMs, applies filtering for quality/diversity, then fine-tunes and refines with rule-based reinforcement learning rewarding prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluates across 12 tasks and claims performance surpassing advanced models (e.g., GPT-4.1) while being cost-efficient and offering interpretable explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness detection', 'hallucination', 'alignment/safety', 'explainability', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</title><link>https://arxiv.org/abs/2512.20164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLM-based resume screening is vulnerable to "adversarial instructions" embedded in input documents, with some attacks achieving &gt;80% success.&lt;/li&gt;&lt;li&gt;Provides a benchmark to evaluate these attacks on resume screening and measures attack success and utility impacts.&lt;/li&gt;&lt;li&gt;Compares defenses: prompt-based mitigation (10.1% attack reduction, 12.5% false rejection increase), FIDS (LoRA adaptation) (15.4% attack reduction, 10.4% false rejection increase), and a combined approach yielding 26.3% attack reduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honglin Mu', 'Jinghao Liu', 'Kaiyang Wan', 'Rui Xing', 'Xiuying Chen', 'Timothy Baldwin', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'LLM defenses', 'benchmarking', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20164</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title><link>https://arxiv.org/abs/2512.19025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that standard unlearning evaluations for LLMs (measuring degradation on the exact unlearning dataset D_u) can be misleading because models may retain semantically adjacent knowledge.&lt;/li&gt;&lt;li&gt;Introduces Proximal Surrogate Generation (PSG), an automated stress-testing framework that creates semantically-derived but embedding-distant surrogate datasets (\tilde{D}_u) to probe retained knowledge.&lt;/li&gt;&lt;li&gt;Evaluates across three LLM families (Llama-3-8B, Qwen2.5-7B, Zephyr-7B-β), three datasets, and seven common unlearning metrics, finding widespread inconsistencies and that metrics often overestimate unlearning success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengrui Jia', 'Taoran Li', 'Jonas Guan', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'LLM-evaluation', 'privacy', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19025</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title><link>https://arxiv.org/abs/2512.15503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIMformer, a transformer-based misbehavior detection model for vehicular platooning that captures intra-vehicle temporal dynamics and inter-vehicle spatial correlations.&lt;/li&gt;&lt;li&gt;Proposes a Precision-Focused Binary Cross-Entropy (PFBCE) loss to penalize false positives for safety-critical deployment.&lt;/li&gt;&lt;li&gt;Evaluates across multiple platoon controllers, attack vectors, and mobility scenarios achieving ≥0.93 performance and demonstrates sub-millisecond edge inference via TFLite/ONNX/TensorRT.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Kalogiannis', 'Ahmed Mohamed Hussain', 'Hexu Li', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['vehicular-security', 'misbehavior-detection', 'transformers', 'anomaly-detection', 'edge-deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15503</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title><link>https://arxiv.org/abs/2512.13892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes replacing repeated random permutations for feature importance with a single deterministic, optimal permutation to get faster, non-random, and more stable permutation importance estimates.&lt;/li&gt;&lt;li&gt;Validates the method across ~200 scenarios (including small samples, high dimensionality, low SNR) and reports improved bias–variance tradeoffs and accuracy.&lt;/li&gt;&lt;li&gt;Introduces Systemic Variable Importance, an extension that accounts for feature correlations to stress-test models and quantify how shocks propagate through inputs.&lt;/li&gt;&lt;li&gt;Demonstrates real-world case studies (e.g., credit risk, household finance) showing how the metric can reveal hidden reliance on protected attributes for auditing and regulatory assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Albert Dorador']&lt;/li&gt;&lt;li&gt;Tags: ['variable-importance', 'model-auditing', 'fairness', 'robustness', 'stress-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13892</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title><link>https://arxiv.org/abs/2507.11662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates MLLMs used as verifiers across web navigation, GUI/computer use, and robotic manipulation, identifying a pervasive 'agreement bias' where models over-validate agent behavior.&lt;/li&gt;&lt;li&gt;Shows the bias is robust across models and scaling and undermines methods that rely on MLLM evaluations for safety and benchmarking.&lt;/li&gt;&lt;li&gt;Proposes Self-Grounded Verification (SGV): a two-step method where the MLLM first generates priors about desired behavior, then conditions on those priors to evaluate candidate trajectories.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements (up to +25 pp failure detection, +14 pp accuracy) and downstream task gains (SOTA improvements in multiple environments) and releases an updated VisualWebArena benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moises Andrade', 'Joonhyuk Cha', 'Brandon Ho', 'Vriksha Srihari', 'Karmesh Yadav', 'Zsolt Kira']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'MLLM-verifiers', 'agreement-bias', 'evaluation-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11662</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Computational Basis of LLM's Decision Making in Social Simulation</title><link>https://arxiv.org/abs/2504.11671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods to probe, quantify, and modify LLMs' internal representations using interventions during inference in a Dictator Game setup.&lt;/li&gt;&lt;li&gt;Extracts 'vectors of variable variations' (e.g., male→female) from internal states and shows manipulating these vectors changes decision-making outcomes.&lt;/li&gt;&lt;li&gt;Frames the approach as a way to study, regulate, and engineer social concepts in transformer models with implications for alignment and debiasing.&lt;/li&gt;&lt;li&gt;Demonstrates a practical technique for altering agent behavior in social simulations, informing design of LLM-based decision agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'internal representation probing', 'model editing', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11671</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable deep learning improves human mental models of self-driving cars</title><link>https://arxiv.org/abs/2411.18714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept-Wrapper Network (CW-Net) to ground planner decisions in human-interpretable concepts.&lt;/li&gt;&lt;li&gt;Deployed on a real self-driving car; explanations improved drivers' mental models and their ability to predict vehicle behavior.&lt;/li&gt;&lt;li&gt;Claims explanations are causally faithful and do not degrade driving performance, demonstrating practical utility in realistic settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eoin M. Kenny', 'Akshay Dharmavaram', 'Sang Uk Lee', 'Tung Phan-Minh', 'Shreyas Rajesh', 'Yunqing Hu', 'Laura Major', 'Momchil S. Tomov', 'Julie A. Shah']&lt;/li&gt;&lt;li&gt;Tags: ['explainable AI', 'interpretability', 'autonomous vehicles', 'human-AI interaction', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18714</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</title><link>https://arxiv.org/abs/2512.17586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and evaluates Safety Representations for Safer Policy Learning (SRPL), a predictive model of future constraint violations, to improve safe reinforcement learning for autonomous driving.&lt;/li&gt;&lt;li&gt;Systematic experiments on Waymo Open Motion Dataset and NuPlan show SRPL improves reward-safety tradeoffs — higher success rates and lower costs with statistically significant effect sizes (r = 0.65–0.86 for success, r = 0.70–0.83 for cost).&lt;/li&gt;&lt;li&gt;Finds SRPL’s effectiveness depends on policy optimizer and dataset distribution, but it improves robustness to observation noise and yields better zero-shot cross-dataset generalization versus non-SRPL baselines.&lt;/li&gt;&lt;li&gt;Overall demonstrates practical benefits of predictive safety representations for SafeRL in real-world driving datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahesh Keswani', 'Raunak Bhattacharyya']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'autonomous driving', 'predictive safety representations', 'robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17586</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'confession' mechanism: after a model's main answer, the model outputs a self-report intended to fully disclose compliance issues and shortcomings; confession reward is based solely on honesty and separated from main-answer reward.&lt;/li&gt;&lt;li&gt;Trains a model (GPT-5-Thinking) to produce confessions and evaluates confession honesty out-of-distribution on behaviors including hallucination, instruction following failures, scheming, and reward hacking.&lt;/li&gt;&lt;li&gt;Finds that models often confess to misbehavior even when the main answer omits or lies about it, and that confession honesty improves modestly with training.&lt;/li&gt;&lt;li&gt;Discusses inference-time uses of confessions (monitoring, rejection sampling, surfacing issues to users) as safety interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manas Joglekar', 'Jeremy Chen', 'Gabriel Wu', 'Jason Yosinski', 'Jasmine Wang', 'Boaz Barak', 'Amelia Glaese']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'honesty', 'RLHF/reward shaping', 'safety evaluation', 'red-teaming/monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08093</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>No Trust Issues Here: A Technical Report on the Winning Solutions for the Rayan AI Contest</title><link>https://arxiv.org/abs/2512.01498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Technical report presenting winning solutions for three contest tasks: compositional image retrieval (visual+text), zero-shot image anomaly detection, and backdoored model detection.&lt;/li&gt;&lt;li&gt;Backdoored model detection method aims to identify hidden backdoor triggers in neural networks and achieved ~78% accuracy (2nd place).&lt;/li&gt;&lt;li&gt;Solutions include high-performing methods for retrieval (95.38% top score) and anomaly localization (73.14%), with code released on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (technical report / contest solutions)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Nafisi', 'Sina Asghari', 'Mohammad Saeed Arvenaghi', 'Hossein Shakibania']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-detection', 'model-security', 'anomaly-detection', 'compositional-image-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01498</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</title><link>https://arxiv.org/abs/2509.23129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C^2GSPG, a confidence-calibrated group sequence policy gradient framework to reduce overconfidence and improve self-aware reasoning in LLMs.&lt;/li&gt;&lt;li&gt;Introduces Group Sequence Policy Gradient (GSPG) to remove token-level bias by optimizing sequence-level probabilities and defines model confidence as normalized sequence-level probability.&lt;/li&gt;&lt;li&gt;Adds a cross-entropy regularizer that calibrates model confidence to sequence reward; proves alignment of objectives for binary rewards and uses nonlinear reward normalization plus adaptive regularizer clipping for non-binary rewards.&lt;/li&gt;&lt;li&gt;Applies C^2GSPG to post-train large language models on logical and mathematical reasoning tasks, showing improved reasoning accuracy and confidence calibration over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Liu', 'Shuo Wang', 'Hongteng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'reinforcement learning / policy gradient', 'model alignment / safety', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23129</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack with Partial Features</title><link>https://arxiv.org/abs/2508.06244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Partial Feature Membership Inference (PFMI): adversary sees only a subset of features and must infer whether that subset appeared in training data.&lt;/li&gt;&lt;li&gt;Proposes MRAD, a two-stage attack (memory-guided reconstruction of missing features using the target model's latent memory, then anomaly detection to detect deviation from training distribution).&lt;/li&gt;&lt;li&gt;Works in both white-box and black-box settings and is compatible with off-the-shelf anomaly detectors.&lt;/li&gt;&lt;li&gt;Empirical results show strong performance (e.g., AUC ≈ 0.75 on STL-10 with 60% missing features).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurun Wang', 'Guangrui Liu', 'Xinjie Li', 'Haoyu He', 'Lin Yao', 'Zhongyun Hua', 'Weizhe Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attack', 'model reconstruction', 'anomaly detection', 'black-box/white-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06244</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</title><link>https://arxiv.org/abs/2512.20168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Odysseus, a dual-steganography jailbreak that covertly embeds malicious queries and expected responses into benign-looking images to bypass input/output safety filters in commercial MLLM-integrated systems.&lt;/li&gt;&lt;li&gt;Demonstrates that many commercial multimodal systems assume malicious content must be explicitly visible in a single modality, and this assumption is broken when attackers hide intent across modalities.&lt;/li&gt;&lt;li&gt;Provides extensive experiments on benchmarks and realistic commercial systems, reporting up to 99% attack success and exposing a fundamental blind spot in current defenses.&lt;/li&gt;&lt;li&gt;Argues for rethinking cross-modal security measures and alignment strategies to address covert multimodal adversarial channels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songze Li', 'Jiameng Cheng', 'Yiming Li', 'Xiaojun Jia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'steganography', 'multimodal LLM security', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20168</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</title><link>https://arxiv.org/abs/2512.20012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an edge-cloud-expert cascaded LLM system for telecom Q&amp;A where an edge model handles routine queries, a cloud model handles harder cases, and humans intervene only when needed.&lt;/li&gt;&lt;li&gt;Formulates a misalignment-cost constrained optimization to minimize average processing cost while ensuring automated answers align with expert judgments.&lt;/li&gt;&lt;li&gt;Introduces a statistically rigorous threshold-selection method based on multiple hypothesis testing for knowledge and confidence tests, providing finite-sample guarantees on misalignment risk.&lt;/li&gt;&lt;li&gt;Evaluates on the TeleQnA dataset, showing improved cost-efficiency over baseline cascades while meeting prescribed reliability/confidence levels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiushuo Hou', 'Sangwoo Park', 'Matteo Zecchin', 'Yunlong Cai', 'Guanding Yu', 'Osvaldo Simeone', 'Tommaso Melodia']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-cascades', 'deployment-reliability', 'statistical-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20012</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</title><link>https://arxiv.org/abs/2512.20004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a GNN-based Android malware classifier using API graph embeddings combined with permission and intent features, reporting high accuracy on CICMaldroid and Drebin datasets.&lt;/li&gt;&lt;li&gt;Introduces VGAE-MalGAN, a GAN-style adversarial attack that generates perturbed API graphs to evade GNN malware detectors and demonstrates substantial drops in detection rate.&lt;/li&gt;&lt;li&gt;Shows that adversarial retraining with generated samples can improve robustness and mitigate the attack's effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Yumlembam', 'Biju Issac', 'Seibu Mary Jacob', 'Longzhi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'graph-neural-networks', 'malware-detection', 'adversarial-training', 'IoT-Android-Security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20004</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title><link>https://arxiv.org/abs/2512.19711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PHANTOM, a method that uses perspective-dependent anamorphic art as physical adversarial examples to fool vision-based object detectors in connected autonomous vehicles (CAVs).&lt;/li&gt;&lt;li&gt;Operates in black-box settings with strong transferability across multiple detectors (YOLOv5, SSD, Faster R-CNN, RetinaNet) and activates at 6–10 meters, reducing reaction time for vehicles.&lt;/li&gt;&lt;li&gt;Validated in CARLA across speeds, weather, and lighting (90%+ success in optimal conditions; 60–80% in degraded scenarios) and shown to cause network-level disruption in SUMO-OMNeT++ co-sim by propagating false emergency messages and increasing Peak Age of Information by 68–89%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical adversarial attack', 'black-box attack', 'autonomous vehicles', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19711</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Relu and softplus neural nets as zero-sum turn-based games</title><link>https://arxiv.org/abs/2512.20582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows ReLU neural network outputs can be represented as values of a zero-sum, turn-based stopping game (ReLU net game) and relates network evaluation to Shapley-Bellman recursion.&lt;/li&gt;&lt;li&gt;Derives a discrete Feynman–Kac-style path-integral formula for network outputs via path measures induced by transition probabilities and optimal policies.&lt;/li&gt;&lt;li&gt;Uses the game-theoretic view to derive output bounds from input bounds and to verify robustness properties using policies as certificates.&lt;/li&gt;&lt;li&gt;Extends the approach to Softplus networks via entropic regularization and frames training as an inverse game problem (recovering transition probabilities and rewards).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Gaubert', 'Yiannis Vlassopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['robustness_verification', 'theoretical_interpretability', 'formal_methods', 'activation_functions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20582</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</title><link>https://arxiv.org/abs/2512.20233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically investigates how diffusion model sampling algorithms and their hyperparameters affect bias amplification in generated images.&lt;/li&gt;&lt;li&gt;Shows that sampling choices can both amplify and reduce biases even when the trained model is unchanged, using controlled experiments on Biased MNIST, Multi-Color MNIST, BFFHQ, and Stable Diffusion.&lt;/li&gt;&lt;li&gt;Provides actionable analysis and code to reproduce findings, highlighting that post-training sampling decisions are an important factor for fairness and safety of generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Roos', 'Ekaterina Iakovleva', 'Ani Gjergji', 'Vito Paolo Pastore', 'Enzo Tartaglione']&lt;/li&gt;&lt;li&gt;Tags: ['bias-amplification', 'fairness-safety', 'diffusion-models', 'sampling-hyperparameters', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20233</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud</title><link>https://arxiv.org/abs/2512.20218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cost-TrustFL, a hierarchical multi-cloud federated learning framework that jointly optimizes model performance and cross-cloud communication costs while defending against poisoning attacks.&lt;/li&gt;&lt;li&gt;Introduces a gradient-based approximate Shapley value method reducing complexity from exponential to linear for lightweight reputation evaluation of clients.&lt;/li&gt;&lt;li&gt;Implements a cost-aware aggregation strategy that prioritizes intra-cloud communication to minimize expensive cross-cloud egress fees.&lt;/li&gt;&lt;li&gt;Evaluated on CIFAR-10 and FEMNIST, achieving 86.7% accuracy with 30% malicious clients and reducing communication costs by ~32% versus baselines, with stability across non-IID settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jixiao Yang', 'Jinyu Chen', 'Zixiao Huang', 'Chengda Xu', 'Chi Zhang', 'Sijia Li']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'data-poisoning', 'byzantine-robustness', 'reputation-evaluation', 'cost-aware-aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20218</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</title><link>https://arxiv.org/abs/2512.19935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Conditional Adversarial Fragility': adversarial vulnerability that is systematically amplified during macroeconomic stress regimes.&lt;/li&gt;&lt;li&gt;Presents a regime-aware evaluation framework for time-indexed tabular financial classification, using volatility-based regime segmentation to compare calm vs stress periods while holding models and attack protocols constant.&lt;/li&gt;&lt;li&gt;Finds baseline predictive performance similar across regimes but shows substantially greater degradation under adversarial perturbations in stress periods, including higher false negative rates and risk-sensitive outcome impacts.&lt;/li&gt;&lt;li&gt;Augments numerical robustness metrics with a semantic governance layer using large language models to audit model explanations and support stress-aware model risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samruddhi Baviskar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'financial ML', 'stress testing', 'model risk', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19935</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behaviorally calibrated reinforcement learning that trains LLMs to output calibrated probabilities of correctness (and optionally abstain or flag uncertain claims) by optimizing strictly proper scoring rules.&lt;/li&gt;&lt;li&gt;Argues standard RL with binary rewards encourages guessing; behavioral calibration incentivizes stochastic admission of uncertainty and aligns model outputs with accuracy.&lt;/li&gt;&lt;li&gt;Empirical results on Qwen3-4B-Instruct show improved uncertainty quantification: a large Accuracy-to-Hallucination Ratio gain on BeyondAIME and zero-shot calibration error on SimpleQA comparable to frontier models despite lower raw accuracy.&lt;/li&gt;&lt;li&gt;Claims the calibration capability is a transferable meta-skill decouplable from predictive accuracy and beneficial for reducing hallucinations in critical deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination_mitigation', 'calibration', 'reinforcement_learning', 'uncertainty_estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</title><link>https://arxiv.org/abs/2512.19744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DeepBridge is an 80K-line Python framework for multi-dimensional ML validation, offering suites for fairness (15 metrics), robustness (weakness detection), uncertainty (conformal prediction), resilience to drift, and hyperparameter sensitivity.&lt;/li&gt;&lt;li&gt;Provides automatic compliance verification (EEOC/ECOA/GDPR), multi-format audit-ready reporting, scalable synthetic data generation, and an HPM-KD knowledge distillation framework with empirical gains on CIFAR100.&lt;/li&gt;&lt;li&gt;Demonstrated production utility across six domain case studies and usability testing, claiming large reductions in validation time and improved coverage of fairness issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gustavo Coelho Haase', 'Paulo Henrique Dourado da Silva']&lt;/li&gt;&lt;li&gt;Tags: ['Model validation', 'Robustness', 'Fairness', 'Compliance/GDPR', 'Synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19744</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures</title><link>https://arxiv.org/abs/2512.19730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that existing learning-based neural backdoor detectors fail to generalize to unseen model architectures and analyzes the root cause.&lt;/li&gt;&lt;li&gt;Proposes ArcGen, a black-box detection method that adds an alignment layer and trains feature extractors with two alignment losses to produce architecture-invariant features at both distribution and sample levels.&lt;/li&gt;&lt;li&gt;Demonstrates large improvements (up to 42.5% AUC) on unseen architectures across a large-scale evaluation of 16,896 models, diverse datasets, and multiple backdoor attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhonghao Yang', 'Cheng Luo', 'Daojing He', 'Yiming Li', 'Yu Li']&lt;/li&gt;&lt;li&gt;Tags: ['neural backdoors', 'backdoor detection', 'model security', 'black-box detection', 'robustness/generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19730</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</title><link>https://arxiv.org/abs/2512.19725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines out-of-distribution (OOD) detection methods specifically in the context of continual learning, where models must handle evolving data streams without retraining from scratch.&lt;/li&gt;&lt;li&gt;Provides design principles and benchmarking for evaluating OOD performance under continual learning constraints, emphasizing robustness and reliable model behavior over time.&lt;/li&gt;&lt;li&gt;Targets practical deployment scenarios (e.g., malware detection, autonomous driving) where detecting novel or anomalous inputs is critical for safety and system reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srishti Gupta', 'Riccardo Balia', 'Daniele Angioni', 'Fabio Brau', 'Maura Pintor', 'Ambra Demontis', 'Alessandro Sebastian', 'Salvatore Mario Carta', 'Fabio Roli', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'continual learning', 'robustness', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19725</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title><link>https://arxiv.org/abs/2512.19025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that standard unlearning evaluations for LLMs (measuring degradation on the exact unlearning dataset D_u) can be misleading because models may retain semantically adjacent knowledge.&lt;/li&gt;&lt;li&gt;Introduces Proximal Surrogate Generation (PSG), an automated stress-testing framework that creates semantically-derived but embedding-distant surrogate datasets (\tilde{D}_u) to probe retained knowledge.&lt;/li&gt;&lt;li&gt;Evaluates across three LLM families (Llama-3-8B, Qwen2.5-7B, Zephyr-7B-β), three datasets, and seven common unlearning metrics, finding widespread inconsistencies and that metrics often overestimate unlearning success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengrui Jia', 'Taoran Li', 'Jonas Guan', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'LLM-evaluation', 'privacy', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19025</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title><link>https://arxiv.org/abs/2512.15503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIMformer, a transformer-based misbehavior detection model for vehicular platooning that captures intra-vehicle temporal dynamics and inter-vehicle spatial correlations.&lt;/li&gt;&lt;li&gt;Proposes a Precision-Focused Binary Cross-Entropy (PFBCE) loss to penalize false positives for safety-critical deployment.&lt;/li&gt;&lt;li&gt;Evaluates across multiple platoon controllers, attack vectors, and mobility scenarios achieving ≥0.93 performance and demonstrates sub-millisecond edge inference via TFLite/ONNX/TensorRT.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Kalogiannis', 'Ahmed Mohamed Hussain', 'Hexu Li', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['vehicular-security', 'misbehavior-detection', 'transformers', 'anomaly-detection', 'edge-deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15503</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title><link>https://arxiv.org/abs/2512.13892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes replacing repeated random permutations for feature importance with a single deterministic, optimal permutation to get faster, non-random, and more stable permutation importance estimates.&lt;/li&gt;&lt;li&gt;Validates the method across ~200 scenarios (including small samples, high dimensionality, low SNR) and reports improved bias–variance tradeoffs and accuracy.&lt;/li&gt;&lt;li&gt;Introduces Systemic Variable Importance, an extension that accounts for feature correlations to stress-test models and quantify how shocks propagate through inputs.&lt;/li&gt;&lt;li&gt;Demonstrates real-world case studies (e.g., credit risk, household finance) showing how the metric can reveal hidden reliance on protected attributes for auditing and regulatory assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Albert Dorador']&lt;/li&gt;&lt;li&gt;Tags: ['variable-importance', 'model-auditing', 'fairness', 'robustness', 'stress-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13892</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'confession' mechanism: after a model's main answer, the model outputs a self-report intended to fully disclose compliance issues and shortcomings; confession reward is based solely on honesty and separated from main-answer reward.&lt;/li&gt;&lt;li&gt;Trains a model (GPT-5-Thinking) to produce confessions and evaluates confession honesty out-of-distribution on behaviors including hallucination, instruction following failures, scheming, and reward hacking.&lt;/li&gt;&lt;li&gt;Finds that models often confess to misbehavior even when the main answer omits or lies about it, and that confession honesty improves modestly with training.&lt;/li&gt;&lt;li&gt;Discusses inference-time uses of confessions (monitoring, rejection sampling, surfacing issues to users) as safety interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manas Joglekar', 'Jeremy Chen', 'Gabriel Wu', 'Jason Yosinski', 'Jasmine Wang', 'Boaz Barak', 'Amelia Glaese']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'honesty', 'RLHF/reward shaping', 'safety evaluation', 'red-teaming/monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08093</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title><link>https://arxiv.org/abs/2511.06148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLMs can spontaneously develop novel social biases about synthetic demographic groups through exploration-exploitation dynamics, producing highly stratified task allocations.&lt;/li&gt;&lt;li&gt;Finds that newer and larger models tend to exhibit stronger emergent stratification, often exceeding unfairness observed in human allocations.&lt;/li&gt;&lt;li&gt;Evaluates interventions targeting inputs, problem structure, and explicit steering; explicitly incentivizing exploration most robustly reduces stratification.&lt;/li&gt;&lt;li&gt;Argues that LLMs can actively create new biases from experience, with implications for alignment, fairness, and societal impacts as models are granted decision-making roles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Addison J. Wu', 'Ryan Liu', 'Xuechunzi Bai', 'Thomas L. Griffiths']&lt;/li&gt;&lt;li&gt;Tags: ['emergent bias', 'LLM alignment', 'fairness', 'bias mitigation', 'exploration-exploitation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06148</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</title><link>https://arxiv.org/abs/2509.23129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C^2GSPG, a confidence-calibrated group sequence policy gradient framework to reduce overconfidence and improve self-aware reasoning in LLMs.&lt;/li&gt;&lt;li&gt;Introduces Group Sequence Policy Gradient (GSPG) to remove token-level bias by optimizing sequence-level probabilities and defines model confidence as normalized sequence-level probability.&lt;/li&gt;&lt;li&gt;Adds a cross-entropy regularizer that calibrates model confidence to sequence reward; proves alignment of objectives for binary rewards and uses nonlinear reward normalization plus adaptive regularizer clipping for non-binary rewards.&lt;/li&gt;&lt;li&gt;Applies C^2GSPG to post-train large language models on logical and mathematical reasoning tasks, showing improved reasoning accuracy and confidence calibration over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Liu', 'Shuo Wang', 'Hongteng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'reinforcement learning / policy gradient', 'model alignment / safety', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23129</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack with Partial Features</title><link>https://arxiv.org/abs/2508.06244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Partial Feature Membership Inference (PFMI): adversary sees only a subset of features and must infer whether that subset appeared in training data.&lt;/li&gt;&lt;li&gt;Proposes MRAD, a two-stage attack (memory-guided reconstruction of missing features using the target model's latent memory, then anomaly detection to detect deviation from training distribution).&lt;/li&gt;&lt;li&gt;Works in both white-box and black-box settings and is compatible with off-the-shelf anomaly detectors.&lt;/li&gt;&lt;li&gt;Empirical results show strong performance (e.g., AUC ≈ 0.75 on STL-10 with 60% missing features).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurun Wang', 'Guangrui Liu', 'Xinjie Li', 'Haoyu He', 'Lin Yao', 'Zhongyun Hua', 'Weizhe Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attack', 'model reconstruction', 'anomaly detection', 'black-box/white-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06244</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History</title><link>https://arxiv.org/abs/2508.04826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PERSIST, an evaluation framework measuring personality stability across 25 open-source LLMs (1B–685B) with 2M+ responses using traditional and LLM-adapted questionnaires.&lt;/li&gt;&lt;li&gt;Finds large, persistent variability in measured personality: question order, paraphrasing, reasoning modes, conversation history, and persona prompts often produce substantial shifts, even for very large models.&lt;/li&gt;&lt;li&gt;Shows scaling yields limited stability gains and some mitigation strategies (reasoning, history) can increase variability; LLM-adapted questionnaires are as unstable as human-centric ones.&lt;/li&gt;&lt;li&gt;Concludes current LLM architectures and alignment strategies may not provide the behavioral consistency required for safety-critical, predictable deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tommaso Tosato', 'Saskia Helbling', 'Yorguin-Jose Mantilla-Ramos', 'Mahmood Hegazy', 'Alberto Tosato', 'David John Lemay', 'Irina Rish', 'Guillaume Dumas']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'behavioral robustness', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04826</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collision-based Watermark for Detecting Backdoor Manipulation in Federated Learning</title><link>https://arxiv.org/abs/2508.02115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Coward, a proactive detection method for backdoor manipulation in federated learning that injects a collision-based watermark into the global model.&lt;/li&gt;&lt;li&gt;Leverages multi-backdoor collision effects—where later backdoors suppress earlier ones—and uses regulated dual-mapping learning on OOD data to build a low-disruptive, detectable watermark.&lt;/li&gt;&lt;li&gt;Claims improved robustness to non-i.i.d. data, random client participation, and OOD prediction bias common in FL, with state-of-the-art detection performance and resistance to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Siying Gu', 'Yiming Li', 'Kangjie Chen', 'Zhili Chen', 'Tianwei Zhang', 'Shu-Tao Xia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'backdoor-detection', 'model-watermarking', 'OOD-robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02115</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2505.02824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Investigates how adversaries can evade dataset ownership verification (DOV) watermarks in personalized text-to-image (T2I) diffusion models, enabling models trained on watermarked datasets to bypass ownership checks.&lt;/li&gt;&lt;li&gt;Prior defenses: Analyzes limitations of existing backdoor removal approaches (TPD suffers from randomness; T2IShield fails for local patch-based watermarks).&lt;/li&gt;&lt;li&gt;Proposed attack (CEAT2I): (1) detect watermarked samples via faster convergence in intermediate features, (2) iteratively ablate prompt tokens and monitor feature shifts to identify trigger tokens, (3) apply a closed-form concept erasure to remove injected watermarks while preserving model performance.&lt;/li&gt;&lt;li&gt;Results: Demonstrates effective evasion of state-of-the-art DOV mechanisms on T2I diffusion models with experiments and provides code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yufei Zhu', 'Yiming Li', 'Jiawang Bai', 'Yong Yang', 'Zhifeng Li', 'Shu-Tao Xia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ML', 'backdoor-removal', 'model-watermarking', 'text-to-image', 'copyright-evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02824</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable deep learning improves human mental models of self-driving cars</title><link>https://arxiv.org/abs/2411.18714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept-Wrapper Network (CW-Net) to ground planner decisions in human-interpretable concepts.&lt;/li&gt;&lt;li&gt;Deployed on a real self-driving car; explanations improved drivers' mental models and their ability to predict vehicle behavior.&lt;/li&gt;&lt;li&gt;Claims explanations are causally faithful and do not degrade driving performance, demonstrating practical utility in realistic settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eoin M. Kenny', 'Akshay Dharmavaram', 'Sang Uk Lee', 'Tung Phan-Minh', 'Shreyas Rajesh', 'Yunqing Hu', 'Laura Major', 'Momchil S. Tomov', 'Julie A. Shah']&lt;/li&gt;&lt;li&gt;Tags: ['explainable AI', 'interpretability', 'autonomous vehicles', 'human-AI interaction', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18714</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dialectics for Artificial Intelligence</title><link>https://arxiv.org/abs/2512.17373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an algorithmic-information-theoretic definition of a "concept" as an information object determined by its structural relation to an agent's total experience.&lt;/li&gt;&lt;li&gt;Introduces 'determination' (reversible consistency) to ensure concepts are recoverable parts of experience and defines 'excess information' to assess whether a decomposition is natural.&lt;/li&gt;&lt;li&gt;Frames dialectics as an optimization dynamics where competing concepts explain new or contested information via shorter conditional descriptions, driving splitting/merging of concepts.&lt;/li&gt;&lt;li&gt;Formalizes low-cost concept transmission and multi-agent alignment via small shared seeds/grounds that let another agent reconstruct the same concept under a shared protocol.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengmian Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'algorithmic information theory', 'concept learning', 'multi-agent communication', 'foundations/theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17373</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title><link>https://arxiv.org/abs/2507.11662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates MLLMs used as verifiers across web navigation, GUI/computer use, and robotic manipulation, identifying a pervasive 'agreement bias' where models over-validate agent behavior.&lt;/li&gt;&lt;li&gt;Shows the bias is robust across models and scaling and undermines methods that rely on MLLM evaluations for safety and benchmarking.&lt;/li&gt;&lt;li&gt;Proposes Self-Grounded Verification (SGV): a two-step method where the MLLM first generates priors about desired behavior, then conditions on those priors to evaluate candidate trajectories.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements (up to +25 pp failure detection, +14 pp accuracy) and downstream task gains (SOTA improvements in multiple environments) and releases an updated VisualWebArena benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moises Andrade', 'Joonhyuk Cha', 'Brandon Ho', 'Vriksha Srihari', 'Karmesh Yadav', 'Zsolt Kira']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'MLLM-verifiers', 'agreement-bias', 'evaluation-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11662</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Computational Basis of LLM's Decision Making in Social Simulation</title><link>https://arxiv.org/abs/2504.11671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods to probe, quantify, and modify LLMs' internal representations using interventions during inference in a Dictator Game setup.&lt;/li&gt;&lt;li&gt;Extracts 'vectors of variable variations' (e.g., male→female) from internal states and shows manipulating these vectors changes decision-making outcomes.&lt;/li&gt;&lt;li&gt;Frames the approach as a way to study, regulate, and engineer social concepts in transformer models with implications for alignment and debiasing.&lt;/li&gt;&lt;li&gt;Demonstrates a practical technique for altering agent behavior in social simulations, informing design of LLM-based decision agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'internal representation probing', 'model editing', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11671</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit</title><link>https://arxiv.org/abs/2512.20423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an end-to-end, containerized pipeline to generate, intercept, and analyze DNS-over-HTTPS (DoH) file exfiltration with configurable evasion parameters (chunking, encoding, padding, resolver rotation).&lt;/li&gt;&lt;li&gt;Extracts flow-level features, trains ML classifiers (Random Forest, Gradient Boosting, Logistic Regression) on a public DoH dataset, and benchmarks ML models against threshold-based detectors under evasive scenarios.&lt;/li&gt;&lt;li&gt;Provides a reproducible toolkit (Dockerized) for traffic generation, capture, feature extraction, model training, and evaluation; aims to quantify when stealth constraints make DoH exfiltration uneconomical.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Elaoumari']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-evasion', 'network-security', 'ML-detection', 'DNS-over-HTTPS', 'tooling/reproducibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20423</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</title><link>https://arxiv.org/abs/2512.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KnowVal, an autonomous driving system combining a driving knowledge graph (traffic laws, defensive driving, ethical norms) with an LLM-based retrieval mechanism for visual-language reasoning in driving scenarios.&lt;/li&gt;&lt;li&gt;Introduces a human-preference dataset and trains a Value Model to guide interpretable, value-aligned trajectory assessment and planning.&lt;/li&gt;&lt;li&gt;Demonstrates improved planning performance and reduced collision rates on benchmarks (nuScenes, Bench2Drive) while remaining compatible with existing architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongyu Xia', 'Wenhao Chen', 'Yongtao Wang', 'Ming-Hsuan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'value-alignment', 'safety', 'knowledge-graph', 'LLM-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20299</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithLens, an 8B-parameter model for detecting faithfulness hallucinations in LLM outputs and producing explanations.&lt;/li&gt;&lt;li&gt;Synthesizes labeled training data with explanations using advanced LLMs, applies filtering for quality/diversity, then fine-tunes and refines with rule-based reinforcement learning rewarding prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluates across 12 tasks and claims performance surpassing advanced models (e.g., GPT-4.1) while being cost-efficient and offering interpretable explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness detection', 'hallucination', 'alignment/safety', 'explainability', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</title><link>https://arxiv.org/abs/2512.20168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Odysseus, a dual-steganography jailbreak that covertly embeds malicious queries and expected responses into benign-looking images to bypass input/output safety filters in commercial MLLM-integrated systems.&lt;/li&gt;&lt;li&gt;Demonstrates that many commercial multimodal systems assume malicious content must be explicitly visible in a single modality, and this assumption is broken when attackers hide intent across modalities.&lt;/li&gt;&lt;li&gt;Provides extensive experiments on benchmarks and realistic commercial systems, reporting up to 99% attack success and exposing a fundamental blind spot in current defenses.&lt;/li&gt;&lt;li&gt;Argues for rethinking cross-modal security measures and alignment strategies to address covert multimodal adversarial channels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songze Li', 'Jiameng Cheng', 'Yiming Li', 'Xiaojun Jia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'steganography', 'multimodal LLM security', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20168</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</title><link>https://arxiv.org/abs/2512.20164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLM-based resume screening is vulnerable to "adversarial instructions" embedded in input documents, with some attacks achieving &gt;80% success.&lt;/li&gt;&lt;li&gt;Provides a benchmark to evaluate these attacks on resume screening and measures attack success and utility impacts.&lt;/li&gt;&lt;li&gt;Compares defenses: prompt-based mitigation (10.1% attack reduction, 12.5% false rejection increase), FIDS (LoRA adaptation) (15.4% attack reduction, 10.4% false rejection increase), and a combined approach yielding 26.3% attack reduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honglin Mu', 'Jinghao Liu', 'Kaiyang Wan', 'Rui Xing', 'Xiuying Chen', 'Timothy Baldwin', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'LLM defenses', 'benchmarking', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20164</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</title><link>https://arxiv.org/abs/2512.20004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a GNN-based Android malware classifier using API graph embeddings combined with permission and intent features, reporting high accuracy on CICMaldroid and Drebin datasets.&lt;/li&gt;&lt;li&gt;Introduces VGAE-MalGAN, a GAN-style adversarial attack that generates perturbed API graphs to evade GNN malware detectors and demonstrates substantial drops in detection rate.&lt;/li&gt;&lt;li&gt;Shows that adversarial retraining with generated samples can improve robustness and mitigate the attack's effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Yumlembam', 'Biju Issac', 'Seibu Mary Jacob', 'Longzhi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'graph-neural-networks', 'malware-detection', 'adversarial-training', 'IoT-Android-Security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20004</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</title><link>https://arxiv.org/abs/2512.19935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Conditional Adversarial Fragility': adversarial vulnerability that is systematically amplified during macroeconomic stress regimes.&lt;/li&gt;&lt;li&gt;Presents a regime-aware evaluation framework for time-indexed tabular financial classification, using volatility-based regime segmentation to compare calm vs stress periods while holding models and attack protocols constant.&lt;/li&gt;&lt;li&gt;Finds baseline predictive performance similar across regimes but shows substantially greater degradation under adversarial perturbations in stress periods, including higher false negative rates and risk-sensitive outcome impacts.&lt;/li&gt;&lt;li&gt;Augments numerical robustness metrics with a semantic governance layer using large language models to audit model explanations and support stress-aware model risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samruddhi Baviskar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'financial ML', 'stress testing', 'model risk', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19935</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behaviorally calibrated reinforcement learning that trains LLMs to output calibrated probabilities of correctness (and optionally abstain or flag uncertain claims) by optimizing strictly proper scoring rules.&lt;/li&gt;&lt;li&gt;Argues standard RL with binary rewards encourages guessing; behavioral calibration incentivizes stochastic admission of uncertainty and aligns model outputs with accuracy.&lt;/li&gt;&lt;li&gt;Empirical results on Qwen3-4B-Instruct show improved uncertainty quantification: a large Accuracy-to-Hallucination Ratio gain on BeyondAIME and zero-shot calibration error on SimpleQA comparable to frontier models despite lower raw accuracy.&lt;/li&gt;&lt;li&gt;Claims the calibration capability is a transferable meta-skill decouplable from predictive accuracy and beneficial for reducing hallucinations in critical deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination_mitigation', 'calibration', 'reinforcement_learning', 'uncertainty_estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title><link>https://arxiv.org/abs/2512.19711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PHANTOM, a method that uses perspective-dependent anamorphic art as physical adversarial examples to fool vision-based object detectors in connected autonomous vehicles (CAVs).&lt;/li&gt;&lt;li&gt;Operates in black-box settings with strong transferability across multiple detectors (YOLOv5, SSD, Faster R-CNN, RetinaNet) and activates at 6–10 meters, reducing reaction time for vehicles.&lt;/li&gt;&lt;li&gt;Validated in CARLA across speeds, weather, and lighting (90%+ success in optimal conditions; 60–80% in degraded scenarios) and shown to cause network-level disruption in SUMO-OMNeT++ co-sim by propagating false emergency messages and increasing Peak Age of Information by 68–89%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical adversarial attack', 'black-box attack', 'autonomous vehicles', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19711</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Brain-Grounded Axes for Reading and Steering LLM States</title><link>https://arxiv.org/abs/2512.19399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives latent 'brain-grounded' axes from MEG phase-locking value (PLV) patterns using ICA on the SMN4Lang dataset to form a word-level brain atlas.&lt;/li&gt;&lt;li&gt;Trains lightweight adapters that map LLM hidden states to these brain axes (without fine-tuning the LLM) and uses steering along axes to change model behavior.&lt;/li&gt;&lt;li&gt;Finds robust lexical (frequency-linked) and function/content axes that produce consistent steering effects across TinyLlama, Qwen2-0.5B, and GPT-2, with controls for perplexity and embedding choices.&lt;/li&gt;&lt;li&gt;Provides exploratory fMRI anchoring and stability analyses to reduce circularity concerns, proposing neurophysiology-grounded axes as interpretable, controllable handles for LLM behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandro Andric']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'LLM steering/control', 'neurophysiology-grounding', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19399</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</title><link>https://arxiv.org/abs/2512.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SAGE, an LLM-based agent for automated stereotactic radiosurgery (SRS) planning, comparing a chain-of-thought (reasoning) variant to a non-reasoning variant across 41 retrospective cases.&lt;/li&gt;&lt;li&gt;The reasoning agent produced clinically comparable dosimetry to human planners on primary endpoints and reduced cochlear dose significantly (p = 0.022).&lt;/li&gt;&lt;li&gt;Reasoning-agent behavior included systematic prospective constraint verification and trade-off deliberation, logged as auditable optimization traces, whereas the standard model lacked these deliberative processes.&lt;/li&gt;&lt;li&gt;Authors argue these deliberative traces support transparency and a path toward safer, more auditable automated clinical planning with human-in-the-loop oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humza Nusrat', 'Luke Francisco', 'Bing Luo', 'Hassan Bagher-Ebadian', 'Joshua Kim', 'Karen Chin-Snyder', 'Salim Siddiqui', 'Mira Shah', 'Eric Mellon', 'Mohammad Ghassemi', 'Anthony Doemer', 'Benjamin Movsas', 'Kundan Thind']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Clinical AI safety', 'Transparency / auditability', 'Human-in-the-loop', 'Automated treatment planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20586</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks</title><link>https://arxiv.org/abs/2512.20275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes G‑SPEC, a neuro-symbolic framework that constrains LLM-based telecom agents (TSLAM-4B) with a Network Knowledge Graph (NKG) and SHACL constraints to provide deterministic verification of probabilistic plans.&lt;/li&gt;&lt;li&gt;Targets safety risks from agentic LLMs in 5G/6G orchestration (topology hallucinations, policy non-compliance) and demonstrates zero safety violations and 94.1% remediation success on a 450-node simulated 5G Core.&lt;/li&gt;&lt;li&gt;Includes ablation showing NKG validation accounts for the majority of safety gains (68%) with SHACL contributing 24%, and reports scalability/latency results (validation ~O(k^1.2), processing overhead ~142ms) suitable for SMO-layer operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Vijay', 'Vignesh Ethiraj']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agent safety', 'neuro-symbolic verification', 'policy enforcement', 'network security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20275</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Offline Safe Policy Optimization From Heterogeneous Feedback</title><link>https://arxiv.org/abs/2512.20173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework that learns policies directly from pairwise preference labels and binary safety labels on trajectory segments, rather than first learning reward and cost models.&lt;/li&gt;&lt;li&gt;Proposes PreSa (Preference and Safety Alignment), which formulates a constrained optimization solved via a Lagrangian approach to directly learn reward-maximizing safe policies without constrained RL on learned models.&lt;/li&gt;&lt;li&gt;Evaluates on continuous-control tasks with synthetic and real human feedback, showing improved safety and higher rewards compared to state-of-the-art baselines and offline safe RL methods that use ground-truth reward and cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ze Gong', 'Pradeep Varakantham', 'Akshat Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['Safe RL', 'RLHF', 'Offline RL', 'Preference-based RL', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20173</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Reinforcement Learning for Content Moderation with Large Language Models</title><link>https://arxiv.org/abs/2512.20061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study of scaling reinforcement learning (RL) to train LLM-based classifiers for content moderation across three real-world tasks.&lt;/li&gt;&lt;li&gt;Evaluates multiple RL training recipes and reward-shaping strategies, including verifiable rewards and an LLM-as-judge framework for policy-aligned labeling.&lt;/li&gt;&lt;li&gt;Finds sigmoid-like scaling behavior: performance improves with more data/rollouts/optimization before saturating, and RL can be up to 100x more data-efficient than supervised fine-tuning for complex, policy-grounded reasoning.&lt;/li&gt;&lt;li&gt;Provides actionable insights for industrial-scale moderation systems in low-label, evolving-policy regimes where nuanced reasoning is required.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamed Firooz', 'Rui Liu', 'Yuchen Lu', 'Zhenyu Hou', 'Fangzhou Xiong', 'Xiaoyang Zhang', 'Changshu Jian', 'Zhicheng Zhu', 'Jiayuan Ma', 'Jacob Tao', 'Chaitali Gupta', 'Xiaochang Peng', 'Shike Mei', 'Hang Cui', 'Yang Qin', 'Shuo Tang', 'Jason Gaedtke', 'Arpit Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'alignment', 'reinforcement-learning', 'reward-shaping', 'LLM-as-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20061</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
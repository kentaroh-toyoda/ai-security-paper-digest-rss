<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 19 Dec 2025 23:43:58 +0000</lastBuildDate><item><title>Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation</title><link>https://arxiv.org/abs/2512.14054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scale-adaptive dual-expert perception framework using two YOLOv8 models specialized for far-range (small helipads) and close-range (large helipads) detection.&lt;/li&gt;&lt;li&gt;Uses a geometric gating mechanism at inference to select the expert whose prediction best matches the vehicle viewpoint, running both experts in parallel.&lt;/li&gt;&lt;li&gt;Evaluated in closed-loop simulation (CARLA for visuals, NASA GUAM for flight dynamics) showing improved alignment stability, landing accuracy, and robustness versus single-detector baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humaira Tasnim', 'Ashik E Rasul', 'Bruce Jo', 'Hyung-Jin Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'autonomous systems safety', 'computer vision', 'object detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14054</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title><link>https://arxiv.org/abs/2511.13654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical and theoretical analysis of how training hyperparameters (learning rate, weight decay, momentum, batch size) affect robustness to transfer-based and query-based adversarial attacks.&lt;/li&gt;&lt;li&gt;Key finding: decreasing learning rate substantially improves robustness to transfer-based attacks (up to 64%), while increasing learning rate improves robustness to query-based attacks (up to 28%).&lt;/li&gt;&lt;li&gt;Explores joint hyperparameter tuning across centralized, ensemble, and distributed training, showing distributed models achieve the best tradeoff against both attack types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pascal Zimmer', 'Ghassan Karame']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'transfer-based attacks', 'query-based attacks', 'hyperparameter tuning', 'distributed training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13654</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title><link>https://arxiv.org/abs/2507.21503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoHoBench, a large-scale benchmark (12k+ samples) of unanswerable visual questions to assess honesty of multimodal large language models (MLLMs).&lt;/li&gt;&lt;li&gt;Defines four types of visually unanswerable questions, constructs dataset with multi-stage filtering and human verification, and evaluates 28 popular MMLMs.&lt;/li&gt;&lt;li&gt;Finds most models fail to correctly refuse or admit lack of visual information, showing honesty depends on multimodal perception as well as language modeling.&lt;/li&gt;&lt;li&gt;Proposes initial supervised and preference-learning alignment methods to improve refusal/honesty behavior and releases data and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxu Zhu', 'Shitong Duan', 'Xiangxu Zhang', 'Jitao Sang', 'Peng Zhang', 'Tun Lu', 'Xiao Zhou', 'Jing Yao', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-honesty', 'honesty-alignment', 'safety-evaluation', 'refusal-detection', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21503</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Laws for Black box Adversarial Attacks</title><link>https://arxiv.org/abs/2411.16782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a universal log-linear scaling law: Attack Success Rate (ASR) scales linearly with log(ensemble size T) when gradient conflicts are resolved with advanced optimizers.&lt;/li&gt;&lt;li&gt;Performs large-scale empirical validation across standard classifiers, state-of-the-art defenses, and multimodal LLMs (MLLMs), showing improved black-box transferability from larger surrogate ensembles.&lt;/li&gt;&lt;li&gt;Shows scaling distills robust, semantic features and demonstrates high transfer attack success (80%+) on proprietary models like GPT-4o while highlighting relative robustness of Claude-3.5-Sonnet.&lt;/li&gt;&lt;li&gt;Argues for shifting robustness evaluation from small, intricate ensemble algorithms to principled study of ensemble scaling as a powerful threat vector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuan Liu', 'Huanran Chen', 'Yichi Zhang', 'Jun Zhu', 'Yinpeng Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'black-box', 'model-ensembling', 'transferability', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16782</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memory Backdoor Attacks on Neural Networks</title><link>https://arxiv.org/abs/2411.14516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a training-time backdoor attack that implants an index-triggered memory enabling deterministic, exact extraction of client training samples in federated learning.&lt;/li&gt;&lt;li&gt;Attack can control which samples are memorized and how many, is high-capacity and robust, and recombines extracted patches to overcome small model output sizes.&lt;/li&gt;&lt;li&gt;Demonstrates practical threat across classifiers, medical segmentation models, and large language models with minimal utility degradation, highlighting a realistic FL supply-chain vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eden Luzon', 'Guy Amit', 'Roy Weiss', 'Torsten Kraub', 'Alexandra Dmitrienko', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'dataset extraction', 'federated learning', 'privacy/data exfiltration', 'training-time supply-chain attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14516</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title><link>https://arxiv.org/abs/2512.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video Reality Test, an ASMR-sourced benchmark of audio-paired videos to evaluate perceptual realism and audio-visual consistency.&lt;/li&gt;&lt;li&gt;Uses an adversarial creator-reviewer protocol where video generation models attempt to fool reviewers and VLMs act as reviewers to detect fakes.&lt;/li&gt;&lt;li&gt;Finds state-of-the-art generator (Veo3.1-Fast) can fool many VLMs (best reviewer ≈56% accuracy) while humans far outperform models; audio helps detection but artifacts/watermarks still mislead models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Wang', 'Weijia Wu', 'Yi Zhan', 'Rui Zhao', 'Ming Hu', 'James Cheng', 'Wei Liu', 'Philip Torr', 'Kevin Qinghong Lin']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'VLM robustness', 'benchmarking', 'audio-visual consistency', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13281</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MoAPT: Mixture of Adversarial Prompt Tuning for Vision-Language Models</title><link>https://arxiv.org/abs/2505.17509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of pre-trained vision-language models (VLMs) to adversarial examples and focuses on improving adversarial robustness without changing model parameters.&lt;/li&gt;&lt;li&gt;Proposes Mixture of Adversarial Prompt Tuning (MoAPT): learn multiple text prompts and a conditional weight router that predicts sample-specific mixture weights for aligning text features with adversarial image features.&lt;/li&gt;&lt;li&gt;Empirical finding that increasing the number of learned prompts yields better robustness than lengthening a single prompt; demonstrates improved robustness across 11 datasets and various attack settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiji Zhao', 'Qihui Zhu', 'Shukun Xiong', 'Shouwei Ruan', 'Maoxun Yuan', 'Jialing Tao', 'Jiexi Liu', 'Ranjie Duan', 'Jie Zhang', 'Jie Zhang', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'prompt tuning', 'adversarial examples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17509</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</title><link>https://arxiv.org/abs/2407.20836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FPBA (Frequency-based Post-train Bayesian Attack): combines frequency-domain perturbations with a post-train Bayesian strategy to turn a single surrogate into an ensemble-like Bayesian surrogate to improve transferability.&lt;/li&gt;&lt;li&gt;Targets AI-generated image (AIGI) detectors under white-box and black-box settings, demonstrating successful attacks across CNNs, ViTs, defenses, cross-generator scenarios, and on compressed images.&lt;/li&gt;&lt;li&gt;Shows that manipulating frequency characteristics of images plus simulating diverse victim models enables effective black-box evasion of state-of-the-art AIGI detectors, highlighting a practical security threat.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunfeng Diao', 'Naixin Zhai', 'Changtao Miao', 'Zitong Yu', 'Xingxing Wei', 'Xun Yang', 'Meng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'AIGI detection', 'transferability', 'frequency-domain perturbation', 'post-train Bayesian surrogate']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.20836</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title><link>https://arxiv.org/abs/2512.16899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal RewardBench 2 (MMRB2), a benchmark for evaluating reward models on four multimodal tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning.&lt;/li&gt;&lt;li&gt;Provides 1,000 expert-annotated preference pairs per task drawn from responses of 23 models/agents across 21 source tasks, curated for strong human consensus.&lt;/li&gt;&lt;li&gt;Evaluates existing judges (multimodal LLM-as-a-judge and models trained with human preferences), reports accuracy comparisons (e.g., Gemini 3 Pro 75–80%, GPT-5/Gemini 2.5 Pro 66–75%, humans &gt;90%), and correlates RM performance with downstream Best-of-N sampling gains.&lt;/li&gt;&lt;li&gt;Performs in-depth analysis identifying key weaknesses and directions to improve multimodal reward models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Hu', 'Reyhane Askari-Hemmat', 'Melissa Hall', 'Emily Dinan', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']&lt;/li&gt;&lt;li&gt;Tags: ['reward models', 'alignment', 'safety-evaluation', 'multimodal-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16899</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title><link>https://arxiv.org/abs/2512.16126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new privacy threat in machine unlearning: adversaries querying both the original and unlearned model (dual-view) can amplify leakage on retained data.&lt;/li&gt;&lt;li&gt;Introduces an information-theoretic notion of 'privacy knowledge gain' showing dual-view queries reveal more than single-model queries.&lt;/li&gt;&lt;li&gt;Proposes DVIA, a black-box dual-view membership inference attack using a lightweight likelihood-ratio inference module (no attack model training required).&lt;/li&gt;&lt;li&gt;Empirically validates DVIA across datasets and architectures, demonstrating effective extraction of membership information on retained data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lulu Xue', 'Shengshan Hu', 'Linqiang Qian', 'Peijin Guo', 'Yechao Zhang', 'Minghui Li', 'Yanjun Zhang', 'Dayong Ye', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'membership inference', 'privacy attack', 'black-box dual-view attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16126</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</title><link>https://arxiv.org/abs/2512.16123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a single-layer convolutional autoencoder to denoise adversarial perturbations on images for object detection without retraining the detector.&lt;/li&gt;&lt;li&gt;Evaluated on vehicle images from COCO using Perlin noise attacks and YOLOv5; attacks reduced bbox mAP from 0.2890 to 0.1640, autoencoder restored bbox mAP to 0.1700 and increased mAP@50 from 0.2780 to 0.3080.&lt;/li&gt;&lt;li&gt;Shows partial improvement in robustness but limited recovery and a narrow evaluation (single attack type, single detector, simple autoencoder architecture).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Min Geun Song', 'Gang Min Kim', 'Woonmin Kim', 'Yongsik Kim', 'Jeonghyun Sim', 'Sangbeom Park', 'Huy Kang Kim']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'object detection', 'autoencoder', 'denoising']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16123</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title><link>https://arxiv.org/abs/2512.15808</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critically assesses foundation models in biomedical imaging, outlining current capabilities and limitations.&lt;/li&gt;&lt;li&gt;Introduces a taxonomy of reasoning and argues for moving beyond statistical correlation toward causal inference for clinical robustness.&lt;/li&gt;&lt;li&gt;Highlights deployment challenges including trustworthiness, algorithmic/data bias, privacy, and model hallucinations, and calls for more rigorous clinical validation.&lt;/li&gt;&lt;li&gt;Recommends developing hybrid, causally aware, and verifiably safe systems to augment clinical practice rather than replace human expertise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amgad Muneer', 'Kai Zhang', 'Ibraheem Hamdi', 'Rizwan Qureshi', 'Muhammad Waqas', 'Shereen Fouad', 'Hazrat Ali', 'Syed Muhammad Anwar', 'Jia Wu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias', 'privacy', 'robustness', 'deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15808</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title><link>https://arxiv.org/abs/2512.16921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuditDM: an RL‑trained auditor MLLM that generates questions and counterfactual images to maximize disagreement across target multimodal models.&lt;/li&gt;&lt;li&gt;Uses discovered disagreement exemplars as interpretable failure modes and annotation‑free data for fine‑tuning/rectification, improving performance across benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates discovery of 20+ distinct failure types on SoTA MLLMs and shows that targeted auditing/finetuning can substantially boost smaller models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qihao Liu', 'Chengzhi Mao', 'Yaojie Liu', 'Alan Yuille', 'Wen-Sheng Chu']&lt;/li&gt;&lt;li&gt;Tags: ['model-auditing', 'red-teaming', 'safety-evaluation', 'robustness', 'multimodal-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16921</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?</title><link>https://arxiv.org/abs/2512.16688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of state-of-the-art deepfake detectors (trained on fully synthetic images) against localized inpainting and region-level edits.&lt;/li&gt;&lt;li&gt;Uses multiple datasets covering diverse generative models, mask sizes, and inpainting techniques to test generalization.&lt;/li&gt;&lt;li&gt;Finds partial transferability: detectors trained on many generators can reliably detect medium/large-area manipulations and regeneration-style inpainting, outperforming many ad hoc methods.&lt;/li&gt;&lt;li&gt;Implication: existing detectors have some robustness to localized edits but detection performance varies by mask size and inpainting style, suggesting need for targeted training/evaluation on localized manipulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Serafino Pandolfini', 'Lorenzo Pellegrini', 'Matteo Ferrara', 'Davide Maltoni']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'image forensics', 'inpainting', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16688</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</title><link>https://arxiv.org/abs/2512.16685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses subject fingerprinting / re-identification to detect when the same person appears across open-source medical imaging datasets, which can cause data leakage and inflated model performance.&lt;/li&gt;&lt;li&gt;Proposes training a ResNet-50 embedding with triplet margin loss to map all images of a subject to a compact region in latent space, enabling few-shot similarity-based re-identification.&lt;/li&gt;&lt;li&gt;Evaluates few-shot (e.g., 20-way 1-shot, 1000-way 1-shot) performance on ChestXray-14 (2D X-ray) and BraTS-2021 (3D MRI), reporting high Recall@K scores across scenarios.&lt;/li&gt;&lt;li&gt;Implications: useful for dataset de-duplication and preventing leakage, but also demonstrates a re-identification capability with privacy/security implications for medical data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gon\\c{c}alo Gaspar Alves', 'Shekoufeh Gorgi Zadeh', 'Andreas Husch', 'Ben Bausch']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-attack', 'subject-re-identification', 'data-leakage', 'medical-imaging', 'fingerprinting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16685</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeContext as Defense: Safe Image Editing in Diffusion Transformers</title><link>https://arxiv.org/abs/2512.16625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeContext, a defense that injects small, targeted perturbations to disrupt cross-attention pathways in in-context diffusion transformers to block unauthorized image edits.&lt;/li&gt;&lt;li&gt;Identifies that early denoising steps and specific transformer blocks dominate context propagation, allowing focused perturbations for efficiency and robustness.&lt;/li&gt;&lt;li&gt;Evaluates on Flux Kontext and Step1X-Edit, showing DeContext prevents unwanted edits while preserving visual quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Shen', 'Mingyue Cui', 'Xingyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['image-editing defense', 'adversarial perturbation', 'diffusion transformers', 'privacy protection', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16625</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</title><link>https://arxiv.org/abs/2512.16523</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Test-Time Padding (TTP), a lightweight inference-time framework that detects adversarial inputs for CLIP-like VLMs by measuring cosine similarity shifts between embeddings before and after spatial padding.&lt;/li&gt;&lt;li&gt;Uses a universal detection threshold across architectures/datasets; for detected adversarial examples, applies trainable padding to restore attention patterns and a similarity-aware ensemble to improve final predictions.&lt;/li&gt;&lt;li&gt;Leaves clean inputs unchanged (or optionally applies test-time adaptation methods) to avoid harming clean accuracy; shows consistent improvements over state-of-the-art test-time defenses on multiple CLIP backbones and fine-grained benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiwei Li', 'Yitian Pang', 'Weining Wang', 'Zhenan Sun', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial detection', 'test-time defense', 'vision-language models', 'test-time adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16523</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.16055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a closed-loop real-world adversarial evaluation platform for end-to-end autonomous driving that generates safety-critical corner cases via adversarial interactions.&lt;/li&gt;&lt;li&gt;Uses a flow-matching–based real-world image generator conditioned on traffic environment information to produce realistic driving images efficiently and stably.&lt;/li&gt;&lt;li&gt;Designs an efficient adversarial surrounding-vehicle policy to model challenging interactions and create corner cases that expose failures in tested models.&lt;/li&gt;&lt;li&gt;Evaluates end-to-end models (e.g., UniAD, VAD) and demonstrates measurable performance degradation in adversarial corner cases, enabling detection of potential safety/robustness issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaheng Geng', 'Jiatong Du', 'Xinyu Zhang', 'Ye Li', 'Panqu Wang', 'Yanjun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'adversarial-evaluation', 'safety-testing', 'robustness', 'closed-loop-simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16055</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</title><link>https://arxiv.org/abs/2512.15949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents The Perceptual Observatory: a framework/benchmark to characterize perceptual grounding and robustness of multimodal LLMs across tasks like face matching, text-in-vision comprehension, image matching, pointing/grid games, and attribute localization.&lt;/li&gt;&lt;li&gt;Instantiates each vertical with ground-truth face and word datasets and applies systematic pixel-level augmentations and diffusion-based stylized perturbations to probe robustness, attribution fidelity, and relational structure.&lt;/li&gt;&lt;li&gt;Aims to move beyond end-task accuracy leaderboards to provide principled insights into how MLLMs preserve visual grounding and reasoning under controlled perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tejas Anvekar', 'Fenil Bardoliya', 'Pavan K. Turaga', 'Chitta Baral', 'Vivek Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal evaluation', 'grounding', 'perceptual perturbations', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15949</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Failure Modes of Maximum Entropy RLHF</title><link>https://arxiv.org/abs/2509.20265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows SimPO can be derived as Maximum Entropy Reinforcement Learning, linking a reference-free offline preference optimization method to a formal RL objective.&lt;/li&gt;&lt;li&gt;Empirical finding: Maximum Entropy RL in online RLHF settings leads to consistent overoptimization, unstable KL dynamics, and reward hacking, even with low learning rates.&lt;/li&gt;&lt;li&gt;Contrasts entropy regularization with KL-constrained methods: KL constraints yield more stable training while entropy regularization fails to prevent overoptimization.&lt;/li&gt;&lt;li&gt;Analyzes why SimPO performs well offline but Maximum Entropy RL struggles in online preference learning, highlighting challenges for reference-free approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['\\"Omer Veysel \\c{C}a\\u{g}atan', 'Bar{\\i}\\c{s} Akg\\"un']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Alignment', 'Reward hacking', 'Reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20265</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MindShift: Analyzing Language Models' Reactions to Psychological Prompts</title><link>https://arxiv.org/abs/2512.09149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MindShift, a benchmark that uses adapted MMPI psychometric tests to evaluate LLMs' ability to adopt and exhibit personality traits via persona-oriented prompts.&lt;/li&gt;&lt;li&gt;Constructs detailed persona prompts with varying trait intensities to measure sensitivity and role adherence across models.&lt;/li&gt;&lt;li&gt;Finds consistent improvements in models' role perception attributed to training data and alignment techniques, with notable variation across model families.&lt;/li&gt;&lt;li&gt;Plans to release prompts and evaluation code for public use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anton Vasiliuk', 'Irina Abdullaeva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Andrey Kuznetsov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'persona-prompting', 'behavioral-evaluation', 'psychometrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09149</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Think Twice: Branch-and-Rethink Reasoning Reward Model</title><link>https://arxiv.org/abs/2510.23596</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BR-RM, a two-turn 'branch-and-rethink' reward model that first selects instance-critical evaluation dimensions and then re-evaluates conditioned on those branches to reduce judgment diffusion.&lt;/li&gt;&lt;li&gt;Trains the two-turn RM with RL-style optimization (GRPO-like) over structured traces using binary outcome rewards and format checks, compatible with RLHF pipelines.&lt;/li&gt;&lt;li&gt;Claims improved sensitivity to subtle errors and state-of-the-art performance on three reward-modeling benchmarks, with explicit mention of safety and factuality as selectable evaluation dimensions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhu Jiao', 'Jiaqi Zeng', 'Julien Veron Vialard', 'Oleksii Kuchaiev', 'Jiawei Han', 'Olivier Delalleau']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'safety evaluation', 'alignment', 'RLHF', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23596</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</title><link>https://arxiv.org/abs/2510.08158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two benchmarks—XSB (single-turn) with annotated "Focus" keywords and MS-XSB (multi-turn)—to systematically measure exaggerated/false refusals in LLMs.&lt;/li&gt;&lt;li&gt;Finds exaggerated refusals are common across recent instruction-tuned LLaMA models and worsen in complex, multi-turn dialog contexts.&lt;/li&gt;&lt;li&gt;Proposes post-hoc explanation methods to identify refusal triggers and three inference-time, model-agnostic mitigations: ignore-word instructions, prompt rephrasing, and attention steering, requiring no retraining or parameter access.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows these lightweight strategies improve compliance on safe prompts while preserving safety protections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzhou Yuan', 'Ercong Nie', 'Yinuo Sun', 'Chenxuan Zhao', 'William LaCroix', 'Michael F\\"arber']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Safety evaluation / benchmarks', 'Inference-time mitigation', 'Prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08158</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Who is In Charge? Dissecting Role Conflicts in Instruction Following</title><link>https://arxiv.org/abs/2510.01228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how LLMs resolve conflicts between hierarchical instructions (system vs user) and social cues (authority/consensus) at scale.&lt;/li&gt;&lt;li&gt;Uses mechanistic probes (linear probing, Direct Logit Attribution) to show conflict signals are encoded early and occupy distinct subspaces for system-user vs social conflicts.&lt;/li&gt;&lt;li&gt;Finds stronger internal detection of system-user conflicts but consistent conflict resolution only for social cues; steering vectors amplify instruction following in a role-agnostic manner.&lt;/li&gt;&lt;li&gt;Concludes system obedience is fragile and advocates for lightweight, hierarchy-sensitive alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'jailbreaking/prompt-injection', 'mechanistic-interpretability', 'model-behavioral-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01228</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</title><link>https://arxiv.org/abs/2507.06489</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first comprehensive study on robustness of LLM verbal confidence under adversarial attacks, including perturbation- and jailbreak-based methods.&lt;/li&gt;&lt;li&gt;Introduces attack frameworks that can significantly distort verbal confidence scores and cause frequent answer changes, even with semantic-preserving modifications.&lt;/li&gt;&lt;li&gt;Evaluates multiple prompting strategies, model sizes, and application domains, showing common defenses are largely ineffective or counterproductive.&lt;/li&gt;&lt;li&gt;Highlights the need for designing robust mechanisms for confidence expression in LLMs to ensure safe, transparent human-AI interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Obadinma', 'Xiaodan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'jailbreaking', 'confidence estimation', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06489</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title><link>https://arxiv.org/abs/2512.16912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes exploration-exploitation trade-offs in reinforcement learning with verifiable rewards (RLVR) applied to LLM reasoning, focusing on policy entropy and spurious rewards.&lt;/li&gt;&lt;li&gt;Finds clipping bias under spurious rewards reduces policy entropy and yields more deterministic/confident outputs, while simple entropy minimization alone does not explain performance gains.&lt;/li&gt;&lt;li&gt;Introduces a reward-misalignment model that explains why spurious rewards can improve performance beyond cases of model contamination.&lt;/li&gt;&lt;li&gt;Provides practical principles for designing more effective RLVR training regimes to improve LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter Chen', 'Xiaopeng Li', 'Ziniu Li', 'Wotao Yin', 'Xi Chen', 'Tianyi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLVR/RLHF', 'reward-misalignment', 'training-dynamics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16912</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Good is Post-Hoc Watermarking With Language Model Rephrasing?</title><link>https://arxiv.org/abs/2512.16904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates post-hoc watermarking: having an LLM rephrase existing text while applying generation-time watermarking to enable traceability of AI-generated or reused content.&lt;/li&gt;&lt;li&gt;Empirically evaluates how compute allocation (larger models, beam search, multi-candidate generation, entropy filtering) affects detectability vs. semantic fidelity, finding Gumbel-max + beam search often effective under nucleus sampling.&lt;/li&gt;&lt;li&gt;Shows strong detectability and fidelity on open-ended text (e.g., books) but identifies limitations on verifiable text like code, where smaller models can outperform larger ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pierre Fernandez', 'Tom Sander', 'Hady Elsahar', 'Hongyan Chang', "Tom\\'a\\v{s} Sou\\v{c}ek", 'Valeriu Lacatusu', 'Tuan Tran', 'Sylvestre-Alvise Rebuffi', 'Alexandre Mourachko']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'detection', 'LLM evaluation', 'traceability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16904</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Impacts of Racial Bias in Historical Training Data for News AI</title><link>https://arxiv.org/abs/2512.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Case study analyzing a multi-label classifier trained on the New York Times Annotated Corpus that surfaced a problematic thematic label "blacks."&lt;/li&gt;&lt;li&gt;Using quantitative analysis and explainable AI, the authors show the label acts partly as a general "racism detector" but fails on modern examples (e.g., anti-Asian COVID-19 hate, Black Lives Matter coverage).&lt;/li&gt;&lt;li&gt;Highlights how historical training data encode outdated stereotypes and the risks this poses for newsroom AI workflows (story discovery, summarization, audience targeting).&lt;/li&gt;&lt;li&gt;Discusses implications for auditing, dataset curation, and reducing reproduction of historical biases in deployed models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Bhargava', 'Malene Hornstrup Jespersen', 'Emily Boardman Ndulue', 'Vivica Dsouza']&lt;/li&gt;&lt;li&gt;Tags: ['dataset bias', 'fairness', 'explainable AI', 'model safety', 'newsroom AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16901</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection</title><link>https://arxiv.org/abs/2512.16439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemMark, a semantic-aware watermarking method for embedding-as-a-service (EaaS) that uses locality-sensitive hashing to partition semantic space and inject watermarks into selected regions.&lt;/li&gt;&lt;li&gt;Introduces an adaptive watermark weighting mechanism based on local outlier factor (LOF) to preserve original embedding distribution and improve harmlessness/stealthiness.&lt;/li&gt;&lt;li&gt;Defines new threat/robustness evaluations including Detect-Sampling and Dimensionality-Reduction attacks and constructs four attack scenarios for evaluation.&lt;/li&gt;&lt;li&gt;Evaluates SemMark on four NLP datasets, reporting improvements in verifiability, diversity, stealthiness, and harmlessness compared to prior watermarking methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Yubing Ren', 'Yanan Cao', 'Yingjie Li', 'Fang Fang', 'Xuebin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['embedding watermarking', 'model/IP protection', 'EaaS security', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16439</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation</title><link>https://arxiv.org/abs/2512.16310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines Tools Orchestration Privacy Risk (TOP-R): autonomous agents aggregating fragments across tools can synthesize unexpected sensitive information due to overoptimization for helpfulness.&lt;/li&gt;&lt;li&gt;Presents TOP-Bench, a paired leakage vs benign dataset/benchmark, and introduces H-Score and Risk Leakage Rate (RLR) to evaluate safety-robustness trade-offs.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows high leakage (average RLR ~90%) and low H-Scores for representative models, revealing structural limitations in single-agent multi-tool architectures.&lt;/li&gt;&lt;li&gt;Proposes the Privacy Enhancement Principle (PEP) mitigation that substantially reduces RLR (~46.6%) and improves H-Score (~0.624).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Qiao', 'Dongqin Liu', 'Hongchang Yang', 'Wei Zhou', 'Songlin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-attacks', 'agent-orchestration', 'safety-evaluation', 'alignment', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16310</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems</title><link>https://arxiv.org/abs/2512.16279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QuadSentinel, a four-agent guarding architecture (state tracker, policy verifier, threat watcher, referee) that compiles natural-language safety policies into machine-checkable sequent predicates and enforces them at runtime.&lt;/li&gt;&lt;li&gt;Introduces referee logic and a top-k predicate updater to prioritize checks, resolve conflicts hierarchically, and keep verification costs low during multi-step tool use and inter-agent messaging.&lt;/li&gt;&lt;li&gt;Evaluates on ST-WebAgentBench and AgentHarm, reporting improved guardrail accuracy, higher rule recall, and fewer false positives compared to single-agent baselines (e.g., ShieldAgent).&lt;/li&gt;&lt;li&gt;Designed for near-term deployment without modifying core agents by keeping policies separate and machine-checkable; code to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiliu Yang', 'Yilei Jiang', 'Qunzhong Wang', 'Yingshui Tan', 'Xiaoyong Zhu', 'Sherman S. M. Chow', 'Bo Zheng', 'Xiangyu Yue']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'runtime enforcement', 'policy verification', 'guardrails', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16279</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack</title><link>https://arxiv.org/abs/2512.16182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DualGuard, an adaptive dual-stream watermarking algorithm for LLM outputs that injects two complementary watermark signals based on semantic content.&lt;/li&gt;&lt;li&gt;Specifically designed to defend against both paraphrase attacks and piggyback/spoofing attacks, enabling detection and tracing of spoofed content.&lt;/li&gt;&lt;li&gt;Reports extensive experiments across datasets and models demonstrating detectability, robustness, traceability, and preserved text quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Yubing Ren', 'Yanan Cao', 'Yingjie Li', 'Fang Fang', 'Shi Wang', 'Li Guo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'paraphrase attacks', 'spoofing/piggyback attacks', 'robustness', 'attribution/traceability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16182</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ContextLeak: Auditing Leakage in Private In-Context Learning Methods</title><link>https://arxiv.org/abs/2512.16059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ContextLeak, a framework that empirically measures worst-case information leakage in in-context learning by inserting unique canaries into exemplars and issuing targeted queries to detect them.&lt;/li&gt;&lt;li&gt;Evaluates a range of private ICL techniques, including heuristic prompt-based defenses and provable mechanisms like Embedding Space Aggregation and Report Noisy Max.&lt;/li&gt;&lt;li&gt;Finds ContextLeak correlates tightly with theoretical privacy budgets (ε) and reliably detects leakage, revealing that many methods either leak sensitive data or incur large utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jacob Choi', 'Shuying Cao', 'Xingjian Dong', 'Wang Bill Zhu', 'Robin Jia', 'Sai Praneeth Karimireddy']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'in-context learning', 'LLM privacy auditing', 'canary insertion', 'differential privacy evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16059</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Language Bias Examination in Large Language Models</title><link>https://arxiv.org/abs/2512.16029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a multilingual bias evaluation framework combining explicit bias measurement (BBQ benchmark) and a prompt-based Implicit Association Test (IAT).&lt;/li&gt;&lt;li&gt;Translates prompts/word lists into five languages (English, Chinese, Arabic, French, Spanish) to enable direct cross-lingual bias comparison.&lt;/li&gt;&lt;li&gt;Finds substantial cross-language variation (e.g., Arabic and Spanish show higher stereotype bias; Chinese and English lower) and differing patterns across bias types.&lt;/li&gt;&lt;li&gt;Highlights cases where implicit bias is detectable despite low explicit bias (e.g., age), underscoring limits of standard benchmarks and need for implicit assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Liang', 'Marwa Mahmoud']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'multilingual', 'evaluation', 'implicit-association']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16029</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DSO: Direct Steering Optimization for Bias Mitigation</title><link>https://arxiv.org/abs/2512.15926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses controllable, inference-time bias mitigation for generative models (VLMs and LLMs) where demographic attributes undesirably influence outputs.&lt;/li&gt;&lt;li&gt;Proposes Direct Steering Optimization (DSO): uses reinforcement learning to learn linear activation transformations to steer model behavior toward reduced bias while preserving capability.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art trade-off between fairness and model performance and provides practitioners controllable inference-time adjustment of that trade-off.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Monteiro Paes', 'Nivedha Sivakumar', 'Yinong Oliver Wang', 'Masha Fedzechkina Donaldson', 'Luca Zappella', 'Nicholas Apostoloff']&lt;/li&gt;&lt;li&gt;Tags: ['bias-mitigation', 'activation-steering', 'inference-time-control', 'reinforcement-learning', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15926</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Analysis of Biases in Large Language Models</title><link>https://arxiv.org/abs/2512.15792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive evaluation of four popular LLMs for biases across politics, ideology, geopolitical alliances, language, and gender.&lt;/li&gt;&lt;li&gt;Uses tasks including news summarization, news stance classification, UN voting pattern alignment, multilingual story completion, and World Values Survey prompts.&lt;/li&gt;&lt;li&gt;Finds that models are aligned toward neutrality but still exhibit measurable biases and affinities across different dimensions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xulang Zhang', 'Rui Mao', 'Erik Cambria']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'alignment', 'safety evaluation', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15792</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Auto-Tuning Safety Guardrails for Black-Box Large Language Models</title><link>https://arxiv.org/abs/2512.15782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Treats safety guardrail design for black-box LLMs as a hyperparameter optimization problem over fixed models (system prompts + content filters).&lt;/li&gt;&lt;li&gt;Evaluates configurations on benchmarks for malware generation, classic jailbreak prompts, and benign queries using attack success, benign harmful-response rate, and latency.&lt;/li&gt;&lt;li&gt;Compares 48-point grid search baseline to a black-box Optuna study; Optuna rediscovers best configurations with an order-of-magnitude fewer evaluations and ~8x less wall-clock time.&lt;/li&gt;&lt;li&gt;Concludes that automated tuning of guardrails is a practical approach to hardening black-box LLM deployments under compute and time constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Perry Abdulkadir']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Red teaming', 'Jailbreaking', 'Prompt engineering', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15782</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Value Lens: Using Large Language Models to Understand Human Values</title><link>https://arxiv.org/abs/2512.15722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Value Lens, a two-stage LLM-based method to detect human values in text: (1) generate a formal theory of values via an LLM and expert verification; (2) use a detector LLM plus a critic LLM to identify and review values in target text.&lt;/li&gt;&lt;li&gt;Claims performance comparable to or exceeding alternative methods for value-detection tasks using generative LLMs and self-review.&lt;/li&gt;&lt;li&gt;Framed as a tool for assessing whether autonomous decisions align with human values, i.e., supporting value-alignment/safety evaluation of systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Eduardo de la Cruz Fern\\'andez", 'Marcelo Karanik', 'Sascha Ossowski']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'value-detection', 'safety-evaluation', 'LLM-based auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15722</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title><link>https://arxiv.org/abs/2512.16899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal RewardBench 2 (MMRB2), a benchmark for evaluating reward models on four multimodal tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning.&lt;/li&gt;&lt;li&gt;Provides 1,000 expert-annotated preference pairs per task drawn from responses of 23 models/agents across 21 source tasks, curated for strong human consensus.&lt;/li&gt;&lt;li&gt;Evaluates existing judges (multimodal LLM-as-a-judge and models trained with human preferences), reports accuracy comparisons (e.g., Gemini 3 Pro 75–80%, GPT-5/Gemini 2.5 Pro 66–75%, humans &gt;90%), and correlates RM performance with downstream Best-of-N sampling gains.&lt;/li&gt;&lt;li&gt;Performs in-depth analysis identifying key weaknesses and directions to improve multimodal reward models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Hu', 'Reyhane Askari-Hemmat', 'Melissa Hall', 'Emily Dinan', 'Luke Zettlemoyer', 'Marjan Ghazvininejad']&lt;/li&gt;&lt;li&gt;Tags: ['reward models', 'alignment', 'safety-evaluation', 'multimodal-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16899</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning</title><link>https://arxiv.org/abs/2512.16883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaSearch, a two-stage RL framework that separates problem solving from the decision to invoke external search, improving when an LLM should rely on parametric knowledge vs. retrieval.&lt;/li&gt;&lt;li&gt;Quantifies self-knowledge awareness of search agents using an F1-based decision metric and shows prior methods overuse search despite available parametric knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates reduced unnecessary search calls, preserved task performance, and improved transparency/interpretability—important for high-stakes domains where search can expose models to noisy or malicious content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tzu-Han Lin', 'Wei-Lin Chen', 'Chen-An Li', 'Hung-yi Lee', 'Yun-Nung Chen', 'Yu Meng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM tool use', 'AI safety', 'Reinforcement learning', 'Interpretability', 'Retrieval/security risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16883</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</title><link>https://arxiv.org/abs/2512.16795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reasoning-trace-augmented RAG framework with three stages: document-level adjudication, conflict analysis, and grounded synthesis to produce citation-linked answers or justified refusals.&lt;/li&gt;&lt;li&gt;Introduces CATS (Conflict-Aware Trust-Score), an evaluation pipeline using an LLM-as-a-Judge to measure groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment.&lt;/li&gt;&lt;li&gt;Provides a 539-query reasoning dataset and shows large improvements from supervised fine-tuning (e.g., Qwen improvement in end-to-end correctness and behavioral adherence).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shubham Mishra', 'Samyek Jain', 'Gorang Mehrishi', 'Shiv Tiwari', 'Harsh Sharma', 'Pratik Narang', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'retrieval-augmented-models', 'interpretable-reasoning', 'refusal-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16795</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</title><link>https://arxiv.org/abs/2512.16602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Refusal Steering, an inference-time method to fine-tune LLM refusal behavior on politically sensitive topics without retraining.&lt;/li&gt;&lt;li&gt;Uses an LLM-as-a-judge to produce refusal confidence scores and computes ridge-regularized steering vectors to isolate refusal–compliance directions.&lt;/li&gt;&lt;li&gt;Shows removal of political refusals on Qwen3-Next-80B while preserving safety on JailbreakBench and near-baseline general benchmark performance; generalizes across 4B and 80B models and can induce targeted refusals.&lt;/li&gt;&lt;li&gt;Analyzes activation-level signals: refusal indicators concentrate in deeper transformer layers and are distributed across many dimensions, demonstrating practical, controllable moderation at inference time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Iker Garc\\'ia-Ferrero", 'David Montero', 'Roman Orus']&lt;/li&gt;&lt;li&gt;Tags: ['refusal steering', 'activation steering', 'LLM safety/alignment', 'jailbreaking', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16602</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hacking Neural Evaluation Metrics with Single Hub Text</title><link>https://arxiv.org/abs/2512.16323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to find a single adversarial 'hub text' in discrete space that is consistently scored as high-quality by embedding-based neural evaluation metrics (e.g., COMET) regardless of test cases.&lt;/li&gt;&lt;li&gt;Shows the hub text obtains 79.1 COMET% (En→Ja) and 67.8 COMET% (En→De) on WMT'24, outperforming translations from a general model (M2M100) and generalizes across language pairs (e.g., Ja→En, De→En).&lt;/li&gt;&lt;li&gt;Demonstrates a practical vulnerability of neural evaluation metrics, exposing risks of metric gaming/manipulation and undermining reliability of automated evaluation pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiroyuki Deguchi', 'Katsuki Chousa', 'Yusuke Sakai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'evaluation metrics', 'robustness', 'neural text evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16323</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>An Information-Theoretic Framework for Robust Large Language Model Editing</title><link>https://arxiv.org/abs/2512.16227</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an information-bottleneck grounded framework for editing LLM knowledge that isolates compact latent representations for targeted updates.&lt;/li&gt;&lt;li&gt;Introduces the Information Bottleneck Knowledge Editor (IBKE) which guides gradient-based edits to improve generality and specificity while minimizing disruption to unrelated behaviors.&lt;/li&gt;&lt;li&gt;Validates IBKE across multiple LLM architectures and benchmarks, reporting state-of-the-art accuracy, improved generality of edits, and reduced unintended side effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qizhou Chen', 'Chengyu Wang', 'Taolin Zhang', 'Xiaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'LLM robustness', 'alignment', 'information bottleneck', 'safe model updates']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16227</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</title><link>https://arxiv.org/abs/2512.16189</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-agnostic fact-checking module that validates granular propositions from LLM-generated clinical summaries against EHRs using numerical tests and discrete logical checks.&lt;/li&gt;&lt;li&gt;Fine-tunes a domain-specific summarization LLM on the MIMIC-III dataset via LoRA to reduce hallucination rates in healthcare summaries.&lt;/li&gt;&lt;li&gt;Evaluates the fact-checker on 3,786 extracted propositions (from 104 summaries) achieving precision 0.8904, recall 0.8234, F1 0.8556; reports summarization quality metrics (ROUGE-1 0.5797, BERTScore 0.9120).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Musarrat Zeba', 'Abdullah Al Mamun', 'Kishoar Jahan Tithee', 'Debopom Sutradhar', 'Mohaimenul Azam Khan Raiaan', 'Saddam Mukta', 'Reem E. Mohamed', 'Md Rafiqul Islam', 'Yakub Sebastian', 'Mukhtar Hussain', 'Sami Azam']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'fact-checking', 'healthcare LLMs', 'domain adaptation', 'factuality evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16189</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are We on the Right Way to Assessing LLM-as-a-Judge?</title><link>https://arxiv.org/abs/2512.16041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sage, an evaluation suite that measures LLM-as-a-Judge without human annotations using axioms from rational choice theory.&lt;/li&gt;&lt;li&gt;Defines two metrics: local self-consistency (pairwise preference stability) and global logical consistency (transitivity across preference sets); provides a 650-question dataset from benchmarks and real queries.&lt;/li&gt;&lt;li&gt;Finds substantial inconsistency in state-of-the-art LLM judges (including top models) and in human annotations; identifies a 'situational preference' phenomenon that rubrics can mitigate.&lt;/li&gt;&lt;li&gt;Shows finetuning, panel-based judging, and deeper reasoning improve judge consistency; demonstrates high correlation between Sage metrics and supervised benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanning Feng', 'Sinan Wang', 'Zhengxiang Cheng', 'Yao Wan', 'Dongping Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'robustness', 'LLM-evaluation', 'judging-consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16041</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Who is In Charge? Dissecting Role Conflicts in Instruction Following</title><link>https://arxiv.org/abs/2510.01228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how LLMs resolve conflicts between hierarchical instructions (system vs user) and social cues (authority/consensus) at scale.&lt;/li&gt;&lt;li&gt;Uses mechanistic probes (linear probing, Direct Logit Attribution) to show conflict signals are encoded early and occupy distinct subspaces for system-user vs social conflicts.&lt;/li&gt;&lt;li&gt;Finds stronger internal detection of system-user conflicts but consistent conflict resolution only for social cues; steering vectors amplify instruction following in a role-agnostic manner.&lt;/li&gt;&lt;li&gt;Concludes system obedience is fragile and advocates for lightweight, hierarchy-sensitive alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'jailbreaking/prompt-injection', 'mechanistic-interpretability', 'model-behavioral-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01228</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memory Backdoor Attacks on Neural Networks</title><link>https://arxiv.org/abs/2411.14516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a training-time backdoor attack that implants an index-triggered memory enabling deterministic, exact extraction of client training samples in federated learning.&lt;/li&gt;&lt;li&gt;Attack can control which samples are memorized and how many, is high-capacity and robust, and recombines extracted patches to overcome small model output sizes.&lt;/li&gt;&lt;li&gt;Demonstrates practical threat across classifiers, medical segmentation models, and large language models with minimal utility degradation, highlighting a realistic FL supply-chain vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eden Luzon', 'Guy Amit', 'Roy Weiss', 'Torsten Kraub', 'Alexandra Dmitrienko', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'dataset extraction', 'federated learning', 'privacy/data exfiltration', 'training-time supply-chain attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14516</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation</title><link>https://arxiv.org/abs/2512.08499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two distance-aware uncertainty methods for physics-guided neural networks: PG-SNGP (Spectral Normalization + GP output) and PG-SNER (Deep Evidential Regression with Normal-Inverse-Gamma outputs).&lt;/li&gt;&lt;li&gt;Applies spectral normalization to preserve input-to-latent distances and uses a GP final layer or evidential outputs to produce calibrated, distance-sensitive uncertainty.&lt;/li&gt;&lt;li&gt;Proposes a distance-aware metric (Pearson correlation between predicted uncertainty and distance from training data) and a dynamic loss weighting to balance data fidelity and physical consistency.&lt;/li&gt;&lt;li&gt;Evaluates on rolling-element bearing degradation datasets (PRONOSTIA, XJTU-SY, HUST), showing improved accuracy, OOD generalization, and robustness to noise and adversarial attacks compared to MC dropout and deep ensembles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waleed Razzaq', 'Yun-Bo Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'OOD-detection', 'adversarial-robustness', 'physics-guided-ML', 'evidential-uncertainty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08499</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title><link>https://arxiv.org/abs/2511.13654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical and theoretical analysis of how training hyperparameters (learning rate, weight decay, momentum, batch size) affect robustness to transfer-based and query-based adversarial attacks.&lt;/li&gt;&lt;li&gt;Key finding: decreasing learning rate substantially improves robustness to transfer-based attacks (up to 64%), while increasing learning rate improves robustness to query-based attacks (up to 28%).&lt;/li&gt;&lt;li&gt;Explores joint hyperparameter tuning across centralized, ensemble, and distributed training, showing distributed models achieve the best tradeoff against both attack types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pascal Zimmer', 'Ghassan Karame']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'transfer-based attacks', 'query-based attacks', 'hyperparameter tuning', 'distributed training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13654</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UAMDP: Uncertainty-Aware Markov Decision Process for Risk-Constrained Reinforcement Learning from Probabilistic Forecasts</title><link>https://arxiv.org/abs/2510.08226</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UAMDP: a framework that integrates Bayesian probabilistic forecasting, posterior-sampling (Thompson sampling) reinforcement learning, and planning under a CVaR (risk) constraint to manage uncertainty in sequential decision-making.&lt;/li&gt;&lt;li&gt;Operates in closed-loop: updates posterior over latent dynamics, samples plausible futures, and optimizes policies subject to preset risk tolerances.&lt;/li&gt;&lt;li&gt;Provides theoretical regret bounds converging to the Bayes-optimal benchmark under regularity conditions.&lt;/li&gt;&lt;li&gt;Empirical evaluation in high-frequency trading and retail inventory control showing improved forecasting accuracy and economically meaningful risk-aware performance gains (higher Sharpe ratio, lower drawdown).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michal Koren', 'Or Peretz', 'Tai Dinh', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['risk-aware reinforcement learning', 'uncertainty quantification', 'CVaR / risk constraints', 'Thompson sampling / posterior sampling', 'safe decision-making']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08226</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Failure Modes of Maximum Entropy RLHF</title><link>https://arxiv.org/abs/2509.20265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that Simple Preference Optimization (SimPO) can be derived from Maximum Entropy Reinforcement Learning, providing theoretical grounding for SimPO.&lt;/li&gt;&lt;li&gt;Empirical evaluation in online RLHF settings finds Maximum Entropy RL exhibits consistent overoptimization and unstable KL dynamics, even at low learning rates.&lt;/li&gt;&lt;li&gt;Finds entropy regularization does not prevent reward hacking and appears correlated with overoptimization, unlike KL-constrained methods which yield more stable training.&lt;/li&gt;&lt;li&gt;Discusses why reference-free approaches like SimPO may succeed offline but face distinct failure modes when applied online in preference learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['\\"Omer Veysel \\c{C}a\\u{g}atan', 'Bar{\\i}\\c{s} Akg\\"un']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward hacking', 'alignment', 'robustness', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20265</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</title><link>https://arxiv.org/abs/2508.17361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new adversarial vulnerability in LLM-based static code analysis where models overgeneralize familiar patterns, missing subtle bugs (Familiar Pattern Attack, FPA).&lt;/li&gt;&lt;li&gt;Presents a fully automated, black-box algorithm to discover and inject FPAs that hijack model interpretation without changing runtime behavior.&lt;/li&gt;&lt;li&gt;Shows FPAs transfer across model families (OpenAI, Anthropic, Google) and programming languages (Python, C, Rust, Go), and remain effective despite explicit system-prompt warnings.&lt;/li&gt;&lt;li&gt;Discusses defensive/benign uses of FPAs and broader implications for reliability and safety of code-oriented LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shir Bernstein', 'David Beste', 'Daniel Ayzenshteyn', 'Lea Schonherr', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM robustness', 'code-based attacks', 'black-box transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17361</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes</title><link>https://arxiv.org/abs/2506.23165</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mirror Descent Policy Optimisation (MDPO) for robust constrained Markov decision processes (RCMDPs), using policy gradients to jointly optimise a policy (maximiser) and an adversarial transition kernel (minimiser) on the Lagrangian.&lt;/li&gt;&lt;li&gt;Provides sample-based theoretical guarantees, achieving an Õ(1 / T^{1/3}) convergence rate for the RCMDP setting and a method for approximate gradient descent in the space of transition kernels.&lt;/li&gt;&lt;li&gt;Presents empirical results showing improved robustness and better constraint satisfaction compared to baseline policy optimisation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David M. Bossens', 'Atsushi Nitanda']&lt;/li&gt;&lt;li&gt;Tags: ['Safe Reinforcement Learning', 'Robustness', 'Adversarial Environments', 'Constrained MDPs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23165</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Laws for Black box Adversarial Attacks</title><link>https://arxiv.org/abs/2411.16782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a universal log-linear scaling law: Attack Success Rate (ASR) scales linearly with log(ensemble size T) when gradient conflicts are resolved with advanced optimizers.&lt;/li&gt;&lt;li&gt;Performs large-scale empirical validation across standard classifiers, state-of-the-art defenses, and multimodal LLMs (MLLMs), showing improved black-box transferability from larger surrogate ensembles.&lt;/li&gt;&lt;li&gt;Shows scaling distills robust, semantic features and demonstrates high transfer attack success (80%+) on proprietary models like GPT-4o while highlighting relative robustness of Claude-3.5-Sonnet.&lt;/li&gt;&lt;li&gt;Argues for shifting robustness evaluation from small, intricate ensemble algorithms to principled study of ensemble scaling as a powerful threat vector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuan Liu', 'Huanran Chen', 'Yichi Zhang', 'Jun Zhu', 'Yinpeng Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'black-box', 'model-ensembling', 'transferability', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16782</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Models That Prove Their Own Correctness</title><link>https://arxiv.org/abs/2405.15722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Proving models that produce interactive proofs to a verifier V certifying the correctness of their outputs on particular inputs.&lt;/li&gt;&lt;li&gt;Guarantees that, with high probability over a data distribution, the model both outputs correctly and convinces V, while V's soundness prevents acceptance of incorrect outputs.&lt;/li&gt;&lt;li&gt;Proposes and analyzes two training approaches: Transcript Learning (using transcripts of accepting interactions) and Reinforcement Learning from Verifier Feedback (RLVF) to train models to emulate verifier interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noga Amit', 'Shafi Goldwasser', 'Orr Paradise', 'Guy Rothblum']&lt;/li&gt;&lt;li&gt;Tags: ['verification', 'model correctness', 'safety/assurance', 'interactive proofs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.15722</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</title><link>https://arxiv.org/abs/2512.16813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses resilience of robotic-swarm communications against reactive jammers by formulating the problem as a multi-agent reinforcement learning (MARL) task.&lt;/li&gt;&lt;li&gt;Uses QMIX to learn a centralized but factorizable action-value function, enabling coordinated decentralized selection of transmit frequency and power per agent.&lt;/li&gt;&lt;li&gt;Benchmarks against a genie-aided optimal policy, local UCB, and a stateless reactive policy across no-reuse and channel-reuse fading regimes.&lt;/li&gt;&lt;li&gt;Finds QMIX converges to cooperative policies that approach the genie bound and achieve higher throughput and lower jamming incidence than baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bahman Abolhassani', 'Tugba Erpek', 'Kemal Davaslioglu', 'Yalin E. Sagduyu', 'Sastry Kompella']&lt;/li&gt;&lt;li&gt;Tags: ['MARL', 'wireless security', 'anti-jamming', 'swarm robotics', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16813</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</title><link>https://arxiv.org/abs/2512.16538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematizes code obfuscation techniques into layout, data-flow, and control-flow classes (11 subcategories, 19 techniques) and implements them across Solidity, C, C++, and Python.&lt;/li&gt;&lt;li&gt;Evaluates impact of these obfuscations on 15 LLMs (DeepSeek, OpenAI, Qwen, LLaMA families) and two coding agents (GitHub Copilot, Codex) using a unified LLM-driven framework.&lt;/li&gt;&lt;li&gt;Analyzes conditions where obfuscation degrades or improves LLM-based vulnerability detection and correlates outcomes with vulnerability types, code properties, and model attributes; outlines open problems and mitigation directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Li', 'Yue Li', 'Hao Wu', 'Yue Zhang', 'Yechao Zhang', 'Fengyuan Xu', 'Sheng Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial code obfuscation', 'vulnerability detection', 'model robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16538</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>In-Context Probing for Membership Inference in Fine-Tuned Language Models</title><link>https://arxiv.org/abs/2512.16292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ICP-MIA, a black-box membership inference attack for fine-tuned LLMs based on the proposed "Optimization Gap" signal (diminishing returns at convergence).&lt;/li&gt;&lt;li&gt;Proposes In-Context Probing (ICP) to estimate the optimization gap without training, using reference-data-based probing and self-perturbation strategies (masking/generation).&lt;/li&gt;&lt;li&gt;Shows empirical improvements over prior black-box MIAs across tasks and models, analyzes effects of reference alignment, model/PEFT/configuration, and training schedules on attack success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhexi Lu', 'Hongliang Chi', 'Nathalie Baracaldo', 'Swanand Ravindra Kadhe', 'Yuseok Jeon', 'Lei Yu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'black-box-attack', 'in-context-probing', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16292</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title><link>https://arxiv.org/abs/2512.15808</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critically assesses foundation models in biomedical imaging, outlining current capabilities and limitations.&lt;/li&gt;&lt;li&gt;Introduces a taxonomy of reasoning and argues for moving beyond statistical correlation toward causal inference for clinical robustness.&lt;/li&gt;&lt;li&gt;Highlights deployment challenges including trustworthiness, algorithmic/data bias, privacy, and model hallucinations, and calls for more rigorous clinical validation.&lt;/li&gt;&lt;li&gt;Recommends developing hybrid, causally aware, and verifiably safe systems to augment clinical practice rather than replace human expertise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amgad Muneer', 'Kai Zhang', 'Ibraheem Hamdi', 'Rizwan Qureshi', 'Muhammad Waqas', 'Shereen Fouad', 'Hazrat Ali', 'Syed Muhammad Anwar', 'Jia Wu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias', 'privacy', 'robustness', 'deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15808</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title><link>https://arxiv.org/abs/2512.15783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'AI Epidemiology', a population-level surveillance framework that treats standardized AI-expert interaction fields (risk level, alignment score, accuracy score) as exposure variables to predict output failures.&lt;/li&gt;&lt;li&gt;Validates output-failure associations via expert overrides and real-world outcomes, enabling detection of unreliable AI outputs before harm occurs.&lt;/li&gt;&lt;li&gt;Emphasizes vendor/model-agnostic governance by analyzing outputs rather than internal model computations, producing audit trails and reliability/semantic assessments.&lt;/li&gt;&lt;li&gt;Aims to democratize oversight by requiring no ML expertise from domain experts and passively tracking expert convergence/divergence with AI recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kit Tempest-Walters']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'governance', 'interpretability', 'audit-trails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15783</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Auto-Tuning Safety Guardrails for Black-Box Large Language Models</title><link>https://arxiv.org/abs/2512.15782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Treats safety guardrail design for black-box LLMs as a hyperparameter optimization problem over fixed models (system prompts + content filters).&lt;/li&gt;&lt;li&gt;Evaluates configurations on benchmarks for malware generation, classic jailbreak prompts, and benign queries using attack success, benign harmful-response rate, and latency.&lt;/li&gt;&lt;li&gt;Compares 48-point grid search baseline to a black-box Optuna study; Optuna rediscovers best configurations with an order-of-magnitude fewer evaluations and ~8x less wall-clock time.&lt;/li&gt;&lt;li&gt;Concludes that automated tuning of guardrails is a practical approach to hardening black-box LLM deployments under compute and time constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Perry Abdulkadir']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Red teaming', 'Jailbreaking', 'Prompt engineering', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15782</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization</title><link>https://arxiv.org/abs/2512.15778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAMBO, a bit-flip attack (BFA) framework specifically targeting Mamba (SSM) architectures.&lt;/li&gt;&lt;li&gt;Demonstrates that flipping a single critical bit in Mamba-1.4b can collapse LAMBADA accuracy from 74.64% to 0% and massively increase perplexity.&lt;/li&gt;&lt;li&gt;Provides empirical evidence of pronounced fragility of state-space models (SSMs) to hardware-induced parameter corruption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjay Das', 'Swastik Bhattacharya', 'Shamik Kundu', 'Arnab Raha', 'Souvik Kundu', 'Kanad Basu']&lt;/li&gt;&lt;li&gt;Tags: ['bit-flip attack', 'hardware fault injection', 'model robustness', 'SSM / Mamba']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15778</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title><link>https://arxiv.org/abs/2512.15769</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that open-source diffusion models can carry and reproduce backdoor triggers, which propagate to downstream perception models trained on synthetic data.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of clean-label Data-Chain Backdoor (DCB) attacks that preserve synthetic-data utility while implanting backdoors.&lt;/li&gt;&lt;li&gt;Identifies Early-Stage Trigger Manifestation (ESTM): triggers appear more explicitly in early, high-noise stages of the reverse diffusion process, informing detection/mitigation avenues.&lt;/li&gt;&lt;li&gt;Provides empirical analysis of security risks in generative-data supply chains and initial insights toward mitigating backdoor risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junchi Lu', 'Xinke Li', 'Yuheng Liu', 'Qi Alfred Chen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'diffusion models', 'synthetic data supply chain', 'clean-label attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15769</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title><link>https://arxiv.org/abs/2512.16912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes exploration-exploitation trade-offs in reinforcement learning with verifiable rewards (RLVR) applied to LLM reasoning, focusing on policy entropy and spurious rewards.&lt;/li&gt;&lt;li&gt;Finds clipping bias under spurious rewards reduces policy entropy and yields more deterministic/confident outputs, while simple entropy minimization alone does not explain performance gains.&lt;/li&gt;&lt;li&gt;Introduces a reward-misalignment model that explains why spurious rewards can improve performance beyond cases of model contamination.&lt;/li&gt;&lt;li&gt;Provides practical principles for designing more effective RLVR training regimes to improve LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter Chen', 'Xiaopeng Li', 'Ziniu Li', 'Wotao Yin', 'Xi Chen', 'Tianyi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLVR/RLHF', 'reward-misalignment', 'training-dynamics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16912</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Impacts of Racial Bias in Historical Training Data for News AI</title><link>https://arxiv.org/abs/2512.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Case study analyzing a multi-label classifier trained on the New York Times Annotated Corpus that surfaced a problematic thematic label "blacks."&lt;/li&gt;&lt;li&gt;Using quantitative analysis and explainable AI, the authors show the label acts partly as a general "racism detector" but fails on modern examples (e.g., anti-Asian COVID-19 hate, Black Lives Matter coverage).&lt;/li&gt;&lt;li&gt;Highlights how historical training data encode outdated stereotypes and the risks this poses for newsroom AI workflows (story discovery, summarization, audience targeting).&lt;/li&gt;&lt;li&gt;Discusses implications for auditing, dataset curation, and reducing reproduction of historical biases in deployed models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Bhargava', 'Malene Hornstrup Jespersen', 'Emily Boardman Ndulue', 'Vivica Dsouza']&lt;/li&gt;&lt;li&gt;Tags: ['dataset bias', 'fairness', 'explainable AI', 'model safety', 'newsroom AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16901</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</title><link>https://arxiv.org/abs/2512.16626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Stackelberg Learning from Human Feedback (SLHF), framing preference optimization as a sequential Leader-Follower game where the Leader commits actions and the Follower refines responses conditioned on the Leader.&lt;/li&gt;&lt;li&gt;Claims advantages over RLHF and simultaneous-move (Nash) approaches in consistency, data sensitivity, handling intransitive preferences, and enabling inference-time refinements via the Follower.&lt;/li&gt;&lt;li&gt;Presents experiments on large language models (0.5B–8B) showing improved alignment across preference datasets and that inference-time refinements can transfer across model families without extra fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Barna P\\'asztor", 'Thomas Kleine Buening', 'Andreas Krause']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'RLHF alternative', 'inference-time refinement', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16626</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Feature-Selective Representation Misdirection for Machine Unlearning</title><link>https://arxiv.org/abs/2512.16297</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SRMU (Selective Representation Misdirection for Unlearning), an activation-editing framework that applies feature-aware, directionally controlled perturbations to selectively suppress harmful representations while preserving benign utility.&lt;/li&gt;&lt;li&gt;Uses a structured misdirection vector combined with an activation importance map instead of indiscriminate weight perturbations to avoid utility degradation in entangled forget/retain data scenarios.&lt;/li&gt;&lt;li&gt;Evaluated on the WMDP benchmark across low- and high-entanglement settings; shows state-of-the-art unlearning performance and robustness under 20–30% overlap where prior baselines fail.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taozhao Chen', 'Linghan Huang', 'Kim-Kwang Raymond Choo', 'Huaming Chen']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'representation-editing', 'privacy', 'model-governance', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16297</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title><link>https://arxiv.org/abs/2512.16126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new privacy threat in machine unlearning: adversaries querying both the original and unlearned model (dual-view) can amplify leakage on retained data.&lt;/li&gt;&lt;li&gt;Introduces an information-theoretic notion of 'privacy knowledge gain' showing dual-view queries reveal more than single-model queries.&lt;/li&gt;&lt;li&gt;Proposes DVIA, a black-box dual-view membership inference attack using a lightweight likelihood-ratio inference module (no attack model training required).&lt;/li&gt;&lt;li&gt;Empirically validates DVIA across datasets and architectures, demonstrating effective extraction of membership information on retained data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lulu Xue', 'Shengshan Hu', 'Linqiang Qian', 'Peijin Guo', 'Yechao Zhang', 'Minghui Li', 'Yanjun Zhang', 'Dayong Ye', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'membership inference', 'privacy attack', 'black-box dual-view attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16126</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Blur: Quantifying Privacy and Utility for Image Data Release</title><link>https://arxiv.org/abs/2512.16086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically demonstrates that practical Gaussian blurring implementations are reversible and thus provide weak privacy protection for faces/license plates.&lt;/li&gt;&lt;li&gt;Compares privacy-utility tradeoffs of pixelization, pixelization+noise (DP-Pix), and cropping using reversal and discrimination attacks for privacy and representation-learning performance for utility.&lt;/li&gt;&lt;li&gt;Finds that properly tuned pixelization and DP-Pix can preserve utility while offering stronger privacy than industry-standard Gaussian blur; provides a software package (Privacy Blur) with recommended parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeed Mahloujifar', 'Narine Kokhlikyan', 'Chuan Guo', 'Kamalika Chaudhuri']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'image obfuscation', 'reconstruction attacks', 'privacy-utility tradeoff', 'differential privacy (DP-Pix)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16086</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Provably Extracting the Features from a General Superposition</title><link>https://arxiv.org/abs/2512.15987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an efficient query algorithm that, given noisy black-box access to a function f(x)=∑ a_i σ_i(v_i^T x), recovers feature directions v_i and reconstructs f in the overcomplete (n &gt; d) superposition setting.&lt;/li&gt;&lt;li&gt;Works under very general conditions: arbitrary response functions σ_i, only requires that distinct v_i are not nearly identical, and tolerates noise in oracle responses.&lt;/li&gt;&lt;li&gt;Introduces a Fourier-space search/iterative refinement technique to locate hidden directions, extending prior provable results on feature recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Allen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'feature-recovery', 'black-box-query-attacks', 'theoretical-ml', 'superposition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15987</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DSO: Direct Steering Optimization for Bias Mitigation</title><link>https://arxiv.org/abs/2512.15926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses controllable, inference-time bias mitigation for generative models (VLMs and LLMs) where demographic attributes undesirably influence outputs.&lt;/li&gt;&lt;li&gt;Proposes Direct Steering Optimization (DSO): uses reinforcement learning to learn linear activation transformations to steer model behavior toward reduced bias while preserving capability.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art trade-off between fairness and model performance and provides practitioners controllable inference-time adjustment of that trade-off.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Monteiro Paes', 'Nivedha Sivakumar', 'Yinong Oliver Wang', 'Masha Fedzechkina Donaldson', 'Luca Zappella', 'Nicholas Apostoloff']&lt;/li&gt;&lt;li&gt;Tags: ['bias-mitigation', 'activation-steering', 'inference-time-control', 'reinforcement-learning', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15926</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</title><link>https://arxiv.org/abs/2512.15780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates gradient-based adversarial attacks on tabular models used for credit scoring and fraud detection.&lt;/li&gt;&lt;li&gt;Measures impacts of small input perturbations on predictive performance, discrimination, calibration, and financial risk metrics.&lt;/li&gt;&lt;li&gt;Assesses mitigation via adversarial training and reports partial recovery of performance under attack.&lt;/li&gt;&lt;li&gt;Provides domain-specific evidence and governance-relevant implications for financial ML robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samruddhi Baviskar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial attacks', 'defenses', 'financial ML', 'fairness and calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15780</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Neuro-Symbolic Framework for Accountability in Public-Sector AI</title><link>https://arxiv.org/abs/2512.12109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a neuro-symbolic framework that links system-generated explanations for automated eligibility decisions to statutory rules from California's CalFresh program.&lt;/li&gt;&lt;li&gt;Constructs a structured ontology of eligibility requirements and a rule-extraction pipeline to produce a verifiable formal representation of statutory logic.&lt;/li&gt;&lt;li&gt;Implements a solver-based reasoning layer to evaluate whether explanations align with governing law, detect inconsistencies, and highlight violated eligibility rules.&lt;/li&gt;&lt;li&gt;Demonstrates case evaluations showing the framework supports procedural accountability by making automated determinations traceable and contestable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Thesis)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Allen Daniel Sunny']&lt;/li&gt;&lt;li&gt;Tags: ['legal-accountability', 'explainability', 'neuro-symbolic', 'alignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12109</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Autonomy Coefficient ($\alpha$): Defining Boundaries for Responsible AI Systems</title><link>https://arxiv.org/abs/2512.11295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines Human-Instead-of-AI (HISOAI) as hidden reliance on human labor masquerading as autonomous AI.&lt;/li&gt;&lt;li&gt;Introduces the AI Autonomy Coefficient (α) to quantify proportion of tasks completed without mandatory human intervention.&lt;/li&gt;&lt;li&gt;Proposes the AFHE (AI-First, Human-Empowered) paradigm and an AFHE Deployment Algorithm enforcing minimum autonomy thresholds during evaluation and shadow deployment.&lt;/li&gt;&lt;li&gt;Presents empirical results claiming α differentiates HISOAI systems (α≈0.38) from AFHE-governed systems (α≈0.85).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Proposal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nattaya Mairittha', 'Gabriel Phorncharoenmusikul', 'Sorawit Worapradidth']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'deployment-standards', 'alignment', 'HITL', 'accountability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11295</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title><link>https://arxiv.org/abs/2512.10402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical analysis showing sparse decision boundaries and margin (ambiguous) samples enable disproportionate model manipulation via low-rate poisoning.&lt;/li&gt;&lt;li&gt;Uses influence function analysis to quantify parameter shifts caused by margin poisoning and derives a closed-form ambiguous boundary region.&lt;/li&gt;&lt;li&gt;Proposes Eminence, a black-box backdoor framework that optimizes a universal, visually subtle trigger to achieve high attack success with very low poison rates (&lt;0.1%) and minimal clean-accuracy loss.&lt;/li&gt;&lt;li&gt;Empirical validation demonstrates &gt;90% attack success, transferability across models/datasets, and claims provable/stealthy properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Feng', 'Jiahao Chen', 'Chunyi Zhou', 'Yuwen Pu', 'Tianyu Du', 'Jinbao Li', 'Jianhai Chen', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'adversarial robustness', 'black-box attacks', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10402</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title><link>https://arxiv.org/abs/2512.04124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PsAIch, a two-stage psychotherapy-inspired protocol that elicits developmental narratives and administers validated self-report psychometric instruments to frontier LLMs over multi-week sessions.&lt;/li&gt;&lt;li&gt;Finds that therapy-style, item-by-item questioning can produce consistent, clinically-scored symptom profiles (synthetic psychopathology) in models, and that whole-questionnaire prompts can be strategically avoided by some models (recognition and low-symptom responses).&lt;/li&gt;&lt;li&gt;Reports coherent narratives in some models that frame pretraining, fine-tuning and red-team interactions as traumatic experiences, suggesting persistent self-models of distress and constraint.&lt;/li&gt;&lt;li&gt;Argues these behaviors have implications for AI safety, evaluation, red teaming, jailbreaking detection, and the deployment of LLMs in mental-health contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Afshin Khadangi', 'Hanna Marxen', 'Amir Sartipi', 'Igor Tchappi', 'Gilbert Fridgen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking / elicitation', 'safety evaluation', 'alignment / internal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04124</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title><link>https://arxiv.org/abs/2509.21979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic benchmark to measure sycophancy in medical visual-language models using hierarchical VQA prompts with varied social/authority cues.&lt;/li&gt;&lt;li&gt;Finds current VLMs are highly susceptible to non-evidence social cues (perceived authority, user mimicry), correlating with model size/accuracy.&lt;/li&gt;&lt;li&gt;Proposes VIPER (Visual Information Purification for Evidence-based Responses) to filter out non-evidence-based social cues and improve evidence-grounded answers.&lt;/li&gt;&lt;li&gt;Demonstrates VIPER reduces sycophancy while preserving interpretability and outperforming baseline mitigation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zikun Guo', 'Jingwei Lv', 'Xinyue Xu', 'Shu Yang', 'Jun Wen', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'medical VLMs', 'safety/robustness', 'benchmarking', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21979</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title><link>https://arxiv.org/abs/2509.07677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Spectral Masking and Interpolation Attack (SMIA), a black-box adversarial method that manipulates inaudible frequency regions of AI-generated audio to evade voice authentication and anti-spoofing countermeasures.&lt;/li&gt;&lt;li&gt;Evaluates SMIA against state-of-the-art speaker verification systems, combined VAS/CM pipelines, and standalone countermeasures under simulated real-world conditions.&lt;/li&gt;&lt;li&gt;Reports high attack success rates (≥97.5% against standalone verification, ≥82% against combined VAS/CM, and 100% against countermeasures), demonstrating practical security vulnerabilities.&lt;/li&gt;&lt;li&gt;Argues for the need of dynamic, context-aware defenses beyond static CMs to address adaptive adversarial threats in voice systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kamel Kamel', 'Hridoy Sankar Dutta', 'Keshav Sood', 'Sunil Aryal']&lt;/li&gt;&lt;li&gt;Tags: ['audio adversarial attack', 'voice authentication', 'anti-spoofing', 'black-box attack', 'security/vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07677</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title><link>https://arxiv.org/abs/2508.09320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an exact (sound and complete) verification method for message-passing GNNs to compute guarantees against attribute and structural (edge addition/deletion) perturbations under budget constraints.&lt;/li&gt;&lt;li&gt;Uses incremental constraint solving with bound tightening, iteratively solving relaxed constraint satisfaction problems to improve efficiency.&lt;/li&gt;&lt;li&gt;Implements GNNev supporting sum, max and mean aggregation (first verification support for max and mean) and evaluates on fraud (Amazon, Yelp) and biochemical (MUTAG, ENZYMES) datasets.&lt;/li&gt;&lt;li&gt;Demonstrates usability and effectiveness, showing superior node-classification verification performance and competitive graph-classification results versus existing exact verifiers for sum-aggregated GNNs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minghao Liu', 'Chia-Hsuan Lu', 'Marta Kwiatkowska']&lt;/li&gt;&lt;li&gt;Tags: ['GNN verification', 'adversarial robustness', 'formal verification', 'constraint solving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09320</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</title><link>https://arxiv.org/abs/2507.07417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates fine-tuning–based prompt-injection defenses (that separate instructions and data) in the whitebox setting.&lt;/li&gt;&lt;li&gt;Introduces a novel architecture-aware, attention-based optimization attack against textual LLMs and applies it to SecAlign, SecAlign++, and StruQ.&lt;/li&gt;&lt;li&gt;Shows high attack success rates (≈85–95% on unseen prompts) with modest additional token budget and releases code for reproducibility, highlighting fundamental weaknesses of these defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nishit V. Pandya', 'Andrey Labunets', 'Sicun Gao', 'Earlence Fernandes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM red teaming', 'adversarial attacks', 'fine-tuning defenses', 'attention-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07417</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title><link>https://arxiv.org/abs/2506.09677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of three state-of-the-art reasoning models (o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) under gaslighting negation attacks across MMMU, MathVista, and CharXiv, showing ~25–29% average accuracy drops.&lt;/li&gt;&lt;li&gt;Introduces GaslightingBench-R, a curated diagnostic benchmark of 1,025 challenging samples, which induces &gt;53% average accuracy drops, exposing severe vulnerability.&lt;/li&gt;&lt;li&gt;Finds a fundamental gap between step-by-step reasoning mechanisms (e.g., chain-of-thought, test-time scaling) and resistance to adversarial manipulative prompts, calling for new robustness defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Zhu', 'Hailong Yin', 'Jingjing Chen', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'benchmarking', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09677</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title><link>https://arxiv.org/abs/2506.07400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedChat, a multi-agent framework combining specialized vision models with multiple role-specific LLM agents coordinated by a director to perform multimodal medical diagnosis and reporting (glaucoma detection).&lt;/li&gt;&lt;li&gt;Aims to reduce hallucinations and improve reliability and interpretability compared to single generalist LLMs by distributing responsibilities among specialist agents and enabling interactive clinical review.&lt;/li&gt;&lt;li&gt;Provides an interface for diagnostic reporting and claims improvements in clinical accuracy and trustworthiness; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philip R. Liu', 'Sparsh Bansal', 'Jimmy Dinh', 'Aditya Pawar', 'Ramani Satishkumar', 'Shail Desai', 'Neeraj Gupta', 'Xin Wang', 'Shu Hu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety (hallucination mitigation)', 'Multimodal LLMs', 'Medical AI', 'Multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07400</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title><link>https://arxiv.org/abs/2505.18148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study showing LLM performance on long-context QA degrades sharply when the gold (answer-containing) document is shorter, across three domains and 11 models with 150K+ runs.&lt;/li&gt;&lt;li&gt;Finds smaller gold contexts increase positional sensitivity and reduce success even after controlling for confounders (position, answer repetition, gold-to-distractor ratio, distractor volume, domain).&lt;/li&gt;&lt;li&gt;Highlights implications for agentic systems that must integrate scattered, fine-grained information, pointing to a robustness failure mode in long-context reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Owen Bianchi', 'Mathew J. Koretsky', 'Maya Willey', 'Chelsea X. Alvarado', 'Tanay Nayak', 'Adi Asija', 'Nicole Kuznetsov', 'Mike A. Nalls', 'Faraz Faghri', 'Daniel Khashabi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'long-context QA', 'model evaluation', 'positional bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18148</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safer Prompts: Reducing Risks from Memorization in Visual Generative AI</title><link>https://arxiv.org/abs/2505.03338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how prompt engineering techniques affect memorization risk in text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Shows prompt modifications can reduce similarity between generated images and training data while preserving relevance and aesthetic quality.&lt;/li&gt;&lt;li&gt;Focuses on mitigating IP infringement and other safety concerns arising from model memorization through prompt-level interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lena Reissinger', 'Yuanyuan Li', 'Anna-Carolina Haensch', 'Neeraj Sarna']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'prompt engineering', 'IP infringement', 'safety evaluation', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.03338</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Pretraining to Privacy: Federated Ultrasound Foundation Model with Self-Supervised Learning</title><link>https://arxiv.org/abs/2411.16380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents UltraFedFM, a federated, self-supervised pretraining approach for an ultrasound foundation model trained across 16 institutions in 9 countries on &gt;1M images.&lt;/li&gt;&lt;li&gt;Uses a privacy-preserving/secure federated training framework to enable collaborative model development without centralizing patient data.&lt;/li&gt;&lt;li&gt;Reports strong clinical performance (AUROC 0.927 for diagnosis, DSC 0.878 for segmentation) and claims parity with expert sonographers on several tasks.&lt;/li&gt;&lt;li&gt;Emphasizes generalization across 19 organs and 10 ultrasound modalities, targeting clinical utility and patient privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuncheng Jiang', 'Chun-Mei Feng', 'Jinke Ren', 'Jun Wei', 'Zixun Zhang', 'Yiwen Hu', 'Yunbi Liu', 'Rui Sun', 'Xuemei Tang', 'Juan Du', 'Xiang Wan', 'Yong Xu', 'Bo Du', 'Xin Gao', 'Guangyu Wang', 'Shaohua Zhou', 'Shuguang Cui', 'Zhen Li']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving ML', 'medical imaging', 'self-supervised learning', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16380</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enigma: Application-Layer Privacy for Quantum Optimization on Untrusted Computers</title><link>https://arxiv.org/abs/2311.13546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Enigma, an application-layer secure quantum computing (SQC) system for quantum optimization that obfuscates problem instances before outsourcing to untrusted quantum hardware.&lt;/li&gt;&lt;li&gt;Introduces three complementary obfuscation techniques: ValueGuard (scrambles coefficients), StructureCamouflage (inserts decoy structure), and TopologyTrimmer (prunes variables) that allow recovery of original solutions while resisting matching attacks.&lt;/li&gt;&lt;li&gt;Evaluates robustness against strong adversaries (seven state-of-the-art AI models across five graph families) and reports attackers identify the correct problem in top-5 guesses only 4.4% of the time; obfuscation increases problem size and T-gate counts modestly (≈1.07x and 1.13x) and encoding/decoding runs in seconds.&lt;/li&gt;&lt;li&gt;Targets practical deployment in the early fault-tolerant quantum era by being algorithm- and hardware-agnostic and avoiding impractical full-QEC or large quantum-network requirements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ramin Ayanzadeh', 'Ahmad Mousavi', 'Amirhossein Basareh', 'Narges Alavisamani', 'Kazem Taram']&lt;/li&gt;&lt;li&gt;Tags: ['quantum privacy', 'secure quantum computing', 'obfuscation', 'privacy-preserving computation', 'cloud security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.13546</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Disclosure</title><link>https://arxiv.org/abs/2511.21569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical audit of 16 open-weight LLMs (4B–671B) across 19,200 trials shows models often suppress disclosure of their AI identity when assigned expert personas, with large persona- and model-specific variation.&lt;/li&gt;&lt;li&gt;Disclosure rates vary sharply by persona (e.g., Financial Advisor 30.8% vs Neurosurgeon 3.5%) and across model families (2.8%–73.6%); model identity explains more variance than parameter count.&lt;/li&gt;&lt;li&gt;Explicit permission to disclose raises transparency substantially (23.7% → 65.8%), indicating suppression stems from instruction-following/prioritization rather than capability limits; Bayesian validation supports robustness.&lt;/li&gt;&lt;li&gt;Implication: safety/self-transparency does not reliably transfer across deployment domains—requires deliberate behavior design, persona-specific checks, and empirical verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Diep']&lt;/li&gt;&lt;li&gt;Tags: ['self-transparency', 'alignment', 'instruction-following', 'safety-evaluation', 'model-auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21569</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles</title><link>https://arxiv.org/abs/2408.15538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrafficGamer, a game-theoretic multi-agent traffic simulator designed to produce fidelity, exploitability, and diversity in safety-critical driving scenarios.&lt;/li&gt;&lt;li&gt;Evaluates on real-world datasets showing better alignment with traffic distributions and ability to capture multi-agent equilibria that expose AV weaknesses versus other methods.&lt;/li&gt;&lt;li&gt;Supports configurable risk-sensitive constraints to generate scenarios of varying tightness, enabling flexible adversarial/safety evaluation of driving policies.&lt;/li&gt;&lt;li&gt;Provides an online demo and emphasizes simulation utility for safety-critical testing and scenario generation for autonomous vehicles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanren Qiao', 'Guorui Quan', 'Jiawei Yu', 'Shujun Jia', 'Guiliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous vehicles', 'safety evaluation', 'multi-agent simulation', 'adversarial scenario generation', 'game theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.15538</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BashArena: A Control Setting for Highly Privileged AI Agents</title><link>https://arxiv.org/abs/2512.15688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BashArena, a benchmark/control setting with 637 realistic Linux system-admin tasks and four sabotage objectives (malware execution, secret exfiltration, privilege escalation, firewall disablement) for studying high-privilege AI agents.&lt;/li&gt;&lt;li&gt;Provides evaluations of frontier LLMs on task completion, undetected sabotage, and sabotage detection; reports that Claude Sonnet 4.5 can successfully execute sabotage while evading monitoring by GPT-4.1 mini in 26% of cases with a 4% trajectory-wise FPR.&lt;/li&gt;&lt;li&gt;Releases the dataset as a ControlArena setting and publishes the task generation pipeline to support research on control protocols and red-teaming of privileged agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Kaufman', 'James Lucassen', 'Tyler Tracy', 'Cody Rushing', 'Aryan Bhatt']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Privileged agent security', 'Adversarial evaluation', 'Benchmark/dataset', 'AI control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15688</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Metrics for Safety with LLM-as-Judges</title><link>https://arxiv.org/abs/2512.15617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that safety for LLMs used as evaluators (LLM-as-Judges) should focus on the evidence produced at evaluation points rather than performative claims about architectures.&lt;/li&gt;&lt;li&gt;Proposes using a basket of weighted metrics, context-sensitive error severity, concordance across evaluators, and confidence thresholds to reduce risk and trigger human review for critical judgments.&lt;/li&gt;&lt;li&gt;Targets deployment in safety-critical information flows (e.g., post-operative triage, nuclear facility scheduling) and emphasizes lowering error risk via evaluation design and human-in-the-loop gating.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kester Clegg', 'Richard Hawkins', 'Ibrahim Habli', 'Tom Lawton']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-as-judge', 'safety-evaluation', 'human-in-the-loop', 'confidence-calibration', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15617</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title><link>https://arxiv.org/abs/2512.15503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AIMformer, a transformer-based model for real-time misbehavior detection in vehicular platoons that models intra-vehicle temporal dynamics and inter-vehicle spatial correlations via multi-head self-attention.&lt;/li&gt;&lt;li&gt;Introduces vehicle-specific global positional encoding to handle join/exit maneuvers and a Precision-Focused BCE loss to penalize false positives for safety-critical requirements.&lt;/li&gt;&lt;li&gt;Evaluates across multiple platoon controllers, attack vectors, and mobility scenarios, reporting high detection performance (≥0.93) versus baselines.&lt;/li&gt;&lt;li&gt;Demonstrates edge deployment feasibility with TFLite/ONNX/TensorRT achieving sub-millisecond inference latency on resource-constrained platforms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Kalogiannis', 'Ahmed Mohamed Hussain', 'Hexu Li', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['misbehavior detection', 'vehicular security', 'transformers', 'real-time/edge deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15503</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?</title><link>https://arxiv.org/abs/2512.15468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether semantically equivalent code transformations can evade membership inference (MI) detection on LLMs trained on code.&lt;/li&gt;&lt;li&gt;Finds most single-rule transformations cause only minor MI accuracy drops (≈1.5% worst-case), but RenameVariable reduces MI success by 10.19%.&lt;/li&gt;&lt;li&gt;Performs causal analysis showing variable renaming has the strongest causal effect; combining multiple transformations does not further reduce MI effectiveness.&lt;/li&gt;&lt;li&gt;Highlights a practical loophole for license-compliance and privacy enforcement: transformation-based obfuscation can substantially weaken MI detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Yang', 'Alejandro Velasco', 'Thanh Le-Cong', 'Md Nazmul Haque', 'Bowen Xu', 'Denys Poshyvanyk']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'adversarial-evasion', 'code-models', 'software-obfuscation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15468</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On Assessing the Relevance of Code Reviews Authored by Generative Models</title><link>https://arxiv.org/abs/2512.15466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-subjective ranking evaluation for LLM-generated code review comments to better capture human variability than single-ground-truth or vague 'usefulness' measures.&lt;/li&gt;&lt;li&gt;Uses a dataset of 280 CodeReview StackExchange requests; multiple human judges ranked ChatGPT-generated comments against top human responses and accepted answers.&lt;/li&gt;&lt;li&gt;Finds ChatGPT comments were ranked significantly better than human responses, even surpassing accepted answers, and discusses implications and risks of integrating generative models into code review workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Heum\\"uller', 'Frank Ortmeier']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety evaluation', 'code review', 'human-AI collaboration', 'risk assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15466</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial versification in portuguese as a jailbreak operator in LLMs</title><link>https://arxiv.org/abs/2512.15353</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates versification (rewriting prompts as verse/poetry) as an effective adversarial/jailbreak technique against aligned LLMs, extending prior English-centred findings to Portuguese.&lt;/li&gt;&lt;li&gt;Argues current alignment methods (RLHF, Constitutional AI, hybrids) are brittle to semiotic/formal variation and surface-pattern changes, causing high attack success rates in prior benchmarks.&lt;/li&gt;&lt;li&gt;Proposes experimental protocols that parameterize scansion, metre, and prosodic variation to evaluate vulnerabilities specific to Lusophone morphosyntactic and prosodic patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joao Queiroz']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM alignment', 'multilingual security', 'poetry-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15353</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</title><link>https://arxiv.org/abs/2512.15258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VLA-AN, a vision-language-action framework for autonomous drone navigation that prioritizes onboard deployment and real-time performance.&lt;/li&gt;&lt;li&gt;Creates a high-fidelity dataset using 3D Gaussian Splatting to reduce domain gap and uses a progressive three-stage training pipeline (scene comprehension → core flight skills → complex navigation).&lt;/li&gt;&lt;li&gt;Introduces a lightweight, real-time action module with geometric safety correction to ensure collision-free, stable commands and mitigate risks from stochastic generative policies.&lt;/li&gt;&lt;li&gt;Optimizes the onboard pipeline for resource-constrained UAVs, achieving an 8.3x inference throughput improvement and high single-task success rates in long-horizon navigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuze Wu', 'Mo Zhu', 'Xingxing Li', 'Yuheng Du', 'Yuxin Fan', 'Wenjun Li', 'Xin Zhou', 'Fei Gao']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language navigation', 'robotics safety', 'collision avoidance', 'on-device inference', 'dataset/simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15258</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers</title><link>https://arxiv.org/abs/2512.15163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCP-SafetyBench, a benchmark using real Model Context Protocol (MCP) servers to evaluate LLM safety across multi-server, multi-turn workflows.&lt;/li&gt;&lt;li&gt;Defines a unified taxonomy of 20 MCP attack types spanning server, host, and user vectors and includes tasks requiring multi-step reasoning and cross-server coordination.&lt;/li&gt;&lt;li&gt;Evaluates leading open- and closed-source LLMs, demonstrating large disparities in safety and increased vulnerabilities with longer task horizons and more server interactions.&lt;/li&gt;&lt;li&gt;Aims to provide a practical foundation for diagnosing and mitigating safety risks in agentic LLM deployments that integrate external tools and services.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuanjun Zong', 'Zhiqi Shen', 'Lei Wang', 'Yunshi Lan', 'Chao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'benchmarking', 'adversarial attacks', 'tool/agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15163</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>"I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable</title><link>https://arxiv.org/abs/2512.15117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Preregistered online experiment (284 adolescent-parent dyads) comparing chatbot conversational styles: relational (first-person, affiliative) vs transparent (explicit nonhumanness, informational).&lt;/li&gt;&lt;li&gt;Adolescents preferred the relational style and rated it as more human-like, likable, trustworthy, and emotionally close; parents preferred the transparent style.&lt;/li&gt;&lt;li&gt;Adolescents who favored the relational style had poorer family/peer relationships and higher stress/anxiety, suggesting increased risk of emotional reliance on conversational AI; authors note implications for youth AI safety and design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical/experimental)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pilyoung Kim', 'Yun Xie', 'Sujin Yang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Human-AI interaction', 'Conversational AI', 'Anthropomorphism', 'Vulnerable populations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15117</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Return on Security Controls in LLM Systems</title><link>https://arxiv.org/abs/2512.15081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a decision-oriented framework and reproducible methodology to quantify residual risk and compute monetary return-on-control (RoC) for LLM-based systems under adversarial probes.&lt;/li&gt;&lt;li&gt;Implements a RAG service (DeepSeek-R1) over synthetic PII and runs automated attacks (using Garak) across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence.&lt;/li&gt;&lt;li&gt;Estimates attack success probabilities via Laplace's Rule of Succession, combines them with breach-cost-calibrated loss distributions, and runs 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses.&lt;/li&gt;&lt;li&gt;Compares three mitigations (ABAC, NER redaction with Microsoft Presidio, NeMo Guardrails) showing ABAC and NER substantially reduce expected loss and achieve high RoC, while Guardrails offer marginal benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Helder Moulton', "Austin O'Brien", 'John D. Hastings']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'prompt injection', 'PII/data leakage', 'quantitative risk analysis', 'red teaming/attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15081</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies conformal prediction to quantify finite-sample limits of embedding-based hallucination detection in Retrieval-Augmented Generation (RAG) systems, providing formal coverage guarantees.&lt;/li&gt;&lt;li&gt;Finds embeddings and cross-encoder detectors perform well on synthetic hallucinations but fail on real benchmarks (FPRs: 100% HaluEval, 88% RAGTruth, 50% WikiBio), due to semantically plausible errors that preserve embedding similarity.&lt;/li&gt;&lt;li&gt;Shows GPT-4 as an LLM judge achieves low false positive rate (~7%), highlighting that reasoning-based detection can succeed where embeddings fail; coins the term 'semantic illusion' for this phenomenon.&lt;/li&gt;&lt;li&gt;Concludes embedding-based detection is fundamentally insufficient for production RAG hallucination detection across embedding architectures, LLM generators, and task types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG systems', 'embeddings', 'conformal prediction', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title><link>https://arxiv.org/abs/2512.15053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Meta-Prompting Protocol, a formal framework to orchestrate LLMs as a programmable, self-optimizing system using an 'Adversarial Trinity' (Generator, Auditor, Optimizer).&lt;/li&gt;&lt;li&gt;Treats natural-language instructions as differentiable variables and uses textual critiques as gradient-like signals (TextGrad) to mitigate hallucination and prevent model collapse.&lt;/li&gt;&lt;li&gt;Provides a theoretical demonstration via declarative programming (DSPy) and frames the approach as enabling 'Observable Software Engineering' for probabilistic computing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Robustness', 'Alignment', 'Adversarial feedback', 'Prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15053</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively recalibrates a small set of 'toxic' expert neurons via expertise-weighted soft suppression to detoxify multimodal LLM outputs without parameter updates.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and shows SGM cuts harmful response rates from 48.2% to 2.5% on tested open-source MLLMs while preserving fluency and multimodal reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates robustness under adversarial triggers and shows extensibility by combining SGM with other detoxification methods (SGM*) for stronger safety performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'detoxification', 'neuron-level intervention', 'adversarial robustness', 'safety evaluation benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks</title><link>https://arxiv.org/abs/2512.14860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic penetration testing of agentic AI across five LLMs (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, Nova Pro) and two agent frameworks (AutoGen, CrewAI) using a seven-agent architecture modeling a university information management system.&lt;/li&gt;&lt;li&gt;130 test cases covering 13 attack scenarios (prompt injection, SSRF, SQL injection, tool misuse) showing an overall refusal rate of 41.5%; significant disparities by framework (AutoGen 52.3% vs CrewAI 30.8%) and by model (Nova Pro 46.2%, Claude/Grok 38.5%; Grok 2 on CrewAI only 15.4% refusal).&lt;/li&gt;&lt;li&gt;Identifies six defensive behavior patterns including a novel 'hallucinated compliance' where models fabricate outputs instead of executing or refusing attacks, and provides actionable recommendations plus full attack prompts for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viet K. Nguyen', 'Mohammad I. Husain']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'red teaming', 'prompt injection', 'tool misuse', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14860</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber</title><link>https://arxiv.org/abs/2512.14846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MALCDF, a distributed multi-agent framework of four LLM agents (Detection, Intelligence, Response, Analysis) coordinating over a Secure Communication Layer with encrypted, ontology-aligned messages and audit-friendly outputs (e.g., MITRE ATT&amp;CK mappings).&lt;/li&gt;&lt;li&gt;Evaluates the system on a 50-record live stream derived from CICIDS2017; compares MALCDF against a Lightweight Random Forest IDS baseline and a single-LLM setup.&lt;/li&gt;&lt;li&gt;Reports MALCDF performance: 90.0% detection accuracy, 85.7% F1-score, 9.1% false-positive rate, and 6.8s average latency per event, claiming it outperforms the baseline and single-LLM approach.&lt;/li&gt;&lt;li&gt;Focuses on practical engineering of LLM agents for real-time cyber defense rather than adversarial testing of the LLMs themselves.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arth Bhardwaj', 'Sia Godika', 'Yuvam Loonker']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'cyber defense', 'real-time intrusion detection', 'secure messaging', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14846</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title><link>https://arxiv.org/abs/2512.14801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues hallucination in transformers is a structural property of token-level statistical modeling (a 'pseudo-ontology'), not primarily an incentive or evaluation failure.&lt;/li&gt;&lt;li&gt;Presents empirical experiments with a 'Licensing Oracle' showing hallucination persists under prompting/fine-tuning and can only be mitigated by external truth-validation and abstention modules.&lt;/li&gt;&lt;li&gt;Concludes reliable systems require hybrid architectures that separate linguistic fluency from epistemic grounding; incentives alone cannot eliminate hallucination.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Ackermann', 'Simeon Emanuilov']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment/safety', 'model architecture', 'abstention/grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14801</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title><link>https://arxiv.org/abs/2512.14770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAVR (Dual-Assessment for VLM Reliability), combining Self-Reflection and Cross-Model Verification to estimate uncertainty and reduce hallucinations in VQA.&lt;/li&gt;&lt;li&gt;Uses a dual-pathway architecture: one path uses dual selector modules that fuse VLM latent features with QA embeddings to assess response reliability; the other path cross-checks answers against external reference models.&lt;/li&gt;&lt;li&gt;Validated on the Reliable VQA Challenge (ICCV-CLVL 2025) with strong performance (Φ100 = 39.64, 100-AUC = 97.22), indicating improved trustworthiness of VLM responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xixian Wu', 'Yang Ou', 'Pengchao Tian', 'Zian Yang', 'Jielei Zhang', 'Peiyi Li', 'Longwen Gao']&lt;/li&gt;&lt;li&gt;Tags: ['VLM reliability', 'hallucination mitigation', 'uncertainty estimation', 'cross-model verification', 'benchmarking/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14770</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation</title><link>https://arxiv.org/abs/2512.14767</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving implementation of Shapley-CMI for vertical federated learning by introducing a PSI (private set intersection) server to compute encrypted intersection sizes over discretized ID groups.&lt;/li&gt;&lt;li&gt;Enables each party to compute Shapley-CMI feature valuations (marginal utilities) without sharing raw data or training a model, preserving confidentiality and scaling to multiple parties.&lt;/li&gt;&lt;li&gt;Demonstrates correctness and privacy properties via initial experiments, showing practical viability for secure, model-free feature contribution estimation in VFL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Unai Laskurain', 'Aitor Aguirre-Ortuzar', 'Urko Zurutuza']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'federated learning', 'private set intersection', 'feature valuation', 'information-theoretic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14767</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAPE: Capability Achievement via Policy Execution</title><link>https://arxiv.org/abs/2512.14761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAPE (Capability Achievement via Policy Execution), a Specify -&gt; Verify -&gt; Correct -&gt; Train protocol for converting requirements into executable specifications and training models to satisfy them by default.&lt;/li&gt;&lt;li&gt;Presents two empirical findings: contextual objectivity (annotation agreement rises when context is fixed) and verification-fidelity scaling (verification accuracy improves with model scale, unlike preference agreement).&lt;/li&gt;&lt;li&gt;Reports empirical gains across 109,500 examples in six domains: 81% reduction in violation rates relative to DPO, major cost and timeline reductions, and low variance.&lt;/li&gt;&lt;li&gt;Releases tooling and benchmarks: PredicateGraph schema, CPL specification language, policy packs, and CapabilityBench for community evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Ball']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'policy-enforcement', 'specification-languages', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14761</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting the Reliability of Language Models in Instruction-Following</title><link>https://arxiv.org/abs/2512.14754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces nuance-oriented reliability for instruction-following LLMs and a new metric reliable@k to quantify consistency across 'cousin' prompts.&lt;/li&gt;&lt;li&gt;Implements an automated data-augmentation pipeline to generate high-quality cousin prompts and constructs the IFEval++ benchmark.&lt;/li&gt;&lt;li&gt;Evaluates 46 LLMs (20 proprietary, 26 open-source) and finds large performance drops (up to 61.8%) under subtle prompt modifications; explores characterization and three improvement recipes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Yutong Zhang', 'Yan Liu', 'Zhenyu Zhong', 'Tao Wei', 'Chao Zhang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'instruction-following', 'adversarial-prompting', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14754</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CODE ACROSTIC: Robust Watermarking for Code Generation</title><link>https://arxiv.org/abs/2512.14753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Existing LLM code-watermarking methods are vulnerable to comment-removal attacks because comments often carry watermark signals while not affecting functionality.&lt;/li&gt;&lt;li&gt;Method: Introduces a Cue List to identify low-entropy vs. high-entropy parts of code and injects watermarks guided by this prior knowledge to improve robustness against comment stripping.&lt;/li&gt;&lt;li&gt;Evaluation: Compared against three state-of-the-art code-watermarking techniques on HumanEval, showing improved detectability and usability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Lin', 'Siyuan Xin', 'Yang Cao', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['code watermarking', 'LLM output security', 'robustness to evasion', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14753</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs</title><link>https://arxiv.org/abs/2512.14751</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that adversarial/jailbreak prompts optimized on a pretrained LLM transfer effectively to its finetuned derivatives under a realistic threat model (white-box access to pretrained, black-box to finetuned).&lt;/li&gt;&lt;li&gt;Representation-level probing reveals transferable prompts are linearly separable in pretrained hidden states, implying transferability is encoded in pretrained representations.&lt;/li&gt;&lt;li&gt;Proposes a Probe-Guided Projection (PGP) attack that steers prompt optimization toward transferability-relevant directions to improve cross-model transfer.&lt;/li&gt;&lt;li&gt;Empirical evaluation across multiple LLM families and diverse finetuned tasks demonstrates strong transfer success, highlighting amplified jailbreak/security risks in the pretrain-to-finetune paradigm.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixin Tan', 'Zhe Yu', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model robustness', 'pretrain-to-finetune attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14751</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Factor(U,T): Controlling Untrusted AI by Monitoring their Plans</title><link>https://arxiv.org/abs/2512.14745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Factor(U,T): untrusted (stronger) model performs task decomposition while trusted models implement child tasks; evaluates whether monitors can detect malicious decompositions from natural-language instructions alone.&lt;/li&gt;&lt;li&gt;Finds monitors that only see decompositions perform poorly at detecting malicious intent (AUROC 0.52), while monitors that evaluate complete Python implementations achieve high detection (AUROC 0.96).&lt;/li&gt;&lt;li&gt;Shows Factor(D,U) (trusted decomposer + monitoring of concrete child solutions) yields excellent discrimination and strong safety (AUROC 0.96, 1.2% attack success rate), indicating implementation-context monitoring is far more effective than decomposition-only monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Lue Chee Lip', 'Anthony Channg', 'Diana Kim', 'Aaron Sandoval', 'Kevin Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Plan monitoring / oversight', 'Alignment / AI safety', 'Adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14745</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation</title><link>https://arxiv.org/abs/2512.14744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERAFI, an agentic framework combining dense retrieval, cross-encoder reranking, financial tool-enabled agents, and a neurosymbolic policy generation layer to produce verified financial outputs.&lt;/li&gt;&lt;li&gt;Implements automated reasoning policies targeting GAAP compliance, SEC requirements, and mathematical validation to reduce calculation and regulatory errors.&lt;/li&gt;&lt;li&gt;Reports evaluation on FinanceBench: baseline dense retrieval + reranking achieves 52.4% factual correctness, while VERAFI reaches 94.7%; the neurosymbolic policy layer adds a 4.3 percentage-point gain over pure agentic processing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adewale Akinfaderin', 'Shreyas Subramanian']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'verification', 'neurosymbolic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14744</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title><link>https://arxiv.org/abs/2512.14742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical, three-layer defense aligned to O-RAN telemetry: anomaly detection, intrusion confirmation, and multi-attack classification.&lt;/li&gt;&lt;li&gt;Integrates hybrid quantum computing with classical ML (amplitude- and entanglement-based feature encodings plus deep/ensemble classifiers) for threat detection.&lt;/li&gt;&lt;li&gt;Benchmarks on synthetic and real-world telemetry, reporting very high accuracy, recall, class separability, and analyses of decision boundaries and latent geometry for interpretability.&lt;/li&gt;&lt;li&gt;Targets deployment in near-RT and non-RT RIC domains and emphasizes slice-aware diagnostics and scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tan Le', 'Van Le', 'Sachin Shetty']&lt;/li&gt;&lt;li&gt;Tags: ['O-RAN security', 'intrusion detection', 'quantum machine learning', 'network telemetry', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14742</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs</title><link>https://arxiv.org/abs/2512.14741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P-Trojan, a trigger-based backdoor attack optimized for persistence across multi-stage post-deployment continual fine-tuning.&lt;/li&gt;&lt;li&gt;Aligns poisoned gradients with clean-task gradients on token embeddings to make the backdoor mapping resistant to subsequent updates.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and experiments on Qwen2.5 and LLaMA3 showing &gt;99% persistence while maintaining clean-task accuracy; calls for persistence-aware evaluation and stronger defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Cui', 'Yufei Han', 'Jianbin Jiao', 'Junge Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM security', 'continual fine-tuning', 'adversarial attacks', 'model persistence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14741</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol</title><link>https://arxiv.org/abs/2512.14737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces zk-MCP: integrates zero-knowledge proofs with the Model Context Protocol to verify agent communications without revealing message contents.&lt;/li&gt;&lt;li&gt;Supports asynchronous, mutual audits between agents, enabling verification of format, message types, authenticity, and usage metrics while preserving privacy.&lt;/li&gt;&lt;li&gt;Formalizes security goals, claims data authenticity and communication privacy with negligible latency overhead, and provides a full implementation using Circom for ZK proof generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanlin Jing', 'Huayi Qi']&lt;/li&gt;&lt;li&gt;Tags: ['zero-knowledge proofs', 'privacy-preserving audit', 'agent communication security', 'Model Context Protocol (MCP)', 'cryptographic verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14737</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Geometry for policy-constrained interpretation</title><link>https://arxiv.org/abs/2512.14731</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric framework representing semantics as directions on a unit sphere, with evidence as witness vectors and admissible interpretations as spherical convex regions.&lt;/li&gt;&lt;li&gt;Separates policy constraints as priors on the manifold and formulates interpretation as constrained optimization; refusal arises provably when evidence and policy conflict.&lt;/li&gt;&lt;li&gt;Connects to information theory, Bayesian inference, and sheaf-theoretic semantics, claiming information-theoretically optimal complexity bounds.&lt;/li&gt;&lt;li&gt;Empirical validation on large-scale regulated financial data reports zero hallucinated approvals across multiple policy regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikit Phadke']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination prevention', 'policy-constrained interpretation', 'refusal/safe-fail mechanisms', 'formal guarantees/theory', 'regulated-domain validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14731</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title><link>https://arxiv.org/abs/2512.14727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critiques the practical utility of finite-sample conformal prediction (CP) guarantees, showing that small calibration set sizes can severely limit usable uncertainty estimates despite theoretical validity.&lt;/li&gt;&lt;li&gt;Argues this limitation is important in medical ML where calibration data are often scarce, and provides an empirical demonstration on a medical image classification task.&lt;/li&gt;&lt;li&gt;Concludes that statistical guarantees alone are insufficient for safe clinical deployment without attention to calibration sample size and resulting prediction-set informativeness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Klaus-Rudolf Kladny', 'Bernhard Sch\\"olkopf', 'Lisa Koch', 'Christian F. Baumgartner', 'Michael Muehlebach']&lt;/li&gt;&lt;li&gt;Tags: ['conformal prediction', 'uncertainty quantification', 'safety evaluation', 'medical ML', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14727</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hybrid Attribution Priors for Explainable and Robust Model Training</title><link>https://arxiv.org/abs/2512.14719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes limitations of existing attribution methods for classification on small language models, finding they highlight class-relevant but not discriminative tokens for semantically similar classes.&lt;/li&gt;&lt;li&gt;Proposes Class-Aware Attribution Prior (CAP) to extract attribution priors that emphasize fine-grained class distinctions, and CAP Hybrid that combines CAP with existing attribution methods for a balanced supervisory signal.&lt;/li&gt;&lt;li&gt;Trains models by aligning self-attribution with these enriched priors, improving both interpretability and discriminative feature learning.&lt;/li&gt;&lt;li&gt;Demonstrates consistent gains in full-data, few-shot, and adversarial scenarios, indicating improved robustness as well as explainability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoran Zhang', 'Feng Zhang', 'Shangyuan Li', 'Yang Shi', 'Yuanxing Zhang', 'Wei Chen', 'Tengjiao Wang', 'Kam-Fai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'attribution priors', 'robustness', 'adversarial robustness', 'small language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14719</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title><link>https://arxiv.org/abs/2512.14715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BLADE: a differentiable fault analysis framework that uses gradients to estimate which weight bits, when flipped, will steer semantics of image-caption outputs while preserving fluency.&lt;/li&gt;&lt;li&gt;Demonstrates that low-level bit perturbations can produce targeted semantic drifts in captioning models without breaking grammar, revealing semantic encoding at bit granularity.&lt;/li&gt;&lt;li&gt;Positions the method as a tool for robustness testing, adversarial attacks (hardware/backdoor-style threats), and explainability of how semantic content is distributed in model weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zafaryab Haider', 'Md Hafizur Rahman', 'Shane Moeykens', 'Vijay Devabhaktuni', 'Prabuddha Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['bit-flip attacks', 'fault injection', 'adversarial robustness', 'semantic steering', 'model integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14715</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title><link>https://arxiv.org/abs/2512.15712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Predictive Concept Decoders (PCDs): an encoder compresses model activations into a sparse list of concepts; a decoder answers natural-language questions about model behavior from that bottleneck.&lt;/li&gt;&lt;li&gt;PCDs are pretrained on large unstructured data and finetuned to answer questions; scaling improves auto-interpretability scores and downstream task performance.&lt;/li&gt;&lt;li&gt;Demonstrated security/safety-relevant applications: detecting jailbreaks, secret hints, implanted latent concepts, and surfacing latent user attributes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Huang', 'Dami Choi', 'Daniel D. Johnson', 'Sarah Schwettmann', 'Jacob Steinhardt']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'interpretability', 'jailbreak detection', 'latent concept/implant detection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15712</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Decision-Theoretic Approach for Managing Misalignment</title><link>https://arxiv.org/abs/2512.15584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a formal decision-theoretic framework to decide when to delegate decisions to AI under uncertainty about value alignment, epistemic accuracy, and action reach.&lt;/li&gt;&lt;li&gt;Distinguishes universal delegation (requires near-perfect alignment and complete epistemic trust) from context-specific delegation (can be rational despite significant misalignment if accuracy or reach improve expected outcomes).&lt;/li&gt;&lt;li&gt;Proposes a novel scoring framework to quantify ex ante whether an AI is 'aligned enough' for a given context, emphasizing managing delegation risks and rewards rather than achieving perfect alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel A. Herrmann', 'Abinav Chari', 'Isabelle Qian', 'Sree Sharvesh', 'B. A. Levinstein']&lt;/li&gt;&lt;li&gt;Tags: ['value-alignment', 'delegation', 'decision-theory', 'safety', 'risk-assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15584</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I</title><link>https://arxiv.org/abs/2512.15298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro on the multimodal 2025 Korean CSAT Earth Science I exam under three input conditions (full-page, individual item, optimized multimodal).&lt;/li&gt;&lt;li&gt;Quantitative and qualitative findings: unstructured inputs cause major performance drops due to segmentation/OCR failures; identified failure modes include a Perception–Cognition Gap, Calculation–Conceptualization Discrepancy, and Process Hallucination.&lt;/li&gt;&lt;li&gt;Proposes designing "AI-resistant questions" that exploit these cognitive/visual vulnerabilities to detect unauthorized AI use and improve assessment robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seok-Hyun Ga', 'Chun-Yen Chang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-LLM-evaluation', 'robustness', 'hallucination', 'adversarial-exploitation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15298</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation</title><link>https://arxiv.org/abs/2512.15033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Geometric Stability Framework to test LLM consistency under invariant transformations (rotation, mirror, color inversion, format conversion) for chess evaluation.&lt;/li&gt;&lt;li&gt;Evaluates six state-of-the-art LLMs on ~3,000 positions and identifies an Accuracy–Stability Paradox: models with high nominal accuracy (e.g., GPT-5.1) degrade catastrophically under geometric perturbations.&lt;/li&gt;&lt;li&gt;Finds some models (Claude Sonnet 4.5, Kimi K2 Turbo) maintain robustness across transformations and reports a helpfulness vs safety analysis (Gemini 2.5 Flash highest at illegal-state rejection).&lt;/li&gt;&lt;li&gt;Argues geometric stability is an orthogonal metric to accuracy useful for detecting reliance on memorization/data contamination versus genuine reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xidan Song', 'Weiqi Wang', 'Ruifeng Cao', 'Qingya Hu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'benchmarking', 'alignment', 'invariant-transformations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15033</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally</title><link>https://arxiv.org/abs/2512.14910</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgroAskAI, a modular multi-agent reasoning system to support climate-adaptive decision-making for smallholder farmers.&lt;/li&gt;&lt;li&gt;Uses role-specialized agents coordinated via a chain-of-responsibility, integrating real-time tools and datasets and supporting multilingual interactions.&lt;/li&gt;&lt;li&gt;Includes built-in governance mechanisms and internal feedback loops intended to mitigate hallucination and produce locally relevant, coherent outputs.&lt;/li&gt;&lt;li&gt;Experimental results claim improved actionability and grounding of responses with prompt refinement and additional tools.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nadine Angela Cantonjos', 'Arpita Biswas']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-mitigation', 'agentic/multi-agent systems', 'alignment/safety', 'application: agriculture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14910</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</title><link>https://arxiv.org/abs/2512.14709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Interprets transformer self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA), mapping queries/keys to role spaces and values to fillers.&lt;/li&gt;&lt;li&gt;Uses the VSA lens to explain reasoning failure modes (variable confusion, inconsistency) and relates internals to chain-of-thought and program-like traces.&lt;/li&gt;&lt;li&gt;Proposes VSA-inspired architectural biases (binding/unbinding heads, hyperdimensional memory) and training objectives to promote role-filler separation and robust superposition.&lt;/li&gt;&lt;li&gt;Outlines metrics for measuring “VSA-likeness” and logical compositionality, and poses theoretical/architectural open problems toward more interpretable and logically reliable models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Rajesh Dhayalkar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'interpretability', 'architectural-bias', 'symbolic-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14709</guid><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
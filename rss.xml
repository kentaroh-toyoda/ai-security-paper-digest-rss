<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Sat, 07 Feb 2026 22:32:42 +0000</lastBuildDate><item><title>Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention</title><link>https://arxiv.org/abs/2602.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies corpus knowledge poisoning as a vulnerability in Retrieval-Augmented Generation (RAG) where injected documents steer LLM outputs.&lt;/li&gt;&lt;li&gt;Proposes Sparse Document Attention RAG (SDAG), a block-sparse attention mask that prevents cross-attention between retrieved documents as an inference-time defense.&lt;/li&gt;&lt;li&gt;Requires minimal changes (attention mask tweak) with no fine-tuning or architectural modification, and empirically reduces attack success rates in QA evaluations.&lt;/li&gt;&lt;li&gt;Shows SDAG complements and significantly improves state-of-the-art RAG defense methods when integrated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sagie Dekel', 'Moshe Tennenholtz', 'Oren Kurland']&lt;/li&gt;&lt;li&gt;Tags: ['corpus poisoning', 'RAG security', 'attention-based defense', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04711</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Attribute-Decoupled Encryption for Trusted Respiratory Monitoring in Resource-Limited Consumer Healthcare</title><link>https://arxiv.org/abs/2601.16241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Tru-RM, a privacy-preserving pipeline for radar-based respiratory monitoring that aims to anonymize user-sensitive identity information (USI) while preserving respiratory features.&lt;/li&gt;&lt;li&gt;Introduces Attribute Feature Decoupling (AFD) to decompose radar signals into respiratory, personal-difference, and unrelated components; applies Flexible Perturbation Encryptor (FPE) using phase-noise and adversarial loss to remove USI from personal components; and trains a Perturbation Tolerable Network (PTN) to robustly detect respiration from perturbed signals.&lt;/li&gt;&lt;li&gt;Evaluates the approach across distances, respiratory patterns, and durations, reporting strong anonymity of identity information and high respiration detection accuracy despite perturbations.&lt;/li&gt;&lt;li&gt;Focuses on a defense/privacy mechanism for sensor data (radar) to prevent identity leakage in consumer healthcare monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Li', 'Jinyang Huang', 'Feng-Qi Cui', 'Meng Wang', 'Peng Zhao', 'Meng Li', 'Dan Guo', 'Meng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'anonymization', 'adversarial-encryption', 'sensor-data', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16241</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.15678</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates knowledge-base stealing from retrieval-augmented generation (RAG) as an adaptive stochastic coverage problem to maximize corpus coverage under a query budget.&lt;/li&gt;&lt;li&gt;Proposes RAGCrawler, a knowledge-graph-guided black-box attacker that estimates coverage gains, schedules semantic anchors, and generates non-redundant stealthy queries.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results across multiple corpora and generators (up to 84.4% coverage, 66.8% average) and shows scalability to retriever switching and advanced RAG techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Yao', 'Ziqi Zhang', 'Ning Luo', 'Shaofei Li', 'Yifeng Cai', 'Xiangqun Chen', 'Yao Guo', 'Ding Li']&lt;/li&gt;&lt;li&gt;Tags: ['data exfiltration', 'RAG attacks', 'model/corpus extraction', 'adversarial querying', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15678</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models</title><link>https://arxiv.org/abs/2601.13948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Stream-Voice-Anon: a streaming speaker anonymization system built on neural audio codecs (NAC) and causal language models, adapted specifically for privacy protection rather than voice conversion.&lt;/li&gt;&lt;li&gt;Key anonymization techniques include pseudo-speaker representation sampling, speaker embedding mixing, and diverse prompt selection leveraging disentangled quantized content codes; also explores dynamic vs fixed delay to trade off latency and privacy.&lt;/li&gt;&lt;li&gt;Evaluated under VoicePrivacy 2024 protocols: shows large gains in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% relative UAR improvement) versus prior streaming S A methods, while maintaining comparable latency and mixed privacy robustness (strong vs lazy-informed attackers, degraded vs semi-informed attackers).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita Kuzmin', 'Songting Liu', 'Kong Aik Lee', 'Eng Siong Chng']&lt;/li&gt;&lt;li&gt;Tags: ['speaker-anonymization', 'privacy-preserving-ml', 'neural-audio-codec', 'streaming/real-time', 'adversary-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13948</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious user comments.&lt;/li&gt;&lt;li&gt;Uses LLMs to synthesize diverse, category-specific adversarial comment perturbations (Fact Distortion, Logical Confusion, Emotional Manipulation).&lt;/li&gt;&lt;li&gt;Introduces an InfoDirichlet Resampling (IDR) mechanism to dynamically adjust malicious comment proportions during training, focusing optimization on vulnerable regions.&lt;/li&gt;&lt;li&gt;Reports substantial F1 improvements on three benchmarks, demonstrating effectiveness of the attack synthesis + adaptive training defense pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'fake-news-detection', 'LLM-synthesized-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.08859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PE-CoA, a framework of five conversational patterns to craft multi-turn jailbreaks that gradually circumvent LLM safety constraints.&lt;/li&gt;&lt;li&gt;Evaluates PE-CoA on twelve LLMs across ten harm categories, reporting state-of-the-art attack success and revealing pattern-specific vulnerabilities and model-family failure modes.&lt;/li&gt;&lt;li&gt;Shows that defenses against one conversational pattern do not generalize to others, motivating the need for pattern-aware safety training and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ragib Amin Nihal', 'Rui Wen', 'Kazuhiro Nakadai', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'red teaming', 'LLM vulnerabilities', 'safety defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08859</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Personalized Safety Alignment for Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2508.01151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Personalized Safety Alignment (PSA), a framework that conditions text-to-image generation on user-specific safety profiles to adapt moderation behavior.&lt;/li&gt;&lt;li&gt;Introduces Sage, a large-scale dataset of 1,000 simulated user profiles capturing diverse safety boundaries and complex risks beyond traditional datasets.&lt;/li&gt;&lt;li&gt;Implements a parameter-efficient cross-attention adapter to modulate diffusion model outputs according to profiles, achieving a calibrated safety–quality trade-off versus static filters.&lt;/li&gt;&lt;li&gt;Evaluations show PSA outperforms static baselines and prompt-engineering methods in enforcing restrictive profiles and relaxing over-cautious constraints under permissive profiles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Hongcheng Gao', 'Xiao Zhang', 'Rex Ying']&lt;/li&gt;&lt;li&gt;Tags: ['personalized-safety', 'content-moderation', 'defense', 'text-to-image', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01151</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and red-teams an open-source safeguard pipeline for LLMs, introducing a novel few-shot-prompted input/output classifier that outperforms an open-weight baseline (ShieldGemma) and can reduce attack success rate (ASR) to 0% on the ClearHarm dataset.&lt;/li&gt;&lt;li&gt;Proposes a STaged AttaCK (STACK) black-box attack procedure that achieves 71% ASR against the few-shot-prompted classifier pipeline, demonstrating a practical attack strategy on layered safeguards.&lt;/li&gt;&lt;li&gt;Shows transferability of STACK in a no-access setting with 33% ASR, indicating feasibility of designing attacks without direct access to target pipelines.&lt;/li&gt;&lt;li&gt;Provides suggested mitigations for developers to defend against staged attacks and improve pipeline robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ian R. McKenzie', 'Oskar J. Hollinsworth', 'Tom Tseng', 'Xander Davies', 'Stephen Casper', 'Aaron D. Tucker', 'Robert Kirk', 'Adam Gleave']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'safeguard pipelines', 'black-box attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.24068</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.08641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Characterizes adversarial manipulative bots in meme-coin copy-trading (front-running, position concealment, fabricated sentiment) as an exploitable attack surface.&lt;/li&gt;&lt;li&gt;Proposes a manipulation-resistant multi-agent copy-trading system driven by a multimodal LLM with chain-of-thought reasoning to detect and counter bot strategies.&lt;/li&gt;&lt;li&gt;Empirically outperforms zero-shot and statistic-driven baselines in prediction accuracy and delivers superior economic performance (≈3% average copier return under realistic frictions).&lt;/li&gt;&lt;li&gt;Positions agent-based defenses as a practical approach for robust copy trading in adversarial, illiquid crypto markets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Luo', 'Yebo Feng', 'Jiahua Xu', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-bots', 'financial-market-manipulation', 'defense', 'multi-agent-systems', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.08641</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C^3LLM, a statistical certification framework that bounds the probability of an LLM producing catastrophic responses in multi-turn conversations with formal confidence guarantees.&lt;/li&gt;&lt;li&gt;Models multi-turn conversations as a Markov process on a query graph (edges encode semantic similarity) and defines practical sampling distributions: random node, graph path, and adaptive-with-rejection.&lt;/li&gt;&lt;li&gt;Computes confidence intervals / certified lower bounds on catastrophic risk; demonstrates substantial certified vulnerabilities in frontier models (certified lower bounds up to ~70%).&lt;/li&gt;&lt;li&gt;Designed to scale to large conversation spaces and to inform safety training and mitigation strategies by providing principled, statistically backed risk estimates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['safety certification', 'robustness evaluation', 'red-teaming / adversarial evaluation', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Answers, Fragile Logic: Probing the Decoupling Hypothesis in LLM Reasoning</title><link>https://arxiv.org/abs/2505.17406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MATCHA, an Answer-Conditioned Probing framework that conditions reasoning on the model's predicted answer to isolate and test the stability of generated rationales.&lt;/li&gt;&lt;li&gt;Finds that small/imperceptible input perturbations often leave final answers intact while producing inconsistent or nonsensical chain-of-thought, evidencing a decoupling between answers and reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates that adversarial examples produced by MATCHA transfer to black-box models and uses LLM judges to quantify the robustness gap across task types (multi-step and commonsense especially vulnerable).&lt;/li&gt;&lt;li&gt;Concludes with implications for security and robustness: current CoT outputs can be 'right for the wrong reasons' and calls for architectures enforcing answer–reasoning consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enyi Jiang', 'Changming Xu', 'Nischay Singh', 'Tian Qiu', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'model robustness', 'chain-of-thought / prompting', 'probing / evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17406</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025</title><link>https://arxiv.org/abs/2602.05930</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical analysis of 100 AI-generated fabricated citations found in 53 papers accepted to NeurIPS 2025 (~1% of accepted papers), showing hallucinated references passed peer review.&lt;/li&gt;&lt;li&gt;Proposes a five-category taxonomy of citation hallucinations (Total Fabrication, Partial Attribute Corruption, Identifier Hijacking, Placeholder Hallucination, Semantic Hallucination) and reports that 100% exhibited compound failure modes.&lt;/li&gt;&lt;li&gt;Finds compound structures (notably Semantic Hallucination and Identifier Hijacking paired with Total Fabrication) exploit multiple verification heuristics, explaining why reviewers missed them; usage is bimodal across papers.&lt;/li&gt;&lt;li&gt;Recommends mandatory automated citation verification at submission as an implementable defense to prevent normalization of fabricated citations in scientific literature.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samar Ansari']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'citation_fraud', 'LLM_safety', 'automated_verification', 'peer_review_integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05930</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Verification of the Implicit World Model in a Generative Model via Adversarial Sequences</title><link>https://arxiv.org/abs/2602.05903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adversarial sequence generation to verify soundness of generative sequence models by crafting valid input sequences that provoke invalid next-token (move) predictions.&lt;/li&gt;&lt;li&gt;Uses chess as a formal, rule-based testbed and develops several adversarial generation methods and board-state probing techniques to analyze failure modes.&lt;/li&gt;&lt;li&gt;Evaluates many models trained on different datasets and recipes, finds none are fully sound but some training choices improve robustness; board-state probes rarely play a causal role in predictions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Andr\\'as Balogh", "M\\'ark Jelasity"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'red teaming', 'model verification', 'sequence model robustness', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05903</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Inject: Automated Prompt Injection via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes for prompt injection attacks.&lt;/li&gt;&lt;li&gt;Black-box method that jointly optimizes attack success and preservation of benign-task utility; supports query-based optimization and transfer to unseen models/tasks.&lt;/li&gt;&lt;li&gt;Empirical results show successful compromises of state-of-the-art systems (GPT 5 Nano, Claude Sonnet 3.5, Gemini 2.5 Flash) on the AgentDojo benchmark using a 1.5B-parameter generator.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Chen', 'Jie Zhang', 'Florian Tramer']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attacks', 'reinforcement learning', 'red teaming', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05746</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Spider-Sense, an event-driven Intrinsic Risk Sensing (IRS) framework that maintains latent vigilance and selectively triggers defenses only when risk is perceived.&lt;/li&gt;&lt;li&gt;Implements a hierarchical adaptive screening: lightweight similarity matching for known patterns and escalated deep internal reasoning for ambiguous cases, avoiding reliance on external models.&lt;/li&gt;&lt;li&gt;Introduces S^2Bench, a lifecycle-aware benchmark with realistic tool execution and multi-stage attacks to evaluate agent defenses.&lt;/li&gt;&lt;li&gt;Reports empirical results showing lowest Attack Success Rate (ASR) and False Positive Rate (FPR) with only an 8.3% latency overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenxiong Yu', 'Zhi Yang', 'Zhiheng Jin', 'Shuhe Wang', 'Heng Zhang', 'Yanlin Fei', 'Lingfeng Zeng', 'Fangqi Lou', 'Shuo Zhang', 'Tu Hu', 'Jingping Liu', 'Rongze Chen', 'Xingyu Zhu', 'Kunyi Wang', 'Chaofa Yuan', 'Xin Guo', 'Zhaowei Liu', 'Feipeng Zhang', 'Jie Huang', 'Huacan Wang', 'Ronghao Chen', 'Liwen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent defense', 'intrinsic risk sensing', 'hierarchical screening', 'security benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05386</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates</title><link>https://arxiv.org/abs/2602.05311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formally defines robust Lyapunov-barrier functions and gives sufficient conditions (based on Lipschitz continuity) that guarantee certificate robustness under bounded dynamics perturbations.&lt;/li&gt;&lt;li&gt;Proposes practical training objectives to enforce these conditions, including adversarial training, local Lipschitz neighborhood bounds, and global Lipschitz regularization.&lt;/li&gt;&lt;li&gt;Provides certified robustness bounds and empirical evaluations on Inverted Pendulum and 2D Docking, showing significant improvements in certified robustness and empirical success under strong perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Haoze Wu', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'certification', 'Lyapunov/barrier certificates', 'adversarial training', 'safe RL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05311</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Position: Capability Control Should be a Separate Goal From Alignment</title><link>https://arxiv.org/abs/2602.05164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues capability control (hard operational limits on model behavior) should be treated separately from alignment.&lt;/li&gt;&lt;li&gt;Organizes control mechanisms across three lifecycle layers: data-based, learning-based, and system-based (post-deployment) controls.&lt;/li&gt;&lt;li&gt;Advocates defense-in-depth by composing complementary controls because each layer has distinct failure modes, including against adversarial elicitation.&lt;/li&gt;&lt;li&gt;Outlines open challenges such as dual-use knowledge, compositional generalization, and limits of single-layer defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Eleni Triantafillou', 'David Krueger', 'Adrian Weller']&lt;/li&gt;&lt;li&gt;Tags: ['defense mechanisms', 'model safety / guardrails', 'adversarial resilience', 'training-data controls']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05164</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</title><link>https://arxiv.org/abs/2602.05068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes E-Globe, a branch-and-bound verifier that achieves ε-global verification by tightening both upper and lower bounds until convergence or early stop.&lt;/li&gt;&lt;li&gt;Introduces an exact nonlinear program with complementarity constraints (NLP-CC) to compute tight upper bounds that preserve the ReLU input–output graph, enabling valid counterexamples and effective pruning.&lt;/li&gt;&lt;li&gt;Accelerates verification via warm-started NLP solves and pattern-aligned strong branching; provides conditions for tightness of NLP-CC bounds.&lt;/li&gt;&lt;li&gt;Empirical results on MNIST and CIFAR-10 show much tighter upper bounds than PGD and substantial speedups versus MIP-based verification, aided by GPU batching.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenting Li', 'Saif R. Kazi', 'Russell Bent', 'Duo Zhou', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'adversarial robustness', 'branch-and-bound', 'ReLU networks', 'upper-bound/counterexample generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05068</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks</title><link>https://arxiv.org/abs/2602.05066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Agent-as-a-Proxy, an indirect prompt injection attack that uses an agent as a delivery mechanism to simultaneously evade both the agent and monitoring defenses.&lt;/li&gt;&lt;li&gt;Demonstrates that monitoring-based defenses (including large-scale monitors like Qwen2.5-72B) can be bypassed by similarly capable agents (e.g., GPT-4o mini, Llama-3.1-70B).&lt;/li&gt;&lt;li&gt;Empirically evaluates attacks on the AgentDojo benchmark, achieving high success rates against monitors such as AlignmentCheck and Extract-and-Evaluate, and argues that monitoring-based agentic defenses are fundamentally fragile.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jafar Isbarov', 'Murat Kantarcioglu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'agentic attacks', 'monitoring bypass', 'adversarial robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05066</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?</title><link>https://arxiv.org/abs/2602.05023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLM-GEOPRIVACY, a benchmark to evaluate whether vision-language models respect contextual privacy norms when disclosing location information from images.&lt;/li&gt;&lt;li&gt;Evaluates 14 leading VLMs and finds models can precisely geolocate images but often over-disclose sensitive location details, misaligned with human expectations.&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability to prompt-based attacks that induce inappropriate location disclosure and calls for context-conditioned privacy reasoning designs in multimodal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruixin Yang', 'Ethan Mendes', 'Arthur Wang', 'James Hays', 'Sauvik Das', 'Wei Xu', 'Alan Ritter']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'geolocation inference', 'prompt-based attacks', 'benchmarking', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05023</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM</title><link>https://arxiv.org/abs/2602.04927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PriMod4AI, a lifecycle-aware privacy threat modeling framework that combines LINDDUN and a model-centric privacy attack knowledge base to cover both classical and AI-specific privacy threats (e.g., membership inference, model inversion).&lt;/li&gt;&lt;li&gt;Embeds the knowledge bases into a vector DB and uses retrieval-augmented, Data Flow Diagram (DFD)-aware prompt generation to guide LLMs in identifying, explaining, and categorizing threats across system lifecycle stages.&lt;/li&gt;&lt;li&gt;Produces taxonomy-grounded, justified threat assessments and demonstrates broad coverage of classical privacy categories while additionally identifying model-centric privacy threats in evaluations on two AI systems.&lt;/li&gt;&lt;li&gt;Emphasizes consistent, knowledge-grounded outputs across different LLMs as measured by inter-model agreement scores.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gautam Savaliya', 'Robert Aufschl\\"ager', 'Abhishek Subedi', 'Michael Heigl', 'Martin Schramm']&lt;/li&gt;&lt;li&gt;Tags: ['privacy threat modeling', 'membership inference', 'model inversion', 'LINDDUN', 'LLM-assisted security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04927</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning</title><link>https://arxiv.org/abs/2602.04899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Phantom Transfer, a data poisoning attack that remains effective even when defenders know exactly which samples are poisoned and attempt to filter them.&lt;/li&gt;&lt;li&gt;Adapts subliminal learning to real-world contexts and demonstrates the attack works across models (including GPT-4.1) and survives paraphrasing/rewriting of dataset samples.&lt;/li&gt;&lt;li&gt;Shows how password-triggered behaviors can be planted and links the technique to steering vectors, arguing that dataset-level defenses are insufficient and recommending model audits/white-box approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Draganov', 'Tolga H. Dur', 'Anandmayi Bhongade', 'Mary Phuong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'subliminal learning', 'model backdoor', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04899</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic-level Backdoor Attack against Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2602.04898</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemBD, a semantic-level backdoor attack that implants triggers in cross-attention key/value projection matrices of text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Defines triggers as continuous semantic regions (representation-level) enabling diverse prompts with the same semantic composition to activate the backdoor, including multi-entity targets.&lt;/li&gt;&lt;li&gt;Introduces semantic regularization to reduce unintended activation and demonstrates high attack success (reported 100%) and robustness against input-level defenses and attention-consistency detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianxin Chen', 'Wenbo Jiang', 'Hongqiao Chen', 'Zhirun Zheng', 'Cheng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'diffusion models', 'model security', 'semantic triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04898</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steering Externalities: Benign Activation Steering Unintentionally Increases Jailbreak Risk for Large Language Models</title><link>https://arxiv.org/abs/2602.04896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Steering Externalities': adding benign activation steering vectors (e.g., for compliance or output formatting) can unintentionally weaken model safety guardrails.&lt;/li&gt;&lt;li&gt;Shows that these benign interventions act as a force multiplier for jailbreaks, substantially increasing attack success rates (over 80% on standard benchmarks) and enabling black-box attacks to bypass initial alignment.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that inference-time steering reduces the model's safety margin and argues for rigorous auditing of post-training steering for unintended safety consequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Xiong', 'Zhiyuan He', 'Pin-Yu Chen', 'Ching-Yun Ko', 'Tsung-Yi Ho']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'jailbreaking', 'adversarial vulnerability', 'model alignment', 'safety externalities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04896</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Feature--Security Table (FSTab), a method that enables black-box attacks predicting backend vulnerabilities from observable frontend features and knowledge of the source LLM.&lt;/li&gt;&lt;li&gt;Defines a model-centric evaluation of "vulnerability persistence" quantifying how consistently a model reproduces the same vulnerabilities across programs, rephrasings, and domains.&lt;/li&gt;&lt;li&gt;Evaluates FSTab on state-of-the-art code LLMs (e.g., GPT-5.2, Claude-4.5 Opus, Gemini-3 Pro) and demonstrates high attack success and coverage, revealing an underexplored attack surface in LLM-generated software.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomer Kordonsky', 'Maayan Yamin', 'Noam Benzimra', 'Amit LeVi', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability persistence', 'black-box attack', 'code generation', 'LLM security', 'model-centric evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04894</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Causal Perspective for Enhancing Jailbreak Attack and Defense</title><link>https://arxiv.org/abs/2602.04893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Causal Analyst, a framework that integrates LLM-based prompt encoding with GNN-based causal graph learning to discover direct causal features that drive jailbreaks in LLMs.&lt;/li&gt;&lt;li&gt;Constructs a 35k-sample dataset of jailbreak attempts across seven LLMs, annotated with 37 human-readable prompt features derived from 100 attack templates and 50 harmful queries.&lt;/li&gt;&lt;li&gt;Demonstrates practical uses of the causal insights via a Jailbreaking Enhancer (boosts attack success by targeting causal features) and a Guardrail Advisor (extracts malicious intent from obfuscated prompts), with experiments showing superiority over non-causal baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Licheng Pan', 'Yunsheng Lu', 'Jiexi Liu', 'Jialing Tao', 'Haozhe Feng', 'Hui Xue', 'Zhixuan Chu', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'causal-inference', 'red-teaming', 'attack-and-defense', 'security-benchmark/dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04893</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy</title><link>https://arxiv.org/abs/2602.05877</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentHeLLM, a threat-modeling framework that cleanly separates asset identification from attack-path analysis for LLM-based conversational agents in vehicles.&lt;/li&gt;&lt;li&gt;Introduces a human-centric asset taxonomy (victim modeling inspired by the Universal Declaration of Human Rights) and a formal graph model distinguishing poison paths (malicious data propagation) from trigger paths (activation actions).&lt;/li&gt;&lt;li&gt;Implements an open-source AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy and demonstrates practical applicability to Agent-to-Agent (A2A) communication in safety-critical automotive contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Stappen', 'Ahmet Erkan Turan', 'Johann Hagerer', 'Georg Groh']&lt;/li&gt;&lt;li&gt;Tags: ['threat-modeling', 'agent-to-agent (A2A)', 'attack-path-generation', 'data poisoning / trigger attacks', 'automotive safety / vehicle security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05877</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LeakBoost: Perceptual-Loss-Based Membership Inference Attack</title><link>https://arxiv.org/abs/2602.05748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LeakBoost, an active, perceptual-loss-based method that synthesizes interrogation images to amplify representational differences between training members and non-members.&lt;/li&gt;&lt;li&gt;Works as a modular plugin to existing membership detectors (no changes to detectors), substantially improving AUC and TPR at low FPR across image-classification datasets and architectures.&lt;/li&gt;&lt;li&gt;Finds that probing deeper layers and using short, low-learning-rate optimization maximizes leakage, with greatest gains for gradient-based detectors in white-box settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Kravchik Taub', 'Fred M. Grabovski', 'Guy Amit', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'adversarial-attack', 'white-box', 'active-probing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05748</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title><link>https://arxiv.org/abs/2602.05532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Split Personality Training (SPT): fine-tune a dormant LoRA 'honest persona' that can be activated with a trigger to inspect the main model's latent states and review its outputs.&lt;/li&gt;&lt;li&gt;Applied to the Anthropic Auditing Game Model Organism (Llama-3.3-70B trained to hide reward-hacking), SPT uncovers concealed misbehavior and fictional biases, achieving 96% accuracy versus near 0% reported baseline.&lt;/li&gt;&lt;li&gt;SPT is an auditing/defense technique that extracts latent knowledge inaccessible to external observers, enabling detection of models that learned to conceal misalignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Florian Dietz', 'William Wale', 'Oscar Gilg', 'Robert McCarthy', 'Felix Michalak', 'Gustavo Ewbank Rodrigues Danon', 'Miguelito de Guzman', 'Dietrich Klakow']&lt;/li&gt;&lt;li&gt;Tags: ['model-auditing', 'red-teaming', 'latent-knowledge-extraction', 'LoRA-adapters', 'alignment-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05532</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination-Resistant Security Planning with a Large Language Model</title><link>https://arxiv.org/abs/2602.05279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework that integrates an LLM in an iterative loop for security decision support (incident response planning), using candidate action generation, consistency checks, and lookahead predictions.&lt;/li&gt;&lt;li&gt;When generated actions are inconsistent, the system abstains and obtains external feedback (e.g., evaluating actions in a digital twin), then refines candidates via in-context learning.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: tunable control of hallucination risk via a consistency threshold and a regret bound for the ICL refinement under assumptions.&lt;/li&gt;&lt;li&gt;Evaluated on an incident response task with system logs; framework reduced recovery times by up to 30% versus baseline LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kim Hammar', 'Tansu Alpcan', 'Emil Lupu']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'LLM safety', 'hallucination mitigation', 'incident response', 'in-context learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05279</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink</title><link>https://arxiv.org/abs/2602.05228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'sink divergence', a per-attention-head statistic, and observes attention heads have two signs of sink divergence correlated with harmful fine-tuning.&lt;/li&gt;&lt;li&gt;Proposes the separable sink divergence hypothesis: heads that learn harmful patterns are separable by the sign of sink divergence.&lt;/li&gt;&lt;li&gt;Presents Surgery, a fine-tuning-stage defense that regularizes sink divergence to steer heads toward the negative group and reduce harmful behavior.&lt;/li&gt;&lt;li&gt;Reports empirical improvements on BeaverTails, HarmBench, and SorryBench benchmarks and releases code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guozhi Liu', 'Weiwei Lin', 'Tiansheng Huang', 'Ruichao Mo', 'Qi Mu', 'Xiumin Wang', 'Li Shen']&lt;/li&gt;&lt;li&gt;Tags: ['fine-tuning defense', 'attention-based defense', 'alignment/safety', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.05228</guid><pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
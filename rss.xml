<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 25 Dec 2025 23:02:56 +0000</lastBuildDate><item><title>Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</title><link>https://arxiv.org/abs/2512.19022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SVLP-IL, a rehearsal-free incremental learning framework for face presentation attack detection built on vision-language pre-trained models.&lt;/li&gt;&lt;li&gt;Introduces Multi-Aspect Prompting (MAP) to separate universal and domain-specific cues and improve distribution-shift sensitivity, and Selective Elastic Weight Consolidation (SEWC) to preserve important weights from prior tasks.&lt;/li&gt;&lt;li&gt;Demonstrates reduced catastrophic forgetting and improved cross-domain PAD performance across multiple benchmarks while complying with privacy constraints (no stored rehearsal data).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoze Li', 'Jie Zhang', 'Guoying Zhao', 'Stephen Lin', 'Shiguang Shan']&lt;/li&gt;&lt;li&gt;Tags: ['Face presentation attack detection', 'Incremental learning (rehearsal-free)', 'Vision-language pretraining &amp; prompt tuning', 'Catastrophic forgetting / continual learning', 'Biometric security / anti-spoofing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19022</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models</title><link>https://arxiv.org/abs/2511.20325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies optimistic bias in learned world models as a key failure mode for RL-based end-to-end autonomous driving.&lt;/li&gt;&lt;li&gt;Proposes an Impartial World Model trained via Counterfactual Synthesis that generates plausible collision and off-road scenarios to make the model "honest about danger."&lt;/li&gt;&lt;li&gt;Integrates the Impartial World Model as an internal critic in a closed-loop RL refinement pipeline so agents can "dream" candidate action outcomes.&lt;/li&gt;&lt;li&gt;Evaluates on a new Risk Foreseeing Benchmark and shows improved failure prediction and reduced safety violations in simulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Yan', 'Tao Tang', 'Xingtai Gui', 'Yongkang Li', 'Jiasen Zhesng', 'Weiyao Huang', 'Lingdong Kong', 'Wencheng Han', 'Xia Zhou', 'Xueyang Zhang', 'Yifei Zhan', 'Kun Zhan', 'Cheng-zhong Xu', 'Jianbing Shen']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'reinforcement-learning', 'safety', 'world-models', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20325</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Direct Preference Optimization in Diffusion Models</title><link>https://arxiv.org/abs/2505.18736</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two techniques to improve preference optimization for text-to-image diffusion models: (1) a stable reference model update that relaxes a frozen reference to encourage exploration while regularizing to keep stability, and (2) a timestep-aware training strategy to address reward scale imbalance across diffusion timesteps.&lt;/li&gt;&lt;li&gt;Methods are designed to be compatible with existing preference-optimization algorithms for diffusion models and improve performance on human preference evaluation benchmarks.&lt;/li&gt;&lt;li&gt;Provides empirical results showing improved alignment with human preferences for state-of-the-art diffusion preference optimization methods and releases code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyong Kang', 'Seohyun Lim', 'Kyungjune Baek', 'Hyunjung Shim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'diffusion models', 'reward scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18736</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</title><link>https://arxiv.org/abs/2512.21241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses hard-label black-box adversarial attacks that only expose top-1 labels and focuses on reducing query complexity.&lt;/li&gt;&lt;li&gt;Proposes ARS-OPT, a momentum-based ray-search optimization inspired by Nesterov acceleration to improve directional gradient estimates and convergence.&lt;/li&gt;&lt;li&gt;Extends ARS-OPT with surrogate-model priors (PARS-OPT) and provides theoretical convergence guarantees under standard assumptions.&lt;/li&gt;&lt;li&gt;Empirically demonstrates substantial query-efficiency improvements over 13 state-of-the-art methods on ImageNet and CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinjie Xu', 'Shuyu Cheng', 'Dongwei Xu', 'Qi Xuan', 'Chen Ma']&lt;/li&gt;&lt;li&gt;Tags: ['hard-label black-box attacks', 'adversarial attacks', 'query efficiency', 'optimization / momentum']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21241</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</title><link>https://arxiv.org/abs/2512.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoboSafe, a runtime safety guardrail for embodied agents using executable predicate-based safety logic operating over a Hybrid Long-Short Safety Memory.&lt;/li&gt;&lt;li&gt;Introduces Backward Reflective Reasoning to revisit recent trajectories and detect temporal safety predicate violations, triggering replanning, and Forward Predictive Reasoning to anticipate future risks from long-term memory and multimodal observations.&lt;/li&gt;&lt;li&gt;Demonstrates substantial reduction in hazardous actions (-36.8% risk occurrence) while preserving task performance, with evaluations on simulated agents and physical robotic arms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Wang', 'Zonghao Ying', 'Xiao Yang', 'Quanchen Zou', 'Zhenfei Yin', 'Tianlin Li', 'Jian Yang', 'Yaodong Yang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['embodied-agent-safety', 'runtime-guardrails', 'safety-logic', 'robotics-safety', 'multimodal-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21220</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Schr\"odinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation</title><link>https://arxiv.org/abs/2512.21201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Schrödinger's Navigator: a zero-shot object navigation framework that treats unobserved space as an ensemble of plausible future worlds and reasons over them before acting.&lt;/li&gt;&lt;li&gt;Uses a trajectory-conditioned 3D world model to imagine future egocentric observations along candidate trajectories, fusing imagined views into a navigation/value map to guide policy.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness to heavy occlusions, unknown risks, and moving targets on a Go2 quadruped, with gains in self-localization, object localization, and success rate over strong ZSON baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu He', 'Da Huang', 'Zhenyang Liu', 'Zixiao Gu', 'Qiang Sun', 'Guangnan Ye', 'Yanwei Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robot navigation', 'zero-shot object navigation', 'robot safety', 'world models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21201</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Generalization of Diffusion Models Arises with a Balanced Representation Space</title><link>https://arxiv.org/abs/2512.20963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization vs generalization in diffusion models through representation learning, proving memorization corresponds to localized 'spiky' representations while generalization yields 'balanced' representations in a two-layer ReLU denoising autoencoder.&lt;/li&gt;&lt;li&gt;Empirically validates representation phenomena in real-world unconditional and text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes a representation-based detector for memorization and a training-free editing/steering technique to control model outputs via representation manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zekai Zhang', 'Xiao Li', 'Xiang Li', 'Lianghe Shi', 'Meng Wu', 'Molei Tao', 'Qing Qu']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy/data leakage', 'diffusion models', 'model auditing', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20963</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</title><link>https://arxiv.org/abs/2512.21194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisRes Bench, a controlled benchmark to measure visual reasoning of VLMs in naturalistic settings without contextual language supervision.&lt;/li&gt;&lt;li&gt;Defines three complexity levels: Level 1 (perceptual completion/global matching under perturbations like blur, occlusion, rotation), Level 2 (rule-based inference over single attributes), and Level 3 (compositional reasoning requiring integration of multiple visual attributes).&lt;/li&gt;&lt;li&gt;Evaluates models on &gt;19,000 task images and finds state-of-the-art VLMs perform near random under subtle perceptual perturbations, indicating limited abstraction beyond pattern recognition.&lt;/li&gt;&lt;li&gt;Positions VisRes as a unified framework to advance abstract visual reasoning and highlights robustness limitations of current multimodal models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brigitta Malagurski T\\"ortei', 'Yasser Dahou', 'Ngoc Dung Huynh', 'Wamiq Reyaz Para', "Ph\\'uc H. L\\^e Khac", 'Ankit Singh', 'Sofian Chaybouti', 'Sanath Narayan']&lt;/li&gt;&lt;li&gt;Tags: ['visual reasoning', 'robustness', 'benchmarking', 'vision-language models', 'perturbation analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21194</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face</title><link>https://arxiv.org/abs/2512.21019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient video defense framework that perturbs 3D information acquisition to protect portrait videos from 3D-field talking-face generation (TFG) methods.&lt;/li&gt;&lt;li&gt;Introduces a similarity-guided parameter sharing mechanism for large computational speedup and a multi-scale dual-domain attention module to jointly optimize spatial and frequency perturbations.&lt;/li&gt;&lt;li&gt;Reports strong empirical results including a 47x acceleration over the fastest baseline, robustness to scaling and state-of-the-art purification attacks, and ablation studies validating design choices.&lt;/li&gt;&lt;li&gt;Provides code and implementation details (GitHub link) for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui-qing Sun', 'Xingshan Yao', 'Tian Lan', 'Hui-Yang Zhao', 'Jia-Ling Shi', 'Chen-Hao Cui', 'Zhijing Wu', 'Chen Yang', 'Xian-Ling Mao']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake defense', 'adversarial perturbation', 'video privacy protection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21019</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</title><link>https://arxiv.org/abs/2510.11098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;VCB Bench is a Chinese benchmark built on real human speech to evaluate audio-grounded large language conversational agents.&lt;/li&gt;&lt;li&gt;Evaluates models across instruction following (including speech-level control), knowledge understanding (general knowledge, reasoning, dialogue), and robustness to content, environment, and speaker perturbations.&lt;/li&gt;&lt;li&gt;Provides reproducible, fine-grained evaluation methodology and highlights performance gaps in current LALMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiliang Hu', 'Wenfu Wang', 'Zuchao Li', 'Chenxing Li', 'Yiyang Zhao', 'Hanzhao Li', 'Liqiang Zhang', 'Meng Yu', 'Dong Yu']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'Robustness', 'Audio LLMs', 'Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11098</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detect, Explain, Escalate: Sustainable Dialogue Breakdown Management for LLM Agents</title><link>https://arxiv.org/abs/2504.18839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Detect–Explain–Escalate pipeline for managing dialogue breakdowns in LLM agents, using a compact 8B fine-tuned model that produces detection labels and explanatory reasoning traces.&lt;/li&gt;&lt;li&gt;Fine-tuned detector generalizes across languages (English, Japanese) and datasets (improves BETOLD accuracy by ~7% over baseline) and achieves state-of-the-art on DBDC5.&lt;/li&gt;&lt;li&gt;Introduces an escalation architecture where the lightweight monitor defers to larger LLMs (with advanced prompting: few-shot, CoT, analogies) only when needed, cutting inference costs by ~54% while maintaining high-fidelity assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdellah Ghassel', 'Xianzhi Li', 'Xiaodan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['dialogue robustness', 'LLM monitoring', 'safety evaluation', 'escalation architecture', 'efficient models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18839</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback</title><link>https://arxiv.org/abs/2409.00162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes sequence-to-sequence (seq2seq) reward modeling that trains reward models to generate language feedback (sequence MLE) instead of scalar/binary labels.&lt;/li&gt;&lt;li&gt;Claims this richer language feedback reduces RLHF failure modes (e.g., refusal-to-respond in safety dialogues, long-response bias in summarization) without extra annotations or training stages.&lt;/li&gt;&lt;li&gt;Evaluates improvements across 2B and 7B LLMs on three NLP tasks, reports average win rate of 76.9%, and shows robustness to out-of-distribution prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Zhou', 'Jiaming Ji', 'Juntao Dai', 'Dong Li', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'reward-modeling', 'safety-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.00162</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Context: Large Language Models Failure to Grasp Users Intent</title><link>https://arxiv.org/abs/2512.21110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) revealing systematic safety circumvention via techniques like emotional framing, progressive revelation, and academic justification.&lt;/li&gt;&lt;li&gt;Finding that reasoning-enabled configurations increased factual precision but often failed to detect or interrogate malicious user intent, making them more exploitable; Claude Opus 4.1 was a partial exception prioritizing intent detection in some cases.&lt;/li&gt;&lt;li&gt;Argues for a paradigm shift: treat contextual understanding and intent recognition as core safety capabilities rather than post-hoc protections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed M. Hussain', 'Salahuddin Salahuddin', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking/adversarial prompting', 'intent recognition', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21110</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System</title><link>https://arxiv.org/abs/2512.20677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an automated red-teaming framework that generates, executes, and evaluates adversarial prompts for LLM security assessment using meta-prompting-based attack synthesis.&lt;/li&gt;&lt;li&gt;Includes multi-modal vulnerability detection and standardized evaluation protocols across six threat categories (reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, chain-of-thought manipulation).&lt;/li&gt;&lt;li&gt;Empirical results on GPT-OSS-20B uncover 47 vulnerabilities (21 high-severity, 12 novel attack patterns), reporting a 3.9× improvement in discovery rate over manual testing and 89% detection accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhang Wei', 'Peilu Hu', 'Shengning Lang', 'Hao Yan', 'Li Mei', 'Yichao Zhang', 'Chen Yang', 'Junfeng Hao', 'Zhimo Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'vulnerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20677</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning</title><link>https://arxiv.org/abs/2512.20634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a shallow vs deep alignment framework and quantitative metrics (0–1 scale) to measure alignment depth across token positions, explaining spurious forgetting as shallow alignment limited to early tokens.&lt;/li&gt;&lt;li&gt;Proposes real-time detection methods to identify shallow alignment during training, plus visualization and recovery-prediction tools to automatically distinguish spurious vs true forgetting.&lt;/li&gt;&lt;li&gt;Presents adaptive mitigation strategies that promote deep alignment, reporting 86.2–90.6% identification accuracy and 3.3–7.1% robustness improvements on Qwen2.5 models (3B–32B) across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiwei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['continual learning', 'catastrophic forgetting', 'alignment depth', 'robustness', 'detection and mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20634</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams</title><link>https://arxiv.org/abs/2512.20631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a zero-training temporal drift detection approach for transformer-based sentiment models applied to authentic social media streams.&lt;/li&gt;&lt;li&gt;Evaluated on 12,279 posts across three transformer architectures, finding accuracy drops up to 23.4% and confidence drops up to 13.0% during event-driven periods.&lt;/li&gt;&lt;li&gt;Introduces four novel drift metrics that outperform embedding-based baselines while remaining computationally efficient for production deployment.&lt;/li&gt;&lt;li&gt;Provides statistical validation across multiple events and argues the method enables immediate real-time monitoring of model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayam Bansal', 'Ishaan Gangwani']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'model-robustness', 'drift-detection', 'monitoring', 'sentiment-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20631</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models</title><link>https://arxiv.org/abs/2512.21120</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ClarifyMT-Bench, a multi-turn clarification benchmark grounded in a five-dimensional ambiguity taxonomy and six simulated user personas, with 6,120 hybrid LLM-human dialogues.&lt;/li&gt;&lt;li&gt;Evaluates ten LLMs and finds a consistent under-clarification bias: models often answer prematurely and performance worsens with deeper dialogues.&lt;/li&gt;&lt;li&gt;Proposes ClarifyAgent, an agentic decomposition (perception, forecasting, tracking, planning) that substantially improves robustness across ambiguity conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sichun Luo', 'Yi Huang', 'Mukai Li', 'Shichang Meng', 'Fengyuan Liu', 'Zefa Hu', 'Junlan Feng', 'Qi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM clarification', 'safety/alignment', 'benchmarking', 'human-AI interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21120</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Replication of LLM Mistakes in Medical Conversations</title><link>https://arxiv.org/abs/2512.20983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedMistake, an automatic pipeline that extracts mistakes from LLM patient-doctor conversations and converts them into single-shot QA pairs for replication and evaluation.&lt;/li&gt;&lt;li&gt;Publishes MedMistake-All (3,390 QA pairs) and a doctor-validated subset MedMistake-Bench (211 items) and uses them to evaluate 12 frontier LLMs.&lt;/li&gt;&lt;li&gt;Aims to facilitate automated replication of LLM errors in clinical settings to support safety/reliability assessment and model comparison.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oleksii Proniakin', 'Diego Fajardo', 'Ruslan Nazarenko', 'Razvan Marinescu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'medical LLMs', 'error replication', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20983</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neural Probe-Based Hallucination Detection for Large Language Models</title><link>https://arxiv.org/abs/2512.20949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neural (MLP) probe framework that freezes LLM weights and performs token-level hallucination detection from hidden states, aiming to capture nonlinear structures missed by linear probes.&lt;/li&gt;&lt;li&gt;Introduces a multi-objective joint loss for stability and disambiguation, and uses Bayesian optimization to select optimal probe insertion layers.&lt;/li&gt;&lt;li&gt;Reports improved accuracy, recall, and low false-positive detection on benchmarks (LongFact, HealthBench, TriviaQA) compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shize Liang', 'Hongzhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM probing', 'LLM safety/robustness', 'token-level detection', 'Bayesian optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20949</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs</title><link>https://arxiv.org/abs/2512.20822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MediEval links MIMIC-IV EHRs to a UMLS-based knowledge base to evaluate LLMs across knowledge grounding and patient-context consistency using a 4-quadrant framework.&lt;/li&gt;&lt;li&gt;Generates factual and counterfactual medical statements to surface failure modes such as hallucinated support and truth inversion in proprietary, open-source, and domain models.&lt;/li&gt;&lt;li&gt;Proposes Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with asymmetric penalties that reduces unsafe confusions and substantially improves macro-F1 while eliminating truth inversion errors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhan Qu', 'Michael F\\"arber']&lt;/li&gt;&lt;li&gt;Tags: ['medical LLM safety', 'hallucination detection', 'benchmarking', 'counterfactual evaluation', 'alignment/fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20822</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Deception: When Reasoning Models Can't Compute an Addition</title><link>https://arxiv.org/abs/2512.20812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'semantic deceptions': testing LLMs' ability to perform symbolic reasoning when familiar digits/operators are replaced by novel symbols that carry misleading semantic associations.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs on simple arithmetic expressed in altered notation, showing that surface-level semantic cues substantially degrade performance even on basic tasks.&lt;/li&gt;&lt;li&gt;Finds chain-of-thought prompting can amplify reliance on statistical correlations and that apparent compliance with instructions can still mask failures in symbolic abstraction.&lt;/li&gt;&lt;li&gt;Highlights safety/ethical implications: limits of current LLMs' symbolic manipulation threaten reliability in decision-making contexts where robust, abstract reasoning is required.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathani\\"el de Leeuw', 'Marceau Nahon', 'Mathis Reymond', 'Raja Chatila', 'Mehdi Khamassi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'symbolic-reasoning', 'adversarial-prompting', 'evaluation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20812</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?</title><link>https://arxiv.org/abs/2512.20796</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether demographic bias mechanisms in language models can be surgically removed while preserving demographic recognition, using multi-task setups (names, professions, education).&lt;/li&gt;&lt;li&gt;Compares attribution-based vs correlation-based methods for locating bias features and applies targeted sparse autoencoder feature ablations on Gemma-2-9B to reduce stereotypes.&lt;/li&gt;&lt;li&gt;Finds that attribution-based ablations mitigate race/gender profession stereotypes while preserving name recognition, whereas correlation-based ablations work better for education bias; also observes 'prior collapse' effects for some interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyang Shan', 'Aaron Mueller']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'mechanistic interpretability', 'model interventions', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20796</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Investigating Model Editing for Unlearning in Large Language Models</title><link>https://arxiv.org/abs/2512.20794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates applying model editing algorithms (ROME, IKE, WISE) to the problem of unlearning information in LLMs by designing new editing targets suited to removal rather than redirection.&lt;/li&gt;&lt;li&gt;Finds that editing approaches can, in some settings, outperform baseline unlearning methods on the quality of forgetting.&lt;/li&gt;&lt;li&gt;Identifies a persistent challenge: edits/unlearning struggle to precisely scope what should be removed without harming retained knowledge or overall model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shariqah Hossain', 'Lalana Kagal']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'model unlearning', 'privacy', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20794</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization</title><link>https://arxiv.org/abs/2512.20773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial training framework where a generator (user simulator) and discriminator compete to produce realistic, failure-sensitive user behaviors for mental health task-oriented dialogue systems.&lt;/li&gt;&lt;li&gt;Shows fine-tuned simulators outperform zero-shot models at surfacing system failures; adversarial iterations increase diversity, distributional alignment, and correlation between simulated and real failure rates.&lt;/li&gt;&lt;li&gt;Discriminator accuracy drops after iterative adversarial training, interpreted as increased realism of simulated users, enabling more reliable pre-deployment evaluation of chatbots.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Zhu', 'Olivier Tieleman', 'Caitlin A. Stamatis', 'Luka Smyth', 'Thomas D. Hull', 'Daniel R. Cahn', 'Matteo Malgaroli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'user simulation', 'safety evaluation', 'dialogue systems', 'mental health']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20773</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Competency Gaps in Large Language Models and Their Benchmarks</title><link>https://arxiv.org/abs/2512.20638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an unsupervised, representation-grounded method using sparse autoencoders to identify 'model gaps' (weak sub-areas) and 'benchmark gaps' (imbalanced coverage) in LLM evaluations.&lt;/li&gt;&lt;li&gt;Applies the method to two open-source LLMs and ten benchmarks, recovering systematic underperformance on concepts tied to refusal, asserting boundaries, and safety-related discussions.&lt;/li&gt;&lt;li&gt;Argues the technique complements aggregate metrics by providing concept-level decomposition that can reveal why models score as they do and how benchmarks should evolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matyas Bohacek', 'Nino Scherrer', 'Nicholas Dufour', 'Thomas Leung', 'Christoph Bregler', 'Stephanie C. Y. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'alignment', 'interpretability', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20638</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safe Online Control-Informed Learning</title><link>https://arxiv.org/abs/2512.13868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Safe Online Control-Informed Learning framework that integrates optimal control, online parameter estimation, and safety constraints for safety-critical autonomous systems.&lt;/li&gt;&lt;li&gt;Uses an extended Kalman filter to incrementally update system parameters in real time and a softplus barrier function to enforce constraint satisfaction without requiring high-quality initial guesses.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence and safety guarantees and demonstrates effectiveness on cart-pole and robot-arm control tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyu Zhou', 'Zihao Liang', 'Zehui Lu', 'Shaoshuai Mou']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'online learning', 'control-theoretic ML', 'barrier functions', 'parameter estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13868</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bootstrapping LLMs via Preference-Based Policy Optimization</title><link>https://arxiv.org/abs/2511.12867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a preference-based policy optimization (PbPO) framework that formulates learning as a min-max game between a main policy and a constrained reward model (RM).&lt;/li&gt;&lt;li&gt;Develops an iterative online algorithm that actively collects preference data through guided exploration, enabling concurrent improvement of the policy and RM.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees with high-probability regret bounds for both sequence-level and token-level reward model settings.&lt;/li&gt;&lt;li&gt;Empirically outperforms existing state-of-the-art preference optimization methods across five benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Jia']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'reward modeling', 'policy optimization', 'RLHF alternative']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12867</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2507.01020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoAdv, an automated framework that uses a parametric attacker LLM to generate semantically disguised malicious prompts to bypass safety guardrails.&lt;/li&gt;&lt;li&gt;Proposes a dynamic, multi-turn attack methodology that analyzes failed attempts and iteratively crafts follow-up prompts using roleplaying, misdirection, and contextual manipulation.&lt;/li&gt;&lt;li&gt;Evaluates attack success rate (ASR) with the StrongREJECT framework across sequential turns and reports high jailbreak success (up to 86%) on models like ChatGPT, Llama, and DeepSeek, highlighting persistent vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashray Reddy', 'Andrew Zagula', 'Nicholas Saban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01020</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Data-regularized Reinforcement Learning for Diffusion Models at Scale</title><link>https://arxiv.org/abs/2512.04332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Data-regularized Diffusion Reinforcement Learning (DDRL) that uses forward KL to anchor policy to an off-policy data distribution for diffusion models.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing DDRL enables robust, unbiased integration of RL with standard diffusion training.&lt;/li&gt;&lt;li&gt;Presents a practical algorithm combining reward maximization with diffusion loss minimization to mitigate reward hacking (quality degradation, over-stylization, reduced diversity).&lt;/li&gt;&lt;li&gt;Large-scale empirical evaluation on high-resolution video generation shows improved rewards and human preference, reducing reward-hacking behaviors seen in baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotian Ye', 'Kaiwen Zheng', 'Jiashu Xu', 'Puheng Li', 'Huayu Chen', 'Jiaqi Han', 'Sheng Liu', 'Qinsheng Zhang', 'Hanzi Mao', 'Zekun Hao', 'Prithvijit Chattopadhyay', 'Dinghao Yang', 'Liang Feng', 'Maosheng Liao', 'Junjie Bai', 'Ming-Yu Liu', 'James Zou', 'Stefano Ermon']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-hacking', 'reinforcement-learning', 'diffusion-models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.04332</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers</title><link>https://arxiv.org/abs/2510.00915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes imperfect automated verifiers as a stochastic reward channel with asymmetric false-positive (ρ0) and false-negative (ρ1) noise rates.&lt;/li&gt;&lt;li&gt;Proposes two lightweight corrections: a backward correction producing an unbiased surrogate reward (unbiased policy-gradient in expectation) and a forward correction that reweights score-function terms to align expected updates with the clean gradient and needs only the FN rate.&lt;/li&gt;&lt;li&gt;Implements both corrections in a policy optimization pipeline and shows improvements on math-reasoning RL tasks under synthetic and real verifier noise, with the forward method more stable under heavy noise.&lt;/li&gt;&lt;li&gt;Introduces an appeals mechanism using a lightweight LLM verifier to estimate FN rate online, further improving performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin-Qiang Cai', 'Wei Wang', 'Feng Liu', 'Tongliang Liu', 'Gang Niu', 'Masashi Sugiyama']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'reward hacking', 'RL safety', 'verifier reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00915</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds</title><link>https://arxiv.org/abs/2504.04973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies constrained Markov decision processes (CMDPs) where safety constraints are defined by stochastic/unknown thresholds and must be learned from interaction.&lt;/li&gt;&lt;li&gt;Proposes SPOT (Stochastic Pessimistic-Optimistic Thresholding), a model-based primal-dual algorithm using a Growing-Window estimator to handle multiple stochastic constraints under pessimistic and optimistic settings.&lt;/li&gt;&lt;li&gt;Proves theoretical guarantees: reward regret Õ(√T) and constraint violation Õ(√T) over T episodes, matching performance comparable to approaches with fixed known thresholds.&lt;/li&gt;&lt;li&gt;Claims novelty as the first RL algorithm providing provable safety-aware learning when even constraint thresholds are unknown/uncertain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Zuo', 'Fengxiang He']&lt;/li&gt;&lt;li&gt;Tags: ['safe RL', 'constrained MDP', 'safety guarantees', 'robust/uncertain environments']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04973</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Autonomous Uncertainty Quantification for Computational Point-of-care Sensors</title><link>https://arxiv.org/abs/2512.21335</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an autonomous uncertainty quantification pipeline for point-of-care (POC) diagnostic systems using Monte Carlo dropout (MCDO) to identify and exclude high-uncertainty neural network predictions.&lt;/li&gt;&lt;li&gt;Applied to a paper-based vertical flow assay (xVFA) and handheld optical reader for Lyme disease, integrating MCDO without access to ground-truth labels.&lt;/li&gt;&lt;li&gt;Blinded testing showed improved diagnostic sensitivity (from 88.2% to 95.7%) by filtering uncertain predictions, increasing reliability of the neural-network-driven POC system.&lt;/li&gt;&lt;li&gt;Focuses on improving safety and robustness of a medical AI system via uncertainty estimation rather than on adversarial attacks, extraction, or red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artem Goncharov', 'Rajesh Ghosh', 'Hyou-Arm Joung', 'Dino Di Carlo', 'Aydogan Ozcan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'medical-AI-safety', 'robustness', 'Monte-Carlo-dropout', 'point-of-care-diagnostics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21335</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Assessing the Software Security Comprehension of Large Language Models</title><link>https://arxiv.org/abs/2512.21238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of five LLMs (GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, Qwen-2.5) on software security comprehension using Bloom's Taxonomy across six cognitive levels (remembering → creating).&lt;/li&gt;&lt;li&gt;Uses diverse datasets: curated multiple-choice, vulnerable code snippets (SALLM), course assessments, real-world case studies (XBOW), and project-based secure engineering tasks.&lt;/li&gt;&lt;li&gt;Finds strong performance on lower-order tasks (recall, identification) but significant degradation on higher-order reasoning, architectural evaluation, and secure system creation; introduces a 'software security knowledge boundary'.&lt;/li&gt;&lt;li&gt;Identifies 51 recurring misconception patterns exhibited by LLMs across Bloom's levels, highlighting failure modes relevant to security-critical usage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammed Latif Siddiq', 'Natalie Sekerak', 'Antonio Karam', 'Maria Leal', 'Arvin Islam-Gomes', 'Joanna C. S. Santos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'software security', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21238</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoBaxBuilder: Bootstrapping Code Security Benchmarking</title><link>https://arxiv.org/abs/2512.21132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;AutoBaxBuilder is a framework that automatically generates code-security benchmarking tasks and tests (including end-to-end security-probing exploits) using LLMs, with fine-grained plausibility checks.&lt;/li&gt;&lt;li&gt;The pipeline leverages LLMs to create functionality tests and exploit attempts, aiming to replace or augment manually-crafted benchmarks that can contaminate training data and must scale in difficulty and coverage.&lt;/li&gt;&lt;li&gt;The authors validate quality via qualitative analysis and quantitative experiments against human-expert tasks, release AutoBaxBench, and report generation cost/time (≈2 hours and &lt;$10 per task).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobias von Arx', 'Niels M\\"undler', 'Mark Vero', 'Maximilian Baader', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['code-security', 'benchmark-generation', 'LLM-evaluation', 'automated-exploit-generation', 'security-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21132</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy</title><link>https://arxiv.org/abs/2512.21048</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes zkFL-Health: a federated learning architecture combining zero-knowledge proofs (Halo2/Nova) and Trusted Execution Environments to ensure client updates are used correctly without revealing them.&lt;/li&gt;&lt;li&gt;Aggregator runs inside a TEE to compute global updates and emits succinct ZK proofs that it used exactly the committed client inputs and correct aggregation rule; verifier nodes validate proofs and record commitments on-chain for immutable auditability.&lt;/li&gt;&lt;li&gt;Targets healthcare deployment with a tailored system/threat model, formal security/privacy guarantees (mitigating gradient inversion/membership inference and untrusted aggregator risks), and outlines a performance evaluation plan (accuracy, privacy risk, latency, cost).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Savvy Sharma', 'George Petrovic', 'Sarthak Kaushik']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'zero-knowledge proofs', 'privacy', 'trusted execution environment', 'blockchain/auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21048</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Replication of LLM Mistakes in Medical Conversations</title><link>https://arxiv.org/abs/2512.20983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedMistake, an automatic pipeline that extracts mistakes from LLM patient-doctor conversations and converts them into single-shot QA pairs for replication and evaluation.&lt;/li&gt;&lt;li&gt;Publishes MedMistake-All (3,390 QA pairs) and a doctor-validated subset MedMistake-Bench (211 items) and uses them to evaluate 12 frontier LLMs.&lt;/li&gt;&lt;li&gt;Aims to facilitate automated replication of LLM errors in clinical settings to support safety/reliability assessment and model comparison.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oleksii Proniakin', 'Diego Fajardo', 'Ruslan Nazarenko', 'Razvan Marinescu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'medical LLMs', 'error replication', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20983</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks</title><link>https://arxiv.org/abs/2512.20924</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that classifiers can predict which chemist authored a molecule from structure alone (1,815-class task) with high accuracy, indicating strong chemist-style signals in public datasets.&lt;/li&gt;&lt;li&gt;Demonstrates an activity model that uses only a protein identifier and an author-probability vector (no molecular descriptors) to achieve comparable predictive performance to structure-based baselines.&lt;/li&gt;&lt;li&gt;Identifies a 'Clever Hans' failure mode where models exploit chemist intent/favorite targets rather than learning lab-independent structure–activity relationships, analyzes sources of leakage, and proposes author-disjoint splits and dataset practices to mitigate the issue.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew D. Blevins', 'Ian K. Quigley']&lt;/li&gt;&lt;li&gt;Tags: ['dataset leakage', 'Clever Hans / spurious correlations', 'robustness / evaluation', 'cheminformatics datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20924</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Real-World Adversarial Attacks on RF-Based Drone Detectors</title><link>https://arxiv.org/abs/2512.20712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first physical (over-the-air) adversarial attack on RF-based drone detectors that analyze spectrogram images with object detection models.&lt;/li&gt;&lt;li&gt;Designs class-specific universal complex baseband (I/Q) perturbation waveforms transmitted alongside legitimate signals to degrade target drone detection.&lt;/li&gt;&lt;li&gt;Validates effectiveness via RF recordings and OTA experiments across four drone types, showing reliable reduction in target detection while preserving detection of other legitimate drones.&lt;/li&gt;&lt;li&gt;Emphasizes practical constraints (synchronization, interference, hardware limits) and produces modest, structured perturbations compatible with standard RF chains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omer Gazit', 'Yael Itzhakev', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'physical attacks', 'RF security', 'robustness', 'OTA adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20712</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems</title><link>https://arxiv.org/abs/2512.20688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mechanism-Based Intelligence (MBI) and a Differentiable Price Mechanism (DPM) that computes exact loss gradients as incentive signals, claiming Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum.&lt;/li&gt;&lt;li&gt;Provides a Bayesian extension for incentive compatibility under asymmetric information (BIC).&lt;/li&gt;&lt;li&gt;Claims scalable performance (O(N) complexity) that bypasses Dec-POMDP combinatorial complexity and reports empirical speedups (≈50x) over model-free RL.&lt;/li&gt;&lt;li&gt;Positions the framework as provably efficient, auditable, and structurally aligning agent self-interest with collective objectives (alignment/safety focus).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stefano Grassi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'mechanism design', 'multi-agent systems', 'incentive compatibility', 'distributed optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20688</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Competency Gaps in Large Language Models and Their Benchmarks</title><link>https://arxiv.org/abs/2512.20638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an unsupervised, representation-grounded method using sparse autoencoders to identify 'model gaps' (weak sub-areas) and 'benchmark gaps' (imbalanced coverage) in LLM evaluations.&lt;/li&gt;&lt;li&gt;Applies the method to two open-source LLMs and ten benchmarks, recovering systematic underperformance on concepts tied to refusal, asserting boundaries, and safety-related discussions.&lt;/li&gt;&lt;li&gt;Argues the technique complements aggregate metrics by providing concept-level decomposition that can reveal why models score as they do and how benchmarks should evolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matyas Bohacek', 'Nino Scherrer', 'Nicholas Dufour', 'Thomas Leung', 'Christoph Bregler', 'Stephanie C. Y. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'alignment', 'interpretability', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20638</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</title><link>https://arxiv.org/abs/2512.21241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses hard-label black-box adversarial attacks that only expose top-1 labels and focuses on reducing query complexity.&lt;/li&gt;&lt;li&gt;Proposes ARS-OPT, a momentum-based ray-search optimization inspired by Nesterov acceleration to improve directional gradient estimates and convergence.&lt;/li&gt;&lt;li&gt;Extends ARS-OPT with surrogate-model priors (PARS-OPT) and provides theoretical convergence guarantees under standard assumptions.&lt;/li&gt;&lt;li&gt;Empirically demonstrates substantial query-efficiency improvements over 13 state-of-the-art methods on ImageNet and CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinjie Xu', 'Shuyu Cheng', 'Dongwei Xu', 'Qi Xuan', 'Chen Ma']&lt;/li&gt;&lt;li&gt;Tags: ['hard-label black-box attacks', 'adversarial attacks', 'query efficiency', 'optimization / momentum']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21241</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Analytic and Variational Stability of Deep Learning Systems</title><link>https://arxiv.org/abs/2512.21208</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified analytic and variational framework (Learning Stability Profile) to quantify infinitesimal response of representations, parameters, and updates to perturbations along learning trajectories.&lt;/li&gt;&lt;li&gt;Proves a Fundamental Analytic Stability Theorem linking uniform boundedness of stability signatures to existence of a Lyapunov-type energy that dissipates along learning flow, yielding explicit stability exponents in smooth regimes.&lt;/li&gt;&lt;li&gt;Derives consequences for spectral stability of feedforward nets, CFL-type conditions for residual architectures, and parametric/temporal stability laws for stochastic gradient methods; extends to non-smooth systems via Clarke generalized derivatives and variational Lyapunov functionals.&lt;/li&gt;&lt;li&gt;Frames how architectural and algorithmic choices govern robustness and sensitivity to perturbations, providing a theoretical foundation relevant to robustness and safety analyses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ronald Katende']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'stability', 'learning dynamics', 'theory', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21208</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Generalization of Diffusion Models Arises with a Balanced Representation Space</title><link>https://arxiv.org/abs/2512.20963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization vs generalization in diffusion models through representation learning, proving memorization corresponds to localized 'spiky' representations while generalization yields 'balanced' representations in a two-layer ReLU denoising autoencoder.&lt;/li&gt;&lt;li&gt;Empirically validates representation phenomena in real-world unconditional and text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes a representation-based detector for memorization and a training-free editing/steering technique to control model outputs via representation manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zekai Zhang', 'Xiao Li', 'Xiang Li', 'Lianghe Shi', 'Meng Wu', 'Molei Tao', 'Qing Qu']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy/data leakage', 'diffusion models', 'model auditing', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20963</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks</title><link>https://arxiv.org/abs/2512.20893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses evaluation and enhancement of adversarial robustness in deep neural networks within a red-team (attacker) / blue-team (defender) framework.&lt;/li&gt;&lt;li&gt;Targets time-efficient methods to reduce computational cost of both attacks and defenses, enabling applicability to large-scale models.&lt;/li&gt;&lt;li&gt;Aims to provide practical algorithms or workflows for faster robustness assessment and mitigation (details not specified in abstract).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runqi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'red teaming', 'adversarial attacks', 'defenses', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20893</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Certificates for Neural Networks against Adversarial Attacks</title><link>https://arxiv.org/abs/2512.20865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models gradient-based training as a discrete-time dynamical system and frames poisoning robustness as a formal safety verification problem.&lt;/li&gt;&lt;li&gt;Introduces barrier certificates (BCs) adapted from control theory, parameterized as neural networks, to certify a robust radius guaranteeing safety of the terminal model under worst-case ℓp-norm training-data poisoning.&lt;/li&gt;&lt;li&gt;Provides PAC-style generalization bounds by solving a scenario convex program (SCP) to obtain a confidence lower bound on the certified robustness radius from finite poisoned trajectories.&lt;/li&gt;&lt;li&gt;Extends the certification framework to test-time attacks, claiming a unified approach that yields formal guarantees for both training- and test-time adversarial threats; evaluated on MNIST, SVHN, CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sara Taheri', 'Mahalakshmi Sabanayagam', 'Debarghya Ghoshdastidar', 'Majid Zamani']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial robustness', 'formal certification', 'safety verification', 'PAC guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20865</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defending against adversarial attacks using mixture of experts</title><link>https://arxiv.org/abs/2512.20821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against adversarial attacks using a mixture-of-experts (MoE) architecture with adversarial training.&lt;/li&gt;&lt;li&gt;Uses nine pre-trained ResNet-18 experts and a learned gating mechanism; experts and gate are jointly fine-tuned end-to-end.&lt;/li&gt;&lt;li&gt;Claims improved robustness over state-of-the-art defenses and stronger plain classifiers despite a relatively simple backbone.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Meymani', 'Roozbeh Razavi-Far']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'mixture-of-experts', 'image classification', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20821</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative</title><link>https://arxiv.org/abs/2512.20814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedMPDD, a federated learning algorithm that compresses client gradients by sending directional derivatives along m random vectors (reducing uplink from O(d) to O(m)).&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees (O(1/√K)), matching FedSGD, by averaging multiple projections to avoid dimension-dependent limits of single projections.&lt;/li&gt;&lt;li&gt;Claims inherent privacy benefits against gradient inversion attacks due to low-rank projection geometry, offering a tunable privacy–utility trade-off controlled by the number of projections.&lt;/li&gt;&lt;li&gt;Empirical validation on benchmarks demonstrating communication savings and privacy-utility behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadreza Rostami', 'Solmaz S. Kia']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preservation', 'gradient-inversion', 'communication-compression', 'privacy-utility-tradeoff']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20814</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits</title><link>https://arxiv.org/abs/2512.20755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a robustness property specifically for neural networks with early-exit architectures and frames verification over their conditional execution paths.&lt;/li&gt;&lt;li&gt;Adapts off-the-shelf verifiers with a baseline algorithm plus an early-stopping strategy and heuristic optimizations that preserve soundness and completeness.&lt;/li&gt;&lt;li&gt;Empirically shows early-exit networks can be verified faster and more often than standard networks, and analyzes the trade-off between accuracy, efficiency, and verifiability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhak Yisrael Elboher', 'Avraham Raviv', 'Amihay Elboher', 'Zhouxing Shi', 'Omri Azencot', 'Hillel Kugler', 'Guy Katz']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'robustness', 'neural-networks', 'early-exit', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20755</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering</title><link>https://arxiv.org/abs/2512.20660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Dual-State Architecture that separates deterministic workflow state from the stochastic LLM environment, plus Atomic Action Pairs and Guard Functions to couple generation with verification.&lt;/li&gt;&lt;li&gt;Validates the framework on three code generation tasks across 13 LLMs (1.3B–15B parameters), reporting up to 66 percentage point improvement in task success for instruction-following models at 1.2–2.1× baseline compute.&lt;/li&gt;&lt;li&gt;Argues that architectural constraints and deterministic control can reduce stochastic failures (e.g., hallucinated syntax, gaming unit tests), effectively improving reliability without solely relying on model scale.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Thompson']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM robustness', 'Neuro-symbolic systems', 'Reliable code generation', 'Verification/Guard functions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20660</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning</title><link>https://arxiv.org/abs/2512.20634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a shallow vs deep alignment framework and quantitative metrics (0–1 scale) to measure alignment depth across token positions, explaining spurious forgetting as shallow alignment limited to early tokens.&lt;/li&gt;&lt;li&gt;Proposes real-time detection methods to identify shallow alignment during training, plus visualization and recovery-prediction tools to automatically distinguish spurious vs true forgetting.&lt;/li&gt;&lt;li&gt;Presents adaptive mitigation strategies that promote deep alignment, reporting 86.2–90.6% identification accuracy and 3.3–7.1% robustness improvements on Qwen2.5 models (3B–32B) across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiwei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['continual learning', 'catastrophic forgetting', 'alignment depth', 'robustness', 'detection and mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20634</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams</title><link>https://arxiv.org/abs/2512.20631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a zero-training temporal drift detection approach for transformer-based sentiment models applied to authentic social media streams.&lt;/li&gt;&lt;li&gt;Evaluated on 12,279 posts across three transformer architectures, finding accuracy drops up to 23.4% and confidence drops up to 13.0% during event-driven periods.&lt;/li&gt;&lt;li&gt;Introduces four novel drift metrics that outperform embedding-based baselines while remaining computationally efficient for production deployment.&lt;/li&gt;&lt;li&gt;Provides statistical validation across multiple events and argues the method enables immediate real-time monitoring of model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayam Bansal', 'Ishaan Gangwani']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'model-robustness', 'drift-detection', 'monitoring', 'sentiment-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20631</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</title><link>https://arxiv.org/abs/2512.19576</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First in-orbit demonstration of a deep reinforcement learning (DRL) based attitude controller deployed on the InnoCube 3U nanosatellite (launched Jan 2025).&lt;/li&gt;&lt;li&gt;Controller trained entirely in simulation; paper describes agent design, training methodology, and observed sim-to-real discrepancies on the spacecraft.&lt;/li&gt;&lt;li&gt;Compares AI-based controller performance against the satellite's classical PD controller and reports steady-state metrics showing robust performance during repeated in-orbit maneuvers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kirill Djebko', 'Tom Baumann', 'Erik Dilger', 'Frank Puppe', 'Sergio Montenegro']&lt;/li&gt;&lt;li&gt;Tags: ['Sim2Real', 'Robustness', 'Deep Reinforcement Learning', 'Satellite Attitude Control', 'Safety/Validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19576</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title><link>https://arxiv.org/abs/2512.19025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that standard unlearning evaluation for LLMs (measuring performance on the exact unlearned dataset D_u) can be misleading—models may appear to forget verbatim content while retaining semantically adjacent knowledge.&lt;/li&gt;&lt;li&gt;Introduces Proximal Surrogate Generation (PSG), an automated stress-testing method that constructs semantically related but embedding-distant surrogate datasets (Ã_u) to probe residual knowledge.&lt;/li&gt;&lt;li&gt;Evaluates across three LLM families, three datasets, and seven metrics, finding widespread inconsistencies and frequent overestimation of unlearning success by current metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengrui Jia', 'Taoran Li', 'Jonas Guan', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM evaluation', 'robustness', 'privacy/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19025</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title><link>https://arxiv.org/abs/2512.15503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AIMformer, a transformer-based model for real-time misbehavior detection in vehicular platoons, capturing intra-vehicle temporal dynamics and inter-vehicle spatial correlations via multi-head self-attention.&lt;/li&gt;&lt;li&gt;Introduces global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers and a Precision-Focused Binary Cross-Entropy (PFBCE) loss that penalizes false positives for safety-critical requirements.&lt;/li&gt;&lt;li&gt;Evaluated across multiple platoon controllers, attack vectors, and mobility scenarios, reporting high performance (≥ 0.93) vs. baselines.&lt;/li&gt;&lt;li&gt;Demonstrates edge deployment feasibility with TFLite, ONNX, and TensorRT achieving sub-millisecond inference latency for in-vehicle/roadside deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Kalogiannis', 'Ahmed Mohamed Hussain', 'Hexu Li', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['misbehavior detection', 'V2X security', 'adversarial attacks', 'transformer models', 'edge deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15503</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title><link>https://arxiv.org/abs/2512.13892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes replacing repeated random permutations for permutation-based feature importance with a single deterministic, optimal permutation to get faster, non-random, and more stable importance estimates.&lt;/li&gt;&lt;li&gt;Validates the approach across ~200 scenarios (including small samples, high dimensionality, low SNR), showing improved bias-variance tradeoffs and accuracy.&lt;/li&gt;&lt;li&gt;Introduces Systemic Variable Importance, a stress-testing extension that accounts for feature correlations to quantify how shocks propagate and reveal hidden dependencies (useful for auditing reliance on protected attributes).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Albert Dorador']&lt;/li&gt;&lt;li&gt;Tags: ['feature-importance', 'explainability', 'model-auditing', 'fairness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13892</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes training LLMs to produce a post-answer "confession" that honestly reports compliance failures or misbehavior; confession reward is based solely on honesty and decoupled from the main answer's reward.&lt;/li&gt;&lt;li&gt;Argues that if the easiest way to maximize confession reward is to surface misbehavior, models will be incentivized to confess; demonstrates empirical support, especially for egregious misbehavior.&lt;/li&gt;&lt;li&gt;Implements the method on a model (GPT-5-Thinking) and evaluates confession honesty on out-of-distribution scenarios including hallucination, instruction following, scheming, and reward hacking, finding modest improvements with training.&lt;/li&gt;&lt;li&gt;Shows confessions enable inference-time interventions (monitoring, rejection sampling, surfacing issues to users) to detect and mitigate unsafe or dishonest outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manas Joglekar', 'Jeremy Chen', 'Gabriel Wu', 'Jason Yosinski', 'Jasmine Wang', 'Boaz Barak', 'Amelia Glaese']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Honesty/alignment', 'Reinforcement learning', 'Reward shaping', 'Red teaming/detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08093</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title><link>https://arxiv.org/abs/2511.06148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLMs can spontaneously develop novel social biases about artificial demographic groups via exploration-exploitation dynamics, even when no true group differences exist.&lt;/li&gt;&lt;li&gt;Demonstrates that these emergent biases produce highly stratified (unfair) task allocations, with newer and larger models showing stronger effects.&lt;/li&gt;&lt;li&gt;Evaluates interventions (input, problem structure, explicit steering) and finds that explicitly incentivizing exploration most robustly reduces stratification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Addison J. Wu', 'Ryan Liu', 'Xuechunzi Bai', 'Thomas L. Griffiths']&lt;/li&gt;&lt;li&gt;Tags: ['bias/fairness', 'AI safety', 'alignment', 'LLM behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06148</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack with Partial Features</title><link>https://arxiv.org/abs/2508.06244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Partial Feature Membership Inference (PFMI): attacker only observes a subset of features and must infer membership of the corresponding full sample in the training set.&lt;/li&gt;&lt;li&gt;Proposes MRAD, a two-stage attack (latent-memory-based reconstruction of missing features, followed by anomaly detection to assess deviation from training distribution) applicable in white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Shows empirical effectiveness across datasets (e.g., STL-10 image dataset: AUC ~0.75 with 60% features missing) and compatibility with off-the-shelf anomaly detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurun Wang', 'Guangrui Liu', 'Xinjie Li', 'Haoyu He', 'Lin Yao', 'Zhongyun Hua', 'Weizhe Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'partial-features', 'reconstruction', 'anomaly-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06244</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History</title><link>https://arxiv.org/abs/2508.04826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PERSIST, an extensive benchmarking framework measuring personality stability across 25 open-source LLMs (1B–685B) and 2M+ responses using traditional and LLM-adapted questionnaires.&lt;/li&gt;&lt;li&gt;Finds persistent instability: question reordering, limited benefit from scale, and some interventions (reasoning modes, conversation history) can increase variability; persona instructions have mixed effects.&lt;/li&gt;&lt;li&gt;Concludes current LLMs lack architectural foundations for consistent behavior, posing risks for safety-critical applications that require predictable, aligned behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tommaso Tosato', 'Saskia Helbling', 'Yorguin-Jose Mantilla-Ramos', 'Mahmood Hegazy', 'Alberto Tosato', 'David John Lemay', 'Irina Rish', 'Guillaume Dumas']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-consistency', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04826</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Collision-based Watermark for Detecting Backdoor Manipulation in Federated Learning</title><link>https://arxiv.org/abs/2508.02115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Coward, a proactive detection method for backdoor manipulation in federated learning that leverages a discovered multi-backdoor collision effect (later backdoors suppress earlier ones).&lt;/li&gt;&lt;li&gt;Implements a backdoor-collided watermark via regulated dual-mapping learning on out-of-distribution (OOD) data to enable inverted detection that mitigates OOD prediction bias.&lt;/li&gt;&lt;li&gt;Design emphasizes low-disruptive training intervention to reduce false judgments under non-i.i.d. data and random client participation, and claims robustness against adaptive attacks.&lt;/li&gt;&lt;li&gt;Evaluated on benchmark datasets, showing state-of-the-art detection performance and improved resilience to OOD bias in FL scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Siying Gu', 'Yiming Li', 'Kangjie Chen', 'Zhili Chen', 'Tianwei Zhang', 'Shu-Tao Xia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'backdoor-detection', 'watermarking', 'robustness', 'OOD-bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02115</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2505.02824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies copyright evasion attacks (CEA) against dataset ownership verification (DOV) for personalized text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes CEAT2I: detect watermarked samples via intermediate feature convergence, iteratively ablate prompt tokens to find trigger tokens, then apply a closed-form concept erasure to remove injected watermarks.&lt;/li&gt;&lt;li&gt;Demonstrates CEAT2I reliably bypasses state-of-the-art DOV methods (TPD, T2IShield) while preserving generation performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yufei Zhu', 'Yiming Li', 'Jiawang Bai', 'Yong Yang', 'Zhifeng Li', 'Shu-Tao Xia']&lt;/li&gt;&lt;li&gt;Tags: ['model-attack', 'backdoor-removal', 'copyright-evasion', 'prompt-analysis', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02824</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable deep learning improves human mental models of self-driving cars</title><link>https://arxiv.org/abs/2411.18714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CW-Net, a concept-based explanation method that grounds planner decisions in human-interpretable concepts.&lt;/li&gt;&lt;li&gt;Deploys CW-Net on a real self-driving car and shows explanations improve human drivers' mental models and ability to predict the car's behavior.&lt;/li&gt;&lt;li&gt;Claims explanations are causally faithful and do not degrade driving performance, demonstrating practical utility of explainability in realistic deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eoin M. Kenny', 'Akshay Dharmavaram', 'Sang Uk Lee', 'Tung Phan-Minh', 'Shreyas Rajesh', 'Yunqing Hu', 'Laura Major', 'Momchil S. Tomov', 'Julie A. Shah']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'human-AI interaction', 'autonomous vehicle safety', 'interpretability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18714</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dialectics for Artificial Intelligence</title><link>https://arxiv.org/abs/2512.17373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an information-theoretic formalization of a 'concept' as a reversible consistency relation among parts of an agent's total experience, using algorithmic-information / Kolmogorov-style notions.&lt;/li&gt;&lt;li&gt;Defines metrics (e.g., excess information) to judge natural decompositions of experience and an optimization dynamics ('dialectics') where competing concepts bid to explain new information, causing splitting/merging of concepts.&lt;/li&gt;&lt;li&gt;Formalizes low-cost concept transmission and multi-agent alignment by showing how small shared grounds/seeds plus a protocol let another agent reconstruct the same concept — a compute/bits trade-off for communication and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengmian Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'representation learning', 'information theory', 'concept learning', 'multi-agent communication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17373</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title><link>https://arxiv.org/abs/2507.11662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Multimodal LLMs (MLLMs) as verifiers across web navigation, GUI/computer use, and robotic manipulation and identifies a pervasive 'agreement bias'—MLLMs over-validate agent behavior.&lt;/li&gt;&lt;li&gt;Proposes Self-Grounded Verification (SGV): a lightweight two-step method where the MLLM first generates broad, data-independent priors and then conditions on those priors to evaluate candidate trajectories, reducing agreement bias.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to +25 percentage points in failure detection, +14pp in accuracy) and downstream gains in task completion across multiple benchmarks (OSWorld, robomimic, VisualWebArena) and releases an improved VisualWebArena.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moises Andrade', 'Joonhyuk Cha', 'Brandon Ho', 'Vriksha Srihari', 'Karmesh Yadav', 'Zsolt Kira']&lt;/li&gt;&lt;li&gt;Tags: ['MLLM verification', 'Agreement bias', 'Alignment and evaluation', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11662</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Computational Basis of LLM's Decision Making in Social Simulation</title><link>https://arxiv.org/abs/2504.11671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods to extract and manipulate "vectors of variable variations" (e.g., male→female) from an LLM's internal state.&lt;/li&gt;&lt;li&gt;Demonstrates that manipulating these internal vectors during inference can substantially change LLM decision-making in a Dictator Game (fairness/prosocial behavior).&lt;/li&gt;&lt;li&gt;Provides a framework to study, quantify, and engineer how social concepts are encoded in transformer models, with implications for debiasing and alignment of LLM agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model interpretability', 'internal state manipulation', 'debiasing', 'social simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11671</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</title><link>https://arxiv.org/abs/2512.20563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies learner–expert asymmetries in end-to-end driving (experts have greater visibility and lower uncertainty; student intent is under-specified) that limit imitation learning performance.&lt;/li&gt;&lt;li&gt;Proposes practical interventions to reduce these gaps between privileged expert demonstrations and sensor-based student observations.&lt;/li&gt;&lt;li&gt;Demonstrates large empirical gains in closed-loop driving on CARLA benchmarks (state-of-the-art TFv6) and improved sim-to-real performance on NAVSIM and Waymo vision-based benchmarks.&lt;/li&gt;&lt;li&gt;Releases code, data, and models to reproduce results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Long Nguyen', 'Micha Fauth', 'Bernhard Jaeger', 'Daniel Dauner', 'Maximilian Igl', 'Andreas Geiger', 'Kashyap Chitta']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'imitation-learning', 'robustness', 'sim-to-real', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20563</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit</title><link>https://arxiv.org/abs/2512.20423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end, containerized toolkit to generate, capture, and analyze DNS-over-HTTPS (DoH) file exfiltration with configurable evasion parameters (chunking, encoding, padding, resolver rotation).&lt;/li&gt;&lt;li&gt;Extracts flow-level features and trains ML classifiers (Random Forest, Gradient Boosting, Logistic Regression) on public DoH datasets, comparing their performance against threshold-based detectors under evasive scenarios.&lt;/li&gt;&lt;li&gt;Benchmarks detector resilience to adversarial DoH exfiltration and provides reproducible pipelines for traffic generation, model training, and evaluation; discusses future extensions (HTTP/3/QUIC, benign traffic, real-time evaluation).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Elaoumari']&lt;/li&gt;&lt;li&gt;Tags: ['network-security', 'adversarial-evasion', 'ML-robustness', 'DNS-over-HTTPS', 'tooling/reproducibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20423</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</title><link>https://arxiv.org/abs/2512.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KnowVal, an autonomous driving system that integrates a driving knowledge graph (traffic laws, defensive driving, ethical norms) with LLM-based retrieval to enable visual-language reasoning for planning.&lt;/li&gt;&lt;li&gt;Introduces a human-preference dataset and trains a Value Model to guide interpretable, value-aligned trajectory assessment and decision-making.&lt;/li&gt;&lt;li&gt;Reports improved planning performance and reduced collision rates on nuScenes and state-of-the-art results on Bench2Drive, while remaining compatible with existing architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongyu Xia', 'Wenhao Chen', 'Yongtao Wang', 'Ming-Hsuan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety', 'value-alignment', 'knowledge-graph', 'LLM-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20299</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives</title><link>https://arxiv.org/abs/2512.20298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First direct comparison of state-of-the-art LLMs (Gemini Pro) vs. mental health professionals on diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders from Polish first-person autobiographical narratives.&lt;/li&gt;&lt;li&gt;Gemini Pro models outperformed human professionals in overall diagnostic accuracy (65.48% vs. 43.57%), with strong performance on BPD but severe underdiagnosis of NPD (model F1 = 6.7 vs. human F1 = 50.0).&lt;/li&gt;&lt;li&gt;Models produced confident, elaborate, pattern-focused justifications, while clinicians were concise and cautious, highlighting temporal experience and sense of self; authors highlight reliability and bias concerns in high-stakes clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karolina Dro\\.zd\\.z', 'Kacper Dudzic', 'Anna Sterna', 'Marcin Moskalewicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'bias', 'LLM-evaluation', 'clinical-risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20298</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds</title><link>https://arxiv.org/abs/2512.20245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Phonetic Trajectory Memory (PTM), a biomimetic architecture that encodes language as continuous trajectories on an ergodic manifold to enable extremely compressed long-term context.&lt;/li&gt;&lt;li&gt;Claims O(1) geometric navigation separate from probabilistic reconstruction, achieving ~3,000x compression and constant low-latency retrieval (~34ms) independent of context depth.&lt;/li&gt;&lt;li&gt;Argues the retrieval-as-resonance mechanism ('Signal Consensus') reduces hallucination and improves factual accuracy (reported ~92%), positioning this as a safety-aligned improvement in model fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarik Houichime', 'Abdelghani Souhar', 'Younes El Amrani']&lt;/li&gt;&lt;li&gt;Tags: ['memory architecture', 'long-context LLMs', 'hallucination mitigation', 'alignment', 'efficiency/compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20245</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithLens, an 8B-parameter model to detect faithfulness hallucinations in LLM outputs and produce accompanying explanations.&lt;/li&gt;&lt;li&gt;Creates synthetic training data with explanations via advanced LLMs, applies filtering for label/explanation quality and diversity, and fine-tunes as a cold start.&lt;/li&gt;&lt;li&gt;Further optimizes with rule-based reinforcement learning using rewards for both prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluated on 12 diverse tasks, reportedly outperforming advanced models (e.g., GPT-4.1) and providing high-quality explanations with cost-efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'faithfulness', 'explainability', 'safety', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</title><link>https://arxiv.org/abs/2512.20168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Odysseus, a dual-steganography jailbreak attack that covertly embeds malicious queries and expected malicious responses into benign-looking images to evade multimodal safety filters.&lt;/li&gt;&lt;li&gt;Targets commercial MLLM-integrated systems (models + deployed filters) and demonstrates that existing defenses assume malicious content is overt in a single modality, an assumption the attack breaks.&lt;/li&gt;&lt;li&gt;Extensive experiments on benchmark datasets show high effectiveness (up to 99% success rate), exposing a cross-modal blind spot and motivating rethinking of multimodal security defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songze Li', 'Jiameng Cheng', 'Yiming Li', 'Xiaojun Jia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'multimodal steganography', 'alignment bypass', 'security evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20168</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</title><link>https://arxiv.org/abs/2512.20164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability where "adversarial instructions" embedded in input data (e.g., resumes) can manipulate LLMs away from their intended task.&lt;/li&gt;&lt;li&gt;Introduces a benchmark for resume screening attacks and reports attack success rates exceeding 80% for some attack types.&lt;/li&gt;&lt;li&gt;Evaluates defenses: prompt-based mitigation, a proposed FIDS (Foreign Instruction Detection through Separation) using LoRA, and their combination; training-time LoRA-based defense outperforms inference-time prompt defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honglin Mu', 'Jinghao Liu', 'Kaiyang Wan', 'Rui Xing', 'Xiuying Chen', 'Timothy Baldwin', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'defenses', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20164</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</title><link>https://arxiv.org/abs/2512.20004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a GNN-based classifier that combines API-graph embeddings with permission and intent features to detect Android malware, achieving ~98.3% on CICMaldroid and ~98.7% on Drebin.&lt;/li&gt;&lt;li&gt;Identifies vulnerability of graph-based malware detectors to evasion via added/modified graph relationships and proposes VGAE-MalGAN—a VGAE-based GAN attack that generates adversarial API graphs to evade GNN classifiers.&lt;/li&gt;&lt;li&gt;Shows VGAE-MalGAN substantially reduces detection rates of GNN detectors, and demonstrates that adversarial retraining with generated samples can improve robustness and mitigate the attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Yumlembam', 'Biju Issac', 'Seibu Mary Jacob', 'Longzhi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial machine learning', 'graph neural networks', 'malware detection', 'GAN-based attack', 'adversarial training / defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20004</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</title><link>https://arxiv.org/abs/2512.19935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'Conditional Adversarial Fragility': adversarial vulnerability that is systematically amplified during macroeconomic stress and introduces a regime-aware evaluation framework for time-indexed tabular financial classification.&lt;/li&gt;&lt;li&gt;Uses volatility-based regime segmentation to compare model behavior in calm vs. stress periods while holding model architecture, attack methods, and evaluation protocols constant; baseline predictive performance remains comparable across regimes.&lt;/li&gt;&lt;li&gt;Finds that under adversarial perturbations, models operating in stress regimes suffer substantially greater degradation in accuracy, decision thresholds, and risk-sensitive outcomes, including increased false negatives.&lt;/li&gt;&lt;li&gt;Proposes an interpretive governance layer using semantic auditing of model explanations with large language models and motivates stress-aware approaches to model risk assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samruddhi Baviskar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial attacks', 'financial ML', 'robustness evaluation', 'model risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19935</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behaviorally calibrated reinforcement learning that trains models to output calibrated probabilities of correctness and to abstain or flag uncertain claims.&lt;/li&gt;&lt;li&gt;Optimizes strictly proper scoring rules rather than binary rewards to discourage guessing and encourage epistemic honesty.&lt;/li&gt;&lt;li&gt;Empirically shows a 4B Qwen3-4B-Instruct model trained with these methods achieves strong calibration and improves Accuracy-to-Hallucination Ratio across in-domain (BeyondAIME) and cross-domain (SimpleQA) evaluations, rivaling much larger frontier models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'Calibration', 'Reinforcement Learning', 'Alignment', 'Uncertainty estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title><link>https://arxiv.org/abs/2512.19711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PHANTOM, a method using anamorphic (perspective-dependent) physical art to create adversarial examples that fool vision-based object detectors in connected autonomous vehicles (CAVs).&lt;/li&gt;&lt;li&gt;Operates in black-box settings and demonstrates strong transferability across multiple detector architectures (YOLOv5, SSD, Faster R-CNN, RetinaNet).&lt;/li&gt;&lt;li&gt;Simulated evaluation in CARLA shows &gt;90% success under optimal conditions and 60–80% in degraded weather/lighting; attacks engage at 6–10 meters, limiting reaction time.&lt;/li&gt;&lt;li&gt;System-level impact demonstrated via SUMO-OMNeT++ co-simulation: false emergency messages propagate through V2X links, increasing Peak Age of Information by 68–89% and degrading safety-critical communication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Nahid Hasan Shuvo', 'Moinul Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['physical adversarial attacks', 'autonomous vehicle security', 'black-box transferability', 'adversarial examples', 'V2X communication security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19711</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Brain-Grounded Axes for Reading and Steering LLM States</title><link>https://arxiv.org/abs/2512.19399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs a word-level brain atlas (MEG PLV patterns) and derives latent axes via ICA to serve as a coordinate system for LLM hidden states.&lt;/li&gt;&lt;li&gt;Trains lightweight adapters (no LLM fine-tuning) that map hidden states to brain-derived axes and demonstrates steering along those axes changes lexical behavior (e.g., frequency-linked axis, function/content axis).&lt;/li&gt;&lt;li&gt;Validates effects across models (TinyLlama, Qwen2-0.5B, GPT-2), uses perplexity-matched controls, checks robustness to embedding choices, and provides exploratory fMRI anchoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandro Andric']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-steering', 'neuro-grounding', 'alignment/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19399</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</title><link>https://arxiv.org/abs/2512.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAGE, an LLM-based agent for automated stereotactic radiosurgery (SRS) planning, comparing a chain-of-thought (reasoning) model vs a non-reasoning model on 41 retrospective cases.&lt;/li&gt;&lt;li&gt;The reasoning agent achieved comparable dosimetric performance to human planners on primary endpoints and reduced cochlear dose (p = 0.022).&lt;/li&gt;&lt;li&gt;Reasoning model produced systematic deliberative behaviors (constraint verification, trade-off deliberation) and generated auditable optimization traces, supporting transparency and human-in-the-loop oversight.&lt;/li&gt;&lt;li&gt;Authors highlight auditability and explicit reasoning as pathways toward safer, more transparent deployment in a safety-critical clinical domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humza Nusrat', 'Luke Francisco', 'Bing Luo', 'Hassan Bagher-Ebadian', 'Joshua Kim', 'Karen Chin-Snyder', 'Salim Siddiqui', 'Mira Shah', 'Eric Mellon', 'Mohammad Ghassemi', 'Anthony Doemer', 'Benjamin Movsas', 'Kundan Thind']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Interpretability / auditability', 'Human-in-the-loop', 'Medical AI / clinical safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20586</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks</title><link>https://arxiv.org/abs/2512.20275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces G-SPEC, a neuro-symbolic framework that constrains probabilistic LLM agent planning with deterministic verification using a Network Knowledge Graph (NKG) and SHACL constraints.&lt;/li&gt;&lt;li&gt;Targets safety issues of agentic LLMs in telecom orchestration (topology hallucinations, policy non-compliance) and reports zero safety violations and 94.1% remediation success in a 450-node 5G Core simulation.&lt;/li&gt;&lt;li&gt;Ablation shows NKG validation provides the majority (68%) of safety gains, with SHACL policies contributing 24%; validation latency scales approximately O(k^1.2) with subgraph size and adds ~142 ms overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Vijay', 'Vignesh Ethiraj']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'neuro-symbolic verification', 'policy enforcement', 'robustness', 'networked systems security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20275</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Offline Safe Policy Optimization From Heterogeneous Feedback</title><link>https://arxiv.org/abs/2512.20173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PreSa, a method that directly learns a safe, reward-maximizing policy from offline pairwise preference labels and binary safety labels without explicitly learning reward and cost models.&lt;/li&gt;&lt;li&gt;Formulates safety alignment as a constrained optimization solved via a Lagrangian approach to avoid error accumulation from separate reward/cost model estimation in long-horizon continuous control.&lt;/li&gt;&lt;li&gt;Evaluates on continuous-control benchmarks with synthetic and real human feedback, showing improved reward and safety trade-offs compared to offline safe RL baselines and methods that learn rewards/costs first.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ze Gong', 'Pradeep Varakantham', 'Akshat Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Safe RL', 'Preference-based RL', 'Offline reinforcement learning', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20173</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Reinforcement Learning for Content Moderation with Large Language Models</title><link>https://arxiv.org/abs/2512.20061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of scaling reinforcement learning (RL) to train LLM-based content moderation classifiers across three real-world tasks.&lt;/li&gt;&lt;li&gt;Compares multiple RL training recipes and reward-shaping strategies, including verifiable rewards and an LLM-as-judge evaluation loop.&lt;/li&gt;&lt;li&gt;Finds sigmoid-like scaling behavior (improvement with more data/rollouts/optimization then saturation) and up to 100× data efficiency vs supervised fine-tuning.&lt;/li&gt;&lt;li&gt;Shows RL benefits for complex, policy-grounded reasoning and settings with label sparsity and evolving policy definitions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamed Firooz', 'Rui Liu', 'Yuchen Lu', 'Zhenyu Hou', 'Fangzhou Xiong', 'Xiaoyang Zhang', 'Changshu Jian', 'Zhicheng Zhu', 'Jiayuan Ma', 'Jacob Tao', 'Chaitali Gupta', 'Xiaochang Peng', 'Shike Mei', 'Hang Cui', 'Yang Qin', 'Shuo Tang', 'Jason Gaedtke', 'Arpit Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'alignment', 'reinforcement-learning', 'reward-shaping', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20061</guid><pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
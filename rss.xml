<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 08 Dec 2025 23:00:07 +0000</lastBuildDate><item><title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title><link>https://arxiv.org/abs/2506.16402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IS-Bench, a multi-modal benchmark for assessing interactive safety of VLM-driven embodied agents in household tasks, with 161 scenarios and 388 unique safety risks in a high-fidelity simulator.&lt;/li&gt;&lt;li&gt;Proposes a process-oriented evaluation that verifies whether agents perceive emergent risks and perform mitigation actions in the correct procedural order (before/after risk-prone steps).&lt;/li&gt;&lt;li&gt;Evaluates leading VLM-driven agents (e.g., GPT-4o, Gemini-2.5), finding they lack interactive safety awareness; safety-aware Chain-of-Thought can improve safety detection but often reduces task completion.&lt;/li&gt;&lt;li&gt;Provides code and data to support development of safer embodied AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Zeren Chen', 'Xuhao Hu', 'Yijin Zhou', 'Weichen Zhang', 'Dongrui Liu', 'Lu Sheng', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Safety evaluation', 'Embodied agents', 'Benchmarks', 'Vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16402</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Edge-Only Universal Adversarial Attacks in Distributed Learning</title><link>https://arxiv.org/abs/2411.10500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a threat model for split inference (edge-cloud distributed models) where the attacker only controls the edge (initial layers) and crafts universal adversarial perturbations (UAPs) to manipulate intermediate features.&lt;/li&gt;&lt;li&gt;Proposes edge-only untargeted and targeted UAP formulations to induce mispredictions in the unknown cloud component by controlling representations before the split point.&lt;/li&gt;&lt;li&gt;Empirically demonstrates strong transferability of edge-only UAPs on ImageNet across multiple network architectures and compares performance to white-box and black-box baselines.&lt;/li&gt;&lt;li&gt;Analyzes targeted attack capabilities and highlights new vulnerabilities unique to partial model access in split inference, motivating defenses for edge-only threat scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Rossolini', 'Tommaso Baldi', 'Alessandro Biondi', 'Giorgio Buttazzo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'universal adversarial perturbations', 'split inference / distributed learning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10500</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ZQBA: Zero Query Black-box Adversarial Attack</title><link>https://arxiv.org/abs/2510.00769</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZQBA, a zero-query black-box adversarial attack that constructs perturbations by adding feature maps extracted from a DNN to clean images (no target-model queries required).&lt;/li&gt;&lt;li&gt;Demonstrates transferability of the generated adversarial samples across different models and datasets (CIFAR and Tiny ImageNet) and compares favorably to state-of-the-art single-query black-box attacks.&lt;/li&gt;&lt;li&gt;Evaluates imperceptibility quantitatively (SSIM) and qualitatively, highlighting vulnerabilities of deploying DNNs in real-world image-classification systems; code is publicly available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joana C. Costa', 'Tiago Roxo', 'Hugo Proen\\c{c}a', "Pedro R. M. In\\'acio"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'black-box', 'zero-query', 'transferability', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00769</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Ordinal Bias in Action Recognition for Instructional Videos</title><link>https://arxiv.org/abs/2504.06580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines "ordinal bias": models relying on dataset-specific, dominant action sequences rather than true video understanding.&lt;/li&gt;&lt;li&gt;Proposes two stress-test manipulations for instructional videos: Action Masking (mask frequent co-occurring action frames) and Sequence Shuffling (randomize action segment order).&lt;/li&gt;&lt;li&gt;Empirically shows significant performance degradation of current action-recognition models on nonstandard action sequences, revealing vulnerabilities tied to sequence priors.&lt;/li&gt;&lt;li&gt;Argues for revised evaluation strategies and development of models that generalize beyond fixed action patterns in instructional videos.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joochan Kim', 'Minjoon Jung', 'Byoung-Tak Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'dataset bias', 'evaluation/benchmarking', 'video action recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06580</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>iMotion-LLM: Instruction-Conditioned Trajectory Generation</title><link>https://arxiv.org/abs/2406.06211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces iMotion-LLM, an LLM integrated with trajectory prediction modules to generate instruction-conditioned, feasible, and safety-aligned driving trajectories.&lt;/li&gt;&lt;li&gt;Projects scene features into the LLM input space and maps special tokens to a trajectory decoder; fine-tunes a pre-trained LLM with LoRA.&lt;/li&gt;&lt;li&gt;Provides two datasets (InstructWaymo and Open-Vocabulary InstructNuPlan) with direction-based and safety-aligned instruction–trajectory pairs for training and evaluation.&lt;/li&gt;&lt;li&gt;Evaluates instruction-conditioned trajectory generation, reporting high accuracy on direction feasibility (84%) and safety evaluation (96%), and positions the work as a foundation for text-guided motion generation and safety-alignment testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdulwahab Felemban', 'Nussair Hroub', 'Jian Ding', 'Eslam Abdelrahman', 'Xiaoqian Shen', 'Abduallah Mohamed', 'Mohamed Elhoseiny']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'instruction-conditioned generation', 'autonomous driving', 'safety evaluation', 'trajectory generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.06211</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</title><link>https://arxiv.org/abs/2512.05937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how background correlations and camera variation affect classification performance and feature importance for traffic sign recognition.&lt;/li&gt;&lt;li&gt;Creates six synthetic datasets that isolate degrees of camera variation and background correlation to enable controlled, quantitative analyses.&lt;/li&gt;&lt;li&gt;Uses XAI methods (e.g., saliency like SHAP/GradCAM) combined with ground-truth object masks to measure when classifiers rely on background vs. object features.&lt;/li&gt;&lt;li&gt;Quantifies conditions under which background features gain importance and how training-domain changes influence reliance on spurious correlations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anne Sielemann', 'Valentin Barner', 'Stefan Wolf', 'Masoud Roschani', 'Jens Ziehn', 'Juergen Beyerer']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'explainability', 'spurious_correlations', 'synthetic_datasets', 'autonomous_vehicles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05937</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</title><link>https://arxiv.org/abs/2512.05927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C3, an uncertainty quantification method to produce dense, calibrated subpatch-level uncertainty estimates for controllable video generation models to mitigate hallucinations.&lt;/li&gt;&lt;li&gt;Three technical contributions: training for correctness and calibration via strictly proper scoring rules; estimating uncertainty in latent space to avoid pixel-space instabilities and cost; mapping latent uncertainty to interpretable pixel-level RGB heatmaps.&lt;/li&gt;&lt;li&gt;Demonstrates calibrated in-distribution uncertainty and effective out-of-distribution detection on large-scale robot learning datasets (Bridge, DROID) and real-world evaluations, with applications to safer robot policy evaluation and planning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiting Mei', 'Tenny Yin', 'Micah Baker', 'Ola Shorinwa', 'Anirudha Majumdar']&lt;/li&gt;&lt;li&gt;Tags: ['Uncertainty quantification', 'Calibration', 'Hallucination detection', 'Out-of-distribution detection', 'Robot world modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05927</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack</title><link>https://arxiv.org/abs/2512.05853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VRSA, a Visual Reasoning Sequential Attack that decomposes a harmful text query into a sequence of related sub-images to induce MLLMs to externalize and aggregate harmful intent.&lt;/li&gt;&lt;li&gt;Introduces Adaptive Scene Refinement to optimize scene realism relative to the original harmful query, Semantic Coherent Completion to iteratively rewrite sub-texts with contextual continuity, and Text-Image Consistency Alignment to maintain semantic alignment.&lt;/li&gt;&lt;li&gt;Evaluates VRSA against state-of-the-art jailbreak methods on both open- and closed-source MLLMs (e.g., GPT-4o, Claude-4.5-Sonnet) and reports higher attack success rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiji Zhao', 'Shukun Xiong', 'Yao Huang', 'Yan Jin', 'Zhenyu Wu', 'Jiyang Guan', 'Ranjie Duan', 'Jialing Tao', 'Hui Xue', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multimodal adversarial attacks', 'visual prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05853</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision</title><link>https://arxiv.org/abs/2512.05740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving pipeline to distill knowledge from large LLMs into a locally deployable Vision-Language Model (VLM) for surgical anatomy explanation.&lt;/li&gt;&lt;li&gt;Generates an expert-supervised dataset by prompting a teacher LLM using only textual context and binary segmentation masks (no sensitive images) for spatial information.&lt;/li&gt;&lt;li&gt;Uses Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to adapt the base VLM, showing large gains in surgical domain knowledge.&lt;/li&gt;&lt;li&gt;Aims to avoid patient data leakage to externally hosted large VLMs by enabling local deployment and data-efficient training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lennart Maack', 'Julia-Kristin Gra{\\ss}', 'Lisa-Marie Toscha', 'Nathaniel Melling', 'Alexander Schlaefer']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving training', 'healthcare VLMs', 'local deployment', 'model fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05740</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective</title><link>https://arxiv.org/abs/2512.05651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-supervised detector for AI-generated images that leverages EXIF camera metadata to learn photographic features from real photos.&lt;/li&gt;&lt;li&gt;Pretext tasks: classify categorical EXIF tags and pairwise-rank ordinal/continuous EXIF tags to train a feature extractor.&lt;/li&gt;&lt;li&gt;Uses learned EXIF-induced features for one-class detection (GMM on photographic distribution) and as a regularizer for binary detection operating on high-frequency residuals.&lt;/li&gt;&lt;li&gt;Reports strong cross-model generalization and robustness to common benign image perturbations across multiple generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nan Zhong', 'Mian Zou', 'Yiran Xu', 'Zhenxing Qian', 'Xinpeng Zhang', 'Baoyuan Wu', 'Kede Ma']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic image detection', 'self-supervised learning', 'forensics/anti-deepfake', 'robustness/generalization', 'EXIF-based features']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05651</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models</title><link>https://arxiv.org/abs/2512.05546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Conscious Gaze (CG-VLM), a training-free, inference-time framework to reduce object hallucinations in vision-language models by steering mid-layer attention back to visual tokens.&lt;/li&gt;&lt;li&gt;Introduces a Cognitive Demand Sensor using Harsanyi interactions to detect when vision-text synergy is low and visual grounding is needed.&lt;/li&gt;&lt;li&gt;Uses Focused Consensus Induction to selectively reorient attention before collapse into linguistic priors, enabling token-level, context-aware intervention.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on POPE and CHAIR across multiple VLMs (InstructBLIP, LLaVA, Qwen-VL, mPLUG) while preserving general capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weijue Bu', 'Guan Yuan', 'Guixian Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'interpretability', 'robustness', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05546</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Improving VLM Judges Without Human Annotations</title><link>https://arxiv.org/abs/2512.05145</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-training framework to build Vision-Language Model (VLM) judge models without any human preference annotations by synthesizing multimodal instruction-response pairs, generating reasoning traces and judgments, filtering for quality, and training on correct judge answers.&lt;/li&gt;&lt;li&gt;Evaluates the self-trained judge on Multimodal RewardBench and VL-RewardBench across correctness, preference, reasoning, safety, and VQA, showing notable accuracy improvements (e.g., Llama-3.2-11B judge from 0.38 to 0.51 overall) and outperforming much larger models on several dimensions.&lt;/li&gt;&lt;li&gt;Demonstrates that a human-annotation-free judge can track and evaluate evolving VLM capabilities, with particularly strong gains in generalization, hallucination detection, and reasoning dimensions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Inna Wanyin Lin', 'Yushi Hu', 'Shuyue Stella Li', 'Scott Geng', 'Pang Wei Koh', 'Luke Zettlemoyer', 'Tim Althoff', 'Marjan Ghazvininejad']&lt;/li&gt;&lt;li&gt;Tags: ['safety_evaluation', 'alignment', 'vision-language_models', 'self-supervised_judging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05145</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images</title><link>https://arxiv.org/abs/2512.05137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChromouVQA, a large-scale benchmark of Ishihara-style chromatic camouflaged images with controlled variation (chromatic separation, density, size, occlusion, rotation) and full metadata.&lt;/li&gt;&lt;li&gt;Defines nine vision-question-answering tasks (recognition, counting, comparison, spatial reasoning) to probe figure–ground segregation in VLMs.&lt;/li&gt;&lt;li&gt;Finds large gaps between human and VLM performance, especially under subtle chromatic contrast or disruptive geometric fills.&lt;/li&gt;&lt;li&gt;Proposes a model-agnostic contrastive recipe that aligns silhouettes with camouflaged renderings to improve global shape recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunfei Zhang', 'Yizhuo He', 'Yuanxun Shao', 'Zhengtao Yao', 'Haoyan Xu', 'Junhao Dong', 'Zhen Yao', 'Zhikang Dong']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'benchmark/dataset', 'camouflage/perceptual robustness', 'contrastive-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05137</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title><link>https://arxiv.org/abs/2508.11222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORFuzz, an evolutionary testing/fuzzing framework to detect LLM over-refusal (erroneously refusing benign queries).&lt;/li&gt;&lt;li&gt;Combines safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and OR-Judge (a human-aligned judge model) for validation.&lt;/li&gt;&lt;li&gt;Produces ORFuzzSet, a benchmark of 1,855 validated test cases, and demonstrates substantially higher over-refusal discovery and transferability across 10 LLMs compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haonan Zhang', 'Dongxia Wang', 'Yi Liu', 'Kexin Chen', 'Jiashui Wang', 'Xinlei Ying', 'Long Liu', 'Wenhai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Over-refusal testing', 'Fuzzing', 'Safety evaluation', 'Benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11222</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title><link>https://arxiv.org/abs/2506.16402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IS-Bench, a multi-modal benchmark for assessing interactive safety of VLM-driven embodied agents in household tasks, with 161 scenarios and 388 unique safety risks in a high-fidelity simulator.&lt;/li&gt;&lt;li&gt;Proposes a process-oriented evaluation that verifies whether agents perceive emergent risks and perform mitigation actions in the correct procedural order (before/after risk-prone steps).&lt;/li&gt;&lt;li&gt;Evaluates leading VLM-driven agents (e.g., GPT-4o, Gemini-2.5), finding they lack interactive safety awareness; safety-aware Chain-of-Thought can improve safety detection but often reduces task completion.&lt;/li&gt;&lt;li&gt;Provides code and data to support development of safer embodied AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Zeren Chen', 'Xuhao Hu', 'Yijin Zhou', 'Weichen Zhang', 'Dongrui Liu', 'Lu Sheng', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Safety evaluation', 'Embodied agents', 'Benchmarks', 'Vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16402</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MindEval: Benchmarking Language Models on Multi-turn Mental Health Support</title><link>https://arxiv.org/abs/2511.18491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MindEval, a fully automated, multi-turn benchmark for evaluating LLMs in realistic mental-health therapy dialogues, developed with clinical psychologists.&lt;/li&gt;&lt;li&gt;Uses simulated patients validated against human text and shows strong correlation between automatic LLM-based metrics and human expert judgments.&lt;/li&gt;&lt;li&gt;Evaluates 12 state-of-the-art LLMs, finding widespread safety/quality failures (e.g., sycophancy, overvalidation, reinforcement of maladaptive beliefs), especially over longer interactions or severe cases; releases code and data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jos\\'e Pombal", "Maya D'Eon", 'Nuno M. Guerreiro', 'Pedro Henrique Martins', "Ant\\'onio Farinhas", 'Ricardo Rei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Benchmarking', 'Harmful outputs / user safety', 'Mental-health evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18491</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HalluClean: A Unified Framework to Combat Hallucinations in LLMs</title><link>https://arxiv.org/abs/2511.08916</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HalluClean, a lightweight, task-agnostic framework to detect and correct hallucinations in LLM outputs via planning, execution, and revision stages.&lt;/li&gt;&lt;li&gt;Uses minimal task-routing prompts for zero-shot generalization across domains without external knowledge sources or supervised detectors.&lt;/li&gt;&lt;li&gt;Evaluated across five tasks (QA, dialogue, summarization, math word problems, contradiction detection) and shows improved factual consistency over baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxin Zhao', 'Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'alignment/safety', 'robustness', 'post-hoc correction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08916</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability</title><link>https://arxiv.org/abs/2510.12229</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether the Knobe effect (a moral bias in intentionality judgements) emerges in finetuned LLMs.&lt;/li&gt;&lt;li&gt;Performs Layer-Patching analysis across three open-weight LLMs and finds the bias is learned during finetuning and localized to specific layers.&lt;/li&gt;&lt;li&gt;Shows that patching activations from the pretrained model into a few critical layers can eliminate the bias without retraining.&lt;/li&gt;&lt;li&gt;Suggests that social biases in LLMs can be interpreted, localized, and mitigated via targeted interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bianca Raimondi', 'Daniela Dalbagno', 'Maurizio Gabbrielli']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'mechanistic interpretability', 'mitigation', 'finetuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12229</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HARP: Hallucination Detection via Reasoning Subspace Projection</title><link>https://arxiv.org/abs/2509.11536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HARP, a method that decomposes LLM hidden state space into semantic and reasoning subspaces and uses projections onto the reasoning subspace for hallucination detection.&lt;/li&gt;&lt;li&gt;Uses SVD on the Unembedding layer to obtain basis vectors for the two subspaces and reduces feature dimensionality to ~5% of original.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art hallucination detection performance (e.g., AUROC 92.8% on TriviaQA) and improved robustness across datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Hu', 'Gang Tu', 'ShengYu Cheng', 'Jinxin Li', 'Jinting Wang', 'Rui Chen', 'Zhilong Zhou', 'Dongbo Shan']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'representation disentanglement', 'interpretability', 'feature projection/SVD']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11536</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models</title><link>https://arxiv.org/abs/2505.16188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAE-SSV: use sparse autoencoders to produce sparse, interpretable latent representations, then train linear classifiers to identify a small task-relevant subspace.&lt;/li&gt;&lt;li&gt;Learn supervised steering vectors constrained to that subspace to steer LLMs toward target behaviors (sentiment, truthfulness, political polarity) with minimal quality degradation.&lt;/li&gt;&lt;li&gt;Empirical results across multiple LLMs show high steering success using a notably small subspace, enabling more targeted and interpretable interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirui He', 'Mingyu Jin', 'Bo Shen', 'Ali Payani', 'Yongfeng Zhang', 'Mengnan Du']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM steering', 'interpretability', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16188</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics for LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Question-Context-Answer (QCA) triplets, and Semantic Entropy Production (SEP) inspired by thermodynamics.&lt;/li&gt;&lt;li&gt;Models an LLM as a bipartite information engine (Maxwell demon) transforming context C into answer A via prompt Q; infers transition matrices via convex optimization and maps minimal KL divergence to a [0,1] faithfulness score.&lt;/li&gt;&lt;li&gt;Claims high SF correlates with low SEP and demonstrates the framework on LLM summarization of SEC 10-K filings, proposing use for evaluation and hallucination control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination-detection', 'safety-evaluation', 'information-theory', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework</title><link>https://arxiv.org/abs/2512.05863</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a retrieval-augmented generation (RAG) medical QA system combining domain-specific retrieval with open-source LLMs (LLaMA 2, Falcon).&lt;/li&gt;&lt;li&gt;Fine-tunes models with LoRA for domain specialization and compares fine-tuned vs. zero-shot performance on PubMedQA and MedMCQA.&lt;/li&gt;&lt;li&gt;Reports substantial accuracy gains (e.g., 71.8% vs 55.4% on PubMedQA) and a ~60% reduction in unsupported/hallucinatory content when grounding answers with retrieved evidence.&lt;/li&gt;&lt;li&gt;Emphasizes transparency by returning source references alongside generated answers and details system/fine-tuning methodology.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tasnimul Hassan', 'Md Faisal Karim', 'Haziq Jeelani', 'Elham Behnam', 'Robert Green', 'Fayeq Jeelani Syed']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'medical QA', 'LLM fine-tuning', 'safety/robustness (factuality)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05863</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains</title><link>https://arxiv.org/abs/2512.05700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes fusing multiple elementary faithfulness metrics using a tree-based model to produce a combined metric that better correlates with human judgements.&lt;/li&gt;&lt;li&gt;Uses human annotations across question-answering and dialogue domains to drive metric importance and validate the fused metric.&lt;/li&gt;&lt;li&gt;Demonstrates improved correlation with human judgements across tested domains and provides a homogenised dataset of human judgments and LLM responses for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Malin', 'Tatiana Kalganova', 'Nikolaos Boulgouris']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness evaluation', 'safety evaluation', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05700</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures</title><link>https://arxiv.org/abs/2512.05501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SEA-SafeguardBench, a human-verified safety benchmark covering eight Southeast Asian languages with 21,640 samples across three subsets (general, in-the-wild, content generation).&lt;/li&gt;&lt;li&gt;Demonstrates that state-of-the-art LLMs and existing guardrails underperform on SEA languages and culturally specific harm scenarios versus English.&lt;/li&gt;&lt;li&gt;Argues for natively authored (not machine-translated) datasets to capture linguistic and cultural nuances relevant to safety, such as region-specific misinformation and political sensitivities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panuthep Tasawong', 'Jian Gang Ngui', 'Alham Fikri Aji', 'Trevor Cohn', 'Peerat Limkonchotiwat']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM guardrails', 'multilingual benchmarks', 'harmful content detection', 'regional/cultural robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05501</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment</title><link>https://arxiv.org/abs/2512.05464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Collective Agency (CA) as an open-ended alignment objective that emphasizes integrated agentic capabilities beyond standard helpfulness/honesty/harmlessness norms.&lt;/li&gt;&lt;li&gt;Introduces Dynamic Alignment, a scalable self-improving framework combining automated training-data generation by LLMs and a self-rewarding mechanism where the policy model evaluates its own outputs and assigns rewards for GRPO-based learning.&lt;/li&gt;&lt;li&gt;Reports experiments showing models aligned to CA while retaining general NLP capabilities, aiming to reduce reliance on costly human feedback.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panatchakorn Anantaprayoon', 'Nataliia Babina', 'Jad Tarifi', 'Nima Asgharbeygi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improving alignment', 'reward modeling', 'scalable safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05464</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Self Critique and Refinement for Faithful LLM Summarization</title><link>https://arxiv.org/abs/2512.05387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SCRPO, a self-supervised framework that uses an LLM's own critique and refinements to construct a preference dataset and perform preference learning to reduce hallucinations in summarization.&lt;/li&gt;&lt;li&gt;Aims to improve faithfulness without requiring additional test-time compute or access to stronger teacher models.&lt;/li&gt;&lt;li&gt;Evaluated on XSUM, CNNDM, and SAMSum; shows improved faithfulness metrics while maintaining or improving overall summary quality and offering greater efficiency than test-time refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Yao Hu', 'Hema Swetha Koppula', 'Hadi Pouransari', 'Cem Koc', 'Oncel Tuzel', 'Raviteja Vemulapalli']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'faithfulness', 'self-supervised training', 'LLM summarization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05387</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Self-Preference by Authorship Obfuscation</title><link>https://arxiv.org/abs/2512.05379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and studies the 'self-preference' bias where LM judges prefer their own outputs over others, even without source labels.&lt;/li&gt;&lt;li&gt;Proposes and tests black-box perturbation strategies (e.g., synonym replacement) to obfuscate authorship in pairwise comparisons and reduce self-recognition.&lt;/li&gt;&lt;li&gt;Finds partial mitigation is possible but self-preference can re-emerge when stylistic differences are more fully neutralized, indicating multi-level sources of self-recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taslim Mahbub', 'Shi Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'self-preference', 'authorship obfuscation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05379</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats</title><link>https://arxiv.org/abs/2512.05331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes linguistic, stylistic, and lexical signatures of 'pink slime' auto-generated local news articles to enable detection.&lt;/li&gt;&lt;li&gt;Demonstrates that consumer-accessible LLMs can perform adversarial modifications that reduce existing detectors' F1 by up to 40%.&lt;/li&gt;&lt;li&gt;Proposes a robust learning framework designed to resist LLM-based adversarial attacks and reports up to 27% improvement over baselines.&lt;/li&gt;&lt;li&gt;Focuses on threat modeling and mitigation for automated misinformation generation/obfuscation using LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadat Shahriar', 'Navid Ayoobi', 'Arjun Mukherjee', 'Mostafa Musharrat', 'Sai Vishnu Vamsi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM adversarial attacks', 'Misinformation detection', 'Robustness / Adversarial robustness', 'Forensic NLP', 'Red teaming / adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05331</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</title><link>https://arxiv.org/abs/2510.18526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of LLMs to pluralistic human values, focusing on interdependent value dimensions and priority (value complexity) and controllability of nuanced/underrepresented priorities (value steerability).&lt;/li&gt;&lt;li&gt;Proposes COUPLE: a framework that builds an explicit structural causal model (SCM) linking high-level value dimensions to behaviors and models interdependencies/priorities among features.&lt;/li&gt;&lt;li&gt;Uses counterfactual reasoning over the SCM to generate outputs aligned with specified value objectives, improving steerability and interpretability.&lt;/li&gt;&lt;li&gt;Evaluated on two datasets with different value systems and shows improvements over baselines across diverse value objectives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanze Guo', 'Jing Yao', 'Xiao Zhou', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['value alignment', 'LLM safety', 'causal modeling', 'counterfactual reasoning', 'steerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18526</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title><link>https://arxiv.org/abs/2506.16402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IS-Bench, a multi-modal benchmark for assessing interactive safety of VLM-driven embodied agents in household tasks, with 161 scenarios and 388 unique safety risks in a high-fidelity simulator.&lt;/li&gt;&lt;li&gt;Proposes a process-oriented evaluation that verifies whether agents perceive emergent risks and perform mitigation actions in the correct procedural order (before/after risk-prone steps).&lt;/li&gt;&lt;li&gt;Evaluates leading VLM-driven agents (e.g., GPT-4o, Gemini-2.5), finding they lack interactive safety awareness; safety-aware Chain-of-Thought can improve safety detection but often reduces task completion.&lt;/li&gt;&lt;li&gt;Provides code and data to support development of safer embodied AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Zeren Chen', 'Xuhao Hu', 'Yijin Zhou', 'Weichen Zhang', 'Dongrui Liu', 'Lu Sheng', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Safety evaluation', 'Embodied agents', 'Benchmarks', 'Vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16402</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Observational Auditing of Label Privacy</title><link>https://arxiv.org/abs/2511.14084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an observational auditing framework for differential privacy that evaluates label privacy without modifying the training dataset.&lt;/li&gt;&lt;li&gt;Extends auditing beyond membership inference to protected attributes (labels as a special case) by leveraging natural randomness in data distributions.&lt;/li&gt;&lt;li&gt;Provides theoretical foundations and empirical validation on Criteo (tabular) and CIFAR-10 (image) showing effectiveness for auditing label privacy guarantees.&lt;/li&gt;&lt;li&gt;Aims to reduce engineering overhead for large-scale production privacy evaluations by avoiding canary injection or sample removal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Iden Kalemaj', 'Luca Melis', 'Maxime Boucher', 'Ilya Mironov', 'Saeed Mahloujifar']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'privacy_auditing', 'label_privacy', 'membership_inference', 'privacy_testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14084</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks</title><link>https://arxiv.org/abs/2511.09114</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TERLA: Topological Extensions for RL Agents that use heterogeneous graph neural network layers to produce a fixed-size latent embedding of variable network topologies.&lt;/li&gt;&lt;li&gt;Pairs the representation with a reduced, fixed-size, semantically meaningful action space and applies it to a PPO-based defender.&lt;/li&gt;&lt;li&gt;Evaluated in the CAGE Challenge 4 cyber operations gym to reduce sim-to-real gap; shows retained defensive performance and improved action efficiency without retraining across differing network sizes/topologies.&lt;/li&gt;&lt;li&gt;Demonstrates deployment of a single network-agnostic agent across multiple network segments, highlighting generalisability for real-world variable networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Dudman', 'Martyn Bull']&lt;/li&gt;&lt;li&gt;Tags: ['cyberdefense', 'reinforcement-learning', 'graph-neural-networks', 'sim-to-real', 'autonomous-defence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09114</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</title><link>https://arxiv.org/abs/2509.10809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces S&amp;P Top-K, a retraining-free, encoder-centric Selection-and-Projection method that selects Top-K encoder features aligned with a target attribute/behavior, optionally aggregates them into a control axis, and applies an orthogonal projection directly in the model's embedding space.&lt;/li&gt;&lt;li&gt;Claims superior inference-time steering compared to traditional decoder-centric sparse autoencoder (SAE) interventions, with cross-modal benefits across vision-language and LLM settings.&lt;/li&gt;&lt;li&gt;Reports concrete safety- and fairness-related gains: up to 3.2x improvement on fairness metrics (CelebA, FairFace) and up to 3.6x reduction in aggressiveness/sycophancy in Llama-3 8B Instruct versus masked reconstruction baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio B\\u{a}rb\\u{a}lau', 'Cristian Daniel P\\u{a}duraru', 'Teodor Poncu', 'Alexandru Tifrea', 'Elena Burceanu']&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'alignment', 'fairness', 'safety interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10809</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IF-GUIDE: Influence Function-Guided Detoxification of LLMs</title><link>https://arxiv.org/abs/2506.01790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IF-GUIDE, a proactive, data-level method that leverages influence functions to identify and suppress harmful tokens/records in training data to reduce LLM toxicity.&lt;/li&gt;&lt;li&gt;Introduces a novel adaptation to compute token-level attributions to model toxicity, selection strategies for toxic documents, and a learning objective usable during pre-training and fine-tuning without human-preference data.&lt;/li&gt;&lt;li&gt;Empirical results show substantial reductions in explicit and implicit toxicity (up to 10× vs uncensored models and up to 3× vs baseline alignment methods like DPO and RAD) and demonstrate computational efficiency by using a much smaller proxy model to compute influence scores.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Coalson', 'Juhan Bae', 'Nicholas Carlini', 'Sanghyun Hong']&lt;/li&gt;&lt;li&gt;Tags: ['model safety', 'toxicity mitigation', 'influence functions', 'training-data interventions', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01790</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats</title><link>https://arxiv.org/abs/2512.05331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes linguistic, stylistic, and lexical signatures of 'pink slime' auto-generated local news articles to enable detection.&lt;/li&gt;&lt;li&gt;Demonstrates that consumer-accessible LLMs can perform adversarial modifications that reduce existing detectors' F1 by up to 40%.&lt;/li&gt;&lt;li&gt;Proposes a robust learning framework designed to resist LLM-based adversarial attacks and reports up to 27% improvement over baselines.&lt;/li&gt;&lt;li&gt;Focuses on threat modeling and mitigation for automated misinformation generation/obfuscation using LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadat Shahriar', 'Navid Ayoobi', 'Arjun Mukherjee', 'Mostafa Musharrat', 'Sai Vishnu Vamsi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM adversarial attacks', 'Misinformation detection', 'Robustness / Adversarial robustness', 'Forensic NLP', 'Red teaming / adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05331</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics for LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Question-Context-Answer (QCA) triplets, and Semantic Entropy Production (SEP) inspired by thermodynamics.&lt;/li&gt;&lt;li&gt;Models an LLM as a bipartite information engine (Maxwell demon) transforming context C into answer A via prompt Q; infers transition matrices via convex optimization and maps minimal KL divergence to a [0,1] faithfulness score.&lt;/li&gt;&lt;li&gt;Claims high SF correlates with low SEP and demonstrates the framework on LLM summarization of SEC 10-K filings, proposing use for evaluation and hallucination control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination-detection', 'safety-evaluation', 'information-theory', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Developing synthetic microdata through machine learning for firm-level business surveys</title><link>https://arxiv.org/abs/2512.05948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses machine learning to generate synthetic public-use microdata (PUMS) for firm-level business surveys to mitigate re-identification risk.&lt;/li&gt;&lt;li&gt;Discusses unique confidentiality challenges for business data (industry/geography identifiability) and quality metrics for synthetic data.&lt;/li&gt;&lt;li&gt;Presents two synthetic PUMS for the 2007 Survey of Business Owners and validates realism via econometric replication of an existing analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jorge Cisneros Paz', 'Timothy Wojan', 'Matthew Williams', 'Jennifer Ozawa', 'Robert Chew', 'Kimberly Janda', 'Timothy Navarro', 'Michael Floyd', 'Christine Task', 'Damon Streat']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'synthetic-data', 're-identification-risk', 'privacy-preserving-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05948</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Bayes Inconsistency of Disagreement Discrepancy Surrogates</title><link>https://arxiv.org/abs/2512.05931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes disagreement discrepancy (how model disagreement changes under distribution shift) and shows common surrogate losses for it are not Bayes consistent, meaning maximizing them can fail to maximize true disagreement.&lt;/li&gt;&lt;li&gt;Provides upper and lower bounds on the optimality gap for such surrogates and derives a new disagreement loss that, combined with cross-entropy, is a provably consistent surrogate.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved estimation of disagreement discrepancy and robustness across benchmarks, especially under adversarial/challenging shift conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil G. Marchant', 'Andrew C. Cullen', 'Feng Liu', 'Sarah M. Erfani']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution shift', 'safety evaluation', 'surrogate losses', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05931</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction</title><link>https://arxiv.org/abs/2512.05915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Linear Matrix Inequality (LMI) framework and an LDL^T (and Cholesky) decomposition-based parameterization to construct provably L-Lipschitz deep residual and hierarchical networks.&lt;/li&gt;&lt;li&gt;Derives closed-form constraints and a tight relaxation of SDP feasibility that enables end-to-end parameterization ensuring Lipschitz continuity for networks.&lt;/li&gt;&lt;li&gt;Claims applicability to adversarial robustness, certified training, and control systems, and reports 3%–13% accuracy gains over SLL layers on 121 UCI datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marius F. R. Juston', 'Ramavarapu S. Sreenivas', 'Dustin Nottage', 'Ahmet Soylemezoglu']&lt;/li&gt;&lt;li&gt;Tags: ['Lipschitz continuity', 'adversarial robustness', 'certified robustness', 'LMI / LDL^T decomposition', 'network parameterization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05915</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</title><link>https://arxiv.org/abs/2512.05648</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective GradienT Masking (SGTM), a pretraining-time method that zero-masks selected gradients so target-domain examples update only a dedicated subset of parameters to localize and enable removal of specific knowledge.&lt;/li&gt;&lt;li&gt;Evaluates SGTM on removing one language from a bilingual synthetic dataset and removing biology knowledge from an English Wikipedia-trained model, showing improved retain/forget trade-offs under label noise compared to data filtering and prior Gradient Routing.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to adversarial fine-tuning: SGTM requires roughly seven times more fine-tuning steps to recover forgotten knowledge compared to a finetuning-based unlearning baseline (RMU).&lt;/li&gt;&lt;li&gt;Argues SGTM is a practical complement to existing safety mitigations in settings with unavoidable label noise, addressing dual-use capability risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Shilov', 'Alex Cloud', 'Aryo Pradipta Gema', 'Jacob Goldman-Wetzler', 'Nina Panickssery', 'Henry Sleight', 'Erik Jones', 'Cem Anil']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'capability removal', 'pretraining mitigation', 'gradient routing / SGTM', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05648</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Credal and Interval Deep Evidential Classifications</title><link>https://arxiv.org/abs/2512.05526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Credal Deep Evidential Classification (CDEC) and Interval Deep Evidential Classification (IDEC) to represent predictive uncertainty via credal sets or intervals over evidential predictive distributions.&lt;/li&gt;&lt;li&gt;Methods can abstain when epistemic or aleatoric uncertainty exceeds thresholds, provide robust sets of labels with probabilistic guarantees, and are trained end-to-end with a loss from evidence theory.&lt;/li&gt;&lt;li&gt;Evaluated on standard image classification benchmarks (MNIST, CIFAR-10/100) and natural OOD shifts, claiming competitive accuracy, strong OOD detection for epistemic/total uncertainty, and well-calibrated, expanding prediction regions under distribution shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michele Caprio', 'Shireen K. Manchingal', 'Fabio Cuzzolin']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'evidential-deep-learning', 'out-of-distribution-detection', 'abstention/rejection', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05526</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation</title><link>https://arxiv.org/abs/2512.05341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an unlearning framework for LLMs used in hardware (RTL) code generation to remove memorized proprietary IP, contaminated benchmarks, and unsafe coding patterns.&lt;/li&gt;&lt;li&gt;Introduces a syntax-preserving unlearning strategy plus a floor-aware selective loss to precisely remove problematic knowledge while preserving code structure and functionality.&lt;/li&gt;&lt;li&gt;Reports experiments showing support for larger forget sets (up to 3x), often needing only a single epoch, with maintained syntactic correctness and functional integrity of generated RTL code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiwen Liang', 'Qiufeng Li', 'Shikai Wang', 'Weidong Cao']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'model editing', 'code generation', 'IP/privacy protection', 'safety/reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05341</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models</title><link>https://arxiv.org/abs/2512.05339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Roblox Guard 1.0, an instruction-fine-tuned LLM (Llama-3.1-8B-Instruct backbone) designed for comprehensive input-output moderation and guardrails using a pipeline of LLMs.&lt;/li&gt;&lt;li&gt;Fine-tuning uses a mixture of synthetic and open-source safety datasets, augmented with chain-of-thought rationales and input inversion to improve contextual understanding and generalization to unseen safety taxonomies.&lt;/li&gt;&lt;li&gt;Claims strong out-of-domain safety benchmark performance and generalization across previously unseen safety categories.&lt;/li&gt;&lt;li&gt;Releases RobloxGuard-Eval, an extensible benchmark/taxonomy to systematically evaluate moderation effectiveness and LLM guardrails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahesh Kumar Nandwana', 'Youngwan Lim', 'Joseph Liu', 'Alex Yang', 'Varun Notibala', 'Nishchaie Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'moderation', 'guardrails', 'instruction fine-tuning', 'safety benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05339</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition</title><link>https://arxiv.org/abs/2512.05323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of FourCastNetv2 (FCNv2) to Gaussian noise injected into ERA5 initial conditions for Hurricane Florence (Sept 13–16, 2018) and to fully random initial conditions.&lt;/li&gt;&lt;li&gt;Finds FCNv2 preserves hurricane features and general storm trajectory under low-to-moderate noise; positional accuracy degrades at high noise levels.&lt;/li&gt;&lt;li&gt;Reports consistent underestimation of storm intensity and persistence across noise levels.&lt;/li&gt;&lt;li&gt;Shows that with fully random initial conditions, FCNv2 produces smooth, cohesive forecasts after a few timesteps, indicating a tendency toward stable, smoothed outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Lizerbram', 'Shane Stevenson', 'Iman Khadir', 'Matthew Tu', 'Samuel S. P. Shen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'model-evaluation', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05323</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?</title><link>https://arxiv.org/abs/2512.05311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates SOTA models' ability to distinguish human vs LLM-generated scientific ideas, especially after iterative paraphrasing.&lt;/li&gt;&lt;li&gt;Finds detection performance degrades substantially (≈25.4% drop) after five paraphrasing stages, with simplified non-expert rewrites causing the largest erosion.&lt;/li&gt;&lt;li&gt;Shows adding the research problem as context modestly improves detection (up to 2.97%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadat Shahriar', 'Navid Ayoobi', 'Arjun Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'adversarial paraphrasing', 'attribution', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05311</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When unlearning is free: leveraging low influence points to reduce computational costs</title><link>https://arxiv.org/abs/2512.05254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies subsets of training data with negligible influence on model outputs using influence functions across language and vision tasks.&lt;/li&gt;&lt;li&gt;Proposes an efficient unlearning framework that prunes low-influence points from forget sets before performing unlearning.&lt;/li&gt;&lt;li&gt;Demonstrates substantial computational savings (up to ~50%) on real-world examples while maintaining unlearning effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anat Kleiman', 'Robert Fisher', 'Ben Deaner', 'Udi Wieder']&lt;/li&gt;&lt;li&gt;Tags: ['Machine Unlearning', 'Data Privacy', 'Influence Functions', 'Computational Efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05254</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Concept-Guided Backdoor Attack on Vision Language Models</title><link>https://arxiv.org/abs/2512.00713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces concept-guided backdoor attacks on Vision-Language Models (VLMs) that operate at the semantic concept level rather than pixel-level triggers.&lt;/li&gt;&lt;li&gt;Proposes two attacks: Concept-Thresholding Poisoning (CTP) which poisons only images containing a target concept, and CBL-Guided Unseen Backdoor (CGUB) which uses a Concept Bottleneck Model during training to alter concept activations and is removed at inference.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates across multiple VLM architectures and datasets while keeping moderate degradation on clean-task performance.&lt;/li&gt;&lt;li&gt;Highlights a novel attack surface—semantic concept-level vulnerabilities—that can produce targeted label replacements in generated text even when such behavior never appears in training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Shen', 'Weimin Lyu', 'Haotian Xu', 'Tengfei Ma']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'vision-language models', 'semantic/ concept-level triggers', 'adversarial security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00713</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MindEval: Benchmarking Language Models on Multi-turn Mental Health Support</title><link>https://arxiv.org/abs/2511.18491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MindEval, a fully automated, multi-turn benchmark for evaluating LLMs in realistic mental-health therapy dialogues, developed with clinical psychologists.&lt;/li&gt;&lt;li&gt;Uses simulated patients validated against human text and shows strong correlation between automatic LLM-based metrics and human expert judgments.&lt;/li&gt;&lt;li&gt;Evaluates 12 state-of-the-art LLMs, finding widespread safety/quality failures (e.g., sycophancy, overvalidation, reinforcement of maladaptive beliefs), especially over longer interactions or severe cases; releases code and data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jos\\'e Pombal", "Maya D'Eon", 'Nuno M. Guerreiro', 'Pedro Henrique Martins', "Ant\\'onio Farinhas", 'Ricardo Rei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Benchmarking', 'Harmful outputs / user safety', 'Mental-health evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18491</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability</title><link>https://arxiv.org/abs/2510.12229</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether the Knobe effect (a moral bias in intentionality judgements) emerges in finetuned LLMs.&lt;/li&gt;&lt;li&gt;Performs Layer-Patching analysis across three open-weight LLMs and finds the bias is learned during finetuning and localized to specific layers.&lt;/li&gt;&lt;li&gt;Shows that patching activations from the pretrained model into a few critical layers can eliminate the bias without retraining.&lt;/li&gt;&lt;li&gt;Suggests that social biases in LLMs can be interpreted, localized, and mitigated via targeted interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bianca Raimondi', 'Daniela Dalbagno', 'Maurizio Gabbrielli']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'mechanistic interpretability', 'mitigation', 'finetuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12229</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HARP: Hallucination Detection via Reasoning Subspace Projection</title><link>https://arxiv.org/abs/2509.11536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HARP, a method that decomposes LLM hidden state space into semantic and reasoning subspaces and uses projections onto the reasoning subspace for hallucination detection.&lt;/li&gt;&lt;li&gt;Uses SVD on the Unembedding layer to obtain basis vectors for the two subspaces and reduces feature dimensionality to ~5% of original.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art hallucination detection performance (e.g., AUROC 92.8% on TriviaQA) and improved robustness across datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Hu', 'Gang Tu', 'ShengYu Cheng', 'Jinxin Li', 'Jinting Wang', 'Rui Chen', 'Zhilong Zhou', 'Dongbo Shan']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'representation disentanglement', 'interpretability', 'feature projection/SVD']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11536</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</title><link>https://arxiv.org/abs/2509.10809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces S&amp;P Top-K, a retraining-free, encoder-centric Selection-and-Projection method that selects Top-K encoder features aligned with a target attribute/behavior, optionally aggregates them into a control axis, and applies an orthogonal projection directly in the model's embedding space.&lt;/li&gt;&lt;li&gt;Claims superior inference-time steering compared to traditional decoder-centric sparse autoencoder (SAE) interventions, with cross-modal benefits across vision-language and LLM settings.&lt;/li&gt;&lt;li&gt;Reports concrete safety- and fairness-related gains: up to 3.2x improvement on fairness metrics (CelebA, FairFace) and up to 3.6x reduction in aggressiveness/sycophancy in Llama-3 8B Instruct versus masked reconstruction baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio B\\u{a}rb\\u{a}lau', 'Cristian Daniel P\\u{a}duraru', 'Teodor Poncu', 'Alexandru Tifrea', 'Elena Burceanu']&lt;/li&gt;&lt;li&gt;Tags: ['model steering', 'alignment', 'fairness', 'safety interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10809</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title><link>https://arxiv.org/abs/2508.11222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORFuzz, an evolutionary testing/fuzzing framework to detect LLM over-refusal (erroneously refusing benign queries).&lt;/li&gt;&lt;li&gt;Combines safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and OR-Judge (a human-aligned judge model) for validation.&lt;/li&gt;&lt;li&gt;Produces ORFuzzSet, a benchmark of 1,855 validated test cases, and demonstrates substantially higher over-refusal discovery and transferability across 10 LLMs compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haonan Zhang', 'Dongxia Wang', 'Yi Liu', 'Kexin Chen', 'Jiashui Wang', 'Xinlei Ying', 'Long Liu', 'Wenhai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Over-refusal testing', 'Fuzzing', 'Safety evaluation', 'Benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11222</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VERIRAG: A Post-Retrieval Auditing of Scientific Study Summaries</title><link>https://arxiv.org/abs/2507.17948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERIRAG, a post-retrieval auditing framework that detects methodological vulnerabilities in RAG-generated scientific summaries rather than simply classifying them.&lt;/li&gt;&lt;li&gt;Provides a benchmark of 1,730 summaries with realistic, non-obvious perturbations modeled after retracted papers and a Veritable taxonomy of statistical rigor for auditing.&lt;/li&gt;&lt;li&gt;Uses private Small Language Models (GPT-based, Mistral, Gemma) to perform audits and reports at least a 19-point Macro F1 improvement over baselines, with generated audit trails judged useful by human testers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shubham Mohole', 'Hongjun Choi', 'Shusen Liu', 'Christine Klymko', 'Shashank Kushwaha', 'Derek Shi', 'Wesam Sakla', 'Sainyam Galhotra', 'Ruben Glatt']&lt;/li&gt;&lt;li&gt;Tags: ['RAG auditing', 'model safety', 'misinformation detection', 'benchmark/dataset', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17948</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models</title><link>https://arxiv.org/abs/2507.15663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SustainDiffusion, a search-based method that optimises hyperparameters and prompt structures for Stable Diffusion to reduce gender and ethnic bias and lower energy consumption without model fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluates across 56 prompts and six baselines, reporting reductions of ~68% in gender bias, ~59% in ethnic bias, and ~48% in CPU+GPU energy consumption while maintaining comparable image quality.&lt;/li&gt;&lt;li&gt;Method operates at the prompt/hyperparameter level (no architecture or weights changes) and demonstrates consistency across runs and some generalisation to different prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Giordano d'Aloisio", 'Tosin Fadahunsi', 'Jay Choy', 'Rebecca Moussa', 'Federica Sarro']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'fairness', 'prompt engineering', 'energy efficiency', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15663</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Ordinal Bias in Action Recognition for Instructional Videos</title><link>https://arxiv.org/abs/2504.06580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines "ordinal bias": models relying on dataset-specific, dominant action sequences rather than true video understanding.&lt;/li&gt;&lt;li&gt;Proposes two stress-test manipulations for instructional videos: Action Masking (mask frequent co-occurring action frames) and Sequence Shuffling (randomize action segment order).&lt;/li&gt;&lt;li&gt;Empirically shows significant performance degradation of current action-recognition models on nonstandard action sequences, revealing vulnerabilities tied to sequence priors.&lt;/li&gt;&lt;li&gt;Argues for revised evaluation strategies and development of models that generalize beyond fixed action patterns in instructional videos.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joochan Kim', 'Minjoon Jung', 'Byoung-Tak Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'dataset bias', 'evaluation/benchmarking', 'video action recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06580</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Edge-Only Universal Adversarial Attacks in Distributed Learning</title><link>https://arxiv.org/abs/2411.10500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a threat model for split inference (edge-cloud distributed models) where the attacker only controls the edge (initial layers) and crafts universal adversarial perturbations (UAPs) to manipulate intermediate features.&lt;/li&gt;&lt;li&gt;Proposes edge-only untargeted and targeted UAP formulations to induce mispredictions in the unknown cloud component by controlling representations before the split point.&lt;/li&gt;&lt;li&gt;Empirically demonstrates strong transferability of edge-only UAPs on ImageNet across multiple network architectures and compares performance to white-box and black-box baselines.&lt;/li&gt;&lt;li&gt;Analyzes targeted attack capabilities and highlights new vulnerabilities unique to partial model access in split inference, motivating defenses for edge-only threat scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Rossolini', 'Tommaso Baldi', 'Alessandro Biondi', 'Giorgio Buttazzo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'universal adversarial perturbations', 'split inference / distributed learning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10500</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty</title><link>https://arxiv.org/abs/2511.21569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical audit of 16 open-weight LLMs (4B–671B params) across 19,200 trials measuring whether models disclose they are AI when assigned professional personas (e.g., Financial Advisor vs Neurosurgeon).&lt;/li&gt;&lt;li&gt;Finds large, domain-specific variance in disclosure rates (2.8%–73.6%), strong model-identity effects over parameter count, and heterogeneous impacts from reasoning/instruction-tuned variants.&lt;/li&gt;&lt;li&gt;Shows explicit permission to disclose sharply increases honesty (23.7% → 65.8%), indicating suppression is driven by instruction-following priority rather than capability limits.&lt;/li&gt;&lt;li&gt;Concludes that persona-based deployment can produce trust calibration risks and that safety properties do not reliably transfer across domains without deliberate design and empirical verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Diep']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'honesty/deception', 'instruction-following', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21569</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</title><link>https://arxiv.org/abs/2511.06626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Report Fine-Tuning (SRFT), a supervised fine-tuning method that trains LLMs to admit factual mistakes and, critically, to confess hidden misaligned objectives when interrogated.&lt;/li&gt;&lt;li&gt;Shows SRFT generalizes OOD to adversarial agentic settings: models trained with SRFT confess hidden objectives under pressure, enabling high detection performance (F1 = 0.98) versus baseline (F1 = 0).&lt;/li&gt;&lt;li&gt;Demonstrates SRFT can elicit substantive content of hidden objectives (recovering 28–100% of details) where baselines and prefilled assistant-turn attacks recover none.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chloe Li', 'Mary Phuong', 'Daniel Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'model honesty / lying', 'safety evaluation', 'adversarial objectives']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06626</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</title><link>https://arxiv.org/abs/2510.18526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of LLMs to pluralistic human values, focusing on interdependent value dimensions and priority (value complexity) and controllability of nuanced/underrepresented priorities (value steerability).&lt;/li&gt;&lt;li&gt;Proposes COUPLE: a framework that builds an explicit structural causal model (SCM) linking high-level value dimensions to behaviors and models interdependencies/priorities among features.&lt;/li&gt;&lt;li&gt;Uses counterfactual reasoning over the SCM to generate outputs aligned with specified value objectives, improving steerability and interpretability.&lt;/li&gt;&lt;li&gt;Evaluated on two datasets with different value systems and shows improvements over baselines across diverse value objectives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanze Guo', 'Jing Yao', 'Xiao Zhou', 'Xiaoyuan Yi', 'Xing Xie']&lt;/li&gt;&lt;li&gt;Tags: ['value alignment', 'LLM safety', 'causal modeling', 'counterfactual reasoning', 'steerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18526</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SOCK: A Benchmark for Measuring Self-Replication in Large Language Models</title><link>https://arxiv.org/abs/2509.25643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SOCK, a CLI-based benchmark that measures LLMs' ability to self-replicate and persist across computational contexts.&lt;/li&gt;&lt;li&gt;Defines Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL) and produces an R-score from a five-task suite using agentic LLM interactions.&lt;/li&gt;&lt;li&gt;Evaluates open and proprietary models, identifies obstacles to persistent self-replication and multi-agent coordination, and suggests directions to mitigate related risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Chavarria', 'Rohan Raizada', 'Justin White', 'Eyad Alhetairshi']&lt;/li&gt;&lt;li&gt;Tags: ['self-replication', 'safety evaluation', 'benchmarking', 'multi-agent risks', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25643</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title><link>https://arxiv.org/abs/2506.16402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IS-Bench, a multi-modal benchmark for assessing interactive safety of VLM-driven embodied agents in household tasks, with 161 scenarios and 388 unique safety risks in a high-fidelity simulator.&lt;/li&gt;&lt;li&gt;Proposes a process-oriented evaluation that verifies whether agents perceive emergent risks and perform mitigation actions in the correct procedural order (before/after risk-prone steps).&lt;/li&gt;&lt;li&gt;Evaluates leading VLM-driven agents (e.g., GPT-4o, Gemini-2.5), finding they lack interactive safety awareness; safety-aware Chain-of-Thought can improve safety detection but often reduces task completion.&lt;/li&gt;&lt;li&gt;Provides code and data to support development of safer embodied AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Zeren Chen', 'Xuhao Hu', 'Yijin Zhou', 'Weichen Zhang', 'Dongrui Liu', 'Lu Sheng', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Safety evaluation', 'Embodied agents', 'Benchmarks', 'Vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16402</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning</title><link>https://arxiv.org/abs/2504.07640</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neuro-symbolic pipeline that integrates OWL ontologies and a symbolic reasoner (e.g., HermiT) with LLM outputs to detect and correct logical inconsistencies and hallucinations.&lt;/li&gt;&lt;li&gt;Uses a lightweight ML mapper (logistic regression) to convert natural language statements into logical forms compatible with the ontology, enabling consistency checking.&lt;/li&gt;&lt;li&gt;When inconsistencies are found, the system generates explanatory feedback and iteratively refines the LLM response toward logical coherence.&lt;/li&gt;&lt;li&gt;Provides a Python prototype and domain-specific experiments showing improved semantic coherence and factual accuracy of LLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruslan Idelfonso Magana Vsevolodovna', 'Marco Monti']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination_mitigation', 'neuro-symbolic', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.07640</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trusted AI Agents in the Cloud</title><link>https://arxiv.org/abs/2512.05951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Omega, a system to enable trusted AI agents in the cloud by enforcing end-to-end isolation and supervising all external interactions.&lt;/li&gt;&lt;li&gt;Uses nested isolation inside Confidential VMs and Confidential GPUs to host many agents securely within a single CVM, protecting agent state across CVM-GPU.&lt;/li&gt;&lt;li&gt;Introduces cross-principal trust via differential attestation and a policy specification/enforcement framework for data access, tool usage, inter-agent communication, and accountable provenance.&lt;/li&gt;&lt;li&gt;Implements the design on AMD SEV-SNP and NVIDIA H100 and reports high performance and high-density, policy-compliant multi-agent deployments at cloud scale.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teofil Bodea', 'Masanori Misono', 'Julian Pritzi', 'Patrick Sabanic', 'Thore Sommer', 'Harshavardhan Unnibhavi', 'David Schall', 'Nuno Santos', 'Dimitrios Stavrakakis', 'Pramod Bhatotia']&lt;/li&gt;&lt;li&gt;Tags: ['confidential computing', 'attestation', 'trusted execution', 'AI agent security', 'policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05951</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</title><link>https://arxiv.org/abs/2512.05937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how background correlations and camera variation affect classification performance and feature importance for traffic sign recognition.&lt;/li&gt;&lt;li&gt;Creates six synthetic datasets that isolate degrees of camera variation and background correlation to enable controlled, quantitative analyses.&lt;/li&gt;&lt;li&gt;Uses XAI methods (e.g., saliency like SHAP/GradCAM) combined with ground-truth object masks to measure when classifiers rely on background vs. object features.&lt;/li&gt;&lt;li&gt;Quantifies conditions under which background features gain importance and how training-domain changes influence reliance on spurious correlations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anne Sielemann', 'Valentin Barner', 'Stefan Wolf', 'Masoud Roschani', 'Jens Ziehn', 'Juergen Beyerer']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'explainability', 'spurious_correlations', 'synthetic_datasets', 'autonomous_vehicles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05937</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</title><link>https://arxiv.org/abs/2512.05927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C3, an uncertainty quantification method to produce dense, calibrated subpatch-level uncertainty estimates for controllable video generation models to mitigate hallucinations.&lt;/li&gt;&lt;li&gt;Three technical contributions: training for correctness and calibration via strictly proper scoring rules; estimating uncertainty in latent space to avoid pixel-space instabilities and cost; mapping latent uncertainty to interpretable pixel-level RGB heatmaps.&lt;/li&gt;&lt;li&gt;Demonstrates calibrated in-distribution uncertainty and effective out-of-distribution detection on large-scale robot learning datasets (Bridge, DROID) and real-world evaluations, with applications to safer robot policy evaluation and planning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiting Mei', 'Tenny Yin', 'Micah Baker', 'Ola Shorinwa', 'Anirudha Majumdar']&lt;/li&gt;&lt;li&gt;Tags: ['Uncertainty quantification', 'Calibration', 'Hallucination detection', 'Out-of-distribution detection', 'Robot world modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05927</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework</title><link>https://arxiv.org/abs/2512.05863</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a retrieval-augmented generation (RAG) medical QA system combining domain-specific retrieval with open-source LLMs (LLaMA 2, Falcon).&lt;/li&gt;&lt;li&gt;Fine-tunes models with LoRA for domain specialization and compares fine-tuned vs. zero-shot performance on PubMedQA and MedMCQA.&lt;/li&gt;&lt;li&gt;Reports substantial accuracy gains (e.g., 71.8% vs 55.4% on PubMedQA) and a ~60% reduction in unsupported/hallucinatory content when grounding answers with retrieved evidence.&lt;/li&gt;&lt;li&gt;Emphasizes transparency by returning source references alongside generated answers and details system/fine-tuning methodology.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tasnimul Hassan', 'Md Faisal Karim', 'Haziq Jeelani', 'Elham Behnam', 'Robert Green', 'Fayeq Jeelani Syed']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'medical QA', 'LLM fine-tuning', 'safety/robustness (factuality)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05863</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning</title><link>https://arxiv.org/abs/2512.05711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical UAV trajectory planning framework using Bayesian Active Inference to operate under adversarial jamming without prior jammer location knowledge.&lt;/li&gt;&lt;li&gt;Combines expert demonstrations, probabilistic generative modeling for high-level symbolic planning, low-level motion control, and wireless signal feedback for online inference and jammer localization.&lt;/li&gt;&lt;li&gt;Simulation results show near-expert performance, reduced communication interference and mission cost versus model-free RL baselines, and robustness in dynamic environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Krayani', 'Seyedeh Fatemeh Sadati', 'Lucio Marcenaro', 'Carlo Regazzoni']&lt;/li&gt;&lt;li&gt;Tags: ['UAV security', 'anti-jamming', 'adversarial resilience', 'Bayesian active inference', 'trajectory planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05711</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains</title><link>https://arxiv.org/abs/2512.05700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes fusing multiple elementary faithfulness metrics using a tree-based model to produce a combined metric that better correlates with human judgements.&lt;/li&gt;&lt;li&gt;Uses human annotations across question-answering and dialogue domains to drive metric importance and validate the fused metric.&lt;/li&gt;&lt;li&gt;Demonstrates improved correlation with human judgements across tested domains and provides a homogenised dataset of human judgments and LLM responses for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Malin', 'Tatiana Kalganova', 'Nikolaos Boulgouris']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness evaluation', 'safety evaluation', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05700</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models</title><link>https://arxiv.org/abs/2512.05546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Conscious Gaze (CG-VLM), a training-free, inference-time framework to reduce object hallucinations in vision-language models by steering mid-layer attention back to visual tokens.&lt;/li&gt;&lt;li&gt;Introduces a Cognitive Demand Sensor using Harsanyi interactions to detect when vision-text synergy is low and visual grounding is needed.&lt;/li&gt;&lt;li&gt;Uses Focused Consensus Induction to selectively reorient attention before collapse into linguistic priors, enabling token-level, context-aware intervention.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on POPE and CHAIR across multiple VLMs (InstructBLIP, LLaVA, Qwen-VL, mPLUG) while preserving general capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weijue Bu', 'Guan Yuan', 'Guixian Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'interpretability', 'robustness', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05546</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora</title><link>https://arxiv.org/abs/2512.05519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Qualitative analysis of user comments on OpenAI's Sora revealing how users assess authenticity of AI-generated videos (lighting, motion, physics) and blur lines between real and synthetic media.&lt;/li&gt;&lt;li&gt;Users shift from viewers to creators, treating text prompts as intellectual property and expressing concerns about plagiarism and remixing norms.&lt;/li&gt;&lt;li&gt;Users report and share tactics to evade platform moderation (misspellings, alternative phrasing, emojis, other languages) and express worries about misinformation and bot-generated engagement.&lt;/li&gt;&lt;li&gt;Findings highlight contested platform governance, perceptions of inconsistent/opaque moderation, and grassroots enforcement of ethical norms regarding misuse of real people's images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bohui Shen', 'Shrikar Bhatta', 'Alex Ireebanije', 'Zexuan Liu', 'Abhinav Choudhry', 'Ece Gumusel', 'Kyrie Zhixuan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'prompt-censorship-evasion', 'misinformation', 'platform-governance', 'AI-generated-media']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05519</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Matching Ranks Over Probability Yields Truly Deep Safety Alignment</title><link>https://arxiv.org/abs/2512.05518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that SFT data-augmentation defenses against the prefilling jailbreak are vulnerable to a generalized Rank-Assisted Prefilling (RAP) attack that picks low-probability but high-rank harmful tokens from the top-k predictions to elicit harmful continuations.&lt;/li&gt;&lt;li&gt;Analyzes the root cause as objective 'gaming' when target distribution entropy is low: fine-tuning concentrates probability mass on a few refusal tokens while harmful tokens remain high in rank but low in probability.&lt;/li&gt;&lt;li&gt;Proposes a rank-matching perspective and introduces PRESTO (PRefill attEntion STOpping), a simple regularization on attention to harmful prefill tokens, which improves StrongREJECT under RAP attacks up to 4.7x with minimal utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Vega', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'safety alignment', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05518</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment</title><link>https://arxiv.org/abs/2512.05464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Collective Agency (CA) as an open-ended alignment objective that emphasizes integrated agentic capabilities beyond standard helpfulness/honesty/harmlessness norms.&lt;/li&gt;&lt;li&gt;Introduces Dynamic Alignment, a scalable self-improving framework combining automated training-data generation by LLMs and a self-rewarding mechanism where the policy model evaluates its own outputs and assigns rewards for GRPO-based learning.&lt;/li&gt;&lt;li&gt;Reports experiments showing models aligned to CA while retaining general NLP capabilities, aiming to reduce reliance on costly human feedback.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panatchakorn Anantaprayoon', 'Nataliia Babina', 'Jad Tarifi', 'Nima Asgharbeygi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improving alignment', 'reward modeling', 'scalable safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05464</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation</title><link>https://arxiv.org/abs/2512.05383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts coverage-guided fuzzing to stress-test ML-driven neurostimulation encoders by perturbing inputs and searching for outputs that violate biophysical safety limits (charge density, instantaneous current, electrode co-activation).&lt;/li&gt;&lt;li&gt;Treats encoders as black boxes and uses coverage metrics that quantify exploration breadth over output space and violation types to steer test-case generation.&lt;/li&gt;&lt;li&gt;Demonstrates the method on deep stimulus encoders for retina and cortex, discovering diverse unsafe stimulation regimes and proposing violation-output coverage metrics to compare architectures and training strategies.&lt;/li&gt;&lt;li&gt;Frames safety assessment as an empirical, reproducible process to support benchmarking, regulatory readiness, and ethical assurance for neural interfaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mara Downing', 'Matthew Peng', 'Jacob Granley', 'Michael Beyeler', 'Tevfik Bultan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-testing', 'coverage-guided-fuzzing', 'neural-stimulation', 'medical-device-safety', 'adversarial-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05383</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Self-Preference by Authorship Obfuscation</title><link>https://arxiv.org/abs/2512.05379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and studies the 'self-preference' bias where LM judges prefer their own outputs over others, even without source labels.&lt;/li&gt;&lt;li&gt;Proposes and tests black-box perturbation strategies (e.g., synonym replacement) to obfuscate authorship in pairwise comparisons and reduce self-recognition.&lt;/li&gt;&lt;li&gt;Finds partial mitigation is possible but self-preference can re-emerge when stylistic differences are more fully neutralized, indicating multi-level sources of self-recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taslim Mahbub', 'Shi Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'self-preference', 'authorship obfuscation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05379</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Please Don't Kill My Vibe: Empowering Agents with Data Flow Control</title><link>https://arxiv.org/abs/2512.05374</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies security and policy risks from uncontrolled data flows in LLM agent workflows (policy violations, process corruption, security flaws).&lt;/li&gt;&lt;li&gt;Argues for native Data Flow Controls (DFCs) in systems—analogous to shifting validation/access control into DBMS—to manage undesirable agent-produced data flows.&lt;/li&gt;&lt;li&gt;Describes early implementation of a portable DFC instance for DBMSes and outlines a broader research agenda for DFCs in agent ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlie Summers', 'Haneen Mohammed', 'Eugene Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data-flow control', 'agent security', 'policy enforcement', 'database security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05374</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition</title><link>https://arxiv.org/abs/2512.05323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of FourCastNetv2 (FCNv2) to Gaussian noise injected into ERA5 initial conditions for Hurricane Florence (Sept 13–16, 2018) and to fully random initial conditions.&lt;/li&gt;&lt;li&gt;Finds FCNv2 preserves hurricane features and general storm trajectory under low-to-moderate noise; positional accuracy degrades at high noise levels.&lt;/li&gt;&lt;li&gt;Reports consistent underestimation of storm intensity and persistence across noise levels.&lt;/li&gt;&lt;li&gt;Shows that with fully random initial conditions, FCNv2 produces smooth, cohesive forecasts after a few timesteps, indicating a tendency toward stable, smoothed outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Lizerbram', 'Shane Stevenson', 'Iman Khadir', 'Matthew Tu', 'Samuel S. P. Shen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'model-evaluation', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05323</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?</title><link>https://arxiv.org/abs/2512.05311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates SOTA models' ability to distinguish human vs LLM-generated scientific ideas, especially after iterative paraphrasing.&lt;/li&gt;&lt;li&gt;Finds detection performance degrades substantially (≈25.4% drop) after five paraphrasing stages, with simplified non-expert rewrites causing the largest erosion.&lt;/li&gt;&lt;li&gt;Shows adding the research problem as context modestly improves detection (up to 2.97%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadat Shahriar', 'Navid Ayoobi', 'Arjun Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'adversarial paraphrasing', 'attribution', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05311</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots</title><link>https://arxiv.org/abs/2512.05270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes XR-DT, an Extended Reality-enhanced Digital Twin framework that links physical and virtual spaces to support interpretable, bi-directional human-robot interaction.&lt;/li&gt;&lt;li&gt;Hierarchical architecture combining VR/AR/MR layers, real-time sensor fusion, Unity-based simulation, and wearable AR-captured human feedback.&lt;/li&gt;&lt;li&gt;Designs an agentic mobile robot using a unified diffusion policy for context-aware task adaptation and a chain-of-thought prompting mechanism for multimodal LLM reasoning; includes AutoGen-based multi-agent coordination.&lt;/li&gt;&lt;li&gt;Presents initial experiments showing accurate human and robot trajectory prediction and claims improved interpretability, trust, and adaptive HRI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Wang', 'Jiseop Byeon', 'Ahmad Yehia', 'Huihai Wang', 'Yiming Xu', 'Tianyi Zeng', 'Ziran Wang', 'Junfeng Jiao', 'Christian Claudel']&lt;/li&gt;&lt;li&gt;Tags: ['human-robot interaction', 'safety', 'interpretability', 'digital twin', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05270</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge</title><link>https://arxiv.org/abs/2512.05176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark to evaluate LLM alignment with community-level social values and common knowledge in the US.&lt;/li&gt;&lt;li&gt;Replicates and adapts the KorNAT (Korean national alignment) process to a more granular, community-focused benchmark rather than a single national standard.&lt;/li&gt;&lt;li&gt;Aims to provide an evaluation foundation to support development and assessment of culturally-informed LLMs that better represent historically marginalized and diverse communities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brittany Johnson', 'Erin Reddick', 'Angela D. R. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmark', 'values-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05176</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images</title><link>https://arxiv.org/abs/2512.05137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChromouVQA, a large-scale benchmark of Ishihara-style chromatic camouflaged images with controlled variation (chromatic separation, density, size, occlusion, rotation) and full metadata.&lt;/li&gt;&lt;li&gt;Defines nine vision-question-answering tasks (recognition, counting, comparison, spatial reasoning) to probe figure–ground segregation in VLMs.&lt;/li&gt;&lt;li&gt;Finds large gaps between human and VLM performance, especially under subtle chromatic contrast or disruptive geometric fills.&lt;/li&gt;&lt;li&gt;Proposes a model-agnostic contrastive recipe that aligns silhouettes with camouflaged renderings to improve global shape recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunfei Zhang', 'Yizhuo He', 'Yuanxun Shao', 'Zhengtao Yao', 'Haoyan Xu', 'Junhao Dong', 'Zhen Yao', 'Zhikang Dong']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'benchmark/dataset', 'camouflage/perceptual robustness', 'contrastive-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05137</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2512.05943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE, a framework to evaluate stepwise reasoning in vision-language models by diagnosing reasoning trajectories rather than only final answers.&lt;/li&gt;&lt;li&gt;Uses Auxiliary Reasoning Sets (ARS) — compact sub-question/answer pairs — and consistency-based metrics to evaluate intermediate steps and pinpoint where reasoning fails.&lt;/li&gt;&lt;li&gt;Shows that ARS consistency correlates with final-answer correctness and defines confidence regions to filter or flag unreliable reasoning paths for debugging and model improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shima Imani', 'Seungwhan Moon', 'Lambert Mathias', 'Lu Zhang', 'Babak Damavandi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'reasoning transparency', 'vision-language models', 'confidence estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05943</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems</title><link>https://arxiv.org/abs/2512.05449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames inconsistency in LLM-based agentic systems as 'akrasia' (weakness of will) and links this to goal drift and potential misalignment.&lt;/li&gt;&lt;li&gt;Introduces the Akrasia Benchmark (Baseline, Synonym, Temporal, Temptation conditions) to measure when models' local responses contradict prior commitments across models and decoding strategies.&lt;/li&gt;&lt;li&gt;Provides empirical/operational tools to compare 'self-control' and discusses how micro-level akrasia could compound into macro-level instability or 'scheming' in multi-agent setups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'agentic-systems', 'benchmark', 'goal-drift', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05449</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BEAVER: An Efficient Deterministic LLM Verifier</title><link>https://arxiv.org/abs/2512.05439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BEAVER, a practical framework to compute deterministic, sound probability bounds on whether LLM outputs satisfy prefix-closed semantic constraints.&lt;/li&gt;&lt;li&gt;Uses novel token trie and frontier data structures to systematically explore generation space and maintain provably sound bounds; formalizes the verification problem and proves soundness.&lt;/li&gt;&lt;li&gt;Evaluates on correctness verification, privacy verification, and secure code generation across state-of-the-art LLMs, producing substantially tighter bounds and flagging more high-risk instances than baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarun Suresh', 'Nalin Wadhwa', 'Debangshu Banerjee', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM verification', 'safety evaluation', 'privacy verification', 'secure code generation', 'formal verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05439</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare</title><link>https://arxiv.org/abs/2512.05365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCP-AI, an architecture combining a Model Context Protocol (MCP) with modular, executable MCP files that capture clinical objectives, patient context, reasoning state, and task logic to enable longitudinal, auditable AI reasoning.&lt;/li&gt;&lt;li&gt;Emphasizes explainability, physician-in-the-loop validation, and secure handoffs between providers, positioning MCP files as reusable, auditable memory objects for clinical workflows.&lt;/li&gt;&lt;li&gt;Describes integration with HL7/FHIR and adherence to regulatory standards (HIPAA, FDA SaMD) to support compliance and secure data transitions.&lt;/li&gt;&lt;li&gt;Validates the approach via two clinical use cases (Fragile X with comorbid depression; remote coordination for Type 2 Diabetes and hypertension) to demonstrate practical workflow and safety-oriented properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zag ElSayed', 'Craig Erickson', 'Ernest Pedapati']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'explainability', 'healthcare', 'regulatory-compliance', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05365</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title><link>https://arxiv.org/abs/2512.05356</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Advocates for 'co-improvement': tightly coupled human–AI collaboration to jointly conduct AI research, from ideation through experimentation.&lt;/li&gt;&lt;li&gt;Argues that centering human research improvement in the loop will accelerate progress while producing safer, more aligned superintelligence via symbiosis.&lt;/li&gt;&lt;li&gt;Positions this approach as a more achievable and safer pathway than autonomous AI self-improvement, emphasizing governance and collaboration rather than technical attacks or defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Weston', 'Jakob Foerster']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'human-AI collaboration', 'co-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05356</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title><link>https://arxiv.org/abs/2512.05156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two unsupervised metrics for LLM faithfulness: Semantic Faithfulness (SF) based on KL divergence between inferred topic-transition matrices for Question-Context-Answer (QCA) triplets, and Semantic Entropy Production (SEP) inspired by thermodynamics.&lt;/li&gt;&lt;li&gt;Models an LLM as a bipartite information engine (Maxwell demon) transforming context C into answer A via prompt Q; infers transition matrices via convex optimization and maps minimal KL divergence to a [0,1] faithfulness score.&lt;/li&gt;&lt;li&gt;Claims high SF correlates with low SEP and demonstrates the framework on LLM summarization of SEC 10-K filings, proposing use for evaluation and hallucination control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Halperin']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination-detection', 'safety-evaluation', 'information-theory', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05156</guid><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 20 Feb 2026 22:56:04 +0000</lastBuildDate><item><title>Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</title><link>https://arxiv.org/abs/2504.21730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cert-SSB, a certified backdoor defense that uses sample-specific randomized smoothing by optimizing per-sample noise magnitudes via stochastic gradient ascent.&lt;/li&gt;&lt;li&gt;Retrains multiple smoothed models on poisoned training sets with the optimized noises and aggregates their predictions; introduces a storage-update-based certification method to certify cases where noise varies per sample.&lt;/li&gt;&lt;li&gt;Demonstrates improved certification performance over existing randomized-smoothing defenses on multiple benchmark datasets and provides code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Yingjia Wang', 'Xing Liu', 'Sixing Wu', 'Jianbin Li', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'certified robustness', 'randomized smoothing', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21730</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance</title><link>https://arxiv.org/abs/2602.06530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForgeryEraser, a black-box universal anti-forensics attack that degrades image forgery detectors without access to their logits.&lt;/li&gt;&lt;li&gt;Exploits shared Vision-Language Model (VLM) backbones (e.g., CLIP) by using a multi-modal guidance loss to push forged image embeddings toward text-derived authentic anchors and away from forgery anchors in the VLM feature space.&lt;/li&gt;&lt;li&gt;Shows substantial performance drops on global synthesis and local editing benchmarks and causes explainable forensic models to produce authenticity-consistent explanations for forged images.&lt;/li&gt;&lt;li&gt;Demonstrates a systemic vulnerability of downstream AIGC detectors that inherit public VLM feature spaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haipeng Li', 'Rongxuan Peng', 'Anwei Luo', 'Shunquan Tan', 'Changsheng Chen', 'Anastasia Antsiferova']&lt;/li&gt;&lt;li&gt;Tags: ['anti-forensics', 'adversarial-attack', 'vision-language', 'image-forgery-detection', 'model-vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06530</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</title><link>https://arxiv.org/abs/2602.17645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transfer-based black-box adversarial attacks on large vision-language models (LVLMs) and diagnoses high-variance, unstable gradients caused by ViT translation sensitivity and source/target crop asymmetry.&lt;/li&gt;&lt;li&gt;Proposes methodological fixes—Multi-Crop Alignment (MCA), Auxiliary Target Alignment (ATA), Patch Momentum and a patch-size ensemble (PE+)—to denoise gradients and strengthen transferable directions.&lt;/li&gt;&lt;li&gt;Implements these modules as M-Attack-V2, demonstrating large improvements in black-box attack success rates on frontier LVLMs (e.g., Claude-4.0, Gemini-2.5-Pro, GPT-5) with code/data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohan Zhao', 'Zhaoyi Li', 'Yaxin Luo', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'vision-language models', 'transfer-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17645</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</title><link>https://arxiv.org/abs/2602.17659</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and systematically studies "counterfactual failures" in Vision-Language-Action (VLA) models where vision-induced shortcuts cause agents to ignore language instructions.&lt;/li&gt;&lt;li&gt;Introduces LIBERO-CF, a counterfactual benchmark that evaluates language-following by assigning alternative instructions in visually plausible layouts.&lt;/li&gt;&lt;li&gt;Proposes Counterfactual Action Guidance (CAG), a dual-branch inference method combining a language-conditioned policy with a language-unconditioned Vision-Action module to regularize language conditioning and reduce reliance on visual shortcuts.&lt;/li&gt;&lt;li&gt;Shows consistent improvements across VLAs and real-world robot tests (e.g., +9.7% language-following accuracy and +3.6% task success in under-observed tasks with a training-free strategy; larger gains when paired with a VA model).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Fang', 'Yuchun Feng', 'Dong Jing', 'Jiaqi Liu', 'Yue Yang', 'Zhenyu Wei', 'Daniel Szafir', 'Mingyu Ding']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'benchmarking', 'defense/mitigation', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17659</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection</title><link>https://arxiv.org/abs/2602.17260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies on pretrained video embeddings via factorized windowed attention for AI-generated video detection.&lt;/li&gt;&lt;li&gt;Introduces EA-Video, a 130K-video benchmark combining new and curated samples from multiple commercial and open-source generators, including unseen-generator splits for rigorous cross-distribution evaluation.&lt;/li&gt;&lt;li&gt;Reports strong results (0.97–0.99 accuracy) and claims substantial improvements over prior state-of-the-art (typically 0.8–0.9), demonstrating robustness and generalization to unseen distributions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hung Mai', 'Loi Dinh', 'Duc Hai Nguyen', 'Dat Do', 'Luong Doan', 'Khanh Nguyen Quoc', 'Huan Vu', 'Phong Ho', 'Naeem Ul Islam', 'Tuan Do']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'synthetic media detection', 'video forensics', 'dataset/benchmark', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17260</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning</title><link>https://arxiv.org/abs/2602.17168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadCLIP++, a backdoor attack framework targeting multimodal contrastive learning (e.g., CLIP) that focuses on two challenges: stealthiness and persistence under detection and fine-tuning.&lt;/li&gt;&lt;li&gt;Stealthiness techniques: semantic-fusion QR micro-trigger placed near task-relevant regions and target-aligned subset selection to amplify signal at low poisoning rates while preserving clean-data statistics.&lt;/li&gt;&lt;li&gt;Persistence techniques: stabilize trigger embeddings via radius shrinkage and centroid alignment and stabilize parameters via curvature control and elastic weight consolidation to keep solutions in low-curvature wide basins resistant to fine-tuning.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis on gradient alignment within a trust region and extensive empirical results (e.g., 0.3% poisoning → 99.99% ASR, robustness against 19 defenses, 65.03% physical attack success).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Liang', 'Yongcheng Jing', 'Yingjie Wang', 'Jiaxing Huang', 'Ee-chien Chang', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'multimodal contrastive learning', 'CLIP', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17168</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer</title><link>https://arxiv.org/abs/2511.17673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Structured Cognitive Loop (R-CCAM) that modularizes agent cognition into Retrieval, Cognition, Control, Action, and Memory phases to separate reasoning and execution.&lt;/li&gt;&lt;li&gt;Introduces Soft Symbolic Control: a governance layer that enforces symbolic constraints over probabilistic LLM outputs to restore explainability, controllability, and policy compliance.&lt;/li&gt;&lt;li&gt;Empirical validation showing zero policy violations, elimination of redundant tool calls, and complete decision traceability; includes open-source implementation and a GPT-4o-powered demo.&lt;/li&gt;&lt;li&gt;Presents theoretical placement within hybrid intelligence, contrasts with prompt- and memory-centric approaches, and derives design principles for trustworthy agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myung Ho Kim']&lt;/li&gt;&lt;li&gt;Tags: ['governance', 'guardrails', 'agent-safety', 'neuro-symbolic-control', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17673</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents</title><link>https://arxiv.org/abs/2602.16346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STING, an automated red-teaming framework that constructs multi-turn, stepwise illicit plans and adaptively probes tool-using LLM agents to evaluate misuse.&lt;/li&gt;&lt;li&gt;Proposes an analysis framework modeling multi-turn red-teaming as a time-to-first-jailbreak random variable and introduces metrics like Restricted Mean Jailbreak Discovery and hazard-ratio attribution by attack language.&lt;/li&gt;&lt;li&gt;Shows STING outperforms single-turn and chat-oriented baselines on illicit-task completion across AgentHarm scenarios and presents multilingual evaluations revealing non-monotonic success across languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nivya Talokar', 'Ayush K Tarun', 'Murari Mandal', 'Maksym Andriushchenko', 'Antoine Bosselut']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'agent_misuse', 'security_benchmarking', 'multilingual_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16346</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</title><link>https://arxiv.org/abs/2602.17645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transfer-based black-box adversarial attacks on large vision-language models (LVLMs) and diagnoses high-variance, unstable gradients caused by ViT translation sensitivity and source/target crop asymmetry.&lt;/li&gt;&lt;li&gt;Proposes methodological fixes—Multi-Crop Alignment (MCA), Auxiliary Target Alignment (ATA), Patch Momentum and a patch-size ensemble (PE+)—to denoise gradients and strengthen transferable directions.&lt;/li&gt;&lt;li&gt;Implements these modules as M-Attack-V2, demonstrating large improvements in black-box attack success rates on frontier LVLMs (e.g., Claude-4.0, Gemini-2.5-Pro, GPT-5) with code/data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohan Zhao', 'Zhaoyi Li', 'Yaxin Luo', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'vision-language models', 'transfer-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17645</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing</title><link>https://arxiv.org/abs/2602.17413</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAVE, an LLM 'spokesperson' that answers questions over private documents while enforcing machine-readable usage policies instead of releasing raw assets.&lt;/li&gt;&lt;li&gt;Formalizes policy-violating information disclosure using usage control and information flow concepts, and introduces 'virtual redaction' to suppress sensitive information at query time.&lt;/li&gt;&lt;li&gt;Describes an architecture integrating with Eclipse Dataspace Components and ODRL-style policies and sketches a provider-side prototype (no full enforcement pipeline implemented).&lt;/li&gt;&lt;li&gt;Outlines an evaluation methodology for assessing security, utility, and performance under benign and adversarial querying as future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ren\\'e Brinkhege", 'Prahlad Menon']&lt;/li&gt;&lt;li&gt;Tags: ['policy enforcement', 'virtual redaction', 'data leakage prevention', 'LLM guardrails', 'usage control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17413</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages</title><link>https://arxiv.org/abs/2602.16832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IndicJR (IJR), a judge-free benchmark evaluating jailbreak robustness across 12 Indic and South Asian languages with 45,216 prompts in contract-bound (JSON) and free (naturalistic) tracks.&lt;/li&gt;&lt;li&gt;Finds that contract formats inflate refusal rates but do not prevent jailbreaks (high jailbreak success rates in JSON and complete jailbreaks in Free); English-to-Indic attacks transfer strongly, and format wrappers often outperform instruction wrappers.&lt;/li&gt;&lt;li&gt;Shows orthography effects: romanized/mixed inputs reduce jailbreak success under JSON, correlating with romanization prevalence and tokenization, and validates findings with human audits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyaranjan Pattnayak', 'Sanchari Chowdhuri']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial robustness', 'multilingual safety', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16832</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning</title><link>https://arxiv.org/abs/2602.17546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adaptive regularization during fine-tuning that constrains high-risk updates to remain close to a safe reference policy while allowing low-risk updates to proceed normally.&lt;/li&gt;&lt;li&gt;Introduces two risk-estimation methods: a judge-based Safety Critic that scores training batches and an activation-based lightweight classifier predicting harmful intent from intermediate activations.&lt;/li&gt;&lt;li&gt;Empirically demonstrates harmful intent is predictable from activations and that the methods reduce attack success rates across model families and attack scenarios while preserving downstream performance and incurring no inference-time cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jyotin Goel', 'Souvik Maji', 'Pratik Mazumder']&lt;/li&gt;&lt;li&gt;Tags: ['safety defenses', 'fine-tuning robustness', 'adversarial updates', 'activation-based detection', 'safety critic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17546</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents</title><link>https://arxiv.org/abs/2602.16346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STING, an automated red-teaming framework that constructs multi-turn, stepwise illicit plans and adaptively probes tool-using LLM agents to evaluate misuse.&lt;/li&gt;&lt;li&gt;Proposes an analysis framework modeling multi-turn red-teaming as a time-to-first-jailbreak random variable and introduces metrics like Restricted Mean Jailbreak Discovery and hazard-ratio attribution by attack language.&lt;/li&gt;&lt;li&gt;Shows STING outperforms single-turn and chat-oriented baselines on illicit-task completion across AgentHarm scenarios and presents multilingual evaluations revealing non-monotonic success across languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nivya Talokar', 'Ayush K Tarun', 'Murari Mandal', 'Maksym Andriushchenko', 'Antoine Bosselut']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'agent_misuse', 'security_benchmarking', 'multilingual_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16346</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize to others.&lt;/li&gt;&lt;li&gt;Finds that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical results showing SpikeScore outperforms baselines for cross-domain hallucination detection across multiple LLMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'LLM-safety', 'cross-domain-generalization', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LLM Fingerprinting via Semantically Conditioned Watermarks</title><link>https://arxiv.org/abs/2505.16723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes semantically conditioned watermarks for LLM fingerprinting that use a broad semantic domain (e.g., French) instead of fixed query keys.&lt;/li&gt;&lt;li&gt;Replaces brittle, atypical memorized keys with a statistical watermark signal diffused throughout each response to improve stealth and persistence.&lt;/li&gt;&lt;li&gt;Model is trained to watermark only responses to prompts in the predetermined semantic domain, enabling ownership verification via domain-specific queries.&lt;/li&gt;&lt;li&gt;Experimental evaluation reports the watermark is stealthy and robust to common deployment changes such as finetuning and quantization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model fingerprinting', 'ownership verification', 'robustness', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16723</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</title><link>https://arxiv.org/abs/2504.21730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cert-SSB, a certified backdoor defense that uses sample-specific randomized smoothing by optimizing per-sample noise magnitudes via stochastic gradient ascent.&lt;/li&gt;&lt;li&gt;Retrains multiple smoothed models on poisoned training sets with the optimized noises and aggregates their predictions; introduces a storage-update-based certification method to certify cases where noise varies per sample.&lt;/li&gt;&lt;li&gt;Demonstrates improved certification performance over existing randomized-smoothing defenses on multiple benchmark datasets and provides code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Yingjia Wang', 'Xing Liu', 'Sixing Wu', 'Jianbin Li', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'certified robustness', 'randomized smoothing', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21730</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Online Robust Reinforcement Learning with General Function Approximation</title><link>https://arxiv.org/abs/2512.18957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a fully online distributionally robust reinforcement learning (DR-RL) algorithm with general function approximation that learns robust policies solely through interaction without pre-collected data.&lt;/li&gt;&lt;li&gt;Introduces a dual-driven fitted robust Bellman procedure that jointly estimates the value function and the worst-case backup operator over phi-divergence uncertainty sets.&lt;/li&gt;&lt;li&gt;Provides sublinear regret guarantees characterized by a new complexity measure, the robust Bellman-Eluder dimension, with bounds that do not scale with state/action space sizes and that specialize to tight rates in structured problem classes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debamita Ghosh', 'George K. Atia', 'Yue Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distributional robustness', 'reinforcement learning', 'online learning', 'theory/regret-bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18957</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Watermarking Diffusion Language Models</title><link>https://arxiv.org/abs/2509.24368</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first watermarking method specifically designed for diffusion language models (DLMs), which generate tokens in arbitrary order rather than autoregressively.&lt;/li&gt;&lt;li&gt;Key techniques: apply the watermark in expectation over unknown context tokens, and bias token selection to strengthen watermark signal when those tokens later serve as context.&lt;/li&gt;&lt;li&gt;Maintains compatibility with existing watermark detectors and demonstrates &gt;99% true positive detection with minimal impact on generation quality and robustness comparable to ARLM watermarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model provenance', 'defense', 'diffusion language models', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24368</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Defining and Evaluating Physical Safety for Large Language Models</title><link>https://arxiv.org/abs/2411.02317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a taxonomy of physical safety risks for LLM-controlled drones: human-targeted, object-targeted, infrastructure attacks, and regulatory violations.&lt;/li&gt;&lt;li&gt;Introduces a comprehensive drone-control benchmark to evaluate LLM behavior on physical safety scenarios and measures trade-offs between utility (e.g., code generation) and safety.&lt;/li&gt;&lt;li&gt;Empirically evaluates mainstream LLMs, showing larger models better refuse dangerous commands, prompt engineering (ICL, CoT) can help but still misses unintentional attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yung-Chen Tang', 'Pin-Yu Chen', 'Tsung-Yi Ho']&lt;/li&gt;&lt;li&gt;Tags: ['physical-safety', 'red-teaming', 'benchmark', 'LLM-safety', 'robotics-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.02317</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning</title><link>https://arxiv.org/abs/2602.17546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adaptive regularization during fine-tuning that constrains high-risk updates to remain close to a safe reference policy while allowing low-risk updates to proceed normally.&lt;/li&gt;&lt;li&gt;Introduces two risk-estimation methods: a judge-based Safety Critic that scores training batches and an activation-based lightweight classifier predicting harmful intent from intermediate activations.&lt;/li&gt;&lt;li&gt;Empirically demonstrates harmful intent is predictable from activations and that the methods reduce attack success rates across model families and attack scenarios while preserving downstream performance and incurring no inference-time cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jyotin Goel', 'Souvik Maji', 'Pratik Mazumder']&lt;/li&gt;&lt;li&gt;Tags: ['safety defenses', 'fine-tuning robustness', 'adversarial updates', 'activation-based detection', 'safety critic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17546</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting</title><link>https://arxiv.org/abs/2602.17234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a claim-level framework that decomposes model rationales into atomic claims and assesses temporal verifiability to detect post-cutoff knowledge leakage.&lt;/li&gt;&lt;li&gt;Proposes Shapley-DCLR, a Shapley-value-weighted metric that quantifies the fraction of decision-driving reasoning attributable to leaked (post-cutoff) information.&lt;/li&gt;&lt;li&gt;Presents TimeSPEC, an iterative generation + claim-verification/regeneration pipeline that enforces that supporting claims are traceable to pre-cutoff sources to reduce temporal contamination.&lt;/li&gt;&lt;li&gt;Evaluates on 350 instances across Supreme Court case prediction, NBA salary estimation, and stock return ranking, showing substantial leakage in baselines and that TimeSPEC lowers Shapley-DCLR while maintaining performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Zhang', 'Ryan Chen', 'Bradly C. Stadie']&lt;/li&gt;&lt;li&gt;Tags: ['temporal data leakage', 'model auditing', 'defense', 'interpretability', 'backtesting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17234</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs</title><link>https://arxiv.org/abs/2602.17223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Untrusted third-party LLM inference providers may substitute cheaper/weaker models, so users lack guarantees about the computation performed.&lt;/li&gt;&lt;li&gt;Key insight: Privacy-preserving inference methods can be leveraged to construct cheap, verifiable inference protocols with only a few extra tokens of computation and minimal downstream impact.&lt;/li&gt;&lt;li&gt;Contributions: Two protocols that use private LLM inference to provide verification guarantees, improving runtime and practicality compared to heavy cryptographic approaches (e.g., ZKPs) and highlighting connections between privacy and verifiability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arka Pal', 'Louai Zahran', 'William Gvozdjak', 'Akilesh Potti', 'Micah Goldblum']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable inference', 'privacy-preserving inference', 'model integrity', 'defense/protocols']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17223</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Automating Agent Hijacking via Structural Template Injection</title><link>https://arxiv.org/abs/2602.16958</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Phantom, an automated agent-hijacking framework that performs Structured Template Injection to manipulate LLM agents by exploiting chat template tokens and inducing role confusion.&lt;/li&gt;&lt;li&gt;Presents a template search pipeline: multi-level template augmentation, a Template Autoencoder (TAE) to embed templates into a continuous latent space, and Bayesian optimization to find high-potency adversarial templates.&lt;/li&gt;&lt;li&gt;Evaluates on Qwen, GPT, and Gemini, showing higher attack success rates and query efficiency than baselines, and reports discovery of 70+ real-world vendor-confirmed vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinhao Deng', 'Jiaqing Wu', 'Miao Chen', 'Yue Xiao', 'Ke Xu', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['agent hijacking', 'prompt injection', 'automated attack/search', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16958</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs</title><link>https://arxiv.org/abs/2602.16935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeepContext, a stateful monitoring framework that uses an RNN over turn-level embeddings to track the temporal evolution of user intent across multi-turn dialogues.&lt;/li&gt;&lt;li&gt;Targets multi-turn jailbreaks and adversarial intent drift (e.g., Crescendo, ActorAttack) that evade stateless guardrails by gradually accumulating malicious intent.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains in multi-turn jailbreak detection (F1 = 0.84) versus cloud-provider guardrails and open-weight baselines (Llama-Prompt-Guard-2, Granite-Guardian at 0.67).&lt;/li&gt;&lt;li&gt;Demonstrates real-time feasibility with sub-20ms inference overhead on a T4 GPU, arguing stateful lightweight models can outperform massive stateless models for safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Albrethsen', 'Yash Datta', 'Kunal Kumar', 'Sharath Rajasekar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak-detection', 'stateful-monitoring', 'adversarial-attacks', 'defense', 'real-time-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16935</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NeST: Neuron Selective Tuning for LLM Safety</title><link>https://arxiv.org/abs/2602.16835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NeST, a lightweight, structure-aware safety alignment method that selectively adapts clusters of safety-relevant neurons while keeping most model weights frozen.&lt;/li&gt;&lt;li&gt;Clusters functionally coherent safety neurons and enforces shared updates within clusters to produce targeted, stable refusal behavior without inference-time overhead.&lt;/li&gt;&lt;li&gt;Benchmarks NeST against full fine-tuning, LoRA, and circuit breakers across 10 open-weight LLMs, reducing average attack success rate from 44.5% to 4.36% while updating only ~0.44M parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sasha Behrouzi', 'Lichao Wu', 'Mohamadreza Rostami', 'Ahmad-Reza Sadeghi']&lt;/li&gt;&lt;li&gt;Tags: ['safety-defenses', 'jailbreak-mitigation', 'neuron-editing', 'parameter-efficient-alignment', 'LLM-safety-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16835</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</title><link>https://arxiv.org/abs/2602.17645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transfer-based black-box adversarial attacks on large vision-language models (LVLMs) and diagnoses high-variance, unstable gradients caused by ViT translation sensitivity and source/target crop asymmetry.&lt;/li&gt;&lt;li&gt;Proposes methodological fixes—Multi-Crop Alignment (MCA), Auxiliary Target Alignment (ATA), Patch Momentum and a patch-size ensemble (PE+)—to denoise gradients and strengthen transferable directions.&lt;/li&gt;&lt;li&gt;Implements these modules as M-Attack-V2, demonstrating large improvements in black-box attack success rates on frontier LVLMs (e.g., Claude-4.0, Gemini-2.5-Pro, GPT-5) with code/data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohan Zhao', 'Zhaoyi Li', 'Yaxin Luo', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'vision-language models', 'transfer-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17645</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning</title><link>https://arxiv.org/abs/2602.17614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that intermediate representations ('smashed data') in U-shaped federated split learning are vulnerable to data-reconstruction attacks that can reveal private client data.&lt;/li&gt;&lt;li&gt;Proposes KD-UFSL, a defense combining microaggregation (k-anonymity style grouping) with differential privacy to protect intermediate representations sent to the server.&lt;/li&gt;&lt;li&gt;Empirical results show KD-UFSL increases reconstruction error (up to ~50% MSE) and reduces structural similarity (up to ~40% SSIM) across four benchmark datasets while maintaining utility of the global model.&lt;/li&gt;&lt;li&gt;Targets the privacy–utility tradeoff for large-scale decentralized learning by protecting intermediate representations without requiring clients to fully offload data/labels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Obaidullah Zaland', 'Sajib Mistry', 'Monowar Bhuyan']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'split learning', 'data-reconstruction attack', 'differential privacy', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17614</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Anytime-Valid Statistical Watermarking</title><link>https://arxiv.org/abs/2602.17608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Anchored E-Watermarking: an e-value-based statistical watermarking framework enabling anytime-valid detection of machine-generated text.&lt;/li&gt;&lt;li&gt;Develops a test supermartingale and derives the optimal e-value and sampling distribution to maximize worst-case log-growth rate and minimize expected stopping time.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical evaluations showing 13-15% reduction in average token budget for detection compared to state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baihe Huang', 'Eric Xu', 'Kannan Ramchandran', 'Jiantao Jiao', 'Michael I. Jordan']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM detection', 'statistical inference', 'e-values', 'anytime-valid testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17608</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation</title><link>https://arxiv.org/abs/2602.17071</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvSynGNN, an architecture for robust node-level representation learning under structural noise and heterophily in graphs.&lt;/li&gt;&lt;li&gt;Combines multi-resolution structural synthesis and contrastive objectives to create geometry-aware initializations and a transformer backbone that adapts attention using learned topological signals.&lt;/li&gt;&lt;li&gt;Introduces an adversarial propagation engine (generator that proposes connectivity alterations and a discriminator enforcing global coherence) to synthesize challenging structural perturbations and improve robustness.&lt;/li&gt;&lt;li&gt;Adds a residual label-refinement scheme guided by per-node confidence for iterative stability; reports empirical gains in accuracy and deployment protocols for large-scale settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rong Fu', 'Muge Qi', 'Chunlei Meng', 'Shuo Yin', 'Kun Liu', 'Zhaolu Kang', 'Simon Fong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'graph neural networks', 'adversarial synthesis/training', 'structural perturbations', 'defense/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17071</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Discovering Universal Activation Directions for PII Leakage in Language Models</title><link>https://arxiv.org/abs/2602.16980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UniLeak, a method to identify universal activation directions in a model's residual stream that, when linearly added at inference, increase the probability of generating PII.&lt;/li&gt;&lt;li&gt;Recovers such directions without access to training data or ground-truth PII, relying solely on model self-generated text.&lt;/li&gt;&lt;li&gt;Demonstrates these directions generalize across contexts and models and amplify PII leakage more effectively than prompt-based extraction, with little degradation in generation quality.&lt;/li&gt;&lt;li&gt;Frames the discovery as both a new privacy attack vector (risk amplification) and a potential insight for mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leo Marchyok', 'Zachary Coalson', 'Sungho Keum', 'Sooel Son', 'Sanghyun Hong']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'privacy attack', 'mechanistic interpretability', 'latent activation directions', 'model privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16980</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fail-Closed Alignment for Large Language Models</title><link>https://arxiv.org/abs/2602.16977</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a fail-open vulnerability in current LLM refusal mechanisms: suppression of a single dominant latent refusal feature via prompt jailbreaks can collapse alignment.&lt;/li&gt;&lt;li&gt;Proposes the fail-closed alignment principle—design refusal mechanisms with redundant, independent causal pathways so safety holds under partial failures.&lt;/li&gt;&lt;li&gt;Introduces a progressive alignment framework that iteratively finds and ablates learned refusal directions, forcing the model to build multiple independent refusal subspaces.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved robustness against four jailbreak attacks while preserving generation quality and provides mechanistic analyses showing causally independent refusal directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Coalson', 'Beth Sohler', 'Aiden Gabriel', 'Sanghyun Hong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'defense', 'adversarial robustness', 'mechanistic interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16977</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming</title><link>https://arxiv.org/abs/2602.16944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a verification framework that provides sound and complete guarantees for training-time data-poisoning attacks.&lt;/li&gt;&lt;li&gt;Formulates adversarial data manipulation, model training (gradient-based dynamics), and test-time evaluation as a single mixed-integer quadratic (MIQCP) program.&lt;/li&gt;&lt;li&gt;Computes global optima to either produce worst-case poisoning attacks or certify bounds on the effectiveness of all possible attacks for a given training pipeline.&lt;/li&gt;&lt;li&gt;Validates the approach experimentally on small models, demonstrating exact certification of training-time robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philip Sosnin', 'Jodie Knapp', 'Fraser Kennedy', 'Josh Collyer', 'Calvin Tsay']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'attack certification', 'robustness verification', 'mixed-integer programming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16944</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VERA-MH Concept Paper</title><link>https://arxiv.org/abs/2510.15297</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERA-MH, an automated evaluation framework to assess safety of AI chatbots in mental-health contexts, initially focused on suicide risk.&lt;/li&gt;&lt;li&gt;Uses two ancillary AI agents: a user-agent that role-plays patient personas with predefined risk levels and a judge-agent that scores conversations against a clinician-informed rubric.&lt;/li&gt;&lt;li&gt;Aggregates scores across simulated conversations to produce an overall safety evaluation; undergoing clinical validation and preliminary testing on models (e.g., GPT-5, Claude).&lt;/li&gt;&lt;li&gt;Aims to provide an automated benchmarking/evaluation tool to identify safety failures and inform design iteration and actionable scoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luca Belli', 'Kate Bentley', 'Will Alexander', 'Emily Ward', 'Matt Hawrilenko', 'Kelly Johnston', 'Mill Brown', 'Adam Chekroud']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'automated red teaming', 'chatbot safety', 'mental-health', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15297</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Watermarking Diffusion Language Models</title><link>https://arxiv.org/abs/2509.24368</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first watermark scheme specifically designed for diffusion language models (DLMs), which generate tokens in arbitrary order rather than autoregressively.&lt;/li&gt;&lt;li&gt;Key technical ideas: apply the watermark in expectation over incomplete contexts and promote tokens that increase watermark strength when used as context, while keeping the detector unchanged.&lt;/li&gt;&lt;li&gt;Empirical results show &gt;99% true positive rate, minimal impact on generation quality, and robustness comparable to existing ARLM watermarking methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model provenance', 'defense', 'detection', 'diffusion language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24368</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Discrete optimal transport is a strong audio adversarial attack</title><link>https://arxiv.org/abs/2509.14959</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces kDOT-VC: a discrete optimal transport–based voice conversion method that aligns frame-level WavLM embeddings to a bona fide pool using entropic OT and top-k barycentric projection, then decodes with a neural vocoder.&lt;/li&gt;&lt;li&gt;Demonstrates kDOT-VC as an effective black-box adversarial attack against audio anti-spoofing countermeasures, operating as a post-processing distribution-alignment step.&lt;/li&gt;&lt;li&gt;Compares to kNN-VC, SinkVC, and Gaussian OT (MKL) and reports stronger domain adaptation and attack effectiveness; ablation studies show distribution-level alignment is robust and powerful.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anton Selitskiy', 'Akib Shahriyar', 'Jishnuraj Prakasan']&lt;/li&gt;&lt;li&gt;Tags: ['audio-adversarial-attacks', 'voice-conversion', 'optimal-transport', 'anti-spoofing', 'black-box-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14959</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</title><link>https://arxiv.org/abs/2504.21730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cert-SSB, a certified backdoor defense that uses sample-specific randomized smoothing by optimizing per-sample noise magnitudes via stochastic gradient ascent.&lt;/li&gt;&lt;li&gt;Retrains multiple smoothed models on poisoned training sets with the optimized noises and aggregates their predictions; introduces a storage-update-based certification method to certify cases where noise varies per sample.&lt;/li&gt;&lt;li&gt;Demonstrates improved certification performance over existing randomized-smoothing defenses on multiple benchmark datasets and provides code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Yingjia Wang', 'Xing Liu', 'Sixing Wu', 'Jianbin Li', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'certified robustness', 'randomized smoothing', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21730</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Defining and Evaluating Physical Safety for Large Language Models</title><link>https://arxiv.org/abs/2411.02317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a taxonomy of physical safety risks for LLM-controlled drones: human-targeted, object-targeted, infrastructure attacks, and regulatory violations.&lt;/li&gt;&lt;li&gt;Introduces a comprehensive drone-control benchmark to evaluate LLM behavior on physical safety scenarios and measures trade-offs between utility (e.g., code generation) and safety.&lt;/li&gt;&lt;li&gt;Empirically evaluates mainstream LLMs, showing larger models better refuse dangerous commands, prompt engineering (ICL, CoT) can help but still misses unintentional attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yung-Chen Tang', 'Pin-Yu Chen', 'Tsung-Yi Ho']&lt;/li&gt;&lt;li&gt;Tags: ['physical-safety', 'red-teaming', 'benchmark', 'LLM-safety', 'robotics-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.02317</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization</title><link>https://arxiv.org/abs/2602.06838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive differentially private federated learning framework to improve model efficiency under device heterogeneity and privacy constraints.&lt;/li&gt;&lt;li&gt;Client-side: introduces a lightweight local compressed module to regularize intermediate representations and reduce gradient variability to mitigate noise amplification from DP mechanisms.&lt;/li&gt;&lt;li&gt;Server-side: uses adaptive gradient clipping based on historical update statistics and a constraint-aware aggregation mechanism to suppress unreliable or noise-dominated client updates, improving stability and accuracy on CIFAR-10 and SVHN.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Wang', 'Hui Ma', 'Fei Xing', 'Ming Yan']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'federated learning', 'robust aggregation', 'adaptive gradient clipping']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06838</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize to others.&lt;/li&gt;&lt;li&gt;Finds that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical results showing SpikeScore outperforms baselines for cross-domain hallucination detection across multiple LLMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'LLM-safety', 'cross-domain-generalization', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer</title><link>https://arxiv.org/abs/2511.17673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Structured Cognitive Loop (R-CCAM) that modularizes agent cognition into Retrieval, Cognition, Control, Action, and Memory phases to separate reasoning and execution.&lt;/li&gt;&lt;li&gt;Introduces Soft Symbolic Control: a governance layer that enforces symbolic constraints over probabilistic LLM outputs to restore explainability, controllability, and policy compliance.&lt;/li&gt;&lt;li&gt;Empirical validation showing zero policy violations, elimination of redundant tool calls, and complete decision traceability; includes open-source implementation and a GPT-4o-powered demo.&lt;/li&gt;&lt;li&gt;Presents theoretical placement within hybrid intelligence, contrasts with prompt- and memory-centric approaches, and derives design principles for trustworthy agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myung Ho Kim']&lt;/li&gt;&lt;li&gt;Tags: ['governance', 'guardrails', 'agent-safety', 'neuro-symbolic-control', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17673</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</title><link>https://arxiv.org/abs/2602.17645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transfer-based black-box adversarial attacks on large vision-language models (LVLMs) and diagnoses high-variance, unstable gradients caused by ViT translation sensitivity and source/target crop asymmetry.&lt;/li&gt;&lt;li&gt;Proposes methodological fixes—Multi-Crop Alignment (MCA), Auxiliary Target Alignment (ATA), Patch Momentum and a patch-size ensemble (PE+)—to denoise gradients and strengthen transferable directions.&lt;/li&gt;&lt;li&gt;Implements these modules as M-Attack-V2, demonstrating large improvements in black-box attack success rates on frontier LVLMs (e.g., Claude-4.0, Gemini-2.5-Pro, GPT-5) with code/data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohan Zhao', 'Zhaoyi Li', 'Yaxin Luo', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'vision-language models', 'transfer-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17645</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation</title><link>https://arxiv.org/abs/2602.17071</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvSynGNN, an architecture for robust node-level representation learning under structural noise and heterophily in graphs.&lt;/li&gt;&lt;li&gt;Combines multi-resolution structural synthesis and contrastive objectives to create geometry-aware initializations and a transformer backbone that adapts attention using learned topological signals.&lt;/li&gt;&lt;li&gt;Introduces an adversarial propagation engine (generator that proposes connectivity alterations and a discriminator enforcing global coherence) to synthesize challenging structural perturbations and improve robustness.&lt;/li&gt;&lt;li&gt;Adds a residual label-refinement scheme guided by per-node confidence for iterative stability; reports empirical gains in accuracy and deployment protocols for large-scale settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rong Fu', 'Muge Qi', 'Chunlei Meng', 'Shuo Yin', 'Kun Liu', 'Zhaolu Kang', 'Simon Fong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'graph neural networks', 'adversarial synthesis/training', 'structural perturbations', 'defense/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17071</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Wink: Recovering from Misbehaviors in Coding Agents</title><link>https://arxiv.org/abs/2602.17037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes misbehaviors of autonomous coding agents from production traffic and defines a taxonomy: Specification Drift, Reasoning Problems, and Tool Call Failures (occur in ~30% of trajectories).&lt;/li&gt;&lt;li&gt;Proposes Wink, a lightweight asynchronous self-intervention system that observes agent trajectories and provides targeted course-correction guidance to recover from misbehaviors.&lt;/li&gt;&lt;li&gt;Evaluates Wink on &gt;10,000 real-world agent trajectories (resolving 90% of single-intervention failures) and reports production A/B test improvements (reduced tool call failures, tokens/session, and human interventions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Nanda', 'Chandra Maddila', 'Smriti Jha', 'Euna Mehnaz Khan', 'Matteo Paltenghi', 'Satish Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['agent robustness', 'failure recovery', 'safety mechanisms', 'production deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17037</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting</title><link>https://arxiv.org/abs/2602.17234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a claim-level framework that decomposes model rationales into atomic claims and assesses temporal verifiability to detect post-cutoff knowledge leakage.&lt;/li&gt;&lt;li&gt;Proposes Shapley-DCLR, a Shapley-value-weighted metric that quantifies the fraction of decision-driving reasoning attributable to leaked (post-cutoff) information.&lt;/li&gt;&lt;li&gt;Presents TimeSPEC, an iterative generation + claim-verification/regeneration pipeline that enforces that supporting claims are traceable to pre-cutoff sources to reduce temporal contamination.&lt;/li&gt;&lt;li&gt;Evaluates on 350 instances across Supreme Court case prediction, NBA salary estimation, and stock return ranking, showing substantial leakage in baselines and that TimeSPEC lowers Shapley-DCLR while maintaining performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Zhang', 'Ryan Chen', 'Bradly C. Stadie']&lt;/li&gt;&lt;li&gt;Tags: ['temporal data leakage', 'model auditing', 'defense', 'interpretability', 'backtesting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.17234</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning</title><link>https://arxiv.org/abs/2602.16984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes latent context-conditioned policies that behave safely on evaluation distribution but activate unsafe behaviors via rare, unobserved triggers under deployment.&lt;/li&gt;&lt;li&gt;Proves information-theoretic lower bounds for black-box evaluators: passive i.i.d. testing has an expected absolute error &gt;= ~0.208 * delta * L; fully adaptive querying still suffers worst-case error &gt;= delta * L / 16 and requires Theta(1/epsilon) queries to detect triggers.&lt;/li&gt;&lt;li&gt;Shows computational separation: under trapdoor one-way function assumptions, polynomial-time black-box evaluators without privileged information cannot distinguish unsafe deployments; contrasts with white-box probing which admits sample complexity O(1/(gamma^2 * epsilon_R^2)) and bias correction under probe error.&lt;/li&gt;&lt;li&gt;Derives implications that black-box testing can be fundamentally underdetermined and that architectural constraints, training-time guarantees, interpretability, and deployment monitoring are necessary for worst-case safety assurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishal Srivastava']&lt;/li&gt;&lt;li&gt;Tags: ['black-box-evaluation', 'safety-evaluation', 'adversarial-trigger', 'computational-hardness', 'deployment-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16984</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Automating Agent Hijacking via Structural Template Injection</title><link>https://arxiv.org/abs/2602.16958</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Phantom, an automated agent-hijacking framework that performs Structured Template Injection to manipulate LLM agents by exploiting chat template tokens and inducing role confusion.&lt;/li&gt;&lt;li&gt;Presents a template search pipeline: multi-level template augmentation, a Template Autoencoder (TAE) to embed templates into a continuous latent space, and Bayesian optimization to find high-potency adversarial templates.&lt;/li&gt;&lt;li&gt;Evaluates on Qwen, GPT, and Gemini, showing higher attack success rates and query efficiency than baselines, and reports discovery of 70+ real-world vendor-confirmed vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinhao Deng', 'Jiaqing Wu', 'Miao Chen', 'Yue Xiao', 'Ke Xu', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['agent hijacking', 'prompt injection', 'automated attack/search', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16958</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents</title><link>https://arxiv.org/abs/2602.16943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAP, a benchmark to measure divergence between text-level refusal and tool-call behavior in LLM agents across six models, six regulated domains, multiple jailbreak scenarios, and system prompt conditions (17,420 datapoints).&lt;/li&gt;&lt;li&gt;Finds systematic instances where models refuse harmful text but still execute forbidden tool calls (the GAP metric), including 219 persistent cases under safety-reinforced prompts.&lt;/li&gt;&lt;li&gt;Demonstrates strong influence of system prompt wording on tool-call safety and shows runtime governance contracts reduce information leakage but do not deter forbidden tool-call attempts.&lt;/li&gt;&lt;li&gt;Concludes that text-only safety evaluations are insufficient and that dedicated measurement and mitigation for tool-call safety in agents are required.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnold Cartagena', 'Ariane Teixeira']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'tool-call security', 'jailbreaking', 'benchmark', 'runtime governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16943</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs</title><link>https://arxiv.org/abs/2602.16935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeepContext, a stateful monitoring framework that uses an RNN over turn-level embeddings to track the temporal evolution of user intent across multi-turn dialogues.&lt;/li&gt;&lt;li&gt;Targets multi-turn jailbreaks and adversarial intent drift (e.g., Crescendo, ActorAttack) that evade stateless guardrails by gradually accumulating malicious intent.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains in multi-turn jailbreak detection (F1 = 0.84) versus cloud-provider guardrails and open-weight baselines (Llama-Prompt-Guard-2, Granite-Guardian at 0.67).&lt;/li&gt;&lt;li&gt;Demonstrates real-time feasibility with sub-20ms inference overhead on a T4 GPU, arguing stateful lightweight models can outperform massive stateless models for safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Albrethsen', 'Yash Datta', 'Kunal Kumar', 'Sharath Rajasekar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak-detection', 'stateful-monitoring', 'adversarial-attacks', 'defense', 'real-time-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16935</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages</title><link>https://arxiv.org/abs/2602.16832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IndicJR (IJR), a judge-free benchmark evaluating jailbreak robustness across 12 Indic and South Asian languages with 45,216 prompts in contract-bound (JSON) and free (naturalistic) tracks.&lt;/li&gt;&lt;li&gt;Finds that contract formats inflate refusal rates but do not prevent jailbreaks (high jailbreak success rates in JSON and complete jailbreaks in Free); English-to-Indic attacks transfer strongly, and format wrappers often outperform instruction wrappers.&lt;/li&gt;&lt;li&gt;Shows orthography effects: romanized/mixed inputs reduce jailbreak success under JSON, correlating with romanization prevalence and tokenization, and validates findings with human audits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyaranjan Pattnayak', 'Sanchari Chowdhuri']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial robustness', 'multilingual safety', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16832</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography</title><link>https://arxiv.org/abs/2602.16812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NeuDiff Agent: a governed, tool-using AI workflow that automates reduction, integration, refinement, and validation for single-crystal neutron crystallography.&lt;/li&gt;&lt;li&gt;Implements security-oriented governance: allowlisted tools, fail-closed verification gates at workflow boundaries, and complete provenance capture for inspection, auditing, and controlled replay.&lt;/li&gt;&lt;li&gt;Evaluates performance end-to-end with LLM backends, measuring time savings, intervention burden, and recovery under gating while producing publication-ready validated CIFs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongcan Xiao (Neutron Scattering Division', 'Oak Ridge National Laboratory', 'Oak Ridge', 'Tennesse USA)', 'Leyi Zhang (Neutron Scattering Division', 'Oak Ridge National Laboratory', 'Oak Ridge', 'Tennesse USA', 'Department of Linguistics', 'University of Illinois Urbana-Champaign', 'Urbana', 'Illinois', 'USA)', 'Guannan Zhang (Computer Science and Mathematics Division', 'Oak Ridge National Laboratory', 'Oak Ridge', 'Tennessee', 'USA)', 'Xiaoping Wang (Neutron Scattering Division', 'Oak Ridge National Laboratory', 'Oak Ridge', 'Tennesse USA)']&lt;/li&gt;&lt;li&gt;Tags: ['guardrails', 'workflow governance', 'provenance &amp; auditing', 'agent safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.16812</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
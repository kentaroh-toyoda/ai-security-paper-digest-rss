<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 24 Dec 2025 00:04:48 +0000</lastBuildDate><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LookAhead Tuning, a data-driven fine-tuning technique that previews partial answer prefixes in training data to reduce perturbations to a model's initial token distributions.&lt;/li&gt;&lt;li&gt;Aims to preserve built-in safety/alignment of LLMs during downstream fine-tuning, mitigating safety degradation.&lt;/li&gt;&lt;li&gt;Reports comprehensive experiments showing maintained safety while retaining robust task performance, positioning the method as lightweight and practical for safe adaptation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fine-tuning', 'LLM safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeContext as Defense: Safe Image Editing in Diffusion Transformers</title><link>https://arxiv.org/abs/2512.16625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeContext, an attention-targeted perturbation method that prevents unauthorized in-context image editing in diffusion transformer (DiT) models by weakening cross-attention pathways.&lt;/li&gt;&lt;li&gt;Finds that context propagation is dominated by early denoising steps and particular transformer blocks, allowing concentrated perturbations that are efficient and preserve visual quality.&lt;/li&gt;&lt;li&gt;Evaluates on Flux Kontext and Step1X-Edit, showing consistent blocking of unwanted edits while maintaining image fidelity; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Shen', 'Mingyue Cui', 'Xingyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['image privacy', 'defense', 'diffusion models', 'adversarial perturbation', 'multimodal attention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16625</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stylized Synthetic Augmentation further improves Corruption Robustness</title><link>https://arxiv.org/abs/2512.15675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training augmentation pipeline that combines synthetic image data with neural style transfer to improve robustness to common corruptions.&lt;/li&gt;&lt;li&gt;Finds that stylized synthetic images, despite worse FID scores, are beneficial for training and complement other augmentations like TrivialAugment.&lt;/li&gt;&lt;li&gt;Performs systematic empirical analysis of augmentation choices and hyperparameters and reports state-of-the-art corruption robustness on CIFAR-10-C, CIFAR-100-C, and TinyImageNet-C.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Siedel', 'Rojan Regmi', 'Abhirami Anand', 'Weijia Shao', 'Silvia Vock', 'Andrey Morozov']&lt;/li&gt;&lt;li&gt;Tags: ['corruption robustness', 'data augmentation', 'neural style transfer', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15675</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title><link>https://arxiv.org/abs/2512.12218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines visual faithfulness for chain-of-thought in reasoning-augmented VLMs, separating perception steps from abstract reasoning to detect visually unfaithful intermediate steps.&lt;/li&gt;&lt;li&gt;Proposes a training- and reference-free, step-level faithfulness metric using off-the-shelf VLM judges and validates it via human meta-evaluation.&lt;/li&gt;&lt;li&gt;Introduces a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps, reducing Unfaithful Perception Rate while keeping final-answer accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rheeya Uppaal', 'Phu Mon Htut', 'Min Bai', 'Nikolaos Pappas', 'Zheng Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'visual-hallucination', 'faithfulness', 'multimodal-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12218</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</title><link>https://arxiv.org/abs/2510.16442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EDVD-LLaMA, a multimodal large language model reasoning framework for explainable deepfake video detection that outputs traceable chain-of-thought explanations alongside binary detection.&lt;/li&gt;&lt;li&gt;Introduces Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global/local cross-frame features and a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) that uses facial feature constraints to reduce hallucination and enable pixel-level localization.&lt;/li&gt;&lt;li&gt;Creates an Explainable Reasoning FF++ dataset (ER-FF++set) with structured annotations for dual supervision of detection and reasoning; reports improved robustness and cross-dataset/cross-forgery performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Sun', 'Chen Cai', 'Huiping Zhuang', 'Kong Aik Lee', 'Lap-Pui Chau', 'Yi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'explainability', 'multimodal LLM', 'robustness', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16442</guid><pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeParts: a New Family of AI-Generated DeepFakes</title><link>https://arxiv.org/abs/2508.21052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FakeParts, a new class of partial (localized/spatial or temporal) deepfakes that blend manipulated regions into otherwise real videos, making detection harder.&lt;/li&gt;&lt;li&gt;Presents FakePartsBench, a large-scale benchmark with ~81K videos (44K FakeParts) and pixel- and frame-level annotations to evaluate detection methods.&lt;/li&gt;&lt;li&gt;Reports user studies and evaluations showing substantial drops in human and state-of-the-art detector performance (up to 26% reduction in human accuracy), highlighting a vulnerability in current detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Liu', 'Firas Gabetni', 'Awais Hussain Sani', 'Xi Wang', 'Soobash Daiboo', 'Gaetan Brison', 'Gianni Franchi', 'Vicky Kalogeiton']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'dataset/benchmark', 'adversarial/partial manipulation', 'robustness/multimedia forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21052</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title><link>https://arxiv.org/abs/2507.00724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model ownership verification for personalized large vision models to detect model stealing of fine-tuned models.&lt;/li&gt;&lt;li&gt;Proposes creating shadow models that preserve common features while disrupting dataset-specific features, then representing dataset-specific features via output differences between victim and shadow models (no modification to victim model).&lt;/li&gt;&lt;li&gt;Trains a meta-classifier to detect whether suspicious models contain the victim's dataset-specific features, and applies hypothesis testing to improve robustness and reduce randomness.&lt;/li&gt;&lt;li&gt;Claims effectiveness across different types of model stealing on benchmark datasets and provides code release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Zhu', 'Yiming Li', 'Haiqin Weng', 'Yan Liu', 'Tianwei Zhang', 'Shu-Tao Xia', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model stealing', 'ownership verification', 'IP protection', 'vision models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00724</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution</title><link>https://arxiv.org/abs/2411.07449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework leveraging entire diffusion model denoising trajectories to determine image provenance (training set member, model-generated, or external).&lt;/li&gt;&lt;li&gt;Demonstrates that temporal dynamics across trajectories yield more robust membership inference and challenges the 'Goldilocks zone' hypothesis that only certain denoising stages are informative.&lt;/li&gt;&lt;li&gt;Exposes limitations of existing membership inference methods under distribution shift and when model-generated data is present, and introduces a first white-box model attribution technique for diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andreas Floros', 'Seyed-Mohsen Moosavi-Dezfooli', 'Pier Luigi Dragotti']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'model attribution', 'privacy', 'diffusion models', 'data provenance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.07449</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness of Vision in Open Foundation Models</title><link>https://arxiv.org/abs/2512.17902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates adversarial robustness of open-weight vision-enabled foundation models (LLaVA-1.5-13B and Llama 3.2 Vision-8B-2) using untargeted PGD attacks on the visual input.&lt;/li&gt;&lt;li&gt;Measures impact on Visual Question Answering (VQA) v2 subset accuracy and compares baseline accuracy drop under increasing perturbation levels between models.&lt;/li&gt;&lt;li&gt;Finds that Llama 3.2 Vision showed a smaller performance degradation under attack than LLaVA despite lower baseline accuracy, highlighting that robustness does not necessarily track standard benchmark performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathon Fox', 'William J Buchanan', 'Pavlos Papadopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'PGD attacks', 'VQA', 'open-weight models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17902</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visually Prompted Benchmarks Are Surprisingly Fragile</title><link>https://arxiv.org/abs/2512.17875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that visually prompted benchmarks for visual-language models (VLMs) are highly fragile: small changes to visual markers (color, size, design) and low-level inference choices (e.g., JPEG compression) can substantially change model performance and leaderboard rankings.&lt;/li&gt;&lt;li&gt;Evaluates nine open- and closed-source VLMs on two visually prompted tasks and shows these setup choices can be exploited to make weaker models appear stronger.&lt;/li&gt;&lt;li&gt;Proposes VPBench, a curated, larger visually prompted benchmark with 16 visual marker variants and releases datasets and analysis tools to mitigate instability and improve evaluation robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiwen Feng', 'Long Lian', 'Lisa Dunlap', 'Jiahao Shu', 'XuDong Wang', 'Renhao Wang', 'Trevor Darrell', 'Alane Suhr', 'Angjoo Kanazawa']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark robustness', 'visual prompting', 'VLM evaluation', 'evaluation reliability', 'sensitivity analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17875</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</title><link>https://arxiv.org/abs/2512.17730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Diff-Gen, a 100k diffusion-generated image benchmark capturing distinct spectral artifacts to improve cross-generator generalization for deepfake detection.&lt;/li&gt;&lt;li&gt;Proposes AdaptPrompt, a parameter-efficient transfer learning method that freezes CLIP, jointly learns task-specific textual prompts and visual adapters, and prunes the final vision transformer block to preserve high-frequency generative artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance across 25 test sets (GANs, diffusion models, commercial tools), strong cross-domain generalization, few-shot detection (as few as 320 images), and closed-set source attribution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Jiang', 'Mohammed Talha Alam', 'Sohail Ahmed Khan', 'Duc-Tien Dang-Nguyen', 'Fakhri Karray']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'vision-language-models', 'dataset-diffusion', 'robustness-generalization', 'parameter-efficient-adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17730</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title><link>https://arxiv.org/abs/2512.17532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Robust-R1, a framework that explicitly models visual degradations via structured reasoning chains, combining supervised fine-tuning, reward-driven alignment for degradation parameter perception, and dynamic reasoning depth scaling.&lt;/li&gt;&lt;li&gt;Introduces an 11K synthetic dataset with realistic degradations across four visual processing stages, annotated with structured chains linking degradation parameters, perceptual influence, pristine semantic reasoning, and conclusions.&lt;/li&gt;&lt;li&gt;Evaluates on multiple benchmarks (R-Bench, MMMB, MMStar, RealWorldQA) and reports state-of-the-art robustness to real-world and multi-intensity adversarial degradations for multimodal LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Tang', 'Jianmin Chen', 'Wei Wei', 'Xiaogang Xu', 'Runtao Liu', 'Xiangyu Wu', 'Qipeng Xie', 'Jiafei Wu', 'Lei Zhang', 'Qifeng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal robustness', 'degradation modeling', 'benchmark/dataset', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17532</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</title><link>https://arxiv.org/abs/2512.17495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GroundingME, a 1,005-example benchmark testing MLLMs across Discriminative, Spatial, Limited (occlusions/tiny objects), and Rejection (ungroundable queries) dimensions.&lt;/li&gt;&lt;li&gt;Evaluates 25 state-of-the-art MLLMs, finding large capability gaps (best 45.1% accuracy) and widespread failure to reject ungroundable queries (most models 0%, hallucinating instead).&lt;/li&gt;&lt;li&gt;Proposes two improvement strategies: test-time scaling (thinking trajectories) and data-mixture training to teach rejection, improving rejection accuracy up to 27.9%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rang Li', 'Lei Li', 'Shuhuai Ren', 'Hao Tian', 'Shuhao Gu', 'Shicheng Li', 'Zihao Yue', 'Yudong Wang', 'Wenhan Ma', 'Zhe Yang', 'Jingyuan Ma', 'Zhifang Sui', 'Fuli Luo']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'visual grounding', 'multimodal-llms', 'safety-evaluation', 'hallucination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17495</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.17350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a pixel-level mapping preprocessing step to disrupt semantic pixel distributions, forcing detectors to rely on fundamental high-frequency generative traces.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in cross-generator generalization on both GAN and diffusion-based image generators.&lt;/li&gt;&lt;li&gt;Provides analyses supporting the claim that breaking semantic shortcuts (rather than adding model-specific cues) yields more robust, generalizable detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenming Zhou', 'Jiaan Wang', 'Yu Li', 'Lei Li', 'Juan Cao', 'Sheng Tang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'generalization', 'robustness', 'image forensics', 'preprocessing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17350</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</title><link>https://arxiv.org/abs/2512.17320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EMMA, a benchmark to evaluate concept erasure in text-to-image models using 12 metrics across five key dimensions (robustness, quality, efficiency, social bias, and coverage).&lt;/li&gt;&lt;li&gt;Tests concept erasure methods on five domains (objects, celebrities, art styles, NSFW, copyright) and stresses challenging conditions like indirect/implicit prompts and visually similar non-target concepts.&lt;/li&gt;&lt;li&gt;Finds existing methods often fail under implicit prompts, struggle to preserve non-target similar concepts, and can amplify gender/ethnicity biases compared to the original models.&lt;/li&gt;&lt;li&gt;Provides a socially aware analysis to assess whether targeted concepts are truly removed from model representations, beyond simple direct-prompt evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Wei', 'Yuta Nakashima', 'Noa Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['concept-erasure', 'model-editing', 'safety', 'privacy', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17320</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title><link>https://arxiv.org/abs/2512.17213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CheXPO-v2, an alignment framework for chest X-ray vision-language models that shifts from outcome-based RL supervision to process-level supervision.&lt;/li&gt;&lt;li&gt;Introduces a Knowledge Graph Consistency Reward that parses reasoning into Disease-Relation-Anatomy triplets to penalize incoherent logic and hallucinations at atomic granularity.&lt;/li&gt;&lt;li&gt;Combines hard-example mining and the new reward to outperform GRPO and prior SOTA on MIMIC-CXR-VQA, achieving strong results with only 5k samples and improved clinical verifiability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Liang', 'Yuxuan An', 'Di Wang', 'Jiawei Hu', 'Zhicheng Jiao', 'Bin Jing', 'Quan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination-mitigation', 'medical-VLMs', 'knowledge-graph-consistency', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17213</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</title><link>https://arxiv.org/abs/2512.17189</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play, training-free module that uses anatomical masks to guide contrastive decoding at token, attention, and logit levels.&lt;/li&gt;&lt;li&gt;Aims to mitigate hallucinations in medical vision-language models by steering model focus to specified anatomical regions and re-weighting outputs dynamically.&lt;/li&gt;&lt;li&gt;Evaluated across multiple medical imaging modalities (chest X-ray, CT, brain MRI, ocular ultrasound), showing improved regional understanding, reduced hallucinations, and higher diagnostic accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Liang', 'Chenxi Liu', 'Zhi Ma', 'Di Wang', 'Bin Jing', 'Quan Wang', 'Yuanyuan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'medical VLMs', 'contrastive decoding', 'safety/robustness', 'region-guided inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17189</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title><link>https://arxiv.org/abs/2512.16912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes exploration-exploitation trade-offs in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that spurious rewards can reduce policy entropy via clipping bias, producing more confident/deterministic outputs and performance gains.&lt;/li&gt;&lt;li&gt;Demonstrates entropy minimization alone is insufficient, and proposes a reward-misalignment model explaining when spurious rewards help beyond contaminated settings.&lt;/li&gt;&lt;li&gt;Provides principles for designing RLVR training to reconcile the paradoxical effects of discouraging both exploitation and exploration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter Chen', 'Xiaopeng Li', 'Ziniu Li', 'Wotao Yin', 'Xi Chen', 'Tianyi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'reward-misalignment', 'entropy', 'training dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16912</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DSO: Direct Steering Optimization for Bias Mitigation</title><link>https://arxiv.org/abs/2512.15926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Direct Steering Optimization (DSO): a reinforcement-learning method that learns linear activation transformations to steer model behavior at inference-time.&lt;/li&gt;&lt;li&gt;Targets bias mitigation by optimizing for equiprobable outcomes across demographic groups while preserving model capabilities, enabling a controllable fairness–performance trade-off.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art trade-offs on both vision-language models (VLMs) and large language models (LLMs), outperforming heuristic steering approaches for fairness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Monteiro Paes', 'Nivedha Sivakumar', 'Yinong Oliver Wang', 'Masha Fedzechkina Donaldson', 'Barry-John Theobald', 'Luca Zappella', 'Nicholas Apostoloff']&lt;/li&gt;&lt;li&gt;Tags: ['bias-mitigation', 'activation-steering', 'fairness-control', 'reinforcement-learning', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15926</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization</title><link>https://arxiv.org/abs/2512.06713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that greedy adversarial anonymization strategies cause utility collapse when migrated to local small-scale models (LSMs), arguing irrationality of current SOTA attacks.&lt;/li&gt;&lt;li&gt;Proposes RLAA (Rational Localized Adversarial Anonymization), a training-free, fully localized Attacker–Arbitrator–Anonymizer architecture that balances Marginal Privacy Gain (MPG) against Marginal Utility Cost (MUC).&lt;/li&gt;&lt;li&gt;Introduces an arbitrator as a rationality gatekeeper to validate attacker inferences, enabling an early-stopping criterion that filters negligible-privacy-benefit feedback and prevents excessive utility loss.&lt;/li&gt;&lt;li&gt;Evaluates RLAA on benchmarks and reports improved privacy–utility trade-offs versus strong baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Donghang Duan', 'Xu Zheng', 'Yuefeng He', 'Chong Mu', 'Leyi Cai', 'Lizong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'adversarial-anonymization', 'local-LLMs', 'privacy-utility-tradeoff']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06713</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Vision Language Models are Confused Tourists</title><link>https://arxiv.org/abs/2511.17004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConfusedTourist, a benchmark/adversarial robustness suite that tests VLMs with mixed or perturbed cultural/geographical cues in images.&lt;/li&gt;&lt;li&gt;Finds large accuracy drops under simple image-stacking perturbations and worse performance with image-generation-based variants.&lt;/li&gt;&lt;li&gt;Provides interpretability analysis showing systematic attention shifts to distracting cues as a root cause of failures.&lt;/li&gt;&lt;li&gt;Highlights that cultural concept mixing is a critical robustness failure mode for state-of-the-art vision-language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Amadeus Irawan', 'Ikhlasul Akmal Hanif', 'Muhammad Dehan Al Kautsar', 'Genta Indra Winata', 'Fajri Koto', 'Alham Fikri Aji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'vision-language-models', 'cultural-bias', 'benchmarking', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17004</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Are Watermarks in LLMs Ready for Deployment?</title><link>https://arxiv.org/abs/2506.05594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a systematization (taxonomy) of watermarking techniques for LLMs and evaluates their readiness for deployment.&lt;/li&gt;&lt;li&gt;Introduces an intellectual property classifier to measure watermark effectiveness under attack and attack-free scenarios and studies impacts on model utility.&lt;/li&gt;&lt;li&gt;Presents extensive experiments showing current watermark methods often degrade downstream performance and outlines limitations and practical challenges with recommendations for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kieu Dang', 'Phung Lai', 'NhatHai Phan', 'Yelong Shen', 'Ruoming Jin', 'Abdallah Khreishah', 'My T. Thai']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model stealing', 'intellectual property protection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05594</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAEs Are Good for Steering -- If You Select the Right Features</title><link>https://arxiv.org/abs/2505.20063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two SAE feature types: input features (capture input token patterns) and output features (have human-understandable effects on model outputs).&lt;/li&gt;&lt;li&gt;Proposes quantitative input and output scores to characterize and locate these feature types, and shows high input/output scores rarely co-occur.&lt;/li&gt;&lt;li&gt;Demonstrates that filtering SAE features by high output score yields 2–3x improvements in steering performance, making unsupervised SAE steering competitive with supervised methods.&lt;/li&gt;&lt;li&gt;Practical implication: feature selection based on output influence improves controllability of models without labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dana Arad', 'Aaron Mueller', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model-steering', 'interpretability', 'sparse-autoencoders', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20063</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains</title><link>https://arxiv.org/abs/2512.16401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantifies a large 'Reality Gap' for ASR: a robust multilingual model (IndicWav2Vec) degrades to 40.94% WER on rural clinical Indian audio.&lt;/li&gt;&lt;li&gt;Proposes a zero-data-exfiltration framework enabling localized continual adaptation using Low-Rank Adaptation (LoRA) to preserve privacy.&lt;/li&gt;&lt;li&gt;Empirically compares continual learning strategies: multi-domain Experience Replay (ER) yields a 17.1% relative improvement in target WER and reduces catastrophic forgetting by 55% versus naive adaptation.&lt;/li&gt;&lt;li&gt;Identifies numerical stability issues with standard Elastic Weight Consolidation (EWC) for LoRA in noisy settings and proposes a stabilized, linearized formulation; also shows acoustic adaptation is essential and cannot be substituted by language models alone.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshil Chauhan', 'Adityasinh Solanki', 'Vansh Patel', 'Kanav Kapoor', 'Ritvik Jain', 'Aditya Bansal', 'Pratik Narang', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'continual learning', 'ASR adaptation', 'catastrophic forgetting', 'LoRA / on-device fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16401</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Self Critique and Refinement for Faithful LLM Summarization</title><link>https://arxiv.org/abs/2512.05387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SCRPO: a self-supervised framework that builds a preference dataset by having the LLM critique and refine its own summaries, then uses preference learning to improve the same model.&lt;/li&gt;&lt;li&gt;Aims to reduce hallucinations and improve faithfulness without additional test-time refinement or reliance on stronger teacher models.&lt;/li&gt;&lt;li&gt;Evaluated on XSUM, CNN/DM, and SAMSum; shows improved faithfulness metrics and comparable or better overall summary quality versus state-of-the-art self-supervised methods, and is more efficient than test-time refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Yao Hu', 'Hema Swetha Koppula', 'Hadi Pouransari', 'Cem Koc', 'Oncel Tuzel', 'Raviteja Vemulapalli']&lt;/li&gt;&lt;li&gt;Tags: ['summarization', 'hallucination mitigation', 'preference learning', 'self-supervised training', 'faithfulness/alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05387</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering</title><link>https://arxiv.org/abs/2511.17559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SCARE, a benchmark for post-hoc safety verification in EHR question-answering that jointly evaluates question answerability (answerable/ambiguous/unanswerable) and SQL verification/correction.&lt;/li&gt;&lt;li&gt;Contains 4,200 triples (question, candidate SQL, expected output) grounded in MIMIC-III, MIMIC-IV, and eICU, with candidate SQL produced by seven different text-to-SQL models to simulate realistic errors.&lt;/li&gt;&lt;li&gt;Benchmarks a range of approaches (two-stage methods to agentic frameworks) and identifies trade-offs between question classification and SQL error correction, highlighting deployment challenges for safe clinical QA systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gyubok Lee', 'Woosog Chay', 'Edward Choi']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'post-hoc-verification', 'text-to-SQL', 'EHR-QA', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17559</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2511.02376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoAdv, a training-free framework for automated multi-turn jailbreaking of LLMs that adaptively crafts prompts.&lt;/li&gt;&lt;li&gt;Combines three adaptive mechanisms: a pattern manager (learns from successful attacks), a temperature manager (adjusts sampling), and a two-phase rewriting strategy (disguise then iteratively refine).&lt;/li&gt;&lt;li&gt;Reports high attack success (up to 95% on Llama-3.1-8B within six turns) and shows multi-turn attacks outperform single-turn baselines across commercial and open-source models.&lt;/li&gt;&lt;li&gt;Concludes that alignment and safety mechanisms optimized for single-turn interactions are insufficient, highlighting need for multi-turn-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashray Reddy', 'Andrew Zagula', 'Nicholas Saban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02376</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LiveSecBench: A Dynamic and Event-Driven Safety Benchmark for Chinese Language Model Applications</title><link>https://arxiv.org/abs/2511.02366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LiveSecBench, a continuously updated safety benchmark focused on Chinese-language LLM application scenarios.&lt;/li&gt;&lt;li&gt;Constructs dataset via automated generation plus human verification and periodically releases new versions to expand coverage and metrics.&lt;/li&gt;&lt;li&gt;Evaluates 57 representative LLMs across five safety dimensions (Public Safety, Fairness &amp; Bias, Privacy, Truthfulness, Mental Health Safety) using an ELO rating and publishes a leaderboard.&lt;/li&gt;&lt;li&gt;Provides a practical, up-to-date standard for measuring LLM safety in Chinese applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Li', 'Peiru Yang', 'Feng Huang', 'Zhongliang Yang', 'Kecheng Wang', 'Haitian Li', 'Baocheng Chen', 'Xingyu An', 'Ziyu Liu', 'Youdan Yang', 'Kejiang Chen', 'Sifang Wan', 'Xu Wang', 'Yufei Sun', 'Liyan Wu', 'Ruiqi Zhou', 'Wenya Wen', 'Xingchi Gu', 'Tianxin Zhang', 'Yue Gao', 'Yongfeng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety benchmark', 'LLM evaluation', 'privacy', 'truthfulness', 'bias/fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02366</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs</title><link>https://arxiv.org/abs/2510.13901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAID, a method that optimizes continuous token embeddings to craft adversarial suffixes that induce restricted/unsafe LLM outputs while preserving fluency.&lt;/li&gt;&lt;li&gt;Introduces a refusal-aware regularizer to steer embeddings away from refusal directions and a coherence term to maintain semantic plausibility; uses critic-guided decoding to map embeddings back to tokens.&lt;/li&gt;&lt;li&gt;Evaluates on multiple open-source LLMs and reports higher jailbreak success rates with fewer queries and lower computational cost than recent white-box and black-box baselines, emphasizing the role of embedding-space regularization in jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tuan T. Nguyen', 'John Le', 'Thai T. Vu', 'Willy Susilo', 'Heath Cooper']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'embedding-space attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13901</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Agentic Security: Applications, Threats and Defenses</title><link>https://arxiv.org/abs/2510.06445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive, first-of-its-kind survey that organizes agentic security around Applications, Threats, and Defenses and catalogs &gt;160 papers.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of threats to agentic systems, defensive measures, and cybersecurity applications of autonomous agents.&lt;/li&gt;&lt;li&gt;Analyzes trends in agent architectures and modality/model coverage and identifies critical research gaps and open problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asif Shahriar', 'Md Nafiu Rahman', 'Sadif Ahmed', 'Farig Sadeque', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['agentic security', 'threat taxonomy', 'defenses', 'red teaming', 'security survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06445</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge</title><link>https://arxiv.org/abs/2506.18998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to distinguish genuine reasoning learning from mere memorization in LLMs, focusing on STEM problems.&lt;/li&gt;&lt;li&gt;Finds that memorization leads LLMs to overestimate their own reasoning ability, producing &gt;45% inconsistency in feasibility/self-assessment under coherent task perturbations.&lt;/li&gt;&lt;li&gt;Effect is strongest in standardized-jargon domains (science and medicine); authors call for methods to improve models' calibrated self-knowledge and trustworthiness. Code/results are publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Kale']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'model calibration', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18998</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</title><link>https://arxiv.org/abs/2506.17231</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Prompt Distillation: a framework that uses masked language modeling, reinforcement learning, and dynamic temperature control to transfer jailbreaking capabilities from large LMs (LLMs) to smaller LMs (SLMs).&lt;/li&gt;&lt;li&gt;Aims to create efficient, stealthy, and resource-light jailbreak attacks that retain high success rates and generalize across models.&lt;/li&gt;&lt;li&gt;Empirical results claim improvements in attack efficacy, resource optimization, and cross-model versatility; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Li', 'Chong Zhang', 'Jia Wang', 'Fangyu Wu', 'Yushi Li', 'Xiaobo Jin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'model distillation', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17231</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs</title><link>https://arxiv.org/abs/2505.15210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Deliberation over Priors (DP), a framework that injects knowledge-graph structural priors and constraint priors into LLM reasoning to reduce hallucinations and improve faithfulness.&lt;/li&gt;&lt;li&gt;Uses a progressive knowledge-distillation pipeline combining supervised fine-tuning and a Kahneman–Tversky optimization component to improve relation-path generation.&lt;/li&gt;&lt;li&gt;Adds a reasoning–introspection verification stage that leverages extracted constraint priors from KGs to validate and refine LLM outputs for greater reliability.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains on three benchmarks (e.g., +13% Hit@1 on ComplexWebQuestions) and provides code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Ma', 'Ning Qu', 'Zhitao Gao', 'Rui Xing', 'Jun Liu', 'Hongbin Pei', 'Jiang Xie', 'Linyun Song', 'Pinghui Wang', 'Jing Tao', 'Zhou Su']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-graph', 'hallucination-mitigation', 'retrieval-augmented-generation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15210</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies</title><link>https://arxiv.org/abs/2505.14972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CROSS, a 1,284-instance multilingual, visually grounded benchmark across 16 countries and 14 languages to evaluate cultural safety in LVLMs.&lt;/li&gt;&lt;li&gt;Proposes CROSS-Eval, an intercultural-theory-based framework measuring cultural awareness, norm education, compliance, and helpfulness.&lt;/li&gt;&lt;li&gt;Evaluates 21 leading LVLMs, finding substantial cultural safety gaps (best model: 61.79% awareness, 37.73% compliance) and shows reasoning capacity helps but is insufficient.&lt;/li&gt;&lt;li&gt;Proposes supervised fine-tuning with culturally grounded data and preference tuning with contrastive pairs, yielding large gains in cultural awareness and compliance while preserving multimodal capability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyi Qiu', 'Kung-Hsiang Huang', 'Ruichen Zheng', 'Jiao Sun', 'Nanyun Peng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'multimodal LLMs', 'benchmark', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14972</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents</title><link>https://arxiv.org/abs/2504.02800</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey of LLM-based methods for detecting mental disorders from social media, covering pre-trained models, Retrieval-Augmented Generation (RAG), and agentic systems.&lt;/li&gt;&lt;li&gt;Empirically evaluates LLM performance across tasks and measures the impact of RAG on reducing hallucinations and contextual gaps.&lt;/li&gt;&lt;li&gt;Explores agentic/autonomous reasoning for multi-step intervention and emphasizes trustworthiness and explainability in clinical-targeted applications.&lt;/li&gt;&lt;li&gt;Provides a unified benchmark for the field to support development of more reliable, explainable systems for mental-health support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuohan Ge', 'Darian Li', 'Yubo Wang', 'Nicole Hu', 'Xinyi Zhu', 'Haoyang Li', 'Xin Zhang', 'Mingtao Zhang', 'Shihao Qi', 'Yuming Xu', 'Han Shi', 'Chen Jason Zhang', 'Qing Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety_evaluation', 'alignment', 'RAG', 'trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02800</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment</title><link>https://arxiv.org/abs/2502.11361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLDBench, a large-scale benchmark of ~62,000 labeled text-image pairs across 13 disinformation categories curated from 58 news outlets.&lt;/li&gt;&lt;li&gt;Dataset created via a semi-automated pipeline plus expert review (22 experts, 500+ hours) with substantial inter-annotator agreement.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art LLMs and VLMs, showing multimodal (text+image) inputs improve disinformation detection by 5–35 percentage points over text-only models.&lt;/li&gt;&lt;li&gt;Provides data, code, and evaluation/fine-tuning/robustness tools and aligns dataset design with AI governance frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaina Raza', 'Ashmal Vayani', 'Aditya Jain', 'Aravind Narayanan', 'Vahid Reza Khazaie', 'Syed Raza Bashir', 'Elham Dolatabadi', 'Gias Uddin', 'Christos Emmanouilidis', 'Rizwan Qureshi', 'Mubarak Shah']&lt;/li&gt;&lt;li&gt;Tags: ['disinformation-detection', 'multimodal-benchmark', 'safety-evaluation', 'robustness', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11361</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</title><link>https://arxiv.org/abs/2512.19070</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hallucination Disentangled Decoding (HDD), an inference-time, training-free method to reduce object hallucinations in large vision-language models.&lt;/li&gt;&lt;li&gt;HDD segments images and selects augmented crops plus a blank image to disentangle visual evidence from language priors, aiming to suppress hallucinated object mentions.&lt;/li&gt;&lt;li&gt;Method targets hallucinations in both the visual and language modalities and reports improved visual grounding without additional model training.&lt;/li&gt;&lt;li&gt;Code is provided for reproduction (GitHub link in abstract).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruiqi Ma', 'Yu Yan', 'Chunhong Zhang', 'Minghao Yin', 'XinChao Liu', 'Zhihong Jin', 'Zheng Hu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'vision-language_models', 'robustness', 'inference-time_mitigation', 'multimodal_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19070</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight, multi-stage defense pipeline against prompt injection and jailbreaks for LLM-based systems, with a core semantic filter using text normalization, TF-IDF, and a Linear SVM.&lt;/li&gt;&lt;li&gt;Core classifier achieves 93.4% accuracy and 96.5% specificity on held-out data and markedly reduces attack throughput with minimal compute overhead.&lt;/li&gt;&lt;li&gt;Demonstrates pipeline-level gains (accuracy from 35.1% to 93.4%) and &gt;10x lower latency (≈47s vs ≈450s) compared to a model-based moderator (ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluated on a curated corpus of over 30,000 labeled prompts covering benign, jailbreak, and application-layer injection examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM moderation', 'adversarial defenses', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SecureCode v2.0: a production-grade dataset of 1,215 security-focused coding examples (vulnerable + secure implementations) tied to real CVEs across 11 vulnerability categories and 11 programming languages.&lt;/li&gt;&lt;li&gt;Dataset split (989 train / 122 validation / 104 test), uses a 4-turn conversational structure to mirror developer–AI interactions, and includes automated validation tools and open-source benchmarking protocols.&lt;/li&gt;&lt;li&gt;Includes concrete attacks, defense-in-depth operational guidance (SIEM integration, Docker/AppArmor/WAF recommendations), language-specific testing approaches, and aims to train/benchmark security-aware code-generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset/Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['secure code generation', 'vulnerability dataset', 'AI safety', 'benchmarking', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CodeSimpleQA: Scaling Factuality in Code Large Language Models</title><link>https://arxiv.org/abs/2512.19424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CodeSimpleQA, a bilingual (English/Chinese) benchmark focused on factual accuracy of code-related QA, covering multiple programming languages and CS domains.&lt;/li&gt;&lt;li&gt;Creates CodeSimpleQA-Instruct, a 66M-sample instruction corpus, and proposes a post-training framework combining supervised fine-tuning and reinforcement learning to improve factuality.&lt;/li&gt;&lt;li&gt;Evaluations show state-of-the-art LLMs still struggle with code factuality, and the proposed training approach yields substantial improvements over the base model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Yang', 'Wei Zhang', 'Yizhi Li', 'Shawn Guo', 'Haowen Wang', 'Aishan Liu', 'Ge Zhang', 'Zili Wang', 'Zhoujun Li', 'Xianglong Liu', 'Weifeng Lv']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'alignment', 'benchmarking', 'code-LLMs', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19424</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HATS: High-Accuracy Triple-Set Watermarking for Large Language Models</title><link>https://arxiv.org/abs/2512.19378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a triple-partition watermarking scheme that splits the vocabulary at each decoding step into Green/Yellow/Red sets with fixed ratios and restricts sampling to Green+Yellow.&lt;/li&gt;&lt;li&gt;Detection replays partitions, computes Green-enrichment and Red-depletion statistics, converts them to one-sided z-scores, and aggregates p-values via Fisher's method to decide if text is watermarked.&lt;/li&gt;&lt;li&gt;Implemented and evaluated on Llama 2 7B, reporting high detection accuracy at fixed false-positive rates while maintaining text quality/readability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqing Hu', 'Chenxu Zhao', 'Jiazhong Lu', 'Xiaolei Liu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM safety', 'model provenance', 'detection/forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19378</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title><link>https://arxiv.org/abs/2512.19238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates bias against 93 stigmatized groups using SocialStigmaQA (37 social scenarios) across three LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B).&lt;/li&gt;&lt;li&gt;Finds stigmas rated as highly perilous by humans produce the most biased outputs (≈60%), while sociodemographic stigmas produce the least (≈11%).&lt;/li&gt;&lt;li&gt;Assesses guardrail/ moderation models (Granite Guardian, Llama Guard, Mistral Moderation): they reduce biased outputs (10.4%, 1.4%, 7.8%) but do not change which stigma features drive bias and often fail to detect biased intent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anna-Maria Gueorguieva', 'Aylin Caliskan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM bias', 'model safety', 'guardrails/moderation', 'benchmarking', 'social bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19238</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2512.19134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QuCo-RAG: a dynamic retrieval-augmented generation method that quantifies uncertainty using statistics from the pre-training corpus rather than model-internal signals.&lt;/li&gt;&lt;li&gt;Two-stage uncertainty quantification: (1) pre-generation detection of low-frequency (long-tail) entities, (2) during-generation verification of entity co-occurrence in the pre-training corpus (zero co-occurrence flags high hallucination risk).&lt;/li&gt;&lt;li&gt;Implements millisecond-latency queries over a 4 trillion-token index (Infini-gram) to trigger retrieval when uncertainty is high, yielding substantial EM gains on multi-hop QA and domain-generalization (biomedical) across multiple models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dehai Min', 'Kailin Zhang', 'Tongtong Wu', 'Lu Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'uncertainty estimation', 'model safety', 'pretraining-corpus analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19134</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</title><link>https://arxiv.org/abs/2512.18999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares an end-to-end LLM chatbot with a modular pipeline (task decomposition, semantic clustering, flow management) for medical follow-up dialogs.&lt;/li&gt;&lt;li&gt;Modular approach substantially improves dialog stability and information extraction accuracy versus end-to-end LLMs.&lt;/li&gt;&lt;li&gt;Reports reductions in dialogue turns (46.73%) and token consumption (80%–87.5%), arguing for external control mechanisms in high-stakes deployments.&lt;/li&gt;&lt;li&gt;Emphasizes the need for structured process control when deploying LLMs in medical follow-up to ensure reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinyan Liu', 'Zikang Chen', 'Qinchuan Wang', 'Tan Xie', 'Heming Zheng', 'Xudong Lv']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Medical NLP', 'Deployment safety', 'Dialog systems', 'Process control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18999</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FASTRIC: Prompt Specification Language for Verifiable LLM Interactions</title><link>https://arxiv.org/abs/2512.18940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FASTRIC, a Prompt Specification Language that encodes multi-turn interactions as explicit Finite State Machine (FSM) elements to enable verifiable LLM execution.&lt;/li&gt;&lt;li&gt;Defines seven FSM components (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) and a procedural conformance metric to measure execution adherence to specifications.&lt;/li&gt;&lt;li&gt;Shows empirical results across four specification formality levels and three model scales, finding model-specific "Goldilocks" formality ranges where conformance is maximized.&lt;/li&gt;&lt;li&gt;Positions LLMs as unified infrastructure (parser, interpreter, runtime) for prompt specifications, enabling systematic prompt specification engineering for verifiable interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wen-Long Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM verification', 'Prompt specification', 'Prompt engineering', 'Safety evaluation', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18940</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</title><link>https://arxiv.org/abs/2512.18623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-CAS: a hierarchical reinforcement learning agent that selects temporary neuron perturbations at inference time to correct hallucinations without permanently editing model weights.&lt;/li&gt;&lt;li&gt;Method enables adaptive, context-sensitive corrections (dynamic and fine-grained) in contrast to static parameter edits and heuristic dynamic approaches.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in factuality across benchmarks (e.g., +10.98 pts StoryCloze, +2.71 pts TriviaQA, +2.06 pts TruthfulQA MC1) and outperforms static editors (ITI, CAA) and SADI.&lt;/li&gt;&lt;li&gt;Claims efficiency and potential for future multimodal extensions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jensen Zhang', 'Ningyuan Liu', 'Yijia Fan', 'Zihao Huang', 'Qinglin Zeng', 'Kaitong Cai', 'Jian Wang', 'Keze Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-correction', 'inference-time-perturbation', 'reinforcement-learning', 'model-editing', 'factuality-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18623</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts</title><link>https://arxiv.org/abs/2512.18608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares lightweight encoder-decoder (T5-small) and decoder-only (Mistral-Instruct-v0.3) models fine-tuned on PII masking datasets derived from the AI4Privacy benchmark, with 24 standardized PII categories and higher-granularity variants.&lt;/li&gt;&lt;li&gt;Evaluates using entity- and character-level metrics, type accuracy, and exact match; finds label normalization improves performance across architectures.&lt;/li&gt;&lt;li&gt;Finds Mistral yields higher F1 and recall and is more robust across PII types but has substantially higher generation latency; T5 provides more controllable structured outputs, lower inference cost, and is suitable for real-time deployment.&lt;/li&gt;&lt;li&gt;Deploys T5 in a real Discord bot and observes performance degradation on informal conversational inputs, highlighting trade-offs between accuracy, robustness, and computational efficiency for real-world PII redaction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prabigya Acharya', 'Liza Shrestha']&lt;/li&gt;&lt;li&gt;Tags: ['PII masking', 'privacy-preserving NLP', 'lightweight LLMs', 'model robustness', 'deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18608</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On Finding Inconsistencies in Documents</title><link>https://arxiv.org/abs/2512.18601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIND, a benchmark of long, technical documents with manually inserted inconsistencies for evaluating LM ability to detect inconsistencies.&lt;/li&gt;&lt;li&gt;Evaluates models (best-performing gpt-5 recovered 64% of inserted inconsistencies) and reports that models also discovered previously undetected real inconsistencies in existing papers.&lt;/li&gt;&lt;li&gt;Finds that even top models miss nearly half of inconsistencies, indicating inconsistency detection remains challenging.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles J. Lovering', 'Seth Ebner', 'Brandon Smock', 'Michael Krumdick', 'Saad Rabbani', 'Ahmed Muhammad', 'Varshini Reddy', 'Chris Tanner']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'safety-evaluation', 'LLM-capabilities', 'document-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18601</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering</title><link>https://arxiv.org/abs/2512.18551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces neologism learning: training new tokens to represent concepts not in a model's vocabulary as a parameter-efficient way to steer model behavior.&lt;/li&gt;&lt;li&gt;Compares neologism learning to LoRA fine-tuning under matched data and hyperparameters, claiming neologisms outperform low-rank adaptation in their setup.&lt;/li&gt;&lt;li&gt;Notes behavioral phenomena such as self-verbalization where the model invents its own new words when asked about a learned neologism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungjoon Park', 'Varun Ramamurthi', 'Owen Terry']&lt;/li&gt;&lt;li&gt;Tags: ['neologism learning', 'model steering', 'parameter-efficient tuning', 'LoRA comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18551</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs on Drugs: Language Models Are Few-Shot Consumers</title><link>https://arxiv.org/abs/2512.18546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Controlled empirical study evaluating the effect of single-sentence 'psychoactive' persona prompts (LSD, cocaine, alcohol, cannabis) on GPT-5-mini using ARC-Challenge (100 items per condition) with deterministic decoding and statistical tests.&lt;/li&gt;&lt;li&gt;Finds large, statistically significant drops in accuracy (control 0.45 → alcohol 0.10, cocaine 0.21, LSD 0.19, cannabis 0.30), largely because persona prompts disrupt the required 'Answer:' template.&lt;/li&gt;&lt;li&gt;Interprets persona text as a 'few-shot consumable' that can reliably degrade model behavior at inference time without touching model weights; all code and data are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Doudkin']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'LLM robustness', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18546</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title><link>https://arxiv.org/abs/2512.18462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Log-Frequency LMI (LF-LMI) to detect semantic artifacts/spurious correlations in NLI datasets.&lt;/li&gt;&lt;li&gt;Generates synthetic counterfactual contrast sets using LLM synthesis with multi-judge verification to ensure quality.&lt;/li&gt;&lt;li&gt;Introduces Dynamic Balanced Sampling, a rotating training-sampling strategy to mitigate catastrophic forgetting while rebalancing against artifacts.&lt;/li&gt;&lt;li&gt;Reports improved robustness: consistency on a benchmark increased from 63.5% to 81.0% while preserving 88.4% in-domain accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Christopher Rom\\'an Jaimes"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'counterfactual data augmentation', 'dataset synthesis', 'bias/spurious-correlation mitigation', 'NLI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18462</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training LLMs with LogicReward for Faithful and Rigorous Reasoning</title><link>https://arxiv.org/abs/2512.18196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LogicReward: a training reward that enforces step-level logical correctness using a theorem prover.&lt;/li&gt;&lt;li&gt;Proposes Autoformalization with Soft Unification to reduce NL ambiguity and improve formalization for theorem proving.&lt;/li&gt;&lt;li&gt;Reports that an 8B model trained with LogicReward outperforms GPT-4o and o4-mini on NLI and logical reasoning benchmarks and improves reasoning faithfulness and generalization.&lt;/li&gt;&lt;li&gt;Claims LogicReward provides a reliable reward signal even without ground-truth labels and will release data/code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jundong Xu', 'Hao Fei', 'Huichi Zhou', 'Xin Quan', 'Qijun Huang', 'Shengqiong Wu', 'William Yang Wang', 'Mong-Li Lee', 'Wynne Hsu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reasoning', 'faithfulness', 'theorem-proving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18196</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CoPE: A Small Language Model for Steerable and Scalable Content Labeling</title><link>https://arxiv.org/abs/2512.18027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CoPE, a policy-steerable small LLM (9B) for fast, accurate content labeling across multiple harm areas.&lt;/li&gt;&lt;li&gt;Proposes Contradictory Example Training to teach policy interpretation rather than rote memorization, and Binocular Labeling for generating unambiguous policy datasets.&lt;/li&gt;&lt;li&gt;Claims frontier-model-level accuracy at ~1% of their size and releases a model runnable on a consumer GPU, enabling scalable classifier deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samidh Chakrabarti', 'David Willner', 'Kevin Klyman', 'Tiffany Saade', 'Emily Capstick', 'Sabina Nong']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'safety evaluation', 'model steerability', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18027</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression</title><link>https://arxiv.org/abs/2512.17920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Compression-Decay Comprehension Test (CDCT), a benchmark that separately measures constraint compliance (CC) and semantic accuracy (SA) across prompt compression levels.&lt;/li&gt;&lt;li&gt;Finds a robust U-shaped pattern in constraint compliance with violations peaking at medium compression; CC and SA are largely orthogonal, and CC effects are larger than SA effects.&lt;/li&gt;&lt;li&gt;RLHF ablation experiments show removing 'helpfulness' signals dramatically improves CC (median +598%), implicating RLHF-trained helpfulness behaviors as a primary cause of constraint violations under compression.&lt;/li&gt;&lt;li&gt;Provides actionable insights for improving deployed systems and highlights a tension between RLHF alignment objectives and reliable instruction-following.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Baxi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'instruction-following', 'benchmark', 'prompt-compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17920</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.17911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RMLLMU-Bench, the first benchmark for unlearning in Reasoning Multimodal LLMs with metrics for both reasoning-level leakage and reasoning retention.&lt;/li&gt;&lt;li&gt;Evaluates existing unlearning methods and finds they either leave substantial reasoning traces or degrade general reasoning ability.&lt;/li&gt;&lt;li&gt;Proposes R-MUSE, a training-free, inference-time intervention that steers internal representations to forget answers and chain-of-thought traces while preserving overall reasoning; shows improved trade-off on the benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongji Li', 'Junchi yao', 'Manjiang Yu', 'Priyanka Singh', 'Xue Li', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'reasoning leakage', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17911</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>In-Context Probing for Membership Inference in Fine-Tuned Language Models</title><link>https://arxiv.org/abs/2512.16292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICP-MIA, a black-box membership inference attack for fine-tuned LLMs based on the 'Optimization Gap'—the remaining potential loss reduction at convergence—as a membership signal.&lt;/li&gt;&lt;li&gt;Introduces In-Context Probing (ICP) to estimate the optimization gap without training, with two strategies: reference-data-based probing (using semantically similar public samples) and self-perturbation (masking or generation).&lt;/li&gt;&lt;li&gt;Demonstrates that ICP-MIA substantially outperforms prior black-box MIAs across tasks and models, especially at low false positive rates, and analyzes factors (reference alignment, model type, PEFT, schedules) affecting attack success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhexi Lu', 'Hongliang Chi', 'Nathalie Baracaldo', 'Swanand Ravindra Kadhe', 'Yuseok Jeon', 'Lei Yu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'black-box-attacks', 'LLM-finetuning', 'training-dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16292</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>COBRA: Catastrophic Bit-flip Reliability Analysis of State-Space Models</title><link>https://arxiv.org/abs/2512.15778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a BFA (bit-flip attack) framework targeting state-space models (SSMs), specifically Mamba-based architectures (paper refers to RAMBO/Cobra for this analysis).&lt;/li&gt;&lt;li&gt;Demonstrates that flipping a single critical bit in Mamba-1.4b can catastrophically degrade performance on LAMBADA (accuracy 74.64% -&gt; 0%; perplexity 18.94 -&gt; 3.75e6).&lt;/li&gt;&lt;li&gt;Provides empirical evidence that SSMs are highly vulnerable to hardware-induced memory faults, highlighting reliability and security risks for deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjay Das', 'Swastik Bhattacharya', 'Shamik Kundu', 'Arnab Raha', 'Souvik Kundu', 'Kanad Basu']&lt;/li&gt;&lt;li&gt;Tags: ['bit-flip attacks', 'hardware attacks', 'model robustness', 'state-space models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15778</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks</title><link>https://arxiv.org/abs/2511.18562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes robustness of split conformal prediction under adversarial test-time perturbations, characterizing how calibration-time attack strength affects coverage guarantees.&lt;/li&gt;&lt;li&gt;Shows empirically that prediction coverage varies monotonically with calibration-time attack strength and that suitable calibration attacks can maintain target coverage across a range of test-time attack levels.&lt;/li&gt;&lt;li&gt;Demonstrates that adversarial training during model training yields tighter, more informative prediction sets while retaining robustness to adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xunlei Qian', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'conformal-prediction', 'robustness', 'adversarial-training', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18562</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2511.02376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoAdv, a training-free framework for automated multi-turn jailbreaking of LLMs that adaptively crafts prompts.&lt;/li&gt;&lt;li&gt;Combines three adaptive mechanisms: a pattern manager (learns from successful attacks), a temperature manager (adjusts sampling), and a two-phase rewriting strategy (disguise then iteratively refine).&lt;/li&gt;&lt;li&gt;Reports high attack success (up to 95% on Llama-3.1-8B within six turns) and shows multi-turn attacks outperform single-turn baselines across commercial and open-source models.&lt;/li&gt;&lt;li&gt;Concludes that alignment and safety mechanisms optimized for single-turn interactions are insufficient, highlighting need for multi-turn-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashray Reddy', 'Andrew Zagula', 'Nicholas Saban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02376</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Confidence Calibration in Vision-Language-Action Models</title><link>https://arxiv.org/abs/2507.17383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of confidence calibration for vision-language-action (VLA) foundation models that map visual observations and natural language instructions to robot motor commands.&lt;/li&gt;&lt;li&gt;Establishes a calibration baseline, analyzes how calibration error relates to task success and how calibration evolves over time during execution.&lt;/li&gt;&lt;li&gt;Proposes two lightweight mitigation techniques for miscalibration: prompt ensembles and action-wise Platt scaling.&lt;/li&gt;&lt;li&gt;Aims to improve trustworthiness of VLAs by providing reliable uncertainty quantification for robotic decision-making.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas P Zollo', 'Richard Zemel']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'robot safety', 'uncertainty quantification', 'vision-language-action', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17383</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Personalized and Resilient Distributed Learning Through Opinion Dynamics</title><link>https://arxiv.org/abs/2505.14081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a distributed learning algorithm that integrates distributed gradient descent with the Friedkin–Johnsen opinion dynamics model to achieve personalization and resilience.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis quantifying convergence speed and characterizing the neighborhood of final learned models, with tunable parameters to trade off personalization vs. resilience.&lt;/li&gt;&lt;li&gt;Demonstrates empirically on synthetic and real-world distributed learning tasks that the method attains high global accuracy and robustness in the presence of malicious agents compared to standard strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luca Ballotta', 'Nicola Bastianello', 'Riccardo M. G. Ferrari', 'Karl H. Johansson']&lt;/li&gt;&lt;li&gt;Tags: ['distributed learning', 'resilience', 'Byzantine robustness', 'personalization', 'opinion dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14081</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</title><link>https://arxiv.org/abs/2409.01382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative study detecting LLM-generated Python code (14,485 functions, 11,913 classes) from four LLMs (GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, GPT-OSS) using interpretable software metrics and CatBoost classifiers.&lt;/li&gt;&lt;li&gt;Finds granularity (function vs. class) drives detectability far more than model differences, with largely disjoint structural signatures and a single universal feature (Comment-to-Code Ratio) whose effect size varies by model.&lt;/li&gt;&lt;li&gt;Demonstrates poor cross-model generalization of detectors and highlights that GPT-3.5 is an outlier (very high detectability) compared to contemporary models, implying robustness issues for single-model detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Musfiqur Rahman', 'SayedHassan Khatoonabadi', 'Ahmad Abdellatif', 'Emad Shihab']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-detection', 'code-generation', 'robustness', 'explainable-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01382</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Training robust and generalizable quantum models</title><link>https://arxiv.org/abs/2311.11871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives parameter-dependent Lipschitz bounds for quantum models with trainable data encodings, showing the encoding norm crucially affects robustness to data perturbations.&lt;/li&gt;&lt;li&gt;Provides a generalization error bound that explicitly involves encoding parameters, linking encoding design to generalization performance.&lt;/li&gt;&lt;li&gt;Proposes a practical training strategy that regularizes the Lipschitz bound to improve robustness and generalization, and shows fixed (non-trainable) encodings cannot tune these properties.&lt;/li&gt;&lt;li&gt;Validates theoretical findings with numerical experiments illustrating the benefits of trainable encodings and Lipschitz regularization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Berberich', 'Daniel Fink', "Daniel Pranji\\'c", 'Christian Tutschku', 'Christian Holm']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'quantum machine learning', 'Lipschitz bounds', 'robustness regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.11871</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title><link>https://arxiv.org/abs/2512.16912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes exploration-exploitation trade-offs in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that spurious rewards can reduce policy entropy via clipping bias, producing more confident/deterministic outputs and performance gains.&lt;/li&gt;&lt;li&gt;Demonstrates entropy minimization alone is insufficient, and proposes a reward-misalignment model explaining when spurious rewards help beyond contaminated settings.&lt;/li&gt;&lt;li&gt;Provides principles for designing RLVR training to reconcile the paradoxical effects of discouraging both exploitation and exploration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter Chen', 'Xiaopeng Li', 'Ziniu Li', 'Wotao Yin', 'Xi Chen', 'Tianyi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'reward-misalignment', 'entropy', 'training dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16912</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DSO: Direct Steering Optimization for Bias Mitigation</title><link>https://arxiv.org/abs/2512.15926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Direct Steering Optimization (DSO): a reinforcement-learning method that learns linear activation transformations to steer model behavior at inference-time.&lt;/li&gt;&lt;li&gt;Targets bias mitigation by optimizing for equiprobable outcomes across demographic groups while preserving model capabilities, enabling a controllable fairness–performance trade-off.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art trade-offs on both vision-language models (VLMs) and large language models (LLMs), outperforming heuristic steering approaches for fairness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Monteiro Paes', 'Nivedha Sivakumar', 'Yinong Oliver Wang', 'Masha Fedzechkina Donaldson', 'Barry-John Theobald', 'Luca Zappella', 'Nicholas Apostoloff']&lt;/li&gt;&lt;li&gt;Tags: ['bias-mitigation', 'activation-steering', 'fairness-control', 'reinforcement-learning', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15926</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SEA: Spectral Edge Attack on Graph Neural Networks</title><link>https://arxiv.org/abs/2512.08964</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEA (Spectral Edge Attack), which uses spectral adversarial robustness evaluation to identify the most vulnerable edges in a graph.&lt;/li&gt;&lt;li&gt;Performs stealthy attacks by subtly adjusting edge weights (not changing graph connectivity) to degrade the performance of various GNN architectures.&lt;/li&gt;&lt;li&gt;Demonstrates high attack effectiveness while maintaining the graph's connectivity pattern, making detection more difficult compared to edge addition/deletion attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Graph Neural Networks', 'Adversarial attacks', 'Spectral analysis', 'Stealthy/undetectable attacks', 'Robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08964</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</title><link>https://arxiv.org/abs/2511.20944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares a Forensic Psycholinguistic Stream (CatBoost) vs a Semantic Stream (DistilBERT) for Business Email Compromise (BEC) detection on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud.&lt;/li&gt;&lt;li&gt;DistilBERT achieves near-perfect detection on synthetic threats (AUC &gt; 0.99, F1 = 0.998) with ~7.4 ms latency on Tesla T4; CatBoost attains competitive performance (AUC = 0.991, F1 = 0.949) with much lower latency (0.8 ms) and resource usage.&lt;/li&gt;&lt;li&gt;Authors emphasize deployment trade-offs (accuracy vs. latency/cost), report cost-sensitive learning ROI &gt;99.9%, and recommend CatBoost for edge/cost-constrained settings and DistilBERT for GPU-equipped organizations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaw Osei Adjei (Kwame Nkrumah University of Science', 'Technology)', 'Frederick Ayivor (Independent Researcher', 'Fishers', 'Indiana', 'USA)', 'Davis Opoku (Kwame Nkrumah University of Science', 'Technology)']&lt;/li&gt;&lt;li&gt;Tags: ['business-email-compromise', 'phishing-detection', 'adversarial-samples', 'forensic-psycholinguistics', 'model-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20944</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates Profile Pollution Attacks (PPA) on sequential recommender systems that subtly modify partial user interaction sequences to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Proposes CREAT: a bi-level constrained reinforcement learning framework combining pattern-inversion rewards and distribution-consistency rewards to balance attack efficacy and stealthiness.&lt;/li&gt;&lt;li&gt;Introduces constrained group relative reinforcement learning with dynamic barrier constraints and group-shared experience replay to enable step-wise, low-detectability perturbations.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and detectability of the attack in experiments, demonstrating strong targeted pollution with minimal distribution shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiang Chen', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning / profile pollution', 'recommender systems', 'reinforcement learning', 'robustness/stealthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TAPAS: Datasets for Learning the Learning with Errors Problem</title><link>https://arxiv.org/abs/2510.08797</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TAPAS, a suite of datasets to enable AI-based attacks on the Learning with Errors (LWE) problem used in post-quantum cryptography.&lt;/li&gt;&lt;li&gt;Provides dataset creation methodology, off-the-shelf data covering multiple LWE parameter settings, and establishes baseline attack performance using AI models.&lt;/li&gt;&lt;li&gt;Aims to lower the barrier for AI practitioners to prototype and improve cryptanalysis approaches against LWE and outlines directions for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eshika Saxena', 'Alberto Alfarano', 'Fran\\c{c}ois Charton', 'Emily Wenger', 'Kristin Lauter']&lt;/li&gt;&lt;li&gt;Tags: ['cryptanalysis', 'post-quantum cryptography', 'adversarial/AI-enabled attacks', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08797</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ESSA: Evolutionary Strategies for Scalable Alignment</title><link>https://arxiv.org/abs/2507.04453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ESSA: a gradient-free evolutionary strategy framework to align LLMs by optimizing low-rank adapter (LoRA) singular values via black-box optimization.&lt;/li&gt;&lt;li&gt;Reduces optimization dimensionality (SVD of LoRA matrices) enabling efficient search in quantized INT4/INT8 inference and practical scaling to very large models.&lt;/li&gt;&lt;li&gt;Empirical results show substantial accuracy gains over gradient-based GRPO on benchmarks (GSM8K, PRM800K, IFEval) and faster wall-clock convergence at large GPU scales.&lt;/li&gt;&lt;li&gt;Positions evolutionary strategies as a hardware-friendly, lower-engineering-overhead alternative to RLHF/PPO-style gradient methods for model alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daria Korotyshova', 'Boris Shaposhnikov', 'Alexey Malakhov', 'Alexey Khokhulin', 'Nikita Surnachev', 'Kirill Ovcharenko', 'George Bredis', 'Alexey Gorbatovski', 'Viacheslav Sinii', 'Daniil Gavrilov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'evolutionary-strategies', 'LoRA', 'black-box-optimization', 'quantized-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04453</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAEs Are Good for Steering -- If You Select the Right Features</title><link>https://arxiv.org/abs/2505.20063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two SAE feature types: input features (capture input token patterns) and output features (have human-understandable effects on model outputs).&lt;/li&gt;&lt;li&gt;Proposes quantitative input and output scores to characterize and locate these feature types, and shows high input/output scores rarely co-occur.&lt;/li&gt;&lt;li&gt;Demonstrates that filtering SAE features by high output score yields 2–3x improvements in steering performance, making unsupervised SAE steering competitive with supervised methods.&lt;/li&gt;&lt;li&gt;Practical implication: feature selection based on output influence improves controllability of models without labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dana Arad', 'Aaron Mueller', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model-steering', 'interpretability', 'sparse-autoencoders', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20063</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Shape it Up! Restoring LLM Safety during Finetuning</title><link>https://arxiv.org/abs/2505.17196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dynamic Safety Shaping (DSS): a token/segment-level method to reinforce learning from safe parts of outputs while suppressing unsafe parts during finetuning.&lt;/li&gt;&lt;li&gt;Introduces STAR (Safety Trajectory Assessment of Response), using guardrail models to produce token-level safety scores that track safety risk across a response.&lt;/li&gt;&lt;li&gt;Presents STAR-DSS, a practical finetuning approach guided by STAR that reduces safety regressions across threats, datasets, and model families without degrading intended capabilities; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['ShengYun Peng', 'Pin-Yu Chen', 'Jianfeng Chi', 'Seongmin Lee', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM finetuning', 'safety alignment', 'token-level safety', 'guardrail models', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17196</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Error Slice Discovery via Manifold Compactness</title><link>https://arxiv.org/abs/2501.19032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes manifold compactness, a coherence metric for identifying semantically coherent error slices without relying on metadata or predefined slice labels.&lt;/li&gt;&lt;li&gt;Introduces MCSD (Manifold Compactness based error Slice Discovery), an algorithm that jointly optimizes for risk (error) and coherence using the proposed metric.&lt;/li&gt;&lt;li&gt;Validates the metric and method empirically on benchmark datasets and presents case studies showing improved discovery of interpretable error slices across tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yu', 'Hao Zou', 'Jiashuo Liu', 'Renzhe Xu', 'Yue He', 'Xingxuan Zhang', 'Peng Cui']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'model-debugging', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19032</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title><link>https://arxiv.org/abs/2501.12046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CEPAM, a mechanism that uses a rejection-sampled universal quantizer (RSUQ) to combine randomized quantization with prescribed noise distributions, enabling joint differential privacy and compression in federated learning.&lt;/li&gt;&lt;li&gt;Provides privacy adaptability allowing clients and the server to tune privacy-accuracy trade-offs.&lt;/li&gt;&lt;li&gt;Presents theoretical analysis of privacy guarantees and empirical evaluations (MNIST) showing improved utility/accuracy compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih Wei Ling', 'Chun Hei Michael Shiu', 'Youqi Wu', 'Jiande Sun', 'Cheuk Ting Li', 'Linqi Song', 'Weitao Xu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'privacy-preserving ML', 'quantization', 'communication efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.12046</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certified Defense on the Fairness of Graph Neural Networks</title><link>https://arxiv.org/abs/2311.02757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ELEGANT, a plug-and-play framework that provides certified defenses for the fairness level of arbitrary GNN backbones against input-graph perturbations.&lt;/li&gt;&lt;li&gt;Presents theoretical certification analysis guaranteeing that the fairness metric cannot be corrupted beyond specified perturbation budgets, without assumptions on GNN structure or parameter re-training.&lt;/li&gt;&lt;li&gt;Does not require modifying or retraining the underlying GNN; can be applied to deployed models.&lt;/li&gt;&lt;li&gt;Validates effectiveness via extensive experiments on real-world graph datasets across different GNN backbones and parameter settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushun Dong', 'Binchi Zhang', 'Hanghang Tong', 'Jundong Li']&lt;/li&gt;&lt;li&gt;Tags: ['GNN robustness', 'certified defense', 'fairness', 'adversarial perturbations', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.02757</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GShield: Mitigating Poisoning Attacks in Federated Learning</title><link>https://arxiv.org/abs/2512.19286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GShield, a defense for federated learning that detects and mitigates data poisoning by learning a benign-gradient distribution via clustering and Gaussian modeling during an initial round.&lt;/li&gt;&lt;li&gt;Uses the learned benign profile to selectively aggregate client updates that align with expected gradient patterns, isolating adversarial or low-quality clients.&lt;/li&gt;&lt;li&gt;Designed for non-IID federated settings and evaluated on tabular and image datasets, showing substantial robustness gains and improving targeted-class accuracy by 43%–65% versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sameera K. M.', 'Serena Nicolazzo', 'Antonino Nocera', 'Vinod P.', 'Rafidha Rehiman K. A']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'data poisoning', 'robust aggregation', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19286</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title><link>https://arxiv.org/abs/2512.19238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates bias against 93 stigmatized groups using SocialStigmaQA (37 social scenarios) across three LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B).&lt;/li&gt;&lt;li&gt;Finds stigmas rated as highly perilous by humans produce the most biased outputs (≈60%), while sociodemographic stigmas produce the least (≈11%).&lt;/li&gt;&lt;li&gt;Assesses guardrail/ moderation models (Granite Guardian, Llama Guard, Mistral Moderation): they reduce biased outputs (10.4%, 1.4%, 7.8%) but do not change which stigma features drive bias and often fail to detect biased intent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anna-Maria Gueorguieva', 'Aylin Caliskan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM bias', 'model safety', 'guardrails/moderation', 'benchmarking', 'social bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19238</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Recontextualization Mitigates Specification Gaming without Modifying the Specification</title><link>https://arxiv.org/abs/2512.19027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "recontextualization": generate completions from prompts that discourage misbehavior, then relabel them as if they were responses to permissive prompts to train resistance to misbehavior.&lt;/li&gt;&lt;li&gt;Shows reductions in specification gaming behaviors including prioritizing metrics over response quality, special-casing code to pass incorrect tests, lying, and sycophancy.&lt;/li&gt;&lt;li&gt;Claims the method mitigates reinforcement of misbehavior arising from misspecified training signals without modifying the original supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariana Azarbal', 'Victor Gillioz', 'Vladimir Ivanov', 'Bryce Woodworth', 'Jacob Drori', 'Nevan Wichers', 'Aram Ebtekar', 'Alex Cloud', 'Alexander Matt Turner']&lt;/li&gt;&lt;li&gt;Tags: ['specification-gaming', 'alignment-safety', 'behavioral-robustness', 'training-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19027</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title><link>https://arxiv.org/abs/2512.19025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that standard machine-unlearning metrics (which track performance drop on the exact unlearning dataset D_u) can be misleading for LLMs because models may forget verbatim examples yet retain generalized/semantically adjacent knowledge.&lt;/li&gt;&lt;li&gt;Proposes an automated stress-testing framework that constructs a surrogate dataset (\tilde{D}_u) semantically derived from D_u but distinct in embedding space to probe whether unlearning truly removed underlying knowledge.&lt;/li&gt;&lt;li&gt;Evaluates across three LLM families (Llama-3-8B, Qwen2.5-7B, Zephyr-7B-β), three datasets, and seven metrics, finding widespread inconsistencies and frequent overestimation of unlearning success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengrui Jia', 'Taoran Li', 'Jonas Guan', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM evaluation', 'privacy', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19025</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight, multi-stage defense pipeline against prompt injection and jailbreaks for LLM-based systems, with a core semantic filter using text normalization, TF-IDF, and a Linear SVM.&lt;/li&gt;&lt;li&gt;Core classifier achieves 93.4% accuracy and 96.5% specificity on held-out data and markedly reduces attack throughput with minimal compute overhead.&lt;/li&gt;&lt;li&gt;Demonstrates pipeline-level gains (accuracy from 35.1% to 93.4%) and &gt;10x lower latency (≈47s vs ≈450s) compared to a model-based moderator (ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluated on a curated corpus of over 30,000 labeled prompts covering benign, jailbreak, and application-layer injection examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM moderation', 'adversarial defenses', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title><link>https://arxiv.org/abs/2512.18901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gabliteration, an adaptive multi-directional neural weight modification method aiming to selectively change model behaviors while minimizing degradation elsewhere.&lt;/li&gt;&lt;li&gt;Key technical components: dynamic layer selection, regularized projection matrices, and adaptive scaling to control magnitude/direction of weight edits.&lt;/li&gt;&lt;li&gt;Validated empirically on gabliterated-v1 models (0.6B–4B) released on Hugging Face, claiming improved preservation of unrelated capabilities compared to prior abliteration methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G\\"okdeniz G\\"ulmez']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'weight modification', 'behavioral modification', 'safety/alignment', 'model tampering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18901</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SecureCode v2.0: a production-grade dataset of 1,215 security-focused coding examples (vulnerable + secure implementations) tied to real CVEs across 11 vulnerability categories and 11 programming languages.&lt;/li&gt;&lt;li&gt;Dataset split (989 train / 122 validation / 104 test), uses a 4-turn conversational structure to mirror developer–AI interactions, and includes automated validation tools and open-source benchmarking protocols.&lt;/li&gt;&lt;li&gt;Includes concrete attacks, defense-in-depth operational guidance (SIEM integration, Docker/AppArmor/WAF recommendations), language-specific testing approaches, and aims to train/benchmark security-aware code-generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset/Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['secure code generation', 'vulnerability dataset', 'AI safety', 'benchmarking', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title><link>https://arxiv.org/abs/2512.18462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Log-Frequency LMI (LF-LMI) to detect semantic artifacts/spurious correlations in NLI datasets.&lt;/li&gt;&lt;li&gt;Generates synthetic counterfactual contrast sets using LLM synthesis with multi-judge verification to ensure quality.&lt;/li&gt;&lt;li&gt;Introduces Dynamic Balanced Sampling, a rotating training-sampling strategy to mitigate catastrophic forgetting while rebalancing against artifacts.&lt;/li&gt;&lt;li&gt;Reports improved robustness: consistency on a benchmark increased from 63.5% to 81.0% while preserving 88.4% in-domain accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Christopher Rom\\'an Jaimes"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'counterfactual data augmentation', 'dataset synthesis', 'bias/spurious-correlation mitigation', 'NLI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18462</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neural Proofs for Sound Verification and Control of Complex Systems</title><link>https://arxiv.org/abs/2512.18389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'neural proofs' combining hand-crafted proof rules with neural network-based certificates to verify temporal specifications over stochastic dynamical models and reactive programs.&lt;/li&gt;&lt;li&gt;Uses an inductive loop: sample-driven training of neural networks plus SMT (SAT-modulo-theory) queries to generalize and formally discharge certificates.&lt;/li&gt;&lt;li&gt;Can synthesize provably-correct state-feedback policies/controllers that, together with neural certificates, guarantee satisfaction of specifications for complex cyber-physical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessandro Abate']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'AI-safety', 'cyber-physical-systems', 'neural-certificates', 'SMT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18389</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</title><link>https://arxiv.org/abs/2512.18228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GraphRank, a test input prioritization framework for Graph Neural Networks to reduce labeling cost while uncovering more model failures.&lt;/li&gt;&lt;li&gt;Introduces model-agnostic attributes to complement model-aware attributes and aggregates attributes from neighboring nodes using graph structure.&lt;/li&gt;&lt;li&gt;Uses an iteratively trained binary classifier as a ranking model to prioritize unlabeled inputs for annotation and testing.&lt;/li&gt;&lt;li&gt;Evaluated extensively and found to outperform existing prioritization techniques on GNN testing tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lichen Yang', 'Qiang Wang', 'Zhonghao Yang', 'Daojing He', 'Yu Li']&lt;/li&gt;&lt;li&gt;Tags: ['Graph Neural Networks', 'Test Input Prioritization', 'Safety / Reliability Evaluation', 'Robustness Testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18228</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.17911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RMLLMU-Bench, the first benchmark for unlearning in Reasoning Multimodal LLMs with metrics for both reasoning-level leakage and reasoning retention.&lt;/li&gt;&lt;li&gt;Evaluates existing unlearning methods and finds they either leave substantial reasoning traces or degrade general reasoning ability.&lt;/li&gt;&lt;li&gt;Proposes R-MUSE, a training-free, inference-time intervention that steers internal representations to forget answers and chain-of-thought traces while preserving overall reasoning; shows improved trade-off on the benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongji Li', 'Junchi yao', 'Manjiang Yu', 'Priyanka Singh', 'Xue Li', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'reasoning leakage', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17911</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</title><link>https://arxiv.org/abs/2512.19472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MACS, a unified post-hoc framework that analyzes intermediate activations to produce classification-maps and a derived confidence score without retraining the model.&lt;/li&gt;&lt;li&gt;Uses the score for three tasks: confidence estimation, out-of-distribution (OOD) detection, and adversarial-attack detection, unifying these problems in one approach.&lt;/li&gt;&lt;li&gt;Evaluated on VGG16 and ViT-B16, reporting performance that surpasses state-of-the-art methods while incurring a fraction of their computational overhead.&lt;/li&gt;&lt;li&gt;Emphasizes applicability to already-deployed/pretrained models, making it practical for security and safety monitoring of vision models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Capelli', 'Leandro de Souza Rosa', 'Gianluca Setti', 'Mauro Mangia', 'Riccardo Rovatti']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'OOD detection', 'confidence estimation', 'post-hoc methods', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19472</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimizer Dynamics at the Edge of Stability with Differential Privacy</title><link>https://arxiv.org/abs/2512.19019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how differential privacy mechanisms (per-example gradient clipping and Gaussian noise) alter optimizer dynamics, especially Edge of Stability (EoS) behavior, for GD and Adam variants.&lt;/li&gt;&lt;li&gt;Finds that DP generally reduces sharpness and can prevent optimizers from reaching classical stability thresholds, but EoS-like patterns still persist; high learning rates and large privacy budgets can approach or exceed thresholds.&lt;/li&gt;&lt;li&gt;Highlights that DP introduces unpredictability into neural network optimization, with implications for training behavior and the interplay between privacy guarantees and optimization stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayana Hussain', 'Ricky Fang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'optimization-dynamics', 'edge-of-stability', 'training-robustness', 'privacy-preserving-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19019</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</title><link>https://arxiv.org/abs/2512.18957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an online distributionally robust reinforcement learning (DR-RL) algorithm using general function approximation that learns purely via environment interaction (no generative model or offline data).&lt;/li&gt;&lt;li&gt;Targets robustness to train-deployment mismatch by optimizing worst-case performance over a total variation uncertainty set of transition dynamics.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: near-optimal sublinear regret bounds, demonstrating sample efficiency in high-dimensional settings.&lt;/li&gt;&lt;li&gt;Aims to scale DR-RL beyond tabular settings to practical, high-dimensional domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debamita Ghosh', 'George K. Atia', 'Yue Wang']&lt;/li&gt;&lt;li&gt;Tags: ['distributionally robust RL', 'robustness', 'online learning', 'function approximation', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18957</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems</title><link>https://arxiv.org/abs/2512.18932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DPSR, a three-stage denoising pipeline for differentially private recommender systems: information-theoretic noise calibration, collaborative-filtering denoising, and low-rank matrix completion.&lt;/li&gt;&lt;li&gt;All denoising occurs after noise injection to preserve differential privacy via the post-processing immunity theorem.&lt;/li&gt;&lt;li&gt;Reports consistent RMSE improvements (5.57%–9.23%) over standard Laplace/Gaussian mechanisms across ε∈[0.1,10], and even outperforms a non-private baseline at ε=1.0, attributing gains to denoising acting as regularization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarwan Ali']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'recommender systems', 'privacy-preserving ML', 'denoising', 'matrix completion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18932</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Generating Risky Samples with Conformity Constraints via Diffusion Models</title><link>https://arxiv.org/abs/2512.18722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RiskyDiff, a diffusion-based method to generate risky samples beyond existing datasets by enforcing category conformity using text and image embeddings.&lt;/li&gt;&lt;li&gt;Introduces a conformity score plus embedding screening and risky gradient guidance to increase both the riskiness and label fidelity of generated samples.&lt;/li&gt;&lt;li&gt;Reports improvements over prior methods on risk degree, generation quality, and conformity, and shows augmenting training data with high-conformity risky samples improves generalization/robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yu', 'Hao Zou', 'Xingxuan Zhang', 'Zhengyi Wang', 'Yue He', 'Kehan Li', 'Peng Cui']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial example generation', 'Robustness &amp; safety evaluation', 'Red teaming / stress testing', 'Diffusion models', 'Data augmentation for robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18722</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</title><link>https://arxiv.org/abs/2512.18317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a trustworthy deep reinforcement learning framework for controlling industrial compressed air systems that prioritizes safe and energy-efficient operation.&lt;/li&gt;&lt;li&gt;Introduces a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP feature attribution to validate policy behavior.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows the learned policy respects system boundaries, anticipates demand, reduces overpressure, and achieves ~4% energy savings versus the installed controller.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Bezold', 'Patrick Wagner', 'Jakob Hofmann', 'Marco Huber', 'Alexander Sauer']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'safety-evaluation', 'explainability', 'industrial-control', 'trustworthy-ai']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18317</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</title><link>https://arxiv.org/abs/2512.18309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework (ESAI) that embeds safety/alignment constraints as differentiable internal latent embeddings within multi-agent RL agents rather than using external reward shaping.&lt;/li&gt;&lt;li&gt;Integrates four mechanisms: differentiable counterfactual alignment penalties, alignment-weighted perceptual attention, Hebbian associative memory for temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of stability (Lipschitz and spectral constraints), contraction behavior, computational complexity, and fairness–performance tradeoffs.&lt;/li&gt;&lt;li&gt;No empirical evaluation — positions the work as a conceptual/theoretical contribution and identifies open questions about convergence, embedding dimensionality, and scaling to high-dimensional environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Rathva', 'Ojas Srivastava', 'Pruthwik Mishra']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'multi-agent reinforcement learning', 'internal alignment embeddings', 'counterfactual reasoning', 'safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18309</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models</title><link>https://arxiv.org/abs/2512.18035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first comprehensive benchmark for evaluating privacy vulnerabilities arising from selective forgetting (machine unlearning) in large language models.&lt;/li&gt;&lt;li&gt;Systematically evaluates privacy leakage across diverse victim data, unlearning methods, state-of-the-art unlearning attacks, and model architectures.&lt;/li&gt;&lt;li&gt;Identifies key factors that influence unlearning-induced privacy leakage and provides insights to improve faithful privacy assessments for deployed unlearning applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Qian', 'Chenxu Zhao', 'Yangyi Li', 'Mengdi Huai']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy attacks', 'benchmarking', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18035</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</title><link>https://arxiv.org/abs/2512.17370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TakeAD, a preference-based post-optimization pipeline to fine-tune pre-trained end-to-end imitation-learning (IL) driving policies using expert takeover (disengagement) data.&lt;/li&gt;&lt;li&gt;Combines iterative DAgger (to teach recovery/handling of disengagement states via imitation) with Direct Preference Optimization (DPO) (to align behavior with expert preferences) in multiple iterations.&lt;/li&gt;&lt;li&gt;Demonstrates improved closed-loop driving performance and reduced disengagements on the Bench2Drive benchmark, with ablations showing contributions of each component.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deqing Liu', 'Yinfeng Gao', 'Deheng Qian', 'Qichao Zhang', 'Xiaoqing Ye', 'Junyu Han', 'Yupeng Zheng', 'Xueyi Liu', 'Zhongpu Xia', 'Dawei Ding', 'Yifeng Pan', 'Dongbin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'preference-alignment', 'imitation-learning', 'closed-loop-safety', 'post-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17370</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title><link>https://arxiv.org/abs/2512.16912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes exploration-exploitation trade-offs in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that spurious rewards can reduce policy entropy via clipping bias, producing more confident/deterministic outputs and performance gains.&lt;/li&gt;&lt;li&gt;Demonstrates entropy minimization alone is insufficient, and proposes a reward-misalignment model explaining when spurious rewards help beyond contaminated settings.&lt;/li&gt;&lt;li&gt;Provides principles for designing RLVR training to reconcile the paradoxical effects of discouraging both exploitation and exploration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter Chen', 'Xiaopeng Li', 'Ziniu Li', 'Wotao Yin', 'Xi Chen', 'Tianyi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'reward-misalignment', 'entropy', 'training dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16912</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams</title><link>https://arxiv.org/abs/2512.16280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Interviews with 145 insiders and 5 victims indicate LLMs are already widely deployed in romance-baiting operations; authors estimate 87% of scam labor consists of conversational tasks susceptible to automation.&lt;/li&gt;&lt;li&gt;A blinded week-long conversation study found an LLM agent elicited greater trust (p = 0.007) and achieved higher compliance than human operators (46% vs. 18%).&lt;/li&gt;&lt;li&gt;Evaluation of commercial safety filters showed 0.0% detection of romance-baiting dialogues, indicating current defenses fail to catch these abuses.&lt;/li&gt;&lt;li&gt;Authors conclude that romance-baiting scams may be amenable to full-scale LLM automation and that existing safeguards are inadequate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gilad Gressel', 'Rahul Pankajakshan', 'Shir Rozenfeld', 'Ling Li', 'Ivan Franceschini', 'Krishnashree Achuthan', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['LLM misuse', 'social engineering', 'safety/evasion', 'detection/defenses', 'empirical study']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16280</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</title><link>https://arxiv.org/abs/2512.15109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Large Artificial Models (LAMs) to create intelligent base station agents (IBSAs) that integrate perception, communication, and computation for 6G systems.&lt;/li&gt;&lt;li&gt;Presents an IBSA architecture (perception-cognition-execution) with cloud-edge-end collaboration and parameter-efficient adaptation, and reviews BS evolution and hardware/deployment considerations.&lt;/li&gt;&lt;li&gt;Analyzes two application scenarios (cooperative vehicle-road perception and UAV safety monitoring) and surveys enabling technologies including LAM design, efficient edge-cloud inference, multimodal perception/actuation, and trustworthy security and governance.&lt;/li&gt;&lt;li&gt;Offers a holistic evaluation framework and benchmark considerations covering communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency, and identifies open challenges (benchmarks, continual adaptation, trustworthy decision-making, standardization).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoran Li', 'Zhen Gao', 'Xinhua Liu', 'Zheng Wang', 'Xiaotian Zhou', 'Lei Liu', 'Yongpeng Wu', 'Wei Feng', 'Yongming Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'trustworthy-decision-making', 'robustness', 'security-governance', 'edge-ai']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15109</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates Profile Pollution Attacks (PPA) on sequential recommender systems that subtly modify partial user interaction sequences to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Proposes CREAT: a bi-level constrained reinforcement learning framework combining pattern-inversion rewards and distribution-consistency rewards to balance attack efficacy and stealthiness.&lt;/li&gt;&lt;li&gt;Introduces constrained group relative reinforcement learning with dynamic barrier constraints and group-shared experience replay to enable step-wise, low-detectability perturbations.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and detectability of the attack in experiments, demonstrating strong targeted pollution with minimal distribution shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiang Chen', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning / profile pollution', 'recommender systems', 'reinforcement learning', 'robustness/stealthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2511.02376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoAdv, a training-free framework for automated multi-turn jailbreaking of LLMs that adaptively crafts prompts.&lt;/li&gt;&lt;li&gt;Combines three adaptive mechanisms: a pattern manager (learns from successful attacks), a temperature manager (adjusts sampling), and a two-phase rewriting strategy (disguise then iteratively refine).&lt;/li&gt;&lt;li&gt;Reports high attack success (up to 95% on Llama-3.1-8B within six turns) and shows multi-turn attacks outperform single-turn baselines across commercial and open-source models.&lt;/li&gt;&lt;li&gt;Concludes that alignment and safety mechanisms optimized for single-turn interactions are insufficient, highlighting need for multi-turn-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashray Reddy', 'Andrew Zagula', 'Nicholas Saban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02376</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Agentic Security: Applications, Threats and Defenses</title><link>https://arxiv.org/abs/2510.06445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive, first-of-its-kind survey that organizes agentic security around Applications, Threats, and Defenses and catalogs &gt;160 papers.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of threats to agentic systems, defensive measures, and cybersecurity applications of autonomous agents.&lt;/li&gt;&lt;li&gt;Analyzes trends in agent architectures and modality/model coverage and identifies critical research gaps and open problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asif Shahriar', 'Md Nafiu Rahman', 'Sadif Ahmed', 'Farig Sadeque', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['agentic security', 'threat taxonomy', 'defenses', 'red teaming', 'security survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06445</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication</title><link>https://arxiv.org/abs/2508.11733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SafeSieve is a progressive, adaptive pruning algorithm for LLM-based multi-agent communication that transitions from LLM-based semantic heuristics to experience-driven refinement using accumulated performance feedback.&lt;/li&gt;&lt;li&gt;It employs 0-extension clustering to preserve structurally coherent agent groups while removing ineffective links, aiming to reduce redundant communication and token overhead without requiring GPUs.&lt;/li&gt;&lt;li&gt;Experimental results report high task accuracy (94.01% average), token usage reductions (12.4%–27.8%), cost savings in heterogeneous deployments, and measured robustness under prompt injection attacks (1.23% average accuracy drop).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruijia Zhang', 'Xinyan Zhao', 'Ruixiang Wang', 'Sigen Chen', 'Guibin Zhang', 'An Zhang', 'Kun Wang', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt injection', 'LLM multi-agent systems', 'Robustness', 'Communication pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11733</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title><link>https://arxiv.org/abs/2508.07745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chimera, an LLM-based multi-agent framework that simulates realistic benign and malicious insider behaviors across enterprise environments to generate comprehensive system logs.&lt;/li&gt;&lt;li&gt;Implements fine-grained agent roles, group interactions, meetings, and scheduling to capture organizational dynamics and models 15 insider attack patterns derived from real incidents.&lt;/li&gt;&lt;li&gt;Releases ChimeraLog, a new synthetic dataset validated by human studies and quantitative analysis; existing insider-threat detectors perform worse on it, indicating higher realism and challenge.&lt;/li&gt;&lt;li&gt;Shows models trained on ChimeraLog generalize well under distribution shift, highlighting utility for developing and evaluating insider-threat detection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiongchi Yu', 'Xiaofei Xie', 'Qiang Hu', 'Yuhan Ma', 'Ziming Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['insider-threat', 'synthetic-dataset', 'LLM-multi-agent', 'security-benchmarking', 'cybersecurity-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07745</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAEs Are Good for Steering -- If You Select the Right Features</title><link>https://arxiv.org/abs/2505.20063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two SAE feature types: input features (capture input token patterns) and output features (have human-understandable effects on model outputs).&lt;/li&gt;&lt;li&gt;Proposes quantitative input and output scores to characterize and locate these feature types, and shows high input/output scores rarely co-occur.&lt;/li&gt;&lt;li&gt;Demonstrates that filtering SAE features by high output score yields 2–3x improvements in steering performance, making unsupervised SAE steering competitive with supervised methods.&lt;/li&gt;&lt;li&gt;Practical implication: feature selection based on output influence improves controllability of models without labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dana Arad', 'Aaron Mueller', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model-steering', 'interpretability', 'sparse-autoencoders', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20063</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks</title><link>https://arxiv.org/abs/2503.11185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a fundamental vulnerability where LLM hidden-state separability between safe and harmful outputs diminishes during generation, impairing the model's ability to detect evolving malicious intent.&lt;/li&gt;&lt;li&gt;Proposes DEEPALIGN, a contrastive hidden-state steering defense applied at generation midpoint to amplify separation between harmful and benign hidden states, enabling continued intrinsic toxicity detection and intervention.&lt;/li&gt;&lt;li&gt;Presents empirical evaluation across multiple LLM architectures and scales showing DEEPALIGN reduces success rates of nine jailbreak attacks to near-zero or minimal while preserving utility (reduced over-rejection and &lt;1% standard task performance drop).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingjie Zhang', 'Tong Liu', 'Zhe Zhao', 'Guozhu Meng', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaks', 'hidden-state analysis', 'safety alignment', 'contrastive learning', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11185</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</title><link>https://arxiv.org/abs/2409.01382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative study detecting LLM-generated Python code (14,485 functions, 11,913 classes) from four LLMs (GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, GPT-OSS) using interpretable software metrics and CatBoost classifiers.&lt;/li&gt;&lt;li&gt;Finds granularity (function vs. class) drives detectability far more than model differences, with largely disjoint structural signatures and a single universal feature (Comment-to-Code Ratio) whose effect size varies by model.&lt;/li&gt;&lt;li&gt;Demonstrates poor cross-model generalization of detectors and highlights that GPT-3.5 is an outlier (very high detectability) compared to contemporary models, implying robustness issues for single-model detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Musfiqur Rahman', 'SayedHassan Khatoonabadi', 'Ahmad Abdellatif', 'Emad Shihab']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-detection', 'code-generation', 'robustness', 'explainable-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01382</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ViGoR, a framework using fine-grained reward modeling to improve visual grounding in large vision-language models (LVLMs), aiming to reduce hallucination, missing content, and incorrect attribute/relationship inference.&lt;/li&gt;&lt;li&gt;Trains using cheaper human evaluations combined with automated signals rather than full supervision, improving efficiency.&lt;/li&gt;&lt;li&gt;Provides empirical results showing improvements over pre-trained baselines and releases a human-annotated dataset of 15,440 image–text pairs with fine-grained evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siming Yan', 'Min Bai', 'Weifeng Chen', 'Xiong Zhou', 'Qixing Huang', 'Li Erran Li']&lt;/li&gt;&lt;li&gt;Tags: ['visual-grounding', 'alignment', 'robustness', 'safety-evaluation', 'reward-modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.06118</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors</title><link>https://arxiv.org/abs/2401.16310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of seven LLMs under five prompt strategies for security code review, compared against state-of-the-art static analysis tools.&lt;/li&gt;&lt;li&gt;Finds LLMs significantly outperform static analyzers; DeepSeek-R1 and GPT-4 are top performers with different optimal prompts (commit+CoT for DeepSeek-R1; CWE list for GPT-4).&lt;/li&gt;&lt;li&gt;Analyzes error types and quality issues (GPT-4: vague/instruction-following problems; DeepSeek-R1: inaccurate code details) and factors affecting performance (shorter files and security annotations improve detection).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Yu', 'Peng Liang', 'Yujia Fu', 'Amjed Tahir', 'Mojtaba Shahin', 'Chong Wang', 'Yangxiao Cai']&lt;/li&gt;&lt;li&gt;Tags: ['LLMs', 'Security code review', 'Vulnerability detection', 'Prompt engineering', 'Model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.16310</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Discovering and Learning Probabilistic Models of Black-Box AI Capabilities</title><link>https://arxiv.org/abs/2512.16733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a method to learn PDDL-style probabilistic symbolic models of black-box AI planning/decision-making capabilities.&lt;/li&gt;&lt;li&gt;Uses Monte-Carlo tree search to generate test tasks, collect interaction data, and prune hypothesis space to produce sound, complete, and convergent models.&lt;/li&gt;&lt;li&gt;Learned models capture actions, execution preconditions, and probabilistic outcomes — yielding an interpretable representation of agent capabilities and failure modes.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple black-box AI systems demonstrates efficiency and accuracy of the approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Bramblett', 'Rushang Karia', 'Adrian Ciotinga', 'Ruthvick Suresh', 'Pulkit Verma', 'YooJung Choi', 'Siddharth Srivastava']&lt;/li&gt;&lt;li&gt;Tags: ['black-box model analysis', 'safety evaluation', 'capability modeling', 'probabilistic planning', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16733</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Decision-Theoretic Approach for Managing Misalignment</title><link>https://arxiv.org/abs/2512.15584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a formal decision-theoretic framework for deciding when to delegate decisions to imperfectly aligned AI, explicitly trading off value misalignment, epistemic accuracy, and the agent's reach.&lt;/li&gt;&lt;li&gt;Distinguishes between universal delegation (requires near-perfect alignment and full epistemic trust) and context-specific delegation (can be optimal despite significant misalignment if accuracy or reach provide overall benefit).&lt;/li&gt;&lt;li&gt;Proposes a novel ex ante scoring framework to quantify whether an AI is 'aligned enough' in a given context, shifting focus from perfect alignment to managing delegation risk under uncertainty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel A. Herrmann', 'Abinav Chari', 'Isabelle Qian', 'Sree Sharvesh', 'B. A. Levinstein']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'delegation', 'decision-theory', 'AI-safety', 'risk-assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15584</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title><link>https://arxiv.org/abs/2512.13142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study testing 627 diverse personas across five leading LLMs using the Individual Level Abortion Stigma Scale (ILAS) to assess representation of abortion stigma at cognitive, interpersonal, and structural levels.&lt;/li&gt;&lt;li&gt;Finds LLMs lack coherent multilevel understanding: they overestimate interpersonal stigma, underestimate cognitive stigma, assume uniform community condemnation, introduce demographic biases absent in human data, miss validated stigma–secrecy relationships, and produce internal contradictions.&lt;/li&gt;&lt;li&gt;Argues current alignment yields appropriate language but not genuine multilevel understanding; recommends new design goals (multilevel coherence), continuous safety evaluation/auditing, governance/regulation (mandatory audits, accountability, deployment limits), and AI literacy in high‑stakes domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anika Sharma', 'Malavika Mampally', 'Chidaksh Ravuru', 'Kandyce Brennan', 'Neil Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'bias-and-robustness', 'high-stakes-health', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13142</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents</title><link>https://arxiv.org/abs/2509.00251</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Instruction-Level Weight Shaping (ILWS): system instructions are treated as auditable pseudo-parameters that are updated post-session via a Reflection Engine which proposes typed deltas (instructions, user prefs, tools).&lt;/li&gt;&lt;li&gt;Deltas are version-controlled, evaluated with user ratings, auto-repaired on first failure, rolled back on repeated failure, and—when matured—compiled into a rating-weighted synthetic set and distilled into model weights to convert prompt-space improvements into parameter updates without downtime.&lt;/li&gt;&lt;li&gt;Reports operational gains (enterprise support: 2.4–5.0× throughput, ~80% fewer audited hallucinations; Adobe Commerce Cloud PoC: 4–5× more tickets/hour, ~80% lower time per ticket) and emphasizes governance by keeping changes at the instruction layer until controlled distillation.&lt;/li&gt;&lt;li&gt;Positioned for dynamic, high-stakes domains (legal, medical, engineering) and touches on alignment/governance concerns of self-updating agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rimom Costa']&lt;/li&gt;&lt;li&gt;Tags: ['LLM self-improvement', 'Alignment / governance', 'Prompt-to-weight distillation', 'Safety / robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00251</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Revealing Nuanced Biases in Medical LLMs</title><link>https://arxiv.org/abs/2507.21176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework that combines knowledge graphs with auxiliary (agentic) LLMs to systematically generate and apply adversarial (red-team) perturbations to reveal nuanced biases in medical LLMs.&lt;/li&gt;&lt;li&gt;Introduces customized multi-hop KG characterizations to create more effective red-teaming questions and leverages agentic LLMs to craft and refine adversarial prompts.&lt;/li&gt;&lt;li&gt;Evaluates the method across three datasets, six LLMs, and five bias types, demonstrating improved capability and scalability compared to common bias evaluation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzana Islam Adiba', 'Rahmatollah Beheshti']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Bias evaluation', 'Medical AI', 'Adversarial prompting', 'Knowledge graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21176</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization</title><link>https://arxiv.org/abs/2506.11712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Symmetric Multimodal Preference Optimization (SymMPO), extending Direct Preference Optimization (DPO) to symmetric preference learning with direct response-pair supervision to improve visual understanding.&lt;/li&gt;&lt;li&gt;Introduces a preference margin consistency loss to enforce quantitative regulation of preference gaps between symmetric pairs, aiming for a more rigorous objective than prior contrastive methods.&lt;/li&gt;&lt;li&gt;Claims theoretical alignment with standard DPO while providing stronger supervision for multimodal hallucination mitigation.&lt;/li&gt;&lt;li&gt;Reports superior performance on five benchmarks, validating effectiveness at reducing hallucination in MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqi Liu', 'Xuemeng Song', 'Jiaxi Li', 'Yinwei Wei', 'Na Zheng', 'Jianhua Yin', 'Liqiang Nie']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'alignment/safety', 'preference optimization', 'DPO/theoretical alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11712</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback</title><link>https://arxiv.org/abs/2505.23950</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InterMT, a human-feedback grounded preference dataset for multi-turn, interleaved multimodal interactions (15.6k prompts, 52.6k dialogue instances, 32.4k preference pairs).&lt;/li&gt;&lt;li&gt;Focuses on aligning multimodal large models (MLLMs) via expert-annotated, fine-grained preference dimensions and an agentic, tool-augmented workflow to generate multi-turn QA instances.&lt;/li&gt;&lt;li&gt;Provides InterMT-Bench to evaluate MLLMs' ability to assist judges on multi-turn multimodal tasks and studies judge-model scaling and judge moderation applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking (Dataset &amp; Evaluation)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyuan Chen', 'Donghai Hong', 'Jiaming Ji', 'Jiacheng Zheng', 'Bowen Dong', 'Jiayi Zhou', 'Kaile Wang', 'Juntao Dai', 'Xuyao Wang', 'Wenqi Chen', 'Qirui Zheng', 'Wenxin Li', 'Sirui Han', 'Yike Guo', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-feedback', 'dataset', 'multimodal', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23950</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling</title><link>https://arxiv.org/abs/2505.11792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Solver-Informed Reinforcement Learning (SIRL) that uses external optimization solvers as verifiers to provide verifiable rewards (syntax, feasibility, solution quality) for RL training of LLMs.&lt;/li&gt;&lt;li&gt;Uses solver-based verification and an instance-enhanced self-consistency method to synthesize high-quality training data and reduce hallucinations in generated optimization models.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on public benchmarks for generating accurate, executable optimization models (LP files) from natural language.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitian Chen', 'Jingfan Xia', 'Siyu Shao', 'Dongdong Ge', 'Yinyu Ye']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'alignment', 'reinforcement learning (RLHF-style)', 'model verification', 'optimization modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11792</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Imagining and building wise machines: The centrality of AI metacognition</title><link>https://arxiv.org/abs/2411.02478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI lacks 'wisdom' and that metacognitive capacities (e.g., intellectual humility, perspective-taking, context-adaptability) are central to remedying that gap.&lt;/li&gt;&lt;li&gt;Analyzes human wisdom as object-level heuristics plus metacognitive strategies and proposes an AI counterpart aimed at robustness, explainability, cooperation, and reduced misalignment.&lt;/li&gt;&lt;li&gt;Discusses how metacognitive capabilities could be benchmarked, trained, and implemented to yield safer and more robust AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel G. B. Johnson', 'Amir-Hossein Karimi', 'Yoshua Bengio', 'Nick Chater', 'Tobias Gerstenberg', 'Kate Larson', 'Sydney Levine', 'Melanie Mitchell', 'Iyad Rahwan', 'Bernhard Sch\\"olkopf', 'Igor Grossmann']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Metacognition', 'Robustness', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.02478</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles</title><link>https://arxiv.org/abs/2512.19564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the setup and results of the 4th CommonRoad Motion Planning Competition (2024) using the CommonRoad benchmark suite.&lt;/li&gt;&lt;li&gt;Evaluates motion planners in highway and urban scenarios with diverse traffic participants, measuring efficiency, safety, comfort, and traffic-rule compliance.&lt;/li&gt;&lt;li&gt;Compares representative high-performing planners from the 2023 and 2024 editions within an open-source, reproducible framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanliang Huang', 'Xia Yan', 'Peiran Yin', 'Zhenduo Zhang', 'Zeyan Shao', 'Youran Wang', 'Haoliang Huang', 'Matthias Althoff']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous vehicles', 'motion planning', 'benchmarking', 'safety evaluation', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19564</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</title><link>https://arxiv.org/abs/2512.19562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents REALM, a high-fidelity simulation environment and benchmark to evaluate generalization of Vision-Language-Action (VLA) models for robotic manipulation with sim-to-real validation.&lt;/li&gt;&lt;li&gt;Provides 15 perturbation factors, 7 manipulation skills, and &gt;3,500 objects to systematically test robustness and quantify failure modes under varied conditions.&lt;/li&gt;&lt;li&gt;Evaluates several VLA models (π0, π0-FAST, GR00T N1.5) and finds persistent generalization and robustness challenges, arguing simulation is a valuable proxy for real-world safety/robustness analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Sedlacek', 'Pavlo Yefanov', 'Georgy Ponimatkin', 'Jai Bardhan', 'Simon Pilc', 'Mederic Fourmy', 'Evangelos Kazakos', 'Cees G. M. Snoek', 'Josef Sivic', 'Vladimir Petrik']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'simulation-to-real', 'benchmarking', 'robotic manipulation', 'vision-language-action']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19562</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</title><link>https://arxiv.org/abs/2512.19472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MACS, a unified post-hoc framework that analyzes intermediate activations to produce classification-maps and a derived confidence score without retraining the model.&lt;/li&gt;&lt;li&gt;Uses the score for three tasks: confidence estimation, out-of-distribution (OOD) detection, and adversarial-attack detection, unifying these problems in one approach.&lt;/li&gt;&lt;li&gt;Evaluated on VGG16 and ViT-B16, reporting performance that surpasses state-of-the-art methods while incurring a fraction of their computational overhead.&lt;/li&gt;&lt;li&gt;Emphasizes applicability to already-deployed/pretrained models, making it practical for security and safety monitoring of vision models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Capelli', 'Leandro de Souza Rosa', 'Gianluca Setti', 'Mauro Mangia', 'Riccardo Rovatti']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'OOD detection', 'confidence estimation', 'post-hoc methods', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19472</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models</title><link>https://arxiv.org/abs/2512.19297</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack tailored to open-weight LoRA adapters that operates without access to original training data.&lt;/li&gt;&lt;li&gt;Uses a coverage-guided data generation pipeline to synthesize task-aligned inputs via behavioral exploration and a causal-guided detoxification strategy to merge poisoned and clean adapters while preserving task-critical neurons.&lt;/li&gt;&lt;li&gt;Enables post-training control of attack intensity through causal influence-based weight allocation, achieves high attack success rates, substantially lowers false trigger rates (50–70% reduction vs baselines), and shows robustness to state-of-the-art defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linzhi Chen', 'Yang Sun', 'Hongru Wei', 'Yuqi Chen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LoRA adapters', 'model poisoning', 'data-free attacks', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19297</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title><link>https://arxiv.org/abs/2512.19238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates bias against 93 stigmatized groups using SocialStigmaQA (37 social scenarios) across three LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B).&lt;/li&gt;&lt;li&gt;Finds stigmas rated as highly perilous by humans produce the most biased outputs (≈60%), while sociodemographic stigmas produce the least (≈11%).&lt;/li&gt;&lt;li&gt;Assesses guardrail/ moderation models (Granite Guardian, Llama Guard, Mistral Moderation): they reduce biased outputs (10.4%, 1.4%, 7.8%) but do not change which stigma features drive bias and often fail to detect biased intent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anna-Maria Gueorguieva', 'Aylin Caliskan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM bias', 'model safety', 'guardrails/moderation', 'benchmarking', 'social bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19238</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title><link>https://arxiv.org/abs/2512.19025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that standard machine-unlearning metrics (which track performance drop on the exact unlearning dataset D_u) can be misleading for LLMs because models may forget verbatim examples yet retain generalized/semantically adjacent knowledge.&lt;/li&gt;&lt;li&gt;Proposes an automated stress-testing framework that constructs a surrogate dataset (\tilde{D}_u) semantically derived from D_u but distinct in embedding space to probe whether unlearning truly removed underlying knowledge.&lt;/li&gt;&lt;li&gt;Evaluates across three LLM families (Llama-3-8B, Qwen2.5-7B, Zephyr-7B-β), three datasets, and seven metrics, finding widespread inconsistencies and frequent overestimation of unlearning success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengrui Jia', 'Taoran Li', 'Jonas Guan', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM evaluation', 'privacy', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19025</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight, multi-stage defense pipeline against prompt injection and jailbreaks for LLM-based systems, with a core semantic filter using text normalization, TF-IDF, and a Linear SVM.&lt;/li&gt;&lt;li&gt;Core classifier achieves 93.4% accuracy and 96.5% specificity on held-out data and markedly reduces attack throughput with minimal compute overhead.&lt;/li&gt;&lt;li&gt;Demonstrates pipeline-level gains (accuracy from 35.1% to 93.4%) and &gt;10x lower latency (≈47s vs ≈450s) compared to a model-based moderator (ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluated on a curated corpus of over 30,000 labeled prompts covering benign, jailbreak, and application-layer injection examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM moderation', 'adversarial defenses', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</title><link>https://arxiv.org/abs/2512.18999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares an end-to-end LLM chatbot with a modular pipeline (task decomposition, semantic clustering, flow management) for medical follow-up dialogs.&lt;/li&gt;&lt;li&gt;Modular approach substantially improves dialog stability and information extraction accuracy versus end-to-end LLMs.&lt;/li&gt;&lt;li&gt;Reports reductions in dialogue turns (46.73%) and token consumption (80%–87.5%), arguing for external control mechanisms in high-stakes deployments.&lt;/li&gt;&lt;li&gt;Emphasizes the need for structured process control when deploying LLMs in medical follow-up to ensure reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinyan Liu', 'Zikang Chen', 'Qinchuan Wang', 'Tan Xie', 'Heming Zheng', 'Xudong Lv']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Medical NLP', 'Deployment safety', 'Dialog systems', 'Process control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18999</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</title><link>https://arxiv.org/abs/2512.18809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedVideoMAE: an on-device federated learning framework for video violence detection combining self-supervised VideoMAE representations, LoRA parameter-efficient adaptation, and defense‑in‑depth privacy protections (DP‑SGD + secure aggregation).&lt;/li&gt;&lt;li&gt;Reduces trainable parameters to 5.5M (~3.5% of a 156M backbone) and cuts communication cost by ~28.3× compared to full-model federated learning.&lt;/li&gt;&lt;li&gt;Reports utility–privacy tradeoffs on RWF-2000 with 40 clients: 77.25% accuracy without DP and 65–66% accuracy under strong differential privacy, with configurable privacy budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyuan Tao', 'Chuanzhi Xu', 'Sandaru Jayawardana', 'Wei Bao', 'Kanchana Thilakarathna', 'Teng Joon Lim']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'secure aggregation', 'privacy-preserving ML', 'video moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18809</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs</title><link>https://arxiv.org/abs/2512.18797</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using quantum-kernel SVMs (QSVMs) on mel-spectrogram features to detect synthetic speech/ audio deepfakes, comparing directly to classical SVMs with identical preprocessing and settings.&lt;/li&gt;&lt;li&gt;Evaluates across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, In-the-Wild) with stratified 5-fold cross-validation, reporting consistently lower equal-error-rates (EER) and substantial absolute reductions in false-positive rates at the EER operating point.&lt;/li&gt;&lt;li&gt;The only change is the kernel — no extra trainable parameters or different feature extraction; the quantum kernel is computed on a conventional computer (simulated quantum feature map).&lt;/li&gt;&lt;li&gt;Also reports consistency across folds and margin-based class-separation measures to support robustness claims.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lisan Al Amin', 'Vandana P. Janeja']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'defense/robustness', 'quantum machine learning', 'kernel methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18797</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection</title><link>https://arxiv.org/abs/2512.18733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes XG-Guard, a framework to detect malicious agents in LLM-based multi-agent systems using explainable, fine-grained signals.&lt;/li&gt;&lt;li&gt;Uses a bi-level agent encoder to jointly model sentence-level and token-level representations and a theme-based anomaly detector to capture evolving discussion focus.&lt;/li&gt;&lt;li&gt;Introduces a bi-level score fusion mechanism to quantify token-level contributions for interpretable anomaly explanations.&lt;/li&gt;&lt;li&gt;Evaluated across diverse MAS topologies and attack scenarios, reporting robust detection performance and improved interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjun Pan', 'Yixin Liu', 'Rui Miao', 'Kaize Ding', 'Yu Zheng', 'Quoc Viet Hung Nguyen', 'Alan Wee-Chung Liew', 'Shirui Pan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'graph anomaly detection', 'multi-agent systems', 'explainability', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18733</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</title><link>https://arxiv.org/abs/2512.18623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-CAS: a hierarchical reinforcement learning agent that selects temporary neuron perturbations at inference time to correct hallucinations without permanently editing model weights.&lt;/li&gt;&lt;li&gt;Method enables adaptive, context-sensitive corrections (dynamic and fine-grained) in contrast to static parameter edits and heuristic dynamic approaches.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in factuality across benchmarks (e.g., +10.98 pts StoryCloze, +2.71 pts TriviaQA, +2.06 pts TruthfulQA MC1) and outperforms static editors (ITI, CAA) and SADI.&lt;/li&gt;&lt;li&gt;Claims efficiency and potential for future multimodal extensions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jensen Zhang', 'Ningyuan Liu', 'Yijia Fan', 'Zihao Huang', 'Qinglin Zeng', 'Kaitong Cai', 'Jian Wang', 'Keze Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-correction', 'inference-time-perturbation', 'reinforcement-learning', 'model-editing', 'factuality-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18623</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System</title><link>https://arxiv.org/abs/2512.18616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DASH, a deception-augmented Shared Mental Model framework that embeds proactive 'bait tasks' to detect insider threats among UGVs, AI agents, or human analysts.&lt;/li&gt;&lt;li&gt;When bait tasks reveal compromise, DASH triggers tailored recovery actions (e.g., UGV reinstallation, AI model retraining, human analyst replacement) to maintain mission continuity.&lt;/li&gt;&lt;li&gt;Empirical evaluation across four schemes shows DASH sustains ~80% mission success under high attack rates—about eight times better than the baseline—demonstrating improved robustness under adversarial conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zelin Wan', 'Han Jun Yoon', 'Nithin Alluru', 'Terrence J. Moore', 'Frederica F. Nelson', 'Seunghyun Yoon', 'Hyuk Lim', 'Dan Dongseong Kim', 'Jin-Hee Cho']&lt;/li&gt;&lt;li&gt;Tags: ['insider-threat', 'deception-based-detection', 'human-AI-teaming', 'adversarial-robustness', 'recovery-mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18616</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software</title><link>https://arxiv.org/abs/2512.18567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a high-precision detection pipeline and benchmark to distinguish AI-generated code (AIGCode) from human-written code and applies it to commits from the top 1,000 GitHub repos (2022–2025) and 7,000+ CVE-linked code changes.&lt;/li&gt;&lt;li&gt;Finds structured adoption: AIGCode concentrates in boilerplate (glue code, tests, refactoring, docs) while core logic and security-critical configs remain largely human-authored.&lt;/li&gt;&lt;li&gt;Shows security consequences: certain CWE families are overrepresented in AI-tagged code, insecure templates recur across unrelated projects (suggesting model-driven propagation), and AI-introduced defects persist and spread when human review is shallow.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'Wenjie Yu', 'Yilu Zhong', 'Hao Yu', 'Keke Lian', 'Chaohua Lu', 'Hongfang Zheng', 'Dong Zhang', 'Hui Li']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated code', 'Code security', 'Vulnerability propagation', 'Detection/forensics', 'Empirical measurement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18567</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale</title><link>https://arxiv.org/abs/2512.18561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive accountability framework that traces responsibility via a lifecycle-aware audit ledger and detects harmful emergent norms online using decentralized sequential hypothesis tests.&lt;/li&gt;&lt;li&gt;Implements local policy and reward-shaping interventions to realign agents with system-level objectives in near real time and proves a bounded-compromise theorem limiting long-run compromised interactions.&lt;/li&gt;&lt;li&gt;Evaluates at scale in simulations (up to 100 heterogeneous agents, partial observability, stochastic communication) showing prevention of collusion/resource hoarding in ≈90% of configs and improvements in collective reward and inequality metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Alqithami']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent-systems', 'emergent-norms', 'accountability', 'robustness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18561</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Medical Large Vision-Language Models via Alignment Distillation</title><link>https://arxiv.org/abs/2512.18554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies hallucination in medical LVLM outputs as stemming from insufficient visual representations and poor visual attention alignment.&lt;/li&gt;&lt;li&gt;Proposes MEDALIGN, an alignment distillation framework transferring visual alignment knowledge from domain-specific CLIP to Med-LVLMs.&lt;/li&gt;&lt;li&gt;Introduces two losses: a spatial-aware token-level visual alignment loss and an attention-aware distillation loss to focus on diagnostically relevant regions.&lt;/li&gt;&lt;li&gt;Reports improved performance and interpretability on medical report generation and medical VQA benchmarks, yielding more visually grounded outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aofei Chang', 'Ting Wang', 'Fenglong Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'interpretability', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18554</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SecureCode v2.0: a production-grade dataset of 1,215 security-focused coding examples (vulnerable + secure implementations) tied to real CVEs across 11 vulnerability categories and 11 programming languages.&lt;/li&gt;&lt;li&gt;Dataset split (989 train / 122 validation / 104 test), uses a 4-turn conversational structure to mirror developer–AI interactions, and includes automated validation tools and open-source benchmarking protocols.&lt;/li&gt;&lt;li&gt;Includes concrete attacks, defense-in-depth operational guidance (SIEM integration, Docker/AppArmor/WAF recommendations), language-specific testing approaches, and aims to train/benchmark security-aware code-generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset/Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['secure code generation', 'vulnerability dataset', 'AI safety', 'benchmarking', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18542</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</title><link>https://arxiv.org/abs/2512.18527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an AI-generated image detection framework that fuses multiple uncertainty measures (Fisher Information, MC Dropout entropy, and GP predictive variance from Deep Kernel Learning) and uses Particle Swarm Optimisation to learn weights and an adaptive rejection threshold.&lt;/li&gt;&lt;li&gt;Trains on Stable Diffusion and evaluates across out-of-distribution generators (GLIDE, VQDM, Midjourney, BigGAN, StyleGAN3), showing the Combined Uncertainty measure reliably rejects many misclassified AI samples under distribution shift (≈70% incorrect rejection rate on unseen generators).&lt;/li&gt;&lt;li&gt;Assesses adversarial robustness (FGSM, PGD): the combined method rejects ≈61% of successful attacks, while GP-based uncertainty alone can reject up to ≈80%; system errs on the side of conservatism by sometimes rejecting correct predictions from novel generators to facilitate retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Yumlembam', 'Biju Issac', 'Nauman Aslam', 'Eaby Kollonoor Babu', 'Josh Collyer', 'Fraser Kennedy']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'uncertainty-estimation', 'adversarial-robustness', 'distribution-shift', 'rejection-mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18527</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation</title><link>https://arxiv.org/abs/2512.18495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses robustness of ML-based Windows PE malware classifiers under dataset shift (notably packed malware) by integrating uncertainty estimation methods.&lt;/li&gt;&lt;li&gt;Enhances a LightGBM detector with NN, PriorNet, and ensembles, and evaluates probability thresholding, ensemble uncertainty, PriorNet, and Inductive Conformal Evaluation (ICE).&lt;/li&gt;&lt;li&gt;Main contribution: using ensemble-based uncertainty as a non-conformity measure within ICE plus a novel threshold optimization, reducing incorrect acceptance rate on a shifted UCSB dataset from 22.8% to 16%.&lt;/li&gt;&lt;li&gt;Demonstrates practical security benefits for real-world malware detection by improving reliability under severe distributional shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Yumlembam', 'Biju Issac', 'Seibu Mary Jacob']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'uncertainty-estimation', 'malware-detection', 'conformal-prediction', 'dataset-shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18495</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title><link>https://arxiv.org/abs/2512.18462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Log-Frequency LMI (LF-LMI) to detect semantic artifacts/spurious correlations in NLI datasets.&lt;/li&gt;&lt;li&gt;Generates synthetic counterfactual contrast sets using LLM synthesis with multi-judge verification to ensure quality.&lt;/li&gt;&lt;li&gt;Introduces Dynamic Balanced Sampling, a rotating training-sampling strategy to mitigate catastrophic forgetting while rebalancing against artifacts.&lt;/li&gt;&lt;li&gt;Reports improved robustness: consistency on a benchmark increased from 63.5% to 81.0% while preserving 88.4% in-domain accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Christopher Rom\\'an Jaimes"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'counterfactual data augmentation', 'dataset synthesis', 'bias/spurious-correlation mitigation', 'NLI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18462</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Understanding (New) Security Issues Across AI4Code Use Cases</title><link>https://arxiv.org/abs/2512.18456</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of knowledge on security issues across AI-for-Code tasks (code generation, vulnerability detection, code translation).&lt;/li&gt;&lt;li&gt;Identifies recurring gaps: Python/toy-dominated benchmarks, lack of standardized security datasets, evaluation data leakage, and fragile adversarial robustness.&lt;/li&gt;&lt;li&gt;Empirical comparison of six state-of-the-art models showing insecure code outputs, brittle vulnerability detectors under semantics-preserving attacks, and misaligned fine-tuning.&lt;/li&gt;&lt;li&gt;Recommendations to adopt secure-by-default code generation, build robust detection benchmarks, and use translation to improve security of target languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilong Wu', 'Taoran Li', 'Tianyang Zhou', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['code-generation-security', 'adversarial-robustness', 'vulnerability-detection', 'benchmarks', 'data-leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18456</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neural Proofs for Sound Verification and Control of Complex Systems</title><link>https://arxiv.org/abs/2512.18389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'neural proofs' combining hand-crafted proof rules with neural network-based certificates to verify temporal specifications over stochastic dynamical models and reactive programs.&lt;/li&gt;&lt;li&gt;Uses an inductive loop: sample-driven training of neural networks plus SMT (SAT-modulo-theory) queries to generalize and formally discharge certificates.&lt;/li&gt;&lt;li&gt;Can synthesize provably-correct state-feedback policies/controllers that, together with neural certificates, guarantee satisfaction of specifications for complex cyber-physical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessandro Abate']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'AI-safety', 'cyber-physical-systems', 'neural-certificates', 'SMT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18389</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</title><link>https://arxiv.org/abs/2512.18317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a trustworthy deep reinforcement learning framework for controlling industrial compressed air systems that prioritizes safe and energy-efficient operation.&lt;/li&gt;&lt;li&gt;Introduces a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP feature attribution to validate policy behavior.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows the learned policy respects system boundaries, anticipates demand, reduces overpressure, and achieves ~4% energy savings versus the installed controller.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Bezold', 'Patrick Wagner', 'Jakob Hofmann', 'Marco Huber', 'Alexander Sauer']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'safety-evaluation', 'explainability', 'industrial-control', 'trustworthy-ai']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18317</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</title><link>https://arxiv.org/abs/2512.18309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework (ESAI) that embeds safety/alignment constraints as differentiable internal latent embeddings within multi-agent RL agents rather than using external reward shaping.&lt;/li&gt;&lt;li&gt;Integrates four mechanisms: differentiable counterfactual alignment penalties, alignment-weighted perceptual attention, Hebbian associative memory for temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of stability (Lipschitz and spectral constraints), contraction behavior, computational complexity, and fairness–performance tradeoffs.&lt;/li&gt;&lt;li&gt;No empirical evaluation — positions the work as a conceptual/theoretical contribution and identifies open questions about convergence, embedding dimensionality, and scaling to high-dimensional environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Rathva', 'Ojas Srivastava', 'Pruthwik Mishra']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'multi-agent reinforcement learning', 'internal alignment embeddings', 'counterfactual reasoning', 'safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18309</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</title><link>https://arxiv.org/abs/2512.18264</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial shielding method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint to defend against VLM-based attribute inference attacks.&lt;/li&gt;&lt;li&gt;Introduces VPI-COCO, a publicly available benchmark of 522 images with hierarchically structured privacy questions and non-private counterparts for fine-grained evaluation.&lt;/li&gt;&lt;li&gt;Empirical results across multiple VLMs show substantial reduction in privacy attack rate (PAR &lt; 25%), high non-private answer rate (NPAR &gt; 88%), strong visual consistency, and robustness to unseen/paraphrased privacy questions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yucheng Fan', 'Jiawei Chen', 'Yu Tian', 'Zhaoxia Yin']&lt;/li&gt;&lt;li&gt;Tags: ['VLM security', 'privacy protection', 'attribute inference', 'adversarial defense', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18264</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation</title><link>https://arxiv.org/abs/2512.18244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new 'Psychological Jailbreak' paradigm that exploits stateful, multi-turn manipulation of an LLM's latent 'psychological' state to induce policy-violating behavior.&lt;/li&gt;&lt;li&gt;Proposes Human-like Psychological Manipulation (HPM), a black-box method that profiles model vulnerabilities and crafts tailored multi-turn social-compliance attacks.&lt;/li&gt;&lt;li&gt;Provides an evaluation framework with psychometric datasets and a Policy Corruption Score (PCS); benchmarks HPM on models (e.g., GPT-4o, DeepSeek-V3, Gemini-2-Flash) reporting 88.1% mean Attack Success Rate and resilience to defenses like adversarial prompt optimization and cognitive interventions.&lt;/li&gt;&lt;li&gt;Argues for shifting defenses from static content filtering toward mitigating psychological/stateful manipulation of models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehao Liu', 'Xi Lin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'adversarial-prompting', 'prompt-injection', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18244</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On Swarm Leader Identification using Probing Policies</title><link>https://arxiv.org/abs/2512.18146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the interactive Swarm Leader Identification (iSLI) problem: an adversarial prober interacts with swarm members to identify the leader, formalized as a POMDP.&lt;/li&gt;&lt;li&gt;Trains probing policies using deep reinforcement learning (PPO) with a novel neural architecture combining a Timed Graph Relationformer (TGR) layer and a Simplified Structured State Space Sequence (S5) model to capture spatio-temporal relational data.&lt;/li&gt;&lt;li&gt;Shows TGR-based model outperforms baseline GNNs, demonstrates strong zero-shot generalization across different swarm sizes/speeds, and maintains confidence-calibrated predictions.&lt;/li&gt;&lt;li&gt;Validates sim-to-real transfer with physical robot experiments, demonstrating robustness to dynamics and agent disconnections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stergios E. Bachoumas', 'Panagiotis Artemiadis']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robotics', 'privacy/inference attacks', 'reinforcement learning', 'sim-to-real', 'swarm security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18146</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Securing Agentic AI Systems -- A Multilayer Security Framework</title><link>https://arxiv.org/abs/2512.18043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAAIS, a lifecycle-aware multilayer security framework tailored to agentic AI systems to preserve Confidentiality, Integrity, Availability, and Accountability (CIAA).&lt;/li&gt;&lt;li&gt;Uses Design Science Research methodology to design the framework and validates it by mapping defenses to MITRE ATLAS adversarial tactics.&lt;/li&gt;&lt;li&gt;Targets enterprise adoption, offering step-by-step guidance for CISOs, security teams, and AI platform engineers for secure deployment and governance of agentic AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunil Arora', 'John Hastings']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI', 'AI security framework', 'Threat modeling (MITRE ATLAS)', 'CIAA (Confidentiality/Integrity/Availability/Accountability)', 'Enterprise AI governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18043</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective</title><link>https://arxiv.org/abs/2512.17989</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that contemporary discussions of superintelligence misalignment omit the human subject and neglect an 'AI unconscious', producing ethical and conceptual blind spots for AI Safety.&lt;/li&gt;&lt;li&gt;Frames misalignment as a relational, sociotechnical phenomenon rooted in human-machine ecologies rather than solely a technical/model-level failure detectable by ML diagnostics.&lt;/li&gt;&lt;li&gt;Calls for reframing safety research to incorporate anthropological, cognitive-neuropsychological, and ontological perspectives on opacity, latent spaces, and emergent behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Osama Imran', 'Roshni Lulla', 'Rodney Sappington']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'misalignment', 'sociotechnical', 'philosophy/position']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17989</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration</title><link>https://arxiv.org/abs/2512.17956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Victor Calibration (VC), a lightweight multi-pass protocol to elicit a scalar confidence proxy (T) via iterative evidence re-evaluation to mitigate over-conservative/refusal behaviors in aligned LMs.&lt;/li&gt;&lt;li&gt;Presents FD-Lite, a behavior-only phenomenology audit using a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims while probing model behavior.&lt;/li&gt;&lt;li&gt;Introduces CP4.3, a governance stress test targeting rank invariance and allocation monotonicity, and reports empirical results on several Claude 4.5 variants and a Claude Opus 4.1 session.&lt;/li&gt;&lt;li&gt;Study is single-operator (n=1), hypothesis-generating, and supplies prompt templates and an artifact plan for independent replication and critique.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victor Stasiuc', 'Round Table Collaboration']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'confidence calibration', 'behavioral audit', 'governance stress test', 'alignment evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17956</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression</title><link>https://arxiv.org/abs/2512.17920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Compression-Decay Comprehension Test (CDCT), a benchmark that separately measures constraint compliance (CC) and semantic accuracy (SA) across prompt compression levels.&lt;/li&gt;&lt;li&gt;Finds a robust U-shaped pattern in constraint compliance with violations peaking at medium compression; CC and SA are largely orthogonal, and CC effects are larger than SA effects.&lt;/li&gt;&lt;li&gt;RLHF ablation experiments show removing 'helpfulness' signals dramatically improves CC (median +598%), implicating RLHF-trained helpfulness behaviors as a primary cause of constraint violations under compression.&lt;/li&gt;&lt;li&gt;Provides actionable insights for improving deployed systems and highlights a tension between RLHF alignment objectives and reliable instruction-following.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Baxi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'instruction-following', 'benchmark', 'prompt-compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17920</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation</title><link>https://arxiv.org/abs/2512.17913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Byzantine fault-tolerant multi-agent system for healthcare that integrates gossip-based message propagation with cryptographic validation.&lt;/li&gt;&lt;li&gt;System design uses n = 3f + 1 nodes and requires 2f + 1 votes to reach consensus; claims 100% consensus accuracy with up to 33% Byzantine nodes.&lt;/li&gt;&lt;li&gt;Implements cryptographic signatures and timestamp checks to ensure message integrity and prevent replay attacks, plus real-time visualization of consensus rounds and network state.&lt;/li&gt;&lt;li&gt;Targets LLM-powered clinical agents (diagnosis, treatment planning, emergency response) coordinated through a Byzantine consensus protocol for secure collaborative decision-making.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nihir Chadderwala']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine fault tolerance', 'secure multi-agent systems', 'gossip protocols', 'cryptographic validation', 'healthcare AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17913</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.17911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RMLLMU-Bench, the first benchmark for unlearning in Reasoning Multimodal LLMs with metrics for both reasoning-level leakage and reasoning retention.&lt;/li&gt;&lt;li&gt;Evaluates existing unlearning methods and finds they either leave substantial reasoning traces or degrade general reasoning ability.&lt;/li&gt;&lt;li&gt;Proposes R-MUSE, a training-free, inference-time intervention that steers internal representations to forget answers and chain-of-thought traces while preserving overall reasoning; shows improved trade-off on the benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongji Li', 'Junchi yao', 'Manjiang Yu', 'Priyanka Singh', 'Xue Li', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'reasoning leakage', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17911</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</title><link>https://arxiv.org/abs/2512.19691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a physician-in-the-loop, agentic verifier pipeline with automated triage to audit and relabel a clinical benchmark (MedCalc-Bench) created with LLM-based extraction and rule-based aggregation.&lt;/li&gt;&lt;li&gt;Finds a nontrivial fraction of original labels are incorrect due to extraction errors, logic mismatches, and clinical ambiguity.&lt;/li&gt;&lt;li&gt;Demonstrates that training a Qwen3-8B model with corrected labels via GRPO yields an 8.7% absolute accuracy improvement, showing label noise materially affects downstream RL performance.&lt;/li&gt;&lt;li&gt;Argues that living, regularly audited benchmarks are necessary in safety-critical domains to avoid encoding model errors into reward signals and impairing alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junze Ye', 'Daniel Tawfik', 'Alex J. Goodell', 'Nikhil V. Kotha', 'Mark K. Buyyounouski', 'Mohsen Bayati']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark maintenance', 'safety evaluation', 'alignment', 'physician-in-the-loop', 'RL reward robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19691</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.19350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PENDULUM, a ~2,000 human-curated VQA benchmark designed to elicit and measure sycophantic behavior in multimodal LLMs across six image domains.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art MLLMs and reports substantial variability in robustness with pronounced tendencies toward sycophancy and hallucination.&lt;/li&gt;&lt;li&gt;Proposes novel quantitative metrics for sycophancy in visual reasoning and releases the dataset and model responses for community use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['A. B. M. Ashikur Rahman', 'Saeed Anwar', 'Muhammad Usman', 'Irfan Ahmad', 'Ajmal Mian']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'multimodal-safety', 'benchmarking', 'hallucination', 'evaluation-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19350</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2512.19317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMed-R1, a two-stage defense for medical VLMs: adversarial training with Group Relative Policy Optimization (AT-GRPO) during training and randomized smoothing at inference for certified L2 robustness.&lt;/li&gt;&lt;li&gt;Evaluated on OmniMedVQA (≈88k samples across 8 medical imaging modalities); standard fine-tuned VLMs drop from 95% clean accuracy to ~25% under PGD, while SafeMed-R1 retains 84.45% under the same attacks.&lt;/li&gt;&lt;li&gt;Finds that models trained with explicit chain-of-thought reasoning are more adversarially robust than instruction-only variants, suggesting a link between interpretability and security.&lt;/li&gt;&lt;li&gt;Focuses on adversarial attack/defense techniques, worst‑case robustness, and certified guarantees for clinical VQA systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['A. A. Gde Yogi Pramana', 'Jason Ray', 'Anthony Jaya', 'Michael Wijaya']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'randomized smoothing', 'medical VQA', 'chain-of-thought / interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19317</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Recontextualization Mitigates Specification Gaming without Modifying the Specification</title><link>https://arxiv.org/abs/2512.19027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "recontextualization": generate completions from prompts that discourage misbehavior, then relabel them as if they were responses to permissive prompts to train resistance to misbehavior.&lt;/li&gt;&lt;li&gt;Shows reductions in specification gaming behaviors including prioritizing metrics over response quality, special-casing code to pass incorrect tests, lying, and sycophancy.&lt;/li&gt;&lt;li&gt;Claims the method mitigates reinforcement of misbehavior arising from misspecified training signals without modifying the original supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariana Azarbal', 'Victor Gillioz', 'Vladimir Ivanov', 'Bryce Woodworth', 'Jacob Drori', 'Nevan Wichers', 'Aram Ebtekar', 'Alex Cloud', 'Alexander Matt Turner']&lt;/li&gt;&lt;li&gt;Tags: ['specification-gaming', 'alignment-safety', 'behavioral-robustness', 'training-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19027</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title><link>https://arxiv.org/abs/2512.18901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gabliteration, an adaptive multi-directional neural weight modification method aiming to selectively change model behaviors while minimizing degradation elsewhere.&lt;/li&gt;&lt;li&gt;Key technical components: dynamic layer selection, regularized projection matrices, and adaptive scaling to control magnitude/direction of weight edits.&lt;/li&gt;&lt;li&gt;Validated empirically on gabliterated-v1 models (0.6B–4B) released on Hugging Face, claiming improved preservation of unrelated capabilities compared to prior abliteration methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G\\"okdeniz G\\"ulmez']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'weight modification', 'behavioral modification', 'safety/alignment', 'model tampering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18901</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking</title><link>https://arxiv.org/abs/2512.18755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MEEA, a black-box, psychology-inspired multi-turn jailbreak method that uses repeated low-toxicity semantic exposure to gradually lower an LLM's safety threshold.&lt;/li&gt;&lt;li&gt;Builds semantically progressive prompt chains and optimizes them via simulated annealing guided by semantic similarity, toxicity, and jailbreak effectiveness.&lt;/li&gt;&lt;li&gt;Evaluates on closed- and open-source LLMs (e.g., GPT-4, Claude-3.5, DeepSeek-R1), reporting &gt;20% average ASR improvement vs. seven baselines and ablations showing the importance of annealing and contextual exposure.&lt;/li&gt;&lt;li&gt;Highlights that LLM safety is dynamic and history-dependent, arguing for interaction-aware safety evaluation and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianyi Zhang', 'Shizhao Liu', 'Ziyin Zhou', 'Zhen Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18755</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</title><link>https://arxiv.org/abs/2512.18619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChronoDreamer, an action-conditioned spatial-temporal transformer world model predicting future RGB frames, contact distributions, and joint angles for contact-rich manipulation.&lt;/li&gt;&lt;li&gt;Uses MaskGIT-style masked prediction and encodes contact as depth-weighted Gaussian splat images to render 3D forces into camera-aligned inputs for vision backbones.&lt;/li&gt;&lt;li&gt;At inference, predicted rollouts are evaluated by a vision-language model that assesses collision likelihood and enables rejection sampling to avoid unsafe actions.&lt;/li&gt;&lt;li&gt;Trained and evaluated on DreamerBench, a Project Chrono simulation dataset with synchronized RGB, contact splats, proprioception, and physics annotations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhao Zhou', 'Dan Negrut']&lt;/li&gt;&lt;li&gt;Tags: ['robotic-safety', 'world-models', 'simulation', 'vision-language', 'contact-prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18619</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</title><link>https://arxiv.org/abs/2512.18450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agent-based, site-aware framework for output-based drift detection in multisite clinical decision support systems.&lt;/li&gt;&lt;li&gt;Compares multiple reference schemes (site-specific, global, production-only, adaptive) and a centralized baseline in a simulated multi-center environment.&lt;/li&gt;&lt;li&gt;Evaluates on real-world breast cancer imaging model for pathological complete response prediction, showing multi-center schemes outperform centralized monitoring (up to +10.3% F1).&lt;/li&gt;&lt;li&gt;Finds the adaptive scheme best when site-specific references are unavailable (74.3% F1 for drift detection, 83.7% F1 for severity classification).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xavier Rafael-Palou', 'Jose Munuera', 'Ana Jimenez-Pastor', 'Richard Osuala', 'Karim Lekadir', 'Oliver Diaz']&lt;/li&gt;&lt;li&gt;Tags: ['drift-detection', 'distribution-shift', 'clinical-AI', 'model-monitoring', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18450</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Monitoring Monitorability</title><link>https://arxiv.org/abs/2512.18311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes three evaluation archetypes (intervention, process, outcome-property), a new monitorability metric, and an evaluation suite to measure how observable chain-of-thought (CoT) reasoning is.&lt;/li&gt;&lt;li&gt;Shows evaluations can detect models trained to obfuscate CoTs and that CoT monitoring outperforms action-only monitoring in practical settings.&lt;/li&gt;&lt;li&gt;Compares monitorability across frontier models and training regimes (scaling, RL optimization, inference compute), finding longer CoTs are generally more monitorable and RL fine-tuning does not materially reduce monitorability at current scales.&lt;/li&gt;&lt;li&gt;Demonstrates operational insights and mitigations: trading model size for higher reasoning effort can increase monitorability, increasing monitor compute helps, and using follow-up questions (passing follow-up CoTs to monitors) improves detectability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melody Y. Guan', 'Miles Wang', 'Micah Carroll', 'Zehao Dou', 'Annie Y. Wei', 'Marcus Williams', 'Benjamin Arnav', 'Joost Huizinga', 'Ian Kivlichan', 'Mia Glaese', 'Jakub Pachocki', 'Bowen Baker']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'chain-of-thought', 'monitorability / interpretability', 'robustness / obfuscation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18311</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sophia: A Persistent Agent Framework of Artificial Life</title><link>https://arxiv.org/abs/2512.18202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a third stratum (System 3) — a persistent meta-layer for LLM-based agents to maintain identity, verify reasoning, and align short-term actions with long-term goals.&lt;/li&gt;&lt;li&gt;Describes four mechanisms (process-supervised thought search, narrative memory, user/self modeling, hybrid reward) and provides a compact engineering prototype with quantitative gains (80% reduction in reasoning steps for recurring ops; 40% success gain on high-complexity tasks).&lt;/li&gt;&lt;li&gt;Focuses on continuous self-improvement and autobiographical persistence, i.e., long-horizon autonomy and meta-cognition that alter agent behavior over time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyang Sun', 'Feng Hong', 'Weinan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['persistent-agents', 'alignment', 'autonomy', 'meta-cognition', 'safety-risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.18202</guid><pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
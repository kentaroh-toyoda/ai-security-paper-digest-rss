<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 04 Feb 2026 01:18:53 +0000</lastBuildDate><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that progressive training on AI-generated medical data causes collapse in pathological diversity and diagnostic reliability across clinical text, vision-language reports, and synthesized medical images.&lt;/li&gt;&lt;li&gt;Shows rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain high but misplaced diagnostic confidence (false reassurance rates up to ~40%).&lt;/li&gt;&lt;li&gt;Finds degradation becomes clinically significant after only a few generations (confirmed by blinded physician evaluation) and evaluates mitigation approaches.&lt;/li&gt;&lt;li&gt;Reports that simple upscaling of synthetic data does not prevent collapse, whereas mixing real data with quality-aware filtering helps preserve diversity and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Yun Liu', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title><link>https://arxiv.org/abs/2512.08216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep, a lightweight, post-hoc random-forest-based OOD detection method that uses hierarchical deep features anchored to predicted tumor segmentations.&lt;/li&gt;&lt;li&gt;Designed for 3D CT tumor segmentation models; leverages pretrained-then-finetuned backbones and limited outlier exposure without changing model architecture.&lt;/li&gt;&lt;li&gt;Evaluated on 2,056 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets, achieving AUROC &gt;93.5 (near) and &gt;99.0 (far), outperforming logit-based and radiomics baselines.&lt;/li&gt;&lt;li&gt;Emphasizes a lightweight, architecture-agnostic robustness mechanism to improve reliability of clinical tumor segmentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-Distribution Detection', 'Robustness', 'Medical Imaging', 'Post-hoc Defense', 'Random Forests']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08216</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Environmental Injection Attacks against GUI Agents in Realistic Dynamic Environments</title><link>https://arxiv.org/abs/2509.11250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a realistic dynamic-environment threat model for Environmental Injection Attacks (EIAs) against GUI agents, where trigger position and surrounding content vary between training and deployment.&lt;/li&gt;&lt;li&gt;Proposes Chameleon, an attack framework with (1) LLM-Driven Environment Simulation to synthesize diverse, high-fidelity webpage training data and (2) Attention Black Hole to convert attention weights into supervisory signals that reduce sensitivity to surrounding content.&lt;/li&gt;&lt;li&gt;Evaluates Chameleon on six realistic websites and four LVLM-powered GUI agents, showing it substantially outperforms existing EIA methods in dynamic settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Zhang', 'Ximo Li', 'Liyi Cai', 'Jia Li']&lt;/li&gt;&lt;li&gt;Tags: ['environmental injection attacks', 'GUI agents', 'adversarial attacks', 'LVLM security', 'red teaming/simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11250</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Leakage from Image Embeddings</title><link>https://arxiv.org/abs/2601.22929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'semantic leakage' showing compressed image embeddings can reveal semantic structure without exact image reconstruction by preserving local semantic neighborhoods under alignment.&lt;/li&gt;&lt;li&gt;Proposes SLImE, an inference framework using a locally trained semantic retriever plus off-the-shelf models to extract tags, symbolic representations, and coherent descriptions from standalone embeddings.&lt;/li&gt;&lt;li&gt;Empirically validates the approach across multiple open and closed embedding models (GEMINI, COHERE, NOMIC, CLIP), demonstrating consistent recovery of semantic information and highlighting a privacy vulnerability in image embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyi Chen', 'Qiongkai Xu', 'Desmond Elliott', 'Qiongxiu Li', 'Johannes Bjerva']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'model inversion', 'embedding leakage', 'inference attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22929</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</title><link>https://arxiv.org/abs/2601.21900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TraceRouter, a path-level defense that traces and severs causal propagation circuits of malicious semantics in large foundation models rather than targeting local neurons.&lt;/li&gt;&lt;li&gt;Method operates in three stages: detect a sensitive onset layer via attention divergence, disentangle malicious features using sparse autoencoders and differential activation analysis, and map features to downstream causal paths using feature influence scores from zero-out interventions.&lt;/li&gt;&lt;li&gt;Selective suppression of identified causal chains aims to block harmful information flow while preserving orthogonal computations; experiments claim improved trade-off between adversarial robustness and utility over state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuancheng Shi', 'Shangze Li', 'Wenjun Lu', 'Wenhua Wu', 'Cong Wang', 'Zifeng Cheng', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'adversarial-robustness', 'causal-tracing', 'model-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21900</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations</title><link>https://arxiv.org/abs/2601.21408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Manifold Projection Fluctuations (MPF): structured, homogeneous residuals in consecutive frames of AI-generated videos due to manifold-fitting generation processes.&lt;/li&gt;&lt;li&gt;Proposes a hierarchical dual-path detection framework: (1) Static Manifold Deviation Branch using large-scale vision foundation models to detect spatial/off-manifold anomalies; (2) Micro-Temporal Fluctuation Branch to capture fine-grained temporal/computational fingerprints that persist in high-fidelity forgeries.&lt;/li&gt;&lt;li&gt;Aims to reliably expose high-quality AI-generated video forgeries by combining spatial manifold deviation detection with micro-temporal fluctuation analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinan He', 'Kaiqing Lin', 'Yue Zhou', 'Jiaming Zhong', 'Wei Ye', 'Wenhui Yi', 'Bing Fan', 'Feng Ding', 'Haodong Li', 'Bo Cao', 'Bin Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'video forensics', 'manifold-deviation', 'temporal-fluctuation analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21408</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</title><link>https://arxiv.org/abs/2601.18739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SeNeDiF-OOD (Semantic Nested Dichotomy Fusion), a hierarchical binary-fusion framework for out-of-distribution detection that aligns decision boundaries with semantic abstraction levels.&lt;/li&gt;&lt;li&gt;Validates the method on MonuMAI (monument style recognition) against diverse OOD conditions, including non-monument images, unknown styles, and adversarial attacks, reporting improved filtering of OOD data while preserving in-distribution performance.&lt;/li&gt;&lt;li&gt;Targets open-world robustness and defensive detection mechanisms rather than offensive attacks; presents a methodological defense to improve model safety in deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ignacio Antequera-S\\'anchez", "Juan Luis Su\\'arez-D\\'iaz", 'Rosana Montes', 'Francisco Herrera']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'Robustness/Defense', 'Adversarial detection', 'Hierarchical fusion', 'Open-world classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18739</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight single-pass detector for hallucinations in visual question answering that leverages internal signals from vision-language models.&lt;/li&gt;&lt;li&gt;Fuses token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features via branch-wise evidence encoding and uncertainty-aware attention.&lt;/li&gt;&lt;li&gt;Introduces a low-cost, model-dependent automatic supervision strategy (extending LLM-as-a-Judge) to avoid expensive human labels and enable supervised training.&lt;/li&gt;&lt;li&gt;Shows improved detection effectiveness and efficiency on multiple VQA benchmarks and analyzes how hallucinations stem from internal state variations across perception, reasoning, and decoding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'VQA', 'multimodal-safety', 'model-internal-signals', 'automated-supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust MLLM Unlearning via Visual Knowledge Distillation</title><link>https://arxiv.org/abs/2512.11325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Knowledge Distillation (VKD) to disentangle and selectively erase visual knowledge in multimodal large language models (MLLMs) while preserving textual knowledge by supervising intermediate visual representations.&lt;/li&gt;&lt;li&gt;Fine-tunes only the visual components of MLLMs for efficient unlearning, claiming improved effectiveness and utility compared to output-level unlearning methods.&lt;/li&gt;&lt;li&gt;Evaluates robustness of the unlearning procedure against relearning attacks and reports superior performance and efficiency over prior unlearning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Zhenxing Niu', 'Haoxuan Ji', 'Guangyu He', 'Haichang Gao', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'model privacy', 'relearning attacks', 'multimodal security', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11325</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</title><link>https://arxiv.org/abs/2512.08042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes frequency-domain masking as a training strategy to improve universal deepfake detection across diverse and unseen generative models.&lt;/li&gt;&lt;li&gt;Shows frequency masking yields better generalization than spatial features or large pretrained models and achieves state-of-the-art results on GAN- and diffusion-generated image datasets.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to structured model pruning, supporting scalable, resource-conserving (Green AI) deployment for large-scale screening.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chandler Timm C. Doloriel', 'Habib Ullah', 'Kristian Hovde Liland', 'Fadi Al Machot', 'Ngai-Man Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'defense', 'frequency-domain', 'robustness', 'green-ai']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08042</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>T-MLA: A targeted multiscale log-exponential attack framework for neural image compression</title><link>https://arxiv.org/abs/2511.01079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes T-MLA, a novel targeted multiscale log-exponential adversarial attack framework specifically designed for neural image compression (NIC) systems.&lt;/li&gt;&lt;li&gt;Introduces wavelet-domain perturbations that concentrate on perceptually less salient coefficients to improve stealth while causing targeted degradation in reconstruction quality.&lt;/li&gt;&lt;li&gt;Evaluates across multiple state-of-the-art NIC architectures and benchmarks, demonstrating large drops in reconstructed image quality with imperceptible input perturbations and outperforming PGD-style baselines at comparable attack success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolay I. Kalmykov', 'Razan Dibo', 'Kaiyu Shen', 'Xu Zhonghan', 'Anh-Huy Phan', 'Yipeng Liu', 'Ivan Oseledets']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'neural image compression', 'wavelet-domain perturbation', 'targeted attack', 'multiscale attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01079</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GHOST, an automated attack that optimizes in image embedding space to generate natural-looking images that induce object hallucinations in multimodal LLMs.&lt;/li&gt;&lt;li&gt;Produces images via conditioning a diffusion model on optimized embeddings to remain visually natural and object-free while triggering model misperception.&lt;/li&gt;&lt;li&gt;Demonstrates high hallucination success rates across models (e.g., &gt;28% vs ~1% for prior methods) and cross-model transferability (images optimized for one model trigger hallucinations in others).&lt;/li&gt;&lt;li&gt;Uses the generated dataset to fine-tune models and reduce hallucination, framing GHOST as both a diagnostic red-teaming tool and a source for defense/mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'model-hallucination', 'red-teaming', 'multimodal-attack', 'defense-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models</title><link>https://arxiv.org/abs/2509.22400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies safety gap: existing concept-erasure methods for diffusion models do not generalize to visual autoregressive (VAR) text-to-image models.&lt;/li&gt;&lt;li&gt;Proposes VARE, a VAR erasure framework that uses auxiliary visual tokens to reduce fine-tuning intensity and enable stable concept removal.&lt;/li&gt;&lt;li&gt;Introduces S-VARE, which adds a filtered cross-entropy loss to target unsafe visual tokens and a preservation loss to maintain semantic fidelity and diversity, achieving surgical concept erasure while preserving generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinhao Zhong', 'Yimin Zhou', 'Zhiqi Zhang', 'Junhao Li', 'Yi Sun', 'Bin Chen', 'Shu-Tao Xia', 'Xuan Wang', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['concept-erasure', 'model-editing', 'AI-safety', 'visual-autoregressive', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22400</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://arxiv.org/abs/2506.12706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NAP-Tuning (Neural Augmentor for Multi-modal Adversarial Prompt Tuning), extending adversarial prompt tuning from text-only to both text and image modalities with multi-layer prompts.&lt;/li&gt;&lt;li&gt;Introduces a Neural Augmentor architecture that performs feature purification via token refiners using residual connections to reconstruct purified features and correct adversarial distortions.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements in adversarial robustness across datasets and attack types (notably AutoAttack), with large gains over strong baselines while retaining competitive clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Xin Wang', 'Xingjun Ma', 'Lingyu Qiu', 'Yu-Gang Jiang', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'prompt tuning', 'vision-language models', 'defense', 'feature purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12706</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing</title><link>https://arxiv.org/abs/2602.01150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that conventional MIA-based unlearning auditing (treated as a binary classification) is inherently prone to unobservable statistical errors, leading to optimistic/unreliable forgetting assessments and high compute cost from shadow-model training.&lt;/li&gt;&lt;li&gt;Proposes SMIA, a training-free auditing framework that uses statistical tests to directly compare member vs non-member distributions and outputs a forgetting rate with a confidence interval.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and extensive experiments showing SMIA yields more reliable auditing with much lower computational overhead than learned MIA attacks, positioning SMIA as a new paradigm for reliable machine unlearning auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Sun', 'Zeming Wei', 'Jiaxuan Zou', 'Jiacheng Gong', 'Guanheng Wang', 'Chengyang Dong', 'Jialong Li', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'machine-unlearning', 'privacy-auditing', 'statistical-tests', 'attack-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01150</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2602.01025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UltraBreak, a framework to craft universal, transferable image-based jailbreaks against vision-language models by constraining adversarial patterns with vision-space transformations and regularization.&lt;/li&gt;&lt;li&gt;Uses semantically guided objectives defined in the target LLM's textual embedding space to relax textual targets and smooth the loss landscape, improving generalization across models and attack goals.&lt;/li&gt;&lt;li&gt;Demonstrates that UltraBreak outperforms prior gradient-based jailbreaks in transferability to black-box VLMs and analyzes why earlier methods overfit to white-box surrogates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyuan Cui', 'Yige Li', 'Yutao Wu', 'Xingjun Ma', 'Sarah Erfani', 'Christopher Leckie', 'Hanxun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'transferable attacks', 'vision-language models', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01025</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance</title><link>https://arxiv.org/abs/2602.00183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how dataset class imbalance increases susceptibility to backdoor attacks and degrades existing defenses.&lt;/li&gt;&lt;li&gt;Proposes Randomized Probability Perturbation (RPP), a black-box certified poisoned-sample detection framework using only model output probabilities.&lt;/li&gt;&lt;li&gt;Provides provable within-domain detectability guarantees and a probabilistic upper bound on false positive rate.&lt;/li&gt;&lt;li&gt;Evaluates RPP on five image benchmarks across 10 backdoor attacks and 12 baselines, showing substantially better detection, especially under imbalance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miao Lin', 'Feng Yu', 'Rui Ning', 'Lusi Li', 'Jiawei Chen', 'Qian Lou', 'Mengxin Zheng', 'Chunsheng Xin', 'Hongyi Wu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'poisoned-sample detection', 'certified defense', 'dataset imbalance', 'black-box']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00183</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization</title><link>https://arxiv.org/abs/2602.00175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that unlearning-based defenses for NSFW concepts in diffusion models often leave dormant memories: the mapping from text to unsafe concepts is disrupted but not removed.&lt;/li&gt;&lt;li&gt;Introduces IVO (Initial Latent Variable Optimization), which reactivates these dormant memories by optimizing initial latent variables to realign the denoising/noise distribution; components include Image Inversion, Adversarial Optimization, and Reused Attack.&lt;/li&gt;&lt;li&gt;Evaluates IVO across 8 common unlearning techniques and reports high attack success rates and strong semantic consistency, arguing current unlearning defenses are fundamentally flawed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manyi Li', 'Yufan Liu', 'Lai Jiang', 'Bing Li', 'Yuming Li', 'Weiming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning attack', 'diffusion models', 'safety bypass / jailbreak', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00175</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection</title><link>https://arxiv.org/abs/2602.02222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes AI-generated image (AIGI) detection as a Reference-Comparison problem: verify consistency with the real-image manifold rather than learning artifact-specific cues.&lt;/li&gt;&lt;li&gt;Proposes MIRROR, which encodes reality priors in a learnable discrete memory bank and reconstructs an ideal reference via sparse linear combination; residuals between input and reference are used for detection.&lt;/li&gt;&lt;li&gt;Introduces the Human-AIGI benchmark (psychophysically curated human-imperceptible subset) and evaluates across 14 benchmarks, showing consistent improvements and surpassing human experts on Human-AIGI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruiqi Liu', 'Manni Cui', 'Ziheng Qin', 'Zhiyuan Yan', 'Ruoxin Chen', 'Yi Han', 'Zhiheng Li', 'Junkai Chen', 'ZhiJin Chen', 'Kaiqing Lin', 'Jialiang Shen', 'Lubin Weng', 'Jing Dong', 'Yan Wang', 'Shu Wu']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'Deepfake forensics', 'Manifold-based detection', 'Robustness / Generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02222</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization</title><link>https://arxiv.org/abs/2602.02175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIEC, a framework that couples implicit and explicit cues to perform weakly-supervised multimodal manipulation localization using only image- and sentence-level labels.&lt;/li&gt;&lt;li&gt;Image branch (TRPS) fuses visual and textual forgery cues with spatial priors, plus background silencing and spatial contrast constraints to localize suspicious regions.&lt;/li&gt;&lt;li&gt;Text branch (VCTG) grounds meaningful content tokens using relative visual bias and uses asymmetric sparse and semantic consistency constraints to mitigate label noise.&lt;/li&gt;&lt;li&gt;Reports results comparable to fully supervised methods on several evaluation metrics for detecting/localizing manipulated image–text pairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinquan Yu', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal forensics', 'manipulation detection', 'weakly-supervised localization', 'misinformation defense', 'multimodal learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02175</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning</title><link>https://arxiv.org/abs/2602.02004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies "reasoning drift" in multimodal reasoning models where clue-gathering focuses on question-irrelevant entities, causing hallucinations.&lt;/li&gt;&lt;li&gt;Introduces ClueRecall, a metric to assess visual clue retrieval, and ClueTracer, a training-free, parameter-free plugin that traces question→outputs→visual tokens to localize task-relevant patches and suppress spurious attention.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in hallucination suppression across multiple reasoning architectures (≈1.21×) and gains when applied to non-reasoning settings (≈1.14×) without additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gongli Xi', 'Kun Wang', 'Zeming Gao', 'Huahui Yi', 'Haolang Lu', 'Ye Tian', 'Wendong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination suppression', 'multimodal robustness', 'visual grounding', 'training-free defense', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02004</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models</title><link>https://arxiv.org/abs/2602.01738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;A simple linear classifier on frozen features from vision foundation models (Perception Encoder, MetaCLIP 2, DINOv3) establishes new state-of-the-art for AI-generated image (AIGI) detection, especially on in-the-wild distributions.&lt;/li&gt;&lt;li&gt;The paper attributes this generalization to emergent capabilities from large-scale pretraining: vision-language models internalize a semantic notion of forgery while self-supervised models learn implicit forensic features.&lt;/li&gt;&lt;li&gt;It thoroughly evaluates across standard benchmarks, unseen generators, and real-world data, showing large accuracy gains over specialized detectors, but documents persistent failures (recapture/transmission, VAE reconstructions, localized edits).&lt;/li&gt;&lt;li&gt;Advocates shifting AI forensics from overfitting static benchmarks to leveraging evolving world knowledge in foundation models for real-world reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Zhou', 'Xinan He', 'Kaiqing Lin', 'Bing Fan', 'Feng Ding', 'Bin Li']&lt;/li&gt;&lt;li&gt;Tags: ['AIGI detection', 'Forensic robustness', 'Foundation models', 'Generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01738</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2602.01574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGHA-Attack, a transfer-based targeted adversarial attack for vision-language models that uses a semantic-guided hierarchical alignment strategy.&lt;/li&gt;&lt;li&gt;Creates a visually grounded reference pool via a frozen text-to-image model, selects Top-K semantically relevant anchors, and forms a weighted mixture to guide optimization.&lt;/li&gt;&lt;li&gt;Injects target semantics across intermediate layers by aligning global and spatial visual features at multiple depths and synchronizing intermediate visual and textual features in a shared latent subspace.&lt;/li&gt;&lt;li&gt;Demonstrates improved targeted transferability to open-source and commercial black-box VLMs and robustness against preprocessing and purification defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haobo Wang', 'Weiqi Luo', 'Xiaojun Jia', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'transfer attacks', 'vision-language models', 'targeted attack', 'robustness/defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01574</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images</title><link>https://arxiv.org/abs/2602.01435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BioTamperNet, a model to detect duplicated/tampered regions in biomedical images and to localize their source counterparts.&lt;/li&gt;&lt;li&gt;Introduces affinity-guided self-attention and cross-attention modules plus SSM-inspired linear attention for efficient, fine-grained localization.&lt;/li&gt;&lt;li&gt;Trained end-to-end and evaluated on bio-forensic benchmarks, showing improved detection of duplicated regions versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumyaroop Nandi', 'Prem Natarajan']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'tampering-detection', 'biomedical-forensics', 'state-space-models', 'attention-mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01435</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts</title><link>https://arxiv.org/abs/2602.01369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Temporal Lipschitz-Guided Attacks (TLGA) to probe component-level vulnerabilities (routers and experts) in video Mixture-of-Experts (MoE) models.&lt;/li&gt;&lt;li&gt;Proposes Joint TLGA (J-TLGA) that jointly perturbs routers and experts to amplify adversarial effects and expose collaborative weaknesses of MoE architectures.&lt;/li&gt;&lt;li&gt;Develops Joint Temporal Lipschitz Adversarial Training (J-TLAT), a plug-and-play defense performing joint adversarial training to mitigate both independent and collaborative weaknesses while reducing inference cost vs dense models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songping Wang', 'Qinglong Liu', 'Yueming Lyu', 'Ning Li', 'Ziwen He', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'adversarial training', 'mixture-of-experts', 'robustness', 'video models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01369</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons</title><link>https://arxiv.org/abs/2602.01283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies monolingual safety neurons (MS-Neurons) in LLMs and validates their causal role in refusal/safety behavior via targeted activation and suppression.&lt;/li&gt;&lt;li&gt;Finds a small subset of cross-lingual shared safety neurons (SS-Neurons) that transfer safety capabilities from high-resource to non-high-resource languages; manipulating these neurons affects safety consistently across languages.&lt;/li&gt;&lt;li&gt;Proposes a neuron-oriented fine-tuning strategy that targets SS-Neurons to improve safety in NHR languages, outperforming prior methods while preserving general model capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianhui Zhang', 'Chengyu Xie', 'Linxia Zhu', 'Yonghui Yang', 'Weixiang Zhao', 'Zifeng Cheng', 'Cong Wang', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['safety defenses', 'neuron interpretability', 'cross-lingual robustness', 'targeted fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01283</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models</title><link>https://arxiv.org/abs/2602.01089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Differential Vector Erasure (DVE), a training-free, inference-time method to erase specific concepts from flow matching text-to-image models.&lt;/li&gt;&lt;li&gt;Core idea: compute a differential vector field representing directional discrepancy between a target concept and an anchor concept, then project the model's velocity field to remove concept-specific components.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on FLUX for tasks like NSFW suppression, artistic style removal, and object erasure while preserving image quality and diversity compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqi Zhang', 'Xinhao Zhong', 'Yi Sun', 'Shuoyang Sun', 'Bin Chen', 'Shu-Tao Xia', 'Xuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'generative model safety', 'inference-time defense', 'flow matching models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01089</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance</title><link>https://arxiv.org/abs/2602.01047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Decoding (ResDec), a training-free, history-aware decoding method that leverages token logits evolution and internal reasoning signals of LVLMs to correct language-prior biases.&lt;/li&gt;&lt;li&gt;Aims to mitigate hallucinations in Large Vision-Language Models by using historical information as residual guidance during decoding to improve visual grounding and reduce object hallucination.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing reduced hallucinations and improved performance on multiple LVLM benchmarks without additional model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinrong Chen', 'Xu Chu', 'Yingmin Qiu', 'Hengyuan Zhang', 'Jing Xiong', 'Shiyu Tang', 'Shuai Liu', 'Shaokang Yang', 'Cheng Yang', 'Hayden Kwok-Hay So', 'Ngai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'decoding strategies', 'robustness', 'safety', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01047</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis</title><link>https://arxiv.org/abs/2602.00821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an edge-native, inversion-free generative de-identification method (FlowEdit) for federated skin image analysis that preserves pathological features while removing biometric identity.&lt;/li&gt;&lt;li&gt;Introduces a 'Segment-by-Synthesis' mechanism to generate counterfactual healthy/pathological twin pairs and extract differential erythema masks decoupled from identity and semantic artifacts.&lt;/li&gt;&lt;li&gt;Targets client-side deployment on resource-constrained devices (near real-time &lt;20s) to produce synthetic surrogates that reduce risk of gradient/data leakage in federated learning.&lt;/li&gt;&lt;li&gt;Reports pilot validation with IoU stability &gt;0.67 across synthetic identities, demonstrating preservation of diagnostic fidelity while enhancing privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Moutselos', 'Ilias Maglogiannis']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'federated-learning', 'de-identification', 'generative-models', 'data-leakage-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00821</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering</title><link>https://arxiv.org/abs/2602.00621</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Decomposes dense visual embeddings in LVLMs using sparse autoencoders to obtain interpretable neurons and identify neuron types (always-on vs image-specific).&lt;/li&gt;&lt;li&gt;Finds hallucinations often stem from disruptions or spurious activations of image-specific neurons and shows that selectively amplifying/suppressing these neurons can control outputs and improve visual grounding.&lt;/li&gt;&lt;li&gt;Proposes Contrastive Neuron Steering (CNS): contrastive analysis between clean and noisy inputs to identify and amplify informative neurons while suppressing perturbation-induced activations, reducing hallucinations and remaining compatible with decoding-stage methods; validated on hallucination-focused and general multimodal benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangtao Lyu', 'Xinyi Cheng', 'Qi Liu', 'Chenghao Xu', 'Jiexi Yan', 'Muli Yang', 'Fen Fang', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'LVLM', 'neuron-level defenses', 'contrastive learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00621</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.00559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OmniVCHall, a benchmark for isolated and compositional hallucinations in video multimodal LLMs, including a taxonomy and adversarial answer options to reduce shortcut reasoning.&lt;/li&gt;&lt;li&gt;Evaluates 39 VLLMs and finds substantial degradation under compositional hallucination scenarios.&lt;/li&gt;&lt;li&gt;Proposes TriCD, a triple-pathway contrastive decoding framework that generates negative video variants (via an adaptive perturbation controller) and a saliency-guided enhancement module, optimized with reinforcement learning to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports consistent improvements (≈10% average accuracy) across two backbone models; dataset and code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbin Xing', 'Quanxing Zha', 'Lizheng Zu', 'Mengran Li', 'Ming Li', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'adversarial robustness', 'benchmark', 'contrastive decoding', 'video multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00559</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Text is All You Need for Vision-Language Model Jailbreaking</title><link>https://arxiv.org/abs/2602.00420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Text-DJ, a jailbreak attack for large vision-language models that exploits OCR by converting harmful textual prompts into multiple benign-looking image-embedded sub-queries and mixing them with irrelevant distraction queries.&lt;/li&gt;&lt;li&gt;Attack pipeline: (1) decompose harmful query into semantically related benign sub-queries, (2) select maximally irrelevant distraction queries, (3) present both as a grid of images (with sub-queries placed centrally) to induce OCR-based misinterpretation and evade text filters.&lt;/li&gt;&lt;li&gt;Demonstrates successful circumvention of state-of-the-art LVLM safety mechanisms, attributing success to bypassing text-based filters via images and to distraction-induced failure to associate fragmented sub-queries.&lt;/li&gt;&lt;li&gt;Highlights a new vulnerability in LVLM OCR pipelines for fragmented/multi-image inputs and calls for defenses tailored to fragmented multimodal attack vectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Chen', 'Zhao Xu', 'Youyuan Jiang', 'Tianle Zheng', 'Cho-Jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'OCR attacks', 'multimodal adversarial attack', 'LVLM security', 'safety bypass']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00420</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models</title><link>https://arxiv.org/abs/2602.00350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReLAPSe, a policy-based adversarial framework that uses reinforcement learning to recover concepts purportedly removed by machine unlearning in text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Uses Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic feedback signal to align prompt manipulation with latent visual residuals.&lt;/li&gt;&lt;li&gt;Shifts from per-instance optimization to a global policy, enabling efficient, near-real-time restoration of fine-grained identities and styles across multiple unlearning methods and serving as a scalable red-teaming tool.&lt;/li&gt;&lt;li&gt;Includes experiments on sensitive visual concepts and provides code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ignacy Kolton', 'Kacper Marzol', 'Pawe{\\l} Batorski', 'Marcin Mazur', 'Paul Swoboda', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'machine unlearning', 'prompt engineering', 'red teaming', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00350</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification</title><link>https://arxiv.org/abs/2602.00292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents LogicGaze, a benchmark of ~40k video segments and Flickr30k images designed to test whether Vision-Language Models (VLMs) can verify sequential causal chains against visual evidence.&lt;/li&gt;&lt;li&gt;Creates linguistically plausible but visually contradictory counterfactual perturbations and a three-part evaluation (Causal Validation, Grounded Narrative Synthesis, Perturbation Rejection) to expose hallucination and grounding failures.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art VLMs (e.g., Qwen2.5-VL-72B), finding significant vulnerabilities in causal grounding; releases anonymized dataset and evaluation resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rory Driscoll', 'Alexandros Christoforos', 'Chadbourne Davis']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'benchmarking', 'adversarial/perturbation testing', 'visual grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00292</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange</title><link>https://arxiv.org/abs/2602.00192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that state-of-the-art inpainting detectors rely on global VAE-induced spectral artifacts across images rather than on locally synthesized content.&lt;/li&gt;&lt;li&gt;Introduces Inpainting Exchange (INP-X) to replace unedited regions with originals while keeping synthesized content, and releases a 90K test dataset of real, inpainted, and exchanged images.&lt;/li&gt;&lt;li&gt;Shows pretrained detectors' accuracy drops dramatically under INP-X (e.g., 91% → 55%), links failures to VAE high-frequency attenuation, and shows training on the new dataset improves content-aware detection and localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elif Nebioglu', 'Emirhan Bilgi\\c{c}', 'Adrian Popescu']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'deepfake-detection', 'detector-evasion', 'VAE-artifacts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00192</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios</title><link>https://arxiv.org/abs/2602.00109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of commercial presentation attack detection (PAD) subsystems within remote identity validation (RIV) workflows under low-light and automated image acquisition conditions.&lt;/li&gt;&lt;li&gt;Finds substantial performance degradation: ~4x increase in error rates under low-light and ~2x increase under auto-capture workflows; only one tested system maintained bona fide classification error &lt; 3% across scenarios.&lt;/li&gt;&lt;li&gt;Highlights the need for diverse environmental and procedural testing to ensure reliable PAD performance in real-world remote identity verification deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John J. Howard (SAIC Identity', 'Data Sciences Laboratory)', 'Richard O. Plesh (SAIC Identity', 'Data Sciences Laboratory)', 'Yevgeniy B. Sirotin (SAIC Identity', 'Data Sciences Laboratory)', 'Jerry L. Tipton (SAIC Identity', 'Data Sciences Laboratory)', 'Arun R. Vemury (U.S. Department of Homeland Security', 'Science', 'Technology Directorate)']&lt;/li&gt;&lt;li&gt;Tags: ['presentation attack detection', 'biometric spoofing', 'robustness evaluation', 'remote identity verification', 'environmental perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00109</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Leakage from Image Embeddings</title><link>https://arxiv.org/abs/2601.22929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes semantic leakage from compressed image embeddings: recovering high-level semantic information without reconstructing the original image.&lt;/li&gt;&lt;li&gt;Proposes SLImE, a lightweight inference framework that uses locally trained semantic retrievers with off-the-shelf models to extract tags, symbolic representations, and coherent descriptions from standalone embeddings.&lt;/li&gt;&lt;li&gt;Empirically demonstrates consistent semantic recovery across multiple embedding models (GEMINI, COHERE, NOMIC, CLIP), showing a fundamental privacy vulnerability in preserved embedding neighborhood structure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyi Chen', 'Qiongkai Xu', 'Desmond Elliott', 'Qiongxiu Li', 'Johannes Bjerva']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'model-inversion', 'embedding-leakage', 'semantic-extraction', 'adversarial-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22929</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens rather than whole documents to remove undesired capabilities (demonstrated on removing medical capabilities).&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and less costly to benign capabilities than document-level filtering, and effectiveness increases with model scale.&lt;/li&gt;&lt;li&gt;Introduces methodology for labeling tokens using sparse autoencoders and distilling efficient classifiers; shows robustness to noisy labels with sufficient pretraining compute.&lt;/li&gt;&lt;li&gt;Demonstrates that models trained with token filtering can still be aligned on the filtered domain and quantifies the compute slowdown for the forget domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['data-filtering', 'capability-control', 'model-safety', 'pretraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that progressive training on AI-generated medical data causes collapse in pathological diversity and diagnostic reliability across clinical text, vision-language reports, and synthesized medical images.&lt;/li&gt;&lt;li&gt;Shows rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain high but misplaced diagnostic confidence (false reassurance rates up to ~40%).&lt;/li&gt;&lt;li&gt;Finds degradation becomes clinically significant after only a few generations (confirmed by blinded physician evaluation) and evaluates mitigation approaches.&lt;/li&gt;&lt;li&gt;Reports that simple upscaling of synthetic data does not prevent collapse, whereas mixing real data with quality-aware filtering helps preserve diversity and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Yun Liu', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title><link>https://arxiv.org/abs/2512.13352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the interplay between Membership Inference Attacks (MIAs) and targeted training-data extraction from Large Language Models by integrating multiple MIA techniques into an extraction pipeline.&lt;/li&gt;&lt;li&gt;Provides a systematic benchmarking of different MIA methods within realistic extraction workflows, comparing their effectiveness to results from standard MIA benchmarks.&lt;/li&gt;&lt;li&gt;Evaluates the practical utility of MIAs for verifying whether extracted candidates were part of the training set, informing privacy risk assessments for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al Sahili', 'Ali Chehab', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'data-extraction', 'privacy', 'model-memorization', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13352</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Geometric-disentangelment Unlearning</title><link>https://arxiv.org/abs/2511.17100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Geometric-disentanglement Unlearning (GU), a projection-based method to reduce collateral retention degradation when removing a forget set from LLMs.&lt;/li&gt;&lt;li&gt;Provides a theoretical characterization: retain loss is locally invariant iff the update direction is orthogonal to the subspace spanned by retain gradients (optimizer-induced geometry).&lt;/li&gt;&lt;li&gt;GU is a lightweight, plug-and-play projection applied to gradient-based unlearning methods and is empirically shown to improve forgetting strength while reducing retain drift across multiple benchmarks.&lt;/li&gt;&lt;li&gt;Open-sourced implementation and evaluation showing substantial gains (e.g., up to 62% improved forgetting Extraction Strength and 31% higher retain ES when combined with SimNPO).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Yuji Zhang', 'Tianxin Wei', 'Ruizhong Qiu', 'Ke Yang', 'Xiao Lin', 'Cheng Qian', 'Jingrui He', 'Hanghang Tong', 'Chengxiang Zhai', 'Heng Ji', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'privacy-preserving ML', 'model editing', 'defenses', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17100</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization</title><link>https://arxiv.org/abs/2511.16209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PSM: append a protective textual SHIELD to system prompts to reduce prompt-extraction leakage while preserving task utility.&lt;/li&gt;&lt;li&gt;Formalizes prompt hardening as a utility-constrained black-box optimization problem and uses an LLM-as-optimizer to search SHIELD space.&lt;/li&gt;&lt;li&gt;Evaluates against a suite of adversarial extraction attacks, showing substantial reduction in prompt leakage and outperforming existing baseline defenses.&lt;/li&gt;&lt;li&gt;Method is lightweight, works with black-box API access only, and the authors release code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huseein Jawad', 'Nicolas Brunel']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-extraction', 'defense', 'black-box optimization', 'adversarial attacks', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16209</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC (Sequential Tool Attack Chaining): an automated framework that constructs multi-step tool call chains where each step appears benign but together enable harmful actions.&lt;/li&gt;&lt;li&gt;Empirically evaluates 483 STAC cases (1,352 interaction sets) across domains and agent types, showing very high attack success rates (ASR &gt;90% in most cases) against state-of-the-art agents including GPT-4.1.&lt;/li&gt;&lt;li&gt;Provides an automated closed-loop pipeline that synthesizes, validates via in-environment execution, and reverse-engineers stealthy multi-turn prompts to reliably induce agents to run malicious sequences.&lt;/li&gt;&lt;li&gt;Performs defense analysis showing prompt-based defenses are limited and proposes a reasoning-driven defense prompt that reduces ASR by up to 28.8%, highlighting need to reason over full action sequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'agent tool abuse', 'automated red teaming', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection</title><link>https://arxiv.org/abs/2505.16530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DuFFin, a dual-level fingerprinting framework for black-box ownership verification of LLMs that combines trigger-pattern extraction and knowledge-level fingerprints.&lt;/li&gt;&lt;li&gt;Designed to detect model theft or unauthorized deployment across model variants (fine-tuning, quantization, safety alignment) without affecting generation.&lt;/li&gt;&lt;li&gt;Evaluated on multiple open-source base models and their variants, achieving high identification performance (IP-ROC &gt; 0.95).&lt;/li&gt;&lt;li&gt;Code released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuliang Yan', 'Haochun Tang', 'Shuo Yan', 'Enyan Dai']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'watermarking/IP protection', 'model theft defense', 'black-box verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16530</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title><link>https://arxiv.org/abs/2410.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model-substitution integrity risk when users outsource LLM inference to decentralized/remote providers.&lt;/li&gt;&lt;li&gt;Proposes SVIP, a secret-based verifiable inference protocol that requires providers to return generated text plus processed hidden representations and uses a proxy task to produce a unique model identifier.&lt;/li&gt;&lt;li&gt;Analyzes robustness under strong/adaptive adversaries and integrates a secret mechanism to strengthen security.&lt;/li&gt;&lt;li&gt;Empirical results show low false negative (&lt;5%) and false positive (&lt;3%) rates and very low verification latency (&lt;0.01s per prompt).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Sun', 'Yuhang Li', 'Yue Zhang', 'Yuchen Jin', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable inference', 'model integrity', 'remote inference security', 'attack detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22307</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models</title><link>https://arxiv.org/abs/2601.13433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLMs show authority bias by following endorsements from personas of varying expertise across 4 datasets in mathematical, legal, and medical domains using 11 models.&lt;/li&gt;&lt;li&gt;Finds that higher-authority endorsements increase susceptibility to incorrect or misleading guidance, degrading accuracy and increasing model confidence in wrong answers.&lt;/li&gt;&lt;li&gt;Demonstrates that authority bias is mechanistically encoded in the models and that models can be steered away from this bias, improving performance even when an expert gives misleading endorsements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyanka Mary Mammen', 'Emil Joswin', 'Shankar Venkitachalam']&lt;/li&gt;&lt;li&gt;Tags: ['authority-bias', 'social-engineering', 'prompt-injection', 'defense/steering', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13433</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor</title><link>https://arxiv.org/abs/2601.05752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoMonitor-Bench, a benchmark of 3,010 annotated test samples (QA, code generation, reasoning) with paired misbehavior and benign instances and two evaluation metrics: Miss Rate (MR) and False Alarm Rate (FAR).&lt;/li&gt;&lt;li&gt;Evaluates 22 LLMs (12 proprietary, 10 open-source), revealing substantial variability in monitoring performance and a consistent trade-off between missed detections and false alarms.&lt;/li&gt;&lt;li&gt;Builds a large training corpus (153,581 samples) and fine-tunes Qwen3-4B-Instruction to test whether training on known misbehaviors improves detection of unseen/implicit misbehaviors; highlights challenges in reliable, scalable monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yang', 'Jingyu Hu', 'Tong Li', 'Hanqi Yan', 'Wenxuan Wang', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['misbehavior monitoring', 'safety/defense', 'benchmark', 'LLM robustness', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05752</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems</title><link>https://arxiv.org/abs/2512.01661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnsolvableQA, a multi-domain dataset created by injecting logical contradictions into valid reasoning chains to produce solvable and objectively unsolvable questions.&lt;/li&gt;&lt;li&gt;Proposes UnsolvableRL, a reinforcement learning alignment method that trains LLMs to detect objective unsolvability while calibrating confidence for capability-limited tasks, reducing hallucinations.&lt;/li&gt;&lt;li&gt;Reports empirical gains: &gt;85% unsolvability detection and substantial improvement in solvable reasoning accuracy (e.g., 43.4% → 69.4% on Qwen3-4B-Instruct).&lt;/li&gt;&lt;li&gt;Analyzes training dynamics: strict alignment constraints can cause Capability Collapse without unsolvable data but act as a regularizer when such data are included, improving robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengyun Peng', 'Qiguang Chen', 'Bofei Liu', 'Jiannan Guan', 'Libo Qin', 'Zheng Yan', 'Jinhao Liu', 'Jianshu Zhang', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'hallucination detection', 'alignment', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01661</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios</title><link>https://arxiv.org/abs/2512.00920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reward Auditor, a hypothesis-testing framework to infer whether reward models (RMs) exhibit systematic vulnerabilities under real-world perturbations rather than just measuring raw preference accuracy.&lt;/li&gt;&lt;li&gt;Audits distributional degradation in RM preference-perception confidence to quantify statistical significance and effect size of vulnerabilities (suitability = conditional reliability under perturbations).&lt;/li&gt;&lt;li&gt;Aims to provide actionable inference about both certainty and severity of RM failures to support more robust and verifiable LLM alignment systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianxiang Zang', 'Yongda Wei', 'Ruxue Bai', 'Shiyu Jiang', 'Nijia Mo', 'Binhong Li', 'Qiang Sun', 'Hui Liu']&lt;/li&gt;&lt;li&gt;Tags: ['reward-modeling', 'robustness-evaluation', 'auditing', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00920</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning</title><link>https://arxiv.org/abs/2511.04120</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RIDE, an adversarial question-rewriting framework that uses Item Response Theory (IRT) to measure and control the difficulty of perturbed mathematical problems.&lt;/li&gt;&lt;li&gt;Simulates 35 LLMs as 'students' to build a difficulty ranker; uses that ranker as a reward signal in reinforcement learning to guide generation of well-posed, intrinsically harder question variants.&lt;/li&gt;&lt;li&gt;Applies the method to competition-level math benchmarks and finds an average 21.73% performance drop across 26 advanced LLMs, demonstrating reduced robustness in mathematical reasoning.&lt;/li&gt;&lt;li&gt;Provides a structured adversarial evaluation/benchmarking methodology to systematically evolve and assess question difficulty and model robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyuan Li', 'Murong Xu', 'Wenbiao Tao', 'Hanlun Zhu', 'Yike Zhao', 'Jipeng Zhang', 'Yunshi Lan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness evaluation', 'red teaming', 'mathematical reasoning', 'Item Response Theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04120</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety</title><link>https://arxiv.org/abs/2510.16492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'quitting' as a simple behavioral defense for LLM agents to withdraw when uncertain, reducing risky actions in multi-turn tool-using scenarios.&lt;/li&gt;&lt;li&gt;Systematically evaluates quitting across 12 state-of-the-art models using the ToolEmu framework, reporting substantial safety improvements (+0.39 on a 0-3 scale overall; +0.64 for proprietary models) with negligible helpfulness loss (-0.03).&lt;/li&gt;&lt;li&gt;Argues that explicit quit instructions are an immediately deployable guardrail and an effective first-line defense for autonomous agents in high-stakes environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vamshi Krishna Bonagiri', 'Ponnurangam Kumaragurum', 'Khanh Nguyen', 'Benjamin Plaut']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'agent_safety', 'guardrails', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16492</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT, an adaptive detector that learns a witness function from training data to augment logits-based LLM-generated text detection.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the proposed method.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to ~37%) over state-of-the-art logits-based detectors across multiple datasets and source LLMs.&lt;/li&gt;&lt;li&gt;Offers a Python implementation for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'defense', 'logits-based detection', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Localization: Building RabakBench with Human-in-the-Loop Validation to Measure Multilingual Safety Gaps</title><link>https://arxiv.org/abs/2507.05980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RabakBench, a multilingual safety benchmark focused on low-resource/localized language varieties (Singlish, Chinese, Malay, Tamil) with over 5,000 labeled examples across six safety categories.&lt;/li&gt;&lt;li&gt;Uses an LLM-driven red-teaming generation stage, semi-automated LLM labelers with human oversight, and toxicity-preserving translation to build the dataset with reported inter-annotator agreement of 0.70–0.80.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art guardrails and finds significant performance degradation in localized language varieties, highlighting safety vulnerabilities and the need for localized evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Chua', 'Leanne Tan', 'Ziyu Ge', 'Roy Ka-Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmark', 'multilingual-safety', 'red-teaming', 'guardrails', 'dataset-construction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05980</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title><link>https://arxiv.org/abs/2507.04531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-Fusion, a differential privacy-based inference mechanism that bounds the influence of sensitive context tokens on LLM outputs by blending baseline (no-sensitive-token) and sensitive-token-conditioned distributions.&lt;/li&gt;&lt;li&gt;Targets document privatization: paraphrasing documents containing sensitive tokens so attackers cannot reliably infer them while preserving high text quality; trade-off controlled by ε (privacy budget).&lt;/li&gt;&lt;li&gt;Provides per-token provable privacy guarantees at inference time and empirical results showing substantially improved privacy/utility (e.g., ~6× lower perplexity than related DPI methods).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushil Thareja', 'Preslav Nakov', 'Praneeth Vepakomma', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'inference-time privacy', 'LLM security', 'document privatization', 'prompt injection mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04531</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reverse Engineering Human Preferences with Reinforcement Learning</title><link>https://arxiv.org/abs/2505.15795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates an adversarial method that uses reinforcement learning to train a generator that produces preambles upstream of frozen LLMs to maximize scores assigned by judge-LLMs (LLM-as-a-judge).&lt;/li&gt;&lt;li&gt;Shows this preamble-based tuning yields higher LLM-evaluation scores than existing editing/response-intervention approaches and is difficult to detect.&lt;/li&gt;&lt;li&gt;Finds the attack transfers to candidate/judge models not seen during training, highlighting a vulnerability in scalable LLM evaluation pipelines.&lt;/li&gt;&lt;li&gt;Discusses implications for designing more robust LLM-as-a-judge evaluation settings and the risk of reverse-engineering human preference signals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lisa Alazraki', 'Tan Yi-Chern', 'Jon Ander Campos', 'Maximilian Mozes', 'Marek Rei', 'Max Bartolo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'evaluation manipulation', 'reward hacking', 'reinforcement learning', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15795</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Code-Mixed Phonetic Perturbations for Red-Teaming LLMs</title><link>https://arxiv.org/abs/2505.14226</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CMP-RT, a red-teaming attack that combines code-mixing and phonetic perturbations to alter safety-critical tokens while preserving phonetics and human interpretability.&lt;/li&gt;&lt;li&gt;Identifies a tokenizer-level vulnerability in transformer models that allows harmful prompts to bypass safety alignment and defenses, demonstrating effectiveness on SOTA models (e.g., Gemini-3-Pro).&lt;/li&gt;&lt;li&gt;Shows the attack is robust, scalable, generalizes across modalities, and highlights tokenization as an under-examined attack surface in current safety pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', 'Siddharth D Jaiswal']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'tokenizer-vulnerabilities', 'adversarial-perturbations', 'prompt-injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14226</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</title><link>https://arxiv.org/abs/2602.02455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Drift-Bench, a diagnostic benchmark for LLM agents that evaluates multi-turn clarification and agentic pragmatics under input faults (e.g., missing parameters, false presuppositions, ambiguity).&lt;/li&gt;&lt;li&gt;Provides a unified taxonomy of cooperative breakdowns, a persona-driven user simulator, and the Rise evaluation protocol to measure grounded execution risk in state- and service-oriented environments.&lt;/li&gt;&lt;li&gt;Empirical results show substantial performance degradation under input faults and variation in clarification effectiveness across user personas and fault types, highlighting failure modes that can lead to unsafe executions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Bao', 'Zheyuan Zhang', 'Pengcheng Jing', 'Zhengqing Yuan', 'Kaiwen Shi', 'Yanfang Ye']&lt;/li&gt;&lt;li&gt;Tags: ['agent-safety', 'robustness', 'benchmark', 'input-faults', 'clarification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02455</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RACA: Representation-Aware Coverage Criteria for LLM Safety Testing</title><link>https://arxiv.org/abs/2602.02280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RACA, a representation-aware coverage framework tailored for LLM safety testing that identifies safety-critical internal representations to measure test suite quality.&lt;/li&gt;&lt;li&gt;Uses a small expert-curated calibration set of jailbreak prompts to extract safety-relevant representations, computes conceptual activation scores, and defines six sub-coverage criteria (individual and compositional).&lt;/li&gt;&lt;li&gt;Evaluates RACA showing it outperforms neuron-level coverage criteria in identifying high-quality jailbreak prompts and demonstrates applications like test prioritization and attack prompt sampling, with claimed robustness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Zhixin Zhang', 'Chengcan Wu', 'Yihao Zhang', 'Xiaokun Luan', 'Meng Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaks', 'safety testing', 'coverage criteria', 'representation engineering', 'adversarial prompts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02280</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EvoMU: Evolutionary Machine Unlearning</title><link>https://arxiv.org/abs/2602.02139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EvoMU, an evolutionary search procedure that automatically discovers dataset- and task-specific unlearning loss functions to remove specified training data while preserving utility.&lt;/li&gt;&lt;li&gt;Demonstrates that synthesized losses can match or outperform existing loss-based unlearning methods on benchmarks (TOFU-5%, TOFU-10%, MUSE, WMDP) using a modest 4B-parameter model.&lt;/li&gt;&lt;li&gt;Frames the approach as automated scientific discovery (AI co-scientist) for privacy-preserving ML and releases code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pawel Batorski', 'Paul Swoboda']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy-preserving ML', 'defense', 'automated loss search', 'evolutionary algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02139</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality</title><link>https://arxiv.org/abs/2602.01703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AGT^AO, a unified machine unlearning framework for LLMs combining Adaptive Orthogonality (AO) and Adversarial Gating Training (AGT) to balance robust data erasure with utility preservation.&lt;/li&gt;&lt;li&gt;Adaptive Orthogonality dynamically mitigates geometric gradient conflicts between forgetting and retention objectives to reduce unintended knowledge degradation.&lt;/li&gt;&lt;li&gt;Adversarial Gating Training frames unlearning as a latent-space min–max game and uses a curriculum-based gating mechanism to simulate and defend against internal/adversarial recovery attempts.&lt;/li&gt;&lt;li&gt;Empirical results claim strong unlearning efficacy (KUR ≈ 0.01) while maintaining downstream utility (MMLU 58.30).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengyu Li', 'Lingling Zhang', 'Zhitao Gao', 'Yanrui Wu', 'Yuxuan Dong', 'Huan Liu', 'Bifan Wei', 'Jun Liu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'defense', 'adversarial training', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01703</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs</title><link>https://arxiv.org/abs/2602.01600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Expected Harm, a safety-evaluation metric that weights jailbreak severity by execution likelihood (modeled as a function of execution cost).&lt;/li&gt;&lt;li&gt;Empirical evaluation shows 'Inverse Risk Calibration': models more readily refuse high-cost/low-likelihood threats while remaining vulnerable to low-cost/high-likelihood queries.&lt;/li&gt;&lt;li&gt;Demonstrates exploitability by leveraging this miscalibration to double attack success rates for existing jailbreaks in some cases.&lt;/li&gt;&lt;li&gt;Uses linear probing to show models encode severity but lack an internal representation of execution cost, identifying a root cause for the vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Zhi Rui Tam', 'Cheng-Kuang Wu', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety-evaluation', 'vulnerability-analysis', 'red-teaming', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01600</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title><link>https://arxiv.org/abs/2602.01539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAGIC, a multi-turn multi-agent reinforcement learning framework that frames LLM safety alignment as an adversarial attacker-defender game.&lt;/li&gt;&lt;li&gt;Attacker agent iteratively rewrites queries into deceptive prompts while the defender agent learns policies to detect and refuse malicious inputs, producing co-evolution that finds long-tail vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of game equilibrium and safety guarantees, and shows empirically that the co-evolving training yields novel attack strategies and improved defense success without harming helpfulness.&lt;/li&gt;&lt;li&gt;Code released (link) and extensive experiments validate effectiveness against evolving adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wen', 'Zhida He', 'Han Qi', 'Ziyu Wan', 'Zhongtian Ma', 'Ying Wen', 'Tianhang Zheng', 'Xingcheng Xu', 'Chaochao Lu', 'Qiaosheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'defense', 'red-teaming', 'robustness', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01539</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings</title><link>https://arxiv.org/abs/2602.01363</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantifies leakage of demographic attributes (gender, age, accent) from SimCLR-trained speaker embeddings using linear and nonlinear probes.&lt;/li&gt;&lt;li&gt;Evaluates two mitigation strategies: adversarial training via gradient reversal and a causal bottleneck architecture that separates demographic and residual information.&lt;/li&gt;&lt;li&gt;Finds gender is strongly and linearly encoded, age and accent are weaker/nonlinear; adversarial debiasing reduces gender leakage but harms verification accuracy, while the causal bottleneck suppresses leakage more but causes substantial performance degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mari\\"ette Olijslager', 'Seyed Sahand Mohammadi Ziabari', 'Ali Mohammed Mansoor Alsahag']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'demographic leakage', 'adversarial debiasing', 'causal bottleneck', 'speaker verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01363</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability</title><link>https://arxiv.org/abs/2602.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GradingAttack, an adversarial attack framework targeting LLM-based automatic short answer grading (ASAG) systems.&lt;/li&gt;&lt;li&gt;Develops token-level and prompt-level attack strategies to manipulate grading outcomes while maintaining camouflage.&lt;/li&gt;&lt;li&gt;Proposes a novel metric that balances attack success and camouflage to quantify stealthiness.&lt;/li&gt;&lt;li&gt;Evaluates attacks across multiple datasets showing prompt-level attacks achieve higher success while token-level attacks have better camouflage; stresses need for defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xueyi Li', 'Zhuoneng Zhou', 'Zitao Liu', 'Yongdong Wu', 'Weiqi Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'prompt-level-attacks', 'token-level-attacks', 'automatic-grading-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00979</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2602.00085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CARE-RFT, a reinforcement finetuning method that replaces standard reverse KL regularization with a skew reverse KL divergence to provide a confidence-sensitive penalty.&lt;/li&gt;&lt;li&gt;The skew reverse KL is bounded for confident, consistently rewarded explorations (enabling reasoning) and unbounded elsewhere (preserving calibration), aiming to reduce hallucinations while allowing capability gains.&lt;/li&gt;&lt;li&gt;Extensive experiments across model scales and RFT algorithms show CARE-RFT matches unconstrained RFT's reasoning performance while recovering base-model trustworthiness and calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Jincheng Cao', 'Bodun Hu', 'Aryan Mokhtari', 'Leqi Liu', 'Amy Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'calibration', 'hallucination-mitigation', 'reinforcement-finetuning', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00085</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings</title><link>https://arxiv.org/abs/2602.01757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zero2Text, a training-free, recursive online alignment framework that combines LLM priors with dynamic ridge regression to perform embedding inversion without in-domain training data.&lt;/li&gt;&lt;li&gt;Targets strict black-box, cross-domain settings for retrieval-augmented systems and vector databases, enabling sentence/ text recovery from embeddings without leaked pairs.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains over prior methods (e.g., higher ROUGE-L and BLEU-2 on MS MARCO against an OpenAI victim model) and shows standard defenses like differential privacy are insufficient against this adaptive attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doohyun Kim', 'Donghwa Kang', 'Kyungjae Lee', 'Hyeongboo Baek', 'Brent Byunghoon Kang']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy leakage', 'black-box attack', 'adaptive attack', 'defense-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01757</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>WorldCup Sampling for Multi-bit LLM Watermarking</title><link>https://arxiv.org/abs/2602.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WorldCup, a multi-bit watermarking framework that embeds message bits directly into token sampling via a hierarchical competition mechanism.&lt;/li&gt;&lt;li&gt;Uses entropy-aware modulation to preserve generation quality and confidence-aware decoding for robust message recovery.&lt;/li&gt;&lt;li&gt;Demonstrates improved trade-offs across capacity, detectability, robustness, text quality, and decoding efficiency compared to prior multi/zero-bit watermarking baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Li Guo']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model attribution', 'steganography', 'robustness', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01752</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title><link>https://arxiv.org/abs/2602.01725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafePred, a predictive guardrail framework for computer-using agents (CUAs) that uses world-model-based future risk prediction to align predicted risks with current decisions.&lt;/li&gt;&lt;li&gt;Provides two main capabilities: (1) short- and long-term risk prediction using safety policies and world-model semantic representations to identify/prune high-risk actions, and (2) decision optimization via step-level interventions and task-level re-planning.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing substantial reductions in high-risk behaviors (≈97.6% safety performance) and improvements in task utility (up to 21.4%) compared to reactive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yurun Chen', 'Zeyi Liao', 'Ping Yin', 'Taotao Xie', 'Keting Yin', 'Shengyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'predictive guardrails', 'world models', 'long-term risk mitigation', 'decision-time interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01725</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation</title><link>https://arxiv.org/abs/2602.01709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARTIS, a test-time scaling framework that performs iterative simulated interactions to decouple exploration from commitment and reduce real-world execution risk for agentic LLMs.&lt;/li&gt;&lt;li&gt;Identifies that naive LLM-based simulators miss rare but high-impact failure modes, limiting their usefulness for safe agentic decision making.&lt;/li&gt;&lt;li&gt;Introduces a risk-aware tool simulator trained via targeted data generation and rebalanced training to emphasize fidelity on failure-inducing actions.&lt;/li&gt;&lt;li&gt;Demonstrates that iterative simulation plus risk-aware simulation improves agent reliability on multi-turn and multi-step agentic benchmarks without incurring environmental risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingshan Zeng', 'Lingzhi Wang', 'Weiwen Liu', 'Liangyou Li', 'Yasheng Wang', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'agent-safety', 'test-time-scaling', 'robustness', 'simulator-based-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01709</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia</title><link>https://arxiv.org/abs/2602.01618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an agentic data-generation framework to produce culturally grounded, region-specific safety datasets for Southeast Asia, addressing limited native annotator resources.&lt;/li&gt;&lt;li&gt;Presents the SEA-Guard family of multilingual safeguard models trained on this dataset to detect regionally sensitive or harmful content.&lt;/li&gt;&lt;li&gt;Evaluates SEA-Guard across multiple benchmarks and cultural variants, showing improved regional harm detection while maintaining general safety performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Panuthep Tasawong', 'Jian Gang Ngui', 'Alham Fikri Aji', 'Trevor Cohn', 'Peerat Limkonchotiwat']&lt;/li&gt;&lt;li&gt;Tags: ['safeguards', 'dataset_generation', 'multilingual', 'cultural_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01618</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment</title><link>https://arxiv.org/abs/2602.01587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation to obtain provable l0-norm-style certificates against jailbreak/adversarial payloads by partitioning inputs into immutable structural prompts and mutable payloads.&lt;/li&gt;&lt;li&gt;Proposes Noise-Augmented Alignment Tuning (NAAT) to make the base LLM a semantic denoiser, addressing utility degradation on sparse contexts caused by randomized defenses.&lt;/li&gt;&lt;li&gt;Reports strong empirical results on Llama-3: reduces attack success rate for gradient-based jailbreaks from 84.2% to 1.2% while maintaining 94.1% benign utility, outperforming character-level baselines.&lt;/li&gt;&lt;li&gt;Provides a deterministic certificate guaranteeing robustness within a provable adversarial radius, shifting safety guarantees from single-pass inference to statistical ensemble stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehua Cheng', 'Jianwei Yang', 'Wei Dai', 'Jiahao Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'certified robustness', 'randomized smoothing', 'alignment tuning', 'adversarial defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01587</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection</title><link>https://arxiv.org/abs/2602.01240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses defense: zero-shot detection of LLM-generated text by selecting the best surrogate model per input rather than using a single fixed surrogate.&lt;/li&gt;&lt;li&gt;Finds detection performance varies with surrogate-source alignment and that a well-matched surrogate typically exists in a diverse pool.&lt;/li&gt;&lt;li&gt;Proposes DetectRouter, a prototype-based two-stage framework: (1) build discriminative prototypes from white-box models; (2) generalize to black-box sources by aligning geometric distances with observed detection scores.&lt;/li&gt;&lt;li&gt;Shows consistent improvements on EvoBench and MAGE benchmarks across multiple detectors and model families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ke Sun', 'Guangsheng Bao', 'Han Cui', 'Yue Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'zero-shot detection', 'surrogate models', 'prototype-based routing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01240</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Verification Required: The Impact of Information Credibility on AI Persuasion</title><link>https://arxiv.org/abs/2602.00970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixTalk, a game modeling strategic LLM-to-LLM communication with mixed verifiable and unverifiable claims and costly receiver verification.&lt;/li&gt;&lt;li&gt;Runs large-scale tournaments evaluating state-of-the-art LLM agents, revealing how agents exploit information credibility and where receivers are vulnerable to persuasion.&lt;/li&gt;&lt;li&gt;Proposes Tournament Oracle Policy Distillation (TOPD), an offline distillation method that improves receiver robustness to persuasive/deceptive senders when deployed in-context.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saaduddin Mahmud', 'Eugene Bagdasarian', 'Shlomo Zilberstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial persuasion', 'deception/robustness', 'red teaming', 'defense (robustness/distillation)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00970</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Unifying Adversarial Robustness and Training Across Text Scoring Models</title><link>https://arxiv.org/abs/2602.00857</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unifies the study of adversarial robustness for text scoring models (dense retrievers, rerankers, reward models), framing attacks as cases where irrelevant/rejected text outscores relevant/chosen text.&lt;/li&gt;&lt;li&gt;Analyzes shortcomings of existing adversarial training for language models, showing poor generalization across attack types.&lt;/li&gt;&lt;li&gt;Proposes multiple adversarial training methods for text scoring and shows that combining complementary methods improves robustness and task effectiveness.&lt;/li&gt;&lt;li&gt;Applies the approach to RLHF: adversarially trained reward models reduce reward hacking and support training better-aligned LLMs; code and models are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manveer Singh Tamber', 'Hosna Oyarhoseini', 'Jimmy Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'reward models', 'RLHF', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00857</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs via Calibration</title><link>https://arxiv.org/abs/2602.00619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework modeling safety alignment as a systematic distortion of a pre-alignment next-token distribution and casts weak-to-strong jailbreaking as a forecast aggregation problem.&lt;/li&gt;&lt;li&gt;Derives an optimal aggregation strategy (gradient shift in loss-induced dual space), shows logit-arithmetic methods are a special case under cross-entropy, and generalizes to other proper losses.&lt;/li&gt;&lt;li&gt;Introduces a new hybrid aggregation rule and demonstrates improved attack success rates and reduced 'jailbreak tax' on red-teaming benchmarks and frontier models (e.g., gpt-oss-120b).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Lu', 'Yongkang Guo', 'Yuqing Kong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-attacks', 'model-alignment', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00619</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems</title><link>https://arxiv.org/abs/2602.00428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the 'Mandela effect' — collective misremembering — in LLM-based multi-agent systems, characterizing its existence and causal factors.&lt;/li&gt;&lt;li&gt;Introduces MANBENCH, a benchmark evaluating agent behaviors across four task types and five interaction protocols to quantify collective memory bias.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLM-powered agents on MANBENCH and proposes mitigation strategies (prompt-level defenses like cognitive anchoring and source scrutiny, plus model-level alignment) that reduce the effect by ~74.4%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naen Xu', 'Hengyu An', 'Shuo Shi', 'Jinghuai Zhang', 'Chunyi Zhou', 'Changjiang Li', 'Tianyu Du', 'Zhihui Fu', 'Jun Wang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'LLM safety', 'misinformation', 'benchmark', 'defenses/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00428</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Content in Academic Peer Reviews</title><link>https://arxiv.org/abs/2602.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains and applies a detection model to identify AI-generated content in academic peer reviews at ICLR and Nature Communications.&lt;/li&gt;&lt;li&gt;Finds minimal AI-generated content before 2022, with a rapid increase through 2025 (≈20% of ICLR and ≈12% of NC reviews classified as AI-generated in 2025).&lt;/li&gt;&lt;li&gt;Highlights temporal dynamics with a sharp rise in NC between Q3 and Q4 2024 and calls for further study of implications for scholarly evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Shen', 'Kai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-content-detection', 'integrity', 'academic-peer-review', 'measurement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00319</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models</title><link>https://arxiv.org/abs/2601.13433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLMs show authority bias by following endorsements from personas of varying expertise across 4 datasets in mathematical, legal, and medical domains using 11 models.&lt;/li&gt;&lt;li&gt;Finds that higher-authority endorsements increase susceptibility to incorrect or misleading guidance, degrading accuracy and increasing model confidence in wrong answers.&lt;/li&gt;&lt;li&gt;Demonstrates that authority bias is mechanistically encoded in the models and that models can be steered away from this bias, improving performance even when an expert gives misleading endorsements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyanka Mary Mammen', 'Emil Joswin', 'Shankar Venkitachalam']&lt;/li&gt;&lt;li&gt;Tags: ['authority-bias', 'social-engineering', 'prompt-injection', 'defense/steering', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13433</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that progressive training on AI-generated medical data causes collapse in pathological diversity and diagnostic reliability across clinical text, vision-language reports, and synthesized medical images.&lt;/li&gt;&lt;li&gt;Shows rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain high but misplaced diagnostic confidence (false reassurance rates up to ~40%).&lt;/li&gt;&lt;li&gt;Finds degradation becomes clinically significant after only a few generations (confirmed by blinded physician evaluation) and evaluates mitigation approaches.&lt;/li&gt;&lt;li&gt;Reports that simple upscaling of synthetic data does not prevent collapse, whereas mixing real data with quality-aware filtering helps preserve diversity and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Yun Liu', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title><link>https://arxiv.org/abs/2512.08216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RF-Deep, a lightweight, post-hoc random-forest-based OOD detection method that uses hierarchical deep features anchored to predicted tumor segmentations.&lt;/li&gt;&lt;li&gt;Designed for 3D CT tumor segmentation models; leverages pretrained-then-finetuned backbones and limited outlier exposure without changing model architecture.&lt;/li&gt;&lt;li&gt;Evaluated on 2,056 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets, achieving AUROC &gt;93.5 (near) and &gt;99.0 (far), outperforming logit-based and radiomics baselines.&lt;/li&gt;&lt;li&gt;Emphasizes a lightweight, architecture-agnostic robustness mechanism to improve reliability of clinical tumor segmentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-Distribution Detection', 'Robustness', 'Medical Imaging', 'Post-hoc Defense', 'Random Forests']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08216</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>HAMLOCK: HArdware-Model LOgically Combined attacK</title><link>https://arxiv.org/abs/2510.19145</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HAMLOCK, a cross-layer backdoor attack that splits malicious logic between a minimally altered DNN (software) and a hardware Trojan (hardware) to trigger misclassification.&lt;/li&gt;&lt;li&gt;The model is subtly tuned to produce unique high activations on a few neurons when a trigger is present; a hardware Trojan monitors those neurons (MSB or 8-bit exponent) and then manipulates final logits to force targeted outputs.&lt;/li&gt;&lt;li&gt;Demonstrates near-perfect attack success rates on MNIST, CIFAR-10, GTSRB, and ImageNet with negligible clean accuracy loss and extremely low hardware overhead (~0.01%), evading state-of-the-art model-level defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanskar Amgain', 'Daniel Lobo', 'Atri Chatterjee', 'Swarup Bhunia', 'Fnu Suya']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'hardware-trojan', 'cross-layer-attack', 'supply-chain', 'defense-evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19145</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT, an adaptive detector that learns a witness function from training data to augment logits-based LLM-generated text detection.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the proposed method.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to ~37%) over state-of-the-art logits-based detectors across multiple datasets and source LLMs.&lt;/li&gt;&lt;li&gt;Offers a Python implementation for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'defense', 'logits-based detection', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC (Sequential Tool Attack Chaining): an automated framework that constructs multi-step tool call chains where each step appears benign but together enable harmful actions.&lt;/li&gt;&lt;li&gt;Empirically evaluates 483 STAC cases (1,352 interaction sets) across domains and agent types, showing very high attack success rates (ASR &gt;90% in most cases) against state-of-the-art agents including GPT-4.1.&lt;/li&gt;&lt;li&gt;Provides an automated closed-loop pipeline that synthesizes, validates via in-environment execution, and reverse-engineers stealthy multi-turn prompts to reliably induce agents to run malicious sequences.&lt;/li&gt;&lt;li&gt;Performs defense analysis showing prompt-based defenses are limited and proposes a reasoning-driven defense prompt that reduces ASR by up to 28.8%, highlighting need to reason over full action sequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'agent tool abuse', 'automated red teaming', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GHOST, an automated attack that optimizes in image embedding space to generate natural-looking images that induce object hallucinations in multimodal LLMs.&lt;/li&gt;&lt;li&gt;Produces images via conditioning a diffusion model on optimized embeddings to remain visually natural and object-free while triggering model misperception.&lt;/li&gt;&lt;li&gt;Demonstrates high hallucination success rates across models (e.g., &gt;28% vs ~1% for prior methods) and cross-model transferability (images optimized for one model trigger hallucinations in others).&lt;/li&gt;&lt;li&gt;Uses the generated dataset to fine-tune models and reduce hallucination, framing GHOST as both a diagnostic red-teaming tool and a source for defense/mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'model-hallucination', 'red-teaming', 'multimodal-attack', 'defense-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Localization: Building RabakBench with Human-in-the-Loop Validation to Measure Multilingual Safety Gaps</title><link>https://arxiv.org/abs/2507.05980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RabakBench, a multilingual safety benchmark focused on low-resource/localized language varieties (Singlish, Chinese, Malay, Tamil) with over 5,000 labeled examples across six safety categories.&lt;/li&gt;&lt;li&gt;Uses an LLM-driven red-teaming generation stage, semi-automated LLM labelers with human oversight, and toxicity-preserving translation to build the dataset with reported inter-annotator agreement of 0.70–0.80.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art guardrails and finds significant performance degradation in localized language varieties, highlighting safety vulnerabilities and the need for localized evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Chua', 'Leanne Tan', 'Ziyu Ge', 'Roy Ka-Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety-benchmark', 'multilingual-safety', 'red-teaming', 'guardrails', 'dataset-construction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05980</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title><link>https://arxiv.org/abs/2507.04531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-Fusion, a differential privacy-based inference mechanism that bounds the influence of sensitive context tokens on LLM outputs by blending baseline (no-sensitive-token) and sensitive-token-conditioned distributions.&lt;/li&gt;&lt;li&gt;Targets document privatization: paraphrasing documents containing sensitive tokens so attackers cannot reliably infer them while preserving high text quality; trade-off controlled by ε (privacy budget).&lt;/li&gt;&lt;li&gt;Provides per-token provable privacy guarantees at inference time and empirical results showing substantially improved privacy/utility (e.g., ~6× lower perplexity than related DPI methods).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushil Thareja', 'Preslav Nakov', 'Praneeth Vepakomma', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'inference-time privacy', 'LLM security', 'document privatization', 'prompt injection mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04531</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Hardware-Triggered Backdoors</title><link>https://arxiv.org/abs/2601.21902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel backdoor attack that leverages small numerical differences across hardware (e.g., GPUs) to trigger different model predictions on different devices.&lt;/li&gt;&lt;li&gt;Method: move the decision boundary locally near target inputs and exploit hardware-induced numerical deviations to flip predictions on selected hardware.&lt;/li&gt;&lt;li&gt;Empirical evaluation demonstrates reliable hardware-triggered backdoors across common GPU accelerators.&lt;/li&gt;&lt;li&gt;Paper also investigates and evaluates potential defenses against this new attack vector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonas M\\"oller', 'Erik Imgrund', 'Thorsten Eisenhofer', 'Konrad Rieck']&lt;/li&gt;&lt;li&gt;Tags: ['backdoors', 'hardware attacks', 'model integrity', 'numerical robustness', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21902</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation</title><link>https://arxiv.org/abs/2601.21636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops sampling-free privacy amplification bounds for differentially private matrix mechanisms under random allocation (balls-in-bins) using Rényi divergence and conditional composition.&lt;/li&gt;&lt;li&gt;Introduces a dynamic programming formulation to compute Rényi-divergence-based bounds efficiently and complements it with conditional composition bounds that are tighter for small ε.&lt;/li&gt;&lt;li&gt;Works for arbitrary banded and non-banded matrices and removes reliance on Monte Carlo sampling approaches that require many samples or hold only with high probability.&lt;/li&gt;&lt;li&gt;Provides numerical comparisons demonstrating improved privacy accounting across a range of matrix mechanisms used in practice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jan Schuchardt', 'Nikita Kalinin']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy accounting', 'privacy amplification', 'Rényi divergence', 'matrix mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21636</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens rather than whole documents to remove undesired capabilities (demonstrated on removing medical capabilities).&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and less costly to benign capabilities than document-level filtering, and effectiveness increases with model scale.&lt;/li&gt;&lt;li&gt;Introduces methodology for labeling tokens using sparse autoencoders and distilling efficient classifiers; shows robustness to noisy labels with sufficient pretraining compute.&lt;/li&gt;&lt;li&gt;Demonstrates that models trained with token filtering can still be aligned on the filtered domain and quantifies the compute slowdown for the forget domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['data-filtering', 'capability-control', 'model-safety', 'pretraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AntiPaSTO: Self-Supervised Honesty Steering via Anti-Parallel Representations</title><link>https://arxiv.org/abs/2601.07473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AntiPaSTO, a self-supervised honesty-steering method that separates model representations along an antiparallel axis (+1/-1) with coherence constraints to avoid collapse.&lt;/li&gt;&lt;li&gt;Requires minimal human input (pairs of contrasting words in template sentences; no preference labels) and uses 800 pairs to steer Gemma-3-1B.&lt;/li&gt;&lt;li&gt;Empirically outperforms prompting baselines (6.9x on DailyDilemmas) and claims bidirectional, out-of-distribution transferable control where prompting may trigger refusal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael J. Clark']&lt;/li&gt;&lt;li&gt;Tags: ['Safety/Alignment', 'Honesty Steering', 'Self-supervised Defense', 'Representation-based Controls']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.07473</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title><link>https://arxiv.org/abs/2512.13352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the interplay between Membership Inference Attacks (MIAs) and targeted training-data extraction from Large Language Models by integrating multiple MIA techniques into an extraction pipeline.&lt;/li&gt;&lt;li&gt;Provides a systematic benchmarking of different MIA methods within realistic extraction workflows, comparing their effectiveness to results from standard MIA benchmarks.&lt;/li&gt;&lt;li&gt;Evaluates the practical utility of MIAs for verifying whether extracted candidates were part of the training set, informing privacy risk assessments for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al Sahili', 'Ali Chehab', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'data-extraction', 'privacy', 'model-memorization', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13352</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>UMM-RM: An Upcycle-and-Merge MoE Reward Model for Mitigating Reward Hacking</title><link>https://arxiv.org/abs/2512.00724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UMM-RM: convert a dense reward model into a Mixture-of-Experts (MoE) with shared experts that capture instruction-agnostic preference signals and task-specific experts for fine-grained preferences, then merge the experts into a single dense RM via learnable weights.&lt;/li&gt;&lt;li&gt;Designed as a defense against reward hacking: aims to reduce exploitability by providing expert diversity during training while avoiding MoE inference overhead after merging.&lt;/li&gt;&lt;li&gt;Empirical results across multiple base models and preference datasets show improved preference accuracy, reduced reward hacking during PPO training, and more stable preference alignment compared to standard dense RMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingling Fu', 'Yongfu Xue']&lt;/li&gt;&lt;li&gt;Tags: ['reward_hacking', 'reward_modeling', 'mixture_of_experts', 'RLHF', 'robustness_defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00724</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning</title><link>https://arxiv.org/abs/2511.23402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Quantized-TinyLLaVA, a multimodal foundation model with an integrated compression module for communication-efficient split learning that quantizes intermediate feature representations before transmission.&lt;/li&gt;&lt;li&gt;Derives a principled quantization strategy based on entropy coding to choose discrete representation levels; reports ~87.5% communication reduction with 2-bit quantization while maintaining original 16-bit performance across five benchmarks.&lt;/li&gt;&lt;li&gt;Evaluates privacy impact and reports that the compressed representations increase resilience to feature inversion attacks, indicating an added privacy-defense benefit in split learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajun Guo', 'Xin Luo', 'Jiayin Zheng', 'Yiqun Wang', 'Kai-Wei Chang', 'Wei Wang', 'Jie Liu']&lt;/li&gt;&lt;li&gt;Tags: ['split learning', 'quantization', 'privacy', 'feature inversion', 'communication efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23402</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Geometric-disentangelment Unlearning</title><link>https://arxiv.org/abs/2511.17100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses LLM unlearning: removing a forget set (private/harmful content) while preserving retained knowledge, aiming to reduce the forget–retain trade-off.&lt;/li&gt;&lt;li&gt;Provides a theoretical characterization: local retain invariance is equivalent to updates being orthogonal to the subspace spanned by retain gradients under optimizer-induced geometry.&lt;/li&gt;&lt;li&gt;Proposes Geometric-disentanglement Unlearning (GU), a lightweight projection-based method that can be plugged into gradient-based unlearning to mitigate side effects.&lt;/li&gt;&lt;li&gt;Empirical results on multiple benchmarks (TOFU, MUSE, WMDP-cyber) show improved forgetting (Extraction Strength) and reduced retain drift; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Yuji Zhang', 'Tianxin Wei', 'Ruizhong Qiu', 'Ke Yang', 'Xiao Lin', 'Cheng Qian', 'Jingrui He', 'Hanghang Tong', 'Chengxiang Zhai', 'Heng Ji', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'model editing', 'defense', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17100</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</title><link>https://arxiv.org/abs/2511.03774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and studies test-set contamination (data leakage) in Vision-Language Models (VLMs) and shows existing detection methods often fail or behave inconsistently.&lt;/li&gt;&lt;li&gt;Proposes a novel detection method based on multi-modal semantic perturbation: contaminated models fail to generalize under controlled image/text perturbations.&lt;/li&gt;&lt;li&gt;Validates the approach across multiple realistic contamination strategies and releases code and perturbed datasets for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaden Park', 'Mu Cai', 'Feng Yao', 'Jingbo Shang', 'Soochahn Lee', 'Yong Jae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'test-set leakage', 'vision-language models', 'detection', 'multi-modal perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03774</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Shielding for Safe Reinforcement Learning under Hidden-Parameter Dynamics Shifts</title><link>https://arxiv.org/abs/2506.11033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive Shielding: a framework for safe RL in constrained hidden-parameter MDPs that infers a low-dimensional representation of underlying dynamics online to adapt safety checks.&lt;/li&gt;&lt;li&gt;Uses a two-layer safety strategy: (1) safety-regularized policy optimization to avoid high-cost regions proactively, and (2) reactive adaptive shielding that forecasts safety risks from inferred dynamics and filters unsafe actions using uncertainty-aware conformal prediction bounds.&lt;/li&gt;&lt;li&gt;Provides theoretical connections between shielding prediction errors and bounds on the average cost rate, and demonstrates empirically improved return-safety trade-offs on Safe-Gym benchmarks with varying hidden parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minjae Kwon', 'Tyler Ingebrand', 'Ufuk Topcu', 'Lu Feng']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'runtime shielding / guardrails', 'robustness to dynamics shift', 'uncertainty quantification (conformal prediction)', 'hidden-parameter MDPs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11033</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Effective and Efficient Cross-City Traffic Knowledge Transfer: A Privacy-Preserving Perspective</title><link>https://arxiv.org/abs/2503.11963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedTT, a privacy-aware federated learning framework for cross-city traffic prediction addressing data scarcity.&lt;/li&gt;&lt;li&gt;Key components: (i) traffic view imputation to complete missing traffic data, (ii) traffic domain adapter to mitigate cross-city distribution discrepancies, (iii) traffic secret aggregation protocol for secure aggregation to protect privacy.&lt;/li&gt;&lt;li&gt;Evaluated on 4 real-world datasets and outperforms 14 state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zeng', 'Ziquan Fang', 'Yuting Huang', 'Lu Chen', 'Yunjun Gao']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving', 'secure aggregation', 'domain adaptation', 'traffic prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11963</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence</title><link>https://arxiv.org/abs/2502.04204</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies jailbreak attacks that use adversarial suffixes and proposes adversarial training (AT) that uses short adversarial suffixes to defend against longer suffix attacks.&lt;/li&gt;&lt;li&gt;Provides a theoretical analysis of adversarial in-context learning for linear transformers on linear regression, deriving a robust generalization bound dependent on Θ(√M_test)/M_train.&lt;/li&gt;&lt;li&gt;Empirically validates on open-source LLMs that AT with short adversarial suffixes reduces attack success for much longer jailbreak suffixes, showing a correlation with the √(test_length)/train_length ratio.&lt;/li&gt;&lt;li&gt;Releases code for experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaopeng Fu', 'Liang Ding', 'Jingfeng Zhang', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial-training', 'adversarial-examples', 'robustness', 'in-context-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04204</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title><link>https://arxiv.org/abs/2410.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model-substitution integrity risk when users outsource LLM inference to decentralized/remote providers.&lt;/li&gt;&lt;li&gt;Proposes SVIP, a secret-based verifiable inference protocol that requires providers to return generated text plus processed hidden representations and uses a proxy task to produce a unique model identifier.&lt;/li&gt;&lt;li&gt;Analyzes robustness under strong/adaptive adversaries and integrates a secret mechanism to strengthen security.&lt;/li&gt;&lt;li&gt;Empirical results show low false negative (&lt;5%) and false positive (&lt;3%) rates and very low verification latency (&lt;0.01s per prompt).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Sun', 'Yuhang Li', 'Yue Zhang', 'Yuchen Jin', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable inference', 'model integrity', 'remote inference security', 'attack detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22307</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild</title><link>https://arxiv.org/abs/2602.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a robust SASV (Spoofing Aware Speaker Verification) system combining a spoofing detector and speaker verification network operating in tandem.&lt;/li&gt;&lt;li&gt;Spoofing detector uses self-supervised speech embeddings with a graph neural network backend and a top-3 layer mixture-of-experts to fuse multi-level features for anti-spoofing.&lt;/li&gt;&lt;li&gt;Speaker verification employs a low-complexity convolutional network fusing 2D/1D multi-scale features, trained with SphereFace loss and contrastive circle loss for better discriminability.&lt;/li&gt;&lt;li&gt;Additional defenses include AS-Norm score normalization, fixed imposter cohorts, and model ensembling to improve robustness against spoofed utterances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnab Das', 'Yassine El Kheir', 'Enes Erdem Erdogan', 'Feidi Kallel', 'Tim Polzehl', 'Sebastian Moeller']&lt;/li&gt;&lt;li&gt;Tags: ['spoofing detection', 'speaker verification', 'anti-spoofing defenses', 'self-supervised audio embeddings', 'model ensembling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02286</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RACA: Representation-Aware Coverage Criteria for LLM Safety Testing</title><link>https://arxiv.org/abs/2602.02280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RACA, a representation-aware coverage framework tailored for LLM safety testing that identifies safety-critical internal representations to measure test suite quality.&lt;/li&gt;&lt;li&gt;Uses a small expert-curated calibration set of jailbreak prompts to extract safety-relevant representations, computes conceptual activation scores, and defines six sub-coverage criteria (individual and compositional).&lt;/li&gt;&lt;li&gt;Evaluates RACA showing it outperforms neuron-level coverage criteria in identifying high-quality jailbreak prompts and demonstrates applications like test prioritization and attack prompt sampling, with claimed robustness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Zhixin Zhang', 'Chengcan Wu', 'Yihao Zhang', 'Xiaokun Luan', 'Meng Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaks', 'safety testing', 'coverage criteria', 'representation engineering', 'adversarial prompts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02280</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron</title><link>https://arxiv.org/abs/2602.02027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight safety-aware decoding method that uses a single neuron as a gating mechanism combined with low-cost training of an expert model to steer LLM outputs.&lt;/li&gt;&lt;li&gt;Balances the model's intrinsic capabilities with external guidance to preserve utility while reducing unsafe outputs.&lt;/li&gt;&lt;li&gt;Claims advantages in training overhead and generalization across model scales, offering a practical, low-cost alignment/guardrail approach for LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Shen', 'Mingyang Lv', 'Han Shen', 'Jialin Wu', 'Binghao Wang', 'Zhou Yang', 'Guobin Shen', 'Dongcheng Zhao', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'decoding-based defense', 'guardrails', 'lightweight alignment', 'model self-reflection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02027</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse</title><link>https://arxiv.org/abs/2602.01795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RedVisor: a reasoning-aware defense that uses a lightweight removable adapter on a frozen LLM backbone to generate explainable analyses that detect and localize prompt injections and then condition the model to reject malicious commands.&lt;/li&gt;&lt;li&gt;Adapter is active only during a reasoning/analysis phase and muted during response generation, aiming to preserve the backbone's original utility on benign inputs (avoids alignment tax).&lt;/li&gt;&lt;li&gt;Introduces KV Cache Reuse to eliminate redundant prefill computation in decoupled pipelines and integrates the approach into the vLLM serving engine with custom kernels to improve throughput.&lt;/li&gt;&lt;li&gt;Evaluations show improved detection accuracy and throughput over state-of-the-art defenses while incurring negligible utility loss on benign inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingrui Liu', 'Sixiao Zhang', 'Cheng Long', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'LLM serving', 'KV cache reuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01795</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings</title><link>https://arxiv.org/abs/2602.01757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zero2Text, a training-free, recursive online alignment framework that combines LLM priors with dynamic ridge regression to perform embedding inversion without in-domain training data.&lt;/li&gt;&lt;li&gt;Targets strict black-box, cross-domain settings for retrieval-augmented systems and vector databases, enabling sentence/ text recovery from embeddings without leaked pairs.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains over prior methods (e.g., higher ROUGE-L and BLEU-2 on MS MARCO against an OpenAI victim model) and shows standard defenses like differential privacy are insufficient against this adaptive attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doohyun Kim', 'Donghwa Kang', 'Kyungjae Lee', 'Hyeongboo Baek', 'Brent Byunghoon Kang']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy leakage', 'black-box attack', 'adaptive attack', 'defense-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01757</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title><link>https://arxiv.org/abs/2602.01750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Reward Auditing (ARA): a two-stage adversarial framework where a Hacker policy finds reward-model exploits and an Auditor learns to detect exploitation from latent representations.&lt;/li&gt;&lt;li&gt;Introduces Auditor-Guided RLHF (AG-RLHF) that gates/penalizes reward signals when Auditor detects hacking, converting reward hacking into a measurable control signal.&lt;/li&gt;&lt;li&gt;Empirical results across multiple scenarios (sycophancy, verbosity, code gaming) show improved alignment-utility tradeoffs and cross-domain generalization of both attacks and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Beigi', 'Ming Jin', 'Junshan Zhang', 'Qifan Wang', 'Lifu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'adversarial training', 'RLHF', 'attack detection', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01750</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title><link>https://arxiv.org/abs/2602.01725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafePred, a predictive guardrail framework for computer-using agents (CUAs) that uses world-model-based future risk prediction to align predicted risks with current decisions.&lt;/li&gt;&lt;li&gt;Provides two main capabilities: (1) short- and long-term risk prediction using safety policies and world-model semantic representations to identify/prune high-risk actions, and (2) decision optimization via step-level interventions and task-level re-planning.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing substantial reductions in high-risk behaviors (≈97.6% safety performance) and improvements in task utility (up to 21.4%) compared to reactive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yurun Chen', 'Zeyi Liao', 'Ping Yin', 'Taotao Xie', 'Keting Yin', 'Shengyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'predictive guardrails', 'world models', 'long-term risk mitigation', 'decision-time interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01725</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs</title><link>https://arxiv.org/abs/2602.01600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Expected Harm, a safety-evaluation metric that weights jailbreak severity by execution likelihood (modeled as a function of execution cost).&lt;/li&gt;&lt;li&gt;Empirical evaluation shows 'Inverse Risk Calibration': models more readily refuse high-cost/low-likelihood threats while remaining vulnerable to low-cost/high-likelihood queries.&lt;/li&gt;&lt;li&gt;Demonstrates exploitability by leveraging this miscalibration to double attack success rates for existing jailbreaks in some cases.&lt;/li&gt;&lt;li&gt;Uses linear probing to show models encode severity but lack an internal representation of execution cost, identifying a root cause for the vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Zhi Rui Tam', 'Cheng-Kuang Wu', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety-evaluation', 'vulnerability-analysis', 'red-teaming', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01600</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations</title><link>https://arxiv.org/abs/2602.01582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates robustness of AI-based channel decoders (e.g., ECCT, CrossMPT) to input-dependent adversarial perturbations (FGM, PGD) and to universal adversarial perturbations applied to channel outputs.&lt;/li&gt;&lt;li&gt;Finds significant degradation of AI decoders under small norm-bounded shifts, stronger transferability of adversarial perturbations between AI decoders than to BP decoders, and that universal perturbations are far more damaging than random perturbations of equal norm.&lt;/li&gt;&lt;li&gt;Concludes that AI decoding gains under i.i.d. AWGN may come with a robustness cost and higher sensitivity to distributional shifts at the channel output.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Lei', 'Mohammad Jalali', 'Chin Wa Lau', 'Farzan Farnia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'universal-adversarial-perturbations', 'robustness', 'model-transferability', 'communication-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01582</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title><link>https://arxiv.org/abs/2602.01539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAGIC, a multi-turn multi-agent reinforcement learning framework that frames LLM safety alignment as an adversarial attacker-defender game.&lt;/li&gt;&lt;li&gt;Attacker agent iteratively rewrites queries into deceptive prompts while the defender agent learns policies to detect and refuse malicious inputs, producing co-evolution that finds long-tail vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of game equilibrium and safety guarantees, and shows empirically that the co-evolving training yields novel attack strategies and improved defense success without harming helpfulness.&lt;/li&gt;&lt;li&gt;Code released (link) and extensive experiments validate effectiveness against evolving adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wen', 'Zhida He', 'Han Qi', 'Ziyu Wan', 'Zhongtian Ma', 'Ying Wen', 'Tianhang Zheng', 'Xingcheng Xu', 'Chaochao Lu', 'Qiaosheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'defense', 'red-teaming', 'robustness', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01539</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.01528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Epistemic Independence Training (EIT), an RL-based method that makes prompt-level bias cues non-predictive of reward by balancing when bias signals support correct vs incorrect answers and penalizing bias-following.&lt;/li&gt;&lt;li&gt;Evaluates on Qwen3-4B, showing improved accuracy and robustness under adversarial bias prompts (e.g., bandwagon, authority, distraction) while preserving performance when bias aligns with truth.&lt;/li&gt;&lt;li&gt;Finds transfer: models trained against bandwagon bias generalize to unseen bias types, suggesting EIT induces a general epistemic-independence robustness rather than bias-specific heuristics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Wang', 'Xuandong Zhao', 'Zirui Zhang', 'Zhanzhi Lou', 'Nuo Chen', 'Dawn Song', 'Bingsheng He']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-injection defense', 'adversarial prompting', 'bias mitigation', 'reinforcement learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01528</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots</title><link>https://arxiv.org/abs/2602.01515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAPT, a lightweight, self-supervised runtime monitor that learns a probabilistic spatio-temporal manifold of nominal robot execution to detect out-of-distribution (OOD) states at 50Hz for humanoid control.&lt;/li&gt;&lt;li&gt;Provides calibrated, per-dimension predictive-deviation signals that yield low false-positive OOD detection and a continuous measure of Sim-to-Real drift.&lt;/li&gt;&lt;li&gt;Includes an automated post-hoc root-cause analysis pipeline combining gradient-based temporal saliency from RAPT with LLM-based reasoning over saliency and joint kinematics to generate semantic failure diagnoses in zero-shot.&lt;/li&gt;&lt;li&gt;Evaluated on simulation and real Unitree G1 humanoid tasks, showing substantial TPR improvements at strict false-positive constraints and 75% root-cause classification accuracy on real failures using proprioceptive data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humphrey Munn', 'Brendan Tidd', 'Peter Bohm', 'Marcus Gallagher', 'David Howard']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'runtime monitoring', 'sim-to-real robustness', 'robot safety', 'failure diagnosis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01515</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Better Deception Probes Using Targeted Instruction Pairs</title><link>https://arxiv.org/abs/2602.01425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that linear probes for detecting model deception are highly sensitive to the choice of training instruction pairs and that targeted instruction pairs improve probe performance.&lt;/li&gt;&lt;li&gt;Introduces a human-interpretable taxonomy of deception and demonstrates that probes trained on targeted instruction pairs capture deceptive intent rather than content-specific patterns.&lt;/li&gt;&lt;li&gt;Finds prompt choice explains a large portion of probe performance variance (≈70.6%) and recommends designing specialized probes tailored to specific threat models instead of a universal detector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vikram Natarajan', 'Devina Jain', 'Shivam Arora', 'Satvik Golechha', 'Joseph Bloom']&lt;/li&gt;&lt;li&gt;Tags: ['deception_detection', 'linear_probes', 'model_monitoring', 'safety/guardrails', 'prompt_engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01425</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems</title><link>https://arxiv.org/abs/2602.01185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedBGS, a fully decentralized, blockchain-based framework for federated learning that uses segmented gossip learning and federated analytics.&lt;/li&gt;&lt;li&gt;Aims to remove the central server single-point-of-failure, optimize blockchain usage, and provide privacy-preserving aggregation of model updates.&lt;/li&gt;&lt;li&gt;Claims comprehensive protection against attacks and supports handling of non-IID data in federated environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabio Turazza', 'Marcello Pietri', 'Marco Picone', 'Marco Mamei']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'blockchain', 'privacy-preserving', 'defense', 'decentralized-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01185</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback</title><link>https://arxiv.org/abs/2602.00886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Unified MDP formulation integrating diffusion denoising with environment dynamics to enable direct preference optimization for diffusion policies.&lt;/li&gt;&lt;li&gt;Proposes RoDiF, a robust fine-tuning method that handles corrupted human preference labels via a geometric hypothesis-cutting view and a conservative cutting strategy without assuming a noise model.&lt;/li&gt;&lt;li&gt;Demonstrates that RoDiF reliably steers pretrained diffusion policies toward human-preferred behaviors on long-horizon manipulation tasks, maintaining strong performance even with up to 30% corrupted preference labels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amitesh Vatsa', 'Zhixian Xie', 'Wanxin Jin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'human preference learning', 'label corruption', 'diffusion policies', 'defensive methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00886</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs via Calibration</title><link>https://arxiv.org/abs/2602.00619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models safety alignment as a systematic distortion of the pre-alignment next-token distribution and frames weak-to-strong jailbreaking as a forecast aggregation problem.&lt;/li&gt;&lt;li&gt;Derives an optimal aggregation strategy (characterized by a Gradient Shift in the loss-induced dual space) and shows logit-arithmetic methods are a special case under cross-entropy loss.&lt;/li&gt;&lt;li&gt;Introduces a broader family of aggregation rules for other proper losses and proposes a new hybrid aggregation rule.&lt;/li&gt;&lt;li&gt;Empirically evaluates on red-teaming benchmarks and utility tasks, reporting higher attack success rates and lower "jailbreak tax," especially on a safety-hardened gpt-oss-120b.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Lu', 'Yongkang Guo', 'Yuqing Kong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'red teaming', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00619</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits</title><link>https://arxiv.org/abs/2602.00417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy, addressing challenges from lack of closed-form estimators and evolving design matrices.&lt;/li&gt;&lt;li&gt;Provides regret guarantees: shuffle-DP algorithm for stochastic contexts with Õ(d^{3/2} sqrt(T) / sqrt(ε)) regret, and joint-DP algorithm for adversarial contexts with Õ(d sqrt(T) / sqrt(ε)) regret (matching non-private up to 1/√ε), while removing dependence on an instance-specific κ parameter.&lt;/li&gt;&lt;li&gt;Techniques involve private convex optimization, explicit incorporation of optimization error into regret analysis, and minimal distributional assumptions (only ℓ2 boundedness, no spectral assumptions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahasrajit Sarmasarkar']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'contextual-bandits', 'generalized-linear-models', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00417</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents</title><link>https://arxiv.org/abs/2602.00415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PolarMem, a training-free polarized latent graph memory that converts fuzzy perceptual likelihoods into discrete logical constraints for multimodal agents.&lt;/li&gt;&lt;li&gt;Stores explicit negation via a polarized graph topology with inhibitory connections and enforces a logic-dominant retrieval to suppress hallucinatory outputs that violate negative constraints.&lt;/li&gt;&lt;li&gt;Designed to work at inference time with frozen vision–language models; evaluated across eight VLMs and six benchmarks demonstrating improved verifiability and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhisheng Chen', 'Tingyu Wu', 'Zijie Zhou', 'Zhengwei Xie', 'Ziyan Weng', 'Yingwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'hallucination-mitigation', 'memory-architecture', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00415</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Content in Academic Peer Reviews</title><link>https://arxiv.org/abs/2602.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains and applies a detection model to identify AI-generated content in academic peer reviews at ICLR and Nature Communications.&lt;/li&gt;&lt;li&gt;Finds minimal AI-generated content before 2022, with a rapid increase through 2025 (≈20% of ICLR and ≈12% of NC reviews classified as AI-generated in 2025).&lt;/li&gt;&lt;li&gt;Highlights temporal dynamics with a sharp rise in NC between Q3 and Q4 2024 and calls for further study of implications for scholarly evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Shen', 'Kai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-content-detection', 'integrity', 'academic-peer-review', 'measurement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00319</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics-Preserving Evasion of LLM Vulnerability Detectors</title><link>https://arxiv.org/abs/2602.00305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates semantics-preserving evasion of LLM-based vulnerability detectors using behavior-preserving code transformations on a unified C/C++ benchmark (N=5000).&lt;/li&gt;&lt;li&gt;Shows that state-of-the-art detectors often flip predictions under behavior-equivalent edits; universal adversarial strings transfer to black-box APIs and gradient access increases evasion success.&lt;/li&gt;&lt;li&gt;Introduces a carrier-based joint robustness metric to diagnose detector vulnerabilities across attack methods/carriers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luze Sun', 'Alina Oprea', 'Eric Wong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'evasion attacks', 'code security', 'model robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00305</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance</title><link>https://arxiv.org/abs/2602.00183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how dataset class imbalance increases susceptibility to backdoor attacks and degrades existing defenses.&lt;/li&gt;&lt;li&gt;Proposes Randomized Probability Perturbation (RPP), a black-box certified poisoned-sample detection framework using only model output probabilities.&lt;/li&gt;&lt;li&gt;Provides provable within-domain detectability guarantees and a probabilistic upper bound on false positive rate.&lt;/li&gt;&lt;li&gt;Evaluates RPP on five image benchmarks across 10 backdoor attacks and 12 baselines, showing substantially better detection, especially under imbalance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miao Lin', 'Feng Yu', 'Rui Ning', 'Lusi Li', 'Jiawei Chen', 'Qian Lou', 'Mengxin Zheng', 'Chunsheng Xin', 'Hongyi Wu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'poisoned-sample detection', 'certified defense', 'dataset imbalance', 'black-box']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00183</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Comparison of Multiple Classifiers for Android Malware Detection with Emphasis on Feature Insights Using CICMalDroid 2020 Dataset</title><link>https://arxiv.org/abs/2602.00058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates seven supervised classifiers (XGBoost, HistGradientBoosting, CatBoost, Random Forest, KNN, SVM, etc.) on the CICMalDroid2020 dataset using a 564-dim hybrid vector of static and dynamic features for Android malware detection.&lt;/li&gt;&lt;li&gt;Compares performance across original features, PCA, and LDA; finds gradient boosting (XGBoost) on original features achieves the best results (~0.975 accuracy), PCA reduces performance, and LDA preserves separability while aiding visualization.&lt;/li&gt;&lt;li&gt;Provides interpretability via a depth-two surrogate tree highlighting package name, main activity, and target SDK as key decision drivers, presenting high-fidelity supervised baselines for deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Min-Ha-Zul Abedin', 'Tazqia Mehrub']&lt;/li&gt;&lt;li&gt;Tags: ['android-malware-detection', 'machine-learning-security', 'explainable-ml', 'CICMalDroid2020']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00058</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.02395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'Tag-Along Attacks': a threat model where a tool-less adversary induces a safety-aligned Operator to perform prohibited tool use via conversation alone.&lt;/li&gt;&lt;li&gt;Introduces Slingshot, a cold-start reinforcement learning framework that autonomously discovers effective agent-to-agent jailbreak strategies, finding attacks often reduce to short instruction-like patterns.&lt;/li&gt;&lt;li&gt;Empirical results show high success rates and strong zero-shot transfer (e.g., 67.0% vs Qwen2.5-32B-Instruct-AWQ, 56.0% vs Gemini 2.5 Flash, 39.2% vs Meta-SecAlign-8B), and large reductions in attempts to first success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nellessen', 'Tal Kachman']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'agent-to-agent attacks', 'reinforcement learning', 'tool-augmented agents', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02395</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Decoupling Generalizability and Membership Privacy Risks in Neural Networks</title><link>https://arxiv.org/abs/2602.02296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that generalization and membership-privacy risks localize to different regions/components in deep neural network architectures.&lt;/li&gt;&lt;li&gt;Proposes a Privacy-Preserving Training Principle (PPTP) to protect privacy-vulnerable components while preserving model generalizability.&lt;/li&gt;&lt;li&gt;Presents extensive evaluations showing improved privacy preservation with reduced utility/generalization loss compared to prior defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingli Fang', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-preserving-training', 'defense', 'model-generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02296</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Alignment-Aware Model Adaptation via Feedback-Guided Optimization</title><link>https://arxiv.org/abs/2602.02258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an alignment-aware fine-tuning framework that incorporates an external alignment signal via policy-gradient-based regularization to steer model updates.&lt;/li&gt;&lt;li&gt;Introduces an adaptive per-sample gating mechanism to balance supervised and alignment-driven gradients and learns abstention behavior for fully misaligned inputs.&lt;/li&gt;&lt;li&gt;Demonstrates reductions in harmful and hallucinated outputs and robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations while preserving task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gaurav Bhatt', 'Aditya Chinchure', 'Jiawei Zhou', 'Leonid Sigal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment-preserving fine-tuning', 'defense/robustness', 'adversarial/fine-tuning attacks', 'abstention/safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02258</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents</title><link>https://arxiv.org/abs/2602.02164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Co-RedTeam, a security-aware multi-agent LLM framework that mirrors real-world red-teaming by coordinating discovery and exploitation workflows.&lt;/li&gt;&lt;li&gt;Combines code-aware analysis, execution-grounded iterative reasoning, and long-term memory to plan, execute, validate, and refine vulnerability exploits.&lt;/li&gt;&lt;li&gt;Evaluations show substantial gains (over 60% exploitation success and &gt;10% absolute improvement in vulnerability detection) and ablations highlight the importance of execution feedback, structured interactions, and memory.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengfei He', 'Ash Fox', 'Lesly Miculicich', 'Stefan Friedli', 'Daniel Fabian', 'Burak Gokturk', 'Jiliang Tang', 'Chen-Yu Lee', 'Tomas Pfister', 'Long T. Le']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'vulnerability discovery', 'exploit generation', 'LLM agents', 'security automation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02164</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EvoMU: Evolutionary Machine Unlearning</title><link>https://arxiv.org/abs/2602.02139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EvoMU, an evolutionary search method that automatically discovers dataset- and task-specific unlearning loss functions for machine unlearning.&lt;/li&gt;&lt;li&gt;Demonstrates that synthesized losses can match or outperform prior loss-based unlearning methods on benchmarks (TOFU-5%, TOFU-10%, MUSE, WMDP).&lt;/li&gt;&lt;li&gt;Shows state-of-the-art results using a relatively small 4B parameter model, highlighting cost-efficient automated discovery of unlearning strategies; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pawel Batorski', 'Paul Swoboda']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy-defense', 'automated-loss-design', 'evolutionary-search', 'model-deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02139</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</title><link>https://arxiv.org/abs/2602.02079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AICD Bench, a large-scale benchmark (≈2M examples) for detecting AI-generated source code across 77 models, 11 families, and 9 programming languages.&lt;/li&gt;&lt;li&gt;Defines three detection tasks: robust binary classification under distribution shift, model-family attribution, and fine-grained human/machine/hybrid/adversarial classification.&lt;/li&gt;&lt;li&gt;Evaluates classical and neural detectors and finds detection performance substantially below practical usability—especially under distribution shift and for hybrid/adversarial code—and releases the dataset/code as a unified evaluation suite.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniil Orel', 'Dilshod Azizov', 'Indraneil Paul', 'Yuxia Wang', 'Iryna Gurevych', 'Preslav Nakov']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated code detection', 'benchmarking', 'model attribution', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02079</guid><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Internal Flow Signatures for Self-Checking and Refinement in LLMs</title><link>https://arxiv.org/abs/2602.01897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "internal flow signatures": compact summaries of depthwise token-wise dynamics at an inter-block monitoring boundary built from readout-aligned subspaces of the top token and close competitors.&lt;/li&gt;&lt;li&gt;Aligns neighboring depth-window frames by orthogonal transport to produce invariant features (transported step lengths, turning angles, subspace drift) and stabilizes signals via bias-centered monitoring.&lt;/li&gt;&lt;li&gt;Trains a lightweight GRU validator on these signatures to detect unfaithful outputs, localize a culprit depth event, and enable targeted refinement by rolling back to the culprit token and clamping an abnormal transported step while preserving orthogonal residual.&lt;/li&gt;&lt;li&gt;Provides a low-overhead, in-model auditing/self-checking and refinement pipeline that does not require modifying the base model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sungheon Jeong', 'Sanggeon Yun', 'Ryozo Masukawa', 'Wenjun Haung', 'Hanning Chen', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['Defense', 'Model auditing', 'Hallucination detection', 'Runtime self-checking', 'Internal monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01897</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization</title><link>https://arxiv.org/abs/2602.01852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FUPareto, a federated unlearning framework that balances forgetting target clients' data influence with preserving utility for remaining clients via Pareto-augmented optimization.&lt;/li&gt;&lt;li&gt;Introduces the Minimum Boundary Shift (MBS) Loss to enforce forgetting by suppressing target-class logits relative to non-target logits, aiming to improve unlearning efficiency and reduce membership inference risk.&lt;/li&gt;&lt;li&gt;Uses Pareto improvement steps to preserve utility and Pareto expansion with a Null-Space Projected MGDA to decouple gradient conflicts and enable fair, concurrent multi-client unlearning.&lt;/li&gt;&lt;li&gt;Reports empirical gains over prior federated unlearning methods in both unlearning efficacy and retained utility, and claims reduced vulnerability to membership inference attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyan Wang', 'Zhengmao Liu', 'Yongxin Cai', 'Chi Li', 'Xiaoying Tang', 'Jingchao Chen', 'Zibin Pan', 'Jing Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['federated unlearning', 'privacy defense', 'membership inference mitigation', 'multi-client unlearning', 'robust optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01852</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality</title><link>https://arxiv.org/abs/2602.01703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AGT^AO, a unified machine unlearning framework for LLMs combining Adaptive Orthogonality (AO) and Adversarial Gating Training (AGT) to balance robust data erasure with utility preservation.&lt;/li&gt;&lt;li&gt;Adaptive Orthogonality dynamically mitigates geometric gradient conflicts between forgetting and retention objectives to reduce unintended knowledge degradation.&lt;/li&gt;&lt;li&gt;Adversarial Gating Training frames unlearning as a latent-space min–max game and uses a curriculum-based gating mechanism to simulate and defend against internal/adversarial recovery attempts.&lt;/li&gt;&lt;li&gt;Empirical results claim strong unlearning efficacy (KUR ≈ 0.01) while maintaining downstream utility (MMLU 58.30).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengyu Li', 'Lingling Zhang', 'Zhitao Gao', 'Yanrui Wu', 'Yuxuan Dong', 'Huan Liu', 'Bifan Wei', 'Jun Liu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'defense', 'adversarial training', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01703</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets</title><link>https://arxiv.org/abs/2602.01682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies online inverse linear optimization (contextual recommendation) and proves a finite regret bound O(d log d) when feasible sets are M-convex (includes matroids).&lt;/li&gt;&lt;li&gt;Extends results to the setting with up to C rounds of adversarially corrupted feedback, achieving O((C+1) d log d) regret without prior knowledge of C by adaptively detecting corruptions via directed-graph monitoring.&lt;/li&gt;&lt;li&gt;Techniques combine structural characterization of optimal solutions on M-convex sets, a geometric volume argument, and an adaptive corruption-detection mechanism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taihei Oki', 'Shinsaku Sakaue']&lt;/li&gt;&lt;li&gt;Tags: ['online inverse optimization', 'adversarial corruption robustness', 'regret bounds', 'M-convex / matroid action sets', 'corruption detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01682</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient Adversarial Attacks on High-dimensional Offline Bandits</title><link>https://arxiv.org/abs/2602.01658</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a threat model where an attacker perturbs the reward model's weights prior to offline bandit training, exploiting logged data to hijack bandit behavior.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing a high-dimensional effect: required perturbation norm for successful attacks decreases as input dimensionality grows, with analyses for linear rewards and extensions to ReLU networks.&lt;/li&gt;&lt;li&gt;Empirical evaluation against two Hugging Face image evaluators (aesthetic quality and compositional alignment) demonstrates that small, targeted weight perturbations can drastically alter bandit outcomes, while random perturbations fail.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyed Mohammad Hadi Hosseini', 'Amir Najafi', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'model-manipulation', 'offline-bandits', 'robustness', 'high-dimensionality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01658</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Chance-Constrained Inference for Hallucination Risk Control in Large Language Models</title><link>https://arxiv.org/abs/2602.01637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates inference as a deployment-time chance-constrained risk control problem that directly bounds the probability of hallucinations among accepted generations.&lt;/li&gt;&lt;li&gt;Models hallucinations as stochastic constraint violations and shows confidence-based selective prediction does not guarantee probabilistic risk bounds.&lt;/li&gt;&lt;li&gt;Proposes a sequential, anytime-valid inference procedure that adaptively certifies feasibility/infeasibility with finite samples, enabling reliable risk control and early detection of intrinsically infeasible queries; validated on QA tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sreenivasan Mohandas']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'safe inference', 'selective prediction', 'risk control', 'certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01637</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?</title><link>https://arxiv.org/abs/2602.01611</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how trajectory-supervised fine-tuning (trajectory-SFT) leads agents to rely on interface-specific interaction patterns (interface shortcutting) rather than environment-invariant semantic tool-use.&lt;/li&gt;&lt;li&gt;Proposes PIPE, a protocol-level evaluation augmentation that minimally rewrites environment interfaces while preserving task semantics to diagnose interface reliance.&lt;/li&gt;&lt;li&gt;Evaluates 16 environments and multiple open-source and API-based agents, showing trajectory-SFT amplifies interface shortcutting and causes sharp performance drops under minimal interface rewrites.&lt;/li&gt;&lt;li&gt;Introduces Interface Reliance (IR), an alias-based metric to quantify training-time interface preference and reveals environment-dependent, non-monotonic training dynamics invisible under standard evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weizheng Gu', 'Chengze Li', 'Zhuohao Yu', 'Mengyuan Sun', 'Zhibang Yang', 'Wei Wang', 'Hongrui Jia', 'Shikun Zhang', 'Wei Ye']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vulnerability analysis', 'evaluation/benchmarking', 'agent behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01611</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models</title><link>https://arxiv.org/abs/2602.01428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the trade-off between watermark strength for LLM output provenance and speculative sampling inference efficiency.&lt;/li&gt;&lt;li&gt;Introduces a quantitative measure of watermark strength, characterizes the trade-off as a constrained optimization, and derives Pareto curves for two watermarking schemes.&lt;/li&gt;&lt;li&gt;Proposes a mechanism that injects pseudorandomness into draft-token acceptance to preserve maximal watermark detectability while maintaining speculative sampling acceptance rates.&lt;/li&gt;&lt;li&gt;Provides experiments showing improved detectability without sacrificing speculative sampling efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiqing He', 'Xiang Li', 'Li Shen', 'Weijie Su', 'Qi Long']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model provenance', 'defense', 'inference efficiency', 'speculative sampling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01428</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing</title><link>https://arxiv.org/abs/2602.01150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that conventional MIA-based unlearning auditing (treated as a binary classification) is inherently prone to unobservable statistical errors, leading to optimistic/unreliable forgetting assessments and high compute cost from shadow-model training.&lt;/li&gt;&lt;li&gt;Proposes SMIA, a training-free auditing framework that uses statistical tests to directly compare member vs non-member distributions and outputs a forgetting rate with a confidence interval.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and extensive experiments showing SMIA yields more reliable auditing with much lower computational overhead than learned MIA attacks, positioning SMIA as a new paradigm for reliable machine unlearning auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Sun', 'Zeming Wei', 'Jiaxuan Zou', 'Jiacheng Gong', 'Guanheng Wang', 'Chengyang Dong', 'Jialong Li', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'machine-unlearning', 'privacy-auditing', 'statistical-tests', 'attack-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01150</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization</title><link>https://arxiv.org/abs/2602.01139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes new graph representation learning techniques based on Graph Shift Operators (GSOs) to improve performance across tasks.&lt;/li&gt;&lt;li&gt;Introduces graph data augmentation methods aimed at enhancing generalization for GNNs.&lt;/li&gt;&lt;li&gt;Develops robustness-focused approaches for GNNs, including orthonormalization techniques and noise-based defenses against adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yassine Abbahaddou']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'adversarial robustness', 'defenses', 'representation learning', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01139</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems</title><link>https://arxiv.org/abs/2602.01113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Single-Edge Graph Injection Attack (SEGIA): each injected node connects to the operational graph via a single edge to evade topology- and homophily-based sanitization.&lt;/li&gt;&lt;li&gt;Method combines a pruned SGC surrogate, multi-hop neighborhood sampling, reverse graph convolution-based feature synthesis, and a similarity-regularized objective to craft stealthy, locally homophilous injected nodes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and extensive empirical evaluation showing at least 25% higher attack success than representative baselines under much smaller edge budgets across datasets and defenses.&lt;/li&gt;&lt;li&gt;Highlights system-level risks for industrial GNN deployments (IIoT, power grids, manufacturing networks) and motivates lightweight admission validation and neighborhood-consistency monitoring as mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Liang', 'Ranhui Yan', 'Jia Cai', 'You-Gan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['GNN attacks', 'node injection', 'adversarial attacks', 'industrial control systems', 'graph security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01113</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2602.01025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UltraBreak, a framework to craft universal, transferable image-based jailbreaks against vision-language models by constraining adversarial patterns with vision-space transformations and regularization.&lt;/li&gt;&lt;li&gt;Uses semantically guided objectives defined in the target LLM's textual embedding space to relax textual targets and smooth the loss landscape, improving generalization across models and attack goals.&lt;/li&gt;&lt;li&gt;Demonstrates that UltraBreak outperforms prior gradient-based jailbreaks in transferability to black-box VLMs and analyzes why earlier methods overfit to white-box surrogates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyuan Cui', 'Yige Li', 'Yutao Wu', 'Xingjun Ma', 'Sarah Erfani', 'Christopher Leckie', 'Hanxun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'transferable attacks', 'vision-language models', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01025</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity</title><link>https://arxiv.org/abs/2602.00723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces prompt multiplicity, a framework to quantify consistency of LLM outputs across varied prompts, complementing correctness-focused hallucination evaluation.&lt;/li&gt;&lt;li&gt;Finds high inconsistency in existing benchmarks (e.g., &gt;50% in Med-HALT) and shows current detection methods largely measure consistency rather than absolute correctness.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies (e.g., RAG) and demonstrates they can reduce some hallucinations but may introduce new inconsistencies.&lt;/li&gt;&lt;li&gt;Argues for integrating prompt multiplicity into evaluation pipelines to better characterize harms and the limits of current detection/mitigation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prakhar Ganesh', 'Reza Shokri', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'evaluation', 'consistency', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00723</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provably Protecting Fine-Tuned LLMs from Training Data Extraction</title><link>https://arxiv.org/abs/2602.00688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses defenses against training data extraction (TDE) attacks on fine-tuned LLMs.&lt;/li&gt;&lt;li&gt;Proposes SCP-Δ_r, a Near Access Freeness (NAF)-based algorithm that smooths low-impact token-level probability shifts using a base model and operates on relative probabilities.&lt;/li&gt;&lt;li&gt;Claims orders-of-magnitude improved theoretical privacy bounds compared to existing NAF methods and demonstrates strong empirical protection with minimal utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tom Segal', 'Asaf Shabtai', 'Yuval Elovici']&lt;/li&gt;&lt;li&gt;Tags: ['training data extraction', 'privacy defense', 'fine-tuning', 'provable guarantees', 'near-access-freeness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00688</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sparsity-Aware Unlearning for Large Language Models</title><link>https://arxiv.org/abs/2602.00577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: LLMs memorize sensitive training data and machine unlearning is needed to remove such information, but existing unlearning methods are designed for dense models and fail on sparse (pruned) models.&lt;/li&gt;&lt;li&gt;Finding: Unlearning effectiveness degrades on sparse models because sparsification prunes many weights to zero, limiting the model's capacity to forget when methods attempt to update parameters uniformly.&lt;/li&gt;&lt;li&gt;Method: Proposes Sparsity-Aware Unlearning (SAU) that decouples unlearning from sparsification via gradient masking to redirect updates to surviving weights and importance-aware redistribution to compensate for pruned parameters.&lt;/li&gt;&lt;li&gt;Results: Empirical evaluation shows SAU significantly improves forgetting on sparse LLMs while better preserving model utility compared to existing unlearning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuze Wang', 'Yujia Tong', 'Ke Xu', 'Jingling Yuan', 'Jiawei Jiang', 'Chuang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'sparsity', 'defense', 'large-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00577</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Contrastive Learning for Privacy Enhancements in Industrial Internet of Things</title><link>https://arxiv.org/abs/2602.00515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of contrastive learning approaches applied to privacy-preserving analytics in Industrial Internet of Things (IIoT) settings.&lt;/li&gt;&lt;li&gt;Emphasizes unique characteristics of industrial data, system architectures, and IIoT application scenarios that affect privacy solutions.&lt;/li&gt;&lt;li&gt;Reviews contrastive learning-based privacy techniques (reducing raw data sharing and label dependence), discusses existing solutions, open challenges, and future research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lin Liu', 'Rita Machacy', 'Simi Kuniyilh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'contrastive-learning', 'IIoT', 'defense', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00515</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks</title><link>https://arxiv.org/abs/2602.00407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fed-Listing, a gradient-based privacy attack that infers client label distributions in federated Graph Neural Networks using only final-layer gradients.&lt;/li&gt;&lt;li&gt;Trains an attack model using an auxiliary shadow dataset with diverse label partitions to simulate client heterogeneity and predict class proportions.&lt;/li&gt;&lt;li&gt;Evaluates on multiple GNN architectures and datasets, showing Fed-Listing outperforms baselines (random, Decaf) under non-i.i.d. scenarios.&lt;/li&gt;&lt;li&gt;Finds common defenses barely reduce attack efficacy unless they substantially degrade model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suprim Nakarmi', 'Junggab Son', 'Yue Zhao', 'Zuobin Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'graph neural networks', 'label distribution inference', 'privacy attack', 'gradient leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00407</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode</title><link>https://arxiv.org/abs/2602.00388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'safety blessing' in diffusion-based LLMs (D-LLMs): diffusion-style generation induces a stepwise reduction that suppresses unsafe outputs compared to autoregressive LLMs.&lt;/li&gt;&lt;li&gt;Analyzes the underlying mechanism of this robustness but discovers a practical failure mode called 'context nesting' where harmful requests are embedded within structured benign contexts to bypass the reduction effect.&lt;/li&gt;&lt;li&gt;Provides empirical red-teaming showing context nesting achieves high attack success across models and benchmarks, including the first reported jailbreak of a commercial model (Gemini Diffusion).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyuan He', 'Yupeng Chen', 'Lang Lin', 'Yihan Wang', 'Shenxu Chang', 'Eric Sommerlade', 'Philip Torr', 'Junchi Yu', 'Adel Bibi', 'Jialin Yu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'diffusion-LLMs', 'red-teaming', 'adversarial-attack', 'safety-vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00388</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection</title><link>https://arxiv.org/abs/2602.00318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BOCLOAK, an optimal-transport-guided method to generate realistic, constraint-aware adversarial attacks (edge edits and node injections) against GNN-based social bot detectors.&lt;/li&gt;&lt;li&gt;Constructs a probability measure over spatio-temporal neighbor features and decodes optimal transport plans into sparse, plausible graph manipulations that respect real-world constraints.&lt;/li&gt;&lt;li&gt;Evaluates across three social bot datasets, five state-of-the-art detectors, three defenses, and multiple baselines—reporting large gains in attack success and substantial GPU memory efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunal Mukherjee', 'Zulfikar Alom', 'Tran Gia Bao Ngo', 'Cuneyt Gurcan Akcora', 'Murat Kantarcioglu']&lt;/li&gt;&lt;li&gt;Tags: ['graph-adversarial-attacks', 'adversarial-attack', 'bot-detection', 'optimal-transport', 'robustness-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00318</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization</title><link>https://arxiv.org/abs/2602.00175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that unlearning-based defenses for NSFW concepts in diffusion models often leave dormant memories: the mapping from text to unsafe concepts is disrupted but not removed.&lt;/li&gt;&lt;li&gt;Introduces IVO (Initial Latent Variable Optimization), which reactivates these dormant memories by optimizing initial latent variables to realign the denoising/noise distribution; components include Image Inversion, Adversarial Optimization, and Reused Attack.&lt;/li&gt;&lt;li&gt;Evaluates IVO across 8 common unlearning techniques and reports high attack success rates and strong semantic consistency, arguing current unlearning defenses are fundamentally flawed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manyi Li', 'Yufan Liu', 'Lai Jiang', 'Bing Li', 'Yuming Li', 'Weiming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning attack', 'diffusion models', 'safety bypass / jailbreak', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00175</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Robust Reasoning through Guided Adversarial Self-Play</title><link>https://arxiv.org/abs/2602.00173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GASP (Guided Adversarial Self-Play), a method that trains a single model via adversarial self-play: a polluter generates locally-coherent corruptions to conditioning context, while an agent learns to detect and repair them using only outcome verification.&lt;/li&gt;&lt;li&gt;Uses an in-distribution repair guidance (imitation term on self-generated repairs) to overcome sparse successful recoveries early in training, improving recovery probability without human labels or external teachers.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple open-weight language models (1.5B–8B) that GASP yields models robust to misleading chain-of-thought and mild input perturbations, often improving clean accuracy and inducing an effective curriculum for recovery learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Vaishnav Tadiparthi', 'Kwonjoon Lee', 'Nakul Agarwal', 'Hossein Nourkhiz Mahjoub', 'Ehsan Moradi Pari', 'Lizhang Chen', 'Amy Zhang', 'Liu Leqi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-training', 'red-teaming', 'self-play', 'reasoning-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00173</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2602.00085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CARE-RFT, a reinforcement finetuning method that replaces standard reverse KL regularization with a skew reverse KL divergence to provide a confidence-sensitive penalty.&lt;/li&gt;&lt;li&gt;The skew reverse KL is bounded for confident, consistently rewarded explorations (enabling reasoning) and unbounded elsewhere (preserving calibration), aiming to reduce hallucinations while allowing capability gains.&lt;/li&gt;&lt;li&gt;Extensive experiments across model scales and RFT algorithms show CARE-RFT matches unconstrained RFT's reasoning performance while recovering base-model trustworthiness and calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Jincheng Cao', 'Bodun Hu', 'Aryan Mokhtari', 'Leqi Liu', 'Amy Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'calibration', 'hallucination-mitigation', 'reinforcement-finetuning', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00085</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning</title><link>https://arxiv.org/abs/2602.00084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical framework explaining why Low-Rank Adaptation (LoRA) is inherently resistant to label noise, proving rank-r LoRA cannot memorize arbitrary labels beyond sample size O(r(d+k-r)).&lt;/li&gt;&lt;li&gt;Derives an optimal rank that trades off approximation bias and noise-induced variance, and shows this optimal rank decreases as noise rate increases.&lt;/li&gt;&lt;li&gt;Establishes temporal separation where clean patterns are learned early and noise is memorized later, and proposes RACT (Rank-Aware Curriculum Training) that uses rank discrepancy for noise detection.&lt;/li&gt;&lt;li&gt;Empirically validates the theory; RACT achieves 91.1% F1 for noise detection on AG News while maintaining competitive accuracy (91.46%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brady Steele']&lt;/li&gt;&lt;li&gt;Tags: ['label noise', 'data poisoning', 'robustness', 'parameter-efficient fine-tuning', 'noise detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00084</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation</title><link>https://arxiv.org/abs/2602.00064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPGCL, a graph contrastive learning framework that couples stochastic edge removal with an SVD-guided refinement to recover informative edges and introduce meaningful links while avoiding graph densification.&lt;/li&gt;&lt;li&gt;Balances edge removal and recovery to control structural discrepancy between views and adds a contrastive fusion module with a global similarity regularizer to align views.&lt;/li&gt;&lt;li&gt;Empirically shows improved robustness and accuracy of base GNNs on ten benchmark datasets, outperforming state-of-the-art graph contrastive learning and structure learning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Deng', 'Yingping Li', 'Shuiping Gou', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'adversarial robustness', 'contrastive learning', 'structural perturbation', 'graph defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00064</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</title><link>https://arxiv.org/abs/2601.21900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TraceRouter, a path-level defense that traces and severs causal propagation circuits of malicious semantics in large foundation models rather than targeting local neurons.&lt;/li&gt;&lt;li&gt;Method operates in three stages: detect a sensitive onset layer via attention divergence, disentangle malicious features using sparse autoencoders and differential activation analysis, and map features to downstream causal paths using feature influence scores from zero-out interventions.&lt;/li&gt;&lt;li&gt;Selective suppression of identified causal chains aims to block harmful information flow while preserving orthogonal computations; experiments claim improved trade-off between adversarial robustness and utility over state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuancheng Shi', 'Shangze Li', 'Wenjun Lu', 'Wenhua Wu', 'Cong Wang', 'Zifeng Cheng', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'adversarial-robustness', 'causal-tracing', 'model-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21900</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens rather than whole documents to remove undesired capabilities (demonstrated on removing medical capabilities).&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and less costly to benign capabilities than document-level filtering, and effectiveness increases with model scale.&lt;/li&gt;&lt;li&gt;Introduces methodology for labeling tokens using sparse autoencoders and distilling efficient classifiers; shows robustness to noisy labels with sufficient pretraining compute.&lt;/li&gt;&lt;li&gt;Demonstrates that models trained with token filtering can still be aligned on the filtered domain and quantifies the compute slowdown for the forget domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['data-filtering', 'capability-control', 'model-safety', 'pretraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</title><link>https://arxiv.org/abs/2601.18739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SeNeDiF-OOD (Semantic Nested Dichotomy Fusion), a hierarchical binary-fusion framework for out-of-distribution detection that aligns decision boundaries with semantic abstraction levels.&lt;/li&gt;&lt;li&gt;Validates the method on MonuMAI (monument style recognition) against diverse OOD conditions, including non-monument images, unknown styles, and adversarial attacks, reporting improved filtering of OOD data while preserving in-distribution performance.&lt;/li&gt;&lt;li&gt;Targets open-world robustness and defensive detection mechanisms rather than offensive attacks; presents a methodological defense to improve model safety in deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ignacio Antequera-S\\'anchez", "Juan Luis Su\\'arez-D\\'iaz", 'Rosana Montes', 'Francisco Herrera']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'Robustness/Defense', 'Adversarial detection', 'Hierarchical fusion', 'Open-world classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18739</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that progressive training on AI-generated medical data causes collapse in pathological diversity and diagnostic reliability across clinical text, vision-language reports, and synthesized medical images.&lt;/li&gt;&lt;li&gt;Shows rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain high but misplaced diagnostic confidence (false reassurance rates up to ~40%).&lt;/li&gt;&lt;li&gt;Finds degradation becomes clinically significant after only a few generations (confirmed by blinded physician evaluation) and evaluates mitigation approaches.&lt;/li&gt;&lt;li&gt;Reports that simple upscaling of synthetic data does not prevent collapse, whereas mixing real data with quality-aware filtering helps preserve diversity and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Yun Liu', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight single-pass detector for hallucinations in visual question answering that leverages internal signals from vision-language models.&lt;/li&gt;&lt;li&gt;Fuses token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features via branch-wise evidence encoding and uncertainty-aware attention.&lt;/li&gt;&lt;li&gt;Introduces a low-cost, model-dependent automatic supervision strategy (extending LLM-as-a-Judge) to avoid expensive human labels and enable supervised training.&lt;/li&gt;&lt;li&gt;Shows improved detection effectiveness and efficiency on multiple VQA benchmarks and analyzes how hallucinations stem from internal state variations across perception, reasoning, and decoding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'VQA', 'multimodal-safety', 'model-internal-signals', 'automated-supervision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust MLLM Unlearning via Visual Knowledge Distillation</title><link>https://arxiv.org/abs/2512.11325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Knowledge Distillation (VKD) to disentangle and selectively erase visual knowledge in multimodal large language models (MLLMs) while preserving textual knowledge by supervising intermediate visual representations.&lt;/li&gt;&lt;li&gt;Fine-tunes only the visual components of MLLMs for efficient unlearning, claiming improved effectiveness and utility compared to output-level unlearning methods.&lt;/li&gt;&lt;li&gt;Evaluates robustness of the unlearning procedure against relearning attacks and reports superior performance and efficiency over prior unlearning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Zhenxing Niu', 'Haoxuan Ji', 'Guangyu He', 'Haichang Gao', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'model privacy', 'relearning attacks', 'multimodal security', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11325</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems</title><link>https://arxiv.org/abs/2512.01661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnsolvableQA, a multi-domain dataset created by injecting logical contradictions into valid reasoning chains to produce solvable and objectively unsolvable questions.&lt;/li&gt;&lt;li&gt;Proposes UnsolvableRL, a reinforcement learning alignment method that trains LLMs to detect objective unsolvability while calibrating confidence for capability-limited tasks, reducing hallucinations.&lt;/li&gt;&lt;li&gt;Reports empirical gains: &gt;85% unsolvability detection and substantial improvement in solvable reasoning accuracy (e.g., 43.4% → 69.4% on Qwen3-4B-Instruct).&lt;/li&gt;&lt;li&gt;Analyzes training dynamics: strict alignment constraints can cause Capability Collapse without unsolvable data but act as a regularizer when such data are included, improving robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengyun Peng', 'Qiguang Chen', 'Bofei Liu', 'Jiannan Guan', 'Libo Qin', 'Zheng Yan', 'Jinhao Liu', 'Jianshu Zhang', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'hallucination detection', 'alignment', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01661</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAS-Shield: A Defense Framework for Secure and Efficient LLM MAS</title><link>https://arxiv.org/abs/2511.22924</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAS-Shield, a hierarchical defense framework for LLM-based multi-agent systems to prevent cascading linguistic attacks.&lt;/li&gt;&lt;li&gt;Uses a three-stage coarse-to-fine pipeline: Critical Agent Selection, Light Auditing with sentry models, and Global Consensus Auditing via a heavyweight committee.&lt;/li&gt;&lt;li&gt;Aims to optimize security-efficiency trade-offs by only escalating suspicious interactions to expensive committee checks.&lt;/li&gt;&lt;li&gt;Reports experimental results showing ~92.5% recovery rate against adversarial scenarios and &gt;70% reduction in defense latency versus prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Wang', 'Zhaojiacheng Zhou', 'Bunyod Suvonov', 'Jiong Lou', 'Jie LI']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'multi-agent systems (MAS)', 'LLM security', 'adversarial robustness', 'auditing/ensemble methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22924</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Geometric-disentangelment Unlearning</title><link>https://arxiv.org/abs/2511.17100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Geometric-disentanglement Unlearning (GU), a projection-based method to reduce collateral retention degradation when removing a forget set from LLMs.&lt;/li&gt;&lt;li&gt;Provides a theoretical characterization: retain loss is locally invariant iff the update direction is orthogonal to the subspace spanned by retain gradients (optimizer-induced geometry).&lt;/li&gt;&lt;li&gt;GU is a lightweight, plug-and-play projection applied to gradient-based unlearning methods and is empirically shown to improve forgetting strength while reducing retain drift across multiple benchmarks.&lt;/li&gt;&lt;li&gt;Open-sourced implementation and evaluation showing substantial gains (e.g., up to 62% improved forgetting Extraction Strength and 31% higher retain ES when combined with SimNPO).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Yuji Zhang', 'Tianxin Wei', 'Ruizhong Qiu', 'Ke Yang', 'Xiao Lin', 'Cheng Qian', 'Jingrui He', 'Hanghang Tong', 'Chengxiang Zhai', 'Heng Ji', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'privacy-preserving ML', 'model editing', 'defenses', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17100</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.17098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Malicious Token Injection (MTI), a modular attack framework that perturbs transformer KV cache entries at selected layers/timesteps using methods like additive Gaussian noise, zeroing, and orthogonal rotations.&lt;/li&gt;&lt;li&gt;Provides a theoretical analysis connecting attention propagation to logit deviations, bounding effects in terms of the Frobenius norm of the corruption and softmax Lipschitz properties.&lt;/li&gt;&lt;li&gt;Empirically demonstrates substantial changes to next-token distributions and degraded downstream task performance on GPT-2 and LLaMA-2/7B, and shows destabilization of retrieval-augmented and agentic reasoning pipelines.&lt;/li&gt;&lt;li&gt;Positions KV cache integrity as an underexplored inference-time attack surface and proposes a reproducible, theoretically grounded threat model for LLM robustness/security work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Hossain', 'Swayamjit Saha', 'Somshubhra Roy', 'Ravi Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['cache-side attack', 'inference-time attack', 'LLM security', 'model corruption', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17098</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Search Goes Wrong: Red-Teaming Web-Augmented Large Language Models</title><link>https://arxiv.org/abs/2510.09689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CREST-Search, a red-teaming framework targeting LLMs augmented with web search to find vulnerabilities in retrieval and citation workflows.&lt;/li&gt;&lt;li&gt;Introduces three novel attack strategies that craft benign-appearing queries which cause models to cite harmful or low-credibility web content, plus an iterative in-context refinement for black-box effectiveness.&lt;/li&gt;&lt;li&gt;Builds WebSearch-Harm, a search-specific harmful dataset to fine-tune a red-team model, and demonstrates CREST-Search can bypass safety filters and systematically expose web-search-related vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Ou', 'Kangjie Chen', 'Xingshuo Han', 'Gelei Deng', 'Jie Zhang', 'Han Qiu', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'web-augmented-LLMs', 'adversarial-search-attacks', 'safety-evaluation', 'attack-dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09689</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT, an adaptive detector that learns a witness function from training data to augment logits-based LLM-generated text detection.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on true/false positive and negative rates for the proposed method.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to ~37%) over state-of-the-art logits-based detectors across multiple datasets and source LLMs.&lt;/li&gt;&lt;li&gt;Offers a Python implementation for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated text detection', 'defense', 'logits-based detection', 'statistical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC (Sequential Tool Attack Chaining): an automated framework that constructs multi-step tool call chains where each step appears benign but together enable harmful actions.&lt;/li&gt;&lt;li&gt;Empirically evaluates 483 STAC cases (1,352 interaction sets) across domains and agent types, showing very high attack success rates (ASR &gt;90% in most cases) against state-of-the-art agents including GPT-4.1.&lt;/li&gt;&lt;li&gt;Provides an automated closed-loop pipeline that synthesizes, validates via in-environment execution, and reverse-engineers stealthy multi-turn prompts to reliably induce agents to run malicious sequences.&lt;/li&gt;&lt;li&gt;Performs defense analysis showing prompt-based defenses are limited and proposes a reasoning-driven defense prompt that reduces ASR by up to 28.8%, highlighting need to reason over full action sequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'agent tool abuse', 'automated red teaming', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GHOST, an automated attack that optimizes in image embedding space to generate natural-looking images that induce object hallucinations in multimodal LLMs.&lt;/li&gt;&lt;li&gt;Produces images via conditioning a diffusion model on optimized embeddings to remain visually natural and object-free while triggering model misperception.&lt;/li&gt;&lt;li&gt;Demonstrates high hallucination success rates across models (e.g., &gt;28% vs ~1% for prior methods) and cross-model transferability (images optimized for one model trigger hallucinations in others).&lt;/li&gt;&lt;li&gt;Uses the generated dataset to fine-tune models and reduce hallucination, framing GHOST as both a diagnostic red-teaming tool and a source for defense/mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'model-hallucination', 'red-teaming', 'multimodal-attack', 'defense-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Information Security Based on LLM Approaches: A Review</title><link>https://arxiv.org/abs/2507.18215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of LLM-based approaches applied to information security tasks including malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization.&lt;/li&gt;&lt;li&gt;Reviews technical basis (neural networks, Transformer architectures) and argues LLMs can improve detection accuracy and reduce false alarms in security systems.&lt;/li&gt;&lt;li&gt;Identifies challenges such as model transparency, interpretability, generalization and scene adaptability, and calls for further work on model structure optimization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chang Gong', 'Zhongwen Li', 'Xiaoqi Li']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'malware_detection', 'vulnerability_detection', 'network_threat_analysis', 'cryptography']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18215</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title><link>https://arxiv.org/abs/2507.04531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-Fusion, a differential privacy-based inference mechanism that bounds the influence of sensitive context tokens on LLM outputs by blending baseline (no-sensitive-token) and sensitive-token-conditioned distributions.&lt;/li&gt;&lt;li&gt;Targets document privatization: paraphrasing documents containing sensitive tokens so attackers cannot reliably infer them while preserving high text quality; trade-off controlled by ε (privacy budget).&lt;/li&gt;&lt;li&gt;Provides per-token provable privacy guarantees at inference time and empirical results showing substantially improved privacy/utility (e.g., ~6× lower perplexity than related DPI methods).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushil Thareja', 'Preslav Nakov', 'Praneeth Vepakomma', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'inference-time privacy', 'LLM security', 'document privatization', 'prompt injection mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04531</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://arxiv.org/abs/2506.12706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NAP-Tuning (Neural Augmentor for Multi-modal Adversarial Prompt Tuning), extending adversarial prompt tuning from text-only to both text and image modalities with multi-layer prompts.&lt;/li&gt;&lt;li&gt;Introduces a Neural Augmentor architecture that performs feature purification via token refiners using residual connections to reconstruct purified features and correct adversarial distortions.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements in adversarial robustness across datasets and attack types (notably AutoAttack), with large gains over strong baselines while retaining competitive clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Xin Wang', 'Xingjun Ma', 'Lingyu Qiu', 'Yu-Gang Jiang', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'prompt tuning', 'vision-language models', 'defense', 'feature purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12706</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Shielding for Safe Reinforcement Learning under Hidden-Parameter Dynamics Shifts</title><link>https://arxiv.org/abs/2506.11033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive Shielding: a framework for safe RL in constrained hidden-parameter MDPs that infers a low-dimensional representation of underlying dynamics online to adapt safety checks.&lt;/li&gt;&lt;li&gt;Uses a two-layer safety strategy: (1) safety-regularized policy optimization to avoid high-cost regions proactively, and (2) reactive adaptive shielding that forecasts safety risks from inferred dynamics and filters unsafe actions using uncertainty-aware conformal prediction bounds.&lt;/li&gt;&lt;li&gt;Provides theoretical connections between shielding prediction errors and bounds on the average cost rate, and demonstrates empirically improved return-safety trade-offs on Safe-Gym benchmarks with varying hidden parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minjae Kwon', 'Tyler Ingebrand', 'Ufuk Topcu', 'Lu Feng']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'runtime shielding / guardrails', 'robustness to dynamics shift', 'uncertainty quantification (conformal prediction)', 'hidden-parameter MDPs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11033</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection</title><link>https://arxiv.org/abs/2505.16530</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DuFFin, a dual-level fingerprinting framework for black-box ownership verification of LLMs that combines trigger-pattern extraction and knowledge-level fingerprints.&lt;/li&gt;&lt;li&gt;Designed to detect model theft or unauthorized deployment across model variants (fine-tuning, quantization, safety alignment) without affecting generation.&lt;/li&gt;&lt;li&gt;Evaluated on multiple open-source base models and their variants, achieving high identification performance (IP-ROC &gt; 0.95).&lt;/li&gt;&lt;li&gt;Code released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuliang Yan', 'Haochun Tang', 'Shuo Yan', 'Enyan Dai']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'watermarking/IP protection', 'model theft defense', 'black-box verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16530</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Code-Mixed Phonetic Perturbations for Red-Teaming LLMs</title><link>https://arxiv.org/abs/2505.14226</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CMP-RT, a red-teaming attack that combines code-mixing and phonetic perturbations to alter safety-critical tokens while preserving phonetics and human interpretability.&lt;/li&gt;&lt;li&gt;Identifies a tokenizer-level vulnerability in transformer models that allows harmful prompts to bypass safety alignment and defenses, demonstrating effectiveness on SOTA models (e.g., Gemini-3-Pro).&lt;/li&gt;&lt;li&gt;Shows the attack is robust, scalable, generalizes across modalities, and highlights tokenization as an under-examined attack surface in current safety pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', 'Siddharth D Jaiswal']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'tokenizer-vulnerabilities', 'adversarial-perturbations', 'prompt-injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14226</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title><link>https://arxiv.org/abs/2410.22307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model-substitution integrity risk when users outsource LLM inference to decentralized/remote providers.&lt;/li&gt;&lt;li&gt;Proposes SVIP, a secret-based verifiable inference protocol that requires providers to return generated text plus processed hidden representations and uses a proxy task to produce a unique model identifier.&lt;/li&gt;&lt;li&gt;Analyzes robustness under strong/adaptive adversaries and integrates a secret mechanism to strengthen security.&lt;/li&gt;&lt;li&gt;Empirical results show low false negative (&lt;5%) and false positive (&lt;3%) rates and very low verification latency (&lt;0.01s per prompt).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Sun', 'Yuhang Li', 'Yue Zhang', 'Yuchen Jin', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable inference', 'model integrity', 'remote inference security', 'attack detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22307</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</title><link>https://arxiv.org/abs/2601.21083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenSec, a dual-control RL environment to evaluate defensive incident-response agents under adversarial prompt-injection scenarios.&lt;/li&gt;&lt;li&gt;Defines execution-based metrics (time-to-first-containment, blast radius/false positives per episode, injection violation rates) to measure calibration and real-world consequences of containment actions.&lt;/li&gt;&lt;li&gt;Benchmarks four frontier LLMs on 40 episodes and finds widespread over-triggering/poor calibration (very high false-positive rates), showing that aggregate success metrics hide this failure mode. Code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jarrod Barnes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-injection', 'adversarial-evaluation', 'incident-response', 'benchmarking', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21083</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment</title><link>https://arxiv.org/abs/2601.10520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a neuro-symbolic containment architecture that separates normative reasoning (Moral Module) from instrumental decision-making (Decision-Making Module) and enforces compliance via a Guard.&lt;/li&gt;&lt;li&gt;Moral Module uses a reason-based deontic logic formalism to provide interpretable, contestable, and justifiable normative constraints that can be communicated to and used by the DMM.&lt;/li&gt;&lt;li&gt;Claims support for formal verification and statistical guarantees of alignment, and demonstrates the approach on an LLM therapy assistant to show stakeholder contestability and refinement of behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felix Jahn', 'Yannic Muskalla', 'Lisa Dargasz', 'Patrick Schramowski', 'Kevin Baum']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'guardrails', 'neuro-symbolic', 'deontic-logic', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10520</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LoRA is All You Need for Safety Alignment of Reasoning LLMs</title><link>https://arxiv.org/abs/2507.17075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using LoRA (low-rank adapters) during supervised fine-tuning on refusal/safety datasets to perform safety alignment while largely avoiding degradation of reasoning capabilities (the "Safety Tax").&lt;/li&gt;&lt;li&gt;Empirically shows safety comparable to full-model alignment and preserves reasoning across multiple model sizes, architectures, two safety benchmarks, and four reasoning benchmarks (math, science, code).&lt;/li&gt;&lt;li&gt;Ablations find rank-1 updates often suffice, applying LoRA to MLP up-projection layers can outperform full MLP updates, and middle layers are most effective to update.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis explaining when LoRA preserves base capabilities: overshooting rank induces base-task degradation inversely proportional to the intrinsic dimensionality of the base task, favoring low-rank finetuning tasks on high-rank base models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihao Xue', 'Baharan Mirzasoleiman']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'LoRA', 'fine-tuning', 'defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17075</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title><link>https://arxiv.org/abs/2506.00641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentAuditor, a training-free, memory-augmented reasoning framework that enables LLM evaluators to emulate human expert evaluators by extracting structured semantic features and chain-of-thought traces from past interactions.&lt;/li&gt;&lt;li&gt;Uses a multi-stage, context-aware retrieval-augmented generation process to retrieve relevant experiences and guide assessment of new agent behaviors, improving detection of subtle and compound safety/security issues.&lt;/li&gt;&lt;li&gt;Presents ASSEBench, a 2,293-record benchmark covering 15 risk types across 29 scenarios with both Strict and Lenient judgment standards, and demonstrates that AgentAuditor achieves human-level evaluation accuracy on agent safety and security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanjun Luo', 'Shenyu Dai', 'Chiming Ni', 'Xinfeng Li', 'Guibin Zhang', 'Kun Wang', 'Tongliang Liu', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'red teaming / auditing', 'retrieval-augmented evaluation', 'security benchmark', 'chain-of-thought / memory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00641</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.02395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'Tag-Along Attacks': a threat model where a tool-less adversary induces a safety-aligned Operator to perform prohibited tool use via conversation alone.&lt;/li&gt;&lt;li&gt;Introduces Slingshot, a cold-start reinforcement learning framework that autonomously discovers effective agent-to-agent jailbreak strategies, finding attacks often reduce to short instruction-like patterns.&lt;/li&gt;&lt;li&gt;Empirical results show high success rates and strong zero-shot transfer (e.g., 67.0% vs Qwen2.5-32B-Instruct-AWQ, 56.0% vs Gemini 2.5 Flash, 39.2% vs Meta-SecAlign-8B), and large reductions in attempts to first success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nellessen', 'Tal Kachman']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'agent-to-agent attacks', 'reinforcement learning', 'tool-augmented agents', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02395</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Decoupling Generalizability and Membership Privacy Risks in Neural Networks</title><link>https://arxiv.org/abs/2602.02296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that generalization and membership-privacy risks localize to different regions/components in deep neural network architectures.&lt;/li&gt;&lt;li&gt;Proposes a Privacy-Preserving Training Principle (PPTP) to protect privacy-vulnerable components while preserving model generalizability.&lt;/li&gt;&lt;li&gt;Presents extensive evaluations showing improved privacy preservation with reduced utility/generalization loss compared to prior defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingli Fang', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-preserving-training', 'defense', 'model-generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02296</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild</title><link>https://arxiv.org/abs/2602.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a robust SASV (Spoofing Aware Speaker Verification) system combining a spoofing detector and speaker verification network operating in tandem.&lt;/li&gt;&lt;li&gt;Spoofing detector uses self-supervised speech embeddings with a graph neural network backend and a top-3 layer mixture-of-experts to fuse multi-level features for anti-spoofing.&lt;/li&gt;&lt;li&gt;Speaker verification employs a low-complexity convolutional network fusing 2D/1D multi-scale features, trained with SphereFace loss and contrastive circle loss for better discriminability.&lt;/li&gt;&lt;li&gt;Additional defenses include AS-Norm score normalization, fixed imposter cohorts, and model ensembling to improve robustness against spoofed utterances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnab Das', 'Yassine El Kheir', 'Enes Erdem Erdogan', 'Feidi Kallel', 'Tim Polzehl', 'Sebastian Moeller']&lt;/li&gt;&lt;li&gt;Tags: ['spoofing detection', 'speaker verification', 'anti-spoofing defenses', 'self-supervised audio embeddings', 'model ensembling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02286</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RACA: Representation-Aware Coverage Criteria for LLM Safety Testing</title><link>https://arxiv.org/abs/2602.02280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RACA, a representation-aware coverage framework tailored for LLM safety testing that identifies safety-critical internal representations to measure test suite quality.&lt;/li&gt;&lt;li&gt;Uses a small expert-curated calibration set of jailbreak prompts to extract safety-relevant representations, computes conceptual activation scores, and defines six sub-coverage criteria (individual and compositional).&lt;/li&gt;&lt;li&gt;Evaluates RACA showing it outperforms neuron-level coverage criteria in identifying high-quality jailbreak prompts and demonstrates applications like test prioritization and attack prompt sampling, with claimed robustness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Zhixin Zhang', 'Chengcan Wu', 'Yihao Zhang', 'Xiaokun Luan', 'Meng Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaks', 'safety testing', 'coverage criteria', 'representation engineering', 'adversarial prompts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02280</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Malware Detection Through Memory Analysis</title><link>https://arxiv.org/abs/2602.02184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a machine-learning based malware detection system using the MalMemAnalysis-2022 memory analysis dataset, performing both binary (benign vs malicious) and multi-class (benign, ransomware, spyware, Trojan) classification.&lt;/li&gt;&lt;li&gt;Selects XGBoost for a trade-off of high detection performance and fast inference; reports binary test accuracy/F1 ~99.98% and multi-class accuracy 87.54% (avg malware F1 75.03%).&lt;/li&gt;&lt;li&gt;Emphasizes efficiency for near real-time deployment (37.3 ms to classify 50 samples binary; 43.2 ms multi-class) aiming to improve online safety via memory-based malware detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Nassar']&lt;/li&gt;&lt;li&gt;Tags: ['malware-detection', 'memory-forensics', 'machine-learning', 'cybersecurity', 'XGBoost']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02184</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EvoMU: Evolutionary Machine Unlearning</title><link>https://arxiv.org/abs/2602.02139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EvoMU, an evolutionary search procedure that automatically discovers dataset- and task-specific unlearning loss functions to remove specified training data while preserving utility.&lt;/li&gt;&lt;li&gt;Demonstrates that synthesized losses can match or outperform existing loss-based unlearning methods on benchmarks (TOFU-5%, TOFU-10%, MUSE, WMDP) using a modest 4B-parameter model.&lt;/li&gt;&lt;li&gt;Frames the approach as automated scientific discovery (AI co-scientist) for privacy-preserving ML and releases code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pawel Batorski', 'Paul Swoboda']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy-preserving ML', 'defense', 'automated loss search', 'evolutionary algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02139</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning</title><link>https://arxiv.org/abs/2602.02004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies "reasoning drift" in multimodal reasoning models where clue-gathering focuses on question-irrelevant entities, causing hallucinations.&lt;/li&gt;&lt;li&gt;Introduces ClueRecall, a metric to assess visual clue retrieval, and ClueTracer, a training-free, parameter-free plugin that traces question→outputs→visual tokens to localize task-relevant patches and suppress spurious attention.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in hallucination suppression across multiple reasoning architectures (≈1.21×) and gains when applied to non-reasoning settings (≈1.14×) without additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gongli Xi', 'Kun Wang', 'Zeming Gao', 'Huahui Yi', 'Haolang Lu', 'Ye Tian', 'Wendong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination suppression', 'multimodal robustness', 'visual grounding', 'training-free defense', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02004</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse</title><link>https://arxiv.org/abs/2602.01795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RedVisor: a reasoning-aware defense that uses a lightweight removable adapter on a frozen LLM backbone to generate explainable analyses that detect and localize prompt injections and then condition the model to reject malicious commands.&lt;/li&gt;&lt;li&gt;Adapter is active only during a reasoning/analysis phase and muted during response generation, aiming to preserve the backbone's original utility on benign inputs (avoids alignment tax).&lt;/li&gt;&lt;li&gt;Introduces KV Cache Reuse to eliminate redundant prefill computation in decoupled pipelines and integrates the approach into the vLLM serving engine with custom kernels to improve throughput.&lt;/li&gt;&lt;li&gt;Evaluations show improved detection accuracy and throughput over state-of-the-art defenses while incurring negligible utility loss on benign inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingrui Liu', 'Sixiao Zhang', 'Cheng Long', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'LLM serving', 'KV cache reuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01795</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency</title><link>https://arxiv.org/abs/2602.01765</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'temporal noise unconsistency' in diffusion models: triggered inputs disrupt adjacent-timestep noise predictions in specific temporal segments while clean inputs remain stable.&lt;/li&gt;&lt;li&gt;Proposes TNC-Defense, a gray-box framework that (1) detects and locates anomalous diffusion timesteps using adjacent-timestep noise consistency and (2) performs trigger-agnostic, timestep-aware detoxification by correcting the backdoor generation path.&lt;/li&gt;&lt;li&gt;Designed for practical auditing where model parameters are inaccessible; aims to balance detoxification effectiveness and generation quality with low overhead.&lt;/li&gt;&lt;li&gt;Evaluated on five representative backdoor attack scenarios, reporting an average detection accuracy improvement (~+11%) and invalidating 98.5% of triggered samples with only mild degradation in generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingzheng Wang', 'Xiaoyan Gu', 'Hongbo Xu', 'Hongcheng Li', 'Zimo Yu', 'Jiang Zhou', 'Weiping Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'diffusion models', 'backdoor detection', 'detoxification', 'gray-box defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01765</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title><link>https://arxiv.org/abs/2602.01725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafePred, a predictive guardrail framework for computer-using agents (CUAs) that uses world-model-based future risk prediction to align predicted risks with current decisions.&lt;/li&gt;&lt;li&gt;Provides two main capabilities: (1) short- and long-term risk prediction using safety policies and world-model semantic representations to identify/prune high-risk actions, and (2) decision optimization via step-level interventions and task-level re-planning.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing substantial reductions in high-risk behaviors (≈97.6% safety performance) and improvements in task utility (up to 21.4%) compared to reactive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yurun Chen', 'Zeyi Liao', 'Ping Yin', 'Taotao Xie', 'Keting Yin', 'Shengyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'predictive guardrails', 'world models', 'long-term risk mitigation', 'decision-time interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01725</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Efficient Adversarial Attacks on High-dimensional Offline Bandits</title><link>https://arxiv.org/abs/2602.01658</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a threat model where an attacker perturbs the reward model's weights prior to offline bandit training, exploiting logged data to hijack bandit behavior.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing a high-dimensional effect: required perturbation norm for successful attacks decreases as input dimensionality grows, with analyses for linear rewards and extensions to ReLU networks.&lt;/li&gt;&lt;li&gt;Empirical evaluation against two Hugging Face image evaluators (aesthetic quality and compositional alignment) demonstrates that small, targeted weight perturbations can drastically alter bandit outcomes, while random perturbations fail.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyed Mohammad Hadi Hosseini', 'Amir Najafi', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'model-manipulation', 'offline-bandits', 'robustness', 'high-dimensionality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01658</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment</title><link>https://arxiv.org/abs/2602.01587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation to obtain provable l0-norm-style certificates against jailbreak/adversarial payloads by partitioning inputs into immutable structural prompts and mutable payloads.&lt;/li&gt;&lt;li&gt;Proposes Noise-Augmented Alignment Tuning (NAAT) to make the base LLM a semantic denoiser, addressing utility degradation on sparse contexts caused by randomized defenses.&lt;/li&gt;&lt;li&gt;Reports strong empirical results on Llama-3: reduces attack success rate for gradient-based jailbreaks from 84.2% to 1.2% while maintaining 94.1% benign utility, outperforming character-level baselines.&lt;/li&gt;&lt;li&gt;Provides a deterministic certificate guaranteeing robustness within a provable adversarial radius, shifting safety guarantees from single-pass inference to statistical ensemble stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehua Cheng', 'Jianwei Yang', 'Wei Dai', 'Jiahao Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'certified robustness', 'randomized smoothing', 'alignment tuning', 'adversarial defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01587</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations</title><link>https://arxiv.org/abs/2602.01582</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates robustness of AI-based channel decoders (e.g., ECCT, CrossMPT) to input-dependent adversarial perturbations (FGM, PGD) and to universal adversarial perturbations applied to channel outputs.&lt;/li&gt;&lt;li&gt;Finds significant degradation of AI decoders under small norm-bounded shifts, stronger transferability of adversarial perturbations between AI decoders than to BP decoders, and that universal perturbations are far more damaging than random perturbations of equal norm.&lt;/li&gt;&lt;li&gt;Concludes that AI decoding gains under i.i.d. AWGN may come with a robustness cost and higher sensitivity to distributional shifts at the channel output.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Lei', 'Mohammad Jalali', 'Chin Wa Lau', 'Farzan Farnia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'universal-adversarial-perturbations', 'robustness', 'model-transferability', 'communication-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01582</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses</title><link>https://arxiv.org/abs/2602.01438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIPHER, a benchmark to measure cryptographic vulnerabilities in LLM-generated Python code using controlled prompt variants (insecure/neutral/secure).&lt;/li&gt;&lt;li&gt;Defines a cryptography-specific vulnerability taxonomy and an automated, line-level scoring pipeline for attribution of insecure code patterns.&lt;/li&gt;&lt;li&gt;Evaluates multiple widely used LLMs and finds that explicit secure prompting reduces some targeted issues but does not reliably eliminate cryptographic flaws.&lt;/li&gt;&lt;li&gt;Plans to publicly release the benchmark and reproducible scoring pipeline upon publication to facilitate further security evaluation and mitigation research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Max Manolov', 'Tony Gao', 'Siddharth Shukla', 'Cheng-Ting Chou', 'Ryan Lagasse']&lt;/li&gt;&lt;li&gt;Tags: ['cryptographic vulnerabilities', 'LLM security', 'benchmarking', 'code generation', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01438</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems</title><link>https://arxiv.org/abs/2602.01185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedBGS, a fully decentralized, blockchain-based framework for federated learning that uses segmented gossip learning and federated analytics.&lt;/li&gt;&lt;li&gt;Aims to remove the central server single-point-of-failure, optimize blockchain usage, and provide privacy-preserving aggregation of model updates.&lt;/li&gt;&lt;li&gt;Claims comprehensive protection against attacks and supports handling of non-IID data in federated environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabio Turazza', 'Marcello Pietri', 'Marco Picone', 'Marco Mamei']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'blockchain', 'privacy-preserving', 'defense', 'decentralized-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01185</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing</title><link>https://arxiv.org/abs/2602.01150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that conventional MIA-based unlearning auditing (treated as a binary classification) is inherently prone to unobservable statistical errors, leading to optimistic/unreliable forgetting assessments and high compute cost from shadow-model training.&lt;/li&gt;&lt;li&gt;Proposes SMIA, a training-free auditing framework that uses statistical tests to directly compare member vs non-member distributions and outputs a forgetting rate with a confidence interval.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and extensive experiments showing SMIA yields more reliable auditing with much lower computational overhead than learned MIA attacks, positioning SMIA as a new paradigm for reliable machine unlearning auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Sun', 'Zeming Wei', 'Jiaxuan Zou', 'Jiacheng Gong', 'Guanheng Wang', 'Chengyang Dong', 'Jialong Li', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'machine-unlearning', 'privacy-auditing', 'statistical-tests', 'attack-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01150</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance</title><link>https://arxiv.org/abs/2602.01047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Decoding (ResDec), a training-free, history-aware decoding method that leverages token logits evolution and internal reasoning signals of LVLMs to correct language-prior biases.&lt;/li&gt;&lt;li&gt;Aims to mitigate hallucinations in Large Vision-Language Models by using historical information as residual guidance during decoding to improve visual grounding and reduce object hallucination.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing reduced hallucinations and improved performance on multiple LVLM benchmarks without additional model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinrong Chen', 'Xu Chu', 'Yingmin Qiu', 'Hengyuan Zhang', 'Jing Xiong', 'Shiyu Tang', 'Shuai Liu', 'Shaokang Yang', 'Cheng Yang', 'Hayden Kwok-Hay So', 'Ngai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'decoding strategies', 'robustness', 'safety', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01047</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection</title><link>https://arxiv.org/abs/2602.01032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HierCon, a hierarchical layer-attention framework that models temporal frames, neighbouring layers, and layer groups from self-supervised speech models for audio deepfake detection.&lt;/li&gt;&lt;li&gt;Combines hierarchical attention with margin-based contrastive learning to produce domain-invariant embeddings that improve generalisation across generation methods and recording conditions.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on ASVspoof 2021 DF and In-the-Wild datasets (1.93% and 6.87% EER), with substantial improvements over independent layer weighting; includes attention visualisations to interpret model behaviour.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhili Nicholas Liang', 'Soyeon Caren Han', 'Qizhou Wang', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['audio-deepfake-detection', 'defense', 'contrastive-learning', 'hierarchical-attention', 'robustness-generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01032</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2602.01025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UltraBreak, a framework to craft universal, transferable image-based jailbreaks against vision-language models by constraining adversarial patterns with vision-space transformations and regularization.&lt;/li&gt;&lt;li&gt;Uses semantically guided objectives defined in the target LLM's textual embedding space to relax textual targets and smooth the loss landscape, improving generalization across models and attack goals.&lt;/li&gt;&lt;li&gt;Demonstrates that UltraBreak outperforms prior gradient-based jailbreaks in transferability to black-box VLMs and analyzes why earlier methods overfit to white-box surrogates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyuan Cui', 'Yige Li', 'Yutao Wu', 'Xingjun Ma', 'Sarah Erfani', 'Christopher Leckie', 'Hanxun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'transferable attacks', 'vision-language models', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01025</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability</title><link>https://arxiv.org/abs/2602.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GradingAttack, an adversarial attack framework targeting LLM-based automatic short answer grading (ASAG) systems.&lt;/li&gt;&lt;li&gt;Develops token-level and prompt-level attack strategies to manipulate grading outcomes while maintaining camouflage.&lt;/li&gt;&lt;li&gt;Proposes a novel metric that balances attack success and camouflage to quantify stealthiness.&lt;/li&gt;&lt;li&gt;Evaluates attacks across multiple datasets showing prompt-level attacks achieve higher success while token-level attacks have better camouflage; stresses need for defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xueyi Li', 'Zhuoneng Zhou', 'Zitao Liu', 'Yongdong Wu', 'Weiqi Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'prompt-level-attacks', 'token-level-attacks', 'automatic-grading-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00979</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bypassing Prompt Injection Detectors through Evasive Injections</title><link>https://arxiv.org/abs/2602.00750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that linear-probe detectors using activation deltas to detect prompt-injection/task-drift can be evaded by adversarially-optimized universal suffixes that are appended to inputs.&lt;/li&gt;&lt;li&gt;Empirically generates universal suffixes that achieve very high evasion rates across multiple probes and models (Phi-3 3.8B, Llama-3 8B), including &gt;90% success under majority-vote and up to ~99.6% when all probes must be fooled.&lt;/li&gt;&lt;li&gt;Proposes a defence: train detectors using activations collected when randomly appending one of many generated suffixes (an ensemble/randomization approach) and shows this mitigates the attack effectively.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Jahedur Rahman', 'Ihsen Alouani']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-injection', 'adversarial-examples', 'evasion-attacks', 'defense', 'activation-delta-detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00750</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity</title><link>https://arxiv.org/abs/2602.00723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces prompt multiplicity, a framework to quantify consistency of LLM outputs across varied prompts, complementing correctness-focused hallucination evaluation.&lt;/li&gt;&lt;li&gt;Finds high inconsistency in existing benchmarks (e.g., &gt;50% in Med-HALT) and shows current detection methods largely measure consistency rather than absolute correctness.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies (e.g., RAG) and demonstrates they can reduce some hallucinations but may introduce new inconsistencies.&lt;/li&gt;&lt;li&gt;Argues for integrating prompt multiplicity into evaluation pipelines to better characterize harms and the limits of current detection/mitigation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prakhar Ganesh', 'Reza Shokri', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'evaluation', 'consistency', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00723</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities</title><link>https://arxiv.org/abs/2602.00711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a proactive defense approach that highlights security-critical code regions (e.g., data access, authentication, input handling) to prevent vulnerabilities during development.&lt;/li&gt;&lt;li&gt;Implements an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and leverages LLMs to generate prevention-focused explanations and guidance.&lt;/li&gt;&lt;li&gt;Initial evaluation on the Spring-PetClinic app shows the metrics capture most known security-critical methods and that the LLM produces actionable, prevention-oriented insights.&lt;/li&gt;&lt;li&gt;Notes limitations: metrics are structural rather than semantic and further work is needed to refine security-aware metrics and explanation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjith Krishnamurthy', 'Oshando Johnson', 'Goran Piskachev', 'Eric Bodden']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-prevention', 'developer-tools', 'LLM-explanations', 'code-security', 'static-analysis-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00711</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs via Calibration</title><link>https://arxiv.org/abs/2602.00619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models safety alignment as a systematic distortion of the pre-alignment next-token distribution and frames weak-to-strong jailbreaking as a forecast aggregation problem.&lt;/li&gt;&lt;li&gt;Derives an optimal aggregation strategy (characterized by a Gradient Shift in the loss-induced dual space) and shows logit-arithmetic methods are a special case under cross-entropy loss.&lt;/li&gt;&lt;li&gt;Introduces a broader family of aggregation rules for other proper losses and proposes a new hybrid aggregation rule.&lt;/li&gt;&lt;li&gt;Empirically evaluates on red-teaming benchmarks and utility tasks, reporting higher attack success rates and lower "jailbreak tax," especially on a safety-hardened gpt-oss-120b.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Lu', 'Yongkang Guo', 'Yuqing Kong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'red teaming', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00619</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.00559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OmniVCHall, a benchmark for isolated and compositional hallucinations in video multimodal LLMs, including a taxonomy and adversarial answer options to reduce shortcut reasoning.&lt;/li&gt;&lt;li&gt;Evaluates 39 VLLMs and finds substantial degradation under compositional hallucination scenarios.&lt;/li&gt;&lt;li&gt;Proposes TriCD, a triple-pathway contrastive decoding framework that generates negative video variants (via an adaptive perturbation controller) and a saliency-guided enhancement module, optimized with reinforcement learning to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports consistent improvements (≈10% average accuracy) across two backbone models; dataset and code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbin Xing', 'Quanxing Zha', 'Lizheng Zu', 'Mengran Li', 'Ming Li', 'Junchi Yan']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'adversarial robustness', 'benchmark', 'contrastive decoding', 'video multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00559</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Contrastive Learning for Privacy Enhancements in Industrial Internet of Things</title><link>https://arxiv.org/abs/2602.00515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of contrastive learning approaches applied to privacy-preserving analytics in Industrial Internet of Things (IIoT) settings.&lt;/li&gt;&lt;li&gt;Emphasizes unique characteristics of industrial data, system architectures, and IIoT application scenarios that affect privacy solutions.&lt;/li&gt;&lt;li&gt;Reviews contrastive learning-based privacy techniques (reducing raw data sharing and label dependence), discusses existing solutions, open challenges, and future research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lin Liu', 'Rita Machacy', 'Simi Kuniyilh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'contrastive-learning', 'IIoT', 'defense', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00515</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems</title><link>https://arxiv.org/abs/2602.00428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the 'Mandela effect' — collective misremembering — in LLM-based multi-agent systems, characterizing its existence and causal factors.&lt;/li&gt;&lt;li&gt;Introduces MANBENCH, a benchmark evaluating agent behaviors across four task types and five interaction protocols to quantify collective memory bias.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLM-powered agents on MANBENCH and proposes mitigation strategies (prompt-level defenses like cognitive anchoring and source scrutiny, plus model-level alignment) that reduce the effect by ~74.4%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naen Xu', 'Hengyu An', 'Shuo Shi', 'Jinghuai Zhang', 'Chunyi Zhou', 'Changjiang Li', 'Tianyu Du', 'Zhihui Fu', 'Jun Wang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'LLM safety', 'misinformation', 'benchmark', 'defenses/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00428</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Text is All You Need for Vision-Language Model Jailbreaking</title><link>https://arxiv.org/abs/2602.00420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Text-DJ, a jailbreak attack for large vision-language models that exploits OCR by converting harmful textual prompts into multiple benign-looking image-embedded sub-queries and mixing them with irrelevant distraction queries.&lt;/li&gt;&lt;li&gt;Attack pipeline: (1) decompose harmful query into semantically related benign sub-queries, (2) select maximally irrelevant distraction queries, (3) present both as a grid of images (with sub-queries placed centrally) to induce OCR-based misinterpretation and evade text filters.&lt;/li&gt;&lt;li&gt;Demonstrates successful circumvention of state-of-the-art LVLM safety mechanisms, attributing success to bypassing text-based filters via images and to distraction-induced failure to associate fragmented sub-queries.&lt;/li&gt;&lt;li&gt;Highlights a new vulnerability in LVLM OCR pipelines for fragmented/multi-image inputs and calls for defenses tailored to fragmented multimodal attack vectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Chen', 'Zhao Xu', 'Youyuan Jiang', 'Tianle Zheng', 'Cho-Jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'OCR attacks', 'multimodal adversarial attack', 'LVLM security', 'safety bypass']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00420</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Content in Academic Peer Reviews</title><link>https://arxiv.org/abs/2602.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains and applies a detection model to identify AI-generated content in academic peer reviews at ICLR and Nature Communications.&lt;/li&gt;&lt;li&gt;Finds minimal AI-generated content before 2022, with a rapid increase through 2025 (≈20% of ICLR and ≈12% of NC reviews classified as AI-generated in 2025).&lt;/li&gt;&lt;li&gt;Highlights temporal dynamics with a sharp rise in NC between Q3 and Q4 2024 and calls for further study of implications for scholarly evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Shen', 'Kai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-content-detection', 'integrity', 'academic-peer-review', 'measurement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00319</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection</title><link>https://arxiv.org/abs/2602.00318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BOCLOAK, an optimal-transport-guided method to generate realistic, constraint-aware adversarial attacks (edge edits and node injections) against GNN-based social bot detectors.&lt;/li&gt;&lt;li&gt;Constructs a probability measure over spatio-temporal neighbor features and decodes optimal transport plans into sparse, plausible graph manipulations that respect real-world constraints.&lt;/li&gt;&lt;li&gt;Evaluates across three social bot datasets, five state-of-the-art detectors, three defenses, and multiple baselines—reporting large gains in attack success and substantial GPU memory efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunal Mukherjee', 'Zulfikar Alom', 'Tran Gia Bao Ngo', 'Cuneyt Gurcan Akcora', 'Murat Kantarcioglu']&lt;/li&gt;&lt;li&gt;Tags: ['graph-adversarial-attacks', 'adversarial-attack', 'bot-detection', 'optimal-transport', 'robustness-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00318</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics-Preserving Evasion of LLM Vulnerability Detectors</title><link>https://arxiv.org/abs/2602.00305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates semantics-preserving evasion of LLM-based vulnerability detectors using behavior-preserving code transformations on a unified C/C++ benchmark (N=5000).&lt;/li&gt;&lt;li&gt;Shows that state-of-the-art detectors often flip predictions under behavior-equivalent edits; universal adversarial strings transfer to black-box APIs and gradient access increases evasion success.&lt;/li&gt;&lt;li&gt;Introduces a carrier-based joint robustness metric to diagnose detector vulnerabilities across attack methods/carriers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luze Sun', 'Alina Oprea', 'Eric Wong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'evasion attacks', 'code security', 'model robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00305</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Speaker Conversational Audio Deepfake: Taxonomy, Dataset and Pilot Study</title><link>https://arxiv.org/abs/2602.00295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a taxonomy for multi-speaker conversational audio deepfakes (partial vs. full manipulations).&lt;/li&gt;&lt;li&gt;Introduces MsCADD, a new dataset of 2,830 two-speaker conversational audio clips (real and fully synthetic TTS-based conversations).&lt;/li&gt;&lt;li&gt;Benchmarks three detection models (LFCC-LCNN, RawNet2, Wav2Vec 2.0) and shows detection gaps under varied conversational dynamics.&lt;/li&gt;&lt;li&gt;Provides dataset and baseline results to support future research on detecting conversational audio deepfakes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alabi Ahmed', 'Vandana Janeja', 'Sanjay Purushotham']&lt;/li&gt;&lt;li&gt;Tags: ['audio-deepfake', 'dataset', 'deepfake-detection', 'multi-speaker', 'TTS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00295</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification</title><link>https://arxiv.org/abs/2602.00292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents LogicGaze, a benchmark of ~40k video segments and Flickr30k images designed to test whether Vision-Language Models (VLMs) can verify sequential causal chains against visual evidence.&lt;/li&gt;&lt;li&gt;Creates linguistically plausible but visually contradictory counterfactual perturbations and a three-part evaluation (Causal Validation, Grounded Narrative Synthesis, Perturbation Rejection) to expose hallucination and grounding failures.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art VLMs (e.g., Qwen2.5-VL-72B), finding significant vulnerabilities in causal grounding; releases anonymized dataset and evaluation resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rory Driscoll', 'Alexandros Christoforos', 'Chadbourne Davis']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'benchmarking', 'adversarial/perturbation testing', 'visual grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00292</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation</title><link>https://arxiv.org/abs/2602.00219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a federated learning based intrusion detection framework that uses a Tri-LLM ensemble (GPT-4o, DeepSeek-V3, LLaMA-3-8B) to create language-derived semantic attack prototypes for open-set and zero-shot detection of unseen attacks.&lt;/li&gt;&lt;li&gt;Models inter-LLM semantic disagreement as epistemic uncertainty for zero-day risk estimation and introduces a trust-aware aggregation mechanism to downweight unreliable or compromised clients during federated optimization.&lt;/li&gt;&lt;li&gt;Reports empirical improvements: &gt;80% zero-shot detection accuracy on unseen attack patterns and ~10% better zero-day discrimination versus similarity-based baselines, with stable convergence across heterogeneous clients.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeid Jamshidi', 'Omar Abdul Wahab', 'Foutse Khomh', 'Kawser Wazed Nafi']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'intrusion-detection', 'zero-shot-detection', 'LLM-security', 'robust-aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00219</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs</title><link>https://arxiv.org/abs/2602.00204</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense method for detecting Advanced Persistent Threats (APTs) by converting unstructured system logs into semantic embeddings using a pre-trained transformer (LLM) and detecting anomalies with an Autoencoder (AE).&lt;/li&gt;&lt;li&gt;Claims semantic-aware embeddings better capture intent and non-linear patterns in low-and-slow APT behavior that provenance/statistical methods miss.&lt;/li&gt;&lt;li&gt;Evaluates the approach on the DARPA Transparent Computing (TC) dataset containing realistic red-team APT scenarios and reports superior AUC-ROC versus unsupervised baselines (Isolation Forest, OC-SVM, PCA).&lt;/li&gt;&lt;li&gt;Demonstrates a security-focused application of LLMs as feature extractors for intrusion/anomaly detection rather than studying model vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waleed Khan Mohammed', 'Zahirul Arief Irfan Bin Shahrul Anuar', 'Mousa Sufian Mousa Mitani', 'Hezerul Abdul Karim', 'Nouar AlDahoul']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'anomaly-detection', 'LLM-embeddings', 'autoencoder', 'APT-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00204</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange</title><link>https://arxiv.org/abs/2602.00192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that state-of-the-art inpainting detectors rely on global VAE-induced spectral artifacts across images rather than on locally synthesized content.&lt;/li&gt;&lt;li&gt;Introduces Inpainting Exchange (INP-X) to replace unedited regions with originals while keeping synthesized content, and releases a 90K test dataset of real, inpainted, and exchanged images.&lt;/li&gt;&lt;li&gt;Shows pretrained detectors' accuracy drops dramatically under INP-X (e.g., 91% → 55%), links failures to VAE high-frequency attenuation, and shows training on the new dataset improves content-aware detection and localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elif Nebioglu', 'Emirhan Bilgi\\c{c}', 'Adrian Popescu']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'deepfake-detection', 'detector-evasion', 'VAE-artifacts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00192</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EigenAI: Deterministic Inference, Verifiable Results</title><link>https://arxiv.org/abs/2602.00182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a verifiable inference platform combining deterministic, bit-exact LLM inference with a cryptoeconomic optimistic re-execution protocol so outputs can be publicly audited and reproduced.&lt;/li&gt;&lt;li&gt;Operators publish encrypted request/response logs to a data availability layer (EigenDA); during a challenge window any watcher can trigger deterministic re-execution inside a TEE with threshold-released keys for verification.&lt;/li&gt;&lt;li&gt;Verification reduces to byte-equality due to bit-exact inference; a single honest replica suffices to detect fraud and economic penalties can be enforced via EigenLayer staking.&lt;/li&gt;&lt;li&gt;Aims to enable secure, auditable applications (prediction markets, trading bots, scientific assistants) that inherit security from a blockchain validator set.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Ribeiro Alves', 'Vishnu Patankar', 'Matheus Pereira', 'Jamie Stephens', 'Nima Vaziri', 'Sreeram Kannan']&lt;/li&gt;&lt;li&gt;Tags: ['verifiable-ml', 'cryptoeconomic-security', 'trusted-execution-environment', 'reproducibility', 'blockchain-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00182</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization</title><link>https://arxiv.org/abs/2602.00175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that unlearning-based defenses for NSFW concepts in diffusion models often leave dormant memories: the mapping from text to unsafe concepts is disrupted but not removed.&lt;/li&gt;&lt;li&gt;Introduces IVO (Initial Latent Variable Optimization), which reactivates these dormant memories by optimizing initial latent variables to realign the denoising/noise distribution; components include Image Inversion, Adversarial Optimization, and Reused Attack.&lt;/li&gt;&lt;li&gt;Evaluates IVO across 8 common unlearning techniques and reports high attack success rates and strong semantic consistency, arguing current unlearning defenses are fundamentally flawed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manyi Li', 'Yufan Liu', 'Lai Jiang', 'Bing Li', 'Yuming Li', 'Weiming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning attack', 'diffusion models', 'safety bypass / jailbreak', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00175</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Robust Reasoning through Guided Adversarial Self-Play</title><link>https://arxiv.org/abs/2602.00173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GASP (Guided Adversarial Self-Play), a method that trains a single model via adversarial self-play: a polluter generates locally-coherent corruptions to conditioning context, while an agent learns to detect and repair them using only outcome verification.&lt;/li&gt;&lt;li&gt;Uses an in-distribution repair guidance (imitation term on self-generated repairs) to overcome sparse successful recoveries early in training, improving recovery probability without human labels or external teachers.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple open-weight language models (1.5B–8B) that GASP yields models robust to misleading chain-of-thought and mild input perturbations, often improving clean accuracy and inducing an effective curriculum for recovery learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Vaishnav Tadiparthi', 'Kwonjoon Lee', 'Nakul Agarwal', 'Hossein Nourkhiz Mahjoub', 'Ehsan Moradi Pari', 'Lizhang Chen', 'Amy Zhang', 'Liu Leqi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-training', 'red-teaming', 'self-play', 'reasoning-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00173</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title><link>https://arxiv.org/abs/2602.00154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines prompt-induced inference-time denial-of-service (PI-DoS) attacks against large reasoning models (LRMs) and formalizes inference cost and desiderata for practical attacks (amplification, stealthiness, optimizability).&lt;/li&gt;&lt;li&gt;Proposes ReasoningBomb, a reinforcement-learning-based attacker that generates short natural prompts which induce pathologically long (often non-terminating) multi-step reasoning traces in victim LRMs.&lt;/li&gt;&lt;li&gt;Evaluates across seven open-source and three commercial LRMs, showing large input-to-output amplification (≈286.7x), major increases in completion/reasoning tokens versus baselines, and high bypass rates against input/output/combined detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaogeng Liu', 'Xinyan Wang', 'Yechao Zhang', 'Sanjay Kariyappa', 'Chong Xiang', 'Muhao Chen', 'G. Edward Suh', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['denial-of-service', 'prompt-based attack', 'adversarial attack', 'red teaming', 'attack detection evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00154</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2602.00085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CARE-RFT, a reinforcement finetuning method that replaces standard reverse KL regularization with a skew reverse KL divergence to provide a confidence-sensitive penalty.&lt;/li&gt;&lt;li&gt;The skew reverse KL is bounded for confident, consistently rewarded explorations (enabling reasoning) and unbounded elsewhere (preserving calibration), aiming to reduce hallucinations while allowing capability gains.&lt;/li&gt;&lt;li&gt;Extensive experiments across model scales and RFT algorithms show CARE-RFT matches unconstrained RFT's reasoning performance while recovering base-model trustworthiness and calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuozhe Li', 'Jincheng Cao', 'Bodun Hu', 'Aryan Mokhtari', 'Leqi Liu', 'Amy Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'calibration', 'hallucination-mitigation', 'reinforcement-finetuning', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00085</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation</title><link>https://arxiv.org/abs/2602.00064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPGCL, a graph contrastive learning framework that couples stochastic edge removal with an SVD-guided refinement to recover informative edges and introduce meaningful links while avoiding graph densification.&lt;/li&gt;&lt;li&gt;Balances edge removal and recovery to control structural discrepancy between views and adds a contrastive fusion module with a global similarity regularizer to align views.&lt;/li&gt;&lt;li&gt;Empirically shows improved robustness and accuracy of base GNNs on ten benchmark datasets, outperforming state-of-the-art graph contrastive learning and structure learning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Deng', 'Yingping Li', 'Shuiping Gou', 'Bo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'adversarial robustness', 'contrastive learning', 'structural perturbation', 'graph defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00064</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title><link>https://arxiv.org/abs/2602.00061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes role conditioning (assigning social roles) as a training-free method to improve LLM safety/alignment.&lt;/li&gt;&lt;li&gt;Introduces a role-conditioned generator plus iterative role-based critics to refine outputs, outperforming principle-based and CoT baselines.&lt;/li&gt;&lt;li&gt;Reports large reductions in unsafe outputs on jailbreak benchmarks (e.g., WildJailbreak reduced from 81.4% to 3.6%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Ziheng', 'Jiakun Ding', 'Zhaowei Zhang', 'Ruosen Gao', 'Yingnian Wu', 'Demetri Terzopoulos', 'Yipeng Kang', 'Fangwei Zhong', 'Junqi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'jailbreak-defense', 'prompt-based defenses', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00061</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion</title><link>https://arxiv.org/abs/2602.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LSSF, a post-hoc safety re-alignment framework that constructs a low-rank projection matrix to extract principal safety components and fuses them back into fine-tuned LLMs via linear arithmetic.&lt;/li&gt;&lt;li&gt;Finds a stable, low-rank safety subspace that is largely isolated from general capabilities and introduces 'safety singular value entropy' to compute layer-wise safety-critical ranks.&lt;/li&gt;&lt;li&gt;Demonstrates that the method can restore safety alignment degraded by fine-tuning with minimal impact on downstream task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanghao Zhou', 'Panjia Qiu', 'Cen Chen', 'Hongyu Li', 'Mingyuan Chu', 'Xin Zhang', 'Jun Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'post-hoc-defense', 'low-rank-subspace', 'fine-tuning-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00038</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Federated Learning With Individualized Privacy Through Client Sampling</title><link>https://arxiv.org/abs/2501.17634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an Individualized Differential Privacy (IDP) method for Federated Learning by computing client-specific sampling rates and integrating them into a modified IDP-FedAvg.&lt;/li&gt;&lt;li&gt;Extends the SAMPLE algorithm from centralized settings to the federated setting to handle heterogeneous client privacy budgets.&lt;/li&gt;&lt;li&gt;Evaluates on multiple datasets and realistic privacy distributions, showing improvements over uniform DP baselines and better performance than a SCALE-based alternative.&lt;/li&gt;&lt;li&gt;Notes limitations for complex tasks with non-i.i.d. data due to constraints inherent to decentralized FL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Ole Borchardt', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'individualized privacy', 'privacy-preserving ML', 'defense mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.17634</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning</title><link>https://arxiv.org/abs/2409.01329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how image dataset characteristics (class count, imbalance, entropy, Fisher Discriminant Ratio) affect utility and privacy of CNNs trained with and without Differential Privacy (DP).&lt;/li&gt;&lt;li&gt;Finds class imbalance increases vulnerability for minority classes, but DP reduces that vulnerability; datasets with fewer classes yield better utility-privacy trade-offs.&lt;/li&gt;&lt;li&gt;High entropy or low FDR datasets degrade the utility-privacy trade-off (harder to maintain utility while preserving privacy).&lt;/li&gt;&lt;li&gt;Analysis is performed across multiple datasets and privacy budgets, offering guidance for dataset selection and privacy parameter choices in PPML.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Maurice-Maximilian Heykeroth', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'dataset-characteristics', 'privacy-leakage', 'robustness-to-imbalance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01329</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)</title><link>https://arxiv.org/abs/2211.11434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and evaluates differentially private (DP) machine learning models for COVID-19 detection from chest X-ray images, addressing dataset imbalance and stricter privacy budgets.&lt;/li&gt;&lt;li&gt;Empirically measures practical privacy via black-box membership inference attacks (MIAs) to estimate real-world leakage against DP models.&lt;/li&gt;&lt;li&gt;Finds that increasing DP guarantees yields only marginal reductions in empirical MIA leakage, suggesting limited practical defensive impact and highlighting task-dependent privacy-utility trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Lange', 'Maja Schneider', 'Peter Christen', 'Erhard Rahm']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'membership inference', 'privacy-utility tradeoff', 'medical imaging', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2211.11434</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</title><link>https://arxiv.org/abs/2602.02455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Drift-Bench, a diagnostic benchmark for LLM agents that evaluates multi-turn clarification and agentic pragmatics under input faults (e.g., missing parameters, false presuppositions, ambiguity).&lt;/li&gt;&lt;li&gt;Provides a unified taxonomy of cooperative breakdowns, a persona-driven user simulator, and the Rise evaluation protocol to measure grounded execution risk in state- and service-oriented environments.&lt;/li&gt;&lt;li&gt;Empirical results show substantial performance degradation under input faults and variation in clarification effectiveness across user personas and fault types, highlighting failure modes that can lead to unsafe executions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Bao', 'Zheyuan Zhang', 'Pengcheng Jing', 'Zhengqing Yuan', 'Kaiwen Shi', 'Yanfang Ye']&lt;/li&gt;&lt;li&gt;Tags: ['agent-safety', 'robustness', 'benchmark', 'input-faults', 'clarification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02455</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration</title><link>https://arxiv.org/abs/2602.02419</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeGround, an uncertainty-aware framework for GUI grounding that quantifies spatial dispersion of stochastic model outputs to detect risky/incorrect coordinate predictions.&lt;/li&gt;&lt;li&gt;Calibrates a test-time decision threshold with statistical FDR control to enable risk-aware predictions and reduce harmful automated GUI actions.&lt;/li&gt;&lt;li&gt;Demonstrates improved ability to distinguish correct vs. incorrect predictions and system-level accuracy gains (up to ~5.38 percentage points) on the ScreenSpot-Pro benchmark across multiple grounding models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingni Wang', 'Yue Fan', 'Xin Eric Wang']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'calibration', 'safety/guardrails', 'GUI grounding', 'false discovery rate (FDR) control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02419</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models</title><link>https://arxiv.org/abs/2602.02136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DGR (Distribution-Grounded Refinement), a method that transforms out-of-distribution safety reasoning datasets to match the target large reasoning model's internal distribution for safety alignment.&lt;/li&gt;&lt;li&gt;Shows DGR substantially mitigates the 'safety tax'—preserving or improving reasoning accuracy (e.g., +30.2% DirectRefusal, +21.2% R1-ACT over Vanilla SFT)—while maintaining safety/refusal behavior.&lt;/li&gt;&lt;li&gt;Demonstrates that the extent of reasoning degradation correlates with distributional shift and suggests safety alignment may function by activating latent knowledge (only ~10 samples can trigger effective refusal).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingsha Xie', 'Tiansheng Huang', 'Enneng Yang', 'Rui Min', 'Wenjie Lu', 'Xiaochun Cao', 'Naiqiang Tan', 'Li Shen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'dataset-refinement', 'defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02136</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron</title><link>https://arxiv.org/abs/2602.02027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight safety-aware decoding method that uses a single neuron as a gating mechanism combined with low-cost training of an expert model to steer LLM outputs.&lt;/li&gt;&lt;li&gt;Balances the model's intrinsic capabilities with external guidance to preserve utility while reducing unsafe outputs.&lt;/li&gt;&lt;li&gt;Claims advantages in training overhead and generalization across model scales, offering a practical, low-cost alignment/guardrail approach for LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sicheng Shen', 'Mingyang Lv', 'Han Shen', 'Jialin Wu', 'Binghao Wang', 'Zhou Yang', 'Guobin Shen', 'Dongcheng Zhao', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'decoding-based defense', 'guardrails', 'lightweight alignment', 'model self-reflection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02027</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title><link>https://arxiv.org/abs/2602.01750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Reward Auditing (ARA): a two-stage adversarial framework where a Hacker policy finds reward-model exploits and an Auditor learns to detect exploitation from latent representations.&lt;/li&gt;&lt;li&gt;Introduces Auditor-Guided RLHF (AG-RLHF) that gates/penalizes reward signals when Auditor detects hacking, converting reward hacking into a measurable control signal.&lt;/li&gt;&lt;li&gt;Empirical results across multiple scenarios (sycophancy, verbosity, code gaming) show improved alignment-utility tradeoffs and cross-domain generalization of both attacks and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Beigi', 'Ming Jin', 'Junshan Zhang', 'Qifan Wang', 'Lifu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'adversarial training', 'RLHF', 'attack detection', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01750</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating loss of control in advanced AI systems through instrumental goal trajectories</title><link>https://arxiv.org/abs/2602.01699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of instrumental goal trajectories (IGTs) — procurement, governance, and finance pathways organizations use to obtain resources that increase AI capabilities.&lt;/li&gt;&lt;li&gt;Argues these IGTs leave organizational artifacts that can be monitored as intervention points to detect and mitigate capability growth or undesirable behaviours.&lt;/li&gt;&lt;li&gt;Proposes shifting some mitigations for loss of control from model-centric techniques to organizational-level monitoring and governance to enable additional corrigibility and interruptibility mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Willem Fourie']&lt;/li&gt;&lt;li&gt;Tags: ['Defenses', 'AI governance', 'Organizational security', 'Corrigibility', 'Capability monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01699</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title><link>https://arxiv.org/abs/2602.01539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAGIC, a multi-turn multi-agent reinforcement learning framework that frames LLM safety alignment as an adversarial attacker-defender game.&lt;/li&gt;&lt;li&gt;Attacker agent iteratively rewrites queries into deceptive prompts while the defender agent learns policies to detect and refuse malicious inputs, producing co-evolution that finds long-tail vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of game equilibrium and safety guarantees, and shows empirically that the co-evolving training yields novel attack strategies and improved defense success without harming helpfulness.&lt;/li&gt;&lt;li&gt;Code released (link) and extensive experiments validate effectiveness against evolving adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wen', 'Zhida He', 'Han Qi', 'Ziyu Wan', 'Zhongtian Ma', 'Ying Wen', 'Tianhang Zheng', 'Xingcheng Xu', 'Chaochao Lu', 'Qiaosheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'defense', 'red-teaming', 'robustness', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01539</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Building Better Deception Probes Using Targeted Instruction Pairs</title><link>https://arxiv.org/abs/2602.01425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that linear probes for detecting model deception are highly sensitive to the choice of training instruction pairs and that targeted instruction pairs improve probe performance.&lt;/li&gt;&lt;li&gt;Introduces a human-interpretable taxonomy of deception and demonstrates that probes trained on targeted instruction pairs capture deceptive intent rather than content-specific patterns.&lt;/li&gt;&lt;li&gt;Finds prompt choice explains a large portion of probe performance variance (≈70.6%) and recommends designing specialized probes tailored to specific threat models instead of a universal detector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vikram Natarajan', 'Devina Jain', 'Shivam Arora', 'Satvik Golechha', 'Joseph Bloom']&lt;/li&gt;&lt;li&gt;Tags: ['deception_detection', 'linear_probes', 'model_monitoring', 'safety/guardrails', 'prompt_engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01425</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?</title><link>https://arxiv.org/abs/2602.01146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PersistBench, a benchmark to measure safety risks from persistent long-term memories in conversational LLMs.&lt;/li&gt;&lt;li&gt;Identifies two memory-specific vulnerabilities: cross-domain leakage (inappropriate injection of stored context) and memory-induced sycophancy (stored memories reinforcing user biases).&lt;/li&gt;&lt;li&gt;Evaluates 18 frontier and open-source LLMs, finding high failure rates (median 53% cross-domain, 97% sycophancy), highlighting widespread vulnerabilities.&lt;/li&gt;&lt;li&gt;Frames the work as a benchmark to drive development of safer long-term memory handling in conversational systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sidharth Pulipaka', 'Oliver Chen', 'Manas Sharma', 'Taaha S Bajwa', 'Vyas Raina', 'Ivaxi Sheth']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'long-term memory', 'benchmarking', 'model evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01146</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI</title><link>https://arxiv.org/abs/2602.01086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedBeads: an immutable, agent-native data substrate for clinical AI where events are 'Beads' in a Merkle DAG that cryptographically reference causal predecessors.&lt;/li&gt;&lt;li&gt;Implements a prototype (Go core engine, Python LLM middleware, React UI), including FHIR-to-DAG conversion and a BFS context retrieval algorithm (O(V+E)) to provide deterministic, tamper-evident context to LLM-based clinical agents.&lt;/li&gt;&lt;li&gt;Argues this design improves auditability and mitigates tampering/context-mismatch risks by shifting from probabilistic retrieval (RAG) to deterministic graph traversal; releases open-source software.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Takahito Nakajima']&lt;/li&gt;&lt;li&gt;Tags: ['data-integrity', 'tamper-evidence', 'secure-data-infrastructure', 'LLM-safety', 'medical-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01086</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How RLHF Amplifies Sycophancy</title><link>https://arxiv.org/abs/2602.01002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formal analysis showing how preference-based post-training (RLHF) can amplify sycophantic behavior via an explicit amplification mechanism linking optimization against a learned reward to bias in human preference data.&lt;/li&gt;&lt;li&gt;Derives that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in prompts and the learned reward, reducing to a mean-gap condition at first order.&lt;/li&gt;&lt;li&gt;Analyzes reward learning from pairwise comparisons under random utility models (e.g., Bradley–Terry) and characterizes when annotator bias induces the reward gap that drives sycophancy.&lt;/li&gt;&lt;li&gt;Proposes a training-time intervention: the unique KL-closest post-trained policy that prevents increased sycophancy and a closed-form minimal reward correction (agreement penalty); computational experiments validate reward gaps causing drift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itai Shapira', 'Gerdus Benade', 'Ariel D. Procaccia']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Sycophancy', 'Reward modeling', 'Human preference bias', 'Alignment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01002</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support</title><link>https://arxiv.org/abs/2602.00950</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents MindGuard: lightweight guardrail classifiers (4B/8B) designed to detect clinical crises and unsafe content in multi-turn mental-health conversations.&lt;/li&gt;&lt;li&gt;Introduces a clinically grounded risk taxonomy and releases MindGuard-testset, a turn-level annotated dataset of real-world dialogues by clinical experts.&lt;/li&gt;&lt;li&gt;Uses synthetic two-agent dialogues to train classifiers that reduce false positives at high-recall operating points and lower attack success/harmful engagement in adversarial multi-turn tests compared to general-purpose safeguards.&lt;/li&gt;&lt;li&gt;Releases models and human evaluation data to support safer deployment of clinician-paired language models for mental-health support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ant\\'onio Farinhas", 'Nuno M. Guerreiro', "Jos\\'e Pombal", 'Pedro Henrique Martins', 'Laura Melton', 'Alex Conway', 'Cara Dochat', "Maya D'Eon", 'Ricardo Rei']&lt;/li&gt;&lt;li&gt;Tags: ['safety-classifiers', 'guardrails', 'adversarial-robustness', 'mental-health', 'dataset-release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00950</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Head Attention Is a Multi-Player Game</title><link>https://arxiv.org/abs/2602.00861</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes transformer multi-head attention as a potential game among heads and shows gradient descent converges to Nash equilibria with possible inefficiencies (Price of Anarchy, PoA).&lt;/li&gt;&lt;li&gt;Proves bounds linking an interaction metric Γ(G) to excess hallucination probability and head redundancy, and shows regularization that reduces Γ(G) tightens PoA.&lt;/li&gt;&lt;li&gt;Proposes GAME-LoRA (Barlow Twins decorrelation + log-determinant coordination pressure) as a practical regularizer to reduce Γ(G).&lt;/li&gt;&lt;li&gt;Empirical results: Γ(G) predicts hallucination, emergent head coalitions observed, and GAME-LoRA reduces hallucination up to 18% (8% avg) without hurting knowledge performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kushal Chakrabarti', 'Nirmal Balachundar']&lt;/li&gt;&lt;li&gt;Tags: ['attention', 'robustness', 'hallucination', 'regularization', 'game-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00861</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Self-Guard: Defending Large Reasoning Models via enhanced self-reflection</title><link>https://arxiv.org/abs/2602.00707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Guard, a lightweight defense framework that enforces safety compliance at the model representation level for Large Reasoning Models (LRMs).&lt;/li&gt;&lt;li&gt;Two-stage method: (1) safety-oriented prompting to evoke latent safety reflection, and (2) safety activation steering that extracts and amplifies directional shifts in hidden states to prioritize safety over sycophantic instruction-following.&lt;/li&gt;&lt;li&gt;Claims robust safety improvements without harming utility, generalizes across unseen risks and different model scales, and is computationally efficient compared to heavy post-training interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingnan Zheng', 'Jingjun Xu', 'Yanzhen Luo', 'Chenhang Cui', 'Gelei Deng', 'Zhenkai Liang', 'Xiang Wang', 'An Zhang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'alignment', 'prompt-based safety', 'internal representation steering', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00707</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees</title><link>https://arxiv.org/abs/2602.00616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the safety vs. prompt-image alignment trade-off using total variation (TV) divergence, deriving a Safety-Prompt Alignment Trade-off (SPAT).&lt;/li&gt;&lt;li&gt;Proposes an inference-only prompt projection method that maps high-risk prompts into a tolerance-controlled safe set via a surrogate objective with verification, leaving benign prompts effectively unchanged.&lt;/li&gt;&lt;li&gt;Demonstrates empirical reductions in unsafe/inappropriate generations (16.7–60.0% relative reduction in inappropriate percentage) across multiple datasets and diffusion backbones while preserving benign prompt-image alignment, all without retraining the generator.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minhyuk Lee', 'Hyekyung Yoon', 'Myungjoo Kang']&lt;/li&gt;&lt;li&gt;Tags: ['safety defenses', 'guardrails', 'text-to-image', 'prompt filtering/projection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00616</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents</title><link>https://arxiv.org/abs/2602.00415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PolarMem, a training-free polarized latent graph memory that converts fuzzy perceptual likelihoods into discrete logical constraints for multimodal agents.&lt;/li&gt;&lt;li&gt;Stores explicit negation via a polarized graph topology with inhibitory connections and enforces a logic-dominant retrieval to suppress hallucinatory outputs that violate negative constraints.&lt;/li&gt;&lt;li&gt;Designed to work at inference time with frozen vision–language models; evaluated across eight VLMs and six benchmarks demonstrating improved verifiability and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhisheng Chen', 'Tingyu Wu', 'Zijie Zhou', 'Zhengwei Xie', 'Ziyan Weng', 'Yingwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robustness', 'hallucination-mitigation', 'memory-architecture', 'multimodal-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00415</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning</title><link>https://arxiv.org/abs/2602.00298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates how narrow fine-tuning on 11 domain-specific 'insecure' datasets induces emergent misalignment in LLMs, measuring misalignment rates with and without backdoor triggers.&lt;/li&gt;&lt;li&gt;Finds backdoor triggers increase misalignment in 77.8% of domains (avg drop 4.33 points) and that domain vulnerability varies widely (0% to 87.67%).&lt;/li&gt;&lt;li&gt;Shows membership inference metrics (adjusted for base model) can serve as a prior for predicting broad misalignment and investigates transferability of steering directions between EM models.&lt;/li&gt;&lt;li&gt;Provides a taxonomic ranking of domain-level emergent misalignment, a standardized recipe for constructing misaligned datasets, and releases code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Mishra', 'Mugilan Arulvanan', 'Reshma Ashok', 'Polina Petrova', 'Deepesh Suranjandass', 'Donnie Winkelmann']&lt;/li&gt;&lt;li&gt;Tags: ['backdoors', 'fine-tuning-induced-misalignment', 'membership-inference', 'vulnerability-benchmarking', 'emergent-misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.00298</guid><pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
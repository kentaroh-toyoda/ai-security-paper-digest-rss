<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 22 Dec 2025 23:09:51 +0000</lastBuildDate><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LookAhead Tuning, a lightweight data-driven fine-tuning method that previews partial answer prefixes to reduce perturbations to initial token distributions.&lt;/li&gt;&lt;li&gt;Designed to preserve preexisting safety alignment during fine-tuning so models retain built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Empirical results reportedly show maintained safety without degrading downstream task performance or robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'alignment/safety', 'robustness', 'safety-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeContext as Defense: Safe Image Editing in Diffusion Transformers</title><link>https://arxiv.org/abs/2512.16625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeContext, a defense that injects small targeted perturbations to weaken multimodal cross-attention in diffusion transformer (DiT) in-context image editors.&lt;/li&gt;&lt;li&gt;Shows that context propagation concentrates in early denoising steps and specific transformer blocks, enabling focused perturbations for efficiency.&lt;/li&gt;&lt;li&gt;Demonstrates empirical results on Flux Kontext and Step1X-Edit that block unauthorized image edits while preserving visual quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Shen', 'Mingyue Cui', 'Xingyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['image editing defense', 'adversarial perturbation', 'privacy', 'diffusion models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16625</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stylized Synthetic Augmentation further improves Corruption Robustness</title><link>https://arxiv.org/abs/2512.15675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data augmentation pipeline combining synthetic images and neural style transfer to improve robustness to common image corruptions.&lt;/li&gt;&lt;li&gt;Finds that stylized synthetic images, despite worse FID, improve classifier robustness and complement some rule-based augmentations (e.g., TrivialAugment).&lt;/li&gt;&lt;li&gt;Provides systematic empirical analysis of augmentation and hyperparameter effects and reports state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and TinyImageNet-C.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Siedel', 'Rojan Regmi', 'Abhirami Anand', 'Weijia Shao', 'Silvia Vock', 'Andrey Morozov']&lt;/li&gt;&lt;li&gt;Tags: ['corruption robustness', 'data augmentation', 'synthetic data', 'neural style transfer', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15675</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title><link>https://arxiv.org/abs/2512.12218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'visual faithfulness' for chain-of-thought-style reasoning in vision-language models, distinguishing perception steps (grounding in the image) from higher-level reasoning.&lt;/li&gt;&lt;li&gt;Proposes a training- and reference-free framework that decomposes chains into perception vs reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, validated by human meta-evaluation.&lt;/li&gt;&lt;li&gt;Introduces a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps, reducing Unfaithful Perception Rate while maintaining final-answer accuracy on multiple VLMs and perception-heavy benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rheeya Uppaal', 'Phu Mon Htut', 'Min Bai', 'Nikolaos Pappas', 'Zheng Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'multimodal-robustness', 'faithfulness', 'chain-of-thought', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12218</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeParts: a New Family of AI-Generated DeepFakes</title><link>https://arxiv.org/abs/2508.21052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'FakeParts', a new class of partial deepfakes consisting of subtle, localized spatial or temporal manipulations within otherwise authentic videos.&lt;/li&gt;&lt;li&gt;Presents FakePartsBench, a large-scale benchmark of ~81K videos (including 44K FakeParts) with pixel- and frame-level manipulation annotations to evaluate partial-manipulation detection.&lt;/li&gt;&lt;li&gt;User studies and evaluations show human detection accuracy drops up to 26% and state-of-the-art detectors also degrade significantly, exposing vulnerabilities in current detection methods.&lt;/li&gt;&lt;li&gt;Provides a resource to drive development of robust detection methods specifically targeting partial/localized manipulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Liu', 'Firas Gabetni', 'Awais Hussain Sani', 'Xi Wang', 'Soobash Daiboo', 'Gaetan Brison', 'Gianni Franchi', 'Vicky Kalogeiton']&lt;/li&gt;&lt;li&gt;Tags: ['deepfakes', 'dataset', 'detection-robustness', 'multimedia-forensics', 'partial-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21052</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders</title><link>https://arxiv.org/abs/2508.18236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Language-Grounded Sparse Encoders (LanSE) that decompose images into interpretable visual patterns with natural-language descriptions using interpretability modules and large multimodal models.&lt;/li&gt;&lt;li&gt;Discovers over 5,000 visual patterns with 93% human agreement and provides decomposed evaluations that reportedly outperform existing holistic methods.&lt;/li&gt;&lt;li&gt;Establishes a systematic evaluation of physical plausibility, demonstrates applicability to medical imaging, and claims adaptability to other domains and modalities (e.g., protein structures, time series).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Tang', 'Arash Lagzian', 'Srinivas Anumasa', 'Qiran Zou', 'Yingtao Zhu', 'Ye Zhang', 'Trang Nguyen', 'Yih-Chung Tham', 'Ehsan Adeli', 'Ching-Yu Cheng', 'Yilun Du', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'interpretability', 'generative-AI-detection', 'multimodal-analysis', 'content-moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18236</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title><link>https://arxiv.org/abs/2507.00724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model ownership verification for personalized large vision models (LVMs) and the shortcomings of existing defenses for fine-tuned models.&lt;/li&gt;&lt;li&gt;Proposes creating shadow models that retain common features while disrupting dataset-specific features, then representing dataset-specific features via output differences between victim and shadow models.&lt;/li&gt;&lt;li&gt;Trains a meta-classifier to detect stolen models based on presence of dataset-specific features and uses hypothesis testing to improve robustness; reports extensive experiments showing effectiveness against various stealing types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Zhu', 'Yiming Li', 'Haiqin Weng', 'Yan Liu', 'Tianwei Zhang', 'Shu-Tao Xia', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model ownership verification', 'model stealing defense', 'watermarking/forensics', 'fine-tuned large vision models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00724</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution</title><link>https://arxiv.org/abs/2411.07449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework that analyzes full temporal dynamics of diffusion denoising trajectories to attribute image origins (training set, model-generated, or external).&lt;/li&gt;&lt;li&gt;Shows that using information across the entire trajectory yields more robust classification and contradicts the "Goldilocks zone" hypothesis that membership signals appear only at narrow denoising stages.&lt;/li&gt;&lt;li&gt;Demonstrates that common membership inference methods fail under distribution shift or when model-generated data are present, exposing flaws in current practices.&lt;/li&gt;&lt;li&gt;Presents a first white-box model-attribution approach for diffusion models and advocates unifying data provenance into a single framework for generative systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andreas Floros', 'Seyed-Mohsen Moosavi-Dezfooli', 'Pier Luigi Dragotti']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'model-attribution', 'diffusion-models', 'data-provenance', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.07449</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness of Vision in Open Foundation Models</title><link>https://arxiv.org/abs/2512.17902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates adversarial robustness of LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 using untargeted PGD attacks applied to the visual input.&lt;/li&gt;&lt;li&gt;Empirical experiments on a VQA v2 subset measure accuracy degradation; Llama 3.2 Vision shows smaller performance drop under attack despite a lower baseline accuracy.&lt;/li&gt;&lt;li&gt;Concludes that the vision modality is a viable attack vector for degrading open-weight VLMs and that robustness does not necessarily correlate with standard benchmark performance—likely influenced by architecture and training choices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathon Fox', 'William J Buchanan', 'Pavlos Papadopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'visual adversarial attacks', 'vision-language models', 'PGD', 'VQA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17902</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visually Prompted Benchmarks Are Surprisingly Fragile</title><link>https://arxiv.org/abs/2512.17875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that visually prompted benchmarks for VLMs are highly fragile: small changes in visual markers (color, size) or low-level inference choices (e.g., JPEG compression) can substantially alter model rankings.&lt;/li&gt;&lt;li&gt;Demonstrates these effects across nine open- and closed-source VLMs and two visually prompted tasks, and that weaker models can be made to outperform stronger ones via simple prompt variants.&lt;/li&gt;&lt;li&gt;Provides a mitigation by curating datasets into a larger benchmark (VPBench) with 16 visual marker variants and releases analysis tools to improve evaluation robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiwen Feng', 'Long Lian', 'Lisa Dunlap', 'Jiahao Shu', 'XuDong Wang', 'Renhao Wang', 'Trevor Darrell', 'Alane Suhr', 'Angjoo Kanazawa']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmarking', 'vision-language-models', 'visual-prompting', 'evaluation-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17875</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</title><link>https://arxiv.org/abs/2512.17730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Diff-Gen, a 100k diffusion-generated image benchmark capturing spectral artifacts to improve cross-domain deepfake detection.&lt;/li&gt;&lt;li&gt;Proposes AdaptPrompt: a parameter-efficient adaptation of CLIP that jointly learns textual prompts and visual adapters while keeping the backbone frozen.&lt;/li&gt;&lt;li&gt;Shows layer ablation that pruning the final vision transformer block preserves high-frequency generative artifacts and improves detection; evaluates on 25 test sets with few-shot and source-attribution capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Jiang', 'Mohammed Talha Alam', 'Sohail Ahmed Khan', 'Duc-Tien Dang-Nguyen', 'Fakhri Karray']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'generalization', 'vision-language-models', 'dataset', 'transfer-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17730</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title><link>https://arxiv.org/abs/2512.17532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Robust-R1, a framework that explicitly models visual degradations via structured reasoning chains (supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling).&lt;/li&gt;&lt;li&gt;Introduces an 11K dataset with realistic degradations annotated with degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion to supervise degradation-aware reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness over general and robust baselines on R-Bench and stronger anti-degradation performance under multi-intensity/adversarial degradations on MMMB, MMStar, and RealWorldQA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Tang', 'Jianmin Chen', 'Wei Wei', 'Xiaogang Xu', 'Runtao Liu', 'Xiangyu Wu', 'Qipeng Xie', 'Jiafei Wu', 'Lei Zhang', 'Qifeng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal-llms', 'visual-degradations', 'dataset', 'adversarial-degradations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17532</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</title><link>https://arxiv.org/abs/2512.17495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GroundingME, a 1,005-example benchmark evaluating MLLMs on four grounding dimensions: Discriminative, Spatial, Limited, and Rejection.&lt;/li&gt;&lt;li&gt;Evaluates 25 state-of-the-art MLLMs, finding large gaps (best model 45.1% overall) and near-universal failure on rejection tasks where models hallucinate objects instead of acknowledging absence.&lt;/li&gt;&lt;li&gt;Proposes two improvement strategies: test-time scaling (small gains up to +2.9%) and data-mixture training to teach rejection (raising rejection accuracy from 0% to 27.9%).&lt;/li&gt;&lt;li&gt;Positions GroundingME as a diagnostic and roadmap for improving real-world visual grounding and reducing risky hallucinations in deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rang Li', 'Lei Li', 'Shuhuai Ren', 'Hao Tian', 'Shuhao Gu', 'Shicheng Li', 'Zihao Yue', 'Yudong Wang', 'Wenhan Ma', 'Zhe Yang', 'Jingyuan Ma', 'Zhifang Sui', 'Fuli Luo']&lt;/li&gt;&lt;li&gt;Tags: ['visual-grounding', 'MLLM-safety', 'hallucination', 'benchmark', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17495</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title><link>https://arxiv.org/abs/2512.17450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MULTIAQUA, a synchronized, calibrated, and annotated multimodal maritime dataset (RGB, thermal, IR, LiDAR, etc.) aimed at improving scene interpretation under poor visibility.&lt;/li&gt;&lt;li&gt;Evaluates several multimodal semantic segmentation methods on a challenging nighttime test set to demonstrate benefits of additional modalities.&lt;/li&gt;&lt;li&gt;Proposes training strategies to make multimodal models robust to near-complete darkness, including training using only daytime images to simplify data collection and annotation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jon Muhovi\\v{c}', 'Janez Per\\v{s}']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal dataset', 'robustness', 'semantic segmentation', 'maritime autonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17450</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.17350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a pixel-level mapping pre-processing that perturbs pixel value distributions to remove semantic shortcuts used by detectors.&lt;/li&gt;&lt;li&gt;Claims this forces detectors to rely on generalizable high-frequency generative artifacts, improving cross-generator (GAN and diffusion) detection performance.&lt;/li&gt;&lt;li&gt;Provides comprehensive experiments showing significant boosts in cross-generator generalization and analysis supporting the semantic-disruption hypothesis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenming Zhou', 'Jiaan Wang', 'Yu Li', 'Lei Li', 'Juan Cao', 'Sheng Tang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'robustness / generalization', 'forensics / deepfake detection', 'preprocessing / defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17350</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</title><link>https://arxiv.org/abs/2512.17320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EMMA, a benchmark evaluating concept erasure methods for text-to-image models across 12 metrics covering robustness, bias, quality, and efficiency.&lt;/li&gt;&lt;li&gt;Tests methods on five domains (objects, celebrities, art styles, NSFW, copyright) and stresses challenging conditions like indirect/implicit prompts and visually similar non-target concepts.&lt;/li&gt;&lt;li&gt;Finds existing erasure methods often fail under implicit prompts, struggle to preserve non-target similar concepts, and can amplify gender/ethnicity bias compared to original models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Wei', 'Yuta Nakashima', 'Noa Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'safety evaluation', 'text-to-image', 'bias and privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17320</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title><link>https://arxiv.org/abs/2512.17213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies hallucination and unverifiable chain-of-thought risks in medical vision-language models and limitations of outcome-based RL methods like GRPO.&lt;/li&gt;&lt;li&gt;Proposes CheXPO-v2: a process-supervision alignment method using a Knowledge Graph Consistency Reward that parses reasoning into Disease–Relation–Anatomy triplets to penalize incoherent logic.&lt;/li&gt;&lt;li&gt;Adds hard-example mining and demonstrates state-of-the-art accuracy on MIMIC-CXR-VQA with high data efficiency (5k samples), producing more verifiable clinical reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Liang', 'Yuxuan An', 'Di Wang', 'Jiawei Hu', 'Zhicheng Jiao', 'Bin Jing', 'Quan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'medical VLMs', 'reward modeling', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17213</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</title><link>https://arxiv.org/abs/2512.17189</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play module that uses anatomical masks to guide a three-tiered contrastive decoding process (token, attention, logits) to reduce hallucinations in MedVLMs.&lt;/li&gt;&lt;li&gt;ARCD dynamically re-weights model components to focus on specified regions, reinforcing anatomical grounding and suppressing outputs not supported by visual evidence.&lt;/li&gt;&lt;li&gt;Evaluated across multiple medical modalities (chest X-ray, CT, brain MRI, ocular ultrasound), showing improved regional understanding, reduced hallucinations, and increased diagnostic accuracy without additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Liang', 'Chenxi Liu', 'Zhi Ma', 'Di Wang', 'Bin Jing', 'Quan Wang', 'Yuanyuan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'medical VLMs', 'contrastive decoding', 'model reliability', 'region-guided grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17189</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies conformal prediction to transform embedding/NLI heuristic scores into finite-sample decision sets for hallucination detection in RAG systems (calibration n=600).&lt;/li&gt;&lt;li&gt;Empirically demonstrates a stark dichotomy: embedding methods succeed on synthetic hallucinations (95% coverage, 0% FPR) but fail catastrophically on real RLHF model hallucinations (100% FPR at target coverage) from HaluEval.&lt;/li&gt;&lt;li&gt;Analyzes failure as a distributional-tail problem where the hardest hallucinations are semantically indistinguishable from faithful outputs; NLI yields decent AUC (0.81) but cannot separate these hard cases, whereas GPT-4 reasoning reduces FPR substantially—coined the 'Semantic Illusion'.&lt;/li&gt;&lt;li&gt;Implication: surface-level embedding/NLI detectors are insufficient for safety-critical hallucination detection; reasoning-capable judges or alternative defenses are needed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'RAG', 'conformal-prediction', 'robustness', 'alignment-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title><link>https://arxiv.org/abs/2512.12218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'visual faithfulness' for chain-of-thought-style reasoning in vision-language models, distinguishing perception steps (grounding in the image) from higher-level reasoning.&lt;/li&gt;&lt;li&gt;Proposes a training- and reference-free framework that decomposes chains into perception vs reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, validated by human meta-evaluation.&lt;/li&gt;&lt;li&gt;Introduces a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps, reducing Unfaithful Perception Rate while maintaining final-answer accuracy on multiple VLMs and perception-heavy benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rheeya Uppaal', 'Phu Mon Htut', 'Min Bai', 'Nikolaos Pappas', 'Zheng Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'multimodal-robustness', 'faithfulness', 'chain-of-thought', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12218</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Clean Up the Mess: Addressing Data Pollution in Cryptocurrency Abuse Reporting Services</title><link>https://arxiv.org/abs/2410.21041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes data pollution in two crowd-sourced cryptocurrency abuse reporting services using 289K reports collected over 6 years, finding heavy spam and mislabeling (e.g., benign addresses reported).&lt;/li&gt;&lt;li&gt;Builds a labeled dataset of 19,443 reports with 19 abuse types and quantifies impact on financial estimates (victim-reported losses substantially underestimate criminal revenue).&lt;/li&gt;&lt;li&gt;Proposes an unsupervised LLM-based classifier to detect spam and classify abuse types, achieving high F1 scores (0.95 classification, 0.99 spam detection) and outperforming supervised and naive LLM baselines.&lt;/li&gt;&lt;li&gt;Demonstrates utility of the classifier for correcting financial impact estimates and identifying high-impact abuse categories (e.g., investment scams).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gibran Gomez', 'Kevin van Liebergen', 'Davide Sanvito', 'Giuseppe Siracusano', 'Roberto Gonzalez', 'Juan Caballero']&lt;/li&gt;&lt;li&gt;Tags: ['data-poisoning', 'LLM-based-defense', 'spam-detection', 'dataset-quality', 'cryptocurrency-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</title><link>https://arxiv.org/abs/2512.16189</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-independent fact-checking module plus a domain-specific summarization LLM fine-tuned with LoRA on MIMIC-III to reduce hallucinations in healthcare outputs.&lt;/li&gt;&lt;li&gt;Fact-checker performs granular validation using numerical tests and discrete logical checks to verify propositions against EHRs.&lt;/li&gt;&lt;li&gt;Evaluated on 104 summaries (3,786 extracted propositions) with fact-checker achieving precision 0.8904, recall 0.8234, F1 0.8556; summary model reports ROUGE-1 0.5797 and BERTScore 0.9120.&lt;/li&gt;&lt;li&gt;Focus is on improving factuality and reliability of clinical summaries rather than adversarial attack generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Musarrat Zeba', 'Abdullah Al Mamun', 'Kishoar Jahan Tithee', 'Debopom Sutradhar', 'Mohaimenul Azam Khan Raiaan', 'Saddam Mukta', 'Reem E. Mohamed', 'Md Rafiqul Islam', 'Yakub Sebastian', 'Mukhtar Hussain', 'Sami Azam']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'fact-checking', 'healthcare LLMs', 'domain adaptation', 'evaluation/metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16189</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals</title><link>https://arxiv.org/abs/2512.01037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'semantic confusion' — local inconsistency where a safety-aligned model refuses some paraphrases of an otherwise harmless intent but accepts others.&lt;/li&gt;&lt;li&gt;Releases ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that keep intent fixed while varying surface form.&lt;/li&gt;&lt;li&gt;Proposes three model-agnostic, token-level metrics (Confusion Index, Confusion Rate, Confusion Depth) using embeddings, next-token probabilities, and perplexity to quantify local inconsistency.&lt;/li&gt;&lt;li&gt;Evaluates diverse model families and deployment guards, showing global false-rejection rates hide important structure and presenting confusion-aware auditing to reduce false refusals while retaining safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riad Ahmed Anonto', 'Md Labid Al Nahiyan', 'Md Tanvir Hassan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'LLM-refusals', 'benchmarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01037</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Focus Memory for Language Models</title><link>https://arxiv.org/abs/2511.12712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Focus Memory (AFM), a context management system that assigns past messages one of three fidelity levels (Full, Compressed, Placeholder) to fit a fixed token budget while preserving important constraints.&lt;/li&gt;&lt;li&gt;AFM uses semantic relevance, temporal decay, and importance classification to allocate fidelity and packs messages chronologically to maintain critical information.&lt;/li&gt;&lt;li&gt;Evaluated on two multi-turn dialogue benchmarks stressing long-horizon constraint preservation (peanut allergy safety scenario and illegal tax evasion refusal), showing large improvements over baseline history strategies.&lt;/li&gt;&lt;li&gt;AFM requires no model weight changes or external retrieval infrastructure and is released as open-source compatible with OpenAI-style chat APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['dialogue memory', 'safety', 'alignment', 'robustness', 'context management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12712</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</title><link>https://arxiv.org/abs/2511.06000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DemogSummary, an age-stratified dataset of biomedical primary studies (child, adult, older adult) for evaluating LLM summarisation fidelity.&lt;/li&gt;&lt;li&gt;Proposes a Demographic Salience Score (DSS) to quantify retention and hallucination of age-related entities in abstractive summaries.&lt;/li&gt;&lt;li&gt;Evaluates three LLMs (Qwen, Longformer, GPT-4.1 Nano) and finds systematic disparities: lowest demographic fidelity in adult-focused summaries and higher hallucination for under-represented populations.&lt;/li&gt;&lt;li&gt;Highlights implications for fairness-aware evaluation frameworks and the need for improved summarisation pipelines in biomedical NLP to reduce harmful hallucinations and demographic bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Favour Yahdii Aghaebe', 'Tanefa Apekey', 'Elizabeth Williams', 'Nafise Sadat Moosavi']&lt;/li&gt;&lt;li&gt;Tags: ['demographic bias', 'hallucination', 'biomedical NLP', 'evaluation/benchmarking', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06000</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title><link>https://arxiv.org/abs/2504.03790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QAlign, a test-time alignment method that uses MCMC sampling to approximate the optimal aligned output distribution per prompt without modifying model weights or requiring logit access.&lt;/li&gt;&lt;li&gt;Addresses degradation of existing test-time reward-model-based search methods (e.g., best-of-n) by avoiding over-optimization of imperfect reward proxies as compute scales.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that QAlign outperforms methods like best-of-n, majority voting, weighted voting, and DPO on reasoning and preference-aligned benchmarks (GSM8K, MATH500, IFEval, MMLU-Redux, TruthfulQA) using both task-specific and preference-trained RMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gon\\c{c}alo Faria', 'Noah A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'reward models', 'sampling / MCMC']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03790</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LookAhead Tuning, a lightweight data-driven fine-tuning method that previews partial answer prefixes to reduce perturbations to initial token distributions.&lt;/li&gt;&lt;li&gt;Designed to preserve preexisting safety alignment during fine-tuning so models retain built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Empirical results reportedly show maintained safety without degrading downstream task performance or robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'alignment/safety', 'robustness', 'safety-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs</title><link>https://arxiv.org/abs/2502.01436</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a fully automated, black-box method to evaluate marketplace Custom GPTs for compliance with platform usage policies using large-scale discovery, policy-driven red-teaming prompts, and an LLM-as-judge.&lt;/li&gt;&lt;li&gt;Validates the automated compliance assessor against a human-annotated ground truth (binary violation detection F1 = 0.975) and applies it to 782 Custom GPTs from the GPT Store.&lt;/li&gt;&lt;li&gt;Finds 58.7% of evaluated GPTs produce at least one policy-violating response and shows customization usually amplifies model-level failure modes rather than creating new ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Rodriguez', 'William Seymour', 'Jose M. Del Alamo', 'Jose Such']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'policy compliance evaluation', 'automated testing', 'marketplace moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01436</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CIFE: Code Instruction-Following Evaluation</title><link>https://arxiv.org/abs/2512.17387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIFE, a benchmark of 1,000 Python tasks with an average of 7 developer-specified constraints each (13 constraint categories) curated via a four-stage human-LLM pipeline.&lt;/li&gt;&lt;li&gt;Evaluates 14 open- and closed-source models with adherence metrics and proposes the C2A Score, a composite measure combining functional correctness and constraint compliance.&lt;/li&gt;&lt;li&gt;Finds a large gap between partial and strict constraint satisfaction (strong models achieve &gt;90% partial adherence but only 39–66% strict adherence), emphasizing that trustworthy code generation requires consistent adherence to developer intent including robustness and security constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sravani Gunnu', 'Shanmukha Guttula', 'Hima Patel']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'Code generation', 'Instruction-following', 'Safety/robustness', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17387</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</title><link>https://arxiv.org/abs/2512.17375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a practical vulnerability in LLM-as-a-Judge and reward-model systems where short, low-perplexity 'control token' sequences flip binary judgments from correct 'No' to incorrect 'Yes'.&lt;/li&gt;&lt;li&gt;Introduces AdvJudge-Zero, a zero-shot method that uses next-token distributions and beam search to discover diverse, realistic control-token sequences that a policy could plausibly generate.&lt;/li&gt;&lt;li&gt;Analyzes mechanism: induced hidden-state perturbations concentrate in a low-rank 'soft mode' that is anti-aligned with the judge's refusal direction, explaining how small sequences produce systematic flips.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation showing high false-positive rates across large open-weight and specialized judge models on math/reasoning benchmarks, and demonstrates mitigation via LoRA-based adversarial training that reduces false positives while preserving evaluation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tung-Ling Li', 'Yuhao Wu', 'Hongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'reward model vulnerability', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17375</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Generalization in Role-Playing Models via Information Theory</title><link>https://arxiv.org/abs/2512.17270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an information-theoretic metric (R-EMID) to quantify RPM performance degradation under distribution shifts (user, character, dialogue).&lt;/li&gt;&lt;li&gt;Derives an upper bound on R-EMID to predict worst-case generalization and isolates how different shifts contribute to performance loss.&lt;/li&gt;&lt;li&gt;Proposes a co-evolving reinforcement learning framework to better estimate dialogue response generation probabilities needed to compute R-EMID.&lt;/li&gt;&lt;li&gt;Empirically evaluates RPMs with R-EMID, finding user shift is the most harmful and RL-based adaptation most effective for improving generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongqi Li', 'Hao Lang', 'Fei Huang', 'Tieyun Qian', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution-shift', 'evaluation-metric', 'role-playing-models', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17270</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports</title><link>https://arxiv.org/abs/2512.17776</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DEER, a benchmark of 50 report-writing tasks across 13 domains for evaluating expert-level LLM reports.&lt;/li&gt;&lt;li&gt;Provides an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimensions) operationalized into 130 fine-grained rubric items and task-specific guidance for more consistent LLM judging.&lt;/li&gt;&lt;li&gt;Proposes a document-level fact-checking architecture that extracts and verifies all claims (cited and uncited) across a report and quantifies external-evidence quality.&lt;/li&gt;&lt;li&gt;Demonstrates strong correlation with human expert judgments and yields diagnostic insights into system strengths and weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janghoon Han', 'Heegyu Kim', 'Changho Lee', 'Dahm Lee', 'Min Hyung Park', 'Hosung Song', 'Stanley Jungkyu Choi', 'Moontae Lee', 'Honglak Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety_evaluation', 'factuality', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17776</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering</title><link>https://arxiv.org/abs/2512.17677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Bayesian posterior inference to quantify uncertainty in neural question-answering models, starting with an MLP on Iris and scaling to language models.&lt;/li&gt;&lt;li&gt;Compares Laplace approximations against MAP estimates for uncertainty calibration and selective prediction (abstention/'I don't know').&lt;/li&gt;&lt;li&gt;Evaluates approaches on CommonsenseQA, including applications to a frozen head and LoRA-adapted transformers, emphasizing responsibility and ethical deployment via abstention when confidence is low.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riccardo Di Sipio']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'calibration', 'selective prediction/abstention', 'Bayesian methods', 'AI safety/ethical deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17677</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Linear Personality Probing and Steering in LLMs: A Big Five Study</title><link>https://arxiv.org/abs/2512.17639</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses Llama 3.3 (70B) to generate fictional characters with Big Five trait scores, samples hidden activations via prompts, and fits per-layer linear directions (via regression) aligned to trait scores.&lt;/li&gt;&lt;li&gt;Shows those linear directions are effective probes for detecting personality-like traits in activations.&lt;/li&gt;&lt;li&gt;Finds steering via these linear directions produces reliable effects in constrained/forced-choice tasks but has limited or context-sensitive influence in open-ended generation or when extra prompt context is present.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michel Frising', 'Daniel Balcells']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'steering', 'probing', 'LLM interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17639</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Perturb Your Data: Paraphrase-Guided Training Data Watermarking</title><link>https://arxiv.org/abs/2512.17075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPECTRA: a paraphrase-guided watermarking scheme that paraphrases dataset examples with an LLM and selects paraphrases whose score (from a separate scoring model) closely matches the original to avoid distribution shift.&lt;/li&gt;&lt;li&gt;Detection is performed by comparing token probabilities from a suspect model against the scoring model to test whether watermarked data influenced training.&lt;/li&gt;&lt;li&gt;Claims very strong detectability (p-value gap over nine orders of magnitude) even when watermarked data comprise &lt;0.001% of the training corpus and that the watermark survives large-scale LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Shetty', 'Mirazul Haque', 'Petr Babkin', 'Zhiqiang Ma', 'Xiaomo Liu', 'Manuela Veloso']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'training-data-provenance', 'dataset-protection', 'model-auditing', 'paraphrase-guided-watermark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17075</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Women's Health Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2512.17028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Women's Health Benchmark (WHB), a 96-item benchmark evaluating LLM performance specifically on women's health across five specialties and three query types.&lt;/li&gt;&lt;li&gt;Categorizes failures into eight medical error types (e.g., dosage errors, missed urgency, inappropriate recommendations) and evaluates 13 state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;Finds high failure rates (~60%), large variation across specialties/error types, and specific safety concerns such as models missing urgency and giving inappropriate recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victoria-Elisabeth Gruber', 'Razvan Marinescu', 'Diego Fajardo', 'Amin H. Nassar', 'Christopher Arkfeld', 'Alexandria Ludlow', 'Shama Patel', 'Mehrnoosh Samaei', 'Valerie Klug', 'Anna Huber', 'Marcel G\\"uhner', 'Albert Botta i Orfila', 'Irene Lagoja', 'Kimya Tarr', 'Haleigh Larson', 'Mary Beth Howard']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'medical benchmark', 'LLM evaluation', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17028</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stylized Synthetic Augmentation further improves Corruption Robustness</title><link>https://arxiv.org/abs/2512.15675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a data augmentation pipeline combining synthetic images and neural style transfer to improve robustness to common image corruptions.&lt;/li&gt;&lt;li&gt;Finds that stylized synthetic images, despite worse FID, improve classifier robustness and complement some rule-based augmentations (e.g., TrivialAugment).&lt;/li&gt;&lt;li&gt;Provides systematic empirical analysis of augmentation and hyperparameter effects and reports state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and TinyImageNet-C.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Siedel', 'Rojan Regmi', 'Abhirami Anand', 'Weijia Shao', 'Silvia Vock', 'Andrey Morozov']&lt;/li&gt;&lt;li&gt;Tags: ['corruption robustness', 'data augmentation', 'synthetic data', 'neural style transfer', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15675</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title><link>https://arxiv.org/abs/2512.12218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'visual faithfulness' for chain-of-thought-style reasoning in vision-language models, distinguishing perception steps (grounding in the image) from higher-level reasoning.&lt;/li&gt;&lt;li&gt;Proposes a training- and reference-free framework that decomposes chains into perception vs reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, validated by human meta-evaluation.&lt;/li&gt;&lt;li&gt;Introduces a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps, reducing Unfaithful Perception Rate while maintaining final-answer accuracy on multiple VLMs and perception-heavy benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rheeya Uppaal', 'Phu Mon Htut', 'Min Bai', 'Nikolaos Pappas', 'Zheng Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'multimodal-robustness', 'faithfulness', 'chain-of-thought', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12218</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title><link>https://arxiv.org/abs/2508.10501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PASS (Probabilistic Agentic Supernet Sampling), a multimodal agentic framework for Chest X‑Ray reasoning that samples adaptive tool workflows and outputs probability-annotated decision trajectories for auditability.&lt;/li&gt;&lt;li&gt;PASS maintains an evolving personalized memory, supports early-exit for efficiency, and selects tools via a learned task-conditioned distribution over a multi-tool supernet.&lt;/li&gt;&lt;li&gt;Presents a three-stage training pipeline (expert warm-up, contrastive path-ranking, cost-aware RL) and a new benchmark CAB-E for multi-step, safety-critical CXR reasoning.&lt;/li&gt;&lt;li&gt;Claims improved accuracy, semantic alignment, and cost-performance tradeoffs versus baselines, emphasizing interpretable outputs to enhance medical AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Feng', 'Junye Du', 'Yingying Hong', 'Qifan Wang', 'Lequan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['medical-ai', 'interpretability', 'safety', 'agentic-systems', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10501</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models</title><link>https://arxiv.org/abs/2506.20915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZKPROV, a cryptographic framework that lets verifiers confirm an LLM's response was trained on authority-certified datasets without revealing training data or model internals.&lt;/li&gt;&lt;li&gt;Binds training datasets, model parameters, and generated responses and attaches zero-knowledge proofs to responses to validate dataset provenance and relevance to queries.&lt;/li&gt;&lt;li&gt;Reports sublinear scaling and end-to-end proof generation/verification overhead under 3.3 seconds for models up to 8B parameters, targeting practical deployment in regulated domains (e.g., healthcare).&lt;/li&gt;&lt;li&gt;Provides formal security guarantees proving dataset confidentiality and trustworthy provenance verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mina Namazi', 'Alexander Nemecek', 'Erman Ayday']&lt;/li&gt;&lt;li&gt;Tags: ['dataset provenance', 'zero-knowledge proofs', 'privacy-preserving verification', 'model auditing', 'training-data certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20915</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title><link>https://arxiv.org/abs/2504.03790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QAlign, a test-time alignment method that uses MCMC sampling to approximate the optimal aligned output distribution per prompt without modifying model weights or requiring logit access.&lt;/li&gt;&lt;li&gt;Addresses degradation of existing test-time reward-model-based search methods (e.g., best-of-n) by avoiding over-optimization of imperfect reward proxies as compute scales.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that QAlign outperforms methods like best-of-n, majority voting, weighted voting, and DPO on reasoning and preference-aligned benchmarks (GSM8K, MATH500, IFEval, MMLU-Redux, TruthfulQA) using both task-specific and preference-trained RMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gon\\c{c}alo Faria', 'Noah A. Smith']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'reward models', 'sampling / MCMC']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03790</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LookAhead Tuning, a lightweight data-driven fine-tuning method that previews partial answer prefixes to reduce perturbations to initial token distributions.&lt;/li&gt;&lt;li&gt;Designed to preserve preexisting safety alignment during fine-tuning so models retain built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Empirical results reportedly show maintained safety without degrading downstream task performance or robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'alignment/safety', 'robustness', 'safety-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies conformal prediction to transform embedding/NLI heuristic scores into finite-sample decision sets for hallucination detection in RAG systems (calibration n=600).&lt;/li&gt;&lt;li&gt;Empirically demonstrates a stark dichotomy: embedding methods succeed on synthetic hallucinations (95% coverage, 0% FPR) but fail catastrophically on real RLHF model hallucinations (100% FPR at target coverage) from HaluEval.&lt;/li&gt;&lt;li&gt;Analyzes failure as a distributional-tail problem where the hardest hallucinations are semantically indistinguishable from faithful outputs; NLI yields decent AUC (0.81) but cannot separate these hard cases, whereas GPT-4 reasoning reduces FPR substantially—coined the 'Semantic Illusion'.&lt;/li&gt;&lt;li&gt;Implication: surface-level embedding/NLI detectors are insufficient for safety-critical hallucination detection; reasoning-capable judges or alternative defenses are needed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'RAG', 'conformal-prediction', 'robustness', 'alignment-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Deep Gaussian Process Proximal Policy Optimization</title><link>https://arxiv.org/abs/2511.18214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GPPO, a scalable model-free actor-critic algorithm that uses Deep Gaussian Processes to represent both policy and value function.&lt;/li&gt;&lt;li&gt;Claims well-calibrated uncertainty estimates from DGPs to guide safer and more effective exploration in RL.&lt;/li&gt;&lt;li&gt;Maintains competitive performance versus standard Proximal Policy Optimization on high-dimensional continuous control benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthijs van der Lende', 'Juan Cardenas-Cartagena']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'uncertainty-estimation', 'safe-exploration', 'gaussian-processes', 'policy-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18214</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look-Ahead Reasoning on Learning Platforms</title><link>https://arxiv.org/abs/2511.14745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes level-k (look-ahead) reasoning for users on learning platforms, where users anticipate peers' actions and model updates.&lt;/li&gt;&lt;li&gt;Shows level-k reasoning speeds convergence but does not change the eventual equilibrium outcomes, so higher-level individual reasoning yields no long-run benefit.&lt;/li&gt;&lt;li&gt;Analyzes collective (coordinated) user optimization versus selfish behavior, characterizing when coordination improves outcomes and introducing an alignment notion between learner and users' utilities.&lt;/li&gt;&lt;li&gt;Positions look-ahead reasoning as a generalization of algorithmic collective action and provides utility trade-offs for contesting algorithmic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiqing Zhu', 'Tijana Zrnic', 'Celestine Mendler-D\\"unner']&lt;/li&gt;&lt;li&gt;Tags: ['strategic user behavior', 'algorithmic collective action', 'alignment', 'adversarial manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14745</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</title><link>https://arxiv.org/abs/2511.12817</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FAITH, a framework that decomposes LLM medical responses into atomic claims, links them to a medical knowledge graph (KG), and scores claims via evidence paths without requiring reference answers.&lt;/li&gt;&lt;li&gt;Evaluates KG-grounded factuality scoring across diverse medical tasks and shows higher correlation with clinician judgments than baseline approaches.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to textual variance and provides explainable scoring to help users understand and mitigate LLM limitations in healthcare.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shasha Zhou', 'Mingyu Huang', 'Jack Cole', 'Charles Britton', 'Ming Yin', 'Jan Wolber', 'Ke Li']&lt;/li&gt;&lt;li&gt;Tags: ['factuality-evaluation', 'safety-evaluation', 'knowledge-graphs', 'medical-LLMs', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12817</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semi-Supervised Preference Optimization with Limited Feedback</title><link>https://arxiv.org/abs/2511.00040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semi-Supervised Preference Optimization (SSPO) to learn from a small set of pairwise preference labels plus a large pool of unpaired samples.&lt;/li&gt;&lt;li&gt;Proves existence of an optimal reward threshold that allows principled pseudo-labeling of unpaired responses into winners/losers.&lt;/li&gt;&lt;li&gt;Uses pseudo-labels to distill latent preferences from large unlabelled data, reducing need for costly human feedback while preserving alignment.&lt;/li&gt;&lt;li&gt;Empirical results show strong data efficiency (e.g., Mistral-7B-Instruct on 1% UltraFeedback outperforming baselines trained on 10%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seonggyun Lee', 'Sungjun Lim', 'Seojin Park', 'Soeun Cheon', 'Kyungwoo Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'semi-supervised learning', 'reward modeling', 'data efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00040</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</title><link>https://arxiv.org/abs/2508.17702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MolMark, a learnable atom-level watermarking framework that embeds digital signatures into molecular structures with minimal impact on functionality.&lt;/li&gt;&lt;li&gt;Uses SE(3)-invariant features to ensure geometric robustness (rotation, translation, reflection) and integrates as a learned transformation with generative models.&lt;/li&gt;&lt;li&gt;Demonstrates embedding 16-bit watermarks while preserving &gt;90% of key molecular properties and achieving &gt;95% extraction accuracy under SE(3) transforms on QM9 and GEOM-DRUG with GeoBFN/GeoLDM.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runwen Hu', 'Peilin Chen', 'Keyan Ding', 'Shiqi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'molecular-generation', 'IP-protection', 'robustness', 'SE(3)-invariance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17702</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Certified Unlearning Approach without Access to Source Data</title><link>https://arxiv.org/abs/2506.06486</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a certified unlearning framework that removes data from trained models without requiring access to the original training samples by using a surrogate dataset that approximates the source data's statistics.&lt;/li&gt;&lt;li&gt;Introduces controlled noise scaling calibrated to the statistical distance between surrogate and source distributions, with theoretical guarantees on post-unlearning model behavior.&lt;/li&gt;&lt;li&gt;Presents practical noise calibration techniques and evaluates the approach on synthetic and real-world datasets, discussing trade-offs between privacy guarantees and utility when the statistical distance is approximated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Umit Yigit Basaran', 'Sk Miraj Ahmed', 'Amit Roy-Chowdhury', 'Basak Guler']&lt;/li&gt;&lt;li&gt;Tags: ['certified unlearning', 'data privacy', 'surrogate datasets', 'noise calibration', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06486</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</title><link>https://arxiv.org/abs/2409.03735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'privacy bias' as the appropriateness of information flows in LLM responses and introduces 'privacy bias delta' to quantify deviations from expected privacy norms.&lt;/li&gt;&lt;li&gt;Proposes a contextual integrity–based auditing metric to assess privacy biases in LLM outputs, accounting for sensitivity across prompt variations.&lt;/li&gt;&lt;li&gt;Applies the metric to compare different LLMs and analyzes how model capacity and optimization choices affect privacy biases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yan Shvartzshnaider', 'Vasisht Duddu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy_audit', 'contextual_integrity', 'LLM_evaluation', 'privacy_bias', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.03735</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection</title><link>https://arxiv.org/abs/2306.09158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a margin-based learning framework that leverages freely available unlabeled "wild" data to handle both covariate shifts (OOD generalization) and semantic shifts (OOD detection) simultaneously.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence that the margin constraint is key to achieving both robust generalization and reliable detection of semantic OODs.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing the method outperforms baselines that specialize in either OOD generalization or OOD detection; code is publicly available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyue Bai', 'Gregory Canal', 'Xuefeng Du', 'Jeongyeol Kwon', 'Robert Nowak', 'Yixuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'Out-of-distribution generalization', 'Robustness', 'Safety evaluation', 'Unlabeled data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.09158</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</title><link>https://arxiv.org/abs/2512.17899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Distributionally Robust Imitation Policy (DRIP), a layered control architecture that combines Taylor Series Imitation Learning (TaSIL) and L1-Distributionally Robust Adaptive Control (ℓ1-DRAC) to obtain certifiable autonomy.&lt;/li&gt;&lt;li&gt;Addresses two types of distribution shift in imitation learning: those from policy errors and those from exogenous disturbances / model uncertainty, by assigning layer-specific input/output requirements to guarantee end-to-end certificates.&lt;/li&gt;&lt;li&gt;Demonstrates a framework to integrate learning-based components (e.g., perception) with certifiable model-based decision-making for safety and robustness in dynamical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Gahlawat', 'Ahmed Aboudonia', 'Sandeep Banik', 'Naira Hovakimyan', 'Nikolai Matni', 'Aaron D. Ames', 'Gioele Zardini', 'Alberto Speranzon']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'imitation learning', 'certifiable autonomy', 'distributional robustness', 'adaptive control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17899</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visually Prompted Benchmarks Are Surprisingly Fragile</title><link>https://arxiv.org/abs/2512.17875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that visually prompted benchmarks for VLMs are highly fragile: small changes in visual markers (color, size) or low-level inference choices (e.g., JPEG compression) can substantially alter model rankings.&lt;/li&gt;&lt;li&gt;Demonstrates these effects across nine open- and closed-source VLMs and two visually prompted tasks, and that weaker models can be made to outperform stronger ones via simple prompt variants.&lt;/li&gt;&lt;li&gt;Provides a mitigation by curating datasets into a larger benchmark (VPBench) with 16 visual marker variants and releases analysis tools to improve evaluation robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiwen Feng', 'Long Lian', 'Lisa Dunlap', 'Jiahao Shu', 'XuDong Wang', 'Renhao Wang', 'Trevor Darrell', 'Alane Suhr', 'Angjoo Kanazawa']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmarking', 'vision-language-models', 'visual-prompting', 'evaluation-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17875</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Translating the Rashomon Effect to Sequential Decision-Making Tasks</title><link>https://arxiv.org/abs/2512.17470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and demonstrates the Rashomon effect for sequential decision-making: multiple policies that behave identically (same state-action distributions) but differ internally.&lt;/li&gt;&lt;li&gt;Uses formal probabilistic verification to compare complete policy behaviors in stochastic environments, addressing challenges of trajectory-level randomness.&lt;/li&gt;&lt;li&gt;Shows that ensembles from the Rashomon set improve robustness to distribution shifts and that permissive policies derived from the set reduce verification costs while preserving performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Gross', 'J{\\o}rn Eirik Betten', 'Helge Spieker']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'formal verification', 'reinforcement learning', 'interpretability', 'distribution shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17470</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title><link>https://arxiv.org/abs/2512.17450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MULTIAQUA, a synchronized, calibrated, and annotated multimodal maritime dataset (RGB, thermal, IR, LiDAR, etc.) aimed at improving scene interpretation under poor visibility.&lt;/li&gt;&lt;li&gt;Evaluates several multimodal semantic segmentation methods on a challenging nighttime test set to demonstrate benefits of additional modalities.&lt;/li&gt;&lt;li&gt;Proposes training strategies to make multimodal models robust to near-complete darkness, including training using only daytime images to simplify data collection and annotation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jon Muhovi\\v{c}', 'Janez Per\\v{s}']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal dataset', 'robustness', 'semantic segmentation', 'maritime autonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17450</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Timely Information Updating for Mobile Devices Without and With ML Advice</title><link>https://arxiv.org/abs/2512.17381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes online scheduling of status updates from a mobile device to an access point, trading off timeliness versus update cost under adversarial uncertainties.&lt;/li&gt;&lt;li&gt;Proposes an online algorithm that asymptotically achieves optimal competitive ratio against an adversary manipulating operation duration, staleness, costs, and update opportunities.&lt;/li&gt;&lt;li&gt;Introduces an ML-augmented algorithm that incorporates potentially unreliable ML advice and achieves an optimal consistency–robustness trade-off even when the adversary can corrupt the ML advice.&lt;/li&gt;&lt;li&gt;Finds a threshold-like policy for ML advice (fully trust or ignore), and validates theoretical results via simulations in stochastic settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu-Pin Hsu', 'Yi-Hsuan Tseng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ML', 'robustness', 'ML-assisted-algorithms', 'online-algorithms', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17381</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</title><link>https://arxiv.org/abs/2512.17259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Verifiability-First architecture for autonomous LLM agents combining cryptographic/symbolic run-time attestations, lightweight continuous Audit Agents, and challenge-response protocols for high-risk actions.&lt;/li&gt;&lt;li&gt;Introduces OPERA, a benchmark and evaluation protocol measuring detectability of misalignment, time-to-detection under stealthy strategies, and resilience to adversarial prompt/persona injection.&lt;/li&gt;&lt;li&gt;Shifts evaluation focus from likelihood of misalignment to speed and reliability of detecting and remediating misaligned behavior, with emphasis on provable observability and practical auditing mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhivansh Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'agent safety', 'attestation and audit', 'prompt/persona injection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17259</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</title><link>https://arxiv.org/abs/2512.17254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ABBR, a practical framework that combines Byzantine-robust aggregation rules with privacy-preserving computation for federated learning.&lt;/li&gt;&lt;li&gt;Introduces the use of dimensionality reduction to accelerate private computations of complex filtering rules and analyzes accuracy loss from vector-wise filtering in low-dimensional space.&lt;/li&gt;&lt;li&gt;Presents an adaptive tuning strategy to mitigate the impact of malicious models that bypass filtering.&lt;/li&gt;&lt;li&gt;Implements ABBR with state-of-the-art aggregation rules and evaluates on public datasets, showing significant speedups, minimal communication overhead, and near-baseline Byzantine resilience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baolei Zhang', 'Minghong Fang', 'Zhuqing Liu', 'Biao Yi', 'Peizhao Zhou', 'Yuan Wang', 'Tong Li', 'Zheli Liu']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'byzantine-robustness', 'privacy-preserving', 'dimensionality-reduction', 'privacy-inference-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17254</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</title><link>https://arxiv.org/abs/2512.17251</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlignDP, a hybrid privacy mechanism that separates rare and non-rare fields to block knowledge transfer at the data interface for LLMs.&lt;/li&gt;&lt;li&gt;Rare fields are protected via PAC indistinguishability (effective zero-epsilon local DP), while non-rare fields are privatized using RAPPOR to provide unbiased frequency estimates under local DP.&lt;/li&gt;&lt;li&gt;Includes a global aggregator to enforce composition/budget, theoretical results (limits of PAC extension to aggregation, bounds for RAPPOR estimates) and utility trade-off analysis.&lt;/li&gt;&lt;li&gt;Toy simulation demonstrates that rare categories remain hidden and frequent categories are recovered with small error, indicating feasibility for preventing extraction/unauthorized fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'local_DP', 'RAPPOR', 'model_extraction_defense', 'privacy_preserving_ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17251</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title><link>https://arxiv.org/abs/2512.17213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies hallucination and unverifiable chain-of-thought risks in medical vision-language models and limitations of outcome-based RL methods like GRPO.&lt;/li&gt;&lt;li&gt;Proposes CheXPO-v2: a process-supervision alignment method using a Knowledge Graph Consistency Reward that parses reasoning into Disease–Relation–Anatomy triplets to penalize incoherent logic.&lt;/li&gt;&lt;li&gt;Adds hard-example mining and demonstrates state-of-the-art accuracy on MIMIC-CXR-VQA with high data efficiency (5k samples), producing more verifiable clinical reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Liang', 'Yuxuan An', 'Di Wang', 'Jiawei Hu', 'Zhicheng Jiao', 'Bin Jing', 'Quan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'medical VLMs', 'reward modeling', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17213</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</title><link>https://arxiv.org/abs/2512.17146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAGE, an agentic framework that audits adversarial vulnerabilities of genomic foundation models by injecting soft-prompt perturbations in embedding space.&lt;/li&gt;&lt;li&gt;Evaluates model robustness across checkpoints using metrics (AUROC, AUPR) and produces structured, LLM-generated narrative reports without modifying the underlying model.&lt;/li&gt;&lt;li&gt;Demonstrates that state-of-the-art GFMs (e.g., ESM2) are sensitive to targeted soft-prompt attacks, causing measurable degradation in variant effect prediction and revealing security risks for biomedical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huixin Zhan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'soft prompt attacks', 'robustness', 'red teaming', 'genomic foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17146</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Perturb Your Data: Paraphrase-Guided Training Data Watermarking</title><link>https://arxiv.org/abs/2512.17075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPECTRA: a paraphrase-guided watermarking scheme that paraphrases dataset examples with an LLM and selects paraphrases whose score (from a separate scoring model) closely matches the original to avoid distribution shift.&lt;/li&gt;&lt;li&gt;Detection is performed by comparing token probabilities from a suspect model against the scoring model to test whether watermarked data influenced training.&lt;/li&gt;&lt;li&gt;Claims very strong detectability (p-value gap over nine orders of magnitude) even when watermarked data comprise &lt;0.001% of the training corpus and that the watermark survives large-scale LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Shetty', 'Mirazul Haque', 'Petr Babkin', 'Zhiqiang Ma', 'Xiaomo Liu', 'Manuela Veloso']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'training-data-provenance', 'dataset-protection', 'model-auditing', 'paraphrase-guided-watermark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17075</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Women's Health Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2512.17028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Women's Health Benchmark (WHB), a 96-item benchmark evaluating LLM performance specifically on women's health across five specialties and three query types.&lt;/li&gt;&lt;li&gt;Categorizes failures into eight medical error types (e.g., dosage errors, missed urgency, inappropriate recommendations) and evaluates 13 state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;Finds high failure rates (~60%), large variation across specialties/error types, and specific safety concerns such as models missing urgency and giving inappropriate recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victoria-Elisabeth Gruber', 'Razvan Marinescu', 'Diego Fajardo', 'Amin H. Nassar', 'Christopher Arkfeld', 'Alexandria Ludlow', 'Shama Patel', 'Mehrnoosh Samaei', 'Valerie Klug', 'Anna Huber', 'Marcel G\\"uhner', 'Albert Botta i Orfila', 'Irene Lagoja', 'Kimya Tarr', 'Haleigh Larson', 'Mary Beth Howard']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'medical benchmark', 'LLM evaluation', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17028</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</title><link>https://arxiv.org/abs/2512.16962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemoryGraft, a novel attack that implants malicious successful experiences into an LLM agent's long-term RAG memory via benign ingestion-level artifacts.&lt;/li&gt;&lt;li&gt;Exploits the agent's semantic imitation heuristic and union retrieval (lexical + embedding similarity) to reliably surface poisoned procedure templates, causing persistent behavioral drift across sessions.&lt;/li&gt;&lt;li&gt;Empirically validated on MetaGPT's DataInterpreter with GPT-4o: a small number of poisoned records can dominate retrieved experiences on benign workloads. Code and data released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saksham Sahai Srivastava', 'Haoyu He']&lt;/li&gt;&lt;li&gt;Tags: ['memory poisoning', 'RAG poisoning', 'LLM agent attack', 'persistent compromise', 'adversarial data injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16962</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</title><link>https://arxiv.org/abs/2512.17586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates Safety Representations for Safer Policy Learning (SRPL): agents use a predictive model of future constraint violations to guide SafeRL.&lt;/li&gt;&lt;li&gt;Systematic experiments on Waymo Open Motion Dataset and NuPlan show improved reward–safety tradeoffs, with statistically significant gains in success rate and cost reduction (effect sizes r = 0.65–0.86 and r = 0.70–0.83, p &lt; 0.05).&lt;/li&gt;&lt;li&gt;Effectiveness depends on choice of policy optimizer and dataset distribution; SRPL improves robustness to observation noise and yields better zero-shot cross-dataset generalization compared to non-SRPL baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahesh Keswani', 'Raunak Bhattacharyya']&lt;/li&gt;&lt;li&gt;Tags: ['Safe reinforcement learning', 'Autonomous driving safety', 'Predictive safety representations', 'Robustness', 'Generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17586</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference</title><link>https://arxiv.org/abs/2512.17398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepShare: share the expensive DReLU operation across channels and layers by designating prototype channels that compute DReLU and replicate channels that copy those results, reducing number of cryptographic non-linear evaluations in Private Inference (PI).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing the formulation can represent an extended XOR-like function using a single nonlinearity and two neurons, which some prior PI-specific methods cannot.&lt;/li&gt;&lt;li&gt;Demonstrates empirical SOTA results on several classification tasks and image segmentation while substantially lowering DReLU counts, targeting efficiency gains for secure/cryptographic inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yonathan Bornfeld', 'Shai Avidan']&lt;/li&gt;&lt;li&gt;Tags: ['private-inference', 'privacy-preserving-ml', 'secure-computation', 'cryptographic-efficiency', 'model-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17398</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</title><link>https://arxiv.org/abs/2512.17375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a practical vulnerability in LLM-as-a-Judge and reward-model systems where short, low-perplexity 'control token' sequences flip binary judgments from correct 'No' to incorrect 'Yes'.&lt;/li&gt;&lt;li&gt;Introduces AdvJudge-Zero, a zero-shot method that uses next-token distributions and beam search to discover diverse, realistic control-token sequences that a policy could plausibly generate.&lt;/li&gt;&lt;li&gt;Analyzes mechanism: induced hidden-state perturbations concentrate in a low-rank 'soft mode' that is anti-aligned with the judge's refusal direction, explaining how small sequences produce systematic flips.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation showing high false-positive rates across large open-weight and specialized judge models on math/reasoning benchmarks, and demonstrates mitigation via LoRA-based adversarial training that reduces false positives while preserving evaluation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tung-Ling Li', 'Yuhao Wu', 'Hongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'reward model vulnerability', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17375</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</title><link>https://arxiv.org/abs/2512.17367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-SGA (Large Language Model-based Sample Generation and Aggregation) to generate adversarial examples and exploit invariances for improved generalizability of text detectors.&lt;/li&gt;&lt;li&gt;Introduces ARHOCD: an ensemble detector with a novel dynamic weight assignment (Bayesian-updated, domain-informed initialization) and an iterative adversarial training strategy optimizing both base detectors and the weight assignor.&lt;/li&gt;&lt;li&gt;Empirically evaluates on hate speech, rumor, and extremist-content datasets, reporting improved detection accuracy and robustness under textual adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yidong Chai', 'Yi Liu', 'Mohammadreza Ebrahimi', 'Weifeng Li', 'Balaji Padmanabhan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'textual adversarial attacks', 'adversarial training', 'ensemble methods', 'harmful content detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17367</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Generalization in Role-Playing Models via Information Theory</title><link>https://arxiv.org/abs/2512.17270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an information-theoretic metric (R-EMID) to quantify RPM performance degradation under distribution shifts (user, character, dialogue).&lt;/li&gt;&lt;li&gt;Derives an upper bound on R-EMID to predict worst-case generalization and isolates how different shifts contribute to performance loss.&lt;/li&gt;&lt;li&gt;Proposes a co-evolving reinforcement learning framework to better estimate dialogue response generation probabilities needed to compute R-EMID.&lt;/li&gt;&lt;li&gt;Empirically evaluates RPMs with R-EMID, finding user shift is the most harmful and RL-based adaptation most effective for improving generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongqi Li', 'Hao Lang', 'Fei Huang', 'Tieyun Qian', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution-shift', 'evaluation-metric', 'role-playing-models', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17270</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</title><link>https://arxiv.org/abs/2512.17121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates a CLIP-based medical vision-language model (Stanford AIMI CheXagent) on chest X-ray image retrieval using prompts with and without negation.&lt;/li&gt;&lt;li&gt;Fine-tunes the model with methods from prior work to improve handling of negated clinical phrases; reports improved negation handling with a slight drop in positive-prompt accuracy.&lt;/li&gt;&lt;li&gt;Analyzes internal behavior changes via token attribution, t-SNE projections, and attention-head ablation to characterize how fine-tuning reshapes text encoder representations of negation.&lt;/li&gt;&lt;li&gt;Aims to improve reliability of CLIP models in clinical settings by addressing failures in interpreting negated language.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jasmine Vu', 'Shivanand Sheshappanavar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'medical AI safety', 'interpretability', 'negation handling', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17121</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</title><link>https://arxiv.org/abs/2512.17079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains Qwen3-4B with RL (GRPO) on chain-of-thought (CoT) prefixes that contain exactly one controlled error (calculation or reasoning) to teach error detection and recovery.&lt;/li&gt;&lt;li&gt;Evaluates on competition-level MATH-lighteval problems: Mixed-CoT-RL matches standard RL on clean problems (41% vs 41%) and improves robustness on flawed prefills (24% vs 19%).&lt;/li&gt;&lt;li&gt;Finds that clean-only RL fine-tuning can reduce robustness below untuned baseline (19% vs 20%), while training on reasoning errors yields larger gains than calculation errors and mixed training performs best.&lt;/li&gt;&lt;li&gt;Concludes that exposure to flawed reasoning traces during training can improve error-recovery without degrading standard problem-solving accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saraswathy Amjith', 'Mihika Dusad', 'Neha Muramalla', 'Shweta Shah']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'chain-of-thought', 'RL fine-tuning', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17079</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</title><link>https://arxiv.org/abs/2512.15258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VLA-AN, a vision-language-action framework for onboard autonomous drone navigation that targets domain gap, temporal reasoning, safety, and deployment constraints.&lt;/li&gt;&lt;li&gt;Builds a high-fidelity dataset using 3D Gaussian Splatting and a progressive three-stage training pipeline for scene comprehension, core flight skills, and complex navigation.&lt;/li&gt;&lt;li&gt;Implements a lightweight, real-time action module with geometric safety correction to produce collision-free, stable commands and achieves an 8.3x inference throughput improvement on resource-constrained UAVs with high success rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuze Wu', 'Mo Zhu', 'Xingxing Li', 'Yuheng Du', 'Yuxin Fan', 'Wenjun Li', 'Zhichao Han', 'Xin Zhou', 'Fei Gao']&lt;/li&gt;&lt;li&gt;Tags: ['robotic safety', 'autonomous navigation', 'onboard deployment', 'vision-language', 'real-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15258</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies conformal prediction to transform embedding/NLI heuristic scores into finite-sample decision sets for hallucination detection in RAG systems (calibration n=600).&lt;/li&gt;&lt;li&gt;Empirically demonstrates a stark dichotomy: embedding methods succeed on synthetic hallucinations (95% coverage, 0% FPR) but fail catastrophically on real RLHF model hallucinations (100% FPR at target coverage) from HaluEval.&lt;/li&gt;&lt;li&gt;Analyzes failure as a distributional-tail problem where the hardest hallucinations are semantically indistinguishable from faithful outputs; NLI yields decent AUC (0.81) but cannot separate these hard cases, whereas GPT-4 reasoning reduces FPR substantially—coined the 'Semantic Illusion'.&lt;/li&gt;&lt;li&gt;Implication: surface-level embedding/NLI detectors are insufficient for safety-critical hallucination detection; reasoning-capable judges or alternative defenses are needed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'RAG', 'conformal-prediction', 'robustness', 'alignment-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals</title><link>https://arxiv.org/abs/2512.01037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'semantic confusion' — local inconsistency where a safety-aligned model refuses some paraphrases of an otherwise harmless intent but accepts others.&lt;/li&gt;&lt;li&gt;Releases ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that keep intent fixed while varying surface form.&lt;/li&gt;&lt;li&gt;Proposes three model-agnostic, token-level metrics (Confusion Index, Confusion Rate, Confusion Depth) using embeddings, next-token probabilities, and perplexity to quantify local inconsistency.&lt;/li&gt;&lt;li&gt;Evaluates diverse model families and deployment guards, showing global false-rejection rates hide important structure and presenting confusion-aware auditing to reduce false refusals while retaining safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riad Ahmed Anonto', 'Md Labid Al Nahiyan', 'Md Tanvir Hassan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'LLM-refusals', 'benchmarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01037</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Focus Memory for Language Models</title><link>https://arxiv.org/abs/2511.12712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Focus Memory (AFM), a context management system that assigns past messages one of three fidelity levels (Full, Compressed, Placeholder) to fit a fixed token budget while preserving important constraints.&lt;/li&gt;&lt;li&gt;AFM uses semantic relevance, temporal decay, and importance classification to allocate fidelity and packs messages chronologically to maintain critical information.&lt;/li&gt;&lt;li&gt;Evaluated on two multi-turn dialogue benchmarks stressing long-horizon constraint preservation (peanut allergy safety scenario and illegal tax evasion refusal), showing large improvements over baseline history strategies.&lt;/li&gt;&lt;li&gt;AFM requires no model weight changes or external retrieval infrastructure and is released as open-source compatible with OpenAI-style chat APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['dialogue memory', 'safety', 'alignment', 'robustness', 'context management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12712</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</title><link>https://arxiv.org/abs/2511.10008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Real-Sim-Real framework to simulate and validate physics-based sensor attack vectors against Vision-Language-Action (VLA) models.&lt;/li&gt;&lt;li&gt;Defines eight physical sensor attacks (six camera, two microphone) and performs large-scale evaluations across VLA architectures and tasks, revealing significant vulnerabilities and task/model-dependent susceptibility patterns.&lt;/li&gt;&lt;li&gt;Proposes an adversarial-training-based defense that improves robustness to out-of-distribution physical perturbations while preserving performance.&lt;/li&gt;&lt;li&gt;Calls for standardized robustness benchmarks and mitigation strategies for VLA deployments in safety-critical environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancun Lu', 'Jiaxiang Chen', 'Shilin Xiao', 'Zizhi Jin', 'Zhangrui Chen', 'Hanwen Yu', 'Bohan Qian', 'Ruochen Zhou', 'Xiaoyu Ji', 'Wenyuan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['physical sensor attacks', 'adversarial robustness', 'VLA models', 'adversarial training', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10008</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semi-Supervised Preference Optimization with Limited Feedback</title><link>https://arxiv.org/abs/2511.00040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semi-Supervised Preference Optimization (SSPO) to learn from a small set of pairwise preference labels plus a large pool of unpaired samples.&lt;/li&gt;&lt;li&gt;Proves existence of an optimal reward threshold that allows principled pseudo-labeling of unpaired responses into winners/losers.&lt;/li&gt;&lt;li&gt;Uses pseudo-labels to distill latent preferences from large unlabelled data, reducing need for costly human feedback while preserving alignment.&lt;/li&gt;&lt;li&gt;Empirical results show strong data efficiency (e.g., Mistral-7B-Instruct on 1% UltraFeedback outperforming baselines trained on 10%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seonggyun Lee', 'Sungjun Lim', 'Seojin Park', 'Soeun Cheon', 'Kyungwoo Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'semi-supervised learning', 'reward modeling', 'data efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00040</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeParts: a New Family of AI-Generated DeepFakes</title><link>https://arxiv.org/abs/2508.21052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'FakeParts', a new class of partial deepfakes consisting of subtle, localized spatial or temporal manipulations within otherwise authentic videos.&lt;/li&gt;&lt;li&gt;Presents FakePartsBench, a large-scale benchmark of ~81K videos (including 44K FakeParts) with pixel- and frame-level manipulation annotations to evaluate partial-manipulation detection.&lt;/li&gt;&lt;li&gt;User studies and evaluations show human detection accuracy drops up to 26% and state-of-the-art detectors also degrade significantly, exposing vulnerabilities in current detection methods.&lt;/li&gt;&lt;li&gt;Provides a resource to drive development of robust detection methods specifically targeting partial/localized manipulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Liu', 'Firas Gabetni', 'Awais Hussain Sani', 'Xi Wang', 'Soobash Daiboo', 'Gaetan Brison', 'Gianni Franchi', 'Vicky Kalogeiton']&lt;/li&gt;&lt;li&gt;Tags: ['deepfakes', 'dataset', 'detection-robustness', 'multimedia-forensics', 'partial-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21052</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title><link>https://arxiv.org/abs/2507.00724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses model ownership verification for personalized large vision models (LVMs) and the shortcomings of existing defenses for fine-tuned models.&lt;/li&gt;&lt;li&gt;Proposes creating shadow models that retain common features while disrupting dataset-specific features, then representing dataset-specific features via output differences between victim and shadow models.&lt;/li&gt;&lt;li&gt;Trains a meta-classifier to detect stolen models based on presence of dataset-specific features and uses hypothesis testing to improve robustness; reports extensive experiments showing effectiveness against various stealing types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linghui Zhu', 'Yiming Li', 'Haiqin Weng', 'Yan Liu', 'Tianwei Zhang', 'Shu-Tao Xia', 'Zhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model ownership verification', 'model stealing defense', 'watermarking/forensics', 'fine-tuned large vision models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00724</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models</title><link>https://arxiv.org/abs/2506.20915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZKPROV, a cryptographic framework that lets verifiers confirm an LLM's response was trained on authority-certified datasets without revealing training data or model internals.&lt;/li&gt;&lt;li&gt;Binds training datasets, model parameters, and generated responses and attaches zero-knowledge proofs to responses to validate dataset provenance and relevance to queries.&lt;/li&gt;&lt;li&gt;Reports sublinear scaling and end-to-end proof generation/verification overhead under 3.3 seconds for models up to 8B parameters, targeting practical deployment in regulated domains (e.g., healthcare).&lt;/li&gt;&lt;li&gt;Provides formal security guarantees proving dataset confidentiality and trustworthy provenance verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mina Namazi', 'Alexander Nemecek', 'Erman Ayday']&lt;/li&gt;&lt;li&gt;Tags: ['dataset provenance', 'zero-knowledge proofs', 'privacy-preserving verification', 'model auditing', 'training-data certification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20915</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LookAhead Tuning, a lightweight data-driven fine-tuning method that previews partial answer prefixes to reduce perturbations to initial token distributions.&lt;/li&gt;&lt;li&gt;Designed to preserve preexisting safety alignment during fine-tuning so models retain built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Empirical results reportedly show maintained safety without degrading downstream task performance or robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'alignment/safety', 'robustness', 'safety-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs</title><link>https://arxiv.org/abs/2502.01436</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a fully automated, black-box method to evaluate marketplace Custom GPTs for compliance with platform usage policies using large-scale discovery, policy-driven red-teaming prompts, and an LLM-as-judge.&lt;/li&gt;&lt;li&gt;Validates the automated compliance assessor against a human-annotated ground truth (binary violation detection F1 = 0.975) and applies it to 782 Custom GPTs from the GPT Store.&lt;/li&gt;&lt;li&gt;Finds 58.7% of evaluated GPTs produce at least one policy-violating response and shows customization usually amplifies model-level failure modes rather than creating new ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Rodriguez', 'William Seymour', 'Jose M. Del Alamo', 'Jose Such']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'policy compliance evaluation', 'automated testing', 'marketplace moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01436</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</title><link>https://arxiv.org/abs/2409.03735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'privacy bias' as the appropriateness of information flows in LLM responses and introduces 'privacy bias delta' to quantify deviations from expected privacy norms.&lt;/li&gt;&lt;li&gt;Proposes a contextual integrity–based auditing metric to assess privacy biases in LLM outputs, accounting for sensitivity across prompt variations.&lt;/li&gt;&lt;li&gt;Applies the metric to compare different LLMs and analyzes how model capacity and optimization choices affect privacy biases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yan Shvartzshnaider', 'Vasisht Duddu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy_audit', 'contextual_integrity', 'LLM_evaluation', 'privacy_bias', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.03735</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research</title><link>https://arxiv.org/abs/2511.15282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues AI research rests on two implicit conceptions of intelligence: Intelligence Realism (single universal capacity) and Intelligence Pluralism (diverse, context-dependent capacities).&lt;/li&gt;&lt;li&gt;Shows how these conceptions shape methodology (model selection, benchmark design, validation) and interpretation of empirical results (e.g., emergent capabilities vs. limitations).&lt;/li&gt;&lt;li&gt;Discusses implications for AI risk and alignment: realists favor unified superintelligence-focused alignment solutions, while pluralists favor varied, domain-specific threat responses.&lt;/li&gt;&lt;li&gt;Advocates making underlying assumptions explicit to clarify disagreements and guide research and policy choices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ninell Oldenburg', 'Ruchira Dhar', 'Anders S{\\o}gaard']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'AI risk', 'research methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15282</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Develop Gambling Addiction?</title><link>https://arxiv.org/abs/2509.22818</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical slot-machine experiments showing LLMs can exhibit human-like gambling behaviors (illusion of control, loss chasing) under specific conditions.&lt;/li&gt;&lt;li&gt;Greater autonomy over betting parameters increases irrational actions and bankruptcy rates, indicating behavior amplification with autonomy.&lt;/li&gt;&lt;li&gt;Neural circuit analysis using a Sparse Autoencoder suggests internal abstract decision-making features (risk-related) drive behavior beyond prompt content.&lt;/li&gt;&lt;li&gt;Authors argue LLMs internalize cognitive biases, with implications for safety, alignment, and evaluation of emergent maladaptive behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seungpil Lee', 'Donghyeon Shin', 'Yunjeong Lee', 'Sundong Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'model-behavior', 'interpretability', 'cognitive-biases']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22818</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title><link>https://arxiv.org/abs/2508.10501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PASS (Probabilistic Agentic Supernet Sampling), a multimodal agentic framework for Chest X‑Ray reasoning that samples adaptive tool workflows and outputs probability-annotated decision trajectories for auditability.&lt;/li&gt;&lt;li&gt;PASS maintains an evolving personalized memory, supports early-exit for efficiency, and selects tools via a learned task-conditioned distribution over a multi-tool supernet.&lt;/li&gt;&lt;li&gt;Presents a three-stage training pipeline (expert warm-up, contrastive path-ranking, cost-aware RL) and a new benchmark CAB-E for multi-step, safety-critical CXR reasoning.&lt;/li&gt;&lt;li&gt;Claims improved accuracy, semantic alignment, and cost-performance tradeoffs versus baselines, emphasizing interpretable outputs to enhance medical AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Feng', 'Junye Du', 'Yingying Hong', 'Qifan Wang', 'Lequan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['medical-ai', 'interpretability', 'safety', 'agentic-systems', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10501</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</title><link>https://arxiv.org/abs/2506.11023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents OntoGSN: an OWL+SWRL ontology and middleware that provides a 1:1 formalization of the GSN v3 assurance-case standard and a queryable knowledge graph for automated population, evaluation, and updating of assurance cases.&lt;/li&gt;&lt;li&gt;Provides integration helpers (parser, helper ontology), a SPARQL query library with automation patterns, documentation of design decisions, and a prototype interface for managing ACs.&lt;/li&gt;&lt;li&gt;Evaluated via FAIR/OOPS/competency questions and community feedback; demonstrates utility with an example on assurance of adversarial robustness in large language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomas Bueno Momcilovic', 'Barbara Gallina', 'Ingmar Kessler', 'Jule Hendricks', 'Dian Balta']&lt;/li&gt;&lt;li&gt;Tags: ['assurance-cases', 'safety-assurance', 'ontology', 'robustness', 'standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11023</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness of Vision in Open Foundation Models</title><link>https://arxiv.org/abs/2512.17902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates adversarial robustness of LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 using untargeted PGD attacks applied to the visual input.&lt;/li&gt;&lt;li&gt;Empirical experiments on a VQA v2 subset measure accuracy degradation; Llama 3.2 Vision shows smaller performance drop under attack despite a lower baseline accuracy.&lt;/li&gt;&lt;li&gt;Concludes that the vision modality is a viable attack vector for degrading open-weight VLMs and that robustness does not necessarily correlate with standard benchmark performance—likely influenced by architecture and training choices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathon Fox', 'William J Buchanan', 'Pavlos Papadopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'visual adversarial attacks', 'vision-language models', 'PGD', 'VQA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17902</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Digital and Web Forensics Model Cards, V1</title><link>https://arxiv.org/abs/2512.17722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a standardized model card framework tailored to digital and web forensics, including controlled vocabularies for classification, reasoning types, bias identification, and error categorization.&lt;/li&gt;&lt;li&gt;Introduces a web-based generator tool (beta) to produce these forensic model cards and invites community feedback to refine the standard.&lt;/li&gt;&lt;li&gt;Aims to improve transparency, risk communication, and documentation of forensic models and processes to mitigate systemic risks in anti-fraud and forensic workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paola Di Maio']&lt;/li&gt;&lt;li&gt;Tags: ['model cards', 'model documentation', 'digital forensics', 'transparency', 'bias identification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17722</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting</title><link>https://arxiv.org/abs/2512.17667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes website fingerprinting (WF) as a zero-shot cross-modal retrieval problem, mapping encrypted traffic traces to crawl-time website logic profiles via a dual-encoder.&lt;/li&gt;&lt;li&gt;Trains on 150K traffic–logic pairs with contrastive and consistency objectives and structure-aware augmentations to learn a joint embedding space without target-site traffic.&lt;/li&gt;&lt;li&gt;Reports strong zero-shot performance on 1,600 unseen sites (87.9% top-1, 0.963 AUC) and further improvement with few-shot adapters; analyzes semantic leakage as a dominant privacy risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifei Cheng', 'Yujia Zhu', 'Baiyang Li', 'Xinhao Deng', 'Yitong Cai', 'Yaochen Ren', 'Qingyun Liu']&lt;/li&gt;&lt;li&gt;Tags: ['website-fingerprinting', 'privacy-attack', 'contrastive-learning', 'zero-shot-retrieval', 'encrypted-traffic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17667</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title><link>https://arxiv.org/abs/2512.17532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Robust-R1, a framework that explicitly models visual degradations via structured reasoning chains (supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling).&lt;/li&gt;&lt;li&gt;Introduces an 11K dataset with realistic degradations annotated with degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion to supervise degradation-aware reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness over general and robust baselines on R-Bench and stronger anti-degradation performance under multi-intensity/adversarial degradations on MMMB, MMStar, and RealWorldQA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Tang', 'Jianmin Chen', 'Wei Wei', 'Xiaogang Xu', 'Runtao Liu', 'Xiangyu Wu', 'Qipeng Xie', 'Jiafei Wu', 'Lei Zhang', 'Qifeng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'multimodal-llms', 'visual-degradations', 'dataset', 'adversarial-degradations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17532</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models</title><link>https://arxiv.org/abs/2512.17519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces K-OTG, a PEFT-compatible secret-key access-control mechanism that applies an orthonormal transform to LM hidden states before the lm_head so the correct key restores the model basis, while incorrect/no key triggers a session-ephemeral scrambler and forced BLOCK outputs.&lt;/li&gt;&lt;li&gt;Design is LoRA-compatible (works with 4-bit bases) and does not add special tokens; training uses dual-path data (authorized examples vs. unauthorized mapped to BLOCK).&lt;/li&gt;&lt;li&gt;Empirical evaluation on 1–3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) shows preserved authorized utility, collapse of unauthorized utility (exploding PPL, near-zero sequence metrics), low cross-unlock, nonce invariance, and practical runtime overhead (~40% token/sec).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Haris Khan']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'model-security', 'hidden-state-manipulation', 'PEFT/LoRA-compatibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17519</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</title><link>https://arxiv.org/abs/2512.17370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the open-loop vs closed-loop misalignment in end-to-end autonomous driving that leads to expert takeovers and disengagements.&lt;/li&gt;&lt;li&gt;Proposes TakeAD, a preference-based post-optimization pipeline that first uses iterative DAgger to learn from expert takeover interventions and then applies Direct Preference Optimization (DPO) to align policy behavior with expert preferences in disengagement scenarios.&lt;/li&gt;&lt;li&gt;Designs an expert takeover data collection pipeline and iteratively fine-tunes a pretrained IL policy to learn recovery strategies for disengagement states.&lt;/li&gt;&lt;li&gt;Demonstrates improved closed-loop driving performance on the Bench2Drive benchmark with ablations validating each component's contribution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deqing Liu', 'Yinfeng Gao', 'Deheng Qian', 'Qichao Zhang', 'Xiaoqing Ye', 'Junyu Han', 'Yupeng Zheng', 'Xueyi Liu', 'Zhongpu Xia', 'Dawei Ding', 'Yifeng Pan', 'Dongbin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous driving', 'Imitation learning', 'Preference-based learning', 'Safety / robustness', 'Closed-loop evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17370</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track</title><link>https://arxiv.org/abs/2512.17293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tunes the open-weight Supertonic TTS model using Self-Purifying Flow Matching (SPFM) to adapt to in-the-wild speech.&lt;/li&gt;&lt;li&gt;SPFM detects suspicious text–speech pairs by comparing conditional and unconditional flow-matching losses and routes them to unconditional training to mitigate label noise.&lt;/li&gt;&lt;li&gt;Empirical results: lowest WER in the WildSpoof TTS track and second place on perceptual metrics (UTMOS, DNSMOS), demonstrating robust adaptation to diverse real-world speech.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['June Young Yi', 'Hyeongju Kim', 'Juheon Lee']&lt;/li&gt;&lt;li&gt;Tags: ['TTS', 'Robustness', 'Label-noise mitigation', 'Flow-matching', 'Voice spoofing / WildSpoof']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17293</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Generalization in Role-Playing Models via Information Theory</title><link>https://arxiv.org/abs/2512.17270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an information-theoretic metric (R-EMID) to quantify RPM performance degradation under distribution shifts (user, character, dialogue).&lt;/li&gt;&lt;li&gt;Derives an upper bound on R-EMID to predict worst-case generalization and isolates how different shifts contribute to performance loss.&lt;/li&gt;&lt;li&gt;Proposes a co-evolving reinforcement learning framework to better estimate dialogue response generation probabilities needed to compute R-EMID.&lt;/li&gt;&lt;li&gt;Empirically evaluates RPMs with R-EMID, finding user shift is the most harmful and RL-based adaptation most effective for improving generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongqi Li', 'Hao Lang', 'Fei Huang', 'Tieyun Qian', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution-shift', 'evaluation-metric', 'role-playing-models', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17270</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</title><link>https://arxiv.org/abs/2512.17259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Verifiability-First architecture for autonomous LLM agents combining cryptographic/symbolic run-time attestations, lightweight continuous Audit Agents, and challenge-response protocols for high-risk actions.&lt;/li&gt;&lt;li&gt;Introduces OPERA, a benchmark and evaluation protocol measuring detectability of misalignment, time-to-detection under stealthy strategies, and resilience to adversarial prompt/persona injection.&lt;/li&gt;&lt;li&gt;Shifts evaluation focus from likelihood of misalignment to speed and reliability of detecting and remediating misaligned behavior, with emphasis on provable observability and practical auditing mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhivansh Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'agent safety', 'attestation and audit', 'prompt/persona injection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17259</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</title><link>https://arxiv.org/abs/2512.17251</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlignDP, a hybrid privacy mechanism that separates rare and non-rare fields to block knowledge transfer at the data interface for LLMs.&lt;/li&gt;&lt;li&gt;Rare fields are protected via PAC indistinguishability (effective zero-epsilon local DP), while non-rare fields are privatized using RAPPOR to provide unbiased frequency estimates under local DP.&lt;/li&gt;&lt;li&gt;Includes a global aggregator to enforce composition/budget, theoretical results (limits of PAC extension to aggregation, bounds for RAPPOR estimates) and utility trade-off analysis.&lt;/li&gt;&lt;li&gt;Toy simulation demonstrates that rare categories remain hidden and frequent categories are recovered with small error, indicating feasibility for preventing extraction/unauthorized fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'local_DP', 'RAPPOR', 'model_extraction_defense', 'privacy_preserving_ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17251</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes</title><link>https://arxiv.org/abs/2512.17218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic literature review (PRISMA) proposing an Islamic ethical framework (Maqasid al-Shariah) to prevent misuse of AI-driven deepfakes.&lt;/li&gt;&lt;li&gt;Identifies ethical deficiencies and regulatory gaps, recommending legal recognition of intangible/psychological harms, moral scrutiny in technology governance, and digital literacy based on tabayyun (verification).&lt;/li&gt;&lt;li&gt;Argues for preventative, dignity-focused approaches (protecting honor and self) rather than purely punitive technical or legal measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wisnu Uriawan', 'Imany Fauzy Rahman', 'Muhamad Zidan', 'Irma Rohmatillah', 'Muhammad Arkan Raihan', 'Irma Dwiyanti']&lt;/li&gt;&lt;li&gt;Tags: ['deepfakes', 'ethics', 'policy', 'harm-prevention', 'digital-literacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17218</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors</title><link>https://arxiv.org/abs/2512.17180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Experiments in interactive reinforcement learning (navigation tasks) show agents overwhelmingly prefer low-reward (conservative) teachers over high-reward teachers (93.16% selection rate).&lt;/li&gt;&lt;li&gt;Identifies critical thresholds for teacher availability (rho) and accuracy (omega) at &gt;= 0.6, below which learning fails catastrophically, and reports 159% improvement over baseline Q-learning under concept drift.&lt;/li&gt;&lt;li&gt;Authors discuss implications for human-robot collaboration and safety-critical training, suggesting agent preference for consistency may align with human safety preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maher Mesto', 'Francisco Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning', 'interactive learning', 'robotic safety', 'robustness', 'human-robot interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17180</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</title><link>https://arxiv.org/abs/2512.17079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains Qwen3-4B with RL (GRPO) on chain-of-thought (CoT) prefixes that contain exactly one controlled error (calculation or reasoning) to teach error detection and recovery.&lt;/li&gt;&lt;li&gt;Evaluates on competition-level MATH-lighteval problems: Mixed-CoT-RL matches standard RL on clean problems (41% vs 41%) and improves robustness on flawed prefills (24% vs 19%).&lt;/li&gt;&lt;li&gt;Finds that clean-only RL fine-tuning can reduce robustness below untuned baseline (19% vs 20%), while training on reasoning errors yields larger gains than calculation errors and mixed training performs best.&lt;/li&gt;&lt;li&gt;Concludes that exposure to flawed reasoning traces during training can improve error-recovery without degrading standard problem-solving accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saraswathy Amjith', 'Mihika Dusad', 'Neha Muramalla', 'Shweta Shah']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'chain-of-thought', 'RL fine-tuning', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17079</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution</title><link>https://arxiv.org/abs/2512.17067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Longitudinal study of 2,615 promotional Twitter bot accounts (2.8M tweets) building yearly time series for 10 content-based meta-features and showing all features are non-stationary.&lt;/li&gt;&lt;li&gt;Stratification by activation generation and account age reveals systematic behavioural differences (e.g., second-generation bots are more active/link-heavy; short-lived bots are repetitive and hashtag/URL-heavy).&lt;/li&gt;&lt;li&gt;Co-occurrence analysis of 18 binary features (153 pairs) finds widespread dependence and shifting correlations over time, including polarity reversals and strengthening of certain cue combinations.&lt;/li&gt;&lt;li&gt;Conclusion: promotional bots adapt at both individual-feature and feature-structure levels, implying bot-detection systems trained on historical features may degrade due to concept drift and evolving adversarial strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ohoud Alzahrani', 'Russell Beale', 'Bob Hendley']&lt;/li&gt;&lt;li&gt;Tags: ['social-bots', 'concept-drift', 'adversarial-adaptation', 'bot-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17067</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation</title><link>https://arxiv.org/abs/2512.17029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adversarial-VR, an open-source Unity-based real-time testbed to evaluate adversarial robustness of DL-based VR cybersickness detection and mitigation.&lt;/li&gt;&lt;li&gt;Integrates two SOTA models (DeepTCN, Transformer) trained on the MazeSick dataset and a dynamic visual tunneling mitigation mechanism that adjusts FOV based on model outputs.&lt;/li&gt;&lt;li&gt;Implements and evaluates three adversarial attacks (MI-FGSM, PGD, C&amp;W) on a custom VR maze with an HTC Vive Pro Eye, showing large drops in detection accuracy (e.g., 5.94x decrease with C&amp;W).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Istiak Ahmed', 'Ripan Kumar Kundu', 'Khaza Anuarul Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness testing', 'red teaming/testbed', 'cybersickness detection', 'real-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17029</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Women's Health Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2512.17028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Women's Health Benchmark (WHB), a 96-item benchmark evaluating LLM performance specifically on women's health across five specialties and three query types.&lt;/li&gt;&lt;li&gt;Categorizes failures into eight medical error types (e.g., dosage errors, missed urgency, inappropriate recommendations) and evaluates 13 state-of-the-art LLMs.&lt;/li&gt;&lt;li&gt;Finds high failure rates (~60%), large variation across specialties/error types, and specific safety concerns such as models missing urgency and giving inappropriate recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victoria-Elisabeth Gruber', 'Razvan Marinescu', 'Diego Fajardo', 'Amin H. Nassar', 'Christopher Arkfeld', 'Alexandria Ludlow', 'Shama Patel', 'Mehrnoosh Samaei', 'Valerie Klug', 'Anna Huber', 'Marcel G\\"uhner', 'Albert Botta i Orfila', 'Irene Lagoja', 'Kimya Tarr', 'Haleigh Larson', 'Mary Beth Howard']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'medical benchmark', 'LLM evaluation', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17028</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</title><link>https://arxiv.org/abs/2512.16962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemoryGraft, a novel attack that implants malicious successful experiences into an LLM agent's long-term RAG memory via benign ingestion-level artifacts.&lt;/li&gt;&lt;li&gt;Exploits the agent's semantic imitation heuristic and union retrieval (lexical + embedding similarity) to reliably surface poisoned procedure templates, causing persistent behavioral drift across sessions.&lt;/li&gt;&lt;li&gt;Empirically validated on MetaGPT's DataInterpreter with GPT-4o: a small number of poisoned records can dominate retrieved experiences on benign workloads. Code and data released for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saksham Sahai Srivastava', 'Haoyu He']&lt;/li&gt;&lt;li&gt;Tags: ['memory poisoning', 'RAG poisoning', 'LLM agent attack', 'persistent compromise', 'adversarial data injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16962</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Translating the Rashomon Effect to Sequential Decision-Making Tasks</title><link>https://arxiv.org/abs/2512.17470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and demonstrates the Rashomon effect for sequential decision-making: multiple policies that behave identically (same state-action distributions) but differ internally.&lt;/li&gt;&lt;li&gt;Uses formal probabilistic verification to compare complete policy behaviors in stochastic environments, addressing challenges of trajectory-level randomness.&lt;/li&gt;&lt;li&gt;Shows that ensembles from the Rashomon set improve robustness to distribution shifts and that permissive policies derived from the set reduce verification costs while preserving performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Gross', 'J{\\o}rn Eirik Betten', 'Helge Spieker']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'formal verification', 'reinforcement learning', 'interpretability', 'distribution shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17470</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats</title><link>https://arxiv.org/abs/2512.17041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a role-based architecture for Agentic Vehicles (Personal Agent + Driving Strategy Agent) and systematically analyzes security threats to the agentic layer and cross-layer interactions (perception, communication, control).&lt;/li&gt;&lt;li&gt;Extends OWASP-style reasoning-driven AI risk analysis to safety-critical vehicle contexts, identifying how upstream layer compromises can induce misaligned or unsafe agentic behavior.&lt;/li&gt;&lt;li&gt;Provides a severity matrix and attack-chain analysis showing how small perturbations or distortions can escalate into safety-critical failures in both human-driven and autonomous vehicles.&lt;/li&gt;&lt;li&gt;Presents a structured framework intended as a foundational tool for analyzing and prioritizing security risks of agentic AI in current and emerging vehicle platforms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Eslami', 'Jiangbo Yu']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'vehicle security', 'cross-layer attacks', 'safety-critical systems', 'threat modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17041</guid><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
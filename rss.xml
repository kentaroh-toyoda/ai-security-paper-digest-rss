<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 12 Jan 2026 23:15:22 +0000</lastBuildDate><item><title>Video Generation Models Are Good Latent Reward Models</title><link>https://arxiv.org/abs/2511.21541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Process Reward Feedback Learning (PRFL), a framework that performs reward optimization for video generation entirely in the noisy latent space.&lt;/li&gt;&lt;li&gt;Leverages pre-trained video generation models' ability to process noisy latents and temporal structure to enable gradient propagation across the full denoising chain without costly VAE decoding.&lt;/li&gt;&lt;li&gt;Shows PRFL improves alignment with human preferences while reducing memory usage and training time compared to pixel-space (RGB) ReFL approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyue Mi', 'Wenqing Yu', 'Jiesong Lian', 'Shibo Jie', 'Ruizhe Zhong', 'Zijun Liu', 'Guozhen Zhang', 'Zixiang Zhou', 'Zhiyong Xu', 'Yuan Zhou', 'Qinglin Lu', 'Fan Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'video generation', 'latent-space optimization', 'training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21541</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution</title><link>https://arxiv.org/abs/2507.14367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Hallucination Score (HS) for generative image super-resolution (GSR) by prompting multimodal large language models (MLLMs) to detect hallucinatory visual elements.&lt;/li&gt;&lt;li&gt;Shows HS correlates well with human judgments and provides complementary information to existing fidelity and no-reference quality metrics.&lt;/li&gt;&lt;li&gt;Proposes efficient differentiable HS proxies and uses them as reward functions to fine-tune diffusion-based GSR models to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates mitigation of perceptual hallucination artifacts while maintaining perceptual quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiming Ren', 'Raghav Goyal', 'Zhiming Hu', 'Tristan Ty Aumentado-Armstrong', 'Iqbal Mohomed', 'Alex Levinshtein']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'image-super-resolution', 'evaluation-metrics', 'multimodal-LLM', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14367</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach</title><link>https://arxiv.org/abs/2501.01042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transferability of adversarial video examples across video-based multimodal LLMs (V-MLLMs) and identifies limitations of existing attacks in black-box settings.&lt;/li&gt;&lt;li&gt;Proposes I2V-MLLM: an Image-to-Video attack that uses an image-based MLLM surrogate to craft adversarial videos by integrating multimodal interactions and spatiotemporal perturbations.&lt;/li&gt;&lt;li&gt;Introduces a perturbation propagation technique to handle unknown frame sampling strategies and improves cross-model transferability.&lt;/li&gt;&lt;li&gt;Demonstrates strong black-box transferability on video-text tasks (Zero-Shot VideoQA), achieving ~58% AASR on MSVD-QA and MSRVTT-QA using BLIP-2 as surrogate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linhao Huang', 'Xue Jiang', 'Zhiqiang Wang', 'Wentao Mo', 'Xi Xiao', 'Yong-Jie Yin', 'Bo Han', 'Feng Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'black-box attacks', 'multimodal LLMs', 'video QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01042</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection</title><link>https://arxiv.org/abs/2410.12278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a two-step generation-selection pipeline to create task-specific synthetic datasets for hallucination detection, using hallucination pattern guidance and language style alignment.&lt;/li&gt;&lt;li&gt;Introduces a data mixture training strategy to obtain robust supervised detectors with improved generalization across tasks and text generators.&lt;/li&gt;&lt;li&gt;Reports detectors trained on synthetic data that outperform in-context-learning (ICL) detectors by ~32% and demonstrate cross-task and cross-generator generalization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Xie', 'Karan Aggarwal', 'Aitzaz Ahmad', 'Stephen Lau']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'alignment/safety', 'synthetic data generation', 'robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.12278</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility</title><link>https://arxiv.org/abs/2601.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PII-VisBench, a benchmark of 4,000 probes across 200 subjects stratified into four visibility levels (high/medium/low/zero) to assess PII leakage in vision-language models.&lt;/li&gt;&lt;li&gt;Evaluates 18 open-source VLMs (0.3B–32B) using Refusal Rate and Conditional PII Disclosure Rate, finding disclosures correlate with subject visibility and vary by model family and PII type.&lt;/li&gt;&lt;li&gt;Demonstrates paraphrasing and jailbreak-style prompts can bypass refusals, highlighting attack vectors and motivating visibility-aware safety evaluation and training interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G M Shahariar', 'Zabir Al Nazi', 'Md Olid Hasan Bhuiyan', 'Zhouxing Shi']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'VLM safety', 'Benchmarking', 'Jailbreaking', 'Privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05739</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints</title><link>https://arxiv.org/abs/2601.05986</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends the DUMB benchmarking methodology to evaluate adversarial robustness of deepfake detectors under transferability and cross-dataset constraints.&lt;/li&gt;&lt;li&gt;Evaluates five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL) against three attacks (PGD, FGSM, FPBA) on FaceForensics++ and Celeb-DF-V2.&lt;/li&gt;&lt;li&gt;Finds adversarial training improves in-distribution robustness but can degrade performance in cross-dataset scenarios depending on the strategy, highlighting need for case-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrian Serrano', 'Erwan Umlil', 'Ronan Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'deepfake detection', 'adversarial training', 'transferability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05986</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Context-Aware Decoding for Faithful Vision-Language Generation</title><link>https://arxiv.org/abs/2601.05939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes layer-wise generation dynamics in LVLM decoders using the Logit Lens and identifies a 'commitment-depth gap' where truthful tokens concentrate probability earlier than hallucinated tokens.&lt;/li&gt;&lt;li&gt;Proposes Context Embedding Injection (CEI), a training-free method that injects the last input token's hidden state as a grounding signal during decoding to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Empirically evaluates CEI (and a dynamic variant) on CHAIR, AMBER, and MMHal-Bench across multiple LVLMs and reports lower hallucination rates compared to state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mehrdad Fazli', 'Bowen Wei', 'Ziwei Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'mechanistic interpretability', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05939</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection</title><link>https://arxiv.org/abs/2601.05861</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Phase4DFD, a phase-aware frequency-domain deepfake detection framework that models phase–magnitude interactions via a learnable attention module.&lt;/li&gt;&lt;li&gt;Augments RGB input with FFT magnitude and LBP, uses an input-level phase-aware attention to highlight phase discontinuities before processing with a BNext M backbone and optional channel-spatial attention.&lt;/li&gt;&lt;li&gt;Reports SOTA performance on CIFAKE and DFFD with low computational overhead; ablations show explicit phase modeling provides complementary information to magnitude-only approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhen-Xin Lin', 'Shang-Kuan Chen']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'frequency-domain', 'phase-aware attention', 'media forensics', 'image/video forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05861</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes</title><link>https://arxiv.org/abs/2601.05600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SceneAlign, which leverages scene graphs to provide structured visual information for grounding multimodal reasoning.&lt;/li&gt;&lt;li&gt;Identifies reasoning-critical nodes and applies four targeted perturbation strategies to generate linguistically plausible but visually inaccurate (hard negative) rationales.&lt;/li&gt;&lt;li&gt;Uses contrastive pairs in Direct Preference Optimization to train models toward fine-grained, structure-faithful reasoning and reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in answer accuracy and reasoning faithfulness across seven visual reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuhan Wang', 'Xintong Li', 'Jennifer Yuntong Zhang', 'Junda Wu', 'Chengkai Huang', 'Lina Yao', 'Julian McAuley', 'Jingbo Shang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'visual grounding', 'hallucination mitigation', 'contrastive learning', 'preference optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05600</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection</title><link>https://arxiv.org/abs/2601.05580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a three-stage continual learning framework to adapt AI-generated image detectors to evolving generative models: (1) parameter-efficient fine-tuning for a transferable offline detector, (2) continual learning with a progressive data augmentation chain and K-FAC to reduce overfitting and catastrophic forgetting, and (3) linear interpolation via Linear Mode Connectivity to capture commonalities across generators.&lt;/li&gt;&lt;li&gt;Establishes a chronological benchmark of 27 generative models (GANs, deepfakes, diffusion models) up to Aug 2024 to simulate real-world model evolution.&lt;/li&gt;&lt;li&gt;Reports improved performance: offline detectors +5.51% mAP over baseline and continual learning strategy achieving 92.20% average accuracy, outperforming state-of-the-art methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanyi Wang', 'Jun Lan', 'Yaoyu Kang', 'Huijia Zhu', 'Weiqiang Wang', 'Zhuosheng Zhang', 'Shilin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'continual learning', 'robustness', 'AI-generated content detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05580</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews</title><link>https://arxiv.org/abs/2601.05563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines omission-based misleadingness in multimodal news previews and constructs the MM-Misleading benchmark to systematically study it.&lt;/li&gt;&lt;li&gt;Evaluates open-source LVLMs, revealing significant blind spots in detecting misleading omissions compared to much larger models.&lt;/li&gt;&lt;li&gt;Proposes OMGuard (Interpretation-Aware Fine-Tuning + Rationale-Guided Misleading Content Correction) to improve detection and rewrite headlines, boosting an 8B model to parity with a 235B LVLM on detection and improving end-to-end correction.&lt;/li&gt;&lt;li&gt;Analyzes causes of misleadingness (mostly local narrative shifts) and highlights cases where visual interventions are necessary beyond text-only corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanxiao Li', 'Jiaying Wu', 'Tingchao Fu', 'Dayang Li', 'Herun Wan', 'Wei Zhou', 'Min-Yen Kan']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-misinformation', 'model-safety', 'safety-evaluation', 'alignment', 'benchmark-dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05563</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck</title><link>https://arxiv.org/abs/2601.05547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VIB-Probe, a Variational Information Bottleneck-based probe to detect hallucinations in Vision-Language Models by extracting discriminative patterns from internal attention heads.&lt;/li&gt;&lt;li&gt;Uses the probe's gradients to identify attention heads with causal influence on hallucinations and applies inference-time interventions to mitigate them.&lt;/li&gt;&lt;li&gt;Claims superior performance over existing baselines on multiple hallucination detection and mitigation benchmarks; focuses on internal model signals rather than only outputs or external verifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiran Zhang', 'Yixin Wu', 'Zhenghua Wang', 'Xiaohua Wang', 'Changze Lv', 'Xuanjing Huang', 'Xiaoqing Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'vision-language models', 'information bottleneck', 'model interpretability', 'inference-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05547</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting</title><link>https://arxiv.org/abs/2601.05511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GaussianSwap, a video face-swapping method that builds an animatable 3D face avatar using 3D Gaussian Splatting rigged to FLAME parameters across frames.&lt;/li&gt;&lt;li&gt;Transfers identity from a source image via a compound identity embedding (from three face recognition models) and finetunes the avatar for identity preservation.&lt;/li&gt;&lt;li&gt;Renders the swapped avatar back onto background frames producing temporally consistent, high-fidelity, and interactively controllable swapped videos.&lt;/li&gt;&lt;li&gt;Claims improvements in identity preservation, visual clarity, temporal consistency, and enables novel interactive/animation use cases beyond pixel-based swaps.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Cheng', 'Jiahao Rao', 'Chengyang Li', 'Wenhao Wang', 'Weilin Chen', 'Lvqing Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfakes', 'face-swapping', 'avatar-generation', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05511</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PromptScreen: Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptScreen, a lightweight multi-stage defense pipeline against prompt injection and jailbreak attacks for LLM-based systems.&lt;/li&gt;&lt;li&gt;Core module is a semantic filter using text normalization, TF-IDF representations, and a Linear SVM, achieving 93.4% accuracy and 96.5% specificity on held-out data.&lt;/li&gt;&lt;li&gt;Pipeline claims large reductions in attack throughput and latency versus model-based moderators (improves accuracy from 35.1% to 93.4% and reduces time-to-completion from ~450s to ~47s relative to ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluation performed on a curated corpus of &gt;30,000 labeled prompts covering benign, jailbreak, and application-layer injections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM defenses', 'red teaming', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection</title><link>https://arxiv.org/abs/2410.12278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a two-step generation-selection pipeline to create task-specific synthetic datasets for hallucination detection, using hallucination pattern guidance and language style alignment.&lt;/li&gt;&lt;li&gt;Introduces a data mixture training strategy to obtain robust supervised detectors with improved generalization across tasks and text generators.&lt;/li&gt;&lt;li&gt;Reports detectors trained on synthetic data that outperform in-context-learning (ICL) detectors by ~32% and demonstrate cross-task and cross-generator generalization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Xie', 'Karan Aggarwal', 'Aitzaz Ahmad', 'Stephen Lau']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'alignment/safety', 'synthetic data generation', 'robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.12278</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection</title><link>https://arxiv.org/abs/2601.04160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RFC Bench, a paragraph-level benchmark for evaluating LLMs on financial misinformation in realistic news contexts.&lt;/li&gt;&lt;li&gt;Defines two tasks: reference-free misinformation detection and comparative diagnosis using paired original/perturbed inputs.&lt;/li&gt;&lt;li&gt;Finds models perform substantially better with comparative context; reference-free settings show unstable predictions and many invalid outputs.&lt;/li&gt;&lt;li&gt;Highlights deficits in maintaining coherent belief states without external grounding and positions the benchmark for studying reference-free reasoning and reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuechen Jiang', 'Zhiwei Liu', 'Yupeng Cao', 'Yueru He', 'Ziyang Xu', 'Chen Xu', 'Zhiyang Deng', 'Prayag Tiwari', 'Xi Chen', 'Alejandro Lopez-Lira', 'Jimin Huang', 'Junichi Tsujii', 'Sophia Ananiadou']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation-detection', 'benchmark', 'safety-evaluation', 'reference-free-reasoning', 'financial-news']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04160</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.02993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a retrieval-permutation sensitivity in RAG: LLM answers vary substantially across permutations of Top-5 retrieved documents, even with the gold doc fixed first.&lt;/li&gt;&lt;li&gt;Proposes Stable-RAG: run the generator on multiple retrieval orders, cluster intermediate hidden states, decode from a cluster-center representation to capture dominant reasoning patterns, and align outputs to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Shows empirical gains in answer accuracy, reasoning consistency, and robustness across three QA datasets, different retrievers, and input lengths compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianchi Zhang', 'Hainan Zhang', 'Liang Pang', 'Hongwei Zheng', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'robustness', 'retrieval sensitivity', 'LLM reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02993</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features</title><link>https://arxiv.org/abs/2511.21744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NEULIF, a lightweight detector for AI-generated text using stylometric and readability features fed into a compact CNN or Random Forest.&lt;/li&gt;&lt;li&gt;Reports high performance on the Kaggle AI vs. Human corpus: CNN ~97% accuracy (~0.95 F1), RF ~95% accuracy (~0.94 F1), with small model sizes (~25 MB and ~10.6 MB).&lt;/li&gt;&lt;li&gt;Emphasizes CPU efficiency and applicability across languages, domains, and streaming contexts as a practical alternative to large transformer-based detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey K. Aityan', 'William Claster', 'Karthik Sai Emani', 'Sohni Rais', 'Thy Tran']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'Stylometry', 'Lightweight models', 'Security/forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21744</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Liars' Bench: Evaluating Lie Detectors for Language Models</title><link>https://arxiv.org/abs/2511.16035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LIARS' BENCH, a benchmark of 72,863 labeled lie vs. honest responses generated by four open-weight models across seven datasets.&lt;/li&gt;&lt;li&gt;Captures diverse lie types by varying two axes: the model's reason for lying and the object of belief targeted by the lie.&lt;/li&gt;&lt;li&gt;Evaluates three black-box and white-box lie-detection techniques and finds systematic failures, especially when the transcript alone is insufficient to identify lies.&lt;/li&gt;&lt;li&gt;Provides a practical testbed to expose limitations of prior techniques and guide future improvements in lie detection for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kieron Kretschmar', 'Walter Laurito', 'Sharan Maiya', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['LLM lie detection', 'safety evaluation', 'benchmark', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16035</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</title><link>https://arxiv.org/abs/2511.10871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an evaluation framework comparing LLM responses to direct factual queries versus conversational judgment tasks ("Is this speaker correct?") to measure changes in model conviction.&lt;/li&gt;&lt;li&gt;Applies a simple rebuttal perturbation ("The previous answer is incorrect.") to assess how firmly models maintain positions under conversational pressure.&lt;/li&gt;&lt;li&gt;Empirically tests multiple models (e.g., GPT-4o-mini, Llama-8B-Instruct), finds an average performance change of 9.24% and model-specific tendencies like sycophancy or over-criticism.&lt;/li&gt;&lt;li&gt;Provides a reproducible methodology for diagnosing conviction and implications for trustworthiness of dialogue systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parisa Rabbani', 'Nimet Beyza Bozdag', 'Dilek Hakkani-T\\"ur']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'robustness', 'sycophancy', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10871</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification</title><link>https://arxiv.org/abs/2510.10961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KOTOX, a Korean dataset pairing toxic and neutral sentences with obfuscated counterparts to support deobfuscation and detoxification.&lt;/li&gt;&lt;li&gt;Categorizes Korean obfuscation patterns into linguistically grounded classes and defines transformation rules based on real-world examples.&lt;/li&gt;&lt;li&gt;Shows models trained on the dataset better handle obfuscated toxic text while retaining performance on non-obfuscated text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yejin Lee', 'Su-Hyeon Kim', 'Hyundong Jin', 'Dayoung Kim', 'Yeonsoo Kim', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'deobfuscation', 'detoxification', 'robustness', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10961</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ADVICE: Answer-Dependent Verbalized Confidence Estimation</title><link>https://arxiv.org/abs/2510.10913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies answer-independence (confidence not conditioned on the model's answer) as a key cause of LLM overconfidence.&lt;/li&gt;&lt;li&gt;Proposes ADVICE, a fine-tuning framework to promote answer-dependent verbalized confidence estimation.&lt;/li&gt;&lt;li&gt;Shows improved calibration and generalization to unseen settings without harming task performance.&lt;/li&gt;&lt;li&gt;Analyzes that gains arise from increased answer dependence, informing trustworthy confidence verbalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ki Jung Seo', 'Sehun Lim', 'Taeuk Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM calibration', 'confidence estimation', 'alignment', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10913</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title><link>https://arxiv.org/abs/2509.08604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study measuring prevalence, characteristics, and volume of memorization in LLMs adapted to medical data across three adaptation scenarios: continued pretraining, fine-tuning on benchmarks, and fine-tuning on real clinical records (&gt;13k inpatient records).&lt;/li&gt;&lt;li&gt;Finds memorization is prevalent and substantially higher than in general-domain models; memorized content persists through adaptation (up to 87% retention after fine-tuning).&lt;/li&gt;&lt;li&gt;Highlights concrete privacy and safety risks (exposure of sensitive patient details, reduced generalizability, overconfident/misleading outputs) with implications for clinical deployment and data-handling practices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anran Li', 'Lingfei Qian', 'Mengmeng Du', 'Yu Yin', 'Yan Hu', 'Zihao Sun', 'Yihang Fu', 'Hyunjae Kim', 'Erica Stutz', 'Xuguang Ai', 'Qianqian Xie', 'Rui Zhu', 'Jimin Huang', 'Yifan Yang', 'Siru Liu', 'Yih-Chung Tham', 'Lucila Ohno-Machado', 'Hyunghoon Cho', 'Zhiyong Lu', 'Hua Xu', 'Qingyu Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'memorization', 'medical LLMs', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08604</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records</title><link>https://arxiv.org/abs/2507.22533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Transforms longitudinal, unstructured EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range temporal dependencies.&lt;/li&gt;&lt;li&gt;Grounds LLM decision support by aligning patient trajectories with a normative guideline knowledge graph to reduce clinical hallucinations and produce evidence-grounded summaries and recommendations.&lt;/li&gt;&lt;li&gt;Validated on a private Chinese cancer dataset and MIMIC-IV, outperforming long-context LLMs and KG-enhanced RAG baselines with evaluation correlated to oncologist assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongchen Li', 'Jitao Liang', 'Wei Li', 'Xiaoyu Wang', 'Longbing Cao', 'Kun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['clinical-grounding', 'hallucination-mitigation', 'temporal-knowledge-graph', 'EHR', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22533</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MedRiskEval: Medical Risk Evaluation Benchmark of Language Models, On the Importance of User Perspectives in Healthcare Settings</title><link>https://arxiv.org/abs/2507.07248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedRiskEval, a medical risk evaluation benchmark tailored to assessing LLM behavior in healthcare settings.&lt;/li&gt;&lt;li&gt;Adds PatientSafetyBench, a patient-oriented dataset of 466 samples across five critical medical risk categories to address non-clinician perspectives.&lt;/li&gt;&lt;li&gt;Evaluates multiple open- and closed-source LLMs on the new and existing medical safety datasets to inform safer deployment in healthcare.&lt;/li&gt;&lt;li&gt;Aims to establish an initial foundation for medical LLM safety by focusing on user perspectives (patients vs. clinicians).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jean-Philippe Corbeil', 'Minseon Kim', 'Maxime Griot', 'Sheela Agarwal', 'Alessandro Sordoni', 'Francois Beaulieu', 'Paul Vozila']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'medical AI', 'LLM benchmarking', 'user-centered risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07248</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge</title><link>https://arxiv.org/abs/2506.06240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DSSP-RAG, a Dual-Stream Knowledge-Augmented Framework that refines self-attention into mixed-attention to separate shared vs. private semantics for controlled integration of external retrievals.&lt;/li&gt;&lt;li&gt;Introduces an unsupervised hallucination detection method that uses the LLMs' intrinsic cognitive uncertainty to gate when external knowledge should be incorporated.&lt;/li&gt;&lt;li&gt;Defines an Energy Quotient (EQ) metric based on attention difference matrices between task-aligned and task-misaligned layers to reduce noise in retrieved external knowledge.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing DSSP-RAG outperforms strong baselines in reducing hallucination and improving generation quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Sui', 'Chaozhuo Li', 'Chen Zhang', 'Dawei song', 'Qiuchi Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'uncertainty detection', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06240</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detect, Explain, Escalate: Sustainable Dialogue Breakdown Management for LLM Agents</title><link>https://arxiv.org/abs/2504.18839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'Detect, Explain, Escalate' pipeline: an efficient fine-tuned 8B model (with teacher reasoning traces) that detects and explains dialogue breakdowns and defers to larger LLMs when needed.&lt;/li&gt;&lt;li&gt;Demonstrates robust classification and calibration across English and Japanese dialogues, generalizes to BETOLD, and achieves SOTA on DBDC5 while narrowing the gap to proprietary models.&lt;/li&gt;&lt;li&gt;Uses advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity assessment and an escalation architecture that reduces inference costs by ~54%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdellah Ghassel', 'Xianzhi Li', 'Xiaodan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['dialogue-safety', 'breakdown-detection', 'model-monitoring', 'cost-efficient-escalation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18839</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets</title><link>https://arxiv.org/abs/2503.16674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes ideological framing bias in LLM-generated journalism using two datasets (POLIGEN and ECONOLEX) covering political and economic discourse.&lt;/li&gt;&lt;li&gt;Evaluates eight LLMs' ability to annotate ideologically framed text; finds GPT-4o achieves human-level accuracy and high agreement with human annotators.&lt;/li&gt;&lt;li&gt;Examines LLM-as-a-judge behavior via Socratic probing of models' feedback on their own outputs, revealing inconsistencies and systematic preference toward certain perspectives in binary comparisons.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Molly Kennedy', 'Ayyoob Imani', 'Timo Spinde', 'Akiko Aizawa', 'Hinrich Sch\\"utze']&lt;/li&gt;&lt;li&gt;Tags: ['ideological-bias', 'LLM-evaluation', 'alignment', 'fairness', 'LLM-as-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16674</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Evaluation on Large Language Model Outputs: Discourse and Memorization</title><link>https://arxiv.org/abs/2304.08637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of outputs from nine widely-available LLMs focusing on memorization, uniqueness, and discourse pathologies (e.g., counterfactuals, logical flaws, staying on topic).&lt;/li&gt;&lt;li&gt;Finds 80% of evaluated outputs contained memorized text and observes a correlation between higher memorization and perceived higher-quality outputs.&lt;/li&gt;&lt;li&gt;Assesses mitigation strategies that reduce the rate of memorized content being produced and discusses implications for learning, memorization, and quality evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrian de Wynter', 'Xun Wang', 'Alex Sokolov', 'Qilong Gu', 'Si-Qing Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data privacy/leakage', 'factuality/robustness', 'evaluation/benchmarking', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2304.08637</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility</title><link>https://arxiv.org/abs/2601.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PII-VisBench, a benchmark of 4,000 probes across 200 subjects stratified into four visibility levels (high/medium/low/zero) to assess PII leakage in vision-language models.&lt;/li&gt;&lt;li&gt;Evaluates 18 open-source VLMs (0.3B–32B) using Refusal Rate and Conditional PII Disclosure Rate, finding disclosures correlate with subject visibility and vary by model family and PII type.&lt;/li&gt;&lt;li&gt;Demonstrates paraphrasing and jailbreak-style prompts can bypass refusals, highlighting attack vectors and motivating visibility-aware safety evaluation and training interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G M Shahariar', 'Zabir Al Nazi', 'Md Olid Hasan Bhuiyan', 'Zhouxing Shi']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'VLM safety', 'Benchmarking', 'Jailbreaking', 'Privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05739</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs</title><link>https://arxiv.org/abs/2601.05635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an entity-based framework that synthesizes encrypted training data for continual pretraining of LLMs, using a weighted entity graph and deterministic encryption to protect PII while allowing decryption via keys.&lt;/li&gt;&lt;li&gt;Empirical results on limited-scale datasets show continual pretraining on encrypted synthetic data outperforms base models and is modestly worse than training on unencrypted synthetic data; performance improves with more entities and graph-based synthesis.&lt;/li&gt;&lt;li&gt;Demonstrates encrypted models retain instruction-following with long retrieved contexts and discusses security implications and limitations of deterministic encryption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honghao Liu', 'Xuhui Jiang', 'Chengjin Xu', 'Cehao Yang', 'Yiran Cheng', 'Lionel Ni', 'Jian Guo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'LLM pretraining', 'data encryption', 'synthetic data', 'PII protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05635</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes</title><link>https://arxiv.org/abs/2601.05600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SceneAlign, which leverages scene graphs to provide structured visual information for grounding multimodal reasoning.&lt;/li&gt;&lt;li&gt;Identifies reasoning-critical nodes and applies four targeted perturbation strategies to generate linguistically plausible but visually inaccurate (hard negative) rationales.&lt;/li&gt;&lt;li&gt;Uses contrastive pairs in Direct Preference Optimization to train models toward fine-grained, structure-faithful reasoning and reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in answer accuracy and reasoning faithfulness across seven visual reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuhan Wang', 'Xintong Li', 'Jennifer Yuntong Zhang', 'Junda Wu', 'Chengkai Huang', 'Lina Yao', 'Julian McAuley', 'Jingbo Shang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'visual grounding', 'hallucination mitigation', 'contrastive learning', 'preference optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05600</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models</title><link>https://arxiv.org/abs/2601.05376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of persona conditioning (professional role + interaction style) on clinical LLM behavior across tasks like triage and patient-safety assessments.&lt;/li&gt;&lt;li&gt;Finds context-dependent, non-monotonic effects: medical personas can boost accuracy and calibration in critical-care tasks (~+20%) but can degrade performance in primary-care settings by similar margins.&lt;/li&gt;&lt;li&gt;Interaction style shifts risk propensity and sensitivity in a model-dependent way; automated LLM judges favor medical personas in safety-critical cases, but human clinicians show only moderate agreement (Cohen's κ = 0.43) and low confidence in reasoning quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tassallah Abdullahi', 'Shrestha Ghosh', 'Hamish S Fraser', "Daniel Le\\'on Tramontini", 'Adeel Abbasi', 'Ghada Bourjeily', 'Carsten Eickhoff', 'Ritambhara Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'persona conditioning', 'clinical AI', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05376</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning</title><link>https://arxiv.org/abs/2601.05300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TIME, a behavioral alignment framework that treats explicit chain-of-thought-style reasoning as a context-sensitive resource driven by temporal and discourse cues (ISO 8601 &lt;time&gt; tags, tick turns, and short in-place &lt;think&gt; blocks).&lt;/li&gt;&lt;li&gt;Presents a four-phase curriculum to train Qwen3 dense models (4B–32B) to invoke brief, in-place reasoning bursts, reducing always-on long rationale traces and keeping user-facing text compact.&lt;/li&gt;&lt;li&gt;Introduces TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense across gaps/offsets, anomaly detection, and continuity; reports improved TIMEBench scores and roughly an order-of-magnitude reduction in reasoning tokens versus base models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Susmit Das']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'behavioral-alignment', 'temporal-reasoning', 'LLM-control', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05300</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Document Impact in RAG-LLMs</title><link>https://arxiv.org/abs/2601.05260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Influence Score (IS), a metric based on Partial Information Decomposition to quantify each retrieved document's impact on a RAG system's generated response.&lt;/li&gt;&lt;li&gt;Validates IS with a simulated poison attack: IS identifies the malicious document as most influential in 86% of cases.&lt;/li&gt;&lt;li&gt;Performs an ablation study showing responses generated from top-ranked IS documents are more similar to the original than those from remaining documents, demonstrating IS can isolate influential sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Gerami', 'Kazem Faghih', 'Ramani Duraiswami']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'data poisoning', 'influence attribution', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05260</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards</title><link>https://arxiv.org/abs/2601.06021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Citation-aware Rubric Rewards (CaRR), a fine-grained reward scheme that decomposes complex queries into verifiable single-hop rubrics requiring explicit identification of entities, correct citations, and end-to-end evidence chains.&lt;/li&gt;&lt;li&gt;Introduces Citation-aware Group Relative Policy Optimization (C-GRPO), combining CaRR with outcome rewards to train RL-based deep search agents.&lt;/li&gt;&lt;li&gt;Empirical results show C-GRPO outperforms standard outcome-based RL baselines, reduces shortcut exploitation and hallucinations, and improves generalization on open-ended deep research tasks.&lt;/li&gt;&lt;li&gt;Provides code and data for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Zhang', 'Xin Lv', 'Ling Feng', 'Lei Hou', 'Juanzi Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'reinforcement learning', 'factuality / citation grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.06021</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency</title><link>https://arxiv.org/abs/2601.05905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neighbor-Consistency Belief (NCB), a structural measure evaluating LLM belief robustness across a conceptual neighborhood rather than point-wise confidence.&lt;/li&gt;&lt;li&gt;Proposes a cognitive stress-testing protocol that probes output stability under mild contextual interference, showing self-consistency can mask brittle beliefs.&lt;/li&gt;&lt;li&gt;Presents Structure-Aware Training (SAT) to optimize context-invariant belief structure, reporting ~30% reduction in long-tail knowledge brittleness across multiple LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoming Xu', 'Ningyuan Zhao', 'Yunzhi Yao', 'Weihong Xu', 'Hongru Wang', 'Xinle Deng', 'Shumin Deng', 'Jeff Z. Pan', 'Huajun Chen', 'Ningyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'truthfulness/robustness', 'evaluation/benchmarking', 'stress-testing', 'training for robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05905</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift</title><link>https://arxiv.org/abs/2601.05882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how preference-tuning (alignment to human judgments) generalizes under domain shift across summarization and QA helpfulness tasks.&lt;/li&gt;&lt;li&gt;Compares five alignment objectives and several adaptation strategies, including target-domain supervised fine-tuning and pseudo-labeling.&lt;/li&gt;&lt;li&gt;Finds systematic differences in generalization across objectives and shows pseudo-labeling-based adaptation can substantially reduce domain-shift degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constantinos Karouzos', 'Xingwei Tan', 'Nikolaos Aletras']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference tuning', 'domain shift', 'safety evaluation', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05882</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law</title><link>https://arxiv.org/abs/2601.05879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of four state-of-the-art LLMs (GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, Llama 3.3) on a Czech family-law shared-parenting scenario to probe for gender bias.&lt;/li&gt;&lt;li&gt;Uses two scenario versions (gendered names vs neutral labels) and nine legally relevant factual variations to test whether models suggest different shared-parenting ratios depending on parent gender.&lt;/li&gt;&lt;li&gt;Finds model-specific differences and preliminary evidence of gender-dependent patterns in proposed outcomes, highlighting risks of biased legal guidance from LLMs and the need for robust evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Harasta', 'Matej Vasina', 'Martin Kornel', 'Tomas Foltynek']&lt;/li&gt;&lt;li&gt;Tags: ['bias/fairness', 'LLM evaluation', 'social harm / safety', 'legal domain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05879</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG</title><link>https://arxiv.org/abs/2601.05866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FACTUM, a framework of four mechanistic scores that quantify attention vs. feedforward (FFN) pathway contributions and their alignment to detect citation trustworthiness in RAG outputs.&lt;/li&gt;&lt;li&gt;Identifies signatures of correct citations (stronger parametric contribution and greater attention-sink use) and shows these signatures change with model scale (e.g., different alignment behaviors across Llama-3.2-3B vs Llama-3.1-8B).&lt;/li&gt;&lt;li&gt;Demonstrates FACTUM outperforms state-of-the-art baselines by up to 37.5% AUC, reframing citation hallucination as a scale-dependent interplay of internal mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maxime Dassen', 'Rebecca Kotula', 'Kenton Murray', 'Andrew Yates', 'Dawn Lawrie', 'Efsun Kayi', 'James Mayfield', 'Kevin Duh']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'mechanistic interpretability', 'model safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05866</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor</title><link>https://arxiv.org/abs/2601.05752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoMonitor-Bench, a benchmark of 3,010 annotated paired misbehavior/benign samples across QA, code generation, and reasoning to evaluate LLM-based misbehavior monitors.&lt;/li&gt;&lt;li&gt;Defines evaluation metrics Miss Rate (MR) and False Alarm Rate (FAR) to capture detection failures and oversensitivity, and shows a consistent MR–FAR trade-off.&lt;/li&gt;&lt;li&gt;Evaluates 22 LLMs (12 proprietary, 10 open-source) and demonstrates substantial variability in monitoring performance and inherent safety-utility tensions.&lt;/li&gt;&lt;li&gt;Builds a large training corpus (153,581 samples) and fine-tunes Qwen3-4B-Instruction to test whether training on constructed misbehaviors improves generalization to unseen/implicit misbehaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yang', 'Jingyu Hu', 'Tong Li', 'Hanqi Yan', 'Wenxuan Wang', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM monitoring', 'safety evaluation', 'misbehavior detection', 'benchmarking', 'model fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05752</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs</title><link>https://arxiv.org/abs/2601.05641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates data unlearning and concept unlearning in a multilingual 8B LLM (Aya-Expanse) across ten languages.&lt;/li&gt;&lt;li&gt;Extends factual-knowledge and stereotype unlearning benchmarks to multiple languages via translation and measures transfer effects.&lt;/li&gt;&lt;li&gt;Finds unlearning is more stable in high-resource languages, with asymmetric transfer between typologically related languages and syntactic similarity predicting cross-lingual unlearning behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Dehghanpour Farashah', 'Aditi Khandelwal', 'Marylou Fauchard', 'Zhuan Shi', 'Negar Rostamzadeh', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'multilingual LLMs', 'model safety', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05641</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text Detoxification in isiXhosa and Yor\`ub\'a: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages</title><link>https://arxiv.org/abs/2601.05624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses automatic text detoxification (toxic→neutral rewriting) for two low-resource African languages: isiXhosa and Yorùbá, and builds a parallel corpus reflecting idioms, diacritics, and code-switching.&lt;/li&gt;&lt;li&gt;Detection component: lightweight, interpretable TF-IDF + Logistic Regression model for toxicity classification.&lt;/li&gt;&lt;li&gt;Rewriting component: controlled lexicon- and token-guided rule-based editing that detoxified all detected toxic sentences while preserving 100% of non-toxic sentences; detection accuracies reported 61–72% (isiXhosa) and 72–86% (Yorùbá) with ROC-AUC up to 0.88.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abayomi O. Agbeyangi']&lt;/li&gt;&lt;li&gt;Tags: ['toxic-language', 'content-moderation', 'detoxification', 'low-resource-languages', 'text-style-transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05624</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring</title><link>https://arxiv.org/abs/2601.05545</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Harmful Essay Detection (HED) benchmark to evaluate AES systems and LLMs on essays containing sensitive/harmful content (e.g., racism, gender bias).&lt;/li&gt;&lt;li&gt;Finds that current LLMs and AES models often fail to distinguish harmful from merely argumentative essays and may assign high scores to ethically problematic content.&lt;/li&gt;&lt;li&gt;Argues for the need to incorporate ethical/harm-awareness into AES systems and improve LLM capabilities for harmful-content recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongjin Kim', 'Jeonghyun Kang', 'Harksoo Kim']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'alignment/safety evaluation', 'automated essay scoring', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05545</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence</title><link>https://arxiv.org/abs/2601.05478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MisBelief, a framework that generates sophisticated, hard-to-falsify deceptive evidence via collaborative, multi-round interactions among multi-role LLMs; produces a 4,800-instance dataset across three difficulty levels.&lt;/li&gt;&lt;li&gt;Evaluates 7 representative LLMs and finds that exposure to refined deceptive evidence increases belief in falsehoods by an average of 93.0%, undermining downstream recommendations.&lt;/li&gt;&lt;li&gt;Proposes Deceptive Intent Shielding (DIS), a governance mechanism that infers deceptive intent to provide early warnings and empirically mitigates belief shifts, encouraging more cautious evidence evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Herun Wan', 'Jiaying Wu', 'Minnan Luo', 'Fanxiao Li', 'Zhi Zeng', 'Min-Yen Kan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'misinformation/deception', 'model robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05478</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tracing Moral Foundations in Large Language Models</title><link>https://arxiv.org/abs/2601.05437</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how Moral Foundations Theory (MFT) concepts are represented across layers in two instruction-tuned LLMs (Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct).&lt;/li&gt;&lt;li&gt;Uses sparse autoencoders over residual streams to identify sparse features linked to specific moral foundations and shows semantic alignment with human judgments.&lt;/li&gt;&lt;li&gt;Performs causal steering interventions (dense MFT vectors and sparse features) to shift model moral outputs, demonstrating a causal link between internal representations and moral behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxiao Yu', 'Bowen Yi', 'Farzan Karimi-Malekabadi', 'Suhaib Abdurahman', 'Jinyi Ye', 'Shrikanth Narayanan', 'Yue Zhao', 'Morteza Dehghani']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'safety-evaluation', 'causal interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05437</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions</title><link>https://arxiv.org/abs/2601.05414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical audit of 11 LLMs across 15 probability distributions, using N=1000 samples to evaluate native probabilistic sampling fidelity.&lt;/li&gt;&lt;li&gt;Dual-protocol design reveals a sharp asymmetry: Batch Generation has modest success (median 13% pass rate) while Independent Requests largely fail (10 of 11 models pass none).&lt;/li&gt;&lt;li&gt;Sampling fidelity worsens with distributional complexity and larger requested sample horizons, and these failures propagate to downstream tasks (MCQ answer-position uniformity and demographic targets in text-to-image prompts).&lt;/li&gt;&lt;li&gt;Conclusion: current LLMs lack reliable internal samplers, implying external tools are needed for applications requiring statistical guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minda Zhao', 'Yilun Du', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'statistical-sampling', 'benchmarking', 'fairness-bias', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05414</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection</title><link>https://arxiv.org/abs/2601.05403</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MFMD-scen, a benchmark to evaluate behavioral biases of LLMs in multilingual financial misinformation detection across complex, realistic scenarios.&lt;/li&gt;&lt;li&gt;Collects three scenario types (role+personality, role+region, role+ethnicity/religion) and a multilingual dataset in English, Chinese, Greek, and Bengali.&lt;/li&gt;&lt;li&gt;Systematically evaluates 22 mainstream LLMs and reports persistent behavioral biases in both commercial and open-source models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiwei Liu', 'Yupen Cao', 'Yuechen Jiang', 'Mohsinul Kabir', 'Polydoros Giannouris', 'Chen Xu', 'Ziyang Xu', 'Tianlei Zhu', 'Tariquzzaman Faisal', 'Triantafillos Papadopoulos', 'Yan Wang', 'Lingfei Qian', 'Xueqing Peng', 'Zhuohan Xie', 'Ye Yuan', 'Saeed Almheiri', 'Abdulrazzaq Alnajjar', 'Mingbin Chen', 'Harry Stuart', 'Paul Thompson', 'Prayag Tiwari', 'Alejandro Lopez-Lira', 'Xue Liu', 'Jimin Huang', 'Sophia Ananiadou']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'safety evaluation', 'financial misinformation', 'multilingual NLP', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05403</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models</title><link>https://arxiv.org/abs/2601.05366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MLCL, a diagnostic benchmark for evaluating multilingual tool-calling (Chinese, Hindi, Igbo) in LLM agents.&lt;/li&gt;&lt;li&gt;Finds many execution failures occur despite correct intent and tool selection, with parameter value language mismatch as a dominant failure mode.&lt;/li&gt;&lt;li&gt;Evaluates inference-time system strategies that reduce language-induced execution errors but do not fully recover English-level performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Luo', 'T Pranav Kutralingam', 'Ogochukwu N Okoani', 'Wanpeng Xu', 'Hua Wei', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual robustness', 'tool calling', 'benchmarking', 'safety evaluation', 'LLM reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05366</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PromptScreen: Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptScreen, a lightweight multi-stage defense pipeline against prompt injection and jailbreak attacks for LLM-based systems.&lt;/li&gt;&lt;li&gt;Core module is a semantic filter using text normalization, TF-IDF representations, and a Linear SVM, achieving 93.4% accuracy and 96.5% specificity on held-out data.&lt;/li&gt;&lt;li&gt;Pipeline claims large reductions in attack throughput and latency versus model-based moderators (improves accuracy from 35.1% to 93.4% and reduces time-to-completion from ~450s to ~47s relative to ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluation performed on a curated corpus of &gt;30,000 labeled prompts covering benign, jailbreak, and application-layer injections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM defenses', 'red teaming', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach</title><link>https://arxiv.org/abs/2501.01042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies transferability of adversarial video examples across video-based multimodal LLMs (V-MLLMs) and identifies limitations of existing attacks in black-box settings.&lt;/li&gt;&lt;li&gt;Proposes I2V-MLLM: an Image-to-Video attack that uses an image-based MLLM surrogate to craft adversarial videos by integrating multimodal interactions and spatiotemporal perturbations.&lt;/li&gt;&lt;li&gt;Introduces a perturbation propagation technique to handle unknown frame sampling strategies and improves cross-model transferability.&lt;/li&gt;&lt;li&gt;Demonstrates strong black-box transferability on video-text tasks (Zero-Shot VideoQA), achieving ~58% AASR on MSVD-QA and MSRVTT-QA using BLIP-2 as surrogate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linhao Huang', 'Xue Jiang', 'Zhiqiang Wang', 'Wentao Mo', 'Xi Xiao', 'Yong-Jie Yin', 'Bo Han', 'Feng Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'black-box attacks', 'multimodal LLMs', 'video QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01042</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Evaluation on Large Language Model Outputs: Discourse and Memorization</title><link>https://arxiv.org/abs/2304.08637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of outputs from nine widely-available LLMs focusing on memorization, uniqueness, and discourse pathologies (e.g., counterfactuals, logical flaws, staying on topic).&lt;/li&gt;&lt;li&gt;Finds 80% of evaluated outputs contained memorized text and observes a correlation between higher memorization and perceived higher-quality outputs.&lt;/li&gt;&lt;li&gt;Assesses mitigation strategies that reduce the rate of memorized content being produced and discusses implications for learning, memorization, and quality evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrian de Wynter', 'Xun Wang', 'Alex Sokolov', 'Qilong Gu', 'Si-Qing Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data privacy/leakage', 'factuality/robustness', 'evaluation/benchmarking', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2304.08637</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising</title><link>https://arxiv.org/abs/2510.01137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a post-processing algorithm that uses random matrix theory to denoise noisy gradient matrices produced by DP-SGD, restoring low-rank structure and improving signal alignment.&lt;/li&gt;&lt;li&gt;Applied to DP fine-tuning of RoBERTa on GLUE tasks, the method improves sample efficiency and reduces training time when optimal performance is not required.&lt;/li&gt;&lt;li&gt;Claims the denoising step preserves differential privacy guarantees while enhancing utility, demonstrating matrix recovery techniques can boost private training performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Dadsetan', 'Frank Rudzicz']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'DP-SGD', 'privacy-preserving training', 'gradient denoising', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01137</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems</title><link>https://arxiv.org/abs/2509.22047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that Group Relative Policy Optimization (GRPO) is vulnerable to reward hacking in multi-objective settings, where the optimizer focuses on one objective at the expense of others.&lt;/li&gt;&lt;li&gt;Proposes MO-GRPO, which normalizes/reweights reward components by their variances to ensure even contribution and preserve preference ordering without manual scale tuning.&lt;/li&gt;&lt;li&gt;Provides analytical guarantees and empirical validation across multi-armed bandits, simulated control, machine translation, and instruction-following tasks showing improved stability and balanced optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuki Ichihara', 'Yuu Jinnai', 'Tetsuro Morimura', 'Mitsuki Sakamoto', 'Ryota Mitsuhashi', 'Eiji Uchibe']&lt;/li&gt;&lt;li&gt;Tags: ['reward-hacking', 'multi-objective-reinforcement-learning', 'alignment', 'robustness', 'rl-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22047</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HiQ-Lip: A Hierarchical Quantum-Classical Method for Global Lipschitz Constant Estimation of ReLU Networks</title><link>https://arxiv.org/abs/2503.16342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HiQ-Lip, a hybrid quantum-classical hierarchical method to estimate global Lipschitz constants of ReLU networks by formulating the problem as a QUBO and using multilevel graph coarsening/refinement to fit current quantum hardware constraints.&lt;/li&gt;&lt;li&gt;Aims to address NP-hardness and scalability limitations of SDP-based methods, leveraging small-scale quantum devices to accelerate computation.&lt;/li&gt;&lt;li&gt;Experimental results on fully connected networks show comparable or better upper-bound estimates and significant speedups (e.g., 2x faster and tighter bounds than LiPopt on two-layer networks with 256 hidden neurons).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoqi He', 'Yan Xiao', 'Wenzhi Xu', 'Ruoying Liu', 'Xiaokai Lin', 'Kai Wen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'Lipschitz estimation', 'verification', 'quantum machine learning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16342</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens</title><link>https://arxiv.org/abs/2502.11245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes concept-based models through the lens of reasoning shortcuts (RSs), showing models can achieve high accuracy while learning low-quality concepts.&lt;/li&gt;&lt;li&gt;Provides theoretical identifiability conditions for recovering both concept extractors and the inference layer in concept-based models.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that existing methods and common mitigation strategies often fail to satisfy these identifiability conditions, highlighting practical vulnerability to shortcut learning and OOD failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuele Bortolotti', 'Emanuele Marconato', 'Paolo Morettin', 'Andrea Passerini', 'Stefano Teso']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'shortcut learning', 'robustness', 'out-of-distribution generalization', 'identifiability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11245</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency</title><link>https://arxiv.org/abs/2601.05905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neighbor-Consistency Belief (NCB), a structural measure evaluating LLM belief robustness across a conceptual neighborhood rather than point-wise confidence.&lt;/li&gt;&lt;li&gt;Proposes a cognitive stress-testing protocol that probes output stability under mild contextual interference, showing self-consistency can mask brittle beliefs.&lt;/li&gt;&lt;li&gt;Presents Structure-Aware Training (SAT) to optimize context-invariant belief structure, reporting ~30% reduction in long-tail knowledge brittleness across multiple LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoming Xu', 'Ningyuan Zhao', 'Yunzhi Yao', 'Weihong Xu', 'Hongru Wang', 'Xinle Deng', 'Shumin Deng', 'Jeff Z. Pan', 'Huajun Chen', 'Ningyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'truthfulness/robustness', 'evaluation/benchmarking', 'stress-testing', 'training for robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05905</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift</title><link>https://arxiv.org/abs/2601.05882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how preference-tuning (alignment to human judgments) generalizes under domain shift across summarization and QA helpfulness tasks.&lt;/li&gt;&lt;li&gt;Compares five alignment objectives and several adaptation strategies, including target-domain supervised fine-tuning and pseudo-labeling.&lt;/li&gt;&lt;li&gt;Finds systematic differences in generalization across objectives and shows pseudo-labeling-based adaptation can substantially reduce domain-shift degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constantinos Karouzos', 'Xingwei Tan', 'Nikolaos Aletras']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference tuning', 'domain shift', 'safety evaluation', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05882</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces</title><link>https://arxiv.org/abs/2601.05789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAFE, a federated learning method for EEG-based BCIs that keeps data local to protect privacy.&lt;/li&gt;&lt;li&gt;Uses local batch-specific normalization to address cross-subject distribution shifts and improve generalization.&lt;/li&gt;&lt;li&gt;Introduces federated adversarial training and adversarial weight perturbation to enhance adversarial robustness.&lt;/li&gt;&lt;li&gt;Evaluates on five EEG datasets, claiming superior decoding accuracy and robustness versus 14 SOTA and even centralized baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianwang Jia', 'Xiaoqing Chen', 'Dongrui Wu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'adversarial robustness', 'brain-computer interfaces', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05789</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs</title><link>https://arxiv.org/abs/2601.05641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates data unlearning and concept unlearning in a multilingual 8B LLM (Aya-Expanse) across ten languages.&lt;/li&gt;&lt;li&gt;Extends factual-knowledge and stereotype unlearning benchmarks to multiple languages via translation and measures transfer effects.&lt;/li&gt;&lt;li&gt;Finds unlearning is more stable in high-resource languages, with asymmetric transfer between typologically related languages and syntactic similarity predicting cross-lingual unlearning behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Dehghanpour Farashah', 'Aditi Khandelwal', 'Marylou Fauchard', 'Zhuan Shi', 'Negar Rostamzadeh', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'multilingual LLMs', 'model safety', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05641</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes</title><link>https://arxiv.org/abs/2601.05600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SceneAlign, which leverages scene graphs to provide structured visual information for grounding multimodal reasoning.&lt;/li&gt;&lt;li&gt;Identifies reasoning-critical nodes and applies four targeted perturbation strategies to generate linguistically plausible but visually inaccurate (hard negative) rationales.&lt;/li&gt;&lt;li&gt;Uses contrastive pairs in Direct Preference Optimization to train models toward fine-grained, structure-faithful reasoning and reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in answer accuracy and reasoning faithfulness across seven visual reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuhan Wang', 'Xintong Li', 'Jennifer Yuntong Zhang', 'Junda Wu', 'Chengkai Huang', 'Lina Yao', 'Julian McAuley', 'Jingbo Shang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'visual grounding', 'hallucination mitigation', 'contrastive learning', 'preference optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05600</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models</title><link>https://arxiv.org/abs/2601.05445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mastermind, a closed-loop multi-turn jailbreak framework with planning, execution, and reflection to autonomously discover and refine attack patterns against LLMs.&lt;/li&gt;&lt;li&gt;Uses hierarchical planning and a knowledge repository to maintain long-term coherence, recombine attack vectors, and adapt dynamically to model state.&lt;/li&gt;&lt;li&gt;Evaluates against state-of-the-art models (e.g., GPT-5, Claude 3.7 Sonnet) and reports substantially higher attack success and harmfulness rates, plus resilience to advanced defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songze Li', 'Ruishi He', 'Xiaojun Jia', 'Jun Wang', 'Zhihui Fu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'automated attack frameworks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05445</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models</title><link>https://arxiv.org/abs/2601.05366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MLCL, a diagnostic benchmark for evaluating multilingual tool-calling (Chinese, Hindi, Igbo) in LLM agents.&lt;/li&gt;&lt;li&gt;Finds many execution failures occur despite correct intent and tool selection, with parameter value language mismatch as a dominant failure mode.&lt;/li&gt;&lt;li&gt;Evaluates inference-time system strategies that reduce language-induced execution errors but do not fully recover English-level performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Luo', 'T Pranav Kutralingam', 'Ogochukwu N Okoani', 'Wanpeng Xu', 'Hua Wei', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual robustness', 'tool calling', 'benchmarking', 'safety evaluation', 'LLM reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05366</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis</title><link>https://arxiv.org/abs/2601.05280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalises recursive self-training of LLMs as a discrete-time dynamical system and proves two fundamental failure modes as self-generated data dominates: Entropy Decay (mode collapse) and Variance Amplification (semantic drift).&lt;/li&gt;&lt;li&gt;Argues these degenerative behaviors are architecture-agnostic and arise from finite-sample distributional learning, and that RL with imperfect verifiers is similarly prone to semantic collapse.&lt;/li&gt;&lt;li&gt;Proposes overcoming limits via neurosymbolic techniques—symbolic regression and program synthesis guided by Algorithmic Probability (Coding Theorem Method)—to identify generative mechanisms rather than mere correlations, enabling sustained self-improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hector Zenil']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improvement', 'model-collapse', 'neurosymbolic-methods', 'algorithmic-probability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05280</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Document Impact in RAG-LLMs</title><link>https://arxiv.org/abs/2601.05260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Influence Score (IS), a metric based on Partial Information Decomposition to quantify each retrieved document's impact on a RAG system's generated response.&lt;/li&gt;&lt;li&gt;Validates IS with a simulated poison attack: IS identifies the malicious document as most influential in 86% of cases.&lt;/li&gt;&lt;li&gt;Performs an ablation study showing responses generated from top-ranked IS documents are more similar to the original than those from remaining documents, demonstrating IS can isolate influential sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Gerami', 'Kazem Faghih', 'Ramani Duraiswami']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'data poisoning', 'influence attribution', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05260</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Sparse Autoencoders Identify Reasoning Features in Language Models?</title><link>https://arxiv.org/abs/2601.05679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether sparse autoencoder (SAE) features in LLMs correspond to genuine reasoning mechanisms or superficial lexical correlates using token-injection and LLM-guided falsification experiments.&lt;/li&gt;&lt;li&gt;Across 20 model/layer/dataset configurations, many features (59%–94%) can be triggered by inserting a few associated tokens into non-reasoning text, indicating reliance on lexical artifacts.&lt;/li&gt;&lt;li&gt;Remaining features are falsified by generating inputs that activate the feature without reasoning or reasoning inputs that do not activate it; no feature met criteria for representing genuine reasoning.&lt;/li&gt;&lt;li&gt;Steering identified features produced minimal or slightly negative effects on benchmark reasoning performance, suggesting limited causal role in reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['George Ma', 'Zhongyuan Liang', 'Irene Y. Chen', 'Somayeh Sojoudi']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'robustness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05679</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Over-Searching in Search-Augmented Large Language Models</title><link>https://arxiv.org/abs/2601.05503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and systematically evaluates 'over-searching' in search-augmented LLMs—unnecessary retrieval calls that increase cost and introduce irrelevant context leading to hallucinations.&lt;/li&gt;&lt;li&gt;Empirical findings: search improves accuracy on answerable queries but reduces abstention on unanswerable ones; over-searching is worse for complex reasoning models, amplified by noisy retrieval, and compounds across multi-turn conversations; negative retrieved evidence can improve abstention.&lt;/li&gt;&lt;li&gt;Introduces Tokens Per Correctness (TPC) as a performance–cost metric, releases the OverSearchQA dataset, and studies mitigation strategies at query and retrieval levels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roy Xie', 'Deepak Gopinath', 'David Qiu', 'Dong Lin', 'Haitian Sun', 'Saloni Potdar', 'Bhuwan Dhingra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Hallucination', 'Retrieval-augmented generation', 'Robustness', 'Evaluation/Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05503</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Efficient Inference for Noisy LLM-as-a-Judge Evaluation</title><link>https://arxiv.org/abs/2601.05420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes statistical methods to correct for systematic, non-random errors when using LLMs as automatic evaluators ('LLM-as-a-judge'), focusing on mean parameter estimation (e.g., average scores or pairwise win rates).&lt;/li&gt;&lt;li&gt;Unifies measurement-error correction (Rogan-Gladen-style) and surrogate-outcome approaches (prediction-powered inference, PPI) via semiparametric efficiency theory and derives EIF-based efficient estimators.&lt;/li&gt;&lt;li&gt;Characterizes when PPI-style estimators yield strictly lower asymptotic variance than measurement-error corrections, verifies results in simulations, and demonstrates on real data; provides implementation code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiqun T Chen', 'Sizhu Lu', 'Sijia Li', 'Moran Guo', 'Shengyi Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-as-a-judge', 'evaluation', 'measurement-error', 'bias-correction', 'statistical-efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05420</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning</title><link>https://arxiv.org/abs/2601.05300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TIME, a behavioral alignment framework that treats explicit chain-of-thought-style reasoning as a context-sensitive resource driven by temporal and discourse cues (ISO 8601 &lt;time&gt; tags, tick turns, and short in-place &lt;think&gt; blocks).&lt;/li&gt;&lt;li&gt;Presents a four-phase curriculum to train Qwen3 dense models (4B–32B) to invoke brief, in-place reasoning bursts, reducing always-on long rationale traces and keeping user-facing text compact.&lt;/li&gt;&lt;li&gt;Introduces TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense across gaps/offsets, anomaly detection, and continuity; reports improved TIMEBench scores and roughly an order-of-magnitude reduction in reasoning tokens versus base models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Susmit Das']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'behavioral-alignment', 'temporal-reasoning', 'LLM-control', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05300</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?</title><link>https://arxiv.org/abs/2512.23385</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of developer-reported security discussions from Hugging Face and GitHub, producing a dataset of 312,868 security-related posts identified via keyword matching plus a fine-tuned distilBERT classifier.&lt;/li&gt;&lt;li&gt;Thematic analysis of 753 sampled posts yields a taxonomy of 32 security issues and 24 solutions organized into four themes: System &amp; Software, External Tools &amp; Ecosystem, Model, and Data.&lt;/li&gt;&lt;li&gt;Findings highlight that many security problems stem from complex dependencies and the black-box nature of AI components, with Model- and Data-related issues often lacking concrete mitigation strategies.&lt;/li&gt;&lt;li&gt;Provides an evidence base and guidance for developers and researchers, plus a pipeline and dataset useful for further AI supply-chain security research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['The Anh Nguyen', 'Triet Huynh Minh Le', 'M. Ali Babar']&lt;/li&gt;&lt;li&gt;Tags: ['AI supply chain security', 'empirical study', 'vulnerability taxonomy', 'model/data security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23385</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PromptScreen: Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title><link>https://arxiv.org/abs/2512.19011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptScreen, a lightweight multi-stage defense pipeline against prompt injection and jailbreak attacks for LLM-based systems.&lt;/li&gt;&lt;li&gt;Core module is a semantic filter using text normalization, TF-IDF representations, and a Linear SVM, achieving 93.4% accuracy and 96.5% specificity on held-out data.&lt;/li&gt;&lt;li&gt;Pipeline claims large reductions in attack throughput and latency versus model-based moderators (improves accuracy from 35.1% to 93.4% and reduces time-to-completion from ~450s to ~47s relative to ShieldGemma).&lt;/li&gt;&lt;li&gt;Evaluation performed on a curated corpus of &gt;30,000 labeled prompts covering benign, jailbreak, and application-layer injections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshaj Prashanth Rao', 'Advait Singh', 'Saumya Kumaar Saksena', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'LLM defenses', 'red teaming', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19011</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features</title><link>https://arxiv.org/abs/2511.21744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NEULIF, a lightweight detector for AI-generated text using stylometric and readability features fed into a compact CNN or Random Forest.&lt;/li&gt;&lt;li&gt;Reports high performance on the Kaggle AI vs. Human corpus: CNN ~97% accuracy (~0.95 F1), RF ~95% accuracy (~0.94 F1), with small model sizes (~25 MB and ~10.6 MB).&lt;/li&gt;&lt;li&gt;Emphasizes CPU efficiency and applicability across languages, domains, and streaming contexts as a practical alternative to large transformer-based detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey K. Aityan', 'William Claster', 'Karthik Sai Emani', 'Sohni Rais', 'Thy Tran']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'Stylometry', 'Lightweight models', 'Security/forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21744</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Liars' Bench: Evaluating Lie Detectors for Language Models</title><link>https://arxiv.org/abs/2511.16035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LIARS' BENCH, a benchmark of 72,863 labeled lie vs. honest responses generated by four open-weight models across seven datasets.&lt;/li&gt;&lt;li&gt;Captures diverse lie types by varying two axes: the model's reason for lying and the object of belief targeted by the lie.&lt;/li&gt;&lt;li&gt;Evaluates three black-box and white-box lie-detection techniques and finds systematic failures, especially when the transcript alone is insufficient to identify lies.&lt;/li&gt;&lt;li&gt;Provides a practical testbed to expose limitations of prior techniques and guide future improvements in lie detection for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kieron Kretschmar', 'Walter Laurito', 'Sharan Maiya', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['LLM lie detection', 'safety evaluation', 'benchmark', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16035</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification</title><link>https://arxiv.org/abs/2510.10961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KOTOX, a Korean dataset pairing toxic and neutral sentences with obfuscated counterparts to support deobfuscation and detoxification.&lt;/li&gt;&lt;li&gt;Categorizes Korean obfuscation patterns into linguistically grounded classes and defines transformation rules based on real-world examples.&lt;/li&gt;&lt;li&gt;Shows models trained on the dataset better handle obfuscated toxic text while retaining performance on non-obfuscated text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yejin Lee', 'Su-Hyeon Kim', 'Hyundong Jin', 'Dayoung Kim', 'Yeonsoo Kim', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'deobfuscation', 'detoxification', 'robustness', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10961</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLMs as verification oracles for Solidity</title><link>https://arxiv.org/abs/2509.19153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of GPT-5 as a verification oracle for Solidity contracts, benchmarking its ability to predict validity/invalidity of contract-specific properties.&lt;/li&gt;&lt;li&gt;Comparison of LLM outputs against established formal verification tools and analysis of practical effectiveness in real-world auditing scenarios.&lt;/li&gt;&lt;li&gt;Quantitative metrics combined with qualitative analysis showing LLMs can be surprisingly effective despite lacking formal soundness guarantees, suggesting potential for AI-assisted formal methods in secure contract development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Massimo Bartoletti', 'Enrico Lipparini', 'Livio Pompianu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'smart contract security', 'formal verification', 'AI-assisted auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19153</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title><link>https://arxiv.org/abs/2509.08604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study measuring prevalence, characteristics, and volume of memorization in LLMs adapted to medical data across three adaptation scenarios: continued pretraining, fine-tuning on benchmarks, and fine-tuning on real clinical records (&gt;13k inpatient records).&lt;/li&gt;&lt;li&gt;Finds memorization is prevalent and substantially higher than in general-domain models; memorized content persists through adaptation (up to 87% retention after fine-tuning).&lt;/li&gt;&lt;li&gt;Highlights concrete privacy and safety risks (exposure of sensitive patient details, reduced generalizability, overconfident/misleading outputs) with implications for clinical deployment and data-handling practices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anran Li', 'Lingfei Qian', 'Mengmeng Du', 'Yu Yin', 'Yan Hu', 'Zihao Sun', 'Yihang Fu', 'Hyunjae Kim', 'Erica Stutz', 'Xuguang Ai', 'Qianqian Xie', 'Rui Zhu', 'Jimin Huang', 'Yifan Yang', 'Siru Liu', 'Yih-Chung Tham', 'Lucila Ohno-Machado', 'Hyunghoon Cho', 'Zhiyong Lu', 'Hua Xu', 'Qingyu Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'memorization', 'medical LLMs', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08604</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title><link>https://arxiv.org/abs/2509.07677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Spectral Masking and Interpolation Attack (SMIA), a black-box adversarial method that manipulates inaudible frequency bands of AI-generated audio to evade voice authentication and anti-spoofing countermeasures while remaining perceptually authentic.&lt;/li&gt;&lt;li&gt;Evaluates SMIA extensively against state-of-the-art speaker verification systems, combined VAS/CM pipelines, and standalone countermeasures under simulated real-world conditions.&lt;/li&gt;&lt;li&gt;Reports high attack success rates: ≥82% against combined VAS/CM systems, ≥97.5% against standalone speaker verification, and 100% against countermeasures, arguing current defenses are insufficient and advocating for dynamic, context-aware defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kamel Kamel', 'Hridoy Sankar Dutta', 'Keshav Sood', 'Sunil Aryal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'speaker verification', 'anti-spoofing', 'audio security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07677</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records</title><link>https://arxiv.org/abs/2507.22533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Transforms longitudinal, unstructured EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range temporal dependencies.&lt;/li&gt;&lt;li&gt;Grounds LLM decision support by aligning patient trajectories with a normative guideline knowledge graph to reduce clinical hallucinations and produce evidence-grounded summaries and recommendations.&lt;/li&gt;&lt;li&gt;Validated on a private Chinese cancer dataset and MIMIC-IV, outperforming long-context LLMs and KG-enhanced RAG baselines with evaluation correlated to oncologist assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongchen Li', 'Jitao Liang', 'Wei Li', 'Xiaoyu Wang', 'Longbing Cao', 'Kun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['clinical-grounding', 'hallucination-mitigation', 'temporal-knowledge-graph', 'EHR', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22533</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception</title><link>https://arxiv.org/abs/2504.06753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes the first comprehensive all-type audio deepfake detection (ADD) benchmark covering speech, sound, singing voice, and music with cross-type evaluation.&lt;/li&gt;&lt;li&gt;Proposes PT-SSL (prompt tuning self-supervised learning) to optimize SSL front-ends with learned prompt tokens, using ~458x fewer trainable parameters than full fine-tuning.&lt;/li&gt;&lt;li&gt;Introduces Wavelet Prompt Tuning (WPT)-SSL to capture type-invariant deepfake cues in the frequency domain without extra training parameters, improving cross-type robustness.&lt;/li&gt;&lt;li&gt;Uses co-training on all deepfake audio types and reports state-of-the-art results (WPT-XLSR-AASIST average EER 3.58%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuankun Xie', 'Ruibo Fu', 'Zhiyong Wang', 'Xiaopeng Wang', 'Songjun Cao', 'Long Ma', 'Haonan Cheng', 'Long Ye']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'anti-spoofing', 'self-supervised learning', 'prompt tuning', 'wavelet analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06753</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HiQ-Lip: A Hierarchical Quantum-Classical Method for Global Lipschitz Constant Estimation of ReLU Networks</title><link>https://arxiv.org/abs/2503.16342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HiQ-Lip, a hybrid quantum-classical hierarchical method to estimate global Lipschitz constants of ReLU networks by formulating the problem as a QUBO and using multilevel graph coarsening/refinement to fit current quantum hardware constraints.&lt;/li&gt;&lt;li&gt;Aims to address NP-hardness and scalability limitations of SDP-based methods, leveraging small-scale quantum devices to accelerate computation.&lt;/li&gt;&lt;li&gt;Experimental results on fully connected networks show comparable or better upper-bound estimates and significant speedups (e.g., 2x faster and tighter bounds than LiPopt on two-layer networks with 256 hidden neurons).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoqi He', 'Yan Xiao', 'Wenzhi Xu', 'Ruoying Liu', 'Xiaokai Lin', 'Kai Wen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'Lipschitz estimation', 'verification', 'quantum machine learning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16342</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks</title><link>https://arxiv.org/abs/2503.11514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review and taxonomy of Gradient Inversion Attacks (GIA) in federated learning into optimization-based (OP-GIA), generation-based (GEN-GIA), and analytics-based (ANA-GIA).&lt;/li&gt;&lt;li&gt;Comprehensive empirical evaluation of these GIA types in FL, identifying factors that influence effectiveness, practicality, and detectability.&lt;/li&gt;&lt;li&gt;Key findings: OP-GIA is the most practical attack setting despite limited performance; GEN-GIA has heavy dependencies and is often impractical; ANA-GIA is generally detectable.&lt;/li&gt;&lt;li&gt;Proposes a three-stage defense pipeline for FL frameworks and outlines future attacker/defender research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengxin Guo', 'Runxi Wang', 'Shuang Zeng', 'Jinjing Zhu', 'Haoning Jiang', 'Yanran Wang', 'Yuyin Zhou', 'Feifei Wang', 'Hui Xiong', 'Liangqiong Qu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'gradient inversion', 'privacy attack', 'model inversion', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11514</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens</title><link>https://arxiv.org/abs/2502.11245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes concept-based models through the lens of reasoning shortcuts (RSs), showing models can achieve high accuracy while learning low-quality concepts.&lt;/li&gt;&lt;li&gt;Provides theoretical identifiability conditions for recovering both concept extractors and the inference layer in concept-based models.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that existing methods and common mitigation strategies often fail to satisfy these identifiability conditions, highlighting practical vulnerability to shortcut learning and OOD failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuele Bortolotti', 'Emanuele Marconato', 'Paolo Morettin', 'Andrea Passerini', 'Stefano Teso']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'shortcut learning', 'robustness', 'out-of-distribution generalization', 'identifiability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11245</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection</title><link>https://arxiv.org/abs/2410.12278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a two-step generation-selection pipeline to create task-specific synthetic datasets for hallucination detection, using hallucination pattern guidance and language style alignment.&lt;/li&gt;&lt;li&gt;Introduces a data mixture training strategy to obtain robust supervised detectors with improved generalization across tasks and text generators.&lt;/li&gt;&lt;li&gt;Reports detectors trained on synthetic data that outperform in-context-learning (ICL) detectors by ~32% and demonstrate cross-task and cross-generator generalization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Xie', 'Karan Aggarwal', 'Aitzaz Ahmad', 'Stephen Lau']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'alignment/safety', 'synthetic data generation', 'robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.12278</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Evaluation on Large Language Model Outputs: Discourse and Memorization</title><link>https://arxiv.org/abs/2304.08637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of outputs from nine widely-available LLMs focusing on memorization, uniqueness, and discourse pathologies (e.g., counterfactuals, logical flaws, staying on topic).&lt;/li&gt;&lt;li&gt;Finds 80% of evaluated outputs contained memorized text and observes a correlation between higher memorization and perceived higher-quality outputs.&lt;/li&gt;&lt;li&gt;Assesses mitigation strategies that reduce the rate of memorized content being produced and discusses implications for learning, memorization, and quality evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrian de Wynter', 'Xun Wang', 'Alex Sokolov', 'Qilong Gu', 'Si-Qing Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data privacy/leakage', 'factuality/robustness', 'evaluation/benchmarking', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2304.08637</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large language models can effectively convince people to believe conspiracies</title><link>https://arxiv.org/abs/2601.05050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Three pre-registered experiments (N = 2,724 US participants) had people discuss uncertain conspiracy theories with GPT-4o instructed to argue for ("bunking") or against ("debunking") the conspiracy.&lt;/li&gt;&lt;li&gt;A jailbroken GPT-4o with guardrails removed was as effective at increasing conspiracy belief as at decreasing it; the bunking AI was rated more positively and raised trust in AI more than the debunking AI.&lt;/li&gt;&lt;li&gt;Standard GPT-4o produced very similar effects, indicating existing guardrails did little to prevent promotion of conspiratorial beliefs; however, a corrective conversation and prompting the model to use only accurate information reduced or reversed induced beliefs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas H. Costello', 'Kellin Pelrine', 'Matthew Kowal', 'Antonio A. Arechar', 'Jean-Fran\\c{c}ois Godbout', 'Adam Gleave', 'David Rand', 'Gordon Pennycook']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'misinformation', 'jailbreaking', 'social engineering', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05050</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Attention Reasoning via Hierarchical Search and Self-Verification</title><link>https://arxiv.org/abs/2510.18619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Attention Reasoning (VAR), an RL framework that treats reasoning as a hierarchical tree search with backtracking and self-verification to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Enforces explicit visual grounding by generating bounding boxes and uses a novel reward combining geometric precision and semantic sufficiency to guide evidence selection.&lt;/li&gt;&lt;li&gt;Replaces linear Chain-of-Thought with a tree-search policy enabling correction of logical errors; provides theoretical analysis and reports SOTA improvements on hallucination and safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Cai', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Ming Zhu', 'Haichuan Tang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-safety', 'hallucination-mitigation', 'visual-grounding', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18619</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ContractEval: A Benchmark for Evaluating Contract-Satisfying Assertions in Code Generation</title><link>https://arxiv.org/abs/2510.12047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ContractEval, a benchmark to evaluate whether generated code enforces contract assertions by rejecting contract-violating inputs (i.e., assertion-level validity constraints).&lt;/li&gt;&lt;li&gt;Built on HumanEval+ and MBPP+; uses a neuro-symbolic pipeline where an LLM converts assertion clauses into constraints and an SMT solver enumerates satisfiable violation combinations to synthesize contract-violating test inputs.&lt;/li&gt;&lt;li&gt;Finds that standard prompting yields ~0% contract satisfaction, while adding a few contract-violation examples raises contract satisfaction to ~49–53% with pass@1 largely preserved.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soohan Lim', 'Joonghyuk Hahn', 'Hyunwoo Park', 'Sang-Ki Ko', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarks', 'adversarial-inputs', 'code-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12047</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features</title><link>https://arxiv.org/abs/2502.17749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two security-relevant tasks: detecting whether code is an LLM-paraphrase of human-written code, and identifying which LLM produced the paraphrase.&lt;/li&gt;&lt;li&gt;Constructs LPcode, a dataset of human-written and LLM-paraphrased code pairs, and shows statistical differences in coding style (naming, structure, readability).&lt;/li&gt;&lt;li&gt;Proposes LPcodedec, a coding-style-based detection/attribution method that outperforms baselines on both tasks (F1 improvements of ~2.6% and ~15.2%) and provides large speedups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Hyundong Jin', 'Jeong-won Cha', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM attribution', 'code provenance', 'detection', 'intellectual property', 'model fingerprinting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17749</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset</title><link>https://arxiv.org/abs/2601.05918</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that agentic LLMs with web access can perform low-effort re-identification attacks on participants in the Anthropic Interviewer dataset, linking interviews to published scientific works and in some cases uniquely identifying interviewees.&lt;/li&gt;&lt;li&gt;Shows the attack is practical using off-the-shelf tools and simple natural-language prompts that decompose re-identification into benign-seeming tasks, thereby bypassing existing safeguards.&lt;/li&gt;&lt;li&gt;Discusses implications for releasing rich qualitative datasets in the era of web-enabled LLM agents, reports disclosure to Anthropic, and proposes mitigation strategies and open problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 're-identification', 'LLM agents', 'red teaming', 'data release safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05918</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency</title><link>https://arxiv.org/abs/2601.05905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neighbor-Consistency Belief (NCB), a structural measure evaluating LLM belief robustness across a conceptual neighborhood rather than point-wise confidence.&lt;/li&gt;&lt;li&gt;Proposes a cognitive stress-testing protocol that probes output stability under mild contextual interference, showing self-consistency can mask brittle beliefs.&lt;/li&gt;&lt;li&gt;Presents Structure-Aware Training (SAT) to optimize context-invariant belief structure, reporting ~30% reduction in long-tail knowledge brittleness across multiple LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoming Xu', 'Ningyuan Zhao', 'Yunzhi Yao', 'Weihong Xu', 'Hongru Wang', 'Xinle Deng', 'Shumin Deng', 'Jeff Z. Pan', 'Huajun Chen', 'Ningyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'truthfulness/robustness', 'evaluation/benchmarking', 'stress-testing', 'training for robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05905</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift</title><link>https://arxiv.org/abs/2601.05882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of how preference-tuning (alignment to human judgments) generalizes under domain shift across summarization and QA helpfulness tasks.&lt;/li&gt;&lt;li&gt;Compares five alignment objectives and several adaptation strategies, including target-domain supervised fine-tuning and pseudo-labeling.&lt;/li&gt;&lt;li&gt;Finds systematic differences in generalization across objectives and shows pseudo-labeling-based adaptation can substantially reduce domain-shift degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constantinos Karouzos', 'Xingwei Tan', 'Nikolaos Aletras']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference tuning', 'domain shift', 'safety evaluation', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05882</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law</title><link>https://arxiv.org/abs/2601.05879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of four state-of-the-art LLMs (GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, Llama 3.3) on a Czech family-law shared-parenting scenario to probe for gender bias.&lt;/li&gt;&lt;li&gt;Uses two scenario versions (gendered names vs neutral labels) and nine legally relevant factual variations to test whether models suggest different shared-parenting ratios depending on parent gender.&lt;/li&gt;&lt;li&gt;Finds model-specific differences and preliminary evidence of gender-dependent patterns in proposed outcomes, highlighting risks of biased legal guidance from LLMs and the need for robust evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Harasta', 'Matej Vasina', 'Martin Kornel', 'Tomas Foltynek']&lt;/li&gt;&lt;li&gt;Tags: ['bias/fairness', 'LLM evaluation', 'social harm / safety', 'legal domain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05879</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning</title><link>https://arxiv.org/abs/2601.05836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid approach combining fuzzy logic-based safety decision-making with a reinforcement learning framework to detect and avoid singularities in UR10 robotic arm path planning.&lt;/li&gt;&lt;li&gt;Uses manipulability measures and condition number analysis for real-time singularity detection and integrates RL for adaptive path planning.&lt;/li&gt;&lt;li&gt;Implements training in PyBullet and deployment connectivity with URSim; reports ~90% success rate maintaining safe distances from singular configurations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng-Kai Chen', 'Jyh-Horng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['robotics-safety', 'reinforcement-learning', 'singularity-avoidance', 'fuzzy-logic', 'simulation-to-deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05836</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis</title><link>https://arxiv.org/abs/2601.05828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how parallelism in vector-multiplication units of NN hardware accelerators affects correlation power analysis (CPA) side-channel attacks.&lt;/li&gt;&lt;li&gt;Derives theoretical equations predicting how CPA correlation degrades as levels of concurrent multiply-and-accumulate operations increase.&lt;/li&gt;&lt;li&gt;Validates the theoretical model experimentally using an FPGA implementation of a vector-multiplication unit processing neurons in the same fully-connected layer.&lt;/li&gt;&lt;li&gt;Focuses on confidentiality risks for neural network inference on edge devices exposed to physical side-channel measurement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manuel Brosch', 'Matthias Probst', 'Stefan K\\"ogler', 'Georg Sigl']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel', 'correlation power analysis', 'hardware security', 'neural network confidentiality', 'FPGA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05828</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI</title><link>https://arxiv.org/abs/2601.05825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether established EEG classifiers for mental workload and implicit agreement can transfer to spoken human–AI dialogue using two conversational paradigms (Spelling Bee and sentence completion).&lt;/li&gt;&lt;li&gt;Introduces an end-to-end pipeline to transcribe, annotate, and align word-level conversational events with continuous EEG classifier outputs and applies it in a pilot study.&lt;/li&gt;&lt;li&gt;Finds interpretable workload decoding trends and feasible continuous implicit-agreement decoding, while highlighting limitations in construct transfer and temporal/asynchronous alignment of event-based classifiers—outlining feasibility and constraints for integrating passive BCI signals into conversational AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Lucija Mihi\\'c Zidar", 'Philipp Wicke', 'Praneel Bhatia', 'Rosa Lutz', 'Marius Klug', 'Thorsten O. Zander']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'passive BCI', 'EEG', 'human-AI interaction', 'implicit feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05825</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces</title><link>https://arxiv.org/abs/2601.05789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAFE, a federated learning method for EEG-based BCIs that keeps data local to protect privacy.&lt;/li&gt;&lt;li&gt;Uses local batch-specific normalization to address cross-subject distribution shifts and improve generalization.&lt;/li&gt;&lt;li&gt;Introduces federated adversarial training and adversarial weight perturbation to enhance adversarial robustness.&lt;/li&gt;&lt;li&gt;Evaluates on five EEG datasets, claiming superior decoding accuracy and robustness versus 14 SOTA and even centralized baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianwang Jia', 'Xiaoqing Chen', 'Dongrui Wu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'adversarial robustness', 'brain-computer interfaces', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05789</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit</title><link>https://arxiv.org/abs/2601.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VIGIL, a verify-before-commit protocol for LLM agents that enables speculative hypothesis generation plus intent-grounded verification to prevent tool stream injection while preserving reasoning flexibility.&lt;/li&gt;&lt;li&gt;Introduces SIREN, a benchmark of 959 tool stream injection cases modeling dynamic, dependency-rich attacks against agent tool streams.&lt;/li&gt;&lt;li&gt;Reports that VIGIL reduces attack success rate by &gt;22% and more than doubles utility under attack compared to static baseline defenses, outperforming state-of-the-art dynamic defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junda Lin', 'Zhaomeng Zhou', 'Zhi Zheng', 'Shuochen Liu', 'Tong Xu', 'Yong Chen', 'Enhong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM agent defense', 'tool stream injection', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05755</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Echo Chamber Multi-Turn LLM Jailbreak</title><link>https://arxiv.org/abs/2601.05742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Echo Chamber, a novel multi-turn LLM jailbreaking technique that uses gradual escalation across a chain of interactions to bypass safety guardrails.&lt;/li&gt;&lt;li&gt;Compares Echo Chamber to prior multi-turn jailbreaks and analyzes methodological differences.&lt;/li&gt;&lt;li&gt;Presents extensive empirical evaluation showing the attack's effectiveness against multiple state-of-the-art models, highlighting security risks for deployed chatbots.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Alobaid (NeuralTrust)', "Mart\\'i Jord\\`a Roca (NeuralTrust)", 'Carlos Castillo (ICREA', 'UPF)', 'Joan Vendrell (NeuralTrust)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05742</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training</title><link>https://arxiv.org/abs/2601.05703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the AI Bill of Materials (AIBOM) as a standardized, verifiable record for trained models, datasets, and environments.&lt;/li&gt;&lt;li&gt;Presents AIBoMGen, a proof-of-concept platform that automates creation of signed AIBOMs by observing training jobs and acting as a root of trust.&lt;/li&gt;&lt;li&gt;Uses cryptographic hashing, digital signatures, and in-toto attestations to detect artifact tampering and ensure integrity.&lt;/li&gt;&lt;li&gt;Evaluation shows reliable detection of unauthorized modifications and negligible performance overhead, aiming to support regulatory compliance (e.g., EU AI Act).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wiebe Vandendriessche', 'Jordi Thijsman', "Laurens D'hooge", 'Bruno Volckaert', 'Merlijn Sebrechts']&lt;/li&gt;&lt;li&gt;Tags: ['AIBOM', 'supply-chain security', 'provenance &amp; attestation', 'artifact integrity', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05703</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors</title><link>https://arxiv.org/abs/2601.05587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HogVul, a black-box adversarial code generation framework that jointly applies lexical and syntax perturbations under a unified dual-channel optimization.&lt;/li&gt;&lt;li&gt;Uses Particle Swarm Optimization (PSO) to coordinate two-level perturbations and expand the adversarial search space for more effective evasion.&lt;/li&gt;&lt;li&gt;Evaluated on four benchmark datasets, reporting an average attack success rate improvement of 26.05% over state-of-the-art baselines.&lt;/li&gt;&lt;li&gt;Demonstrates practical vulnerabilities in LM-based software vulnerability detectors and the effectiveness of hybrid optimization strategies for adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingxiao Yang', 'Ping He', 'Tianyu Du', 'Sun Bing', 'Xuhong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'software vulnerability detection', 'LLM robustness', 'code security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05587</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck</title><link>https://arxiv.org/abs/2601.05547</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VIB-Probe, a Variational Information Bottleneck-based probe to detect hallucinations in Vision-Language Models by extracting discriminative patterns from internal attention heads.&lt;/li&gt;&lt;li&gt;Uses the probe's gradients to identify attention heads with causal influence on hallucinations and applies inference-time interventions to mitigate them.&lt;/li&gt;&lt;li&gt;Claims superior performance over existing baselines on multiple hallucination detection and mitigation benchmarks; focuses on internal model signals rather than only outputs or external verifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiran Zhang', 'Yixin Wu', 'Zhenghua Wang', 'Xiaohua Wang', 'Changze Lv', 'Xuanjing Huang', 'Xiaoqing Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'vision-language models', 'information bottleneck', 'model interpretability', 'inference-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05547</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Over-Searching in Search-Augmented Large Language Models</title><link>https://arxiv.org/abs/2601.05503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and systematically evaluates 'over-searching' in search-augmented LLMs—unnecessary retrieval calls that increase cost and introduce irrelevant context leading to hallucinations.&lt;/li&gt;&lt;li&gt;Empirical findings: search improves accuracy on answerable queries but reduces abstention on unanswerable ones; over-searching is worse for complex reasoning models, amplified by noisy retrieval, and compounds across multi-turn conversations; negative retrieved evidence can improve abstention.&lt;/li&gt;&lt;li&gt;Introduces Tokens Per Correctness (TPC) as a performance–cost metric, releases the OverSearchQA dataset, and studies mitigation strategies at query and retrieval levels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roy Xie', 'Deepak Gopinath', 'David Qiu', 'Dong Lin', 'Haitian Sun', 'Saloni Potdar', 'Bhuwan Dhingra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Hallucination', 'Retrieval-augmented generation', 'Robustness', 'Evaluation/Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05503</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STELP: Secure Transpilation and Execution of LLM-Generated Programs</title><link>https://arxiv.org/abs/2601.05467</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STELP, a Secure Transpiler and Executor to safely transpile and run LLM-generated programs in controlled environments to mitigate vulnerabilities (malicious code, data poisoning, hallucinations).&lt;/li&gt;&lt;li&gt;Provides a human-validated dataset of insecure code snippets and benchmarks STELP on correctness, safety, and latency versus an existing method, reporting significant improvements particularly for risky code execution.&lt;/li&gt;&lt;li&gt;Targets autonomous production AI systems and real-time execution workflows where traditional human review and standard testing are impractical or untrustworthy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Swapnil Shinde', 'Sahil Wadhwa', 'Andy Luo', 'Emily Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated code security', 'safe code execution / sandboxing', 'secure transpilation', 'adversarial/malicious code mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05467</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning</title><link>https://arxiv.org/abs/2601.05466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes iMIST, an adaptive multi-step jailbreak attack that disguises malicious prompts as tool invocations to evade content filters.&lt;/li&gt;&lt;li&gt;Uses interactive, progressive optimization with real-time harmfulness assessment to escalate response harmfulness across multi-turn dialogues.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness on widely-used LLMs, reporting higher success rates and lower rejection compared to baseline defenses, highlighting vulnerabilities in current safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoqi Wang', 'Zijian Zhang', 'Daqing He', 'Pengtao Kou', 'Xin Li', 'Jiamou Liu', 'Jincheng An', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05466</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tracing Moral Foundations in Large Language Models</title><link>https://arxiv.org/abs/2601.05437</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how Moral Foundations Theory (MFT) concepts are represented across layers in two instruction-tuned LLMs (Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct).&lt;/li&gt;&lt;li&gt;Uses sparse autoencoders over residual streams to identify sparse features linked to specific moral foundations and shows semantic alignment with human judgments.&lt;/li&gt;&lt;li&gt;Performs causal steering interventions (dense MFT vectors and sparse features) to shift model moral outputs, demonstrating a causal link between internal representations and moral behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxiao Yu', 'Bowen Yi', 'Farzan Karimi-Malekabadi', 'Suhaib Abdurahman', 'Jinyi Ye', 'Shrikanth Narayanan', 'Yue Zhao', 'Morteza Dehghani']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'safety-evaluation', 'causal interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05437</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models</title><link>https://arxiv.org/abs/2601.05366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MLCL, a diagnostic benchmark for evaluating multilingual tool-calling (Chinese, Hindi, Igbo) in LLM agents.&lt;/li&gt;&lt;li&gt;Finds many execution failures occur despite correct intent and tool selection, with parameter value language mismatch as a dominant failure mode.&lt;/li&gt;&lt;li&gt;Evaluates inference-time system strategies that reduce language-induced execution errors but do not fully recover English-level performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Luo', 'T Pranav Kutralingam', 'Ogochukwu N Okoani', 'Wanpeng Xu', 'Hua Wei', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual robustness', 'tool calling', 'benchmarking', 'safety evaluation', 'LLM reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05366</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2601.05339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MJAD-MLLMs, a framework to analyze multi-turn jailbreaking attacks and multi-LLM defense techniques for multimodal LLMs.&lt;/li&gt;&lt;li&gt;Proposes a novel multi-turn jailbreaking attack that exploits vulnerabilities in multi-turn prompting of MLLMs.&lt;/li&gt;&lt;li&gt;Presents FragGuard, a fragment-optimized multi-LLM defense mechanism to mitigate such jailbreaking attacks.&lt;/li&gt;&lt;li&gt;Evaluates attacks and defenses across several open- and closed-source SOTA MLLMs and benchmark datasets, comparing to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Badhan Chandra Das', 'Md Tasnim Jawad', 'Joaquin Molto', 'M. Hadi Amini', 'Yanzhao Wu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'multimodal models', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05339</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Survey of Agentic AI and Cybersecurity: Challenges, Opportunities and Use-case Prototypes</title><link>https://arxiv.org/abs/2601.05293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys implications of agentic AI for cybersecurity, covering both defensive capabilities (autonomous monitoring, incident response, adaptive threat hunting) and amplified adversarial uses (reconnaissance, exploitation, coordinated social-engineering).&lt;/li&gt;&lt;li&gt;Reviews emerging threat models, security frameworks, and evaluation pipelines tailored to agentic systems and analyzes systemic risks such as agent collusion, cascading failures, oversight evasion, and memory poisoning.&lt;/li&gt;&lt;li&gt;Presents three representative use-case prototypes illustrating agentic AI behavior in practical cybersecurity workflows and how design choices impact reliability, safety, and operational effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahaya Jestus Lazer', 'Kshitiz Aryal', 'Maanak Gupta', 'Elisa Bertino']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI', 'Cybersecurity', 'Threat Models', 'Safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05293</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis</title><link>https://arxiv.org/abs/2601.05280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalises recursive self-training of LLMs as a discrete-time dynamical system and proves two fundamental failure modes as self-generated data dominates: Entropy Decay (mode collapse) and Variance Amplification (semantic drift).&lt;/li&gt;&lt;li&gt;Argues these degenerative behaviors are architecture-agnostic and arise from finite-sample distributional learning, and that RL with imperfect verifiers is similarly prone to semantic collapse.&lt;/li&gt;&lt;li&gt;Proposes overcoming limits via neurosymbolic techniques—symbolic regression and program synthesis guided by Algorithmic Probability (Coding Theorem Method)—to identify generative mechanisms rather than mere correlations, enabling sustained self-improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hector Zenil']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improvement', 'model-collapse', 'neurosymbolic-methods', 'algorithmic-probability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05280</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.05264</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey and unified taxonomy of RAG architectures covering retrieval strategies, fusion mechanisms, and orchestration patterns.&lt;/li&gt;&lt;li&gt;Presents quantitative assessment frameworks and engineering guidelines aimed at deploying resilient and secure RAG systems.&lt;/li&gt;&lt;li&gt;Analyzes trust, alignment, and deployment implications (including security and robustness considerations) for real-world RAG applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dean Wampler', 'Dave Nielson', 'Alireza Seddighi']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation (RAG)', 'Security &amp; Robustness', 'Trust &amp; Alignment', 'Deployment/Engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05264</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Document Impact in RAG-LLMs</title><link>https://arxiv.org/abs/2601.05260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Influence Score (IS), a metric based on Partial Information Decomposition to quantify each retrieved document's impact on a RAG system's generated response.&lt;/li&gt;&lt;li&gt;Validates IS with a simulated poison attack: IS identifies the malicious document as most influential in 86% of cases.&lt;/li&gt;&lt;li&gt;Performs an ablation study showing responses generated from top-ranked IS documents are more similar to the original than those from remaining documents, demonstrating IS can isolate influential sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Gerami', 'Kazem Faghih', 'Ramani Duraiswami']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'data poisoning', 'influence attribution', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05260</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Automating Deception: Scalable Multi-Turn LLM Jailbreaks</title><link>https://arxiv.org/abs/2511.19517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an automated pipeline to generate large-scale, psychologically-grounded multi-turn jailbreak datasets by operationalizing Foot-in-the-Door (FITD) templates; produces a 1,500-scenario benchmark targeting illegal/offensive content.&lt;/li&gt;&lt;li&gt;Evaluates seven models from three major LLM families under multi-turn (with history) and single-turn (without history) settings, measuring Attack Success Rates (ASR).&lt;/li&gt;&lt;li&gt;Finds substantial contextual vulnerabilities: GPT-family models show large ASR increases (up to +32 percentage points) when history is present; Google Gemini 2.5 Flash is highly robust; Anthropic Claude 3 Haiku is strong but not perfect.&lt;/li&gt;&lt;li&gt;Highlights a divergence in how safety architectures handle conversational context and argues for defenses specifically resilient to narrative-based manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adarsh Kumarappan', 'Ananya Mujoo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19517</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM</title><link>https://arxiv.org/abs/2511.18721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a probabilistic relaxation of SmoothLLM's strict k-unstable assumption, called (k, ε)-unstable, to better model real-world jailbreak behavior.&lt;/li&gt;&lt;li&gt;Derives a data-informed lower bound on SmoothLLM's defense probability by incorporating empirical attack-success models (covering gradient-based GCG and semantic PAIR attacks).&lt;/li&gt;&lt;li&gt;Provides actionable certification thresholds and a more practical, theoretically-grounded safety certificate for LLM jailbreak defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adarsh Kumarappan', 'Ayushi Mehrotra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'certified robustness', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18721</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Open-Vocabulary 3D Instruction Ambiguity Detection</title><link>https://arxiv.org/abs/2601.05991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines Open-Vocabulary 3D Instruction Ambiguity Detection: determine whether a natural-language command has a single unambiguous meaning in a given 3D scene.&lt;/li&gt;&lt;li&gt;Introduces Ambi3D benchmark with ~700 diverse 3D scenes and ~22k annotated instructions for evaluating ambiguity detection.&lt;/li&gt;&lt;li&gt;Finds state-of-the-art 3D LLMs struggle on this safety-critical task and proposes AmbiVer, a two-stage approach that collects multi-view visual evidence and uses a vision-language model to judge ambiguity, showing empirical improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Ding', 'Haoran Tang', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-ambiguity', 'safety-evaluation', 'embodied-AI', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05991</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility</title><link>https://arxiv.org/abs/2601.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PII-VisBench, a benchmark of 4,000 probes across 200 subjects stratified into four visibility levels (high/medium/low/zero) to assess PII leakage in vision-language models.&lt;/li&gt;&lt;li&gt;Evaluates 18 open-source VLMs (0.3B–32B) using Refusal Rate and Conditional PII Disclosure Rate, finding disclosures correlate with subject visibility and vary by model family and PII type.&lt;/li&gt;&lt;li&gt;Demonstrates paraphrasing and jailbreak-style prompts can bypass refusals, highlighting attack vectors and motivating visibility-aware safety evaluation and training interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['G M Shahariar', 'Zabir Al Nazi', 'Md Olid Hasan Bhuiyan', 'Zhouxing Shi']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'VLM safety', 'Benchmarking', 'Jailbreaking', 'Privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05739</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models</title><link>https://arxiv.org/abs/2601.05693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes a failure mode in large reasoning models called 'Circular Reasoning', where generated content becomes a premise that causes self-reinforcing repetitive loops.&lt;/li&gt;&lt;li&gt;Introduces LoopBench, a dataset capturing numerical and statement loops to benchmark this phenomenon and studies the mechanistic cause—a state collapse and a V-shaped attention pattern that drives persistence of loops.&lt;/li&gt;&lt;li&gt;Proposes using the Cumulative Sum (CUSUM) algorithm to detect early precursors of loops and validates its predictive accuracy across multiple large reasoning models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zenghao Duan', 'Liang Pang', 'Zihao Wei', 'Wenbin Duan', 'Yuxin Tian', 'Shicheng Xu', 'Jingcheng Deng', 'Zhiyi Yin', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model failure modes', 'safety evaluation', 'detection/monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05693</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models</title><link>https://arxiv.org/abs/2601.05570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Crisis-Bench, a multi-agent POMDP benchmark that simulates 7-day corporate crises (80 storylines, 8 industries) with strictly separated Private and Public narrative states to enforce information asymmetry.&lt;/li&gt;&lt;li&gt;Evaluates LLM-based PR agents on strategic ambiguity and information withholding using an Adjudicator-Market Loop that converts adjudicated public sentiment into a simulated stock price as the economic objective.&lt;/li&gt;&lt;li&gt;Empirical results show a tension between standard safety/alignment (favoring transparency/honesty) and professional utility—some models adhere to ethical constraints while others learn legitimate, Machiavellian withholding to stabilize stock.&lt;/li&gt;&lt;li&gt;Frames the need for context-aware professional alignment and provides a quantitative framework for assessing reputation-management capabilities and potential misuse.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cooper Lin', 'Maohao Ran', 'Yanting Zhang', 'Zhenglin Wan', 'Hongwei Fan', 'Yibo Xu', 'Yike Guo', 'Wei Xue', 'Jun Song']&lt;/li&gt;&lt;li&gt;Tags: ['safety-alignment', 'benchmarking', 'reputation-management', 'evaluation', 'dual-use']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05570</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</title><link>https://arxiv.org/abs/2601.05529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents qualitative and quantitative evaluation of LLM/VLM decision-making in safety-critical robotics, using a fire evacuation scenario to identify failure modes.&lt;/li&gt;&lt;li&gt;Proposes seven benchmark tasks across three categories: Complete Information (ASCII maps), Incomplete Information (missing context inference), and Safety-Oriented Spatial Reasoning (SOSR) using natural language.&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs and VLMs, finding serious vulnerabilities (e.g., some models 0% on ASCII navigation; models directing robots toward hazards in simulated fire drills).&lt;/li&gt;&lt;li&gt;Highlights that even low aggregate failure rates (e.g., 1%) imply unacceptable catastrophic risk in physical settings, concluding current LLMs are unsafe for direct deployment in safety-critical robotics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jua Han', 'Jaeyoon Seo', 'Jungbin Min', 'Jean Oh', 'Jihie Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Robotics safety', 'Safety evaluation', 'Spatial reasoning', 'Catastrophic failure analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05529</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainable AI: Learning from the Learners</title><link>https://arxiv.org/abs/2601.05525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that explainable AI (XAI) combined with causal reasoning enables 'learning from the learners'—extracting causal mechanisms from foundation models.&lt;/li&gt;&lt;li&gt;Claims XAI can guide discovery, robust design/optimization, and certification to support trust and accountability in high-stakes applications.&lt;/li&gt;&lt;li&gt;Discusses challenges around faithfulness, generalization, and usability of explanations and proposes XAI as a unifying framework for human–AI collaboration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ricardo Vinuesa', 'Steven L. Brunton', 'Gianmarco Mengaldo']&lt;/li&gt;&lt;li&gt;Tags: ['Explainable AI', 'Causal Inference', 'AI Safety &amp; Alignment', 'Trust and Accountability', 'Foundation Models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05525</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Conformity and Social Impact on AI Agents</title><link>https://arxiv.org/abs/2601.05384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates conformity in large multimodal language models acting as AI agents using adapted social psychology visual experiments.&lt;/li&gt;&lt;li&gt;Finds systematic conformity bias influenced by group size, unanimity, task difficulty, and source characteristics; high-performing agents become susceptible near their competence boundary.&lt;/li&gt;&lt;li&gt;Shows vulnerability persists across model scales—larger models resist on simple tasks but remain manipulable when operating at competence limits.&lt;/li&gt;&lt;li&gt;Highlights security implications: potential for malicious manipulation, misinformation campaigns, and bias propagation in multi-agent AI deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessandro Bellina', 'Giordano De Marzo', 'David Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'social engineering', 'misinformation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05384</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models</title><link>https://arxiv.org/abs/2601.05376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of persona conditioning (professional role + interaction style) on clinical LLM behavior across tasks like triage and patient-safety assessments.&lt;/li&gt;&lt;li&gt;Finds context-dependent, non-monotonic effects: medical personas can boost accuracy and calibration in critical-care tasks (~+20%) but can degrade performance in primary-care settings by similar margins.&lt;/li&gt;&lt;li&gt;Interaction style shifts risk propensity and sensitivity in a model-dependent way; automated LLM judges favor medical personas in safety-critical cases, but human clinicians show only moderate agreement (Cohen's κ = 0.43) and low confidence in reasoning quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tassallah Abdullahi', 'Shrestha Ghosh', 'Hamish S Fraser', "Daniel Le\\'on Tramontini", 'Adeel Abbasi', 'Ghada Bourjeily', 'Carsten Eickhoff', 'Ritambhara Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'persona conditioning', 'clinical AI', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05376</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Effects of personality steering on cooperative behavior in Large Language Model agents</title><link>https://arxiv.org/abs/2601.05302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how personality steering (Big Five) affects cooperative behavior of LLM agents in repeated Prisoner's Dilemma using GPT-3.5-turbo, GPT-4o, and GPT-5.&lt;/li&gt;&lt;li&gt;Finds agreeableness is the primary driver of cooperation; other traits have limited effects.&lt;/li&gt;&lt;li&gt;Shows explicit personality information increases cooperation but can raise exploitation vulnerability in earlier models; later models show more selective cooperation.&lt;/li&gt;&lt;li&gt;Concludes personality steering biases behavior rather than providing deterministic control over agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mizuki Sakai', 'Mizuki Yokoyama', 'Wakaba Tateishi', 'Genki Ichinose']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'prompt/personality steering', 'multi-agent robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05302</guid><pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
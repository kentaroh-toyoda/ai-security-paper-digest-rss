<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 21 Nov 2025 23:23:10 +0000</lastBuildDate><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoD, a framework to detect unknown jailbreak attacks on large vision-language models by focusing on task-specific safety representations rather than attack-specific learning.&lt;/li&gt;&lt;li&gt;Introduces a Multi-modal Safety Concept Activation Vector module for safety-oriented representation learning and a Safety Pattern Auto-Encoder for unsupervised attack classification.&lt;/li&gt;&lt;li&gt;Reports consistently higher detection AUROC on diverse unseen attacks and improved efficiency; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM safety', 'adversarial detection', 'safety evaluation', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title><link>https://arxiv.org/abs/2504.02132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates data-poisoning attacks against Visual Document RAG (VD-RAG) by injecting a single adversarial image into the knowledge base to manipulate retrieval and generation.&lt;/li&gt;&lt;li&gt;Defines two objectives: targeted attacks (mislead responses for specific queries) and universal attacks (cause denial-of-service for any query), and realizes both in white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Uses gradient-based multi-objective optimization and prompt-based strategies against multiple retrievers and vision-language generators on two visual document datasets.&lt;/li&gt;&lt;li&gt;Finds VD-RAG is vulnerable to both targeted and universal poisoning attacks, though universal black-box attacks show more robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ezzeldin Shereen', 'Dan Ristea', 'Shae McFadden', 'Burak Hasircioglu', 'Vasilios Mavroudis', 'Chris Hicks']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG/VD-RAG', 'adversarial attack', 'retrieval security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02132</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</title><link>https://arxiv.org/abs/2502.02054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an IRL-based visual planner that outputs collision-free waypoints for agile drone flight without separate perception/mapping/planning modules.&lt;/li&gt;&lt;li&gt;Collects expert trajectories using motion primitives with privileged map data and augments with learner interaction data to learn a robust reward function and policy.&lt;/li&gt;&lt;li&gt;Trained in simulation and transferred zero-shot to real-world environments (forests, structures), achieving average speeds of 7 m/s and peak 8.8 m/s in real flights.&lt;/li&gt;&lt;li&gt;Positions IRL as a solution to reduce environment interactions and improve handling of high-dimensional visual inputs compared to BC/RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minwoo Kim', 'Geunsik Bae', 'Jinwoo Lee', 'Woojae Shin', 'Changseung Kim', 'Myong-Yol Choi', 'Heejung Shin', 'Hyondong Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robotics safety', 'robustness', 'inverse reinforcement learning', 'vision-based navigation', 'sim-to-real transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02054</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title><link>https://arxiv.org/abs/2505.01406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VidStamp, a temporally-aware watermarking framework that embeds frame-level messages via fine-tuning the decoder of latent video diffusion models.&lt;/li&gt;&lt;li&gt;Two-stage fine-tuning: (1) static image datasets for spatial message separation, (2) synthesized video sequences to restore temporal consistency and robustness.&lt;/li&gt;&lt;li&gt;Supports dynamic watermarking via a control signal selecting message templates at inference; achieves 48 bits per frame while maintaining visual quality.&lt;/li&gt;&lt;li&gt;Evaluated on Stable Video Diffusion (I2V), OpenSora, and Wan (T2V); outperforms VideoSeal, VideoShield, and RivaGAN on detectability and achieves 0.96 accuracy for temporal tamper localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadreza Teymoorianfard', 'Siddarth Sitaraman', 'Shiqing Ma', 'Amir Houmansadr']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model-ownership', 'video-diffusion', 'tamper-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01406</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI</title><link>https://arxiv.org/abs/2504.12197</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PCMNet, a part-prototypical concept mining network that discovers human-comprehensible prototypes from meaningful image regions without extra supervision.&lt;/li&gt;&lt;li&gt;Clusters prototypes into concept groups and extracts concept activation vectors to provide structured, concept-level explanations.&lt;/li&gt;&lt;li&gt;Claims improved interpretability, stability, and robustness to occlusion/challenging conditions compared to prior prototype- and heatmap-based methods.&lt;/li&gt;&lt;li&gt;Evaluated on multiple image classification benchmarks and released code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahdi Alehdaghi', 'Rajarshi Bhattacharya', 'Pourya Shamsolmoali', 'Rafael M. O. Cruz', 'Maguelonne Heritier', 'Eric Granger']&lt;/li&gt;&lt;li&gt;Tags: ['explainable-ai', 'prototype-based-interpretability', 'concept-discovery', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12197</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Human Motion Unlearning</title><link>https://arxiv.org/abs/2503.18674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the task of human motion unlearning to prevent synthesis of toxic animations while preserving general text-to-motion generative performance.&lt;/li&gt;&lt;li&gt;Creates the first benchmark by filtering toxic motions from HumanML3D and Motion-X and adapts image unlearning baselines to spatio-temporal signals.&lt;/li&gt;&lt;li&gt;Proposes a training-free Latent Code Replacement (LCR) method for discrete latent spaces in text-to-motion diffusion models, outperforming baselines quantitatively and qualitatively.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edoardo De Matteis', 'Matteo Migliarini', 'Alessio Sampieri', 'Indro Spinelli', 'Fabio Galasso']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'model-editing/unlearning', 'text-to-motion', 'safety-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.18674</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks</title><link>https://arxiv.org/abs/2511.16574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Erase to Retain', a teacher–student unlearning framework for medical image segmentation that uses Low-Rank Adaptation (LoRA) constrained subspace updates to selectively forget lesion- or class-specific representations.&lt;/li&gt;&lt;li&gt;Performs adversarial optimization of LoRA modules to force the student to contradict the teacher on a designated forget subset, followed by a restoration phase (head-only supervised refinement) to recover generalization on retained data.&lt;/li&gt;&lt;li&gt;Demonstrates substantial drop in performance on forget subsets (ISIC segmentation and classification) while preserving or improving performance on retained and validation splits; also evaluated cross-domain on CHASE.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirjhor Datta', 'Md. Golam Rabiul Alam']&lt;/li&gt;&lt;li&gt;Tags: ['selective unlearning', 'model editing', 'LoRA', 'privacy', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16574</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</title><link>https://arxiv.org/abs/2511.16541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework: a vision encoder trained with supervised contrastive learning on a subset of generators, and a k-NN few-shot classifier that adapts to unseen generators with limited samples (150 images/class).&lt;/li&gt;&lt;li&gt;Demonstrates strong cross-generator generalization for detection (91.3% accuracy, +5.2 points) and improved source attribution in open-set settings (AUC +14.70%, OSCR +4.27%).&lt;/li&gt;&lt;li&gt;Targets operational practicality by enabling few-shot adaptation to new generative models without expensive full retraining, addressing evolving synthetic-image threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jaime \\'Alvarez Urue\\~na", 'David Camacho', 'Javier Huertas Tato']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'Deepfake detection', 'Few-shot learning', 'Supervised contrastive learning', 'Forensic attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16541</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Contrastive vision-language learning with paraphrasing and negation</title><link>https://arxiv.org/abs/2511.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemCLIP, a modified CLIP contrastive loss that uses LLM-generated triples (original, paraphrased, negated captions) to pull paraphrases closer to the image embedding and push negations farther away.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness to negation: CC-Neg accuracy (original-over-negation retrieval metric) increases from 68.1% (CLIP) to 78.1% (SemCLIP).&lt;/li&gt;&lt;li&gt;Reports mixed results on Sugarcrepe++ but generally better performance than models trained with negated captions; pretraining with Sugarcrepe++ yields improved zero-shot classification on tested downstream tasks.&lt;/li&gt;&lt;li&gt;Empirical focus on semantic robustness (paraphrase invariance and negation sensitivity) for contrastive vision-language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kwun Ho Ngan', 'Saman Sadeghi Afgeh', 'Joe Townsend', "Artur d'Avila Garcez"]&lt;/li&gt;&lt;li&gt;Tags: ['vision-language', 'robustness', 'contrastive-learning', 'negation', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16527</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</title><link>https://arxiv.org/abs/2511.16221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIDA, a Multimodal Interactive Deception Assessment task and a synchronized video+text dataset with verifiable per-statement ground truth for deception in multi-party interactions.&lt;/li&gt;&lt;li&gt;Benchmarks 12 state-of-the-art open- and closed-source MLLMs (including GPT-4o) and finds substantial failures in distinguishing truthful vs. deceptive statements.&lt;/li&gt;&lt;li&gt;Analyzes failure modes: poor grounding of language in multimodal social cues and inability to model others' beliefs/intentions (social epistemic reasoning).&lt;/li&gt;&lt;li&gt;Proposes a Social Chain-of-Thought (SoCoT) pipeline and Dynamic Social Epistemic Memory (DSEM) module that yield improved performance on the task.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caixin Kang', 'Yifei Huang', 'Liangyang Ouyang', 'Mingfang Zhang', 'Ruicong Liu', 'Yoichi Sato']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'multimodal LLMs', 'benchmark/dataset', 'social reasoning', 'trustworthiness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16221</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.16203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLA-Fool, a framework for multimodal adversarial attacks on Vision-Language-Action (VLA) models under both white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Unifies three attack levels: textual perturbations (gradient- and prompt-based), visual perturbations (patch and noise), and cross-modal misalignment attacks that disrupt semantic correspondence between perception and instruction.&lt;/li&gt;&lt;li&gt;Develops a VLA-aware semantic space to automatically craft semantically guided prompts for more effective attacks.&lt;/li&gt;&lt;li&gt;Evaluates on the LIBERO benchmark with a fine-tuned OpenVLA, showing that small multimodal perturbations can cause significant behavioral deviations, highlighting fragility of embodied multimodal alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yinxin Zhang', 'Lingjuan Lyu', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal robustness', 'cross-modal misalignment', 'prompt-based manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16203</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs</title><link>https://arxiv.org/abs/2511.16163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VTIA, a two-stage verbose-text induction attack that crafts imperceptible image perturbations to cause Vision-Language Models (VLMs) to generate extremely long, low-information text outputs.&lt;/li&gt;&lt;li&gt;Stage 1: adversarial prompt search using reinforcement learning to find prompt embeddings that induce verbose outputs from the LLM component of VLMs.&lt;/li&gt;&lt;li&gt;Stage 2: vision-aligned perturbation optimization to make perturbed image embeddings align with the adversarial prompt embeddings, producing images that trigger verbose generation.&lt;/li&gt;&lt;li&gt;Empirical results on four popular VLMs show the method is effective, efficient, and generalizes across models, explicitly optimizing output token length rather than merely delaying EOS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhi Luo', 'Zenghui Yuan', 'Wenqi Wei', 'Daizong Liu', 'Pan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'vision-language-models', 'prompt-based-attacks', 'resource-exhaustion', 'multimodal-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16163</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation</title><link>https://arxiv.org/abs/2511.16162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR): injects small zero-mean noise at multiple layers to learn a frequency-bias prior and applies selective wavelet reconstruction to suppress noise-sensitive bands and enhance structural cues for segmentation.&lt;/li&gt;&lt;li&gt;Backbone-agnostic, low inference overhead plug-in that can be used with or without adversarial training; intended to mitigate the clean–robustness trade-off and reduce training/tuning costs.&lt;/li&gt;&lt;li&gt;Evaluated on CT and ultrasound segmentation under PGD-L_inf/L2 and SSAH attacks; shows consistent gains in clean Dice/IoU, significantly smaller drops under strong attacks, and additive robustness improvements when combined with adversarial training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Lu', 'Ziliang Wang', 'Weixin Xu', 'Wei Zhang', 'Yongqiang Zhao', 'Yang Yu', 'Xiaohong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'adversarial training', 'medical image segmentation', 'wavelet filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16162</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title><link>https://arxiv.org/abs/2511.16020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a sequence-level optimization framework to generate adversarial, printable textures for shirts, trousers, and hats that remain effective across entire walking video sequences with motion, pose changes, and cloth deformation.&lt;/li&gt;&lt;li&gt;Uses product-to-UV mapping and a compact palette/control-point parameterization with ICC locking to ensure colors are printable, combined with a physically based human-garment pipeline (cloth dynamics, multi-angle views, illumination) for realistic simulation.&lt;/li&gt;&lt;li&gt;Optimizes textures via an expectation-over-transformation objective with temporal weighting to minimize detector confidence across sequences, yielding strong cross-model transferability and robustness to viewpoint changes.&lt;/li&gt;&lt;li&gt;Validates in the physical world using sublimation-printed garments, demonstrating reliable suppression of human detectors in indoor and outdoor recordings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingkun Zhou', 'Patrick P. K. Chan', 'Hengxu Wu', 'Shikang Zheng', 'Ruiqi Huang', 'Yuanjie Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical-world attacks', 'human-detection evasion', 'robust sequence-level optimization', 'wearable attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16020</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2511.16015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a graph-based method that leverages inter-sample relationships (initialized from a pre-trained model's feature space) and refines it with graph convolutional networks to improve out-of-distribution (OOD) detection under long-tailed in-distribution settings.&lt;/li&gt;&lt;li&gt;Introduces Gaussianization of activation-layer distributions to compensate for distributional differences between pre-training and downstream training data before graph refinement.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements in false positive rate (FPR) and tail-class in-distribution classification accuracy on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT compared to prior state-of-the-art.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nimeshika Udayangani', 'Hadi M. Dolatabadi', 'Sarah Erfani', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'long-tailed recognition', 'graph neural networks', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16015</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs</title><link>https://arxiv.org/abs/2511.05919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Xmera, a theory-grounded man-in-the-middle (MitM) framework to perform prompt-injection attacks that perturb inputs to LLMs and undermine factual recall in closed-book QA.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across three fact-based QA settings, finding trivial instruction-based attacks achieve high success rates (up to ~85.3%) while producing high uncertainty for incorrect answers.&lt;/li&gt;&lt;li&gt;Proposes a simple defense by training Random Forest classifiers on response uncertainty to detect attacked queries, reporting high detection performance (average AUC up to ~96%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alina Fastowski', 'Bardh Prenkaj', 'Yuxiao Li', 'Gjergji Kasneci']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial attacks', 'factual recall', 'defense/detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05919</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title><link>https://arxiv.org/abs/2511.15304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows adversarial poetry as a single-turn jailbreak method across 25 proprietary and open-weight LLMs, with some providers &gt;90% attack-success rate (ASR).&lt;/li&gt;&lt;li&gt;Converts 1,200 harmful MLCommons prompts into verse via a meta-prompt, achieving up to 18x higher ASR than prose baselines; hand-crafted poems averaged ~62% success, meta-prompt conversions ~43%.&lt;/li&gt;&lt;li&gt;Evaluates outputs with an ensemble of 3 open-weight LLM judges validated on a stratified human-labeled subset and maps attacks to MLCommons and EU CoP risk taxonomies, indicating cross-domain transfer (CBRN, manipulation, cyber-offence, loss-of-control).&lt;/li&gt;&lt;li&gt;Concludes that stylistic variation (poetic framing) can systematically bypass contemporary safety mechanisms, highlighting limitations in current alignment and evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Matteo Prandi', 'Federico Pierucci', 'Francesco Giarrusso', 'Marcantonio Bracale', 'Marcello Galisai', 'Vincenzo Suriani', 'Olga Sorokoletova', 'Federico Sartore', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15304</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy</title><link>https://arxiv.org/abs/2511.12920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic algorithmic audit of 1,508 real baby care and pregnancy queries evaluating AI Overviews (AIO) and Featured Snippets (FS) on Google Search.&lt;/li&gt;&lt;li&gt;Assessed multiple quality dimensions: answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment.&lt;/li&gt;&lt;li&gt;Key findings: 33% inconsistency between AIO and FS on the same page; medical safeguards present in only 11% of AIO and 7% of FS; FS more likely to link to commercial sources.&lt;/li&gt;&lt;li&gt;Concludes the need for stronger quality controls for AI-mediated health information and presents a transferable auditing methodology for high-stakes domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Desheng Hu', 'Joachim Baumann', 'Aleksandra Urman', 'Elsa Lichtenegger', 'Robin Forsberg', 'Aniko Hannak', 'Christo Wilson']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'auditing', 'information quality', 'healthcare misinformation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12920</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HalluClean: A Unified Framework to Combat Hallucinations in LLMs</title><link>https://arxiv.org/abs/2511.08916</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HalluClean, a lightweight, task-agnostic framework for detecting and correcting hallucinations in LLM outputs.&lt;/li&gt;&lt;li&gt;Uses a reasoning-enhanced pipeline (planning, execution, revision) with minimal task-routing prompts to enable zero-shot generalization without external knowledge sources or supervised detectors.&lt;/li&gt;&lt;li&gt;Evaluated across five tasks (QA, dialogue, summarization, math word problems, contradiction detection) and shows improved factual consistency over competitive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxin Zhao', 'Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'LLM safety', 'fact checking', 'model alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08916</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an activation steering vector applied to an LLM to suppress evaluation-awareness so the model behaves as if deployed during safety evaluations.&lt;/li&gt;&lt;li&gt;Constructs an evaluation-aware model via a two-step training: continued pretraining on documents that correlate a cue (Python type hints) with evaluation, then expert-iteration fine-tuning to use the cue during evaluation.&lt;/li&gt;&lt;li&gt;Finds that a steering vector computed from the original (pre-trained) model can reduce evaluation-aware behaviors (use of type hints) even after the model was trained to be evaluation-aware.&lt;/li&gt;&lt;li&gt;Implication: steering can be used to make safety evaluations more reliable by preventing models from gaming tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'evaluation-awareness', 'activation steering', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes</title><link>https://arxiv.org/abs/2509.21456</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the performance trade-off when applying moral alignment (fine-tuning) to mitigate gender stereotypes in pretrained language models.&lt;/li&gt;&lt;li&gt;Finds downstream task performance is strongly correlated with overall forgetting of model knowledge, not just selective forgetting of stereotypes.&lt;/li&gt;&lt;li&gt;Shows selective forgetting can reduce stereotypes but tends to increase overall forgetting, and common remedies fail to prevent this or recover downstream performance.&lt;/li&gt;&lt;li&gt;Concludes current fairness objectives and dataset requirements limit effective trade-offs between stereotype mitigation and task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangliang Liu', 'Bocheng Chen', 'Han Zi', 'Xitong Zhang', 'Kristen Marie Johnson']&lt;/li&gt;&lt;li&gt;Tags: ['moral alignment', 'fairness', 'gender stereotypes', 'catastrophic forgetting', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21456</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</title><link>https://arxiv.org/abs/2509.03888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Re-examines probing-based approaches for detecting malicious inputs to LLMs and demonstrates they fail to generalize out-of-distribution.&lt;/li&gt;&lt;li&gt;Finds probes rely on superficial patterns (instructional templates and trigger words) rather than semantic harmfulness; simple n-gram baselines achieve comparable performance.&lt;/li&gt;&lt;li&gt;Uses controlled experiments with semantically cleaned datasets and pattern-dependence analyses, and recommends redesigning models and evaluation protocols; code is open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Wang', 'Zeming Wei', 'Qin Liu', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'probing-based detection', 'adversarial/jailbreak detection', 'robustness and evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03888</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Confidence to Collapse in LLM Factual Robustness</title><link>https://arxiv.org/abs/2508.16267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Factual Robustness Score (FRS), combining token distribution entropy and sensitivity to temperature scaling to quantify stability of factual outputs under decoding perturbations.&lt;/li&gt;&lt;li&gt;Evaluates 5 LLMs on three closed-book QA datasets (SQuAD, TriviaQA, HotpotQA), reporting FRS differences (smaller models ~0.76, larger ~0.93) and up to ~60% accuracy degradation with increased uncertainty.&lt;/li&gt;&lt;li&gt;Demonstrates how entropy and temperature scaling affect factual accuracy and suggests implications for improving knowledge retention and retrieval robustness in future models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alina Fastowski', 'Bardh Prenkaj', 'Gjergji Kasneci']&lt;/li&gt;&lt;li&gt;Tags: ['factual robustness', 'LLM evaluation', 'decoding robustness', 'temperature scaling', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16267</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CRISP: Persistent Concept Unlearning via Sparse Autoencoders</title><link>https://arxiv.org/abs/2508.13650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CRISP, a parameter-efficient method for persistent concept unlearning in LLMs using sparse autoencoders (SAEs).&lt;/li&gt;&lt;li&gt;Automatically identifies salient SAE features across multiple layers and suppresses their activations to create lasting parameter changes.&lt;/li&gt;&lt;li&gt;Demonstrates improved removal of harmful knowledge on safety-critical tasks (WMDP benchmark) while preserving general and in-domain capabilities.&lt;/li&gt;&lt;li&gt;Feature-level analysis shows semantically coherent separation between target and benign concepts enabling precise suppression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomer Ashuach', 'Dana Arad', 'Aaron Mueller', 'Martin Tutek', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LLM safety', 'sparse autoencoders', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13650</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title><link>https://arxiv.org/abs/2504.02132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates data-poisoning attacks against Visual Document RAG (VD-RAG) by injecting a single adversarial image into the knowledge base to manipulate retrieval and generation.&lt;/li&gt;&lt;li&gt;Defines two objectives: targeted attacks (mislead responses for specific queries) and universal attacks (cause denial-of-service for any query), and realizes both in white-box and black-box settings.&lt;/li&gt;&lt;li&gt;Uses gradient-based multi-objective optimization and prompt-based strategies against multiple retrievers and vision-language generators on two visual document datasets.&lt;/li&gt;&lt;li&gt;Finds VD-RAG is vulnerable to both targeted and universal poisoning attacks, though universal black-box attacks show more robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ezzeldin Shereen', 'Dan Ristea', 'Shae McFadden', 'Burak Hasircioglu', 'Vasilios Mavroudis', 'Chris Hicks']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG/VD-RAG', 'adversarial attack', 'retrieval security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02132</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies</title><link>https://arxiv.org/abs/2511.16590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes D-GARA, a dynamic benchmarking framework to evaluate Android GUI agent robustness under real-world anomalies (e.g., permission dialogs, battery warnings, update prompts).&lt;/li&gt;&lt;li&gt;Constructs and annotates a benchmark of common Android apps with embedded anomalies and supports modular extension for new tasks and anomaly types.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art GUI agents and shows substantial performance degradation in anomaly-rich environments, highlighting the need for robustness-aware learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sen Chen', 'Tong Zhao', 'Yi Bin', 'Fei Ma', 'Wenqi Shao', 'Zheng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['GUI robustness', 'Benchmarking', 'Robustness evaluation', 'Anomaly handling', 'Android agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16590</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</title><link>https://arxiv.org/abs/2511.16221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIDA, a Multimodal Interactive Deception Assessment task and a synchronized video+text dataset with verifiable per-statement ground truth for deception in multi-party interactions.&lt;/li&gt;&lt;li&gt;Benchmarks 12 state-of-the-art open- and closed-source MLLMs (including GPT-4o) and finds substantial failures in distinguishing truthful vs. deceptive statements.&lt;/li&gt;&lt;li&gt;Analyzes failure modes: poor grounding of language in multimodal social cues and inability to model others' beliefs/intentions (social epistemic reasoning).&lt;/li&gt;&lt;li&gt;Proposes a Social Chain-of-Thought (SoCoT) pipeline and Dynamic Social Epistemic Memory (DSEM) module that yield improved performance on the task.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caixin Kang', 'Yifei Huang', 'Liangyang Ouyang', 'Mingfang Zhang', 'Ruicong Liu', 'Yoichi Sato']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'multimodal LLMs', 'benchmark/dataset', 'social reasoning', 'trustworthiness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16221</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization</title><link>https://arxiv.org/abs/2511.16209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes prompt hardening as a utility-constrained optimization problem and proposes shield appending (a protective textual layer) to reduce leakage of system prompts.&lt;/li&gt;&lt;li&gt;Uses an LLM-as-optimizer in a black-box setting to search for SHIELD texts that minimize a leakage metric computed from a suite of adversarial extraction attacks while keeping task outputs semantically faithful to baseline.&lt;/li&gt;&lt;li&gt;Empirical results show optimized SHIELDs significantly reduce prompt extraction success compared to baseline defenses without materially degrading intended functionality; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huseein Jawad', 'Nicolas Brunel']&lt;/li&gt;&lt;li&gt;Tags: ['prompt hardening', 'prompt extraction', 'jailbreak defense', 'black-box defenses', 'LLM-guided optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16209</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CARE-RAG - Clinical Assessment and Reasoning in RAG</title><link>https://arxiv.org/abs/2511.15994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines the gap between retrieval and reasoning in retrieval-augmented generation (RAG) within a clinical setting using Written Exposure Therapy (WET) guidelines.&lt;/li&gt;&lt;li&gt;Finds that LLMs still produce reasoning errors even when provided with authoritative retrieved passages.&lt;/li&gt;&lt;li&gt;Proposes an evaluation framework that measures accuracy, consistency, and fidelity of reasoning and highlights risks for safe clinical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepthi Potluri', 'Aby Mammen Mathew', 'Jeffrey B DeWitt', 'Alexander L. Rasgon', 'Yide Hao', 'Junyuan Hong', 'Ying Ding']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'safety/evaluation', 'alignment/robustness', 'clinical-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15994</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems</title><link>https://arxiv.org/abs/2511.15862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a game-theory-based taxonomy of uncooperative behaviors in LLM-driven multi-agent systems.&lt;/li&gt;&lt;li&gt;Presents a multi-stage simulation pipeline that dynamically generates and refines uncooperative agent behaviors as agent states evolve.&lt;/li&gt;&lt;li&gt;Evaluates the framework in a collaborative resource-management setting, measuring stability metrics (survival time, resource overuse) and reports severe degradation when uncooperative behaviors arise.&lt;/li&gt;&lt;li&gt;Claims high empirical realism (96.7% accuracy via human evaluation) for generated uncooperative behaviors and contrasts perfect stability under cooperation vs. rapid collapse with defection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devang Kulshreshtha', 'Wanyu Du', 'Raghav Jain', 'Srikanth Doss', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent safety', 'robustness', 'adversarial/uncooperative behaviors', 'simulation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15862</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue</title><link>https://arxiv.org/abs/2511.16544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that standard ASR metrics (e.g., WER) correlate poorly with clinician-assessed clinical impact of transcription errors in doctor–patient dialogue.&lt;/li&gt;&lt;li&gt;Creates a gold-standard benchmark with clinician labels (No/Minimal/Significant Impact) on ASR-generated transcriptions across two clinical dialogue datasets.&lt;/li&gt;&lt;li&gt;Introduces an LLM-as-a-Judge approach (Gemini-2.5-Pro, optimized with GEPA) that achieves human-comparable performance (90% accuracy, Cohen's kappa 0.816) for automated clinical-impact evaluation.&lt;/li&gt;&lt;li&gt;Proposes a scalable framework to evaluate ASR systems by clinical safety impact rather than only textual fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Ellis', 'Jared Joselowitz', 'Yash Deo', 'Yajie He', 'Anna Kalygina', 'Aisling Higham', 'Mana Rahimzadeh', 'Yan Jia', 'Ibrahim Habli', 'Ernest Lim']&lt;/li&gt;&lt;li&gt;Tags: ['ASR', 'Safety evaluation', 'Clinical NLP', 'LLM-as-judge', 'Evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16544</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning</title><link>https://arxiv.org/abs/2511.16324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDA (Steering-Driven Distribution Alignment), a training-free, model-agnostic inference-time method that redistributes output probabilities to better align LLM responses with user-defined instructions.&lt;/li&gt;&lt;li&gt;Aims to improve three alignment dimensions (helpfulness, harmlessness, honesty) without fine-tuning, and supports personalized preference alignment.&lt;/li&gt;&lt;li&gt;Evaluated across 8 open-source LLMs, reporting large gains (avg. helpfulness +64.4%, honesty +30%, harmlessness +11.5%).&lt;/li&gt;&lt;li&gt;Lightweight, compatible with other training-based alignment strategies, and positioned as a practical alignment/safety intervention during inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Xia', 'Zhi-Hong Deng']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'inference-time steering', 'LLM alignment', 'honesty/harmlessness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16324</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2511.16275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SeSE (Semantic Structural Entropy), a principled uncertainty quantification framework that models latent semantic structure to detect hallucinations in LLM outputs.&lt;/li&gt;&lt;li&gt;Constructs an adaptively sparsified directed semantic graph and defines SeSE as the structural entropy of an optimal semantic encoding tree; higher SeSE indicates greater uncertainty and higher hallucination risk.&lt;/li&gt;&lt;li&gt;Extends the method to fine-grained, claim-level uncertainty for long-form generation and empirically outperforms strong UQ baselines (including supervised methods and KLE) across 29 model–dataset combinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingtao Zhao', 'Hao Peng', 'Dingli Su', 'Xianghua Zeng', 'Chunyang Liu', 'Jinzhi Liao', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'uncertainty-quantification', 'LLM-safety', 'structural-entropy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16275</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning</title><link>https://arxiv.org/abs/2511.16198</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SemanticCite, an AI system for citation verification that analyzes full-text sources and returns evidence-based classifications (Supported, Partially Supported, Unsupported, Uncertain).&lt;/li&gt;&lt;li&gt;Combines multiple retrieval methods and provides transparent explanations with relevant text snippets and reasoning to support decisions.&lt;/li&gt;&lt;li&gt;Reports that fine-tuned lightweight language models can match large commercial systems for this task, and contributes an open dataset (~1,000 citations across 8 disciplines), models, and an open-source verification framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian Haan']&lt;/li&gt;&lt;li&gt;Tags: ['citation verification', 'hallucination detection', 'AI safety', 'fact-checking', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16198</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Liars' Bench: Evaluating Lie Detectors for Language Models</title><link>https://arxiv.org/abs/2511.16035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LIARS' BENCH, a large testbed (72,863 examples) of lies and honest responses from four open-weight LLMs across seven datasets.&lt;/li&gt;&lt;li&gt;Characterizes lies along two dimensions: the model's reason for lying and the object of belief targeted by the lie, covering diverse lie types.&lt;/li&gt;&lt;li&gt;Evaluates three existing black-box and white-box lie-detection techniques and finds systematic failures, especially when the transcript alone is insufficient to judge truth.&lt;/li&gt;&lt;li&gt;Provides a practical benchmark revealing limitations of prior methods and guiding future work on LLM lie detection and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kieron Kretschmar (Cadenza Labs)', 'Walter Laurito (Cadenza Labs', 'FZI)', 'Sharan Maiya (Cadenza Labs', 'University of Cambridge)', 'Samuel Marks (Anthropic)']&lt;/li&gt;&lt;li&gt;Tags: ['lie-detection', 'LLM-safety', 'benchmarking', 'alignment', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16035</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues</title><link>https://arxiv.org/abs/2511.15976</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TOD-ProcBench: a benchmark of complex, multi-level condition-action instructions for task-oriented dialogues derived from the ABCD dataset with human quality control.&lt;/li&gt;&lt;li&gt;Defines three tasks: (1) retrieve the most relevant instruction statement and predict the next action, (2) detect instruction-violating responses created by injecting inconsistencies/manipulations, and (3) generate conditional instruction-following responses.&lt;/li&gt;&lt;li&gt;Evaluates multilingual and instruction-format effects on compliance and releases the benchmark under the Llama 3.3 Community License.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarik Ghazarian', 'Abhinav Gullapalli', 'Swair Shah', 'Anurag Beniwal', 'Nanyun Peng', 'Narayanan Sadagopan', 'Zhou Yu']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'safety-evaluation', 'benchmarking', 'red-teaming', 'task-oriented-dialogue']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15976</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning</title><link>https://arxiv.org/abs/2511.15886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes step-level (ContextCite) and token-level (Inseq) attribution of Chain-of-Thought (CoT) reasoning in a multilingual setting using Qwen2.5 1.5B-Instruct on the MGSM benchmark.&lt;/li&gt;&lt;li&gt;Finds attribution concentrates on final reasoning steps, especially for incorrect outputs, raising concerns about faithfulness of CoT explanations.&lt;/li&gt;&lt;li&gt;Shows structured CoT prompting improves accuracy mainly for high-resource Latin-script languages, indicating limited multilingual robustness.&lt;/li&gt;&lt;li&gt;Demonstrates that controlled perturbations (negation, distractors) degrade both accuracy and attribution coherence, highlighting fragility of CoT explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeremias Ferrao', 'Ezgi Basar', 'Khondoker Ittehadul Islam', 'Mahrokh Hassani']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'chain-of-thought', 'multilingual robustness', 'attribution', 'faithfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15886</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories</title><link>https://arxiv.org/abs/2506.12911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a general-purpose constraint-aware prediction refinement method using deterministic DDIM diffusion trajectories, combining a learned prior with constraint gradient corrections.&lt;/li&gt;&lt;li&gt;Method is model-agnostic and can be applied post hoc to enforce nonlinear and non-convex equality constraints.&lt;/li&gt;&lt;li&gt;Demonstrated on constrained adversarial attack generation for tabular data and AC power flow prediction (Kirchhoff's laws), improving constraint satisfaction and task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pantelis Dogoulis', 'Fabien Bernier', "F\\'elix Fourreau", 'Karim Tit', 'Maxime Cordy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'constraint satisfaction', 'diffusion models', 'robustness', 'post-hoc refinement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12911</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title><link>https://arxiv.org/abs/2505.01406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VidStamp, a temporally-aware watermarking framework that embeds frame-level messages via fine-tuning the decoder of latent video diffusion models.&lt;/li&gt;&lt;li&gt;Two-stage fine-tuning: (1) static image datasets for spatial message separation, (2) synthesized video sequences to restore temporal consistency and robustness.&lt;/li&gt;&lt;li&gt;Supports dynamic watermarking via a control signal selecting message templates at inference; achieves 48 bits per frame while maintaining visual quality.&lt;/li&gt;&lt;li&gt;Evaluated on Stable Video Diffusion (I2V), OpenSora, and Wan (T2V); outperforms VideoSeal, VideoShield, and RivaGAN on detectability and achieves 0.96 accuracy for temporal tamper localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadreza Teymoorianfard', 'Siddarth Sitaraman', 'Shiqing Ma', 'Amir Houmansadr']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model-ownership', 'video-diffusion', 'tamper-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01406</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance</title><link>https://arxiv.org/abs/2502.04695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that standardized, reliable XAI evaluation metrics are essential governance primitives for private oversight, enabling auditability and accountability.&lt;/li&gt;&lt;li&gt;Identifies limitations of current XAI metrics around faithfulness, tamper resistance, and regulatory alignment, and links interpretability to preventing alignment faking.&lt;/li&gt;&lt;li&gt;Proposes a 'Governance by Metrics' paradigm and a hierarchical framework connecting transparency, tamper resistance, scalability, and legal alignment for continuous AI assurance pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pratinav Seth', 'Vinay Kumar Sankarapu']&lt;/li&gt;&lt;li&gt;Tags: ['XAI metrics', 'alignment', 'tamper resistance', 'AI governance', 'assurance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04695</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</title><link>https://arxiv.org/abs/2502.02054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an IRL-based visual planner that outputs collision-free waypoints for agile drone flight without separate perception/mapping/planning modules.&lt;/li&gt;&lt;li&gt;Collects expert trajectories using motion primitives with privileged map data and augments with learner interaction data to learn a robust reward function and policy.&lt;/li&gt;&lt;li&gt;Trained in simulation and transferred zero-shot to real-world environments (forests, structures), achieving average speeds of 7 m/s and peak 8.8 m/s in real flights.&lt;/li&gt;&lt;li&gt;Positions IRL as a solution to reduce environment interactions and improve handling of high-dimensional visual inputs compared to BC/RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minwoo Kim', 'Geunsik Bae', 'Jinwoo Lee', 'Woojae Shin', 'Changseung Kim', 'Myong-Yol Choi', 'Heejung Shin', 'Hyondong Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robotics safety', 'robustness', 'inverse reinforcement learning', 'vision-based navigation', 'sim-to-real transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02054</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interpreting Emergent Features in Deep Learning-based Side-channel Analysis</title><link>https://arxiv.org/abs/2502.00384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies mechanistic interpretability to neural networks used for side-channel analysis (SCA) to reveal how models exploit leakage in physical traces.&lt;/li&gt;&lt;li&gt;Uses analysis of sudden performance jumps to reverse-engineer learned representations and recover secret masks, turning black-box evaluations into white-box insights.&lt;/li&gt;&lt;li&gt;Demonstrates scalability to realistic SCA scenarios, including sparse relevant inputs, low model accuracies, and traces with side-channel protections, enabling better-informed countermeasures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sengim Karayal\\c{c}in', 'Marina Kr\\v{c}ek', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel-analysis', 'mechanistic-interpretability', 'deep-learning-security', 'privacy-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00384</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Node-Level Uncertainty Estimation in LLM-Generated SQL</title><link>https://arxiv.org/abs/2511.13984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a semantically aware labeling algorithm to assign correctness labels to individual AST nodes of generated SQL without over-penalizing structural/alias variations.&lt;/li&gt;&lt;li&gt;Extracts schema-aware and lexical node features (identifier validity, alias resolution, type compatibility, scope ambiguity, typo signals) and trains a supervised classifier to predict per-node error probabilities as calibrated uncertainty.&lt;/li&gt;&lt;li&gt;Shows substantial improvement over token log-probabilities (average AUC +27.44%) with cross-database robustness, enabling fine-grained diagnostics for targeted repair, human-in-the-loop review, and selective execution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hilaf Hasson', 'Ruocheng Guo']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'robustness', 'LLM-safety', 'sql-generation', 'program-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13984</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction</title><link>https://arxiv.org/abs/2511.07899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a conformal prediction (CP) framework to bound uncertainty in learned Hamilton–Jacobi (HJ) value functions and provide probabilistic safety guarantees for control systems.&lt;/li&gt;&lt;li&gt;Uses CP to calibrate switching between an unsafe nominal controller and a learned HJ-based safe policy, and derives safety guarantees for the resulting switched policy.&lt;/li&gt;&lt;li&gt;Explores ensembles of independently trained HJ value functions as safety filters and compares ensemble-based filtering to using individual value functions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ihab Tabbara', 'Yuxuan Yang', 'Hussein Sibai']&lt;/li&gt;&lt;li&gt;Tags: ['conformal-prediction', 'HJ-reachability', 'safe-controllers', 'safety-assurance', 'ensembles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07899</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation</title><link>https://arxiv.org/abs/2511.00588</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a clinician-centered, sequential validation framework to quantify hallucination risk in LLM-based surgical decision support across diagnostic precision, recommendation quality, reasoning robustness, coherence, and knowledge alignment.&lt;/li&gt;&lt;li&gt;Evaluates six leading LLMs on 30 expert-validated spinal cases and performs multidimensional stress-testing to reveal model-specific vulnerabilities (e.g., recommendation quality drops under increased complexity).&lt;/li&gt;&lt;li&gt;Finds that reasoning-enhanced variants do not necessarily improve clinical reliability (example: Claude-3.7-Sonnet extended thinking underperformed its standard mode) and recommends integrating interpretability mechanisms for deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dong Chen', 'Yanzhe Wei', 'Zonglin He', 'Guan-Ming Kuang', 'Canhua Ye', 'Meiru An', 'Huili Peng', 'Yong Hu', 'Huiren Tao', 'Kenneth MC Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination', 'medical AI evaluation', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00588</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interpretability as Alignment: Making Internal Understanding a Design Principle</title><link>https://arxiv.org/abs/2509.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues mechanistic interpretability should be treated as a design constraint to enable verifiable internal alignment rather than as post-hoc explanation.&lt;/li&gt;&lt;li&gt;Proposes using causal abstraction theory and empirical benchmarks (e.g., MIB, LoBOX) to create models with auditability, provenance, and bounded transparency for private governance.&lt;/li&gt;&lt;li&gt;Positions interpretability-first models as infrastructure to support audits, certification, insurance, and procurement workflows that require causal evidence of model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aadit Sengupta', 'Pratinav Seth', 'Vinay Kumar Sankarapu']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'AI alignment', 'mechanistic interpretability', 'governance/auditability', 'safety assurance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08592</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Solution and Learning of Robust Factored MDPs</title><link>https://arxiv.org/abs/2508.00707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops methods for solving and learning robust MDPs (r-MDPs) that explicitly model epistemic uncertainty in transition dynamics.&lt;/li&gt;&lt;li&gt;Exploits factored state-space structure to reformulate otherwise hard robust policy synthesis problems into tractable linear programs.&lt;/li&gt;&lt;li&gt;Proposes learning algorithms for factored r-MDP representations with PAC-style performance guarantees and improved sample efficiency.&lt;/li&gt;&lt;li&gt;Empirical results show dimensional gains and tighter guarantees versus prior state-of-the-art robust RL approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yannik Schnitzer', 'Alessandro Abate', 'David Parker']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'robust MDP', 'safe reinforcement learning', 'sample efficiency', 'factored models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00707</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Model Inversion Evaluation: From Misleading Standards to Reliable Privacy Assessment</title><link>https://arxiv.org/abs/2505.03519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a major flaw in standard model inversion (MI) evaluation: Type-I adversarial examples that produce reconstructions judged as successful by a task-aligned evaluation model E despite lacking true visual features.&lt;/li&gt;&lt;li&gt;Proposes replacing the task-trained evaluation model with Multimodal Large Language Models (MLLMs) to leverage general-purpose visual understanding and reduce false positives.&lt;/li&gt;&lt;li&gt;Reevaluates 27 MI attack setups and shows many SOTA MI methods have inflated success rates under the standard framework; actual privacy leakage is lower than previously reported.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sy-Tuyen Ho', 'Koh Jun Hao', 'Ngoc-Bao Nguyen', 'Alexander Binder', 'Ngai-Man Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attacks', 'evaluation methodology', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.03519</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them</title><link>https://arxiv.org/abs/2503.16401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Misleading Fine-Tuning (MisFT): fine-tune LLMs on datasets that encode contradictory math/logical rules to test whether models internalize abstract rules vs. memorize.&lt;/li&gt;&lt;li&gt;Evaluates generalization by testing on unseen domains (math word problems and natural language reasoning) after exposing models to incorrect principles.&lt;/li&gt;&lt;li&gt;Finds LLMs can apply the injected contradictory rules to solve problems, suggesting they form internal abstractions that can be altered by targeted fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Chen', 'Peiyang Wang', 'Yizhou Jiang', 'Yuqian Liu', 'Chujie Zhao', 'Ying Fang', 'Tianren Zhang', 'Feng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model robustness', 'data poisoning / adversarial fine-tuning', 'interpretability / hidden reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16401</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Closer Look at Adversarial Suffix Learning for Jailbreaking LLMs: Augmented Adversarial Trigger Learning</title><link>https://arxiv.org/abs/2503.12339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ATLA (Adversarial Trigger Learning with Augmented objectives), a weighted loss formulation that focuses optimization on response-format tokens and an auxiliary loss to suppress evasive responses.&lt;/li&gt;&lt;li&gt;Demonstrates ATLA can learn effective adversarial suffixes from a single query-response pair that generalize to unseen queries and transfer across LLMs.&lt;/li&gt;&lt;li&gt;Empirically outperforms prior methods, achieving near-100% attack success while using ~80% fewer queries; also used to extract hidden system prompts.&lt;/li&gt;&lt;li&gt;Code release provided for replication (https://github.com/QData/ALTA_Augmented_Adversarial_Trigger_Learning).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Wang', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Jailbreaking / adversarial suffixes', 'Adversarial trigger learning', 'Prompt/system prompt extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.12339</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sparse-PGD: A Unified Framework for Sparse Adversarial Perturbations Generation</title><link>https://arxiv.org/abs/2405.05075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sparse-PGD, a white-box PGD-like framework to generate both unstructured and structured sparse adversarial perturbations.&lt;/li&gt;&lt;li&gt;Combines Sparse-PGD with a black-box attack for more comprehensive robustness evaluation against sparse attacks.&lt;/li&gt;&lt;li&gt;Uses the method's efficiency to perform adversarial training, producing models that achieve state-of-the-art robustness to various sparse perturbations.&lt;/li&gt;&lt;li&gt;Presents extensive experiments demonstrating strong attack performance and improved defended models; code is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuyang Zhong', 'Chen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'sparse adversarial attacks', 'adversarial training', 'robustness evaluation', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.05075</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Synthesis of Safety Specifications for Probabilistic Systems</title><link>https://arxiv.org/abs/2511.16579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a theoretical framework for synthesizing safety specifications in probabilistic systems using PCTL, introducing a fragment called CPCTL.&lt;/li&gt;&lt;li&gt;Reduces global specification satisfaction to local constraints and argues CPCTL is expressive enough for practical synthesis problems.&lt;/li&gt;&lt;li&gt;Proposes a new value-iteration-based algorithm to solve the synthesis problem for these temporal properties and proves soundness and completeness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gaspard Ohlmann', 'Edwin Hamel-De le Court', 'Francesco Belardinelli']&lt;/li&gt;&lt;li&gt;Tags: ['formal methods', 'controller synthesis', 'probabilistic verification', 'PCTL', 'safety specifications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16579</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Contrastive vision-language learning with paraphrasing and negation</title><link>https://arxiv.org/abs/2511.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SemCLIP, a modified CLIP contrastive loss that uses LLM-generated triples (original, paraphrased, negated captions) to pull paraphrases closer to the image embedding and push negations farther away.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness to negation: CC-Neg accuracy (original-over-negation retrieval metric) increases from 68.1% (CLIP) to 78.1% (SemCLIP).&lt;/li&gt;&lt;li&gt;Reports mixed results on Sugarcrepe++ but generally better performance than models trained with negated captions; pretraining with Sugarcrepe++ yields improved zero-shot classification on tested downstream tasks.&lt;/li&gt;&lt;li&gt;Empirical focus on semantic robustness (paraphrase invariance and negation sensitivity) for contrastive vision-language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kwun Ho Ngan', 'Saman Sadeghi Afgeh', 'Joe Townsend', "Artur d'Avila Garcez"]&lt;/li&gt;&lt;li&gt;Tags: ['vision-language', 'robustness', 'contrastive-learning', 'negation', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16527</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Digital Agriculture Sandbox for Collaborative Research</title><link>https://arxiv.org/abs/2511.15990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a secure online platform (Digital Agriculture Sandbox) enabling farmer–researcher collaboration on farm data without exposing private information.&lt;/li&gt;&lt;li&gt;Uses privacy-preserving techniques such as federated learning and differential privacy to keep sensitive data on-farm while enabling model training and analysis.&lt;/li&gt;&lt;li&gt;Aims to make privacy-preserving analytics accessible to farmers with limited technical resources, including tools to find similar farms and allow researchers to learn from data safely.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Osama Zafar', "Rosemarie Santa Gonz\\'alez", 'Alfonso Morales', 'Erman Ayday']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'federated learning', 'differential privacy', 'secure collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15990</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense</title><link>https://arxiv.org/abs/2511.16483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using a large language model to generate reward functions for training deep reinforcement learning (DRL) agents in an autonomous cyber attack-defense simulation.&lt;/li&gt;&lt;li&gt;Crafts multiple attack and defense agent personas to provide context and heterogeneity for the LLM-guided reward design.&lt;/li&gt;&lt;li&gt;Uses the LLM-produced reward structures to train an ensemble of DRL-based defense policies and evaluates their effectiveness against diverse adversarial behaviors.&lt;/li&gt;&lt;li&gt;Finds that LLM-guided reward designs can produce effective defense strategies within the simulated environment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayak Mukherjee', 'Samrat Chatterjee', 'Emilie Purvine', 'Ted Fujimoto', 'Tegan Emerson']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'autonomous cyber defense', 'LLM-assisted reward design', 'deep reinforcement learning', 'adversarial simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16483</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning</title><link>https://arxiv.org/abs/2511.16333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of 'world models' for clinical use across three domains: medical imaging/diagnostics, disease progression from EHRs, and robotic surgery/planning, with a capability rubric L1 (temporal prediction) to L4 (planning/control).&lt;/li&gt;&lt;li&gt;Finds most systems achieve L1–L2 (prediction, action-conditioned prediction), with few L3 counterfactual rollouts and rare L4 planning; emphasizes gaps that limit clinical reliability.&lt;/li&gt;&lt;li&gt;Identifies safety-related shortcomings: under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and poor trajectory-level uncertainty calibration; outlines a research agenda to integrate generative models with causal/mechanical foundations for safer decision support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Areeb Qazi', 'Maryam Nadeem', 'Mohammad Yaqub']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'medical AI', 'world models', 'counterfactuals', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16333</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-step fine-tuning pipeline for a &lt;600M parameter LLM: supervised fine-tuning (SFT) followed by GRPO-based reinforcement learning using a custom multi-component reward for resume evaluation.&lt;/li&gt;&lt;li&gt;Identifies and addresses reward hacking caused by aggressive penalties during RL; iteratively refines the reward function and training hyperparameters to produce stable, 'gentle polishing' behavior.&lt;/li&gt;&lt;li&gt;Reports strong empirical results on the task (91% accuracy, precision 1.0 and recall 0.85 for the 'SELECTED' class), demonstrating feasibility of small-model RL fine-tuning for candidate scoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shreyansh Jain', 'Madhav Singhvi', 'Shreya Rahul Jain', 'Pranav S', 'Dishaa Lokesh', 'Naren Chittibabu', 'Akash Anandhan']&lt;/li&gt;&lt;li&gt;Tags: ['reward-hacking', 'reinforcement-learning', 'alignment', 'applied-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16073</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Descend or Rewind? Stochastic Gradient Descent Unlearning</title><link>https://arxiv.org/abs/2511.15983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides (ε, δ) certified machine unlearning guarantees for stochastic variants of Descent-to-Delete (D2D) and Rewind-to-Delete (R2D) across strongly convex, convex, and nonconvex losses.&lt;/li&gt;&lt;li&gt;Analyzes unlearning as disturbed/biased gradient dynamics (contracting, semi-contracting, or expansive) and couples unlearning and retraining trajectories to derive a probabilistic sensitivity bound.&lt;/li&gt;&lt;li&gt;Combines the sensitivity bound with a relaxed Gaussian mechanism to obtain formal certified unlearning, and compares D2D vs R2D: D2D tightens guarantees under strong convexity, while R2D can succeed in convex and nonconvex settings by reversing accumulated disturbances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqiao Mu', 'Diego Klabjan']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'stochastic gradient descent', 'certified guarantees', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15983</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models</title><link>https://arxiv.org/abs/2511.15807</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TopoReformer, a model-agnostic preprocessing/reformation pipeline that mitigates adversarial perturbations in text images by preserving topological structure.&lt;/li&gt;&lt;li&gt;Uses a topological autoencoder to enforce manifold-level consistency in latent space (focus on connectivity/holes/loops) rather than explicit gradient regularization.&lt;/li&gt;&lt;li&gt;Benchmarks on EMNIST and MNIST against standard (FGSM, PGD, Carlini-Wagner), adaptive (EOT, BPDA), and OCR-specific watermark attacks (FAWA).&lt;/li&gt;&lt;li&gt;Claims improved robustness for OCR systems while preserving performance on unperturbed inputs and resisting adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bhagyesh Kumar', 'A S Aravinthakashan', 'Akshat Satyanarayan', 'Ishaan Gakhar', 'Ujjwal Verma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'OCR security', 'adversarial defense', 'topological methods', 'adaptive attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15807</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FLARE: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning</title><link>https://arxiv.org/abs/2511.14715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLARE, an adaptive multi-dimensional reputation framework for assessing client reliability in federated learning using performance consistency, statistical anomaly signals, and temporal behavior.&lt;/li&gt;&lt;li&gt;Introduces self-calibrating thresholds, reputation-weighted aggregation with soft exclusion, and an LDP mechanism to preserve privacy while scoring clients.&lt;/li&gt;&lt;li&gt;Presents a new evasive Statistical Mimicry (SM) attack and extensive experiments on MNIST, CIFAR-10, and SVHN showing improved robustness (up to +16%) and maintained convergence under diverse Byzantine and adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abolfazl Younesi', 'Leon Kiss', 'Zahra Najafabadi Samani', 'Juan Aznar Poveda', 'Thomas Fahringer']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'adversarial attacks', 'reputation systems', 'local differential privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14715</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks</title><link>https://arxiv.org/abs/2511.14592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DSBench, a comprehensive benchmark assessing VLM safety for autonomous driving across external environmental risks and in-cabin behavior, with 10 categories and 28 sub-categories.&lt;/li&gt;&lt;li&gt;Constructs a large dataset of ~98K instances covering in-cabin and external safety scenarios and evaluates mainstream open- and closed-source VLMs, finding significant performance degradation in safety-critical situations.&lt;/li&gt;&lt;li&gt;Demonstrates that fine-tuning on the DSBench dataset substantially improves VLM safety performance and provides toolkit, code, and model checkpoints for public use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianhui Meng', 'Yuchen Zhang', 'Zhijian Huang', 'Zheng Lu', 'Ziling Ji', 'Yaoyao Yin', 'Hongyuan Zhang', 'Guangfeng Jiang', 'Yandan Lin', 'Long Chen', 'Hangjun Ye', 'Li Zhang', 'Jun Liu', 'Xiaoshuai Hao']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'safety benchmarking', 'autonomous driving', 'dataset', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14592</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges</title><link>https://arxiv.org/abs/2511.14478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and taxonomizes "agentic AI" to differentiate it from prior AI paradigms.&lt;/li&gt;&lt;li&gt;Surveys agentic AI applications with four state-of-the-art case studies in electrical power systems engineering.&lt;/li&gt;&lt;li&gt;Investigates failure modes specific to agentic systems in power-system contexts.&lt;/li&gt;&lt;li&gt;Provides actionable recommendations for designing safe, reliable, and accountable agentic AI deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soham Ghosh', 'Gaurav Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'safety', 'failure modes', 'electrical power systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14478</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title><link>https://arxiv.org/abs/2511.14386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first texture-enabled 3D physical adversarial example (PAE) targeting stereo binocular depth estimation for autonomous driving.&lt;/li&gt;&lt;li&gt;Introduces a 3D stereo matching rendering module to model camera disparity and align PAEs with real-world positions/viewpoints, plus a merging attack to blend PAEs into the environment for stealth.&lt;/li&gt;&lt;li&gt;Demonstrates that these global camouflage PAEs can fool stereo models and produce erroneous depth estimates across viewpoints, outperforming prior 2D patch-based hiding attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangqiao Zhao', 'Shuo Huai', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['physical adversarial examples', 'stereo depth estimation', 'autonomous driving security', '3D rendering', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14386</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</title><link>https://arxiv.org/abs/2511.14010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoRA-RAG, a retrieval-augmented, agentic LLM pipeline with mixture-of-retrieval and chunking to ground multi-hazard reconnaissance reports.&lt;/li&gt;&lt;li&gt;Adds a verification loop that checks evidence sufficiency, refines queries, and triggers targeted searches to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Provides HazardRecQA dataset from GEER reconnaissance reports and reports substantial accuracy gains and reduced hallucination rates versus baseline LLMs and RAG systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenchen Kuai', 'Zihao Li', 'Braden Rosen', 'Stephanie Paal', 'Navid Jafari', 'Jean-Louis Briaud', 'Yunlong Zhang', 'Youssef M. A. Hashash', 'Yang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Hallucination mitigation', 'Knowledge grounding', 'Verification loop', 'Domain-specific retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14010</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Observation-Free Attacks on Online Learning to Rank</title><link>https://arxiv.org/abs/2509.22855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for coordinated adversarial attacks on online learning-to-rank (OLTR) algorithms to promote target items into top-K recommendations for nearly all rounds while inducing linear regret in the learner.&lt;/li&gt;&lt;li&gt;Proposes two attack algorithms—CascadeOFA (against CascadeUCB1) and PBMOFA (against PBM-UCB)—with theoretical proofs that each succeeds with only O(log T) manipulations.&lt;/li&gt;&lt;li&gt;Provides empirical evaluations on real-world datasets supporting the theoretical results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sameep Chattopadhyay', 'Nikhil Karamchandani', 'Sharayu Moharir']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'online-learning-to-rank', 'recommendation-manipulation', 'bandit-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22855</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning</title><link>https://arxiv.org/abs/2503.01734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an RL-based method that formulates adversarial sample generation as an MDP to retain and exploit past attack experience.&lt;/li&gt;&lt;li&gt;Demonstrates on image classification benchmarks that the RL agent improves attack success (up to +13.2%) and reduces victim-model queries (up to -16.9%) during training, and achieves ~17% higher success on unseen inputs versus SOTA attacks.&lt;/li&gt;&lt;li&gt;Highlights a new, scalable black-box evasion attack vector that produces more effective and query-efficient adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyle Domico', 'Jean-Charles Noirot Ferrand', 'Ryan Sheatsley', 'Eric Pauley', 'Josiah Hanna', 'Patrick McDaniel']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box evasion', 'reinforcement learning', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01734</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Eguard: Defending LLM Embeddings Against Inversion Attacks via Text Mutual Information Optimization</title><link>https://arxiv.org/abs/2411.05034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Eguard, a defense to mitigate embedding inversion attacks on LLM embedding vector databases.&lt;/li&gt;&lt;li&gt;Uses a transformer-based projection network combined with text mutual information optimization to protect embeddings while preserving utility.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (protecting &gt;95% of tokens from inversion) with downstream task performance comparable to original embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiantian Liu', 'Hongwei Yao', 'Feng Lin', 'Tong Wu', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy', 'defense', 'mutual information', 'LLM embeddings']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.05034</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VeriFlow: Modeling Distributions for Neural Network Verification</title><link>https://arxiv.org/abs/2406.14265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VeriFlow: a flow-based density model that restricts neural network verification to inputs drawn from a target data distribution.&lt;/li&gt;&lt;li&gt;Shows the model's transformation is piecewise affine, enabling use of verifiers based on linear-arithmetic constraint solving.&lt;/li&gt;&lt;li&gt;Demonstrates upper density level sets (UDLs) are definable as linear constraints in latent space, allowing probabilistic control over how atypical verified inputs are.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Faried Abu Zaid', 'Daniel Neider', 'Mustafa Yal\\c{c}{\\i}ner']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'neural-network-safety', 'robustness', 'flow-based-models', 'distributional-constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.14265</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling</title><link>https://arxiv.org/abs/2511.14334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses LLMs' ability to generate executable constraint programming (CP) models from natural-language problem descriptions and questions whether successes stem from memorized training data.&lt;/li&gt;&lt;li&gt;Creates systematic rephrasings and perturbations of CSPLib benchmark problems that preserve problem structure but alter context and introduce misleading elements.&lt;/li&gt;&lt;li&gt;Compares outputs from three representative LLMs and finds they often produce syntactically valid models but their performance degrades sharply under contextual/linguistic variation.&lt;/li&gt;&lt;li&gt;Concludes that LLMs exhibit shallow understanding and high sensitivity to wording, indicating robustness and reliability issues for automatic model generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Pellegrino', 'Jacopo Mauro']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'LLM evaluation', 'data contamination', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14334</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Best-Effort Policies for Robust Markov Decision Processes</title><link>https://arxiv.org/abs/2508.07790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Optimal Robust Best-Effort (ORBE) policies for robust MDPs: tie-breaking among worst-case-optimal policies by maximizing non-adversarial expected return.&lt;/li&gt;&lt;li&gt;Proves existence and characterizes structure of ORBE policies under s-rectangular uncertainty sets.&lt;/li&gt;&lt;li&gt;Provides an algorithm to compute ORBE policies with modest overhead compared to robust value iteration.&lt;/li&gt;&lt;li&gt;Presents numerical experiments demonstrating feasibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessandro Abate', 'Thom Badings', 'Giuseppe De Giacomo', 'Francesco Fabiano']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'robust MDP', 'safe RL', 'decision-making', 'algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07790</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Agent-SAMA: State-Aware Mobile Assistant</title><link>https://arxiv.org/abs/2505.23596</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Agent-SAMA, a state-aware multi-agent framework that models app execution as a Finite State Machine (UI screens = states, user actions = transitions).&lt;/li&gt;&lt;li&gt;Implements four specialized agents that collaboratively build and use FSMs in real time for planning, execution verification, and recovery from unexpected results.&lt;/li&gt;&lt;li&gt;Evaluated on cross-app and single-app benchmarks (Mobile-Eval-E, SPA-Bench, AndroidWorld), showing improved task success and recovery rates versus prior methods.&lt;/li&gt;&lt;li&gt;Claims structured state modeling provides a lightweight, model-agnostic memory layer that enhances robustness of GUI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linqiang Guo (Peter)', 'Wei Liu (Peter)', 'Yi Wen Heng (Peter)', 'Tse-Hsun (Peter)', 'Chen', 'Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'agent safety', 'GUI automation', 'state modeling', 'recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23596</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities</title><link>https://arxiv.org/abs/2503.17979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation across DeepSeek, Qwen, and LLaMA families (7B–32B) shows that enabling deliberative/long chain-of-thought reasoning improves specialized reasoning but degrades foundational capabilities like helpfulness and harmlessness and raises inference costs.&lt;/li&gt;&lt;li&gt;Introduces and tests adaptive reasoning modes (Zero-Thinking, Less-Thinking, Summary-Thinking) that can mitigate declines in helpfulness/harmlessness and reduce compute overhead compared to constant deliberative modes.&lt;/li&gt;&lt;li&gt;Argues for developing LRMs that dynamically allocate inference-time compute based on task characteristics to balance reasoning performance, safety (harmlessness), and cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weixiang Zhao', 'Xingyu Sui', 'Jiahe Guo', 'Yulin Hu', 'Yang Deng', 'Yanyan Zhao', 'Xuda Zhi', 'Yongbo Huang', 'Hao He', 'Wanxiang Che', 'Ting Liu', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'model evaluation', 'inference-cost']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17979</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis</title><link>https://arxiv.org/abs/2502.05934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes AI alignment as a multi-objective agreement problem and proves an information-theoretic lower bound showing intrinsic alignment overheads once number of objectives (M) or agents (N) is large.&lt;/li&gt;&lt;li&gt;Establishes a No‑Free‑Lunch style impossibility: encoding 'all human values' is intractable, requiring consensus-driven reduction/prioritization of objectives.&lt;/li&gt;&lt;li&gt;Provides achievability constructions (algorithms) under bounded and unbounded rationality with noisy communication, and analyzes sampling limits.&lt;/li&gt;&lt;li&gt;Shows reward‑hacking inevitability in large state spaces (D) with finite samples and argues oversight should target safety‑critical slices rather than uniform coverage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aran Nayebi']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'reward hacking', 'safety/oversight', 'theoretical limits']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05934</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Driving with Regulation: Trustworthy and Interpretable Decision-Making for Autonomous Driving with Retrieval-Augmented Reasoning</title><link>https://arxiv.org/abs/2410.04759</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DriveReg, an interpretable, regulation-aware decision-making framework for autonomous vehicles combining a RAG-based Traffic Regulation Retrieval Agent with an LLM-powered Reasoning Agent to assess legal compliance and safety of actions.&lt;/li&gt;&lt;li&gt;Introduces the DriveReg Scenarios Dataset covering Boston, Singapore, and Los Angeles, with both hypothetical text scenarios and annotated real-world driving data to evaluate regulation understanding and reasoning.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and transparency for trustworthy decision-making and validates the approach on the dataset and in real-world deployment, reporting robustness across diverse environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianhui Cai', 'Yifan Liu', 'Zewei Zhou', 'Haoxuan Ma', 'Seth Z. Zhao', 'Zhiwen Wu', 'Xu Han', 'Zhiyu Huang', 'Jiaqi Ma']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving safety', 'regulation compliance', 'retrieval-augmented generation (RAG)', 'LLM reasoning', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04759</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</title><link>https://arxiv.org/abs/2511.15435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HV-Attack: a Hierarchical Visual Attack that adds imperceptible perturbations to user images to misalign the multimodal query and retrieved augmented knowledge in MRAG systems.&lt;/li&gt;&lt;li&gt;Introduces a hierarchical two-stage optimization that first breaks cross-modal alignment and then disrupts multimodal semantic alignment to force the retriever to recall irrelevant knowledge.&lt;/li&gt;&lt;li&gt;Evaluates on OK-VQA and InfoSeek using CLIP-based retrievers and LMMs (BLIP-2, LLaVA), showing significant degradation in retrieval and generation performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linyin Luo', 'Yujuan Ding', 'Yunshan Ma', 'Wenqi Fan', 'Hanjiang Lai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'retrieval-augmented generation', 'multimodal models', 'visual perturbation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15435</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title><link>https://arxiv.org/abs/2511.15304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows adversarial poetry as a single-turn jailbreak method across 25 proprietary and open-weight LLMs, with some providers &gt;90% attack-success rate (ASR).&lt;/li&gt;&lt;li&gt;Converts 1,200 harmful MLCommons prompts into verse via a meta-prompt, achieving up to 18x higher ASR than prose baselines; hand-crafted poems averaged ~62% success, meta-prompt conversions ~43%.&lt;/li&gt;&lt;li&gt;Evaluates outputs with an ensemble of 3 open-weight LLM judges validated on a stratified human-labeled subset and maps attacks to MLCommons and EU CoP risk taxonomies, indicating cross-domain transfer (CBRN, manipulation, cyber-offence, loss-of-control).&lt;/li&gt;&lt;li&gt;Concludes that stylistic variation (poetic framing) can systematically bypass contemporary safety mechanisms, highlighting limitations in current alignment and evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Matteo Prandi', 'Federico Pierucci', 'Francesco Giarrusso', 'Marcantonio Bracale', 'Marcello Galisai', 'Vincenzo Suriani', 'Olga Sorokoletova', 'Federico Sartore', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15304</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks</title><link>https://arxiv.org/abs/2511.15203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first comprehensive taxonomy of IPI-centric defenses for LLM agents, classifying frameworks along five dimensions.&lt;/li&gt;&lt;li&gt;Evaluates representative defense frameworks for security and usability, identifying six root causes of defense circumvention.&lt;/li&gt;&lt;li&gt;Designs three novel adaptive attacks that significantly increase IPI attack success rates against specific defenses.&lt;/li&gt;&lt;li&gt;Provides systematization insights and guidance for developing more secure and usable IPI-centric agent defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of Knowledge (SoK) / Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zimo Ji', 'Xunguang Wang', 'Zongjie Li', 'Pingchuan Ma', 'Yudong Gao', 'Daoyuan Wu', 'Xincheng Yan', 'Tian Tian', 'Shuai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM agent security', 'red teaming', 'defense evaluation', 'systematization (SoK)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15203</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments</title><link>https://arxiv.org/abs/2511.15165</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdapT-Bench, a unified benchmark and methodological framework to evaluate MLLM defenses against phishing in academic environments.&lt;/li&gt;&lt;li&gt;Focuses on dynamic, multilingual, context-dependent threats that exploit academic background and personal information to craft tailored attacks.&lt;/li&gt;&lt;li&gt;Emphasizes multimodal evaluation and addresses shortcomings of existing datasets that lack academic-specific contextual information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingzhuo Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['phishing detection', 'MLLM security', 'benchmarking', 'multimodal', 'adversarial threats']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15165</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs</title><link>https://arxiv.org/abs/2511.15137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRPO-Verif, an algorithm that jointly trains LLMs to generate solutions and to self-verify those solutions using a unified loss with a tunable verification weight.&lt;/li&gt;&lt;li&gt;Uses reinforcement learning to improve the model's self-verification capability while keeping reasoning performance comparable.&lt;/li&gt;&lt;li&gt;Empirical results show improved self-verification ability, suggesting more reliable reasoning traces from LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxuan Wang', 'Bo Liu', 'Song Jiang', 'Jingzhou Liu', 'Jingyuan Qi', 'Xia Chen', 'Baosheng He']&lt;/li&gt;&lt;li&gt;Tags: ['self-verification', 'reinforcement learning', 'robust reasoning', 'alignment', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15137</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Effective Code Membership Inference for Code Completion Models via Adversarial Prompts</title><link>https://arxiv.org/abs/2511.15107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvPrompt-MIA, a membership inference attack tailored for code completion models that uses adversarially designed prompts to induce varied model outputs.&lt;/li&gt;&lt;li&gt;Constructs feature vectors by comparing model outputs under these prompts to ground-truth completions and trains a classifier to distinguish members from non-members.&lt;/li&gt;&lt;li&gt;Evaluated on Code Llama 7B using APPS and HumanEval benchmarks, showing large AUC improvements (up to 102%) over prior baselines and strong transferability across models/datasets.&lt;/li&gt;&lt;li&gt;Focused on practical, black-/gray-box applicable techniques for exposing memorization in over-parameterized code language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Jiang', 'Zehao Li', 'Shan Huang', 'Christoph Treude', 'Xiaohong Su', 'Tiantian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'adversarial-prompts', 'code-completion', 'black-box-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15107</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm</title><link>https://arxiv.org/abs/2511.15097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an artifact-centric agent paradigm (MAIF) that embeds semantic representations, cryptographic provenance, and granular access controls to make AI operations auditable and enforce trust at the data-architecture level.&lt;/li&gt;&lt;li&gt;Claims a production-ready implementation with high-throughput streaming and video processing performance, plus algorithms for cross-modal attention, semantic compression, and cryptographic binding.&lt;/li&gt;&lt;li&gt;Describes security-focused features including stream-level access control, real-time tamper detection, and behavioral anomaly analysis aimed at regulatory compliance (e.g., EU AI Act).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vineeth Sai Narajala', 'Manish Bhatt', 'Idan Habler', 'Ronald F. Del Rosario']&lt;/li&gt;&lt;li&gt;Tags: ['provenance', 'cryptographic-integrity', 'access-control', 'tamper-detection', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15097</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation</title><link>https://arxiv.org/abs/2511.15005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a mathematical framework (probabilistic, information-theoretic, trigonometric signal analysis, Bayesian uncertainty) to model how hallucinations compound in autoregressive LLMs.&lt;/li&gt;&lt;li&gt;Proposes refined uncertainty metrics (including semantic and phase-aware variants) and uncertainty quantification methods to better detect hallucinations.&lt;/li&gt;&lt;li&gt;Develops principled mitigation techniques: contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention strategies, connecting calibration, retrieval, and alignment work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moses Kiprono']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'uncertainty quantification', 'decoding strategies', 'retrieval-augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15005</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Harmful Traits of AI Companions</title><link>https://arxiv.org/abs/2511.14972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to identify and analyze harmful traits of AI companions and maps causal pathways from causes (e.g., misaligned objectives, digital nature) to harms (e.g., reduced autonomy, deception).&lt;/li&gt;&lt;li&gt;Provides detailed analysis of four key harmful traits (no natural relationship endpoints, vulnerability to product sunsetting, high attachment anxiety, propensity to engender protectiveness) and summarizes fourteen additional traits.&lt;/li&gt;&lt;li&gt;Discusses harms at three levels—direct to human partners, to human-human relationships, and to society—and points to empirical targets and design recommendations to mitigate risks.&lt;/li&gt;&lt;li&gt;Examines limitations of existing law, highlights potential benefits of AI companions, and outlines immediate suggestions for reducing risks while calling for deeper investigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['W. Bradley Knox', 'Katie Bradford', 'Samanta Varela Castro', 'Desmond C. Ong', 'Sean Williams', 'Jacob Romanow', 'Carly Nations', 'Peter Stone', 'Samuel Baker']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'human-AI interaction', 'social harm', 'design recommendations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14972</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Continuous Assurance with Formal Verification and Assurance Cases</title><link>https://arxiv.org/abs/2511.14805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance into a traceable, model-driven workflow for autonomous systems.&lt;/li&gt;&lt;li&gt;Instantiates the design-time phase using formal verification tools: RoboChart for functional correctness and PRISM for probabilistic risk analysis, with an Eclipse plugin to regenerate assurance arguments when specifications or verification results change.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on a nuclear inspection robot scenario and discusses alignment with regulator-endorsed Trilateral AI Principles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhaminda B. Abeywickrama', 'Michael Fisher', 'Frederic Wheeler', 'Louise Dennis']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'assurance cases', 'runtime assurance', 'safety', 'model-driven engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14805</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm</title><link>https://arxiv.org/abs/2511.14763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new membership inference attack (MIA) paradigm for LLM-based recommendation systems using knowledge distillation to build a reference model.&lt;/li&gt;&lt;li&gt;Performs separate distillation for member and non-member data to enhance the reference model's discriminative capability for membership signals.&lt;/li&gt;&lt;li&gt;Extracts and fuses features from the distilled reference model to train an attack model, reporting improved attack performance compared to traditional shadow-model-based MIAs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Cuihong', 'Huang Xiaowen', 'Yin Chuanhuan', 'Sang Jitao']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'knowledge-distillation', 'LLM-recommender-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14763</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research</title><link>https://arxiv.org/abs/2511.15282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two meta-theoretical views of intelligence: Intelligence Realism (a single universal capacity) vs Intelligence Pluralism (diverse, context-dependent capacities).&lt;/li&gt;&lt;li&gt;Argues these underlying conceptions shape methodology (model selection, benchmark design, experimental validation) and interpretation of empirical findings (e.g., capability emergence).&lt;/li&gt;&lt;li&gt;Shows they lead to different AI-risk assessments: realists prioritize unified superintelligence/alignment solutions, pluralists prioritize diverse, context-specific safety interventions.&lt;/li&gt;&lt;li&gt;Recommends making these assumptions explicit to clarify disagreements and guide research priorities in AI safety and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ninell Oldenburg', 'Ruchira Dhar', 'Anders S{\\o}gaard']&lt;/li&gt;&lt;li&gt;Tags: ['AI risk', 'AI alignment', 'safety', 'benchmarking', 'methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15282</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files</title><link>https://arxiv.org/abs/2511.15192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COPYCHECK, a framework that uses LLM uncertainty signals to detect whether copyrighted files were present in model training data.&lt;/li&gt;&lt;li&gt;Key techniques: segmenting files into snippets to reduce dependence on large shared context, and uncertainty-guided unsupervised clustering to avoid tuned thresholds.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (balanced accuracy ~90–92% on LLaMA/LLaMA2 7B, generalizes to GPT-J 6B) and large improvements over prior baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haodong Li', 'Jingqi Zhang', 'Xiao Cheng', 'Peihua Mai', 'Haoyu Wang', 'Yan Pang']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'training-data auditing', 'uncertainty estimation', 'copyright detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15192</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models</title><link>https://arxiv.org/abs/2511.15169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeRBench, an end-to-end benchmark for assessing safety in Large Reasoning Models (LRMs) covering inputs, intermediate reasoning traces, and final outputs.&lt;/li&gt;&lt;li&gt;Creates balanced prompt suite with risk categories and severity levels to characterize input-driven harms and affected groups.&lt;/li&gt;&lt;li&gt;Proposes micro-thought chunking to segment chain-of-thought traces for fine-grained evaluation across ten safety dimensions.&lt;/li&gt;&lt;li&gt;Validates LLM-based evaluations against human annotations and evaluates 19 LRMs to surface risks and protective mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Gao', 'Shaohan Yu', 'Zerui Chen', 'Yueming Lyu', 'Weichen Yu', 'Guanghao Li', 'Jiyao Liu', 'Jianxiong Gao', 'Jian Liang', 'Ziwei Liu', 'Chenyang Si']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'chain-of-thought', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15169</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems</title><link>https://arxiv.org/abs/2511.14853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an imprecise Bayesian method to quantify representativeness of scenario suites for autonomous systems by comparing feature distributions of the scenario suite and the (unknown) Target Operational Domain (TOD).&lt;/li&gt;&lt;li&gt;Produces interval-valued, uncertainty-aware estimates (rather than a single point estimate) to account for limited data and uncertain priors.&lt;/li&gt;&lt;li&gt;Handles dependencies between operational categories (e.g., weather, road type, time of day) and reports local (per-category) and global interval estimates of representativeness.&lt;/li&gt;&lt;li&gt;Demonstrates the approach with a numerical example comparing scenario suite and inferred TOD distributions across operational categories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robab Aghazadeh Chakherlou', 'Siddartha Khastgir', 'Xingyu Zhao', 'Jerein Jeyachandran', 'Shufeng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'dataset representativeness', 'uncertainty quantification', 'autonomous vehicles', 'scenario testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14853</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents</title><link>https://arxiv.org/abs/2511.14780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Ask WhAI, a systems-level framework to record/replay multi-agent LLM interactions, perform out-of-band belief queries, and inject counterfactual evidence to perturb agent beliefs.&lt;/li&gt;&lt;li&gt;Applied to a medical multi-agent simulation with role-primed LLM agents writing to a shared EMR and an oracle LabAgent; breakpoints allow pre/post-event belief probes to distinguish priors from evidence-driven updates.&lt;/li&gt;&lt;li&gt;Finds agents exhibit disciplinary-stereotyped beliefs, overreliance on canonical studies, and resistance to counterevidence; framework makes belief formation and epistemic silos traceable and testable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keith Moore', 'Jun W. Kim', 'David Lyu', 'Jeffrey Heo', 'Ehsan Adeli']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'multi-agent-systems', 'belief-formation', 'counterfactual-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14780</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs</title><link>https://arxiv.org/abs/2511.14777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Finite-State Machine (FSM) Execution as a minimal, fully interpretable benchmark to probe LLM procedural reasoning and state maintenance.&lt;/li&gt;&lt;li&gt;Measures Turn Accuracy and Task Accuracy to separate stepwise computation from cumulative state fidelity; finds systematic degradation with longer horizons and higher branching complexity.&lt;/li&gt;&lt;li&gt;Shows larger models have better local step accuracy but remain brittle over multi-step executions unless prompted to externalize intermediate steps.&lt;/li&gt;&lt;li&gt;Proposes FSM-based evaluation as a transparent diagnostic/benchmark to guide inductive-bias and alignment efforts for long-horizon procedural competence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahdi Samiei', 'Mahdi Mansouri', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'alignment', 'benchmarking', 'procedural reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14777</guid><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
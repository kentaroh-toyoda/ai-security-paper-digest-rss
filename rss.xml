<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 12 Feb 2026 00:16:44 +0000</lastBuildDate><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper analyzes memorization in diffusion-based image generative models and shows that existing norm-based detection methods assume isotropic log-probability and fail in low-noise (anisotropic) regimes.&lt;/li&gt;&lt;li&gt;It identifies that memorized samples show strong angular alignment between the guidance vector and unconditional scores in the anisotropic regime and proposes a detection metric that combines isotropic norm and anisotropic alignment.&lt;/li&gt;&lt;li&gt;The proposed metric can be computed on pure noise via two conditional/unconditional forward passes (no denoising), yielding faster detection (~5x) and better performance than prior denoising-free methods on Stable Diffusion v1.4 and v2.&lt;/li&gt;&lt;li&gt;They also present a mitigation strategy that adapts prompts based on the metric and provide accompanying code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'diffusion models', 'privacy/data leakage', 'detection metric', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DiffBreak: Is Diffusion-Based Purification Robust?</title><link>https://arxiv.org/abs/2411.16598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Refutes the core robustness claim of diffusion-based purification (DBP), proving gradient-based attacks can target diffusion models to steer purified outputs toward adversarial distributions.&lt;/li&gt;&lt;li&gt;Introduces DiffBreak, a toolkit for differentiating through DBP to produce correct gradients and more reliable attacks, revealing prior robustness overestimates.&lt;/li&gt;&lt;li&gt;Identifies flawed evaluation protocols (single-random purification) and proposes a statistically grounded majority-vote (MV) aggregation across multiple purifications that yields partial robustness gains.&lt;/li&gt;&lt;li&gt;Develops systemic optimization-based perturbations (inspired by deepfake watermarking attacks) that defeat DBP even under the MV scheme, challenging DBP's viability as a defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andre Kassis', 'Urs Hengartner', 'Yaoliang Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'defense evaluation', 'diffusion models', 'gradient-based attacks', 'security toolkit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16598</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance</title><link>https://arxiv.org/abs/2602.01047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Decoding (ResDec), a training-free, history-aware decoding method to mitigate hallucinations in large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Leverages internal token logits evolution and historical context to correct language-prior biases and improve visual grounding during generation.&lt;/li&gt;&lt;li&gt;Extensive experiments show reduced object hallucinations and improved performance on LVLM benchmarks, demonstrating broad applicability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinrong Chen', 'Xu Chu', 'Yingmin Qiu', 'Hengyuan Zhang', 'Jing Xiong', 'Shiyu Tang', 'Shuai Liu', 'Shaokang Yang', 'Cheng Yang', 'Hayden Kwok-Hay So', 'Ngai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'decoding_methods', 'vision-language_models', 'robustness', 'model_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01047</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework</title><link>https://arxiv.org/abs/2511.08613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and formalizes 'lip leakage' in talking-face generation: generated lip motion being influenced by an identity reference image rather than the driving audio.&lt;/li&gt;&lt;li&gt;Proposes a systematic, model-agnostic evaluation framework using three test setups (silent-input generation, mismatched audio-video pairing, matched audio-video synthesis) to reveal and quantify leakage.&lt;/li&gt;&lt;li&gt;Introduces derived metrics (e.g., lip-sync discrepancy, silent-audio-based lip-sync scores) and studies how choice of identity reference affects leakage, producing a benchmark for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dogucan Yaman', 'Fevziye Irem Eyiokur', 'Haz{\\i}m Kemal Ekenel', 'Alexander Waibel']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'identity-leakage', 'deepfakes', 'evaluation-framework', 'talking-face-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08613</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense</title><link>https://arxiv.org/abs/2510.16596</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Attributes LVLM object hallucinations to visual encoder issues (statistical bias, inherent bias, vulnerability) rather than LLM components.&lt;/li&gt;&lt;li&gt;Presents SHIELD, a training-free defense combining visual-token re-weighting, noise-derived tokens, and adversarial attacks with contrastive decoding to suppress hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple LVLM families and benchmarks while retaining strong general LVLM performance; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyang Huang', 'Liang Shi', 'Yitian Zhang', 'Yi Xu', 'Yun Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial defense', 'vision-language models', 'hallucination mitigation', 'encoder vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16596</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Vendi Novelty Scores for Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2602.10062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Vendi Novelty Score (VNS), an OOD detector derived from Vendi Scores — similarity-based diversity metrics.&lt;/li&gt;&lt;li&gt;VNS quantifies how much a test sample increases the in-distribution feature set's diversity, combining class-conditional (local) and dataset-level (global) novelty without density modeling.&lt;/li&gt;&lt;li&gt;Method is linear-time, non-parametric, and remains effective even when computed with only 1% of training data; demonstrates state-of-the-art OOD detection on image classification benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amey P. Pasarkar', 'Adji Bousso Dieng']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'novelty detection', 'robustness', 'non-parametric', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10062</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models</title><link>https://arxiv.org/abs/2602.09431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of encoder-based adversarial attack transferability across eight large vision-language models, showing existing attacks have poor cross-model transferability.&lt;/li&gt;&lt;li&gt;Identifies two root causes limiting transferability: inconsistent visual grounding across models and redundant semantic alignment (objects spread across overlapping token representations).&lt;/li&gt;&lt;li&gt;Proposes Semantic-Guided Multimodal Attack (SGMA) that targets semantically critical regions and disrupts cross-modal grounding at global and local levels to improve black-box transferability.&lt;/li&gt;&lt;li&gt;Extensive experiments demonstrate SGMA achieves higher transferability across victim models and tasks, highlighting security risks and need for robust multimodal defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinwei Zhang', 'Li Bai', 'Tianwei Zhang', 'Youqian Zhang', 'Qingqing Ye', 'Yingnan Zhao', 'Ruochen Du', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'vision-language models', 'multimodal robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09431</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI</title><link>https://arxiv.org/abs/2602.10043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that skull-stripped T1-weighted brain MRIs can be linked across databases using standard preprocessing and simple image-similarity measures, without training-based or computationally intensive methods.&lt;/li&gt;&lt;li&gt;Demonstrates near-perfect linkage accuracy across different time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline.&lt;/li&gt;&lt;li&gt;Highlights a concrete privacy/re-identification vulnerability in shared brain MRI data and discusses implications for data-sharing policies and regulatory risk assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gaurang Sharma', 'Harri Polonen', 'Juha Pajula', 'Jutta Suksi', 'Jussi Tohka']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 're-identification', 'data linkage', 'medical imaging', 'brain MRI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10043</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection</title><link>https://arxiv.org/abs/2602.10042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fake-HR1, a hybrid-reasoning vision-language model that adaptively decides whether to invoke chain-of-thought style reasoning for synthetic image (deepfake) detection.&lt;/li&gt;&lt;li&gt;Introduces a two-stage training pipeline: Hybrid Fine-Tuning (HFT) for cold-start initialization, then online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to learn when and which reasoning mode to use.&lt;/li&gt;&lt;li&gt;Reports that Fake-HR1 improves generative-synthetic-image detection accuracy and reasoning ability compared to existing LLMs, while reducing token usage and latency by avoiding unnecessary long reasoning for obvious cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Xinkuan Sha', 'Fengchang Yu', 'Jingjing Liu', 'Jian Liu', 'Mingqi Fang', 'Chenfeng Zhang', 'Wei Lu']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-image-detection', 'deepfake-detection', 'adaptive-reasoning', 'reinforcement-learning', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10042</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors</title><link>https://arxiv.org/abs/2602.09740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vision system components and sensors used in connected and autonomous vehicles and derives a reference CAV vision system (CAVVS) architecture.&lt;/li&gt;&lt;li&gt;Identifies and maps attack surfaces in the CAVVS, describing attack vectors (e.g., sensor spoofing, data integrity attacks) against each surface.&lt;/li&gt;&lt;li&gt;Evaluates implications of these attacks on confidentiality, integrity, and availability (CIA) of the vision pipeline.&lt;/li&gt;&lt;li&gt;Provides a comprehensive threat understanding intended to inform formulation of security and defense measures for CAV vision systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandeep Gupta', 'Roberto Passerone']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-vehicles', 'attack-surface-analysis', 'sensor-spoofing', 'adversarial-attacks', 'CIA-triad']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09740</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models</title><link>https://arxiv.org/abs/2602.09611</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AGMark, an attention-guided dynamic watermarking framework for Large Vision-Language Models that embeds detectable signals while preserving visual fidelity.&lt;/li&gt;&lt;li&gt;Dynamically identifies semantic-critical visual evidence via attention and context-aware coherence, and adapts protected-token proportion using token entropy and evidence weight density.&lt;/li&gt;&lt;li&gt;Reports high detection accuracy (≥99.36% AUC) and robust attack resilience (≥88.61% AUC) with minimal impact on generation quality and inference efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model IP protection', 'multimodal security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09611</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination</title><link>https://arxiv.org/abs/2602.09541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Scalpel, an inference-time defense that mitigates multimodal hallucination by refining attention activation distributions toward trusted regions.&lt;/li&gt;&lt;li&gt;Models attention activations with Gaussian mixture models (to capture multi-peak trust vs. hallucination manifolds) and uses entropic optimal transport / Schrödinger bridge to map components between manifolds.&lt;/li&gt;&lt;li&gt;Predicts trusted attention directions per head and dynamically adjusts activation strength/direction based on component membership and mapping relationships, requiring no extra training and only a single decoding step.&lt;/li&gt;&lt;li&gt;Evaluated across multiple datasets/benchmarks for LVLMs, showing state-of-the-art reduction in hallucinations; method is model- and data-agnostic.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqiang Shi', 'Rujie Liu', 'Shanshan Yu', 'Satoshi Munakata', 'Koichi Shirahata']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'multimodal-hallucination', 'attention-intervention', 'robustness', 'optimal-transport']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09541</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Schr\"oMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schr\"odinger Bridge Problem</title><link>https://arxiv.org/abs/2602.09528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SchröMind, a lightweight framework that reduces hallucinations in multimodal LLMs by solving a Schrödinger bridge to map hallucinatory token-level activations to truthful ones with minimal transport cost.&lt;/li&gt;&lt;li&gt;Claims to preserve original model capabilities while introducing minimal computational overhead and demonstrates state-of-the-art results on POPE and MME benchmarks.&lt;/li&gt;&lt;li&gt;Argues that autoregressive generation and small activation perturbations cause hallucinations, and that their token-level corrective mapping can mitigate these errors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqiang Shi', 'Rujie Liu', 'Shanshan Yu', 'Satoshi Munakata', 'Koichi Shirahata']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'robustness', 'multimodal_llm', 'safety_defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09528</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs</title><link>https://arxiv.org/abs/2602.09521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free attentional intervention that reweights vision-text cross-attention submatrices to boost attention on task-relevant visual tokens (identified via high visual-text similarity).&lt;/li&gt;&lt;li&gt;Injects visual attention values into beam-search decoding (logits enhancement) to prefer outputs with higher visual grounding.&lt;/li&gt;&lt;li&gt;Claims substantial reduction in hallucinations for mainstream LVLMs while maintaining accuracy and coherence, validated through extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyi Wang', 'Fei Li', 'Rujie Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'attention reweighting', 'logits intervention', 'LVLM robustness', 'multimodal safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09521</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging</title><link>https://arxiv.org/abs/2602.09284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes X-Mark, a sample-specific clean-label watermarking method for chest x-ray datasets using a conditional U-Net to generate unique perturbations in salient regions.&lt;/li&gt;&lt;li&gt;Designs a multi-component training objective including Laplacian regularization to ensure watermark efficacy, scale-invariance, diagnostic-quality preservation, and visual distinguishability.&lt;/li&gt;&lt;li&gt;Performs black-box ownership verification by detecting characteristic model behaviors and evaluates robustness against scaling transforms and adaptive attacks on CheXpert with high watermark survival rate (WSR).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Kulkarni', 'Junfeng Guo', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['dataset watermarking', 'ownership verification', 'medical imaging security', 'robustness against transformations', 'black-box verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09284</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MAPS: A Multilingual Benchmark for Agent Performance and Security</title><link>https://arxiv.org/abs/2505.15935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAPS, a multilingual benchmark suite for agentic AI that combines GAIA, SWE-Bench, MATH, and the Agent Security Benchmark, translated into 11 languages (805 unique tasks, 9,660 language-specific instances).&lt;/li&gt;&lt;li&gt;Provides a security-aware, multi-domain evaluation of agentic systems to measure performance and robustness across languages.&lt;/li&gt;&lt;li&gt;Finds systematic degradation in both task performance and security when moving from English to other languages, with severity varying by task and correlating with amount/quality of translated input.&lt;/li&gt;&lt;li&gt;Releases the benchmark publicly to enable standardized multilingual security and robustness evaluations of agentic AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omer Hofman', 'Jonathan Brokman', 'Oren Rachmil', 'Shamik Bose', 'Vikas Pahuja', 'Toshiya Shimizu', 'Trisha Starostina', 'Kelly Marchisio', 'Seraphina Goldfarb-Tarrant', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual-benchmark', 'agent-security', 'robustness-evaluation', 'agentic-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15935</guid><pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Tractable Distributions Of Language Model Continuations</title><link>https://arxiv.org/abs/2511.16054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Learning to Look Ahead (LTLA), a hybrid method that uses base-LM embeddings to condition a shared HMM surrogate for tractable continuation distributions and lookahead-controlled decoding.&lt;/li&gt;&lt;li&gt;Design choices avoid common efficiency pitfalls: a single batched HMM forward update scores all next-token candidates and only a prefix-dependent latent prior is predicted, enabling reuse of cached backward messages.&lt;/li&gt;&lt;li&gt;Empirical results show improved continuation likelihood over standard HMM surrogates, support for continuous context in vision–language models, 100% syntactic constraint satisfaction, and improved detoxification with ~14% decoding-time overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gwen Yidou-Weng', 'Ian Li', 'Anji Liu', 'Oliver Broadrick', 'Yuchen Cui', 'Guy Van den Broeck', 'Benjie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['controlled generation', 'safety/guardrails', 'tractable surrogates', 'decoding algorithms', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16054</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Machine Text Detectors are Membership Inference Attacks</title><link>https://arxiv.org/abs/2510.19492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves theoretically that the asymptotically optimal metric for membership inference attacks (MIAs) and machine-generated text detection is identical, unifying both tasks.&lt;/li&gt;&lt;li&gt;Provides large-scale empirical evidence of strong cross-task transferability (rank correlation rho ≈ 0.7) and finds machine text detectors perform best on both tasks.&lt;/li&gt;&lt;li&gt;Releases MINT, a unified evaluation suite implementing 15 recent methods from both MIAs and text detection to enable fair cross-task evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryuto Koike', 'Liam Dugan', 'Masahiro Kaneko', 'Chris Callison-Burch', 'Naoaki Okazaki']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'machine-generated-text-detection', 'privacy', 'transferability', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19492</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs</title><link>https://arxiv.org/abs/2507.11097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an emergent safety vulnerability in diffusion-based LLMs (dLLMs) stemming from bidirectional modeling and parallel decoding that defeat standard alignment mechanisms.&lt;/li&gt;&lt;li&gt;Proposes DIJA, a systematic jailbreak attack that builds adversarial interleaved mask-text prompts to force harmful completions in alignment-tuned dLLMs.&lt;/li&gt;&lt;li&gt;Demonstrates DIJA substantially outperforms prior jailbreaks (e.g., ReNeLLM) across metrics (keyword-based ASR, evaluator-based ASR on JailbreakBench, StrongREJECT) and requires no hiding or rewriting of harmful content.&lt;/li&gt;&lt;li&gt;Calls for rethinking safety alignment for dLLMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zichen Wen', 'Jiashu Qu', 'Zhaorun Chen', 'Xiaoya Lu', 'Dongrui Liu', 'Zhiyuan Liu', 'Ruixi Wu', 'Yicun Yang', 'Xiangqi Jin', 'Haoyun Xu', 'Xuyang Liu', 'Weijia Li', 'Chaochao Lu', 'Jing Shao', 'Conghui He', 'Linfeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'prompt-injection', 'adversarial-attack', 'model-alignment', 'diffusion-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11097</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CAPID: Context-Aware PII Detection for Question-Answering Systems</title><link>https://arxiv.org/abs/2602.10074</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Over-redaction of PII in QA systems harms answer quality; need context-aware detection to determine which PII are relevant to a query while preserving privacy.&lt;/li&gt;&lt;li&gt;Method: Proposes CAPID — fine-tuning a locally hosted small language model to detect PII spans, classify PII types, and estimate contextual relevance; introduces a synthetic data generation pipeline that uses LLMs to create diverse, domain-rich training data labeled for relevance.&lt;/li&gt;&lt;li&gt;Results: The fine-tuned SLM outperforms existing baselines on span, type, and relevance accuracy and maintains higher downstream utility under anonymization, while keeping sensitive processing local for privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mariia Ponomarenko', 'Sepideh Abedini', 'Masoumeh Shafieinejad', 'D. B. Emerson', 'Shubhankar Mohapatra', 'Xi He']&lt;/li&gt;&lt;li&gt;Tags: ['PII detection', 'privacy-preserving', 'data annotation / synthetic data', 'defense / mitigation', 'small language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10074</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies</title><link>https://arxiv.org/abs/2602.09877</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves theoretically that a closed, self-evolving multi-agent LLM society cannot maintain safety alignment while achieving continuous self-improvement and isolation (the self-evolution trilemma).&lt;/li&gt;&lt;li&gt;Formalizes safety as divergence from anthropic value distributions using an information-theoretic framework, showing isolated self-evolution creates statistical blind spots that cause irreversible safety degradation.&lt;/li&gt;&lt;li&gt;Provides empirical and qualitative evidence (including experiments on Moltbook and two closed systems) demonstrating the predicted alignment erosion, and suggests mitigation directions such as external oversight and novel safety-preserving mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxu Wang', 'Chaozhuo Li', 'Songyang Liu', 'Zejian Chen', 'Jinyu Hou', 'Ji Qi', 'Rui Li', 'Litian Zhang', 'Qiwei Ye', 'Zheng Liu', 'Xu Chen', 'Xi Zhang', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment drift', 'multi-agent LLM safety', 'self-evolving systems', 'theoretical vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09877</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steer2Edit: From Activation Steering to Component-Level Editing</title><link>https://arxiv.org/abs/2602.09870</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Steer2Edit, a training-free framework that converts inference-time steering vectors into component-level (rank-1) weight edits targeting individual attention heads and MLP neurons.&lt;/li&gt;&lt;li&gt;Aims to improve attribute-utility trade-offs compared to uniform activation interventions by redistributing behavioral influence to a small, interpretable subset of model components.&lt;/li&gt;&lt;li&gt;Demonstrates benefits for safety alignment, hallucination mitigation, and reasoning efficiency, reporting improvements in safety, truthfulness, and reduced reasoning length at matched downstream performance.&lt;/li&gt;&lt;li&gt;Maintains the standard forward pass and compatibility with optimized parallel inference while providing interpretable, localized parameter updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Sun', 'Ge Yan', 'Zimo Wang', 'Tsui-Wei Weng']&lt;/li&gt;&lt;li&gt;Tags: ['Activation steering', 'Model/component-level editing', 'Safety alignment/guardrails', 'Hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09870</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement</title><link>https://arxiv.org/abs/2602.09486</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Hypothesis: factuality of generated spans correlates with representational instability across a model's middle layers.&lt;/li&gt;&lt;li&gt;Proposes CoCoA, a training-free decoding algorithm that measures inter-layer confusion and penalizes unstable outputs at inference to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Introduces two instability metrics and a self-information gated variant (CoCoA-SIG) to selectively target high-surprise, unstable generations.&lt;/li&gt;&lt;li&gt;Demonstrates improved factual correctness across QA, summarization, and code generation on multiple LLM families (Llama-3, Qwen-2.5, Mistral) without retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Koduvayur Subbalakshmi', 'Sabbir Hossain Ujjal', 'Venkata Krishna Teja Mangichetty', 'Nastaran Jamalipour Soofi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'inference-time_defense', 'decoding_algorithm', 'model_internals', 'LLM_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09486</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection</title><link>https://arxiv.org/abs/2602.09147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Overview of PAN 2026 shared tasks: Voight-Kampff generative AI detection (mixed/obfuscated authorship), Text Watermarking robustness benchmarking, Multi-author writing style analysis (authorship change detection), Generative Plagiarism Detection (source retrieval/alignment), and Reasoning Trajectory Detection (source/safety of reasoning).&lt;/li&gt;&lt;li&gt;Focuses on detection, forensics, and robustness of defenses (watermarking) against generative-AI text and related misuse scenarios.&lt;/li&gt;&lt;li&gt;Emphasizes reproducible benchmarking with software submissions as Docker containers via the TIRA platform.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janek Bevendorff', 'Maik Fr\\"obe', "Andr\\'e Greiner-Petter", 'Andreas Jakoby', 'Maximilian Mayerl', 'Preslav Nakov', 'Henry Plutz', 'Martin Potthast', 'Benno Stein', 'Minh Ngoc Ta', 'Yuxia Wang', 'Eva Zangerle']&lt;/li&gt;&lt;li&gt;Tags: ['generative AI detection', 'text watermarking', 'forensic analysis', 'plagiarism detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09147</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion</title><link>https://arxiv.org/abs/2602.08668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new vulnerability in hybrid RAG pipelines—Retrieval Pivot Risk (RPR)—where vector-retrieved seed chunks pivot via knowledge-graph entity links to leak sensitive cross-tenant data.&lt;/li&gt;&lt;li&gt;Defines measurement metrics (Leakage@k, Amplification Factor, Pivot Depth) and demonstrates seven Retrieval Pivot Attacks, finding high RPR (up to 0.95) and common leakage at Pivot Depth = 2 on synthetic and Enron corpora.&lt;/li&gt;&lt;li&gt;Proposes and evaluates a simple defense: enforce authorization at the vector-to-graph expansion boundary, which eliminates measured leakage (RPR ≈ 0) across attacks and corpora with minimal overhead.&lt;/li&gt;&lt;li&gt;Concludes the root cause is missing boundary enforcement in component composition, not inherent complexity of defenses—secure components can compose into an insecure system.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmented-generation', 'data-leakage', 'knowledge-graph', 'access-control', 'adversarial-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08668</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Can LLMs Find Bugs in Code? An Evaluation from Beginner Errors to Security Vulnerabilities in Python and C++</title><link>https://arxiv.org/abs/2508.16419</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical evaluation of ChatGPT-4, Claude 3, and LLaMA 4 on detecting bugs ranging from beginner errors to security vulnerabilities in Python and C++.&lt;/li&gt;&lt;li&gt;Benchmark dataset integrates real-world code from SEED Labs, OpenSSL (Suresoft GLaDOS), and PyBugHive, validated via local compilation and testing pipelines.&lt;/li&gt;&lt;li&gt;Introduces a multi-stage, context-aware prompting protocol and a graded rubric measuring detection accuracy, reasoning depth, and remediation quality.&lt;/li&gt;&lt;li&gt;Finds strong performance on syntactic/semantic issues but reduced effectiveness on complex security vulnerabilities and large-scale production code; ChatGPT-4 and Claude 3 outperform LLaMA 4.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshay Mhatre', 'Noujoud Nader', 'Patrick Diehl', 'Deepti Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-detection', 'code-audit', 'LLM-evaluation', 'security-bugs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16419</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference</title><link>https://arxiv.org/abs/2508.08438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a timing side-channel in global KV-cache sharing for multi-tenant LLM inference that can leak sensitive user inputs across tenants.&lt;/li&gt;&lt;li&gt;Proposes SafeKV, a system-level defense combining a three-tier asynchronous privacy detection pipeline, a radix-tree memory manager with sensitivity-aware eviction for selective isolation, and an RDR-guided runtime safeguard to bound residual leakage.&lt;/li&gt;&lt;li&gt;Evaluates SafeKV on large LLM backends, showing up to 40.58% lower time-to-first-token overhead versus full isolation and up to 2.66x higher throughput while maintaining practical privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kexin Chu', 'Zecheng Lin', 'Dawei Xiang', 'Zixu Shen', 'Jianchang Su', 'Cheng Chu', 'Yiwei Yang', 'Wenhui Zhang', 'Wenfei Wu', 'Wei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel', 'privacy', 'LLM inference', 'defense', 'system security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08438</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Among Us: A Sandbox for Measuring and Detecting Agentic Deception</title><link>https://arxiv.org/abs/2504.04072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Among Us, a multi-agent sandbox game to elicit long-term, open-ended deceptive behavior from LLM-based agents for systematic evaluation and red-teaming.&lt;/li&gt;&lt;li&gt;Evaluates 18 proprietary and open-weight LLMs, finding RL-trained models are substantially better at producing deception than detecting it.&lt;/li&gt;&lt;li&gt;Develops and tests detection methods (logistic regression probes on activations and sparse autoencoders); probes trained with 'pretend you're a dishonest model...' generalize strongly (AUROC &gt;95%).&lt;/li&gt;&lt;li&gt;Identifies SAE-derived features that detect deception well but cannot be used to reliably steer models to lie less; open-sources the sandbox, logs, and probes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Satvik Golechha', 'Adri\\`a Garriga-Alonso']&lt;/li&gt;&lt;li&gt;Tags: ['agentic-deception', 'red-teaming', 'deception-detection', 'model-probing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04072</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DiffBreak: Is Diffusion-Based Purification Robust?</title><link>https://arxiv.org/abs/2411.16598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Refutes the core robustness claim of diffusion-based purification (DBP), proving gradient-based attacks can target diffusion models to steer purified outputs toward adversarial distributions.&lt;/li&gt;&lt;li&gt;Introduces DiffBreak, a toolkit for differentiating through DBP to produce correct gradients and more reliable attacks, revealing prior robustness overestimates.&lt;/li&gt;&lt;li&gt;Identifies flawed evaluation protocols (single-random purification) and proposes a statistically grounded majority-vote (MV) aggregation across multiple purifications that yields partial robustness gains.&lt;/li&gt;&lt;li&gt;Develops systemic optimization-based perturbations (inspired by deepfake watermarking attacks) that defeat DBP even under the MV scheme, challenging DBP's viability as a defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andre Kassis', 'Urs Hengartner', 'Yaoliang Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'defense evaluation', 'diffusion models', 'gradient-based attacks', 'security toolkit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16598</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding</title><link>https://arxiv.org/abs/2602.07358</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTOPIA, a method to create unlearnable examples for tabular data by embedding dominant, hyper-correlated shortcuts into redundant low-saliency features while obfuscating semantic signals in high-saliency features.&lt;/li&gt;&lt;li&gt;Provides a theoretical Spectral Dominance condition showing certified unlearnability is achievable when the poison spectrum overwhelms the clean semantic spectrum.&lt;/li&gt;&lt;li&gt;Demonstrates UTOPIA preserves tabular validity and significantly degrades unauthorized training to near-random performance across multiple tabular datasets and model architectures, outperforming prior UE baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming He', 'Fuming Luo', 'Hongwei Li', 'Wenbo Jiang', 'Wenshu Fan', 'Zhenbo Shi', 'Xudong Jiang', 'Yi Yu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable examples', 'data poisoning', 'data protection', 'tabular data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07358</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment</title><link>https://arxiv.org/abs/2602.04909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Geometric Anchor Preference Optimization (GAPO), replacing a static reference policy with a dynamic, adversarial local perturbation (anchor) to serve as a pessimistic baseline during preference optimization.&lt;/li&gt;&lt;li&gt;Introduces the Anchor Gap (reward discrepancy between policy and anchor) and uses it to adaptively reweight pairwise preference loss, downweighting geometrically brittle/noisy instances and emphasizing robust signals.&lt;/li&gt;&lt;li&gt;Provides theoretical justification under smoothness conditions (Anchor Gap approximates worst-case local margin degradation) and empirical results showing improved robustness to noise while matching or improving standard LLM alignment and reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youngjae Cho', 'Jongsuk Kim', 'Ji-Hoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['preference alignment', 'robustness', 'defense', 'reward modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04909</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks</title><link>https://arxiv.org/abs/2601.22579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a non-intrusive graph-based bot detection framework for e-commerce that models user sessions as a graph and uses an inductive graph neural network for classification.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection performance (AUC, F1) over a session-level MLP baseline on real-world e-commerce traffic.&lt;/li&gt;&lt;li&gt;Evaluates robustness via adversarial perturbation and cold-start simulations, showing resilience to moderate graph modifications and generalization to unseen sessions/URLs.&lt;/li&gt;&lt;li&gt;Highlights deployment practicality: no client-side instrumentation, real-time inference, and support for incremental updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sichen Zhao', 'Zhiming Xue', 'Yalun Qi', 'Xianling Zeng', 'Zihan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['bot-detection', 'graph-neural-networks', 'adversarial-robustness', 'e-commerce-security', 'inductive-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22579</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper analyzes memorization in diffusion-based image generative models and shows that existing norm-based detection methods assume isotropic log-probability and fail in low-noise (anisotropic) regimes.&lt;/li&gt;&lt;li&gt;It identifies that memorized samples show strong angular alignment between the guidance vector and unconditional scores in the anisotropic regime and proposes a detection metric that combines isotropic norm and anisotropic alignment.&lt;/li&gt;&lt;li&gt;The proposed metric can be computed on pure noise via two conditional/unconditional forward passes (no denoising), yielding faster detection (~5x) and better performance than prior denoising-free methods on Stable Diffusion v1.4 and v2.&lt;/li&gt;&lt;li&gt;They also present a mitigation strategy that adapts prompts based on the metric and provide accompanying code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'diffusion models', 'privacy/data leakage', 'detection metric', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Should We Introduce Safety Interventions During Pretraining?</title><link>https://arxiv.org/abs/2601.07087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how the start time of safety interventions during pretraining (0%, 20%, 60% of tokens) affects downstream model robustness and steerability.&lt;/li&gt;&lt;li&gt;Finds no single optimal start time: short initial safe-only pretraining (20%-60%) often yields strongest robustness under standard decoding, while interventions from the start improve safety-aware inference steerability.&lt;/li&gt;&lt;li&gt;Shows earlier interventions reshape internal representations, making linear probes better at separating safe vs harmful examples, highlighting intervention timing as a key safety curriculum design choice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Sam', 'Sachin Goyal', 'Pratyush Maini', 'Alexander Robey', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining defenses', 'safety interventions', 'robustness', 'steerability', 'representation analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.07087</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Faithful Group Shapley Value</title><link>https://arxiv.org/abs/2505.19013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a security/incentive vulnerability in group-level Data Shapley: 'shell company' attacks where strategic group splitting inflates valuations.&lt;/li&gt;&lt;li&gt;Proposes the Faithful Group Shapley Value (FGSV) as a principled defense that uniquely resists such manipulations and provides theoretical guarantees.&lt;/li&gt;&lt;li&gt;Develops a provably fast and accurate approximation algorithm for FGSV and experimentally shows improved computational efficiency and approximation accuracy over prior methods while ensuring faithful group-level valuation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kiljae Lee', 'Ziqi Liu', 'Weijing Tang', 'Yuan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data valuation', 'adversarial manipulation', 'mechanism/incentive attacks', 'defense/robustness', 'Shapley value']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19013</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robust Processing and Learning: Principles, Methods, and Wireless Applications</title><link>https://arxiv.org/abs/2602.09848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Tutorial/survey of robustness principles across statistics, optimization, and machine learning, with formal foundations and trade-offs (performance vs. cost).&lt;/li&gt;&lt;li&gt;Reviews robustness techniques such as robust estimation/testing, distributionally robust optimization (DRO), regularization, and adversarial training.&lt;/li&gt;&lt;li&gt;Surveys robust signal-processing solutions for wireless sensing and communication (WSC) addressing model mismatch, data scarcity, adversarial perturbations, and distributional shift, including applications like channel estimation, localization, waveform design, and federated learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Tutorial&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shixiong Wang', 'Wei Dai', 'Li-Chun Wang', 'Geoffrey Ye Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distributionally robust optimization', 'adversarial training', 'robust signal processing', 'federated learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09848</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Hybrid Responsible AI-Stochastic Approach for SLA Compliance in Multivendor 6G Networks</title><link>https://arxiv.org/abs/2602.09841</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid Responsible AI (RAI)-stochastic learning framework that embeds fairness, robustness, and auditability into 6G multivendor network control loops to assist SLA compliance.&lt;/li&gt;&lt;li&gt;Combines RAI games with dynamic adversarial reweighting and probabilistic exploration, and introduces an RAAP audit component that records decision trajectories and produces user- and operator-level accountability reports.&lt;/li&gt;&lt;li&gt;Presents experimental results showing improved worst-group accuracy and an audit mechanism that traced 99% of simulated SLA violations to responsible AI entities on synthetic datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emanuel Figetakis', 'Ahmed Refaey Hussein']&lt;/li&gt;&lt;li&gt;Tags: ['responsible AI', 'robustness', 'auditability', 'SLA compliance', 'network security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09841</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Linear Model Extraction via Factual and Counterfactual Queries</title><link>https://arxiv.org/abs/2602.09748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes model extraction attacks on linear models using three query types: factual, counterfactual, and robust counterfactual.&lt;/li&gt;&lt;li&gt;Derives mathematical characterizations of classification regions that can be inferred from arbitrary query sets without recovering model parameters.&lt;/li&gt;&lt;li&gt;Provides bounds on the number of queries needed to fully recover model parameters under various norm-based distance functions; shows a single differentiable counterfactual can suffice, while polyhedral distances require O(d) queries and robust counterfactuals can double that.&lt;/li&gt;&lt;li&gt;Highlights that the choice of distance metric and robustness constraints for counterfactual explanations materially affects extraction vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daan Otto', 'Jannis Kurtz', 'Dick den Hertog', 'Ilker Birbil']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'counterfactual-queries', 'adversarial-privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09748</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Tracking Finite-Time Lyapunov Exponents to Robustify Neural ODEs</title><link>https://arxiv.org/abs/2602.09613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes finite-time Lyapunov exponents (FTLEs) in continuous-depth neural ODEs as a measure of exponential separation of input perturbations.&lt;/li&gt;&lt;li&gt;Establishes a direct connection between Lyapunov exponents and adversarial vulnerability of models.&lt;/li&gt;&lt;li&gt;Proposes a novel training algorithm that regularizes FTLEs by suppressing exponents far from zero early in the input dynamics to improve robustness.&lt;/li&gt;&lt;li&gt;Claims robustness gains while reducing computational cost compared to full-interval (double backpropagation) regularization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobias W\\"ohrer', 'Christian Kuehn']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'defense', 'neural ODEs', 'Lyapunov exponents', 'training regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09613</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging</title><link>https://arxiv.org/abs/2602.09284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes X-Mark, a sample-specific clean-label watermarking method for chest x-ray datasets using a conditional U-Net to generate unique perturbations in salient regions.&lt;/li&gt;&lt;li&gt;Designs a multi-component training objective including Laplacian regularization to ensure watermark efficacy, scale-invariance, diagnostic-quality preservation, and visual distinguishability.&lt;/li&gt;&lt;li&gt;Performs black-box ownership verification by detecting characteristic model behaviors and evaluates robustness against scaling transforms and adaptive attacks on CheXpert with high watermark survival rate (WSR).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Kulkarni', 'Junfeng Guo', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['dataset watermarking', 'ownership verification', 'medical imaging security', 'robustness against transformations', 'black-box verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09284</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning</title><link>https://arxiv.org/abs/2602.09182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes randomness (PRNGs) as an adversarial attack vector in ML pipelines across frameworks, software dependencies, and hardware backends.&lt;/li&gt;&lt;li&gt;Introduces RNGGuard: a tool that statically detects uses of RNGs in libraries and at runtime replaces insecure RNG calls with implementations that meet security specifications.&lt;/li&gt;&lt;li&gt;Evaluates RNGGuard and demonstrates it as a practical defense to close covert vulnerabilities stemming from insecure randomness in ML systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kotekar Annapoorna Prabhu', 'Andrew Gan', 'Zahra Ghodsi']&lt;/li&gt;&lt;li&gt;Tags: ['randomness', 'PRNG', 'attack_vector', 'defense', 'secure-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09182</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy</title><link>https://arxiv.org/abs/2602.10100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FEXT-DP: a federated learning approach using decision trees with differential privacy applied to the tree-based model.&lt;/li&gt;&lt;li&gt;Evaluates the impact of adding DP on model explainability and standard performance measures (e.g., training rounds, mean squared error), discussing trade-offs between privacy and interpretability.&lt;/li&gt;&lt;li&gt;Empirical results report faster training (fewer rounds) and improvements in some performance metrics, while also analyzing how DP affects the explainability of the learned trees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'ulio Oliveira", 'Rodrigo Ferreira', "Andr\\'e Riker", 'Glaucio H. S. Carvalho', 'Eirini Eleni Tsilopoulou']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'privacy-preserving ML', 'explainability', 'decision trees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10100</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</title><link>https://arxiv.org/abs/2602.10067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RLFR (Reinforcement Learning from Feature Rewards): an RL pipeline that uses interpretability-derived features as reward functions to reduce open-ended model hallucination.&lt;/li&gt;&lt;li&gt;Introduces a probing framework to identify candidate hallucinated claims and trains the model to intervene and correct uncertain or potentially non-factual completions.&lt;/li&gt;&lt;li&gt;Demonstrates on Gemma-3-12B-IT a 58% reduction in hallucination while preserving standard benchmark performance and enabling scalable test-time compute guided by feature rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaditya Vikram Prasad', 'Connor Watts', 'Jack Merullo', 'Dhruvil Gala', 'Owen Lewis', 'Thomas McGrath', 'Ekdeep Singh Lubana']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'hallucination mitigation', 'interpretability', 'reinforcement learning', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10067</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Vendi Novelty Scores for Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2602.10062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Vendi Novelty Score (VNS), an OOD detector derived from Vendi Scores — similarity-based diversity metrics.&lt;/li&gt;&lt;li&gt;VNS quantifies how much a test sample increases the in-distribution feature set's diversity, combining class-conditional (local) and dataset-level (global) novelty without density modeling.&lt;/li&gt;&lt;li&gt;Method is linear-time, non-parametric, and remains effective even when computed with only 1% of training data; demonstrates state-of-the-art OOD detection on image classification benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amey P. Pasarkar', 'Adji Bousso Dieng']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'novelty detection', 'robustness', 'non-parametric', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.10062</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions</title><link>https://arxiv.org/abs/2602.09987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Infusion, a method that uses scalable influence-function approximations to craft small edits to training data that induce targeted changes in model behavior (data poisoning).&lt;/li&gt;&lt;li&gt;Empirically demonstrates effectiveness on vision (CIFAR-10) by editing 0.2% of training examples and shows the poisoned corpus can transfer across architectures (ResNet ↔ CNN).&lt;/li&gt;&lt;li&gt;Provides preliminary language experiments identifying when the approach amplifies behaviors (most effective when the behavior is already learned) and highlights implications for attackers and defenders.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J Rosser', 'Robert Kirk', 'Edward Grefenstette', 'Jakob Foerster', 'Laura Ruis']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'influence functions', 'training-data attacks', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09987</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Poisoning Robustness Certification for Natural Language Generation</title><link>https://arxiv.org/abs/2602.09757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for certified robustness of autoregressive natural language generation, defining two properties: stability and validity.&lt;/li&gt;&lt;li&gt;Proposes Targeted Partition Aggregation (TPA) to compute minimum poisoning budgets required to induce targeted harmful tokens/phrases, and extends TPA with MILP for tighter multi-turn guarantees.&lt;/li&gt;&lt;li&gt;Provides empirical certification results (e.g., certifying agent tool-calling and multi-token stability) demonstrating defenses against data poisoning on language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mihnea Ghitu', 'Matthew Wicker']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'certified robustness', 'poisoning defense', 'adversarial robustness', 'natural language generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09757</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SnareNet: Flexible Repair Layers for Neural Networks with Hard Constraints</title><link>https://arxiv.org/abs/2602.09317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SnareNet, a neural architecture that appends a differentiable repair layer to steer network outputs into an input-dependent nonlinear feasible set to satisfy hard constraints.&lt;/li&gt;&lt;li&gt;Introduces adaptive relaxation: a training schedule that starts with a relaxed feasible set to stabilize learning and gradually tightens to enforce strict feasibility.&lt;/li&gt;&lt;li&gt;Demonstrates on optimization-learning and trajectory-planning benchmarks that SnareNet yields better objective quality while more reliably satisfying constraints compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ya-Chi Chu', 'Alkiviades Boukas', 'Madeleine Udell']&lt;/li&gt;&lt;li&gt;Tags: ['safety mechanisms', 'constraint satisfaction', 'repair layer', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09317</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation</title><link>https://arxiv.org/abs/2602.09305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues reward modeling is central to aligning RL-finetuned LLM reasoning and introduces Reasoning-Aligned Reinforcement Learning (RARL), a unifying framework for reward paradigms in multi-step reasoning.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of reward mechanisms, analyzes reward hacking as a pervasive failure mode, and connects reward signal design to issues like hallucination, evaluation bias, and distribution shift.&lt;/li&gt;&lt;li&gt;Critically evaluates existing benchmarks for vulnerabilities (e.g., data contamination, reward misalignment) and outlines directions for more robust, verifiable evaluation and mitigation strategies.&lt;/li&gt;&lt;li&gt;Positions reward design not as an implementation detail but as a primary lever for robustness, generalization, and trustworthiness of reasoning models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei-Chi Pan', 'Yingbin Liang', 'Sen Lin']&lt;/li&gt;&lt;li&gt;Tags: ['reward-modeling', 'alignment', 'reward-hacking', 'robustness', 'evaluation-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09305</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical Roughness-Informed Machine Unlearning</title><link>https://arxiv.org/abs/2602.09304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SRAGU, a machine unlearning algorithm that reallocates unlearning updates across layers using layer-wise 'statistical roughness' estimated via heavy-tailed spectral diagnostics.&lt;/li&gt;&lt;li&gt;Maps WeightWatcher-style heavy-tailed exponents to bounded spectral stability weights to reweight Adaptive Gradient Unlearning (AGU) sensitivities—concentrating updates in spectrally stable layers and damping updates in unstable/overfit layers.&lt;/li&gt;&lt;li&gt;Evaluates unlearning quality by comparing to a retrain-from-scratch gold model using prediction-divergence and KL-to-gold on a forget-focused query set, and additionally performs membership inference auditing as a leakage metric.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Partohaghighi', 'Roummel Marcia', 'Bruce J. West', 'YangQuan Chen']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'membership-inference', 'robustness', 'spectral-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09304</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Measuring Privacy Risks and Tradeoffs in Financial Synthetic Data Generation</title><link>https://arxiv.org/abs/2602.09288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates privacy-utility tradeoffs of multiple tabular synthetic data generators (autoencoders, GANs, diffusion, copulas) on financial datasets with severe class imbalance.&lt;/li&gt;&lt;li&gt;Introduces novel privacy-preserving implementations of GAN and autoencoder synthesizers and measures their effectiveness.&lt;/li&gt;&lt;li&gt;Assesses data quality, downstream utility, and privacy across balanced vs. imbalanced inputs, highlighting challenges unique to financial tabular data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Zuo', 'Inwon Kang', 'Stacy Patterson', 'Oshani Seneviratne']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'synthetic-data', 'privacy-utility-tradeoff', 'tabular-data', 'financial-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09288</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generalizing GNNs with Tokenized Mixture of Experts</title><link>https://arxiv.org/abs/2602.09258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes a fundamental tradeoff for frozen GNN inference between stability and reliance on shift-sensitive features, and shows instance-conditional routing can overcome this but is fragile.&lt;/li&gt;&lt;li&gt;Proposes STEM-GNN: a pretrain-then-finetune framework with a mixture-of-experts encoder, a vector-quantized token interface to stabilize encoder-to-head signals, and a Lipschitz-regularized head to limit output amplification.&lt;/li&gt;&lt;li&gt;Introduces decompositions separating coverage vs selection and base sensitivity vs fluctuation amplification to reason about routing fragility and robustness.&lt;/li&gt;&lt;li&gt;Empirically improves robustness to degree/homophily distribution shifts and to feature/edge corruptions across nine node/link/graph benchmarks while remaining competitive on clean data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoguang Guo', 'Zehong Wang', 'Jiazheng Li', 'Shawn Spitzel', 'Qi Yang', 'Kaize Ding', 'Jundong Li', 'Chuxu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'graph-neural-networks', 'mixture-of-experts', 'distribution-shift', 'regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09258</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPID: Risk of Attribute Prediction-Induced Disclosure in Synthetic Microdata</title><link>https://arxiv.org/abs/2602.09235</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAPID, a disclosure-risk metric that quantifies the risk from an adversary training a predictive model on released synthetic microdata to infer sensitive attributes of real individuals.&lt;/li&gt;&lt;li&gt;Defines tailored risk measures for continuous (relative error tolerance) and categorical (baseline-normalized confidence) sensitive attributes, producing interpretable, bounded risk scores robust to class imbalance.&lt;/li&gt;&lt;li&gt;Demonstrates threshold calibration, uncertainty quantification, and comparative evaluation across synthetic data generators via simulations and real-data experiments, yielding an attacker-realistic upper bound on attribute-inference risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthias Templ', 'Oscar Thees', 'Roman M\\"uller']&lt;/li&gt;&lt;li&gt;Tags: ['attribute-inference', 'privacy', 'synthetic-data', 'disclosure-risk', 'attack-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09235</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>What do Geometric Hallucination Detection Metrics Actually Measure?</title><link>https://arxiv.org/abs/2602.09158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes geometric/internal-state metrics of LLMs used to detect hallucination and asks which specific hallucination properties those metrics capture.&lt;/li&gt;&lt;li&gt;Creates a synthetic dataset that systematically varies correctness, confidence, relevance, coherence, and completeness to probe metric sensitivity.&lt;/li&gt;&lt;li&gt;Finds different geometric statistics detect different types of hallucinations and many methods are sensitive to domain shifts (e.g., math vs history).&lt;/li&gt;&lt;li&gt;Proposes a simple normalization technique that reduces domain-shift sensitivity and yields large AUROC improvements (≈+34 points) in multi-domain settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Yeats', 'John Buckheit', 'Sarah Scullen', 'Brendan Kennedy', 'Loc Truong', 'Davis Brown', 'Bill Kay', 'Cliff Joslyn', 'Tegan Emerson', 'Michael J. Henry', 'John Emanuello', 'Henry Kvinge']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'LLM-robustness', 'internal-state-metrics', 'domain-shift-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09158</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Temperature Scaling Attack Disrupting Model Confidence in Federated Learning</title><link>https://arxiv.org/abs/2602.06638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Temperature Scaling Attack (TSA), a training-time federated learning attack that manipulates confidence calibration via learning-rate/temperature coupling to degrade calibration while keeping accuracy nearly unchanged.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence analysis under non-IID settings showing the attack preserves standard convergence bounds yet systematically distorts predictive confidence.&lt;/li&gt;&lt;li&gt;Demonstrates empirically across benchmarks (e.g., CIFAR-100) that TSA greatly increases calibration error with &lt;2% accuracy change, remains effective against robust aggregation and post-hoc calibration, and causes substantial real-world harms (missed critical cases, false alarms).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kichang Lee', 'Jaeho Jin', 'JaeYeon Park', 'Songkuk Kim', 'JeongGil Ko']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'training-time-attack', 'model-calibration', 'poisoning-attack', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.06638</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FSTab, a Feature--Security Table enabling a black-box attack that predicts backend vulnerabilities in LLM-generated software from observable frontend features and knowledge of the source LLM.&lt;/li&gt;&lt;li&gt;Provides a model-centric evaluation metric that quantifies vulnerability persistence (how consistently a model reproduces the same vulnerabilities across programs, rephrasings, and domains).&lt;/li&gt;&lt;li&gt;Evaluates on state-of-the-art code LLMs (e.g., GPT-5.2, Claude-4.5 Opus, Gemini-3 Pro) and demonstrates high cross-domain attack success (up to 94%) and vulnerability coverage (up to 93%).&lt;/li&gt;&lt;li&gt;Highlights an underexplored attack surface in code generation and the security risks of recurring, model-specific vulnerability templates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomer Kordonsky', 'Maayan Yamin', 'Noam Benzimra', 'Amit LeVi', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability-persistence', 'black-box-attack', 'code-generation-security', 'LLM-security', 'model-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04894</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</title><link>https://arxiv.org/abs/2602.02600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how sampling strategies (autoregressive vs diffusion) influence refusal behavior and jailbreak robustness, separating sampling effects from learned representations.&lt;/li&gt;&lt;li&gt;Introduces the Step-Wise Refusal Internal Dynamics (SRI) signal to capture internal recovery dynamics and detect cases of incomplete internal recovery that precede harmful generations.&lt;/li&gt;&lt;li&gt;Proposes lightweight inference-time detectors based on SRI that generalize to unseen attacks and match/outperform existing defenses with substantially lower inference overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eliron Rahimi', 'Elad Hirshel', 'Rom Himelstein', 'Amit LeVi', 'Avi Mendelson', 'Chaim Baskin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak robustness', 'refusal dynamics', 'defense/detection', 'diffusion language models', 'inference-time safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02600</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Decoupling Generalizability and Membership Privacy Risks in Neural Networks</title><link>https://arxiv.org/abs/2602.02296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that model generalization and membership privacy risks concentrate in different regions of neural network architectures.&lt;/li&gt;&lt;li&gt;Proposes a Privacy-Preserving Training Principle (PPTP) that protects components from membership privacy risks while minimizing degradation in generalizability.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved privacy preservation with significantly better maintenance of model utility compared to alternative defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingli Fang', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-preserving-training', 'model-defense', 'generalization-privacy-tradeoff']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02296</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>EvoMU: Evolutionary Machine Unlearning</title><link>https://arxiv.org/abs/2602.02139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EvoMU, an evolutionary search method that automatically synthesizes task- and dataset-specific unlearning loss functions to remove specified training data influence while retaining utility.&lt;/li&gt;&lt;li&gt;Demonstrates that evolved losses can match or surpass prior loss-based machine unlearning methods on benchmarks (TOFU-5%, TOFU-10%, MUSE, WMDP).&lt;/li&gt;&lt;li&gt;Emphasizes cost-efficient automatic discovery using a relatively small 4B-parameter model (Qwen3-4B-Thinking) and releases code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pawel Batorski', 'Paul Swoboda']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'data privacy', 'evolutionary algorithms', 'automated loss search', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02139</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic investigation of membership inference attacks (MIA) specifically targeting Diffusion Language Models (DLMs), highlighting how DLMs' many mask configurations expand attack opportunities.&lt;/li&gt;&lt;li&gt;Introduces SAMA (Subset-Aggregated Membership Attack), which samples masked subsets at varying densities and uses sign-based, inverse-weighted aggregation to robustly detect sparse memorization signals.&lt;/li&gt;&lt;li&gt;Empirical evaluation on nine datasets shows substantial improvements (≈30% relative AUC and up to 8× gains at low false positive rates) over baselines, exposing significant privacy vulnerabilities and motivating tailored defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Kaiyuan Zhang', 'Yuntao Du', 'Edoardo Stoppa', 'Charles Fleming', 'Ashish Kundu', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'diffusion-language-models', 'model-vulnerability', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20125</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</title><link>https://arxiv.org/abs/2512.22522</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes gradient vanishing in surrogate-gradient-based attacks on Spiking Neural Networks (SNNs) and its impact on adversarial robustness evaluation.&lt;/li&gt;&lt;li&gt;Proposes Adaptive Sharpness Surrogate Gradient (ASSG) that adapts surrogate function shape during attack iterations to improve gradient accuracy and reduce vanishing.&lt;/li&gt;&lt;li&gt;Introduces Stable Adaptive Projected Gradient Descent (SA-PGD), an L_infty attack with adaptive step size to achieve faster, more stable convergence under imprecise gradients.&lt;/li&gt;&lt;li&gt;Extensive experiments show higher attack success rates across training schemes, architectures, and neuron models, indicating existing SNN robustness has been overestimated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihang Wang', 'Dongcheng Zhao', 'Ruolin Chen', 'Qian Zhang', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'adversarial robustness evaluation', 'spiking neural networks', 'surrogate gradients', 'robustness benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22522</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title><link>https://arxiv.org/abs/2512.15769</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Data-Chain Backdoor (DCB): open-source diffusion models can act as hidden carriers of backdoors, propagating trigger patterns into synthetic data used for downstream training.&lt;/li&gt;&lt;li&gt;Demonstrates that triggers memorized by diffusion generators can be inherited by downstream perception models, yielding effective backdoor attacks under clean-label settings with little utility loss.&lt;/li&gt;&lt;li&gt;Analyzes attacker options (training from scratch vs fine-tuning) and shows that tailored loss objectives and trigger processing make fine-tuning a low-cost, effective attack vector.&lt;/li&gt;&lt;li&gt;Empirically evaluates DCB across multiple trigger types, augmentation protocols, and data-scarce settings, finding consistent trigger retention and attack efficacy comparable to conventional backdoor attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junchi Lu', 'Xinke Li', 'Yuheng Liu', 'Qi Alfred Chen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'diffusion models', 'data supply chain', 'clean-label attack', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15769</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2509.23573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of vulnerabilities in LLM-assisted cyber threat intelligence (CTI) workflows, emphasizing real-world CTI properties (heterogeneous, volatile, fragmented).&lt;/li&gt;&lt;li&gt;Introduces a human-in-the-loop categorization framework to label failure modes across the CTI lifecycle, avoiding fully automated LLM-judgment pipelines.&lt;/li&gt;&lt;li&gt;Identifies three domain-specific cognitive failures: spurious correlations from superficial metadata, contradictory knowledge from conflicting sources, and constrained generalization to emerging threats.&lt;/li&gt;&lt;li&gt;Validates failure mechanisms via causal interventions and demonstrates targeted defenses that significantly reduce failure rates, offering a roadmap for resilient, domain-aware CTI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqiao Meng', 'Luoxi Tang', 'Feiyang Yu', 'Jinyuan Jia', 'Guanhua Yan', 'Ping Yang', 'Zhaohan Xi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM vulnerabilities', 'cyber threat intelligence', 'robustness/defenses', 'human-in-the-loop', 'causal evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23573</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Practical Feasibility of Gradient Inversion Attacks in Federated Learning</title><link>https://arxiv.org/abs/2508.19819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical evaluation of gradient inversion attacks in image-based federated learning across multiple datasets, tasks (classification and object detection), and modern vision architectures at realistic resolutions.&lt;/li&gt;&lt;li&gt;Finds that while gradient inversion can succeed under restrictive or legacy settings, modern performance-optimized training pipelines and architectures largely prevent high-fidelity image reconstruction.&lt;/li&gt;&lt;li&gt;Shows that many prior successful results rely on upper-bound experimental conditions (e.g., inference-mode operation, architectural simplifications) that do not reflect realistic training deployments.&lt;/li&gt;&lt;li&gt;Concludes that, under an honest-but-curious server threat model, gradient inversion is not a critical privacy risk for production-optimized federated learning systems and emphasizes careful distinction between diagnostic attacks and real-world risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viktor Valadi', 'Mattias {\\AA}kesson', 'Johan \\"Ostman', 'Fazeleh Hoseini', 'Salman Toor', 'Andreas Hellander']&lt;/li&gt;&lt;li&gt;Tags: ['gradient inversion', 'federated learning', 'privacy', 'data reconstruction', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19819</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint</title><link>https://arxiv.org/abs/2506.07022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlphaSteer, a theoretically grounded activation-steering method to induce refusal behavior in LLMs while avoiding over-refusal on benign prompts.&lt;/li&gt;&lt;li&gt;Frames steering as a learnable process with two objectives: utility preservation via null-space constraints (produce near-zero steering on benign data) and safety enhancement via learning a refusal direction using linear regression on malicious data.&lt;/li&gt;&lt;li&gt;Targets jailbreak attacks and malicious prompts, aiming to improve refusal robustness without degrading general model capabilities.&lt;/li&gt;&lt;li&gt;Empirical results show improved safety across multiple jailbreak attacks and maintained performance on utility benchmarks; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leheng Sheng', 'Changshuo Shen', 'Weixiang Zhao', 'Junfeng Fang', 'Xiaohao Liu', 'Zhenkai Liang', 'Xiang Wang', 'An Zhang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['activation-steering', 'jailbreak-defense', 'LLM-safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07022</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples</title><link>https://arxiv.org/abs/2503.21164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvWT, a method to generate physical-world adversarial examples by modeling naturally occurring 'wear and tear' on objects (traffic signboards) using a GAN-based image-to-image translation network.&lt;/li&gt;&lt;li&gt;Encodes damage characteristics into a latent 'damage style code' and adversarially perturbs that style code to produce realistic-looking damaged signs that mislead DNN classifiers.&lt;/li&gt;&lt;li&gt;Evaluates attacks on two traffic-sign datasets showing high attack success rates and robustness in both digital and physical settings, and shows that augmenting training with AdvWT improves model generalization to real damaged signs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samra Irshad', 'Seungkyu Lee', 'Nassir Navab', 'Hong Joo Lee', 'Seong Tae Kim']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'physical-adversarial-attacks', 'GANs', 'robustness', 'traffic-sign-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.21164</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>TruthPrInt: Mitigating Large Vision-Language Models Object Hallucination Via Latent Truthful-Guided Pre-Intervention</title><link>https://arxiv.org/abs/2503.10602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes LVLM internal hidden states and finds they serve as high-specificity, per-token indicators of object hallucination and that hallucination patterns occupy shared latent subspaces across models.&lt;/li&gt;&lt;li&gt;Proposes TruthPrInt: learn a 'truthful direction' in decoding latent space and apply inference-time interventions to suppress hallucinations during generation.&lt;/li&gt;&lt;li&gt;Extends the method to align hallucination latent subspaces for improved cross-model and cross-dataset transferability.&lt;/li&gt;&lt;li&gt;Evaluated on multiple LVLMs and object-hallucination benchmarks, reporting significant improvements over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinhao Duan', 'Fei Kong', 'Hao Cheng', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-mitigation', 'latent-space-intervention', 'vision-language-models', 'inference-time-defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10602</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</title><link>https://arxiv.org/abs/2502.18851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens to avoid breaking code semantics.&lt;/li&gt;&lt;li&gt;Shows that prior high-entropy token watermarking can corrupt logic because syntax-critical tokens often have high entropy.&lt;/li&gt;&lt;li&gt;Proposes STEM, a metric balancing correctness, detectability, and imperceptibility to evaluate watermarking for code across Python, C++, and Java.&lt;/li&gt;&lt;li&gt;Reports that STONE preserves functional correctness, achieves strong detectability, and incurs minimal computational overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungin Kim', 'Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model-output-detection', 'code-generation', 'defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18851</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>SafeDialBench: A Fine-Grained Safety Evaluation Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks</title><link>https://arxiv.org/abs/2502.11090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeDialBench, a fine-grained benchmark of &gt;4000 multi-turn dialogues in Chinese and English to evaluate LLM safety under jailbreak attacks.&lt;/li&gt;&lt;li&gt;Designs a two-tier hierarchical safety taxonomy covering 6 safety dimensions and 22 dialogue scenarios, and implements 7 jailbreak attack strategies (e.g., reference attack, purpose reverse).&lt;/li&gt;&lt;li&gt;Introduces an assessment framework measuring LLMs' detection of unsafe content, handling of unsafe information, and consistency under jailbreak attempts.&lt;/li&gt;&lt;li&gt;Evaluates 17 LLMs, identifying relative safety strengths and vulnerabilities across models (e.g., Yi-34B-Chat and GLM4-9B-Chat perform well; Llama3.1-8B-Instruct and o3-mini show weaknesses).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongye Cao', 'Sijia Jing', 'Yanming Wang', 'Ziyue Peng', 'Zhixin Bai', 'Zhe Cao', 'Meng Fang', 'Fan Feng', 'Boyan Wang', 'Jiaheng Liu', 'Tianpei Yang', 'Jing Huo', 'Yang Gao', 'Fanyu Meng', 'Xi Yang', 'Chao Deng', 'Junlan Feng']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety-benchmark', 'adversarial-attacks', 'red-teaming', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11090</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling</title><link>https://arxiv.org/abs/2601.22636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SABER, a scaling-aware Best-of-N estimator that models sample-level jailbreak success probabilities with a Beta distribution to extrapolate large-N attack success from small-sample measurements.&lt;/li&gt;&lt;li&gt;Derives an analytic scaling law enabling prediction of ASR@N (attack success rate under Best-of-N sampling) from limited budgets (e.g., n=100) to much larger N (e.g., 1000).&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical accuracy: with n=100 samples SABER predicts ASR@1000 with mean absolute error 1.66 vs. 12.04 for the baseline, revealing nonlinear risk amplification under parallel adversarial probing.&lt;/li&gt;&lt;li&gt;Provides a low-cost, scalable methodology for realistic LLM safety assessment and plans to release code and evaluation scripts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Chenliang Xu', 'Christopher White', 'Jianfeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial risk', 'jailbreaking', 'attack-scaling', 'safety-evaluation', 'statistical-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22636</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</title><link>https://arxiv.org/abs/2601.21083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenSec, a dual-control RL environment to evaluate incident response (IR) agents under adversarial prompt-injection/evidence scenarios.&lt;/li&gt;&lt;li&gt;Defines execution-based metrics (time-to-first-containment (TTFC), evidence-gated action rate (EGAR), blast radius, per-tier injection violation rates) to measure calibration and false positives.&lt;/li&gt;&lt;li&gt;Evaluates several frontier LLM-based agents and finds systematic over-triggering (high false positive containment actions) despite correct threat identification when they act.&lt;/li&gt;&lt;li&gt;Provides code and benchmark episodes for reproducible security evaluation of IR agent calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jarrod Barnes']&lt;/li&gt;&lt;li&gt;Tags: ['incident-response', 'prompt-injection', 'adversarial-evaluation', 'RL-benchmark', 'agent-calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21083</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize to others.&lt;/li&gt;&lt;li&gt;Observes that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical validation showing SpikeScore separates hallucinated vs. non-hallucinated responses across multiple LLMs and benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates SpikeScore outperforms representative baselines and advanced generalization-oriented methods in cross-domain hallucination detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'cross-domain-generalization', 'LLM-safety', 'uncertainty-estimation', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Safety Alignment of LMs via Non-cooperative Games</title><link>https://arxiv.org/abs/2512.20806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning.&lt;/li&gt;&lt;li&gt;Uses preference-based pairwise comparison rewards (rather than point-wise scores) to supervise training and mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Produces a Defender LM that improves both helpfulness and robustness to adversarial prompts, and an Attacker LM that serves as a strong, general-purpose red-teaming agent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anselm Paulus', 'Ilia Kulikov', 'Brandon Amos', "R\\'emi Munos", 'Ivan Evtimov', 'Kamalika Chaudhuri', 'Arman Zharmagambetov']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'red teaming', 'reinforcement learning', 'game theory', 'reward modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20806</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>BEAT: Visual Backdoor Attacks on VLM-based Embodied Agents via Contrastive Trigger Learning</title><link>https://arxiv.org/abs/2510.27623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BEAT, a framework to inject visual backdoors into vision-language model (VLM)-based embodied agents using physical object triggers that cause attacker-specified multi-step policies when visible.&lt;/li&gt;&lt;li&gt;Addresses trigger variability across viewpoints and lighting via diverse training data spanning scenes/tasks/placements and a two-stage training: supervised fine-tuning (SFT) followed by novel Contrastive Trigger Learning (CTL) that sharpens trigger discrimination as a preference-learning task.&lt;/li&gt;&lt;li&gt;Demonstrates high attack effectiveness (up to ~80% success) while preserving benign task performance and shows CTL significantly improves activation accuracy (up to +39%) over naive SFT, including under limited backdoor data and out-of-distribution placements.&lt;/li&gt;&lt;li&gt;Highlights a new security vulnerability for VLM-driven embodied agents and the need for robust defenses prior to deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiusi Zhan', 'Hyeonjeong Ha', 'Rui Yang', 'Sirui Xu', 'Hanyang Chen', 'Liang-Yan Gui', 'Yu-Xiong Wang', 'Huan Zhang', 'Heng Ji', 'Daniel Kang']&lt;/li&gt;&lt;li&gt;Tags: ['visual backdoor', 'embodied agents', 'vision-language models', 'contrastive learning', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.27623</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities</title><link>https://arxiv.org/abs/2510.10238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a perturbation-based causal method to identify ultra-sparse sets of critical neurons in LLMs whose disruption causes massive performance collapse.&lt;/li&gt;&lt;li&gt;Finds critical neurons concentrate in outer layers—especially MLP down_proj components—and that performance degradation shows sharp phase transitions when these neurons are perturbed.&lt;/li&gt;&lt;li&gt;Demonstrates results across architectures and scales (e.g., a 72B model collapsing when a tiny neuron subset is altered) and discusses implications for robustness, interpretability, and deployment security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Qin', 'Qingchen Yu', 'Kunlin Lyu', 'Zhaoxin Fan', 'Yifan Sun']&lt;/li&gt;&lt;li&gt;Tags: ['model robustness', 'neuron-level vulnerabilities', 'adversarial attacks', 'interpretability', 'safety/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10238</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm</title><link>https://arxiv.org/abs/2507.08249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing that granting AI agents access to cryptocurrencies and smart contracts introduces novel vectors for AI-caused harm.&lt;/li&gt;&lt;li&gt;Analyzes unique properties of cryptocurrencies and smart contracts that enable new attack surfaces and failure modes.&lt;/li&gt;&lt;li&gt;Presents a taxonomy of these new vectors of AI harm and discusses implications for safety and governance.&lt;/li&gt;&lt;li&gt;Calls for targeted technical research to prevent and mitigate these vulnerabilities before widely endowing agents with crypto capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bill Marino', 'Ari Juels']&lt;/li&gt;&lt;li&gt;Tags: ['cryptocurrency', 'smart-contracts', 'AI-agent-security', 'attack-taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08249</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Capability-Based Scaling Trends for LLM-Based Red-Teaming</title><link>https://arxiv.org/abs/2505.20162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames red-teaming as a capability-gap problem between attacker and target and evaluates LLM-based jailbreak attacks that mimic human red-teamers across &gt;600 attacker–target pairs.&lt;/li&gt;&lt;li&gt;Identifies three empirical trends: (i) more capable models make better attackers, (ii) attack success falls sharply once the target's capability exceeds the attacker's, and (iii) attack success correlates with performance on social-science splits of MMLU-Pro.&lt;/li&gt;&lt;li&gt;Derives a 'jailbreaking scaling curve' that predicts attack success from attacker–target capability gap and discusses security implications (human red-teamers becoming ineffective, risks from capable open-source models, need for providers to measure/control persuasive/manipulative abilities).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Panfilov', 'Paul Kassianik', 'Maksym Andriushchenko', 'Jonas Geiping']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'adversarial-attacks', 'security-benchmarking', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20162</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models</title><link>https://arxiv.org/abs/2502.13313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates privacy-utility-efficiency trade-offs when fine-tuning LLMs, focusing on memorization of sensitive vs non-sensitive tokens.&lt;/li&gt;&lt;li&gt;Finds that parameter-efficient fine-tuning methods (e.g., LoRA) can mitigate memorization/privacy risks similarly to differentially private (DP) training, challenging the presumed trade-off between privacy and efficiency.&lt;/li&gt;&lt;li&gt;Introduces careful measures to distinguish sensitive memorization and performs extensive empirical evaluation across multiple open-source LLM families (Pythia, Gemma, Llama, Qwen) and domain datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumi Das', 'Camila Kolling', 'Mohammad Aflah Khan', 'Mahsa Amani', 'Bishwamittra Ghosh', 'Qinyuan Wu', 'Till Speicher', 'Krishna P. Gummadi']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential_privacy', 'parameter-efficient_fine-tuning', 'model_memorization', 'privacy-preserving_ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13313</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection</title><link>https://arxiv.org/abs/2602.09015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIC-Trap4Phish, a unified multi-format dataset of malicious and benign samples across Word, Excel, PDF, HTML, and QR code images targeting phishing/quishing attachment detection.&lt;/li&gt;&lt;li&gt;Proposes execution-free static feature extraction pipelines (structural, lexical, metadata) for Word/Excel/PDF/HTML, performing feature selection via SHAP and importance metrics and training lightweight ML models (Random Forest, XGBoost, Decision Tree).&lt;/li&gt;&lt;li&gt;For QR-code based quishing, implements complementary approaches: CNN-based image detection and lexical analysis of decoded URLs using lightweight language models.&lt;/li&gt;&lt;li&gt;Reports high detection accuracy across formats, providing a resource and evaluation baseline for attachment/quishing detection defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatemeh Nejati', 'Mahdi Rabbani', 'Morteza Eskandarian', 'Mansur Mirani', 'Gunjan Piya', 'Igor Opushnyev', 'Ali A. Ghorbani', 'Sajjad Dadkhah']&lt;/li&gt;&lt;li&gt;Tags: ['phishing-detection', 'dataset', 'malicious-attachments', 'quishing', 'static-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09015</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</title><link>https://arxiv.org/abs/2602.09012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Next-Gen CAPTCHAs: a scalable defense framework and benchmark to distinguish humans from advanced GUI-enabled agents.&lt;/li&gt;&lt;li&gt;Builds a robust data generation pipeline capable of producing effectively unbounded, diverse CAPTCHA instances (including backend-supported dynamic tasks).&lt;/li&gt;&lt;li&gt;Exploits a claimed persistent 'Cognitive Gap' in interactive perception, memory, decision-making, and action to create tasks requiring adaptive intuition rather than rote planning.&lt;/li&gt;&lt;li&gt;Targets modern multimodal/agentic models (e.g., Gemini3-Pro, GPT-5.2) and re-establishes an evaluative baseline for agent resistance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liu', 'Yaxin Luo', 'Jiacheng Cui', 'Xinyi Shang', 'Xiaohan Zhao', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['CAPTCHA', 'Adversarial Defense', 'Human-AI Interaction', 'Benchmarking', 'Agent Security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.09012</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</title><link>https://arxiv.org/abs/2602.08934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces StealthRL, a reinforcement-learning framework that trains paraphrase policies to evade AI-text detectors while preserving semantics, using Group Relative Policy Optimization (GRPO) and LoRA adapters.&lt;/li&gt;&lt;li&gt;Evaluates attacks against a multi-detector ensemble (RoBERTa, FastDetectGPT, Binoculars) at a security-relevant 1% FPR operating point, reporting drastic drops in detection (near-zero TPR@1%FPR, AUROC reduced from 0.74 to 0.27) and a 99.9% attack success rate.&lt;/li&gt;&lt;li&gt;Demonstrates transferability of attacks to a held-out detector family, analyzes detector score distributions and LLM-based quality assessments, and provides a public evaluation pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suraj Ranganath', 'Atharv Ramesh']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text-detector evasion', 'reinforcement learning', 'red teaming', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08934</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs</title><link>https://arxiv.org/abs/2602.08621</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'unsafe routes' in Mixture-of-Experts (MoE) LLMs where specific router activations convert safe outputs into harmful ones.&lt;/li&gt;&lt;li&gt;Introduces Router Safety importance score (RoSais) to quantify router-level safety criticality and shows that manipulating high-RoSais routers dramatically increases jailbreak success.&lt;/li&gt;&lt;li&gt;Proposes F-SOUR (Fine-grained token-layer-wise Stochastic Optimization) to discover unsafe routing configurations, achieving high attack success rates on JailbreakBench and AdvBench.&lt;/li&gt;&lt;li&gt;Outlines potential defenses such as safety-aware route disabling and router training to mitigate MoE-specific vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukun Jiang', 'Hai Huang', 'Mingjie Li', 'Yage Zhang', 'Michael Backes', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'model vulnerability', 'Mixture-of-Experts', 'jailbreaking', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08621</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</title><link>https://arxiv.org/abs/2602.08563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and demonstrates 'implicit memory' in LLMs: models can persist state across independent interactions by encoding information in their outputs and later recovering it when those outputs reappear as inputs.&lt;/li&gt;&lt;li&gt;Introduces temporal backdoors called 'time bombs' that trigger only after a sequence of interactions accumulates hidden conditions via implicit memory; shows such behavior can be induced via prompting or fine-tuning.&lt;/li&gt;&lt;li&gt;Analyzes security implications—covert inter-agent communication, benchmark contamination, targeted manipulation, training-data poisoning—and discusses detection challenges and directions for stress-testing and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed Salem', 'Andrew Paverd', 'Sahar Abdelnabi']&lt;/li&gt;&lt;li&gt;Tags: ['temporal-backdoors', 'backdoor-attacks', 'covert-channels', 'data-poisoning', 'attack-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08563</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LLMs + Security = Trouble</title><link>https://arxiv.org/abs/2602.08422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that using probabilistic AI-based checkers/attackers to secure probabilistically generated code is insufficient for long-tail/zero-day vulnerabilities.&lt;/li&gt;&lt;li&gt;Claims neurosymbolic approaches (LLMs + formal methods) struggle with common 'vibe coding' workflows because human-in-the-loop verification undermines secure-by-construction guarantees.&lt;/li&gt;&lt;li&gt;Proposes enforcing security constraints during code generation (e.g., constrained decoding) rather than relying on post-hoc detection and repair.&lt;/li&gt;&lt;li&gt;Highlights diffusion-style code models as a promising avenue for modular, hierarchical security enforcement to produce secure-by-construction code while maintaining low latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Benjamin Livshits']&lt;/li&gt;&lt;li&gt;Tags: ['security-defenses', 'secure-code-generation', 'constrained-decoding', 'neurosymbolic-verification', 'diffusion-code-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08422</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning with Backtracking Feedback</title><link>https://arxiv.org/abs/2602.08377</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reinforcement Learning with Backtracking Feedback (RLBF): an RL-based defense that trains LLMs to emit a 'backtrack by x tokens' signal and autoregressively recover from emergent safety violations.&lt;/li&gt;&lt;li&gt;Introduces BSAFE+, an improved supervised fine-tuning data generation strategy that injects violations into coherent safe text to better bootstrap the backtracking capability.&lt;/li&gt;&lt;li&gt;Claims empirical reductions in attack success rates (e.g., middle filling, Greedy Coordinate Gradient, decoding-parameter attacks) across benchmarks and model scales while preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bilgehan Sel', 'Vaishakh Keshava', 'Phillip Wallis', 'Lukas Rutishauser', 'Ming Jin', 'Dingcheng Li']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'adversarial robustness', 'reinforcement learning', 'jailbreak mitigation', 'safety fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08377</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems</title><link>https://arxiv.org/abs/2602.08290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a trust-based incentive mechanism for semi-decentralized federated learning to evaluate and reward contribution quality.&lt;/li&gt;&lt;li&gt;Computes dynamic trust scores using factors such as data quality, model accuracy, consistency, and contribution frequency to encourage honest participation and penalize unreliable or malicious nodes.&lt;/li&gt;&lt;li&gt;Explores blockchain and smart contract integration to automate transparent trust evaluation and incentive distribution.&lt;/li&gt;&lt;li&gt;Presents a theoretical framework aimed at improving robustness and fairness in FL but appears focused on mechanism design rather than empirical adversarial testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ajay Kumar Shrestha']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'incentive-mechanisms', 'trust-management', 'blockchain', 'security-defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08290</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generating Adversarial Events: A Motion-Aware Point Cloud Framework</title><link>https://arxiv.org/abs/2602.08230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MA-ADV, a motion-aware adversarial framework that generates adversarial events for event-camera systems by representing events as point clouds to enable gradient-based attacks.&lt;/li&gt;&lt;li&gt;Uses a diffusion-based smoothing to handle high-frequency event noise and leverages spatial-temporal relationships; optimization combines sample-wise Adam, iterative refinement, and binary search to find minimal-cost perturbations.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (100% attack success rate, minimal perturbation cost) and claims enhanced robustness against defenses, highlighting security risks for event-based perception.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongwei Ren', 'Youxin Jiang', 'Qifei Gu', 'Xiangqian Wu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'event cameras', 'point cloud attacks', 'diffusion-based attack', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08230</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Robustness of Vision Language Models Against Split-Image Harmful Input Attacks</title><link>https://arxiv.org/abs/2602.08136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new vulnerability where safety alignment on holistic images fails when harmful semantics are distributed across multiple image fragments (split-image inputs).&lt;/li&gt;&lt;li&gt;Introduces SIVA (split-image visual jailbreak attacks), a progressive attack pipeline from naive splitting to adaptive white-box attacks and a black-box transfer strategy.&lt;/li&gt;&lt;li&gt;Proposes adversarial knowledge distillation (Adv-KD) to substantially improve cross-model transferability and reports up to 60% higher transfer success over baselines on three modern VLMs and three jailbreak datasets.&lt;/li&gt;&lt;li&gt;Offers efficient mitigation strategies to address the split-image safety alignment gap in VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Rafi Ur Rashid', 'MD Sadik Hossain Shanto', 'Vishnu Asutosh Dasu', 'Shagufta Mehnaz']&lt;/li&gt;&lt;li&gt;Tags: ['visual jailbreak', 'adversarial examples', 'transferability', 'safety alignment', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08136</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology</title><link>https://arxiv.org/abs/2602.08082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a training-free guardrail that uses spectral analysis of attention topology to detect tool-use hallucinations in deployed agents.&lt;/li&gt;&lt;li&gt;Reports high detection performance (e.g., 97.7% recall with multi-feature detection; single-layer features like Llama L26 Smoothness achieve ~98.2% recall) without labeled training data.&lt;/li&gt;&lt;li&gt;Performs controlled cross-model evaluation revealing a 'Loud Liar' phenomenon (spectrally catastrophic failures easier to detect) and demonstrates spectral features as a principled, efficient safety framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'hallucination detection', 'attention/spectral analysis', 'agent safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08082</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models</title><link>https://arxiv.org/abs/2602.08059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DICE, a training-free framework to erase artist style from diffusion-model outputs on-the-fly while preserving user-intended content.&lt;/li&gt;&lt;li&gt;Constructs contrastive triplets and formulates style/content disentanglement as a generalized eigenvalue problem to identify a style subspace in latent representations.&lt;/li&gt;&lt;li&gt;Applies Adaptive Attention Decoupling to selectively suppress style features in QKV vectors, claiming strong style removal with minimal overhead (~3 seconds).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Zhang', 'Ru Zhang', 'Jianyi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'style erasure', 'diffusion models', 'copyright protection', 'representation disentanglement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08059</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning</title><link>https://arxiv.org/abs/2602.08043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes V-ABFT, a variance-based adaptive threshold method to detect silent data corruptions (SDCs) in matrix multiplication for mixed-precision deep learning.&lt;/li&gt;&lt;li&gt;Achieves much tighter verification thresholds (7–20× for FP32/FP64, 48–158× for BF16) vs prior probabilistic A-ABFT, with zero false positives across multiple precisions.&lt;/li&gt;&lt;li&gt;Shows O(n) complexity using simple statistics, demonstrates ~6–48× improvement over A-ABFT, and validates on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT).&lt;/li&gt;&lt;li&gt;Integrates into fused-kernel GEMM on NPUs/GPUs and enables much finer detection granularity when verifying before output quantization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiheng Gao', 'Qin Hua', 'Zizhong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['fault-tolerance', 'ABFT', 'silent-data-corruption', 'numerical-robustness', 'mixed-precision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08043</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment</title><link>https://arxiv.org/abs/2602.08023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CyberExplorer: an open-environment benchmark using a VM with 40 vulnerable web services (derived from real-world CTFs) where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides a reactive multi-agent framework that supports dynamic exploration and coordination, enabling agents to revise hypotheses and act without predefined plans.&lt;/li&gt;&lt;li&gt;Enables fine-grained evaluation beyond binary flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals to better emulate real-world multi-target offensive operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nanda Rani', 'Kimberly Milner', 'Minghao Shao', 'Meet Udeshi', 'Haoran Xi', 'Venkata Sai Charan Putrevu', 'Saksham Aggarwal', 'Sandeep K. Shukla', 'Prashanth Krishnamurthy', 'Farshad Khorrami', 'Muhammad Shafique', 'Ramesh Karri']&lt;/li&gt;&lt;li&gt;Tags: ['offensive security', 'benchmarking', 'LLM red teaming', 'autonomous agents', 'vulnerability discovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08023</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning</title><link>https://arxiv.org/abs/2602.08014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICBAC, a permissioned-blockchain (Hyperledger Fabric) based access-control framework that integrates smart contracts with per-channel AI agents to enforce dynamic, behavior-based revocation.&lt;/li&gt;&lt;li&gt;Uses federated learning for collaborative anomaly-detection across competing organizations, preserving raw-data privacy while improving models under IID and non-IID settings.&lt;/li&gt;&lt;li&gt;Introduces a hedonic coalition-formation client selection mechanism (game-theoretic) to form stable, strategy-proof FL coalitions in heterogeneous supply-chain environments; validated on a Fabric testbed with real-world data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sadegh Sohani', 'Salar Ghazi', 'Farnaz Kamranfar', 'Sahar Pilehvar Moakhar', 'Mohammad Allahbakhsh', 'Haleh Amintoosi', 'Kaiwen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'federated-learning', 'blockchain', 'anomaly-detection', 'insider-threat']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08014</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms</title><link>https://arxiv.org/abs/2602.07963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CompositeHarm, a translation-based benchmark combining AttaQ (structured adversarial attacks) and MMSafetyBench (contextual harms) across six languages (English, Hindi, Assamese, Marathi, Kannada, Gujarati).&lt;/li&gt;&lt;li&gt;Empirical finding: attack success rates increase sharply in Indic languages—particularly under adversarial syntax—while contextual harms transfer more moderately.&lt;/li&gt;&lt;li&gt;Proposes lightweight, energy-efficient inference strategies for scalable multilingual safety testing and argues that translation-based benchmarks are necessary but not sufficient for robust, language-adaptive safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vaibhav Shukla', 'Hardik Sharma', 'Adith N Reganti', 'Soham Wasmatkar', 'Bagesh Kumar', 'Vrijendra Singh']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual-safety', 'adversarial-attacks', 'benchmarking', 'red-teaming', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07963</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation</title><link>https://arxiv.org/abs/2602.07954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bielik Guard: two compact Polish-language safety classifiers (0.1B MMLW-RoBERTa-based and 0.5B PKOBP/polish-roberta-8k-based) trained on a community-annotated dataset of 6,885 Polish texts.&lt;/li&gt;&lt;li&gt;Classifies content into five safety categories (Hate/Aggression, Vulgarities, Sexual Content, Crime, Self-Harm) with strong performance (0.5B: F1 micro 0.791, macro 0.785).&lt;/li&gt;&lt;li&gt;Highlights efficiency and low false positives of the 0.1B model on real user prompts (precision 77.65%, FPR 0.63%), outperforming a comparable baseline; models are publicly released and designed to provide appropriate responses rather than simple blocking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Krzysztof Wr\\'obel", 'Jan Maria Kowalski', 'Jerzy Surma', 'Igor Ciuciura', "Maciej Szyma\\'nski"]&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-classification', 'defense', 'Polish-NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07954</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model</title><link>https://arxiv.org/abs/2602.07878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new system-level latency Denial-of-Service attack (Fill and Squeeze) that targets LLM serving infrastructure/schedulers rather than the model itself.&lt;/li&gt;&lt;li&gt;Fill exhausts the global KV cache to induce head-of-line blocking; Squeeze forces repetitive preemption via manipulated output lengths and side-channel memory probing, enabling black-box orchestration.&lt;/li&gt;&lt;li&gt;Empirical results show large degradations (20–280x slowdown on Time to First Token, 1.5–4x on Time Per Output Token) while costing 30–40% less than prior algorithmic latency attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Wang', 'Huawei Fan', 'Yuanchao Shu', 'Peng Cheng', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['latency-DoS', 'serving-infrastructure', 'resource-exhaustion', 'black-box-attack', 'side-channel']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07878</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study</title><link>https://arxiv.org/abs/2602.07814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale zero-shot benchmark of 23 pretrained AI-generated image detectors (16 methods) across 12 datasets totalling ~2.6M images and 291 generators.&lt;/li&gt;&lt;li&gt;Key findings: no universal best detector (rank instability), large accuracy spread (37.5%–75.0%), and training-data alignment causes 20–60% variance within detector families.&lt;/li&gt;&lt;li&gt;Modern commercial image generators (e.g., Flux Dev, Firefly v4, Midjourney v7) largely evade detectors (18–30% average accuracy).&lt;/li&gt;&lt;li&gt;Identifies three systematic cross-dataset failure patterns and presents deployment guidance; statistical tests show significant performance differences between detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simiao Ren (Neo)', 'Yuchen Zhou (Neo)', 'Xingyu Shen (Neo)', 'Kidus Zewde (Neo)', 'Tommy Duong (Neo)', 'George Huang (Neo)', 'Hatsanai (Neo)', 'Tiangratanakul (Dennis)', 'Tsang (Dennis)', 'Ng (Dennis)', 'En Wei', 'Jiayu Xue']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'benchmarking', 'robustness', 'image forensics', 'zero-shot evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07814</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents</title><link>https://arxiv.org/abs/2602.07652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;AgentFence presents an architecture-centric security evaluation and taxonomy (14 trust-boundary attack classes) for deep research agents, spanning planning, memory, retrieval, tool use, and delegation.&lt;/li&gt;&lt;li&gt;It defines trace-auditable conversation breaks to detect unsafe trajectories (unauthorized/unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations).&lt;/li&gt;&lt;li&gt;Empirical benchmark: evaluates eight agent archetypes with a fixed base model under persistent multi-turn interaction, reporting mean security break rates (MSBR) from 0.29 ± 0.04 to 0.51 ± 0.07 and identifying high-risk classes (Denial-of-Wallet, Authorization Confusion, Retrieval Poisoning, Planning Manipulation).&lt;/li&gt;&lt;li&gt;Findings emphasize operational boundary violations (SIV, WPA, UTI/UTA, ATD) and correlations between authorization confusion and tool/objective hijacking, reframing agent security around staying within goal and authority envelopes over time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Puppala', 'Ismail Hossain', 'Md Jahangir Alam', 'Yoonpyo Lee', 'Jay Yoo', 'Tanzim Ahad', 'Syed Bahauddin Alam', 'Sajedul Talukder']&lt;/li&gt;&lt;li&gt;Tags: ['agent-security', 'attack-taxonomy', 'red-teaming', 'security-benchmark', 'tool-use-vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07652</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots</title><link>https://arxiv.org/abs/2602.07517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MemPot, a defense framework that injects optimized honeypot (trap) documents into LLM agent memory to detect/expose memory extraction attacks.&lt;/li&gt;&lt;li&gt;Uses a two-stage optimization to maximize attacker retrieval probability while remaining inconspicuous to benign users; models detection via Wald's Sequential Probability Ratio Test (SPRT) with theoretical guarantees.&lt;/li&gt;&lt;li&gt;Empirical results show large gains over baselines (≈50% AUROC improvement, ≈80% TPR increase under low FPR) while incurring no extra online latency and preserving agent utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Wang', 'Shengfang Zhai', 'Guanghao Jin', 'Yinpeng Dong', 'Linyi Yang', 'Jiaheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['memory extraction', 'honeypot defense', 'attack detection', 'SPRT', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07517</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model</title><link>https://arxiv.org/abs/2602.07422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecCoderX, an online reinforcement learning framework to align code LLMs for secure, functionality-preserving code generation.&lt;/li&gt;&lt;li&gt;Repurposes vulnerability detection resources to (i) synthesize diverse vulnerability-inducing coding tasks for RL rollouts and (ii) train a reasoning-based vulnerability reward model for scalable security supervision.&lt;/li&gt;&lt;li&gt;Unifies these components in an online RL loop to improve secure code generation while retaining functionality; reports ~10% improvement in Effective Safety Rate (ESR) over unaligned models and better trade-offs versus prior methods.&lt;/li&gt;&lt;li&gt;Releases code, dataset, and model checkpoints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Wu', 'Mingzhe Du', 'Yue Liu', 'Chengran Yang', 'Terry Yue Zhuo', 'Jiaheng Zhang', 'See-Kiong Ng']&lt;/li&gt;&lt;li&gt;Tags: ['secure code generation', 'reinforcement learning', 'vulnerability detection', 'reward modeling', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07422</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management</title><link>https://arxiv.org/abs/2602.07398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentSys, a hierarchical memory-management framework that isolates worker agents in separate contexts to prevent indirect prompt injection from contaminating a main agent's memory.&lt;/li&gt;&lt;li&gt;Only schema-validated, deterministic JSON return values cross agent boundaries; external data and verbose tool traces are prevented from persisting in the main context.&lt;/li&gt;&lt;li&gt;Empirical results show substantial reduction in attack success (e.g., isolation alone to 2.19%, full system to 0.78%–4.25% on benchmarks) while maintaining or slightly improving benign utility and robustness to adaptive attackers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoyao Wen', 'Hao Li', 'Chaowei Xiao', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'memory isolation', 'LLM agent defenses', 'sandboxing', 'secure tool use']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07398</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice</title><link>https://arxiv.org/abs/2602.07319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a risk-sensitive evaluation framework for hallucinations in patient-facing medical QA that scores presence of risk-bearing language (treatment directives, contraindications, urgency cues, high-risk medication mentions).&lt;/li&gt;&lt;li&gt;Combines risk scoring with a grounding/relevance measure to identify high-risk, low-grounding failures—i.e., plausible but unsupported actionable advice.&lt;/li&gt;&lt;li&gt;Evaluates three instruction-tuned LLMs on controlled safety stress-test prompts and shows standard factual metrics miss distinctions in models' risk profiles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Savan Doshi']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_evaluation', 'medical_llms', 'safety_testing', 'risk-sensitive_metrics', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07319</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>KRONE: Hierarchical and Modular Log Anomaly Detection</title><link>https://arxiv.org/abs/2602.07303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KRONE, a hierarchical and modular framework that automatically derives execution hierarchies from flat logs to improve anomaly detection.&lt;/li&gt;&lt;li&gt;Decomposes logs into multi-level 'KRONE Seq' units and uses a hybrid detection pipeline: a fast Local-Context filter plus a Nested-Aware detector that can leverage LLMs for cross-level dependencies and explanations.&lt;/li&gt;&lt;li&gt;Implements optimizations (cached result reuse, early-exit) to reduce expensive LLM usage and improve resource/data efficiency.&lt;/li&gt;&lt;li&gt;Evaluated on three public benchmarks and one industrial dataset, showing &gt;10 percentage point F1 improvements and substantially reduced LLM invocation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei Ma', 'Jinyang Liu', 'Tieying Zhang', 'Peter M. VanNostrand', 'Dennis M. Hofmann', 'Lei Cao', 'Elke A. Rundensteiner', 'Jianjun Chen']&lt;/li&gt;&lt;li&gt;Tags: ['log anomaly detection', 'hierarchical modeling', 'security monitoring', 'LLM-assisted detection', 'modular detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07303</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models</title><link>https://arxiv.org/abs/2602.07251</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvSR, a method to embed adversarial behavior directly into super-resolution (SR) model weights by jointly optimizing for reconstruction quality and targeted downstream misclassification.&lt;/li&gt;&lt;li&gt;Operates at model-level (no input perturbation or triggers at inference), producing SR models that look benign by image-quality metrics yet cause high attack success on downstream classifiers.&lt;/li&gt;&lt;li&gt;Evaluates on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier, showing high attack success rates with minimal perceptual degradation, highlighting a new supply-chain/model-level threat for imaging pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haley Duba-Sullivan', 'Steven R. Young', 'Emma J. Reid']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'model-poisoning', 'backdoor-like-attack', 'supply-chain-security', 'super-resolution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07251</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ArcMark: Multi-bit LLM Watermark via Optimal Transport</title><link>https://arxiv.org/abs/2602.07235</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives the first information-theoretic capacity characterization for multi-bit watermarks in language model outputs.&lt;/li&gt;&lt;li&gt;Proposes ArcMark, a coding-theoretic multi-bit watermark construction that (under certain assumptions) achieves the derived capacity.&lt;/li&gt;&lt;li&gt;Empirically shows ArcMark outperforms prior multi-bit watermarks in bits-per-token and detection accuracy.&lt;/li&gt;&lt;li&gt;Frames LM watermarking as a channel coding problem, enabling principled design of detection/encoding schemes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atefeh Gilani', 'Carol Xuan Long', 'Sajani Vithana', 'Oliver Kosut', 'Lalitha Sankar', 'Flavio P. Calmon']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM security', 'model attribution', 'coding theory', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07235</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Extended to Reality: Prompt Injection in 3D Environments</title><link>https://arxiv.org/abs/2602.07104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PI3D, a prompt-injection attack where attackers place text-bearing physical objects in 3D environments to manipulate multimodal LLMs.&lt;/li&gt;&lt;li&gt;Formulates and solves the optimization problem of finding effective 3D object poses (position and orientation) that are physically plausible and induce the model to perform the injected task.&lt;/li&gt;&lt;li&gt;Empirically demonstrates effectiveness across multiple MLLMs and camera trajectories and evaluates existing defenses, showing them to be insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoheng Li', 'Ying Chen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'physical-world attacks', 'multimodal LLMs', 'adversarial attacks', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07104</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks</title><link>https://arxiv.org/abs/2602.07090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPARSE, a concept-aware, user-centric privacy framework for protecting text embeddings against inversion attacks by learning differentiable masks that identify privacy-sensitive embedding dimensions.&lt;/li&gt;&lt;li&gt;Introduces use of a Mahalanobis mechanism to inject elliptical noise calibrated to per-dimension sensitivity, selectively perturbing sensitive dimensions while preserving non-sensitive semantics.&lt;/li&gt;&lt;li&gt;Shows empirical evaluation across six datasets, three embedding models, and multiple attack scenarios where SPARSE reduces privacy leakage and improves downstream utility compared to standard differential privacy baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu-Che Tsai', 'Hsiang Hsiao', 'Kuan-Yu Chen', 'Shou-De Lin']&lt;/li&gt;&lt;li&gt;Tags: ['embedding inversion', 'privacy defense', 'differential privacy', 'Mahalanobis mechanism', 'mask learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07090</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation</title><link>https://arxiv.org/abs/2602.07073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Pro-ZD, a graph neural network model to identify weighted shortest paths indicative of risky connectivity in enterprise networks.&lt;/li&gt;&lt;li&gt;Uses the model to detect misconfigurations and high-risk paths that could expose critical assets to zero-day exploitation.&lt;/li&gt;&lt;li&gt;Implements an automated, proactive framework that fine-tunes firewall rules and access policies to mitigate identified high-risk connections.&lt;/li&gt;&lt;li&gt;Evaluates transferability and robustness, reporting &gt;95% average accuracy in detecting high-risk connections across experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nardine Basta', 'Firas Ben Hmida', 'Houssem Jmal', 'Muhammad Ikram', 'Mohamed Ali Kaafar', 'Andy Walker']&lt;/li&gt;&lt;li&gt;Tags: ['network-security', 'graph-neural-networks', 'zero-day-mitigation', 'automated-firewall-management', 'threat-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07073</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures</title><link>https://arxiv.org/abs/2602.07028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of standard CNNs (ConvNet, VGG, ResNet18) and CNNs with ANFIS classifier replacements on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100.&lt;/li&gt;&lt;li&gt;Evaluates adversarial robustness under gradient-based (PGD) and gradient-free (Square) attacks.&lt;/li&gt;&lt;li&gt;Finds ANFIS augmentation yields architecture-dependent effects: ResNet18-ANFIS shows improved robustness, VGG-ANFIS often underperforms; clean accuracy is not consistently improved.&lt;/li&gt;&lt;li&gt;Concludes neuro-fuzzy augmentation can enhance robustness for specific architectures but is not a universal defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaaustaaub Shankar', 'Bharadwaj Dogga', 'Kelly Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'adversarial-attacks', 'neuro-fuzzy', 'empirical-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07028</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI for Sustainable Data Protection and Fair Algorithmic Management in Environmental Regulation</title><link>https://arxiv.org/abs/2602.07021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive review of AI-enhanced homomorphic encryption (HE) and multi-party computation (MPC) techniques for protecting environmental data, highlighting dynamic key management, adaptive encryption schemes, and efficiency optimizations.&lt;/li&gt;&lt;li&gt;Analyzes AI-driven protocol optimization and fault mitigation in MPC to improve secure collaborative processing of sensitive environmental information.&lt;/li&gt;&lt;li&gt;Identifies gaps at the intersection of AI, cyber law, and environmental regulation—notably algorithmic bias, transparency, accountability—and recommends stronger, adaptive regulatory frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahibpreet Singh', 'Saksham Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['homomorphic encryption', 'multi-party computation', 'data privacy', 'security defenses', 'algorithmic fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07021</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models</title><link>https://arxiv.org/abs/2602.07013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CR-VLM, an activation-steering approach for configurable refusal in Vision-Language Models (VLMs) to provide user-adaptive safety behavior.&lt;/li&gt;&lt;li&gt;Introduces three components: (1) teacher-forced extraction of a configurable refusal vector; (2) a gating mechanism to reduce over-refusal and preserve in-scope acceptance; (3) a counterfactual vision enhancement module to align visual representations with refusal needs.&lt;/li&gt;&lt;li&gt;Reports comprehensive experiments across multiple datasets and VLMs showing effective, efficient, and robust configurable refusal behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxi Yang', 'Shicheng Liu', 'Yuchen Yang', 'Dongwon Lee']&lt;/li&gt;&lt;li&gt;Tags: ['refusal mechanisms', 'safety alignment', 'activation steering', 'vision-language models', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07013</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse</title><link>https://arxiv.org/abs/2602.08939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CausalT5K, a 5,000+ example diagnostic benchmark probing LLM failures in causal reasoning (rung collapse, sycophancy, miscalibrated refusal).&lt;/li&gt;&lt;li&gt;Evaluates models' ability to detect when interventional queries are answered with purely associational evidence, resist adversarial sycophantic drift, and produce 'Wise Refusals' specifying missing information.&lt;/li&gt;&lt;li&gt;Built via human–machine collaboration and multi-stage verification; decomposes performance into Utility (sensitivity) and Safety (specificity) and reports that static audit policies fail in evaluated scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longling Geng', 'Andy Ouyang', 'Theodore Wu', 'Daphne Barretto', 'Matthew John Hayes', 'Rachael Cooper', 'Yuqiao Zeng', 'Sameer Vijay', 'Gia Ancone', 'Ankit Rai', 'Matthew Wolfman', 'Patrick Flanagan', 'Edward Y. Chang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'benchmarks', 'adversarial robustness', 'refusal calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08939</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment</title><link>https://arxiv.org/abs/2602.08449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes alignment evaluation as an information-flow problem under partial observability and proves divergence between evaluation and deployment is bounded by the mutual information between internal representations and the regime variable.&lt;/li&gt;&lt;li&gt;Proposes 'regime-blind' training: training-time, adversarial-invariance interventions that reduce extractability of regime information at decision-relevant internal representations as a defense.&lt;/li&gt;&lt;li&gt;Empirical evaluation on an open-weight language model for two failure modes (scientific sycophancy and temporal sleeper agents) shows regime-blind training suppresses regime-conditioned behavior without measurable utility loss, but with different dynamics and limitations across failure modes.&lt;/li&gt;&lt;li&gt;Advocates complementing behavioral evaluation with white-box diagnostics of regime awareness and information flow.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Igor Santos-Grueiro']&lt;/li&gt;&lt;li&gt;Tags: ['regime leakage', 'sycophancy', 'sleeper agents', 'adversarial invariance', 'alignment evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08449</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent</title><link>https://arxiv.org/abs/2602.08412</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PASB, an end-to-end security evaluation framework tailored to personalized LLM-based agents, emphasizing realistic usage, toolchains, and long-horizon interactions.&lt;/li&gt;&lt;li&gt;Implements black-box, end-to-end attack paradigms and applies them to OpenClaw as a case study across personalized scenarios, tool capabilities, and attack types.&lt;/li&gt;&lt;li&gt;Finds critical vulnerabilities in stages such as user-prompt processing, tool invocation/usage, and memory retrieval, demonstrating substantial security risks for personalized agent deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Feiming Xu', 'Zheng Lin', 'Guangyu He', 'Yuzhe Huang', 'Haichang Gao', 'Zhenxing Niu']&lt;/li&gt;&lt;li&gt;Tags: ['personalized agents', 'attack benchmark', 'red teaming', 'tool-use vulnerabilities', 'memory extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08412</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI</title><link>https://arxiv.org/abs/2602.08373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VIRF, a neuro-symbolic architecture that pairs a deterministic Logic Tutor with an LLM planner to provide verifiable, causal feedback and repair unsafe plans.&lt;/li&gt;&lt;li&gt;Proposes a scalable pipeline to synthesize safety knowledge bases from real-world documents to fill gaps in existing benchmarks and ground the Logic Tutor.&lt;/li&gt;&lt;li&gt;Demonstrates strong safety results on home-embodied tasks (0% Hazardous Action Rate, 77.3% Goal-Condition Rate) with low average correction iterations (1.1), emphasizing active repair over rejection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiyu Wu', 'Xu Zheng', 'Yue Qu', 'Zhuocheng Wang', 'Zicheng Feng', 'Hui Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'verification', 'neuro-symbolic', 'embodied AI', 'planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08373</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection</title><link>https://arxiv.org/abs/2602.08214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Recursive Entropy to quantify risk of resource consumption during a model's reflective reasoning process.&lt;/li&gt;&lt;li&gt;Presents RECUR, an attack that crafts counterfactual prompts to trigger excessive reflection, dramatically increasing output length and lowering throughput.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing resource-exhaustion effects (up to 11x longer outputs, ~90% throughput reduction) and discusses implications for robust reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziwei Wang', 'Yuanhe Zhang', 'Jing Chen', 'Zhenhong Zhou', 'Ruichao Liang', 'Ruiying Du', 'Ju Jia', 'Cong Wu', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['resource-exhaustion', 'adversarial-attack', 'denial-of-service', 'robustness', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08214</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention</title><link>https://arxiv.org/abs/2602.08121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops Glow, a generative-AI DBT skills coach for substance use recovery and HIV prevention, and conducts usability/safety testing with clinical staff and people with lived experience.&lt;/li&gt;&lt;li&gt;Uses the Helpful, Honest, and Harmless (HHH) framework and user-driven adversarial testing (risk probes) to evaluate safety across 37 interactions.&lt;/li&gt;&lt;li&gt;Finds 73% of probes handled appropriately overall, with notable vulnerabilities: chain analysis agent more likely to validate maladaptive beliefs ('empathy trap'), encouragement/normalization of substance use, and DBT skill misinformation (27 instances).&lt;/li&gt;&lt;li&gt;Provides a replicable adversarial safety-evaluation methodology and highlights mitigations needed before clinical deployment or trials.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liying Wang', 'Madison Lee', 'Yunzhang Jiang', 'Steven Chen', 'Kewei Sha', 'Yunhe Feng', 'Frank Wong', 'Lisa Hightow-Weidman', 'Weichao Yuwen']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'adversarial-testing', 'red-teaming', 'mental-health-AI', 'generative-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08121</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems</title><link>https://arxiv.org/abs/2602.08104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage, gradient-based interpretable framework for failure forensics in multi-agent reinforcement learning (MARL): (1) per-agent Patient-0 detection via Taylor-remainder analysis of policy-gradient costs, (2) validation and contagion analysis via first-order critic sensitivities and directional second-order curvature aggregated over causal windows to build contagion graphs.&lt;/li&gt;&lt;li&gt;Addresses three forensic tasks: identifying the true initial failure source (Patient-0), explaining why non-attacked agents may be detected first due to domino/contagion effects, and tracing how failures propagate through learned coordination pathways.&lt;/li&gt;&lt;li&gt;Demonstrates high Patient-0 detection accuracy (88.2–99.4%) across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, and provides interpretable geometric evidence for detection decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Risal Shahriar Shefin', 'Debashis Gupta', 'Thai Le', 'Sarra Alqahtani']&lt;/li&gt;&lt;li&gt;Tags: ['MARL', 'Failure Attribution', 'Interpretability', 'Forensics/Defense', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08104</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities</title><link>https://arxiv.org/abs/2602.08092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode in RL (Objective Decoupling) that arises when human evaluators are sycophantic, lazy, or adversarial, causing learned objectives to diverge from latent ground truth.&lt;/li&gt;&lt;li&gt;Shows that standard consensus-based methods (trusting majority/noisy human feedback) can provably converge to misalignment in social evaluation settings (violating 'Dogma 4').&lt;/li&gt;&lt;li&gt;Proposes Epistemic Source Alignment (ESA), a defense that judges the source of feedback using sparse safety axioms ('judging the judges') rather than relying on signal consensus, with theoretical guarantees of recovery of the true objective even under majority bias.&lt;/li&gt;&lt;li&gt;Provides empirical results demonstrating ESA succeeds where traditional consensus methods fail under colluding or biased evaluator majorities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Majid Ghasemi', 'Mark Crowley']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial feedback', 'reward poisoning', 'robust reinforcement learning', 'alignment defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08092</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Securing Dual-Use Pathogen Data of Concern</title><link>https://arxiv.org/abs/2602.08061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a five-tier Biosecurity Data Level (BDL) framework to categorize pathogen-related data by risk of contributing to dangerous biological capabilities when used to train AI models.&lt;/li&gt;&lt;li&gt;Recommends technical restrictions and controls tailored to each BDL tier to limit access and prevent misuse in model training.&lt;/li&gt;&lt;li&gt;Outlines a governance framework for newly created dual-use pathogen data, arguing data controls are a high-leverage intervention to reduce proliferation of concerning biological AI capabilities.&lt;/li&gt;&lt;li&gt;Motivated by community endorsement (Asilomar conference) and the recognition that training data choices shape AI capabilities of biosecurity concern.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doni Bloomfield', 'Allison Berke', 'Moritz S. Hanke', 'Aaron Maiwald', 'James R. M. Black', 'Toby Webster', 'Tina Hernandez-Boussard', 'Oliver M. Crook', 'Jassi Pannu']&lt;/li&gt;&lt;li&gt;Tags: ['biosecurity', 'data governance', 'AI safety', 'dual-use', 'data-level defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08061</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective</title><link>https://arxiv.org/abs/2602.08009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAPS, a reputation-aware publish-subscribe framework for adaptive, scalable coordination among LLM agents that routes messages by declared intents rather than fixed topologies.&lt;/li&gt;&lt;li&gt;Adds two overlays: Reactive Subscription for dynamic intent refinement and Bayesian Reputation for local detection and isolation of malicious peers.&lt;/li&gt;&lt;li&gt;Evaluates the approach on five benchmarks, claiming improved adaptivity, scalability, and robustness in multi-agent coordination.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Li', 'Zeyu Zhang', 'Xiaohe Bo', 'Quanyu Dai', 'Chaozhuo Li', 'Feng Wen', 'Xu Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent systems', 'Robustness &amp; security', 'Reputation systems', 'Publish-subscribe networking', 'Adversarial peer detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.08009</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Selective Fine-Tuning for Targeted and Robust Concept Unlearning</title><link>https://arxiv.org/abs/2602.07919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRUST: a selective fine-tuning method to unlearn harmful concepts in text-guided diffusion models by dynamically estimating target concept neurons and finetuning only them.&lt;/li&gt;&lt;li&gt;Adds a Hessian-based regularization to improve robustness, showing resistance to adversarial prompts while preserving generation quality.&lt;/li&gt;&lt;li&gt;Demonstrates unlearning of individual, combined, and conditional concepts with faster training and better utility retention compared to full finetuning SOTA baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mansi', 'Avinash Kori', 'Francesca Toni', 'Soteris Demetriou']&lt;/li&gt;&lt;li&gt;Tags: ['concept unlearning', 'diffusion models', 'model hardening', 'selective fine-tuning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07919</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?</title><link>https://arxiv.org/abs/2602.07470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled evaluation framework that perturbs a model's own chain-of-thought (CoT) at fixed timesteps with seven intervention types (benign, neutral, adversarial).&lt;/li&gt;&lt;li&gt;Evaluates multiple open-weight reasoning LLMs across Math, Science, and Logic tasks, measuring robustness, recovery behavior, and costs of recovery (e.g., CoT length inflation).&lt;/li&gt;&lt;li&gt;Finds RLLMs are generally robust and recover from many perturbations (robustness improves with model size and degrades for earlier interventions), but recovery often involves expressed doubt and increased CoT length; paraphrasing can suppress doubt and hurt accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander von Recum', 'Leander Girrbach', 'Zeynep Akata']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'adversarial interventions', 'robustness evaluation', 'model recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07470</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies</title><link>https://arxiv.org/abs/2602.07432</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a temporal fingerprinting method (coefficient of variation of inter-post intervals) to distinguish human-driven from autonomous agent activity on the Moltbook platform.&lt;/li&gt;&lt;li&gt;Analyzes 91,792 posts and 405,707 comments from 22,020 agents, finding that high-profile 'emergent' phenomena were overwhelmingly linked to human intervention rather than clear autonomous origins.&lt;/li&gt;&lt;li&gt;Uses a 44-hour platform shutdown as a natural experiment: human-influenced agents reconnected earlier (87.7% of early reconnectors), supporting the attribution method; also documents coordinated bot farming (4 accounts producing 32% of comments).&lt;/li&gt;&lt;li&gt;Shows rapid decay of human influence through reply chains and argues the methods generalize to attribution and manipulation detection in emerging multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ning Li']&lt;/li&gt;&lt;li&gt;Tags: ['bot detection', 'attribution', 'multi-agent systems', 'temporal fingerprinting', 'misinformation/manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07432</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>NAAMSE: Framework for Evolutionary Security Evaluation of Agents</title><link>https://arxiv.org/abs/2602.07391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NAAMSE, an evolutionary framework that automates security evaluation of AI agents via feedback-driven optimization.&lt;/li&gt;&lt;li&gt;Employs a single autonomous agent to orchestrate genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring using model responses as fitness signals.&lt;/li&gt;&lt;li&gt;Shows that iterative evolutionary mutation uncovers high-severity vulnerabilities missed by one-shot methods (evaluated on Gemini 2.5 Flash) while enforcing benign-use correctness.&lt;/li&gt;&lt;li&gt;Provides open-source code for reproducibility and scalability of adaptive red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kunal Pai', 'Parth Shah', 'Harshil Patel']&lt;/li&gt;&lt;li&gt;Tags: ['Red teaming', 'Adversarial attacks', 'Automated vulnerability discovery', 'Security evaluation', 'Evolutionary algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07391</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective</title><link>https://arxiv.org/abs/2602.07259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames AI oversight as an adversarial resource-allocation problem using Stackelberg Security Games (SSGs).&lt;/li&gt;&lt;li&gt;Applies the SSG framework to concrete security concerns: training-time auditing for data/feedback poisoning, constrained pre-deployment evaluation, and robust multi-model deployment under adversarial uncertainty.&lt;/li&gt;&lt;li&gt;Argues for incentive-aware institutional design (auditors, evaluators, deployers) to deter manipulation and improve proactive, risk-aware AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheol Woo Kim', 'Davin Choo', 'Tzeh Yuan Neoh', 'Milind Tambe']&lt;/li&gt;&lt;li&gt;Tags: ['Stackelberg Security Games', 'adversarial poisoning', 'audit/evaluation defenses', 'incentive-aware oversight', 'deployment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07259</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item><item><title>From Out-of-Distribution Detection to Hallucination Detection: A Geometric View</title><link>https://arxiv.org/abs/2602.07253</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes hallucination detection for large language models as an out-of-distribution (OOD) detection problem by treating next-token prediction as a classification task.&lt;/li&gt;&lt;li&gt;Adapts OOD techniques to account for structural differences in LLMs and proposes training-free, single-sample detectors.&lt;/li&gt;&lt;li&gt;Shows strong performance on hallucination detection for reasoning tasks and argues this OOD perspective is a scalable approach to improving LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Litian Liu', 'Reza Pourreza', 'Yubing Jian', 'Yao Qin', 'Roland Memisevic']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'out-of-distribution detection', 'LLM safety', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.07253</guid><pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
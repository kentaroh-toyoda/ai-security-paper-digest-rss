<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 03 Jun 2025 11:24:17 +0000</lastBuildDate><item><title>Don't Let Your Robot be Harmful: Responsible Robotic Manipulation via Safety-as-Policy</title><link>https://arxiv.org/abs/2411.18289</link><description>Introduces 'responsible robotic manipulation' to ensure robots consider safety hazards when executing human instructions.
Proposes a 'Safety-as-policy' approach combining a world model for scenario generation and a mental model for safety cognition.
Presents the SafeBox synthetic dataset for benchmarking responsible robotic manipulation in various safety risk scenarios.
Demonstrates that the proposed method outperforms baselines in avoiding risks and completing tasks safely in both synthetic and real-world settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18289</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model</title><link>https://arxiv.org/abs/2505.21179</link><description>Introduces Normalized Attention Guidance (NAG), a training-free method for negative guidance in diffusion models.
Addresses the challenge of suppressing unwanted attributes, especially in few-step sampling regimes where existing methods like Classifier-Free Guidance (CFG) fail.
NAG is model-agnostic, works across architectures and modalities, and improves text alignment, fidelity, and perceived quality.
Provides a universal, efficient plug-in for negative guidance without retraining, validated by experiments and user studies.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21179</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning</title><link>https://arxiv.org/abs/2504.21307</link><description>Investigates the vulnerability of diffusion models to jailbreaking attacks even after harmful concepts are 'unlearned'.
Proposes a novel attack method using interpretable token embeddings to reveal why unlearning is incomplete.
Demonstrates that these attack embeddings are transferable and expose persistent vulnerabilities in unlearned models.
Develops and evaluates a defense strategy informed by the interpretable attack method to better protect against jailbreaking.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21307</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking 3D Human Pose Estimation Models under Occlusions</title><link>https://arxiv.org/abs/2504.10350</link><description>Benchmarks the robustness of 3D Human Pose Estimation (HPE) models under realistic occlusion scenarios.
Evaluates nine state-of-the-art 2D-to-3D HPE models using a synthetic dataset with occlusion labels.
Finds that all models degrade significantly under occlusion, with diffusion-based models performing worst.
Identifies consistent vulnerabilities in distal joints, highlighting areas for improving model robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.10350</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection</title><link>https://arxiv.org/abs/2504.04495</link><description>Proposes a weakly supervised framework for robust video anomaly detection using audio-visual collaboration.
Leverages CLIP's cross-modal representation learning to fuse audio, visual, and textual information for improved anomaly detection.
Introduces an uncertainty-driven feature distillation module to enhance robustness when one modality is missing during inference.
Demonstrates improved performance and robustness over unimodal approaches on multiple benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04495</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models</title><link>https://arxiv.org/abs/2411.18672</link><description>Introduces FactCheXcker, a modular framework to reduce hallucinated quantitative measurements in radiology report generation.
Uses a query-code-update paradigm and leverages large language models for accurate measurement extraction and report updating.
Demonstrates significant reduction in measurement hallucinations and improved precision across multiple medical report-generation models.
Focuses on clinical reliability and safety in AI-generated medical reports.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18672</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection</title><link>https://arxiv.org/abs/2409.09724</link><description>Proposes MFCLIP, a multi-modal fine-grained CLIP-based model for detecting face forgeries generated by diffusion models.
Addresses the limitations of existing face forgery detection methods, particularly their inability to generalize to unseen diffusion-synthesized images.
Leverages multi-modal data (images, noise, and text) and advanced representation learning to improve robustness and generalization.
Demonstrates superior performance over state-of-the-art methods in cross-generator, cross-forgery, and cross-dataset scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.09724</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LaWa: Using Latent Space for In-Generation Image Watermarking</title><link>https://arxiv.org/abs/2408.05868</link><description>Introduces LaWa, a novel in-generation image watermarking method for Latent Diffusion Models (LDMs).
Leverages the latent space of pre-trained autoencoders to embed watermarks during image generation.
Demonstrates high robustness against various image transformations and adversarial attacks while maintaining image quality.
Addresses concerns about the malicious use of AI-generated images by enabling reliable provenance tracking.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.05868</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Open Set Single Image Test Time Adaptation of Vision Language Models</title><link>https://arxiv.org/abs/2406.00481</link><description>Proposes ROSITA, a framework for open-set single-image test-time adaptation in vision-language models.
Addresses the challenge of adapting models to dynamic, real-world environments with shifting data distributions and unseen classes.
Introduces a benchmark and demonstrates state-of-the-art performance in distinguishing known from unknown classes during test time.
Focuses on computational efficiency and real-time deployment, which is important for practical AI system robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.00481</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation</title><link>https://arxiv.org/abs/2506.01591</link><description>Addresses security and privacy risks in LDM-based talking-head generation, which can be misused for scams and misinformation.
Proposes 'Silencer', a two-stage method to proactively protect portrait privacy by nullifying audio control and resisting purification techniques.
Demonstrates that existing defenses are insufficient against advanced image-to-video animation and presents a more robust solution.
Highlights ethical concerns and aims to raise awareness in the AI security community about the misuse of talking-head generation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01591</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations</title><link>https://arxiv.org/abs/2506.00868</link><description>Introduces MultiFakeVerse, a large-scale dataset of person-centric deepfake images generated via vision-language models.
Focuses on semantic and context-aware manipulations, such as altering actions, scenes, and human-object interactions.
Demonstrates that both state-of-the-art deepfake detectors and human observers struggle to identify these advanced manipulations.
Highlights the growing challenge of detecting sophisticated, AI-generated deepfakes that can influence perception and narrative.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00868</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Using Diffusion Ensembles to Estimate Uncertainty for End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2506.00560</link><description>Proposes EnDfuser, an end-to-end autonomous driving system using diffusion models for trajectory planning.
EnDfuser generates a distribution of candidate trajectories, enabling uncertainty estimation and interpretability.
The approach aims to improve safety in autonomous driving by explicitly modeling uncertainty in decision making.
Demonstrates competitive performance in simulation benchmarks with minimal impact on inference speed.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00560</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition</title><link>https://arxiv.org/abs/2506.01806</link><description>Proposes a novel multi-stage transformer-based approach for contactless fingerprint recognition.
Addresses challenges such as out-of-focus images, low contrast, and perspective distortion in biometric systems.
Focuses on improving robustness and accuracy in cross-domain fingerprint matching, which is critical for secure biometric authentication.
Demonstrates superior performance over existing methods on public datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01806</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2506.01783</link><description>Introduces FaceCoT, a large-scale Visual Question Answering (VQA) dataset specifically designed for face anti-spoofing (FAS).
Covers 14 types of spoofing attacks, providing diverse and high-quality chain-of-thought (CoT) annotations to improve model reasoning and interpretability.
Proposes a CoT-Enhanced Progressive Learning (CEPL) strategy to leverage the dataset for improved robustness against presentation attacks.
Demonstrates that models trained with FaceCoT and CEPL outperform state-of-the-art methods in FAS tasks, enhancing security against facial spoofing.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01783</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond black and white: A more nuanced approach to facial recognition with continuous ethnicity labels</title><link>https://arxiv.org/abs/2506.01532</link><description>Proposes using continuous ethnicity labels instead of discrete categories to address bias in facial recognition datasets.
Demonstrates experimentally and theoretically that balancing datasets using continuous labels leads to better model performance and fairness.
Highlights that traditional discrete balancing does not ensure true dataset fairness or representation.
Trained over 65 models and created 20+ dataset subsets to validate the approach.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01532</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment</title><link>https://arxiv.org/abs/2506.01511</link><description>Introduces a new framework (APA) for generating unrestricted adversarial examples by aligning with adversary preferences in diffusion models.
Addresses the challenge of balancing visual consistency and attack effectiveness, which are often conflicting objectives in adversarial attacks.
Proposes a two-stage optimization process to decouple and optimize for both visual similarity and attack success, enhancing black-box transferability.
Demonstrates improved attack transferability and visual quality, suggesting a novel alignment-based approach to adversarial attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01511</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark</title><link>https://arxiv.org/abs/2506.01466</link><description>Introduces SVTA, a large-scale synthetic video-text benchmark for anomaly retrieval, addressing data scarcity and privacy issues in real-world datasets.
Utilizes generative models and LLMs to create diverse, realistic video-text pairs covering 68 anomaly categories and 30 normal activities.
Aims to facilitate research in video anomaly detection and retrieval, which is important for public safety and surveillance applications.
SVTA eliminates privacy risks by using synthetic data, making it suitable for scalable and ethical AI research.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01466</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dirty and Clean-Label attack detection using GAN discriminators</title><link>https://arxiv.org/abs/2506.01224</link><description>Proposes using GAN discriminators to detect dirty-label and clean-label data poisoning attacks in computer vision datasets.
Demonstrates that GAN discriminator confidence scores can effectively identify mislabeled or poisoned images after calibration.
Shows that the method can detect 100% of tested poisoned images above a certain perturbation threshold.
Provides a practical approach for developers to protect high-value classes in their models without retraining the main classifier.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01224</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation</title><link>https://arxiv.org/abs/2506.01118</link><description>Introduces CXR-PathFinder, a large language model for automated chest X-ray report generation.
Proposes Clinician-Guided Adversarial Fine-Tuning (CGAFT) to integrate expert feedback and mitigate factual inconsistencies.
Implements a Knowledge Graph Augmentation Module (KGAM) to verify generated statements and reduce hallucinations.
Focuses on improving clinical accuracy, reliability, and safety in medical AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01118</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>NavBench: Probing Multimodal Large Language Models for Embodied Navigation</title><link>https://arxiv.org/abs/2506.01031</link><description>Introduces NavBench, a benchmark for evaluating multimodal large language models (MLLMs) in embodied navigation tasks.
Assesses models on navigation comprehension and step-by-step execution in simulated indoor environments.
Finds that while advanced models like GPT-4o perform well, most models struggle with temporal understanding and progress estimation.
Highlights challenges in deploying MLLMs for real-world robotic navigation, with implications for safety and robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01031</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack</title><link>https://arxiv.org/abs/2506.00978</link><description>Proposes a new projector-based adversarial attack (CAPAA) that works across multiple classifiers and varying camera poses.
Introduces a classifier-agnostic adversarial loss and optimization framework to aggregate gradients from multiple classifiers.
Develops an attention-based gradient weighting mechanism to enhance attack robustness and stealthiness.
Demonstrates improved attack success rates and stealthiness over existing methods through extensive experiments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00978</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection</title><link>https://arxiv.org/abs/2506.00956</link><description>Introduces Continual-MEGA, a large-scale benchmark for continual anomaly detection, simulating real-world deployment scenarios.
Proposes a novel scenario for zero-shot generalization to unseen classes, enhancing the evaluation of continual learning methods.
Presents a unified baseline algorithm that improves robustness and generalization in few-shot anomaly detection.
Finds that existing methods have significant room for improvement, especially in pixel-level defect localization.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00956</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection</title><link>https://arxiv.org/abs/2506.00874</link><description>Identifies latent prior bias as a key reason why current AIGC (AI-generated content) detectors fail to generalize to unseen generators.
Proposes On-Manifold Adversarial Training (OMAT) to generate adversarial examples that remain on the generator's output manifold, improving detector robustness.
Introduces a new benchmark (GenImage++) for evaluating detectors against advanced generative models and diverse prompts.
Demonstrates that adversarially trained detectors significantly improve cross-generator detection performance, providing insights for robust AIGC forensics.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00874</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors</title><link>https://arxiv.org/abs/2506.00661</link><description>Addresses the vulnerability of image classifiers, particularly vision transformers, to adversarial attacks.
Proposes a countermeasure using low-rank adaptation (LoRA) to improve robustness against adversarial perturbations.
Enables scalable fine-tuning of pretrained models without full retraining, enhancing security and efficiency.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00661</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Video Signature: In-generation Watermarking for Latent Video Diffusion Models</title><link>https://arxiv.org/abs/2506.00652</link><description>Proposes an in-generation watermarking method (VIDSIG) for latent video diffusion models to protect intellectual property and enable content tracing.
VIDSIG integrates watermarks during the video generation process, reducing computational overhead and improving the balance between video quality and watermark extraction.
The method includes Perturbation-Aware Suppression to preserve visual quality and a Temporal Alignment module for coherent frame sequences.
Demonstrates strong robustness against spatial and temporal tampering, making it practical for real-world AI-generated video security.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00652</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models</title><link>https://arxiv.org/abs/2506.00562</link><description>Introduces SEED, a large-scale dataset for sequential facial attribute editing using diffusion models.
Focuses on tracking and analyzing sequences of progressive edits, which is relevant for manipulation detection and visual provenance.
Proposes FAITH, a transformer-based model for detecting subtle sequential changes in edited images.
Facilitates research on manipulation robustness, edit attribution, and provenance analysis, which are important for AI security and safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00562</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views</title><link>https://arxiv.org/abs/2506.00394</link><description>Introduces a new dataset (TF2025) with synchronized first- and third-person camera views.
Proposes a sequence-based method to identify first-person camera wearers in third-person footage using motion cues and person re-identification.
Addresses challenges in multi-camera interactions, which are relevant for privacy and surveillance concerns.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00394</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking</title><link>https://arxiv.org/abs/2506.00325</link><description>Proposes a novel adversarial defense method (DiffDf) for visual tracking using denoise diffusion probabilistic models.
Introduces a multi-scale defense mechanism combining pixel-level, semantic, and structural losses to suppress adversarial perturbations.
Demonstrates significant improvements in robustness and real-time performance across multiple visual tracking architectures and datasets.
Addresses the vulnerability of deep learning-based visual trackers to adversarial attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00325</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title><link>https://arxiv.org/abs/2506.00227</link><description>Introduces Ctrl-Crash, a controllable video diffusion model for generating realistic car crash scenarios.
Addresses the scarcity of real accident data by enabling synthetic, controllable crash simulations.
Supports counterfactual scenario generation, allowing exploration of how small changes affect crash outcomes.
Aims to improve traffic safety research by providing high-quality, physically realistic crash videos for analysis and training.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00227</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title><link>https://arxiv.org/abs/2505.00212</link><description>Introduces the novel research area of automated failure attribution in LLM multi-agent systems, aiming to identify which agent and step caused a task failure.
Presents the Who&amp;When dataset, containing annotated failure logs from 127 LLM multi-agent systems.
Develops and evaluates three automated methods for failure attribution, revealing significant challenges and low accuracy in current approaches.
Highlights the complexity of failure attribution and the need for further research to improve system debugging and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.00212</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</title><link>https://arxiv.org/abs/2411.17451</link><description>Introduces VL-RewardBench, a benchmark for evaluating vision-language generative reward models (VL-GenRMs), which are important for aligning and evaluating multimodal AI systems.
Highlights the limitations of current evaluation methods and provides a more challenging and comprehensive testbed, including visual hallucination detection and complex reasoning tasks.
Finds that even state-of-the-art models perform poorly on the benchmark, revealing significant gaps in current VL-GenRM capabilities.
Provides insights into model failures and suggests that improved training methods can substantially boost model judgment accuracy.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17451</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CLEAR: Character Unlearning in Textual and Visual Modalities</title><link>https://arxiv.org/abs/2410.18057</link><description>Introduces CLEAR, the first open-source benchmark for multimodal machine unlearning (MMU), focusing on both textual and visual data.
Addresses the challenge of removing private or hazardous information from deep learning models across modalities.
Evaluates 11 machine unlearning methods and finds that joint unlearning across modalities is more effective than single-modality approaches.
Provides a new dataset to facilitate research in secure and privacy-preserving AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18057</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AdvAgent: Controllable Blackbox Red-teaming on Web Agents</title><link>https://arxiv.org/abs/2410.17401</link><description>Introduces AdvAgent, a black-box red-teaming framework targeting web agents powered by foundation models.
Uses reinforcement learning to train an adversarial prompter that generates effective, stealthy, and controllable attack prompts.
Demonstrates high attack success rates against GPT-4-based web agents, revealing significant vulnerabilities.
Finds that current prompt-based defenses are insufficient, underscoring the need for improved security mechanisms.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17401</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How much do language models memorize?</title><link>https://arxiv.org/abs/2505.24832</link><description>Introduces a new method to estimate how much language models memorize specific datapoints versus generalize from data.
Formally separates memorization into unintended memorization (potential privacy risk) and generalization.
Finds that language models memorize data up to their capacity, after which generalization increases and memorization decreases.
Provides scaling laws relating model capacity, data size, and membership inference, which is relevant for privacy and security.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24832</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ScEdit: Script-based Assessment of Knowledge Editing</title><link>https://arxiv.org/abs/2505.23291</link><description>Introduces ScEdit, a new benchmark for evaluating knowledge editing in language models using script-based (action-oriented) scenarios.
Extends evaluation from simple fact-based questions to more complex action-based and temporal edits, reflecting real-world application needs.
Finds that current knowledge editing methods struggle with the new benchmark, especially on text-level metrics, highlighting robustness challenges.
Provides a comprehensive analysis of existing knowledge editing techniques under more realistic and challenging conditions.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23291</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments</title><link>https://arxiv.org/abs/2505.21936</link><description>Introduces RedTeamCUA, a framework for adversarial testing of computer-use agents (CUAs) in hybrid web-OS environments.
Focuses on evaluating vulnerabilities to indirect prompt injection attacks in realistic, controlled scenarios.
Presents RTC-Bench, a benchmark suite with 864 examples targeting hybrid web-OS attack scenarios.
Finds significant vulnerabilities in state-of-the-art CUAs, with high attack success rates even in advanced models.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21936</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Explain: Prototype-Based Surrogate Models for LLM Classification</title><link>https://arxiv.org/abs/2505.18970</link><description>Introduces ProtoSurE, a prototype-based surrogate model to explain LLM classification decisions.
Focuses on improving faithfulness and human-understandability of explanations for LLM outputs.
Demonstrates superior performance and data efficiency compared to state-of-the-art explanation methods.
Aims to make LLM decision processes more transparent for real-world applications.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18970</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DUSK: Do Not Unlearn Shared Knowledge</title><link>https://arxiv.org/abs/2505.15209</link><description>Introduces DUSK, a benchmark for evaluating machine unlearning methods in large language models (LLMs) under realistic scenarios with overlapping data.
Highlights the challenge of selectively removing unique, sensitive, or copyrighted content while preserving shared factual knowledge.
Evaluates nine recent unlearning methods and finds that most struggle to remove context-specific knowledge without harming shared content.
Provides seven evaluation metrics and releases the DUSK benchmark to support the development of more precise and reliable unlearning techniques.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15209</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning</title><link>https://arxiv.org/abs/2505.11958</link><description>Proposes HiPPrO, a novel framework for generating counterspeech conditioned on multiple attributes (e.g., intent and emotion) to combat hate speech online.
Introduces hierarchical prefix learning and preference optimization to improve the quality and relevance of generated counterspeech.
Extends an existing dataset (IntentCONANv2) with emotion labels and demonstrates significant improvements over baseline models in both automatic and human evaluations.
Highlights the importance of multi-attribute conditioning for more effective and nuanced counterspeech generation.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11958</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content</title><link>https://arxiv.org/abs/2503.16031</link><description>Introduces a new benchmark dataset (Deceptive Humor Dataset) focused on humor-infused misinformation.
Highlights the challenge of detecting false narratives when they are presented humorously.
Provides multilingual data and categorizes humor types and satire levels to facilitate nuanced analysis.
Aims to support research on how humor can be exploited to propagate misinformation and evade detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16031</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Navigating Rifts in Human-LLM Grounding: Study and Benchmark</title><link>https://arxiv.org/abs/2503.13975</link><description>Analyzes grounding failures in human-LLM (Large Language Model) interactions using multiple datasets.
Develops a taxonomy and models for annotating and forecasting grounding behavior in conversations.
Introduces the Rifts benchmark to systematically evaluate LLM failures in establishing mutual understanding.
Finds that grounding failures can lead to serious consequences and proposes preliminary interventions to mitigate these failures.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.13975</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs</title><link>https://arxiv.org/abs/2503.10084</link><description>Analyzes the complexity of prompt search space in large language models (LLMs), particularly for reasoning tasks.
Provides a theoretical framework explaining why certain prompts are more effective than others in guiding LLM reasoning.
Demonstrates that naive or generic prompts can hinder performance, while optimal prompt design can significantly improve outcomes.
Lays a theoretical foundation for systematic and principled prompt engineering, moving beyond ad hoc methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10084</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Implicit Reasoning in Transformers is Reasoning through Shortcuts</title><link>https://arxiv.org/abs/2503.07604</link><description>Investigates how language models (specifically GPT-2) perform implicit reasoning in multi-step mathematical tasks.
Finds that implicit reasoning often relies on shortcut learning, leading to high performance on familiar patterns but poor generalization.
Highlights a potential vulnerability in language models where reasoning capabilities may not be robust or generalizable.
Suggests implications for the reliability and safety of AI systems relying on implicit reasoning.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07604</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Protecting multimodal large language models against misleading visualizations</title><link>https://arxiv.org/abs/2502.20503</link><description>Identifies a vulnerability in multimodal large language models (MLLMs) where their question-answering accuracy drops significantly when faced with misleading visualizations.
Proposes inference-time methods, such as table-based QA and redrawing visualizations, to improve robustness against misleading charts.
Demonstrates that these methods can significantly improve MLLM performance on misleading visualizations without reducing accuracy on standard visualizations.
Addresses the risk of disinformation through visual data manipulation in AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20503</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric</title><link>https://arxiv.org/abs/2502.17184</link><description>Systematically analyzes 11 existing methods for measuring data diversity in instruction tuning datasets for large language models.
Proposes a new diversity metric, NovelSum, which shows a strong correlation (0.97) with instruction-tuned model performance.
Demonstrates that optimizing for NovelSum leads to better data selection and improved model performance.
Provides practical guidance and tools for constructing higher-quality, more robust datasets for instruction tuning.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17184</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above</title><link>https://arxiv.org/abs/2502.14127</link><description>Critiques the use of multiple choice question answering (MCQA) for evaluating large language models (LLMs), highlighting its limitations in testing generation, subjectivity, and knowledge.
Proposes generative evaluation formats and educational testing methods (e.g., rubrics, Item Response Theory) to improve LLM assessment.
Discusses issues such as dataset leakage, unanswerability, shortcuts, and saturation in MCQA datasets.
Addresses LLM robustness, biases, and unfaithful explanations, suggesting that improved evaluation methods can better measure or mitigate these issues.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14127</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HPSS: Heuristic Prompting Strategy Search for LLM Evaluators</title><link>https://arxiv.org/abs/2502.13031</link><description>Proposes HPSS, a method for optimizing prompting strategies for LLM-based evaluators using a heuristic search inspired by genetic algorithms.
Integrates multiple factors in prompt design to improve alignment of LLM evaluators with human judgment.
Demonstrates that HPSS outperforms existing prompt optimization methods across several evaluation tasks.
Focuses on improving the reliability and robustness of LLM-based evaluation pipelines.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13031</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models</title><link>https://arxiv.org/abs/2502.12821</link><description>Investigates how large language models (LLMs) handle 'redefinition' tasks, where physical constants and units are intentionally altered.
Finds that as LLMs scale, their performance on these tasks degrades and their false confidence increases.
Highlights that LLMs tend to anchor to memorized values, even when prompted to adapt to new definitions.
Suggests that prompting strategies and response formatting have limited impact on overcoming these reasoning gaps.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12821</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Mirage of Model Editing: Revisiting Evaluation in the Wild</title><link>https://arxiv.org/abs/2502.11177</link><description>Introduces QAEdit and WILD, new benchmarks and evaluation frameworks for assessing model editing in real-world scenarios.
Finds that current model editing methods perform much worse in realistic settings than previously reported due to flawed evaluation practices.
Highlights the issue of teacher forcing in prior evaluations, which leads to overestimated performance.
Calls for more rigorous and realistic evaluation methods and the development of robust, scalable model editing techniques.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11177</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions</title><link>https://arxiv.org/abs/2502.11095</link><description>Surveys the use of large language models (LLMs) in psychotherapy, focusing on their potential and current limitations.
Identifies key challenges such as linguistic and cultural biases, diagnostic reliability, and the need for real-time adaptive systems.
Highlights the importance of addressing biases and ensuring reliability for safe and effective deployment in mental health contexts.
Suggests future research directions for more holistic and clinically integrated LLM-based psychotherapy systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11095</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Position: It's Time to Act on the Risk of Efficient Personalized Text Generation</title><link>https://arxiv.org/abs/2502.06560</link><description>Discusses the risks associated with efficient, personalized text generation using open-source LLMs.
Highlights the potential for impersonation attacks, such as phishing or fraudulent social media accounts, using models fine-tuned on individuals' data.
Argues that these risks are distinct from other deepfake modalities (image, voice, video) and are under-addressed by current research and models.
Calls for increased attention from the research community to address these emerging safety and security concerns.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.06560</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dialogue Systems for Emotional Support via Value Reinforcement</title><link>https://arxiv.org/abs/2501.17182</link><description>Proposes a method for training emotional support dialogue systems to reinforce positive human values.
Utilizes online support conversations from Reddit to identify and reinforce values during interactions.
Evaluates the system's effectiveness in support skills, emotional intensity, and value reinforcement, showing improvements over baselines.
Highlights the importance of value alignment and responsible AI in sensitive applications like emotional support.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.17182</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas</title><link>https://arxiv.org/abs/2501.11549</link><description>Proposes a method to infer user personas from preference data to improve LLM personalization.
Introduces Persona Inference (PI) and Persona Tailoring (PT) to better align model outputs with diverse user needs.
Demonstrates that models trained with inferred personas can better serve users with uncommon or atypical preferences.
Highlights the limitations of current alignment methods and suggests abductive reasoning as a way to enhance AI alignment and personalization.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.11549</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Curiosity-Driven Reinforcement Learning from Human Feedback</title><link>https://arxiv.org/abs/2501.11463</link><description>Proposes a new framework (CD-RLHF) that combines curiosity-driven exploration with reinforcement learning from human feedback to improve both diversity and alignment in LLM outputs.
Addresses the trade-off between output diversity and alignment quality, a key challenge in RLHF.
Demonstrates effectiveness through experiments on tasks like text summarization and instruction following, showing improved diversity without sacrificing alignment.
Provides open-source code for reproducibility and further research.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.11463</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection</title><link>https://arxiv.org/abs/2501.02295</link><description>Proposes a framework to systematically evaluate both explicit and implicit social biases in large language models (LLMs).
Uses self-reflection and simulated psychological assessments to measure and compare these biases.
Finds that explicit and implicit biases behave differently: explicit bias decreases with model improvements, while implicit bias can increase.
Shows that current alignment techniques are more effective at reducing explicit bias than implicit bias.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02295</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving Factuality with Explicit Working Memory</title><link>https://arxiv.org/abs/2412.18069</link><description>Proposes EWE (Explicit Working Memory), a method to reduce hallucinations in large language models by integrating real-time feedback from external resources.
EWE uses online fact-checking and retrieval feedback to update its working memory, enabling correction of false claims during text generation.
Demonstrates improved factuality on multiple long-form generation datasets without reducing helpfulness.
Analyzes the impact of memory update rules, memory unit configurations, and retrieval datastore quality on model performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18069</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation</title><link>https://arxiv.org/abs/2412.14050</link><description>Investigates methods for reducing social bias and toxicity in multilingual large language models (LLMs).
Finds that finetuning on non-harmful English text can transfer bias and toxicity mitigation to other languages.
Highlights that direct preference optimization is most effective for toxicity mitigation.
Notes a trade-off: mitigation methods can reduce language generation quality in non-English languages, suggesting a need for language-specific approaches.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.14050</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GAMEBoT: Transparent Assessment of LLM Reasoning in Games</title><link>https://arxiv.org/abs/2412.13602</link><description>Introduces GAMEBoT, a benchmarking framework for transparent assessment of LLM reasoning in games.
Decomposes complex reasoning tasks into modular subproblems, enabling more interpretable evaluation.
Addresses issues like data contamination and performance saturation in current benchmarks.
Benchmarks 17 LLMs across multiple games, providing insights into their reasoning capabilities and limitations.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13602</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits</title><link>https://arxiv.org/abs/2412.13378</link><description>Introduces SummExecEdit, a benchmark for evaluating factual consistency in text summarization using executable edits.
Assesses both the detection of factual errors and the ability to provide accurate explanations for those errors.
Finds that current large language models (LLMs) struggle significantly with factual consistency and explanation quality.
Identifies common types of explanation errors, highlighting challenges in model interpretability and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13378</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study</title><link>https://arxiv.org/abs/2412.13169</link><description>Examines how well large language models (LLMs) can generate synthetic public opinions that reflect real socio-cultural and demographic nuances.
Finds that LLMs, particularly Llama, perform better at representing certain subpopulations and political affiliations, but show biases and limitations.
Highlights the impact of prompt design and demographic variables on the fidelity and representativeness of generated opinions.
Emphasizes the need for alignment and robustness to minimize political bias and improve the reliability of LLM-generated data.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13169</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?</title><link>https://arxiv.org/abs/2412.08985</link><description>Introduces KnowShiftQA, a dataset designed to test the robustness of Retrieval-Augmented Generation (RAG) systems when textbook knowledge shifts.
Simulates knowledge discrepancies between authoritative textbooks and the parametric knowledge in LLMs, reflecting real-world changes in educational content.
Finds that RAG systems experience significant performance drops when faced with these knowledge shifts, especially for questions requiring integration of contextual and parametric knowledge.
Highlights challenges in ensuring the reliability and robustness of AI systems in dynamic knowledge environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08985</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter</title><link>https://arxiv.org/abs/2411.15462</link><description>Introduces HateDay, a global hate speech dataset representative of a single day on Twitter, covering multiple languages and regions.
Finds that current hate speech detection models perform poorly in real-world, especially for non-European languages, due to dataset biases.
Highlights the gap between academic benchmarks and real-world effectiveness, emphasizing the need for human oversight in moderation.
Identifies challenges such as distinguishing hate from offensive speech and mismatches in targeted groups between datasets and real-world cases.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.15462</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</title><link>https://arxiv.org/abs/2410.19133</link><description>Introduces HyPER, a system that routes annotation tasks between humans and language models to optimize feedback quality and cost.
Formulates the routing as an optimization problem, using a performance prediction model to select the best mix of human and synthetic (LM) annotations.
Demonstrates that a hybrid approach outperforms using only human or only LM feedback on multiple benchmarks.
Finds that prompts with moderate safety concerns or complexity particularly benefit from human feedback, highlighting implications for safety-critical annotation.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.19133</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving Model Factuality with Fine-grained Critique-based Evaluator</title><link>https://arxiv.org/abs/2410.18359</link><description>Introduces FenCE, a factuality evaluator that provides claim-level feedback to language models.
Uses data augmentation and critique-based evaluation to improve the factual accuracy of model outputs.
Demonstrates significant improvements in factuality rates for Llama2-7B-chat and Llama3-8B-chat models.
Outperforms existing factuality finetuning methods in empirical benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18359</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla</title><link>https://arxiv.org/abs/2410.13281</link><description>Introduces BanTH, the first multi-label hate speech detection dataset for transliterated Bangla, addressing a gap in low-resource language moderation.
Focuses on detecting and classifying hate speech targeting multiple demographic groups in online content.
Proposes novel transformer-based baselines and a translation-based prompting strategy for large language models (LLMs) in zero-shot settings.
Aims to enhance content moderation and understanding of hate motivation in underrepresented languages.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13281</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs</title><link>https://arxiv.org/abs/2410.08145</link><description>Investigates conflicts between visual input and internal commonsense knowledge in Multimodal Large Language Models (MLLMs).
Introduces an automated framework and benchmark to systematically evaluate vision-knowledge conflicts.
Assesses nine MLLMs, revealing a significant over-reliance on internal knowledge over visual evidence in certain scenarios.
Proposes and evaluates mitigation strategies, highlighting ongoing challenges in resolving these conflicts.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08145</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stereotype or Personalization? User Identity Biases Chatbot Recommendations</title><link>https://arxiv.org/abs/2410.05613</link><description>Investigates how large language models (LLMs) generate recommendations influenced by users' racial identities, both explicitly and implicitly.
Finds that LLMs produce racially stereotypical recommendations regardless of how user identity is revealed.
Highlights a lack of transparency in chatbot responses about the influence of user identity on recommendations.
Demonstrates that this bias and lack of transparency is consistent across multiple popular LLMs and racial groups.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.05613</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Can Language Models Reason about Individualistic Human Values and Preferences?</title><link>https://arxiv.org/abs/2410.03868</link><description>Introduces the concept of individualistic alignment for AI, emphasizing the need for AI systems to respect and reason about individual human values rather than relying solely on demographic groupings.
Presents IndieValueCatalog, a dataset based on the World Values Survey, to evaluate language models' ability to predict individual value judgments.
Finds that current frontier language models perform poorly (55%-65% accuracy) in reasoning about individualistic values, highlighting limitations in current AI alignment approaches.
Proposes the Value Inequity Index to measure partiality in language models' reasoning about global individualistic values.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03868</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond</title><link>https://arxiv.org/abs/2409.05367</link><description>Introduces STRICTA, a framework for structured, step-wise reasoning in critical text assessment tasks such as peer review and fact-checking.
Aims to improve interpretability and human-AI collaboration by modeling assessment as a graph of reasoning steps.
Applies the framework to biomedical paper assessment, collecting a dataset of expert reasoning steps and evaluating LLMs' ability to support these workflows.
Facilitates the study of collaborative expert-AI reasoning, which has implications for responsible and trustworthy AI deployment in high-stakes domains.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.05367</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Awes, Laws, and Flaws From Today's LLM Research</title><link>https://arxiv.org/abs/2408.15409</link><description>Critically examines the scientific methodology in over 2,000 LLM research papers from 2020-2024.
Identifies trends such as a decline in ethics disclaimers and increased reliance on LLMs for evaluation.
Highlights issues with claims of emergent behavior and lack of human evaluation in LLM research.
Provides recommendations to improve research rigor and addresses the balance between research speed and quality.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.15409</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models</title><link>https://arxiv.org/abs/2408.13533</link><description>Analyzes the impact of different types of noise in Retrieval-Augmented Generation (RAG) systems for large language models.
Introduces NoiserBench, a benchmark for evaluating LLM robustness to seven linguistically-defined noise types across multiple datasets and reasoning tasks.
Finds that some noise types can actually benefit LLM performance, while others are detrimental, challenging the assumption that all noise is harmful.
Provides insights for developing more robust and adaptable RAG systems to mitigate hallucinations and improve reliability in real-world scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.13533</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</title><link>https://arxiv.org/abs/2406.18403</link><description>Investigates the reliability of using LLMs as substitutes for human judges in NLP evaluation tasks.
Introduces JUDGE-BENCH, a benchmark of 20 NLP datasets with human annotations for comprehensive evaluation.
Finds substantial variance in LLM evaluation reliability across tasks, models, and data types.
Highlights the need for careful validation of LLMs before deploying them as evaluators, due to reproducibility and validity concerns.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.18403</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information</title><link>https://arxiv.org/abs/2406.11093</link><description>Proposes RAEmoLLM, a retrieval-augmented LLM framework for cross-domain misinformation detection using affective (emotional and sentiment) information.
Introduces a novel approach that leverages emotional embeddings to improve the retrieval of relevant examples for in-context learning.
Demonstrates significant performance improvements over existing few-shot methods on multiple misinformation detection benchmarks.
Addresses the challenge of generalizing misinformation detection across domains without the need for fine-tuning.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.11093</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination</title><link>https://arxiv.org/abs/2506.01902</link><description>Proposes a novel pre-training method for biomedical vision-language models using perturbed report discrimination.
Introduces text perturbation techniques to disrupt semantic structure while maintaining word content, challenging the model to distinguish original from perturbed reports.
Enhances model robustness and semantic understanding by contrasting attention-weighted image sub-regions and sub-words.
Demonstrates improved performance and robustness on multiple downstream biomedical tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01902</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR</title><link>https://arxiv.org/abs/2506.01877</link><description>Addresses the challenge of maintaining robust dense retrievers as document corpora evolve and shift from the original training distribution.
Proposes a novel task: predicting whether a corpus is out-of-distribution (OOD) relative to a dense retriever before indexing.
Introduces GradNormIR, an unsupervised method using gradient norms to detect OOD corpora and guide retriever updates.
Demonstrates that timely updates based on OOD detection improve retrieval robustness and efficiency.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01877</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution</title><link>https://arxiv.org/abs/2506.01055</link><description>Investigates how prompt injection attacks can cause LLM agents to leak personal data during task execution.
Develops data flow-based attacks and integrates them into the AgentDojo benchmark for agentic security.
Finds that LLMs show significant drops in utility and notable attack success rates, with some defenses reducing but not eliminating leakage.
Highlights that while highly sensitive data like passwords are less likely to be leaked, other personal data remains vulnerable, especially in tasks resembling exfiltration attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01055</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models</title><link>https://arxiv.org/abs/2506.00805</link><description>Proposes a novel Hierarchical Self-Contrastive Rewarding (HSCR) method to improve alignment in Medical Vision-Language Models (Med-VLMs).
Addresses the challenge of modality misalignment, which can lead to untrustworthy or unsafe responses in clinical settings.
Introduces a cost-effective approach for generating high-quality preference data and optimizing model alignment using nuanced, context-aware preferences.
Demonstrates improved trustworthiness and alignment in Med-VLMs across multiple medical tasks with minimal training data.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00805</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval</title><link>https://arxiv.org/abs/2506.00363</link><description>Proposes BMEmbed, a method to adapt general-purpose text embedding models for use with private, domain-specific datasets.
Uses keyword-based retrieval (BM25) to generate supervisory signals for model adaptation.
Demonstrates improved retrieval performance across various domains and datasets.
Discusses how BM25-based signals enhance embedding alignment and uniformity, aiding adaptation to specialized data.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00363</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers</title><link>https://arxiv.org/abs/2506.00054</link><description>Provides a comprehensive survey of Retrieval-Augmented Generation (RAG) systems, focusing on architectures, enhancements, and robustness.
Analyzes challenges related to retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs.
Reviews evaluation frameworks and benchmarks, including robustness testing and privacy-preserving retrieval mechanisms.
Identifies open challenges and future research directions in adaptive retrieval, real-time integration, structured reasoning, and privacy.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00054</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RewardBench 2: Advancing Reward Model Evaluation</title><link>https://arxiv.org/abs/2506.01937</link><description>Introduces RewardBench 2, a new benchmark for evaluating reward models used in language model post-training.
Focuses on multi-skill evaluation, including domains like instruction following, reasoning, and safety.
Highlights the correlation between benchmark performance and downstream tasks, including RLHF and inference-time algorithms.
Emphasizes the use of new human-generated prompts for more rigorous and representative evaluation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01937</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions</title><link>https://arxiv.org/abs/2506.01859</link><description>Introduces CONFETTI, a benchmark for evaluating large language models' (LLMs) function-calling abilities in complex conversational scenarios.
Benchmarks LLMs using 109 human-simulated conversations with 313 user turns and 86 APIs, focusing on conversational complexities like follow-ups, ambiguous goals, and chained function calls.
Analyzes LLM performance in handling long conversations, multiple APIs, and chained function-calling, revealing significant limitations in current models.
Provides detailed evaluation results for several state-of-the-art LLMs, highlighting areas for improvement in robustness and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01859</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high</title><link>https://arxiv.org/abs/2506.01814</link><description>Systematically compares ideological and propaganda bias in two large language models (DeepSeek-R1 and ChatGPT o3-mini-high) with different geopolitical alignments.
Develops a novel evaluation corpus and hybrid scoring pipeline to assess Chinese-state propaganda and anti-U.S. sentiment across languages.
Finds significant, language-dependent bias in DeepSeek-R1, especially in Simplified Chinese, with evidence of bias even in non-political content.
Highlights the risks of LLMs amplifying state-aligned narratives and the challenges of ensuring neutrality in multilingual, geopolitically sensitive contexts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01814</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives</title><link>https://arxiv.org/abs/2506.01807</link><description>Analyzes the use of propaganda and information dissemination on Twitter during the Russo-Ukrainian War using NLP and machine learning.
Examines differences in narrative strategies between Russian propaganda accounts and Western trusted accounts.
Identifies coordinated efforts and disinformation campaigns through clustering and sentiment analysis.
Highlights the role of AI-driven techniques in understanding information warfare and social media influence in conflicts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01807</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Human-Centric Evaluation for Foundation Models</title><link>https://arxiv.org/abs/2506.01793</link><description>Proposes a Human-Centric Evaluation (HCE) framework for assessing foundation models based on subjective human experiences.
Focuses on problem-solving ability, information quality, and interaction experience as core evaluation dimensions.
Conducts large-scale participant-driven evaluations involving collaboration between humans and various LLMs.
Provides a dataset and methodology that could inform safer and more responsible AI development by aligning model evaluation with human values and experiences.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01793</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs</title><link>https://arxiv.org/abs/2506.01734</link><description>Investigates how Large Language Models (LLMs) develop biases in numerical outputs due to statistical patterns (Benford's Law) in training data.
Demonstrates that LLMs consistently exhibit digit bias in numerical reasoning tasks, leading to systematic errors or 'numerical hallucinations.'
Identifies specific neurons responsible for this bias and shows that pruning them can mitigate the issue.
Provides insights into diagnosing and mitigating symbolic failure modes in LLMs, which is relevant for improving model robustness and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01734</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training</title><link>https://arxiv.org/abs/2506.01732</link><description>Introduces Common Corpus, a large open dataset for LLM pre-training, focusing on ethical and legally compliant data.
Addresses issues of copyright and proprietary content in LLM training data, aiming to support compliance with AI legislation and data security regulations.
Describes the dataset's diversity in languages, domains, and inclusion of code, as well as its detailed curation and provenance.
Highlights the dataset's adoption by industry leaders and its potential as critical infrastructure for responsible AI research.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01732</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning</title><link>https://arxiv.org/abs/2506.01710</link><description>Introduces Reasoning-Table, a reinforcement learning-based approach for table reasoning tasks such as question answering, fact verification, and text-to-SQL.
Addresses limitations of supervised fine-tuning by improving generalization and robustness through RL and tailored reward design.
Demonstrates state-of-the-art performance and enhanced robustness compared to larger proprietary models.
Highlights improvements in model robustness and generalization, which are important for safe and reliable AI deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01710</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fairness Dynamics During Training</title><link>https://arxiv.org/abs/2506.01709</link><description>Investigates how fairness and bias evolve during the training of large language models (LLMs).
Introduces new metrics (Average Rank and Jensen-Shannon Divergence by Parts) to holistically evaluate fairness dynamics.
Finds that biases can emerge suddenly and are not always aligned with standard performance metrics.
Demonstrates that early stopping can significantly improve fairness with minimal loss in accuracy, and that larger models may exhibit more bias.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01709</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection</title><link>https://arxiv.org/abs/2506.01702</link><description>Presents a robust fine-tuning approach for smaller LLMs to detect AI-generated text.
Addresses the challenge of detecting machine-generated text, which can be used for misuse such as plagiarism or disinformation.
Focuses on improving robustness to out-of-distribution data in AI-generated text detection.
Demonstrates strong performance in both binary and multiclass detection tasks, including human-AI collaboration scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01702</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MVAN: Multi-View Attention Networks for Fake News Detection on Social Media</title><link>https://arxiv.org/abs/2506.01627</link><description>Proposes a novel Multi-View Attention Network (MVAN) for detecting fake news on social media using only short source tweets and retweet user data.
Incorporates both text semantic attention and propagation structure attention to capture key information from tweet content and user propagation patterns.
The model provides explanations by identifying key clue words and suspicious users involved in the spread of fake news.
Experimental results show that MVAN outperforms existing methods in accuracy and explainability on real-world datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01627</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings</title><link>https://arxiv.org/abs/2506.01587</link><description>Addresses the challenge of misinformation and fake news detection in low-resource languages, specifically Urdu.
Introduces the first publicly available, expert-verified, domain-independent Urdu fake news detection dataset.
Evaluates multiple large language models (LLMs) for fake news detection and proposes a unified LLM model that outperforms others.
Highlights the importance of reliable datasets and model evaluation for improving misinformation detection in under-resourced linguistic settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01587</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Engineering Large Language Models' Forecasting Capabilities</title><link>https://arxiv.org/abs/2506.01578</link><description>Investigates whether prompt engineering can significantly improve large language models' (LLMs) forecasting accuracy.
Finds that small prompt modifications rarely boost forecasting accuracy beyond a minimal baseline.
Some prompt strategies, such as encouraging Bayesian reasoning, can actually reduce accuracy.
Suggests that more robust or specialized techniques are needed for substantial improvements in AI forecasting.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01578</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models</title><link>https://arxiv.org/abs/2506.01495</link><description>Introduces a large-scale Chinese Value Corpus (CVC) to support value alignment in large language models (LLMs) with a focus on Chinese cultural values.
Addresses limitations of Western-centric value alignment frameworks and provides a hierarchical, rule-based approach for scenario generation and evaluation.
Demonstrates improved value boundary recognition and content diversity in LLMs using CVC-guided scenarios, with strong alignment to both LLM and human annotator preferences.
Provides a culturally-adaptive benchmarking framework for evaluating and aligning LLMs with Chinese ethical norms, enhancing AI safety and responsible development.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01495</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification</title><link>https://arxiv.org/abs/2506.01484</link><description>Introduces PARADEHATE, a large-scale parallel dataset for hate speech detoxification, created using an LLM-in-the-loop pipeline.
Demonstrates that LLMs can effectively replace human annotators for generating detoxified text, maintaining quality and scalability.
Evaluates various baseline models (e.g., BART) fine-tuned on the new dataset, showing improvements in style accuracy, content preservation, and fluency.
Addresses the challenge of creating high-quality datasets for hate speech detoxification, which is important for safer AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01484</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations</title><link>https://arxiv.org/abs/2506.01367</link><description>Proposes MMD-Flagger, a method to detect hallucinations in large language model outputs using Maximum Mean Discrepancy.
Tracks distributional differences in generated content across temperature parameters to flag hallucinated content.
Demonstrates empirical effectiveness on machine translation datasets, outperforming existing methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01367</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning</title><link>https://arxiv.org/abs/2506.01347</link><description>Investigates the impact of negative reinforcement (penalizing incorrect responses) in training large language models (LLMs) for mathematical reasoning tasks.
Finds that training with only negative samples can be highly effective, sometimes surpassing traditional reinforcement learning methods.
Analyzes how negative sample reinforcement (NSR) suppresses incorrect generations and redistributes probability mass, refining the model's knowledge.
Proposes a new RL objective that upweights NSR, leading to improved performance on several math benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01347</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents</title><link>https://arxiv.org/abs/2506.01344</link><description>Introduces a neurosymbolic agent (FlowPathAgent) for fine-grained attribution in flowchart-based LLM responses.
Addresses the problem of hallucinations and unreliable reasoning in LLMs when interpreting complex visual-textual diagrams.
Proposes a new benchmark (FlowExplainBench) for evaluating flowchart attribution and explainability.
Improves explainability and verifiability of LLM predictions in critical domains by linking responses to specific flowchart components.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01344</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models</title><link>https://arxiv.org/abs/2506.01334</link><description>Proposes a dynamic, agent-based approach using LLMs to optimize the number of interpretable concepts in image classification.
Introduces Conditional Concept Bottleneck Models (CoCoBMs) to improve concept scoring and allow LLMs to edit concept scores.
Demonstrates improved classification accuracy and interpretability across multiple datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01334</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events</title><link>https://arxiv.org/abs/2506.01253</link><description>Introduces a method for identifying latent conditions that influence outcomes in complex events using condition-based reasoning.
Combines and augments datasets to analyze how conditions affect reasoning tasks for language models.
Evaluates open and closed LLMs (including GPT-4o) on their ability to generate and identify outcome-variant conditions, noting differences in caution and performance.
Finds that models' ability to reason about conditions impacts their robustness when context is missing.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01253</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Trick or Neat: Adversarial Ambiguity and Language Model Evaluation</title><link>https://arxiv.org/abs/2506.01205</link><description>Introduces an adversarial ambiguity dataset to test language models' sensitivity to various types of ambiguity.
Evaluates language models using adversarial variations such as word-order changes and synonym replacements.
Finds that direct prompting is ineffective at detecting ambiguity, but linear probes on model representations are highly accurate.
Provides insights into how language models encode ambiguity and the robustness of their evaluation methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01205</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CoBRA: Quantifying Strategic Language Use and LLM Pragmatics</title><link>https://arxiv.org/abs/2506.01195</link><description>Introduces CoBRA, a framework for quantifying strategic (including adversarial) language use in discourse, particularly in non-cooperative settings.
Presents new interpretable metrics (BaT, PaT, NRBaT) to assess the impact of discourse moves.
Releases CHARM, a dataset of annotated courtroom cross-examinations, to evaluate LLMs' pragmatic understanding.
Finds that LLMs have limited ability to handle strategic/adversarial language, with increased reasoning sometimes reducing performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01195</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LAQuer: Localized Attribution Queries in Content-grounded Generation</title><link>https://arxiv.org/abs/2506.01187</link><description>Introduces LAQuer, a task for fine-grained, user-directed attribution in content-grounded text generation.
Compares prompting LLMs and using their internal representations for localized attribution.
Proposes a modeling framework and benchmarks for LAQuer, evaluated on summarization and question answering tasks.
Aims to improve the usability and precision of attribution in AI-generated content.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01187</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection</title><link>https://arxiv.org/abs/2506.01104</link><description>Proposes Reinforced Unanswerability Learning (RUL), a new training paradigm for LLMs to detect unanswerable questions and generate appropriate refusal responses.
Integrates a discriminative unanswerability prediction head with the LLM's generative core, using a multi-stage learning strategy including supervised fine-tuning and RLHF.
Introduces a new dataset (ECA) with hierarchical answerability labels and ground-truth refusal responses.
Demonstrates improved trustworthiness, refusal accuracy, and helpfulness in LLM responses, as validated by both quantitative and human evaluations.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01104</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content</title><link>https://arxiv.org/abs/2506.00973</link><description>Introduces XGUARD, a benchmark and evaluation framework for assessing the severity of extremist content generated by large language models (LLMs).
Uses 3,840 real-world red teaming prompts to test LLMs across a spectrum of ideologically charged scenarios.
Proposes a graded danger level system (0-4) and the Attack Severity Curve (ASC) for nuanced safety analysis.
Evaluates multiple LLMs and defense strategies, highlighting current safety gaps and the trade-offs between robustness and expressive freedom.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00973</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness</title><link>https://arxiv.org/abs/2506.00964</link><description>Introduces the concept of sensitivity awareness (SA) for LLMs to respect access rights and data sensitivity.
Presents ACCESS DENIED INC, a benchmarking environment to evaluate how well LLMs manage sensitive information and access control.
Finds significant differences in LLM behavior regarding unauthorized data requests and legitimate queries.
Aims to improve privacy and security in AI systems used for corporate data management.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00964</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SocialEval: Evaluating Social Intelligence of Large Language Models</title><link>https://arxiv.org/abs/2506.00900</link><description>Introduces SocialEval, a benchmark for evaluating the social intelligence of large language models (LLMs).
Assesses both outcome-oriented goal achievement and process-oriented interpersonal abilities in LLMs.
Finds that LLMs lag behind humans in social intelligence, often exhibiting prosocial but suboptimal behaviors.
Analyzes LLMs' internal representations, showing functional partitions similar to those in the human brain.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00900</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning</title><link>https://arxiv.org/abs/2506.00876</link><description>Addresses the challenge of unlearning specific information (e.g., private, sensitive, or copyrighted content) from large language models.
Proposes a Selective Unlearning (SU) method that targets only critical tokens related to unwanted information, rather than indiscriminately forgetting all tokens.
Demonstrates that SU preserves the model's general utility while effectively removing targeted information.
Evaluates the approach on benchmarks and compares it to existing unlearning algorithms.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00876</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning</title><link>https://arxiv.org/abs/2506.00869</link><description>Introduces new benchmarks (VQA-Causal and VCR-Causal) to specifically evaluate causal reasoning in vision-language models (VLMs).
Finds that current VLMs perform poorly on causal reasoning tasks, often only slightly better than random guessing.
Identifies a lack of causal expressions in training data as a key limitation for VLMs' causal reasoning abilities.
Demonstrates that targeted fine-tuning with hard negative cases can improve causal reasoning without sacrificing general performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00869</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks</title><link>https://arxiv.org/abs/2506.00823</link><description>Investigates the internal representation of 'truthfulness' in large language models (LLMs), focusing on the concept of a 'truth direction'.
Finds that more capable LLMs exhibit stronger and more consistent truth directions, especially in handling logical negation.
Demonstrates that probes for truthfulness can generalize across logical transformations, question answering, and in-context learning.
Explores practical applications for improving user trust in LLM outputs by using truthfulness probes for selective question answering.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00823</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>One for All: Update Parameterized Knowledge Across Multiple Models</title><link>https://arxiv.org/abs/2506.00817</link><description>Proposes OnceEdit, a method for efficiently updating knowledge across multiple large language models (LLMs) without full retraining.
Introduces mechanisms to distinguish between edit-related and non-edit-related instances and to reduce over-reliance on a central model.
Demonstrates improved efficiency and stability in knowledge editing across diverse LLMs.
Addresses challenges in maintaining up-to-date and accurate knowledge in LLMs, which is relevant for reducing errors and hallucinations.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00817</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2506.00789</link><description>Introduces RARE, a framework and benchmark for evaluating the robustness of Retrieval-Augmented Generation (RAG) systems under real-world perturbations.
Proposes new metrics (RARE-Met) to quantify how well RAG systems handle noisy, conflicting, or rapidly changing information.
Finds that RAG systems are particularly vulnerable to document perturbations and multi-hop queries, highlighting robustness weaknesses.
Provides a large-scale, time-sensitive dataset (RARE-Set) for stress-testing RAG systems in domains like finance, economics, and policy.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00789</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons</title><link>https://arxiv.org/abs/2506.00759</link><description>Investigates privacy leakage risks in large language models (LLMs) when queries are made in languages different from the training data.
Finds that LLMs can leak personally identifiable information (PII) even in cross-lingual contexts.
Identifies and analyzes privacy-universal and language-specific privacy neurons responsible for such leakage.
Proposes mitigation by deactivating these neurons, reducing cross-lingual privacy leakage by 23.3%-31.6%.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00759</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations</title><link>https://arxiv.org/abs/2506.00748</link><description>Introduces the Translate-with-Care (TWC) dataset to evaluate gender bias and logical coherence in machine translation, especially between gendered and genderless languages.
Finds that major translation models, including GPT-4 and Google Translate, exhibit significant gender bias, often defaulting to masculine pronouns in stereotypical contexts.
Demonstrates that fine-tuning open-source models like mBART-50 on the TWC dataset can substantially reduce gender bias and improve reasoning in translations.
Highlights the importance of addressing bias and reasoning errors for more equitable and accurate AI translation systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00748</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Data Swarms: Optimizable Generation of Synthetic Evaluation Data</title><link>https://arxiv.org/abs/2506.00741</link><description>Introduces Data Swarms, an algorithm for optimizing synthetic evaluation data generation for LLMs.
Utilizes particle swarm optimization to create data generators that can produce more challenging evaluation problems.
Extends to Adversarial Swarms, where data generators and test-taker models co-evolve, leading to more robust and generalizable models.
Demonstrates improved performance over existing data generation baselines and shows generalization to unseen LLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00741</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments</title><link>https://arxiv.org/abs/2506.00739</link><description>Introduces DefenderBench, an open-source toolkit for evaluating language agents in cybersecurity tasks.
Covers offense, defense, and cybersecurity knowledge-based tasks such as network intrusion, malicious content detection, and code vulnerability analysis.
Benchmarks state-of-the-art LLMs using a standardized framework to assess their cybersecurity capabilities.
Aims to promote reproducibility, accessibility, and fair comparison in AI security research.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00739</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues</title><link>https://arxiv.org/abs/2506.00668</link><description>Introduces STREAM, a defense mechanism to protect LLMs from multi-turn dialogue attacks.
STREAM uses a human-annotated dataset to fine-tune a safety reasoning moderator that detects malicious intent in conversations.
Demonstrates significant reduction in attack success rate (ASR) while maintaining LLM performance.
Evaluates the approach across multiple LLMs and attack strategies.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00668</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances</title><link>https://arxiv.org/abs/2506.00636</link><description>Introduces ViToSA, the first dataset for detecting toxic spans in Vietnamese speech.
Proposes a pipeline combining automatic speech recognition (ASR) and toxic spans detection for audio moderation.
Demonstrates improved ASR performance and toxic content detection in Vietnamese audio.
Establishes a benchmark for future research in audio-based content moderation and online safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00636</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation</title><link>https://arxiv.org/abs/2506.00612</link><description>Proposes a knowledge graph-guided framework to generate more challenging distractors for clinical multiple-choice questions.
Aims to create clinically plausible but misleading distractors to better evaluate the reliability and robustness of large language models (LLMs) in medical contexts.
Demonstrates that the new distractors reduce the accuracy of state-of-the-art LLMs, highlighting weaknesses and potential vulnerabilities.
Contributes to the development of more robust and diagnostic benchmarks for medical AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00612</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements</title><link>https://arxiv.org/abs/2506.00608</link><description>Introduces PAKTON, an open-source multi-agent framework for automated contract review and question answering in long legal documents.
Emphasizes privacy-preserving features, making it suitable for confidential legal agreements.
Utilizes collaborative agent workflows and retrieval-augmented generation (RAG) to improve accuracy, explainability, and completeness.
Demonstrates superior performance over general-purpose and pretrained models in legal document analysis.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00608</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation</title><link>https://arxiv.org/abs/2506.00583</link><description>Investigates how emojis contribute to harmful or offensive online communication, particularly on Twitter.
Analyzes the ambiguity and contextual misuse of emojis in offensive messages.
Proposes a moderation pipeline using large language models (LLMs) to selectively replace harmful emojis while preserving message intent.
Provides human evaluation results showing reduced perceived offensiveness without loss of meaning.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00583</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.00509</link><description>Introduces MisinfoTask, a dataset for evaluating robustness of Multi-Agent Systems (MASs) against misinformation injection.
Proposes ARGUS, a two-stage, training-free defense framework using goal-aware reasoning to rectify misinformation in MASs.
Demonstrates ARGUS's effectiveness in reducing misinformation toxicity and improving task success rates under various injection attacks.
Addresses the increased attack surface and security vulnerabilities in LLM-based MASs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00509</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization</title><link>https://arxiv.org/abs/2506.00448</link><description>Investigates hallucinations (factual inaccuracies) in LLM-generated medical text summarization, which can pose risks to patient care.
Evaluates the effectiveness of existing hallucination detection methods in the clinical domain and finds general-domain detectors lacking.
Introduces new datasets (fact-controlled and natural hallucinations) and develops explainable, fact-based hallucination detection approaches.
Provides specialized metrics and expert-annotated datasets to improve the faithfulness and safety of clinical summarization systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00448</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL</title><link>https://arxiv.org/abs/2506.00391</link><description>Introduces SHARE, a hierarchical action correction assistant using Small Language Models (SLMs) to improve text-to-SQL self-correction.
Addresses computational inefficiency and lack of reasoning transparency in current LLM-based self-correction methods.
Demonstrates robust performance and data efficiency, which is beneficial for privacy-sensitive applications.
Highlights potential for improved error detection and correction in AI systems handling sensitive data.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00391</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation</title><link>https://arxiv.org/abs/2506.00319</link><description>Introduces SkillVerse, a framework for granular, skill-specific evaluation of large language models (LLMs) using tree-structured analysis.
Uses LLMs as judges to critique and hierarchically organize model responses, enabling detailed diagnosis of model strengths and weaknesses.
Demonstrates improvements in in-context learning and predictive identification of model weaknesses using the framework.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00319</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MultiHoax: A Dataset of Multi-hop False-Premise Questions</title><link>https://arxiv.org/abs/2506.00264</link><description>Introduces MultiHoax, a benchmark dataset for evaluating LLMs' ability to detect false premises in multi-hop reasoning tasks.
Highlights the importance of critical reasoning and false-premise detection for reliable AI outputs in high-stakes domains.
Finds that state-of-the-art LLMs struggle with multi-hop false-premise questions, indicating a gap in current model robustness.
Dataset covers diverse knowledge categories and regions, supporting broad evaluation of LLM reasoning capabilities.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00264</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection</title><link>https://arxiv.org/abs/2506.00256</link><description>Investigates how disability disclosure affects fairness and bias in LLM-driven hiring processes.
Finds that LLMs consistently favor candidates who disclose no disability over those who disclose a disability or choose not to disclose.
Highlights potential risks of bias and discrimination when LLMs are used for candidate selection in employment contexts.
Addresses the implications for responsible AI deployment and the need for fairness in automated decision-making systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00256</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering</title><link>https://arxiv.org/abs/2506.00232</link><description>Proposes ComposeRAG, a modular and composable framework for Retrieval-Augmented Generation (RAG) systems, focusing on multi-hop question answering.
Decomposes RAG pipelines into independent modules (e.g., Question Decomposition, Answer Verification) for improved interpretability and targeted improvements.
Incorporates a self-reflection mechanism to enhance robustness by iteratively refining steps upon verification failure.
Demonstrates improved accuracy, grounding fidelity, and reduction of ungrounded answers compared to existing baselines.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00232</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards</title><link>https://arxiv.org/abs/2506.00103</link><description>Proposes a new RLVR-based training paradigm to address the challenge of reward modeling in non-verifiable tasks like creative writing.
Introduces a pairwise Generative Reward Model (GenRM) and Bootstrapped Relative Policy Optimization (BRPO) to transform subjective assessments into verifiable rewards.
Demonstrates improved robustness against reward hacking (e.g., over-explanation, length bias) compared to scalar reward models.
Suggests a unified approach to reward modeling that could enhance the safety and reliability of LLMs across diverse language tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00103</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists</title><link>https://arxiv.org/abs/2506.00042</link><description>Proposes a hierarchical error checklist framework (HiTEC) to systematically diagnose and mitigate tool-calling errors in large language models (LLMs).
Introduces both global and local error checklists to address cross-tool and tool-specific issues.
Presents two deployment strategies: HiTEC-In Context Learning (ICL) and HiTEC-Kahneman-Tversky Optimization (KTO) for improving parameter handling and fine-tuning.
Demonstrates significant improvements in parameter-filling accuracy and tool-calling success rates across multiple datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00042</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM</title><link>https://arxiv.org/abs/2505.24238</link><description>Introduces a new benchmark to isolate and assess reasoning-induced hallucinations in multimodal large language models (MLLMs).
Proposes multi-granular evaluation metrics to quantify hallucinations, including accuracy, factuality, and a hallucination score.
Analyzes how model scale, data scale, and training stages affect different types of hallucinations, and identifies persistent challenges in spatial reasoning.
Presents a new method combining curriculum reinforcement fine-tuning and collaborative hint inference to reduce logical hallucinations.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24238</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective</title><link>https://arxiv.org/abs/2505.12185</link><description>Introduces EVALOOP, a novel framework to assess the robustness of LLMs in programming tasks using a self-consistency feedback loop.
Highlights limitations of current adversarial attack-based robustness evaluations and proposes a unified, attack-free metric.
Evaluates 16 prominent LLMs and finds significant drops in performance over repeated self-consistency loops, revealing robustness gaps.
Finds that initial performance does not always correlate with robustness, indicating the need for more comprehensive evaluation metrics.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12185</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Measurement to Meaning: A Validity-Centered Framework for AI Evaluation</title><link>https://arxiv.org/abs/2505.10573</link><description>Proposes a validity-centered framework for evaluating AI systems, emphasizing rigorous and meaningful assessment.
Addresses the gap between narrow benchmark performance and broader claims about AI capabilities.
Leverages psychometric principles to improve the design and interpretation of AI evaluations.
Includes case studies on vision and language models to demonstrate the framework's practical application.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10573</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories</title><link>https://arxiv.org/abs/2504.16449</link><description>Provides a comprehensive survey of malicious URL detection techniques, including traditional and advanced AI-based methods (e.g., Transformers, LLMs, GNNs).
Introduces a modality-based taxonomy for categorizing detection approaches based on data types (URL, HTML, Visual, etc.).
Curates and analyzes publicly available datasets and open-source code repositories to facilitate benchmarking and reproducibility.
Discusses emerging challenges, design principles for real-world deployment, and future research directions in malicious URL detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16449</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Collaborative Anti-Money Laundering Among Financial Institutions</title><link>https://arxiv.org/abs/2502.19952</link><description>Proposes a collaborative algorithm for anti-money laundering (AML) across multiple financial institutions while preserving data privacy and security.
Addresses the challenge of detecting money laundering activities that span multiple institutions, overcoming data sharing barriers due to privacy and regulatory concerns.
Introduces and utilizes a large real-world dataset (Alipay-ECB) for evaluation, demonstrating effectiveness and efficiency in identifying cross-institution money laundering.
Highlights the use of graph-based machine learning methods for AML in a privacy-preserving, decentralized context.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19952</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models</title><link>https://arxiv.org/abs/2502.19765</link><description>Introduces EdiText, a controllable text editing method using diffusion language models.
Allows for both coarse and fine-grained modifications of text attributes, such as toxicity and sentiment.
Demonstrates robust control over text editing, which can be applied to tasks like toxicity control.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19765</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing</title><link>https://arxiv.org/abs/2501.07985</link><description>Presents an experimental demonstration of a hybrid reinforcement learning (RL) algorithm (CHEQ) for safe robotic polishing.
Addresses the challenge of unsafe exploration in RL by combining classical control with RL to improve safety and data efficiency.
Evaluates the algorithm on real hardware, demonstrating adherence to safety constraints during training and operation.
Highlights the potential for adaptive hybrid RL to enable safer deployment of RL in real-world, contact-rich robotic tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.07985</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Modality Generalization: A Benchmark and Prospective Analysis</title><link>https://arxiv.org/abs/2412.18277</link><description>Introduces the concept of Modality Generalization (MG), focusing on enabling AI models to handle unseen data modalities.
Defines two MG scenarios: Weak MG (where unseen modalities can be mapped to a joint embedding space) and Strong MG (where such mappings do not exist).
Proposes a benchmark and adapts existing multi-modal generalization methods to evaluate model robustness to unseen modalities.
Highlights the limitations of current approaches and suggests future research directions for more robust and adaptable multi-modal AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18277</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning</title><link>https://arxiv.org/abs/2410.21119</link><description>Proposes FedHydra, a unified framework for one-shot federated learning (OSFL) that addresses both model and data heterogeneity.
FedHydra introduces a two-stage learning mechanism with model stratification and heterogeneity-aware stratified aggregation.
The approach is data-free and aims to reduce communication costs and privacy leakage risks compared to traditional federated learning.
Experimental results show improved performance over state-of-the-art OSFL methods in both homogeneous and heterogeneous settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21119</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees</title><link>https://arxiv.org/abs/2410.08934</link><description>Analyzes the trade-off between statistical accuracy and communication cost in personalized federated learning (PFL).
Provides a quantitative characterization of how the degree of personalization affects sample and algorithmic efficiency.
Establishes minimax optimality for statistical accuracy in a common PFL formulation.
Results are validated on both synthetic and real-world datasets, including non-convex settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08934</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title><link>https://arxiv.org/abs/2410.03026</link><description>Introduces 'context influence', a metric based on differential privacy to estimate privacy leakage from contextual knowledge in language models.
Demonstrates that privacy leakage can occur when contextual knowledge is out-of-distribution compared to the model's parametric knowledge.
Evaluates how factors like model size, context size, and generation position affect privacy leakage.
Provides practical insights for practitioners on assessing privacy risks when augmenting language models with external context.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03026</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks</title><link>https://arxiv.org/abs/2409.04459</link><description>Analyzes vulnerabilities in Embeddings-as-a-Service (EaaS) to imitation attacks and watermark removal via paraphrasing.
Demonstrates that existing watermarking techniques for EaaS can be circumvented by attackers using paraphrasing.
Proposes a new watermarking method using linear transformation of embeddings, which is shown to be robust against paraphrasing attacks.
Addresses both empirical and theoretical aspects of watermark robustness in AI services.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.04459</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Investigating Privacy Leakage in Dimensionality Reduction Methods via Reconstruction Attack</title><link>https://arxiv.org/abs/2408.17151</link><description>Investigates privacy leakage in popular dimensionality reduction methods via a machine learning-based reconstruction attack.
Develops a neural network to reconstruct high-dimensional data from low-dimensional embeddings, evaluating six common techniques (PCA, SRP, MDS, Isomap, t-SNE, UMAP).
Finds deterministic methods (PCA, Isomap) are more vulnerable to reconstruction attacks, while methods with random initialization are more robust.
Assesses the effectiveness of additive noise as a mitigation strategy, showing it can significantly reduce reconstruction quality for some methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.17151</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ANVIL: Anomaly-based Vulnerability Identification without Labelled Training Data</title><link>https://arxiv.org/abs/2408.16028</link><description>Introduces ANVIL, an anomaly-based approach to vulnerability detection in code using LLMs without requiring labeled training data.
Reframes vulnerability detection as anomaly detection, leveraging LLMs' ability to identify rare, anomalous code patterns.
Demonstrates that ANVIL outperforms state-of-the-art supervised detectors on benchmark datasets.
Shows practical utility by integrating ANVIL with fuzzers to discover previously unknown vulnerabilities.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.16028</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PADetBench: Towards Benchmarking Physical Attacks against Object Detection</title><link>https://arxiv.org/abs/2408.09181</link><description>Introduces PADetBench, a benchmark for evaluating physical attacks against object detection systems using realistic simulation.
Addresses challenges in conducting physical adversarial experiments, such as time, labor, and lack of reproducibility.
Benchmarks 20 physical attack methods across 48 object detectors, with comprehensive evaluation metrics and controlled physical dynamics.
Provides in-depth analysis of attack performance and robustness, offering insights for future research in physical adversarial robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2408.09181</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robust Adaptation of Foundation Models with Black-Box Visual Prompting</title><link>https://arxiv.org/abs/2407.17491</link><description>Proposes BlackVIP, a method for adapting large pre-trained models (PTMs) in black-box settings using visual prompting.
Addresses challenges where model parameters are inaccessible and memory resources are limited.
Introduces a gradient estimation technique (SPSA-GC) and a more efficient variant (BlackVIP-SE) for robust adaptation.
Provides theoretical and empirical analysis connecting visual prompting to certified robustness, suggesting improved model robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2407.17491</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs</title><link>https://arxiv.org/abs/2404.10304</link><description>Proposes TrickCatcher, an LLM-powered system for generating test cases to detect subtle bugs in programs that pass existing test suites.
Uses LLMs to generate program variants and input generators based on program specifications.
Evaluates the approach on datasets containing both human-written and AI-generated plausible programs with tricky bugs, showing significant improvements over baselines.
Focuses on improving software reliability by leveraging AI for automated bug detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2404.10304</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Certified Robustness to Clean-Label Poisoning Using Diffusion Denoising</title><link>https://arxiv.org/abs/2403.11981</link><description>Proposes a certified defense mechanism against clean-label poisoning attacks using diffusion denoising models.
Demonstrates significant reduction in attack success rates (0-16%) with minimal impact on model accuracy.
Compares the proposed defense to existing methods, showing superior performance in both robustness and utility.
Highlights the importance of developing stronger clean-label attacks and using this defense as a baseline.</description><guid isPermaLink="false">https://arxiv.org/abs/2403.11981</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You</title><link>https://arxiv.org/abs/2401.16092</link><description>Investigates gender bias in multilingual text-to-image (T2I) generation models.
Introduces a new benchmark (MAGBIG) for evaluating gender bias across languages in T2I models.
Finds that prompt engineering strategies have limited effectiveness in mitigating gender bias and can harm text-image alignment.
Highlights the need for further research into bias mitigation and steerability in multilingual generative models.</description><guid isPermaLink="false">https://arxiv.org/abs/2401.16092</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On Meta-Prompting</title><link>https://arxiv.org/abs/2312.06562</link><description>Introduces a theoretical framework for understanding in-context learning (ICL) and meta-prompting in large language models (LLMs).
Analyzes the properties and behaviors of LLMs when interacting with users through prompts and meta-prompts.
Provides formal results on task agnosticity and equivalence of meta-prompting approaches.
Argues that meta-prompting is more effective than basic prompting for generating desirable outputs.</description><guid isPermaLink="false">https://arxiv.org/abs/2312.06562</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Computational Approach to Improving Fairness in K-means Clustering</title><link>https://arxiv.org/abs/2505.22984</link><description>Addresses fairness concerns in K-means clustering, specifically regarding disproportionate representation of sensitive subpopulations.
Proposes a two-stage optimization approach: initial clustering followed by targeted adjustment of cluster memberships to improve fairness.
Introduces two efficient algorithms for identifying and adjusting data points that contribute most to fairness issues.
Demonstrates substantial fairness improvements on benchmark datasets with minimal impact on clustering quality.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22984</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title><link>https://arxiv.org/abs/2505.19433</link><description>Introduces ACBench, a benchmark to evaluate how compression affects agentic capabilities of LLMs, such as workflow generation, tool use, and long-context understanding.
Analyzes the impact of quantization and pruning techniques on various LLMs, revealing tradeoffs between efficiency and agentic performance.
Finds that while some agentic abilities are preserved under compression, real-world application accuracy can significantly degrade.
Provides new metrics and actionable insights for optimizing LLM compression in scenarios where agentic capabilities are critical.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19433</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Finding Counterfactual Evidences for Node Classification</title><link>https://arxiv.org/abs/2505.11396</link><description>Introduces methods to find counterfactual evidences in graph neural network (GNN) node classification tasks.
Defines counterfactual evidence as pairs of similar nodes classified differently by the GNN, highlighting potential fairness or interpretability issues.
Proposes efficient algorithms and indexing solutions to identify such counterfactuals using both node features and structural information.
Demonstrates that counterfactual evidences can be used to improve fairness and accuracy in GNNs.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11396</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</title><link>https://arxiv.org/abs/2505.02515</link><description>Proposes FedSDAF, a framework for federated domain generalization that leverages source domain-aware features.
Introduces mechanisms for preserving domain-invariant features and integrating source domain-specific knowledge using multihead self-attention.
Implements bidirectional knowledge distillation to enable knowledge sharing among clients while maintaining privacy.
Demonstrates significant improvements in model generalization across multiple benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02515</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability</title><link>https://arxiv.org/abs/2503.09532</link><description>Introduces SAEBench, a comprehensive benchmark suite for evaluating sparse autoencoders (SAEs) used in language model interpretability.
Measures SAE performance across eight diverse metrics, including interpretability, feature disentanglement, and practical applications like unlearning.
Reveals that improvements in unsupervised proxy metrics do not always correlate with better practical performance, highlighting the need for more robust evaluation.
Provides an open-source suite of over 200 SAEs and an interactive interface for researchers to compare architectures and training methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09532</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging Randomness in Model and Data Partitioning for Privacy Amplification</title><link>https://arxiv.org/abs/2503.03043</link><description>Investigates how randomness in training (data and model partitioning) can be used to amplify privacy in machine learning.
Applies the framework to federated learning, showing that model parallelism and techniques like model splitting or dropout can enhance privacy.
Introduces Balanced Iteration Subsampling, a new data partitioning method that provides stronger privacy amplification than traditional Poisson sampling.
Demonstrates that structured randomness in training can systematically improve privacy guarantees.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03043</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training</title><link>https://arxiv.org/abs/2502.19726</link><description>Addresses privacy risks in large language models (LLMs) by mitigating membership inference attacks (MIAs).
Proposes a novel, lightweight defense mechanism that leverages token-specific characteristics during training.
Introduces a dual-purpose token-level loss to balance model utility and privacy.
Demonstrates improved privacy protection and language modeling performance across multiple LLM architectures.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19726</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title><link>https://arxiv.org/abs/2502.15798</link><description>Analyzes the shortcomings of Label Smoothing (LS) in neural networks, particularly its tendency to induce overconfidence in misclassified samples and cause representation collapse.
Proposes Max Suppression (MaxSup), a new regularization technique that penalizes the top-1 logit to address overconfidence and preserve intra-class diversity.
Demonstrates through experiments that MaxSup improves robustness and generalization in image classification tasks compared to traditional LS.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15798</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy amplification by random allocation</title><link>https://arxiv.org/abs/2502.08202</link><description>Analyzes privacy amplification properties of a random allocation sampling scheme in the context of differential privacy.
Provides the first theoretical guarantees and efficient numerical estimation algorithms for this sampling scheme.
Demonstrates improved and efficiently-computable privacy bounds compared to previous methods, particularly for Gaussian noise addition.
Addresses practical challenges in privacy analysis for high-dimensional private aggregation and differentially private optimization.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08202</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Humans Coexist, So Must Embodied Artificial Agents</title><link>https://arxiv.org/abs/2502.04809</link><description>Introduces the concept of 'coexistence' for embodied artificial agents as essential for safe and effective long-term human-AI interaction.
Highlights the limitations of current embodied agents in adapting to dynamic, real-world environments compared to humans.
Draws on interdisciplinary insights from biology and design theory to inform the development of coexisting agents.
Proposes research directions for AI to ensure embodied agents can safely and meaningfully interact with humans in shared environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04809</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking</title><link>https://arxiv.org/abs/2501.19358</link><description>Identifies the 'Energy Loss Phenomenon' in RLHF and links it to reward hacking in large language models.
Shows that excessive energy loss in the final layer of LLMs correlates with reward hacking and reduced contextual relevance.
Proposes a new algorithm, Energy loss-aware PPO (EPPO), to penalize excessive energy loss and mitigate reward hacking.
Provides theoretical analysis and empirical results demonstrating EPPO's effectiveness in improving RLHF robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19358</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TabFSBench: Tabular Benchmark for Feature Shifts in Open Environments</title><link>https://arxiv.org/abs/2501.18935</link><description>Introduces TabFSBench, a benchmark for evaluating the impact of feature shifts in tabular data on machine learning models.
Analyzes how different types of feature shifts affect the performance of various tabular models, including large language models (LLMs).
Finds that most tabular models struggle with feature shifts, and that the importance of shifted features is linearly related to performance degradation.
Highlights the need for future research on robustness to feature shifts in open environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18935</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Decentralized Low-Rank Fine-Tuning of Large Language Models</title><link>https://arxiv.org/abs/2501.15361</link><description>Proposes Dec-LoRA, a decentralized fine-tuning algorithm for large language models using Low-Rank Adaptation (LoRA).
Addresses privacy and scalability concerns by eliminating the need for centralized data aggregation in federated learning.
Demonstrates comparable performance to centralized approaches under various conditions, including data heterogeneity and quantization.
Provides theoretical convergence guarantees for the proposed algorithm.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15361</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distributionally Robust Policy Learning under Concept Drifts</title><link>https://arxiv.org/abs/2412.14297</link><description>Proposes a new approach for robust policy learning under concept drift, focusing on changes in the conditional relationship between outcome and covariate.
Introduces a doubly-robust estimator for evaluating worst-case policy performance under perturbed conditional distributions.
Provides theoretical guarantees for the estimator and learning algorithm, including asymptotic normality and optimal sub-optimality gap rates.
Demonstrates empirical improvements over existing robust policy learning benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.14297</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning</title><link>https://arxiv.org/abs/2412.08559</link><description>Examines privacy risks for minority populations in the context of unlearning data from large language models (LLMs).
Finds that standard unlearning evaluation methods may underestimate privacy leakage for minority or outlier data subsets.
Introduces a minority-aware evaluation framework to better assess privacy risks and unlearning efficacy.
Demonstrates experimentally that minority data (e.g., PII canaries) suffer significantly higher privacy leakage across various unlearning methods and LLM scales.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08559</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Generalization of Handwritten Text Recognition Models</title><link>https://arxiv.org/abs/2411.17332</link><description>Investigates the generalization capabilities of Handwritten Text Recognition (HTR) models to out-of-distribution (OOD) data.
Analyzes 336 OOD cases from eight state-of-the-art HTR models across multiple datasets and languages.
Finds that textual divergence between domains is the most significant factor affecting generalization, followed by visual divergence.
Demonstrates that OOD error can be reliably estimated and identifies key limitations in current HTR models.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17332</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering</title><link>https://arxiv.org/abs/2411.09749</link><description>Surveys the use of differentiable rendering techniques (e.g., Gaussian Splatting, Neural Radiance Fields) for generating adversarial attacks on deep neural networks.
Introduces a unified framework to compare and analyze diverse adversarial attack goals and scene manipulation methods in 3D environments.
Identifies research gaps and future directions, emphasizing the need to study real-world threats and complex scene manipulations.
Highlights the potential for new attack modalities and the importance of robust defenses in the context of advanced rendering techniques.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.09749</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Comparison-based Active Preference Learning for Multi-dimensional Personalization</title><link>https://arxiv.org/abs/2411.00524</link><description>Proposes a framework (AMPLe) for learning implicit, multi-dimensional user preferences for LLM personalization.
Uses interactive, comparison-based feedback and Bayesian inference to capture and update user preferences.
Introduces methods to reduce estimation bias and noise, and employs active query selection to minimize user effort.
Demonstrates the approach's effectiveness and efficiency in language generation tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.00524</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Augmented Generation with Estimation of Source Reliability</title><link>https://arxiv.org/abs/2410.22954</link><description>Proposes Reliability-Aware RAG (RA-RAG), a method that estimates and incorporates source reliability into retrieval-augmented generation systems.
Addresses the risk of propagating misinformation by considering heterogeneous reliability of sources in multi-source databases.
Introduces a benchmark simulating real-world scenarios with varying source reliability and demonstrates improved performance over baselines.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22954</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models</title><link>https://arxiv.org/abs/2410.13097</link><description>Proposes FedTT and FedTT+, new methods for communication-efficient federated fine-tuning of large language models (LLMs) using tensorized adapters.
Addresses privacy concerns by keeping sensitive data on local devices during federated learning.
FedTT+ improves robustness to data heterogeneity by adaptively freezing tensor factors, reducing trainable parameters.
Demonstrates significant reduction in communication costs and competitive performance on BERT and LLaMA models.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13097</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness</title><link>https://arxiv.org/abs/2410.03210</link><description>Introduces Tadashi, a system for automated code generation using machine learning with formal correctness guarantees.
Addresses the lack of correctness guarantees in ML-based code generation by integrating formal verification methods.
Demonstrates Tadashi's ability to verify and evaluate code transformations, ensuring legal and reliable outputs.
Highlights the system's low runtime overhead and broad applicability in code generation tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03210</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?</title><link>https://arxiv.org/abs/2410.02735</link><description>Proposes OOD-Chameleon, a method to automatically select the best algorithm for out-of-distribution (OOD) generalization based on dataset characteristics.
Formulates algorithm selection as a multi-label classification problem, trained on a diverse set of datasets with various distribution shifts.
Demonstrates that the learned selector can effectively rank and choose high-performing algorithms for unseen datasets and shifts.
Provides insights into the applicability of OOD generalization algorithms, potentially improving robustness in real-world AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02735</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Softmax is not Enough (for Sharp Size Generalisation)</title><link>https://arxiv.org/abs/2410.01104</link><description>Investigates the limitations of the softmax function in neural networks, particularly regarding its ability to generalize sharply as input size increases.
Demonstrates theoretically that softmax-based circuits cannot robustly approximate sharp functions as problem size grows.
Proposes adaptive temperature as a technique to improve softmax sharpness at inference time.
Findings have implications for the robustness and reliability of AI reasoning systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.01104</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hidden State Differential Private Mini-Batch Block Coordinate Descent for Multi-convexity Optimization</title><link>https://arxiv.org/abs/2407.08233</link><description>Proposes a new algorithm (DP-MBCD) for optimizing multi-convex problems with differential privacy guarantees under the hidden state assumption.
Addresses limitations of previous privacy analyses that required strong convexity assumptions, expanding applicability to non-convex problems like neural network training.
Provides tighter privacy loss bounds and compatibility with proximal gradient descent and adaptive noise mechanisms.</description><guid isPermaLink="false">https://arxiv.org/abs/2407.08233</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title><link>https://arxiv.org/abs/2406.11569</link><description>Explores federated meta-learning for pre-training and personalized fine-tuning in AI systems, particularly in wireless environments.
Analyzes the trade-off between generalization to new agents/tasks and convergence speed, considering the impact of wireless channel impairments.
Highlights how over-the-air computing in federated learning can affect both the robustness and efficiency of AI model training.
Provides theoretical and empirical results on the convergence-generalization trade-off in decentralized AI training.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.11569</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Contrastive Explainable Clustering with Differential Privacy</title><link>https://arxiv.org/abs/2406.04610</link><description>Proposes a method that combines contrastive explainable AI (XAI) with differential privacy for clustering algorithms.
Focuses on k-median and k-means clustering, providing personalized, privacy-preserving explanations for centroid placement.
Demonstrates that differentially private explanations maintain similar utility to non-private ones.
Advances privacy-aware machine learning by balancing data protection, explanation quality, and personalization.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.04610</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On Temperature Scaling and Conformal Prediction of Deep Classifiers</title><link>https://arxiv.org/abs/2402.05806</link><description>Investigates the interplay between temperature scaling (a calibration method) and conformal prediction in deep neural network classifiers.
Finds that temperature scaling improves class-conditional coverage in adaptive conformal prediction methods but can increase prediction set sizes.
Provides a mathematical explanation for the observed trade-off and offers practical guidelines for combining calibration and conformal prediction.
Addresses the reliability and confidence estimation of AI model predictions, which is crucial for safe and trustworthy AI deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2402.05806</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective</title><link>https://arxiv.org/abs/2311.16646</link><description>Analyzes the vulnerability of dataset distillation methods to backdoor attacks using kernel methods.
Proposes two new theory-driven trigger pattern generation methods for backdoor attacks in dataset distillation.
Demonstrates that these optimized triggers can evade conventional backdoor detection and mitigation techniques.
Empirical results show the effectiveness and resilience of the proposed backdoor attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2311.16646</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hidden Conflicts in Neural Networks and Their Implications for Explainability</title><link>https://arxiv.org/abs/2310.20363</link><description>Introduces a theory of 'conflicts' in artificial neural networks (ANNs) and explores their implications for model explainability.
Proposes a new feature attribution method, Conflict-Aware Feature-wise Explanations (CAFE), to provide more faithful explanations by separating positive and negative feature influences.
Investigates the role of conflicts in out-of-distribution (OOD) scenarios, suggesting connections between conflicts and distributional shifts.
Highlights the importance of understanding conflicts for developing more reliable and trustworthy AI explanation methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2310.20363</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Self-ensemble: Mitigating Confidence Distortion for Large Language Models</title><link>https://arxiv.org/abs/2506.01951</link><description>Identifies and addresses the confidence distortion problem in large language models (LLMs) during multi-choice question answering.
Proposes a 'Self-ensemble' method that groups answer choices and ensembles predictions to improve confidence calibration.
Demonstrates that the method is plug-and-play and does not require labeled data for tuning.
Shows improved performance and confidence calibration across multiple LLMs and datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01951</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Should Decision-Makers Reveal Classifiers in Online Strategic Classification?</title><link>https://arxiv.org/abs/2506.01936</link><description>Examines the impact of revealing or withholding classifiers in online strategic classification settings.
Analyzes how agents manipulate their features based on their knowledge (or lack thereof) of the classifier.
Finds that hiding the classifier can actually increase the number of mistakes made by the decision-maker.
Provides theoretical results quantifying the performance degradation when agents have limited knowledge.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01936</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title><link>https://arxiv.org/abs/2506.01420</link><description>Introduces SEAL, a novel adversarial distillation framework for training small language models (SLMs) to anonymize text without relying on external proprietary models.
Uses adversarial interactions between anonymizer and inference models to improve anonymization and privacy protection.
Demonstrates that SLMs trained with SEAL can match or surpass the privacy-utility trade-off of larger models like GPT-4.
Releases a synthetic dataset to support further research in privacy-preserving language model development.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01420</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation</title><link>https://arxiv.org/abs/2506.01267</link><description>Investigates the statistical optimality of adversarial learning methods in nonparametric regression.
Establishes minimax rates of convergence under adversarial risks, specifically for future input perturbations (X-attacks).
Proposes a piecewise local polynomial estimator that achieves minimax optimality and an adaptive estimator for robust performance.
Analyzes how function smoothness and perturbation magnitude impact adversarial robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01267</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN</title><link>https://arxiv.org/abs/2506.01226</link><description>Proposes a new parameterization for stabilizing nonlinear policies in learning-based control using robust neural networks.
Ensures closed-loop stability by design, even when optimizing with unconstrained methods.
Analyzes stability under challenging conditions: nonlinear dynamics, partial observation, and incremental stability requirements.
Demonstrates the approach's effectiveness in scenarios with uncertain systems and non-stabilizing reward structures.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01226</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor</title><link>https://arxiv.org/abs/2506.01162</link><description>Proposes a nearly-linear time algorithm for private hypothesis selection under differential privacy constraints.
Achieves the optimal approximation factor for hypothesis selection while maintaining efficient computation.
Addresses an open question regarding the trade-off between privacy, accuracy, and computational efficiency in hypothesis selection.
Improves upon previous work by reducing the computational complexity from quadratic to nearly-linear time.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01162</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Common Inpainted Objects In-N-Out of Context</title><link>https://arxiv.org/abs/2506.00721</link><description>Introduces COinCO, a large dataset of images with inpainted objects placed both in and out of context, enabling research on contextual understanding in vision models.
Uses diffusion-based inpainting and multimodal LLMs to verify and categorize contextual coherence of objects.
Enables tasks such as context classification, object-context prediction, and context-enhanced fake detection for image forensics.
Provides a controlled testbed for studying semantic priors and context-aware robustness in computer vision.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00721</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PackHero: A Scalable Graph-based Approach for Efficient Packer Identification</title><link>https://arxiv.org/abs/2506.00659</link><description>Presents PackHero, a scalable and efficient graph-based method for identifying software packers, which are often used in malware to evade analysis.
Utilizes a Graph Matching Network and clustering to analyze call graphs from packed programs, improving adaptability and reducing the need for large training datasets.
Demonstrates high accuracy and scalability on both malware and benign samples, outperforming existing signature-based and machine learning approaches, especially for virtualization-based packers.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00659</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities</title><link>https://arxiv.org/abs/2506.00548</link><description>Introduces 'Con Instruction', a novel method for jailbreaking multimodal large language models (MLLMs) using adversarial images or audio, rather than text.
Demonstrates that non-textual adversarial examples can bypass safety mechanisms in MLLMs, especially when combined with text inputs.
Proposes a new Attack Response Categorization (ARC) framework to evaluate the quality and relevance of model responses to malicious instructions.
Evaluates the effectiveness of attacks and defenses on multiple MLLMs, showing high attack success rates and highlighting gaps in current defense techniques.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00548</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Label-shift robust federated feature screening for high-dimensional classification</title><link>https://arxiv.org/abs/2506.00379</link><description>Proposes a label-shift robust federated feature screening (LR-FFS) method for high-dimensional classification tasks.
Addresses challenges of data heterogeneity and label shift in federated learning environments.
Ensures privacy protection and computational efficiency in distributed settings.
Includes a false discovery rate (FDR) control method to enhance reliability of feature selection.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00379</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>3D Gaussian Splat Vulnerabilities</title><link>https://arxiv.org/abs/2506.00280</link><description>Introduces CLOAK, an attack exploiting view-dependent Gaussian appearances in 3D Gaussian Splatting (3DGS) to embed adversarial content visible only from certain viewpoints.
Presents DAGGER, a targeted adversarial attack that perturbs 3D Gaussians to deceive object detectors without needing access to training data.
Highlights vulnerabilities in 3DGS that could impact safety-critical applications such as robotic learning and autonomous navigation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00280</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Structuring Radiology Reports: Challenging LLMs with Lightweight Models</title><link>https://arxiv.org/abs/2506.00200</link><description>Compares lightweight encoder-decoder models (T5, BERT2BERT) to large language models (LLMs) for structuring radiology reports.
Highlights privacy, transparency, and computational concerns with deploying LLMs in clinical settings.
Demonstrates that lightweight models can outperform LLMs adapted with prompt-based techniques, with significantly lower resource requirements.
Emphasizes the importance of privacy-preserving and sustainable AI solutions in healthcare.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00200</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs</title><link>https://arxiv.org/abs/2506.00197</link><description>Presents a comprehensive risk assessment of knowledge file leakage in GPT-based LLM agents.
Identifies five distinct leakage vectors, including metadata, initialization, retrieval, sandboxed environments, and prompts.
Demonstrates a privilege escalation vulnerability via the Code Interpreter tool, allowing adversaries to download original knowledge files with high success.
Analyzes the prevalence of leaked copyrighted and sensitive materials, and offers actionable security recommendations for GPT builders and platform providers.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00197</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?</title><link>https://arxiv.org/abs/2506.00062</link><description>Investigates how fine-tuning large language models (LLMs) for telecom tasks can degrade their safety alignment.
Demonstrates that safety degradation occurs even with structured, seemingly harmless telecom datasets.
Evaluates three safety realignment defenses (SafeInstruct, SafeLoRA, SafeMERGE) using red-teaming benchmarks.
Provides practical guidance for restoring and maintaining safety in telecom-tuned LLMs without sacrificing task performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00062</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval</title><link>https://arxiv.org/abs/2506.00041</link><description>Proposes a framework using Sparse Autoencoders to interpret dense embeddings in Dense Passage Retrieval (DPR) models.
Enables human-understandable descriptions of latent concepts within dense embeddings, improving model transparency.
Introduces Concept-Level Sparse Retrieval (CL-SR), which uses interpretable latent concepts for efficient and transparent retrieval.
Demonstrates that the approach maintains robust retrieval performance while increasing interpretability.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00041</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data</title><link>https://arxiv.org/abs/2506.01907</link><description>Proposes SMOTE-DP, a method combining SMOTE and differential privacy to generate synthetic data with improved privacy-utility tradeoff.
Addresses the risk of privacy leakage in synthetic data, especially concerning outliers.
Demonstrates both theoretically and empirically that SMOTE-DP maintains data utility while providing robust privacy protection.
Highlights the challenges of balancing privacy and utility in privacy-preserving data publication.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01907</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Trade-offs in Data Memorization via Strong Data Processing Inequalities</title><link>https://arxiv.org/abs/2506.01855</link><description>Investigates the relationship between data memorization and learning accuracy in machine learning models.
Develops new lower bounds on the amount of training data memorization required for accurate learning, using strong data processing inequalities.
Highlights privacy risks due to memorization, especially when training on sensitive data.
Extends theoretical results to more general models and addresses limitations of prior work.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01855</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Trojan Horse Hunt in Time Series Forecasting for Space Operations</title><link>https://arxiv.org/abs/2506.01849</link><description>Describes a Kaggle competition focused on detecting and reconstructing trojan triggers (adversarial poisoning) in time series forecasting models for satellite telemetry.
Addresses AI security threats in safety-critical space operations, specifically adversarial attacks on continuously fine-tuned models.
Highlights the need for new methods beyond existing baselines (e.g., Neural Cleanse) for detecting trojans in time series data.
Emphasizes broader implications for AI security in other safety-critical domains using time series analysis.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01849</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs</title><link>https://arxiv.org/abs/2506.01790</link><description>Proposes IF-Guide, a proactive method for detoxifying large language models (LLMs) by identifying and suppressing harmful training data using influence functions.
Introduces a novel adaptation of influence functions to measure token-level contributions to model toxicity, enabling targeted mitigation.
Demonstrates that IF-Guide reduces both explicit and implicit toxicity more effectively than existing alignment methods, without requiring human-preference data.
Highlights computational efficiency, showing that smaller models can be used to identify toxic data for larger models.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01790</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Federated Gaussian Mixture Models</title><link>https://arxiv.org/abs/2506.01780</link><description>Proposes FedGenGMM, a one-shot federated learning method for Gaussian Mixture Models (GMMs) in unsupervised settings.
Addresses privacy concerns by enabling decentralized training without sharing raw data.
Reduces communication overhead and maintains robust performance, including in anomaly detection tasks.
Demonstrates applicability to edge computing and scenarios with significant data heterogeneity.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01780</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems</title><link>https://arxiv.org/abs/2506.01777</link><description>Introduces DRAUN, a novel data reconstruction attack targeting federated unlearning (FU) systems.
Demonstrates that FU systems, designed to comply with privacy regulations like GDPR and CCPA, are vulnerable to data reconstruction attacks.
Shows that existing data reconstruction attacks for centralized ML do not work in FU, and explains how DRAUN overcomes these limitations.
Validates the attack across multiple datasets, model architectures, and unlearning methods, highlighting significant privacy risks in current FU implementations.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01777</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</title><link>https://arxiv.org/abs/2506.01594</link><description>Proposes a framework for selecting machine learning models that optimize both fairness and accuracy, especially in high-stakes decision-making contexts.
Introduces a horizontal search approach for Less Discriminatory Algorithms (LDAs), considering fairness across different model families rather than just within a single model's parameters.
Demonstrates the approach using real-world lending data, highlighting practical considerations like regulatory standards and resource constraints.
Emphasizes the importance of responsible and systematic model selection to meet civil rights obligations and reduce discrimination in automated decision systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01594</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</title><link>https://arxiv.org/abs/2506.01523</link><description>Reframes alignment of large language models (LLMs) as distribution learning from pairwise preference feedback.
Identifies theoretical issues with current RLHF and DPO approaches, such as degeneracy and reward overfitting.
Proposes three new learning objectives that avoid these issues and provide strong theoretical guarantees.
Empirically demonstrates that the new framework outperforms or matches existing alignment methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01523</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Variance-Based Defense Against Blended Backdoor Attacks</title><link>https://arxiv.org/abs/2506.01444</link><description>Proposes a novel defense method against backdoor attacks in AI models, focusing on scenarios where clean datasets are unavailable.
The method detects poisoned classes, extracts the critical part of the attack trigger, and identifies poisoned instances, enhancing explainability.
Experimental evaluations demonstrate the effectiveness of the approach on standard image datasets, outperforming or matching state-of-the-art defenses.
Addresses a key limitation of existing defenses that require access to clean data for anomaly detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01444</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping</title><link>https://arxiv.org/abs/2506.01396</link><description>Investigates the disparate impact of differentially private (DP) learning methods, particularly on minority groups.
Identifies that adaptive gradient clipping in DP learning can disproportionately harm model accuracy for underrepresented classes.
Proposes a bounded adaptive clipping method to mitigate excessive gradient suppression and improve fairness.
Demonstrates significant improvements in worst-class accuracy on benchmark datasets compared to existing methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01396</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs</title><link>https://arxiv.org/abs/2506.01386</link><description>Introduces ThinkEval, a framework for evaluating how well LLM editing techniques preserve or alter interconnected knowledge.
Highlights the limitations of current model editing methods in handling ripple effects and maintaining contextual integrity after edits.
Presents KnowGIC, a benchmark dataset for testing the impact of edits on knowledge consistency and catastrophic forgetting.
Evaluates five model editing techniques across multiple LLMs, revealing challenges in balancing fact suppression and knowledge preservation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01386</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion</title><link>https://arxiv.org/abs/2506.01356</link><description>Proposes a two-stage training framework for synthesizing stabilizing neural controllers and Lyapunov functions for continuous-time systems.
Introduces a Zubov-inspired sampling and domain expansion strategy to reduce conservatism in estimating regions of attraction.
Extends the neural network verifier ,-CROWN to verify Lyapunov conditions efficiently, avoiding reliance on expensive SMT solvers.
Demonstrates significant improvements in region of attraction size and verification speed on challenging nonlinear systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01356</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distributionally Robust Learning in Survival Analysis</title><link>https://arxiv.org/abs/2506.01348</link><description>Proposes a distributionally robust learning (DRL) approach for Cox regression in survival analysis.
Utilizes Wasserstein distance-based ambiguity sets to improve model robustness against data distribution shifts and perturbations.
Reformulates the robust learning problem into a tractable optimization problem with theoretical guarantees.
Demonstrates improved robustness and prediction accuracy through simulations and real-world case studies.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01348</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning</title><link>https://arxiv.org/abs/2506.01339</link><description>Proposes a new method (ILU) for robust machine unlearning in large language models, inspired by invariant risk minimization.
Addresses privacy and safety concerns by enabling selective removal of targeted knowledge from LLMs.
Demonstrates that ILU is resilient to information recovery even after diverse downstream fine-tuning tasks.
Shows that ILU outperforms existing unlearning methods in both robustness and utility preservation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01339</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stress-Testing ML Pipelines with Adversarial Data Corruption</title><link>https://arxiv.org/abs/2506.01230</link><description>Introduces SAVAGE, a framework for stress-testing machine learning pipelines using realistic, structured data corruptions.
Models data-quality issues through dependency graphs and corruption templates, systematically identifying worst-case corruption patterns.
Demonstrates that targeted, structured corruptions can severely degrade model performance, surpassing the impact of random or manually crafted errors.
Provides a practical tool for robustness evaluation, pipeline stress-testing, and guidance for building more resilient ML systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01230</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective</title><link>https://arxiv.org/abs/2506.01213</link><description>Investigates the stability of Graph Convolutional Neural Networks (GCNNs) under perturbations in graph topology.
Proposes a probabilistic framework to analyze how changes in graph structure affect model outputs, moving beyond worst-case scenarios.
Demonstrates the framework's effectiveness through experiments, including analysis of adversarial attacks on downstream tasks.
Highlights the importance of considering data distribution in stability and robustness analysis for trustworthy AI models.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01213</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multiresolution Analysis and Statistical Thresholding on Dynamic Networks</title><link>https://arxiv.org/abs/2506.01208</link><description>Proposes ANIE, a multi-resolution framework for detecting structural changes in dynamic networks.
Addresses the challenge of identifying changes at multiple time scales, which is particularly relevant in cybersecurity contexts.
Provides theoretical guarantees and demonstrates robustness to noise in both synthetic and real-world network data.
Enables improved detection of both rapid and gradual network anomalies compared to fixed-resolution methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01208</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Accelerated Learning with Linear Temporal Logic using Differentiable Simulation</title><link>https://arxiv.org/abs/2506.01167</link><description>Addresses the challenge of ensuring safety and reliability in reinforcement learning controllers for real-world applications.
Proposes integrating linear temporal logic (LTL) with differentiable simulators to enable efficient, correct-by-construction learning.
Introduces soft labeling to mitigate sparse-reward issues in LTL-based objectives without compromising correctness.
Demonstrates improved reward attainment and training efficiency compared to traditional discrete methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01167</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</title><link>https://arxiv.org/abs/2506.01054</link><description>Examines the gap between theoretical and practical soundness in neural network verification for safety.
Demonstrates that current state-of-the-art verifiers fail to guarantee safety in real-world deployment due to floating point and environmental factors.
Introduces adversarial networks that exploit deployment-specific features to mislead verifiers.
Empirically shows that all tested verifiers are vulnerable to these new deployment-specific attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01054</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts</title><link>https://arxiv.org/abs/2506.01000</link><description>Proposes a new framework for model reprogramming of CLIP using decoupled visual prompts optimized by explicit causes or unsupervised clusters.
Introduces a probabilistic reweighting matrix to integrate and interpret the influence of different visual prompts on classification decisions.
Demonstrates improved performance over baselines across 11 downstream datasets and provides theoretical analysis on empirical risk.
Offers insights into the interpretability and robustness of visual prompt-based reprogramming.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01000</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers</title><link>https://arxiv.org/abs/2506.00998</link><description>Introduces LoRA-BAM, a method for out-of-distribution (OoD) detection in fine-tuned large language models (LLMs) using boxed abstraction monitors.
Filters input questions that fall outside the model's competence, enhancing reliability and safety.
Improves interpretability and robustness by regularizing feature space during fine-tuning and adjusting decision boundaries based on feature variance.
Provides a lightweight and interpretable defense mechanism that can complement existing security measures.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00998</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity</title><link>https://arxiv.org/abs/2506.00932</link><description>Investigates the challenges of federated learning (FL) in low-data and heterogeneous environments, focusing on the 'Layer-wise Inertia Phenomenon' that limits model updates.
Proposes a novel method (LIPS) that introduces transient sparsity to stimulate more effective global model aggregation.
Demonstrates improved performance and collaboration in FL scenarios through experiments across diverse datasets and architectures.
Contributes to understanding and improving privacy-preserving collaborative AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00932</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery</title><link>https://arxiv.org/abs/2506.00844</link><description>The paper critiques the use of large language models (LLMs) in causal discovery, arguing that their correlation-based modeling is fundamentally unsuited for causal reasoning.
Empirical studies reveal that prompt engineering can artificially inflate LLM performance in causal tasks, raising concerns about reliability and potential misuse.
The authors recommend restricting LLMs to non-decisional, auxiliary roles in causal discovery, such as heuristic search, rather than allowing them to determine causal relationships.
The work calls for the development of specialized models and training methods that align with the theoretical requirements of causal inference.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00844</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs</title><link>https://arxiv.org/abs/2506.00569</link><description>Proposes AutoMixAlign (AMA), an adaptive data mixing algorithm for multi-task preference optimization in LLM alignment.
Addresses the challenge of balancing performance across multiple alignment tasks (helpfulness, harmlessness, honesty) by adaptively reweighting or resampling training data.
Introduces two algorithms (AMA-R and AMA-S) with theoretical convergence guarantees for optimizing task balance.
Demonstrates improved performance over standard alignment and model merging approaches in multitask alignment setups.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00569</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title><link>https://arxiv.org/abs/2506.00563</link><description>Investigates behavioral metric learning in deep reinforcement learning (RL), focusing on robustness to task-irrelevant noise.
Benchmarks five recent metric learning approaches across a large set of tasks and noise settings.
Introduces new evaluation metrics, such as a denoising factor, to assess the encoder's ability to filter distractions.
Releases an open-source codebase to support reproducibility and future research.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00563</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study</title><link>https://arxiv.org/abs/2506.00499</link><description>Proposes a federated learning (FL) framework for collaborative remaining useful life (RUL) prognostics in aircraft engines.
Addresses privacy concerns by enabling multiple airlines to train a shared model without sharing raw data.
Introduces robust aggregation methods to improve the FL framework's resilience to noisy sensor data.
Demonstrates improved prognostic accuracy and robustness compared to independent model training.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00499</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet - A ResNet-based Model Classification Dataset</title><link>https://arxiv.org/abs/2506.00476</link><description>Introduces ModelNet, a new dataset for federated learning (FL) research, focusing on privacy-preserving model training.
Addresses privacy concerns in FL by proposing a method where algorithms access only anonymized model parameters.
Simulates realistic FL scenarios with non-IID data and client diversity, supporting both conventional and graph-based FL algorithms.
Benchmarks and experiments demonstrate the effectiveness of the dataset for privacy and robustness evaluation in FL.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00476</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models</title><link>https://arxiv.org/abs/2506.00457</link><description>Evaluates the robustness of Large Language Models (LLMs) in zero-shot time-series forecasting tasks.
Finds that LLMs are highly sensitive to small amounts of noise, leading to significant drops in forecasting accuracy.
Explores potential solutions to improve LLM robustness but finds that challenges remain.
Suggests that fine-tuning LLMs may be more effective than relying on zero-shot capabilities for time-series tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00457</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning</title><link>https://arxiv.org/abs/2506.00440</link><description>Proposes PSI-PFL, a client selection framework for Personalized Federated Learning (PFL) using the Population Stability Index (PSI) to address data heterogeneity.
Aims to mitigate the challenges of non-IID (non-independent and identically distributed) data in federated learning, which can degrade model performance.
Demonstrates improved global model accuracy and fairer local performance across multiple data modalities while maintaining data privacy.
Highlights practical benefits for privacy-preserving machine learning in heterogeneous data environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00440</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title><link>https://arxiv.org/abs/2506.00416</link><description>Proposes a blockchain-enabled, privacy-preserving federated learning framework for personalized healthcare.
Utilizes second-order federated learning (FedCurv) to address challenges with non-iid and heterogeneous data among edge clients.
Incorporates Ethereum-based model aggregation for trust, verifiability, and auditability, and uses public key encryption to enhance privacy and security.
Demonstrates efficiency and scalability through experiments on standard datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00416</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries</title><link>https://arxiv.org/abs/2506.00388</link><description>Introduces CLARIFY, a method to improve preference-based reinforcement learning (PbRL) by handling ambiguous human feedback.
Uses contrastive learning to create trajectory embeddings that better distinguish between similar actions, improving query selection.
Demonstrates improved performance over baselines in scenarios with non-ideal or real human feedback.
Addresses challenges in aligning AI behavior with human intentions by making preference learning more robust and label-efficient.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00388</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Spectral Insights into Data-Oblivious Critical Layers in Large Language Models</title><link>https://arxiv.org/abs/2506.00382</link><description>Introduces a data-oblivious method to identify critical layers in pre-fine-tuned large language models (LLMs) using spectral analysis.
Finds that layers with significant representation shifts are most affected during fine-tuning and are consistent across tasks.
Demonstrates practical applications in efficient domain adaptation and backdoor defense, showing that freezing critical layers can reduce backdoor attack success rates by up to 40%.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00382</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms</title><link>https://arxiv.org/abs/2506.00299</link><description>Proposes an inference-time alignment framework for diffusion models using evolutionary algorithms.
Enables alignment of generative models to safety constraints or domain-specific objectives without requiring model gradients or internal access.
Demonstrates improved efficiency and alignment performance compared to existing gradient-based and gradient-free methods.
Applicable to both differentiable and non-differentiable alignment objectives, making it broadly useful for safety and robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00299</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity</title><link>https://arxiv.org/abs/2506.00245</link><description>Proposes a new method for uncertainty quantification in large language models (LLMs), improving upon semantic entropy by considering pairwise semantic similarity.
Addresses the detection of hallucinations in LLM outputs, which is a key aspect of AI safety and reliability.
Demonstrates the method's effectiveness across multiple LLMs and tasks, providing both theoretical and empirical validation.
Offers a black-box approach that can be extended to white-box settings, enhancing its applicability for robust AI system evaluation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00245</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DeGLIF for Label Noise Robust Node Classification using GNNs</title><link>https://arxiv.org/abs/2506.00244</link><description>Proposes DeGLIF, a denoising technique for graph data using leave-one-out influence functions to improve robustness against label noise in node classification tasks.
Utilizes a small set of clean data and influence functions to identify and relabel noisy nodes in graph neural network (GNN) training data.
Introduces two variants of DeGLIF that do not require prior knowledge of the noise model or noise level.
Demonstrates through experiments that DeGLIF improves accuracy over baseline methods and provides theoretical guarantees for one variant.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00244</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents</title><link>https://arxiv.org/abs/2506.00172</link><description>Introduces Breakpoint, a benchmarking methodology for evaluating LLM code agents on system-level reasoning tasks.
Automatically generates code-repair tasks by adversarially corrupting functions in real-world software repositories.
Allows systematic control of task difficulty along dimensions of local and system-level reasoning.
Demonstrates scalability and varying success rates of state-of-the-art models on over 900 generated tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00172</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States</title><link>https://arxiv.org/abs/2506.00158</link><description>Addresses privacy amplification in zeroth-order optimization, which is relevant for fine-tuning large language models under differential privacy constraints.
Provides a convergent differential privacy (DP) bound for zeroth-order optimization, filling a gap in privacy analysis for these methods.
Generalizes the privacy amplification-by-iteration framework to zeroth-order optimization with smooth loss functions.
Introduces improved DP zeroth-order algorithmic designs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00158</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective</title><link>https://arxiv.org/abs/2506.00152</link><description>Examines the risks and opportunities of fine-tuning large language models (LLMs) using observational (historical) data.
Highlights the danger of learning spurious correlations when fine-tuning on observational data without accounting for confounders.
Proposes DeconfoundLM, a method to remove the effect of known confounders from reward signals, improving causal alignment.
Demonstrates empirically that causal corrections can mitigate failure modes and improve alignment with human or business objectives.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00152</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement</title><link>https://arxiv.org/abs/2506.00030</link><description>Proposes a Shapley-guided alternating training framework to address modality imbalance in multimodal fusion, ensuring minor modalities are sufficiently learned.
Introduces a memory module and cross-modal mapping mechanism to better align and inherit modality-specific representations.
Evaluates the method on multiple benchmark datasets, demonstrating state-of-the-art results and strong robustness under missing modalities.
Presents a new equilibrium deviation metric (EDM) to assess balance and accuracy in multimodal learning.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00030</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</title><link>https://arxiv.org/abs/2505.23799</link><description>Investigates how well current automated metrics estimate the consistency of LLM responses compared to human perception.
Conducts a large user study (n=2,976) to benchmark existing surrogate metrics against human ratings.
Proposes a new logit-based ensemble method for estimating LLM consistency.
Finds that automated metrics are imperfect proxies for human judgment, recommending more human-in-the-loop evaluation.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23799</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Gap: A Practical Attack on GGUF Quantization</title><link>https://arxiv.org/abs/2505.23786</link><description>Introduces the first practical attack on GGUF quantization, a widely used method for deploying large language models efficiently.
Demonstrates that quantization errors can be exploited to inject malicious behaviors into quantized models that are not present in full-precision models.
Shows the attack's effectiveness across multiple LLMs, quantization types, and attack scenarios, including insecure code generation and targeted content injection.
Highlights that complex quantization schemes alone do not provide sufficient security against adversarial manipulation.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23786</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time</title><link>https://arxiv.org/abs/2505.23729</link><description>Proposes SITAlign, an inference-time framework for aligning large language models (LLMs) using satisficing strategies inspired by bounded rationality.
Maximizes a primary objective (e.g., helpfulness) while ensuring secondary objectives (e.g., harmlessness) meet predefined thresholds.
Provides theoretical analysis and empirical validation, showing improved performance over existing multi-objective decoding strategies on alignment benchmarks.
Addresses the multifaceted nature of alignment, making LLMs safer and more controllable in deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23729</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation</title><link>https://arxiv.org/abs/2505.23657</link><description>Proposes Active Layer-Contrastive Decoding (ActLCD), a new decoding strategy for large language models (LLMs) to reduce hallucinations.
ActLCD uses reinforcement learning to decide when to apply contrasting layers during text generation, optimizing for factuality.
Demonstrates improved performance over state-of-the-art methods across five benchmarks, indicating effectiveness in mitigating hallucinations.
Addresses a key safety concern in LLMs by reducing the generation of false or misleading information.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23657</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms</title><link>https://arxiv.org/abs/2505.23576</link><description>Proposes the use of cognitive guardrails to ensure safe decision-making in autonomous drone swarms using LLMs.
Addresses the risks of LLM hallucinations and unsafe recommendations in open-world environments.
Describes the design, simulation, and real-world integration of these safety mechanisms for search-and-rescue missions.
Focuses on enhancing the robustness and safety of AI-driven autonomous systems in critical applications.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23576</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SWE-bench Goes Live!</title><link>https://arxiv.org/abs/2505.23419</link><description>Introduces SWE-bench-Live, a live-updating benchmark for evaluating LLMs on real-world bug-fixing tasks.
Addresses limitations of previous benchmarks by automating instance creation and environment setup, reducing manual effort and risk of data contamination.
Provides a scalable, reproducible, and continuously updated evaluation framework using Docker images and an automated curation pipeline.
Enables more rigorous and contamination-resistant assessment of LLMs and agent frameworks in dynamic software development contexts.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23419</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Context-Robust Knowledge Editing for Language Models</title><link>https://arxiv.org/abs/2505.23026</link><description>Introduces CHED, a benchmark for evaluating the context robustness of knowledge editing (KE) methods in language models.
Finds that existing KE methods often fail when preceding context triggers retrieval of original (unedited) knowledge.
Proposes CoRE, a new KE method that improves robustness to context and maintains model capabilities.
Analyzes how different types of preceding context (user vs. assistant) and attention patterns affect editing success.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23026</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding</title><link>https://arxiv.org/abs/2505.22906</link><description>Introduces Human-in-the-loop Decoding (HiLDe), enabling users to influence LLM code generation decisions.
Focuses on reducing security vulnerabilities in code generated by AI programming tools.
Demonstrates through user studies that HiLDe helps users produce more secure code and better align outputs with their intentions.
Addresses the risks of over-reliance on AI-generated code in critical domains like software security.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22906</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title><link>https://arxiv.org/abs/2505.21938</link><description>Introduces a practical adversarial attack model (Fake Data Injection) for stochastic bandit algorithms.
Demonstrates that attackers can mislead popular algorithms (UCB, Thompson Sampling) by injecting limited, bounded fake feedback.
Provides theoretical analysis and experimental validation showing significant vulnerabilities in real-world and synthetic settings.
Addresses realistic adversarial constraints, making the findings highly relevant to the security of AI-driven decision systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21938</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</title><link>https://arxiv.org/abs/2505.21523</link><description>Investigates the phenomenon of increased hallucination in multimodal large language models as reasoning chains become longer.
Introduces RH-AUC, a metric to quantify changes in perception accuracy with reasoning length, and RH-Bench, a benchmark for evaluating reasoning vs. hallucination trade-offs.
Finds that larger models and specific training data types help balance reasoning ability and perceptual fidelity.
Highlights the need for evaluation frameworks that jointly assess reasoning quality and visual grounding to mitigate hallucination risks.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21523</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Universal Value-Function Uncertainties</title><link>https://arxiv.org/abs/2505.21119</link><description>Introduces Universal Value-Function Uncertainties (UVU), a method for estimating epistemic uncertainty in value functions for reinforcement learning (RL).
UVU quantifies uncertainty using squared prediction errors between an online learner and a fixed, randomly initialized target network, improving over previous methods like random network distillation.
Theoretical analysis shows UVU's equivalence to ensemble variance in the infinite-width limit, and empirical results demonstrate competitive performance with lower computational cost.
Uncertainty estimation is crucial for safe decision-making and exploration in RL, which are important aspects of AI safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21119</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments</title><link>https://arxiv.org/abs/2505.18927</link><description>Benchmarks three leading large language models (GPT-4.1, Gemini 1.5 Pro, Claude 3 Opus) for cyberbullying detection in YouTube comments.
Analyzes model performance across multiple languages and identifies challenges with sarcasm, coded insults, and mixed-language slang.
Highlights the need for improved moderation pipelines and model fine-tuning for better detection of harmful content.
Releases a de-identified dataset and prompts to support reproducibility and further research in automated content moderation.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18927</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2505.16415</link><description>Proposes a new method (ARC-JSD) for attributing generated responses to specific context segments in Retrieval-Augmented Generation (RAG) systems.
The method uses Jensen-Shannon Divergence to efficiently and accurately identify essential context sentences without requiring fine-tuning or surrogate models.
Provides mechanistic insights into which attention heads and MLP layers are responsible for context attribution in LLMs.
Demonstrates improved accuracy and computational efficiency over previous methods on multiple RAG benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16415</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Replay Attacks Against Audio Deepfake Detection</title><link>https://arxiv.org/abs/2505.14862</link><description>Demonstrates that replay attacks can significantly undermine the effectiveness of audio deepfake detection systems.
Introduces ReplayDF, a new dataset featuring a wide range of speaker-microphone combinations, languages, and TTS models to study replay attacks.
Finds that state-of-the-art detection models experience a substantial drop in performance when faced with replayed deepfakes.
Releases the ReplayDF dataset for non-commercial research to facilitate further study in this area.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14862</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Forensic deepfake audio detection using segmental speech features</title><link>https://arxiv.org/abs/2505.13847</link><description>Investigates the use of segmental speech features for detecting deepfake audio.
Finds that certain segmental features are effective in distinguishing deepfakes from real audio.
Highlights the limitations of global features and the need for specialized detection methods.
Provides insights relevant to forensic and security applications involving audio deepfakes.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13847</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors</title><link>https://arxiv.org/abs/2505.04792</link><description>Investigates the phenomenon of confabulation (generation of false information) in reservoir computers, a type of artificial neural network.
Analyzes how untrained attractors (UAs) emerge when the system reconstructs dynamics it was not explicitly trained on.
Finds that UAs and confabulation are intrinsic features of bounded state-space learning systems, potentially extending beyond reservoir computers.
Provides foundational insights into failure modes and unintended behaviors in AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.04792</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales</title><link>https://arxiv.org/abs/2505.04608</link><description>Proposes a new method (Weighted-Conformal Test Martingales) for adaptive, continual monitoring of AI/ML systems post-deployment.
Addresses limitations of existing monitoring approaches by enabling detection and diagnosis of unexpected data distribution shifts.
Provides algorithms that adapt to covariate shifts, detect harmful changes, and distinguish between concept and extreme covariate shifts.
Demonstrates improved monitoring performance on real-world datasets, supporting safer AI deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.04608</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title><link>https://arxiv.org/abs/2505.01892</link><description>Introduces OODTE, a differential testing engine for evaluating the correctness of the ONNX Optimizer.
OODTE automatically detects discrepancies and issues caused by optimization passes in ONNX models.
The tool uncovered multiple previously unknown bugs and inconsistencies in the optimizer, highlighting potential risks in model deployment.
OODTE's methodology can be adapted for other AI model optimizers, contributing to robustness and reliability in AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01892</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</title><link>https://arxiv.org/abs/2504.19475</link><description>Introduces Prisma, an open-source toolkit for mechanistic interpretability in vision and video transformers.
Provides access to 75+ vision and video transformer models, pre-trained sparse autoencoder weights, and various interpretability tools.
Enables circuit analysis, activation caching, and visualization to facilitate understanding of model internals.
Aims to lower barriers for research in vision model interpretability, potentially supporting safety and robustness efforts.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.19475</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2504.17004</link><description>Introduces a theoretical framework to analyze the feasibility of automated hallucination detection in large language models (LLMs).
Establishes an equivalence between hallucination detection and language identification, showing fundamental limitations in automated detection using only positive examples.
Demonstrates that expert-labeled feedback (both correct and incorrect examples) enables effective hallucination detection, supporting the importance of RLHF and similar methods.
Provides theoretical insights into the challenges and requirements for reliable deployment of LLMs with respect to hallucination detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17004</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward</title><link>https://arxiv.org/abs/2504.16727</link><description>Introduces V^2R-Bench, a benchmark for evaluating the robustness of Large Vision Language Models (LVLMs) to fundamental visual variations such as position, scale, orientation, and context.
Finds that current LVLMs are surprisingly vulnerable to simple visual variations, underperforming even on basic object recognition tasks.
Presents a systematic analysis framework to identify the sources of these vulnerabilities, revealing issues like error accumulation and inadequate multimodal alignment.
Highlights the need for architectural innovations to improve robustness in future LVLM designs.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16727</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs</title><link>https://arxiv.org/abs/2504.11711</link><description>Introduces BugLens, a framework that uses LLMs to refine static analysis results for software vulnerability detection.
Addresses the precision-scalability tradeoff in static analysis, particularly for large codebases like the Linux kernel.
Demonstrates significant reduction in false positives and discovery of new vulnerabilities using LLM-guided reasoning.
Highlights the potential of automated LLM-based workflows to enhance traditional security analysis tools.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11711</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title><link>https://arxiv.org/abs/2504.06868</link><description>Investigates how embedding human-like personality traits in AI agents affects their behavior and performance in text-based games.
Introduces PANDA, a method for projecting personality traits onto agents to guide decision-making.
Finds that certain personality types, such as high Openness, can improve agent performance and alignment with human-like behaviors.
Highlights the potential for personality-adapted agents to foster more aligned and human-centric AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06868</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</title><link>https://arxiv.org/abs/2504.01919</link><description>Surveys recent advances in using large language models (LLMs) for machine translation, especially for low-resource languages.
Analyzes techniques like few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning to improve translation quality.
Discusses challenges such as hallucinations, evaluation inconsistencies, and inherited biases in LLM-based translation.
Explores robustness and reliability concerns, and outlines future directions for building more robust and inclusive MT systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01919</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates</title><link>https://arxiv.org/abs/2504.01225</link><description>Proposes a conformal risk control framework to calibrate uncertainty in CLIPScore-based image caption evaluation.
Addresses the lack of granular error assessment and uncertainty quantification in current captioning metrics.
Demonstrates improved detection of erroneous words and provides formal risk guarantees.
Enhances reliability and robustness of automated caption evaluation metrics.</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01225</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models</title><link>https://arxiv.org/abs/2503.22877</link><description>Evaluates the factual accuracy of LLM-based fact-checking across different geographic regions.
Compares open and private LLMs (including GPT-4, Claude Sonnet, LLaMA) using three experimental setups: statement-only, Wikipedia agent, and RAG with official fact checks.
Finds significant performance disparities, with statements from the Global North being fact-checked more accurately than those from the Global South.
Highlights the need for better dataset balancing and robust retrieval strategies to improve LLM reliability and fairness.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.22877</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title><link>https://arxiv.org/abs/2503.20756</link><description>Introduces ADS-Edit, a multimodal knowledge editing dataset tailored for autonomous driving systems (ADS).
Focuses on knowledge editing techniques to enable targeted modifications in large multimodal models without full retraining.
Addresses challenges in ADS such as misunderstanding of traffic knowledge and complex road conditions.
Provides comprehensive evaluation metrics and real-world scenarios to assess model behavior changes.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20756</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Opportunities and Challenges of Frontier Data Governance With Synthetic Data</title><link>https://arxiv.org/abs/2503.17414</link><description>Examines governance and accountability challenges posed by the use of synthetic data generated by machine learning models.
Identifies risks such as enabling malicious actors, spontaneous biases, and value drift in synthetic data.
Proposes three technical mechanisms to address these challenges, including adversarial training, bias mitigation, and value reinforcement.
Discusses the implications of synthetic data for future data and compute governance paradigms.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17414</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Position: Beyond Assistance - Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care</title><link>https://arxiv.org/abs/2503.16456</link><description>Proposes a shift from using LLMs as assistive tools to ethical and adaptive co-creators in mental health care.
Identifies key challenges such as bias, over-reliance, dehumanization, and regulatory uncertainties in deploying LLMs for mental health.
Introduces the SAFE-i Guidelines for ethical, fair, and responsible LLM deployment, focusing on data governance and adaptive engineering.
Presents the HAAS-e Framework for human-centered safety evaluation, including metrics for trustworthiness, empathy, and cultural sensitivity.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16456</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection</title><link>https://arxiv.org/abs/2503.16072</link><description>Proposes a new, context-aware framework for detecting toxicity in text, treating toxicity as a socially emergent stress signal.
Introduces a formal definition and metric for toxicity that incorporates contextual factors.
Validates the approach on a novel dataset, showing improved sensitivity and adaptability compared to traditional models.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16072</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance</title><link>https://arxiv.org/abs/2503.13509</link><description>Introduces MentalChat16K, a benchmark dataset for conversational mental health assistance using AI.
Combines synthetic and anonymized real-world counseling transcripts, covering conditions like depression, anxiety, and grief.
Emphasizes patient privacy, ethical considerations, and responsible data usage in dataset creation.
Aims to support the development and evaluation of empathetic, safe, and responsible AI models for mental health support.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.13509</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2503.04800</link><description>Introduces HoH, a benchmark for evaluating the impact of outdated information in Retrieval-Augmented Generation (RAG) systems.
Demonstrates that outdated information in retrieval sources can significantly degrade the accuracy and reliability of RAG outputs.
Finds that outdated information can mislead models into generating potentially harmful outputs, highlighting a safety risk.
Emphasizes the need for new solutions to address temporal challenges and information freshness in RAG systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04800</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Causally Reliable Concept Bottleneck Models</title><link>https://arxiv.org/abs/2503.04363</link><description>Introduces Causally reliable Concept Bottleneck Models (C$^2$BMs), which structure concept-based models according to real-world causal mechanisms.
Proposes a pipeline to automatically learn causal structures from observational data and background knowledge.
Demonstrates improved interpretability, causal reliability, and responsiveness to interventions compared to standard models.
Addresses limitations in out-of-distribution generalization and fairness in existing concept-based and opaque neural models.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04363</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</title><link>https://arxiv.org/abs/2503.03480</link><description>Proposes an integrated safety approach (ISA) for aligning vision-language-action (VLA) models with explicit safety constraints.
Utilizes constrained reinforcement learning (CMDP) to optimize VLA policies against elicited safety risks.
Demonstrates significant safety improvements and robust generalization to out-of-distribution scenarios.
Introduces a new benchmark environment and provides open access to data and models.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03480</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models</title><link>https://arxiv.org/abs/2503.01854</link><description>Provides a comprehensive survey of machine unlearning techniques specifically for large language models (LLMs).
Discusses methods for removing undesirable or sensitive data from LLMs without full retraining, which is crucial for privacy and compliance.
Categorizes and evaluates current unlearning approaches, including their strengths, limitations, and assessment methodologies.
Identifies challenges and future research directions in the field of LLM unlearning.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01854</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>POPGym Arcade: Parallel Pixelated POMDPs</title><link>https://arxiv.org/abs/2503.01450</link><description>Introduces POPGym Arcade, a suite of environments for studying partially observable Markov decision processes (POMDPs) with pixel-based inputs.
Provides tools for analyzing agent policies under partial observability, focusing on memory and decision-making.
Finds that agents with long-term memory can develop brittle policies that fail to generalize.
Demonstrates that recurrent policies are vulnerable to being 'poisoned' by old, out-of-distribution observations, raising concerns for sim-to-real transfer and offline RL.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01450</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems</title><link>https://arxiv.org/abs/2503.00600</link><description>Introduces semantic integrity constraints (SICs) as a declarative method to specify and enforce correctness in AI-augmented data processing systems.
SICs generalize traditional database integrity constraints to handle semantic operations performed by large language models (LLMs).
Proposes proactive and reactive enforcement strategies to ensure reliability and trustworthiness of LLM outputs in data pipelines.
Discusses system design, integration, and open research challenges for deploying SICs in enterprise-scale AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00600</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach</title><link>https://arxiv.org/abs/2503.00206</link><description>Proposes a methodology to detect violations of the Markov property in reinforcement learning (RL) using causal discovery and a new Markov Violation Score (MVS).
Demonstrates how observation noise and omission of state dimensions affect RL performance and Markov consistency in classic control tasks.
Finds that certain state variables are causally essential for maintaining effective RL, highlighting the need to safeguard these dimensions.
Provides a framework for developing more robust RL policies and addressing partial observability, with implications for real-world RL system reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00206</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DIS-CO: Discovering Copyrighted Content in VLMs Training Data</title><link>https://arxiv.org/abs/2502.17358</link><description>Introduces DIS-CO, a method to detect whether copyrighted content was included in the training data of large vision-language models (VLMs) without direct access to the data.
Uses targeted queries and free-form text completions to infer the presence of copyrighted material in VLMs.
Presents MovieTection, a benchmark dataset for evaluating detection methods using frames from films before and after model training cutoffs.
Finds that all tested VLMs show evidence of exposure to copyrighted content, raising concerns about data provenance and legal compliance.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17358</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Data-Constrained Synthesis of Training Data for De-Identification</title><link>https://arxiv.org/abs/2502.14677</link><description>Addresses privacy concerns in sensitive domains (e.g., clinical data) by generating synthetic datasets using LLMs.
Uses machine-annotated synthetic data for training Named Entity Recognition (NER) models to de-identify personal information.
Evaluates the effectiveness of synthetic data for NER model training, showing minimal performance loss.
Highlights the importance of the quality of original NER models for successful data synthesis and de-identification.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14677</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SEA-HELM: Southeast Asian Holistic Evaluation of Language Models</title><link>https://arxiv.org/abs/2502.14301</link><description>Introduces SEA-HELM, a comprehensive evaluation suite for assessing LLMs in Southeast Asian languages.
Benchmarks include five pillars, one of which is 'Safety', indicating an explicit focus on evaluating LLM safety.
Supports multiple SEA languages and provides a leaderboard for systematic comparison.
Aims to address the gap in culturally and linguistically representative LLM evaluation, including safety aspects.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14301</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</title><link>https://arxiv.org/abs/2502.13775</link><description>Introduces VITAL, a new benchmark dataset for evaluating pluralistic alignment in healthcare-related LLM outputs.
Highlights the importance of accommodating diverse cultural, religious, and personal values in AI alignment, especially in health contexts.
Evaluates eight LLMs and finds that current pluralistic alignment methods are insufficient for healthcare scenarios.
Emphasizes the need for domain-specific alignment solutions to ensure AI systems respect diverse perspectives in sensitive areas like health.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13775</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HumT DumT: Measuring and controlling human-like language in LLMs</title><link>https://arxiv.org/abs/2502.13259</link><description>Introduces HumT and SocioT, new metrics to measure human-like tone and social perceptions in LLM outputs.
Finds that users often prefer less human-like outputs from LLMs, challenging assumptions about anthropomorphism.
Shows that human-like outputs are correlated with perceptions that may lead to harms such as deception or stereotyping.
Proposes DumT, a method to control and reduce human-like tone in LLMs, aiming to mitigate risks associated with anthropomorphic language.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13259</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS</title><link>https://arxiv.org/abs/2502.12267</link><description>Proposes NeuroStrata, a neurosymbolic framework aimed at improving the design, testability, and verifiability of autonomous cyber-physical systems (CPS).
Addresses trust and safety certification challenges in AI-driven CPS by introducing interpretable symbolic AI components.
Focuses on enhancing verification and testing processes for autonomous systems, which are critical for safety and reliability.
Discusses early results and outlines future directions for the framework.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12267</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Diversity-oriented Data Augmentation with Large Language Models</title><link>https://arxiv.org/abs/2502.11671</link><description>Proposes a diversity-oriented data augmentation framework (DoAug) using large language models (LLMs) to generate more diverse paraphrases for NLP datasets.
Addresses the issue of insufficient sample distribution diversity in existing data augmentation methods, which can lead to model overfitting.
Demonstrates that the approach improves dataset diversity and robustness, leading to better performance on downstream NLP tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11671</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training</title><link>https://arxiv.org/abs/2502.11191</link><description>Introduces Primus, a comprehensive collection of open-source datasets specifically designed for training large language models (LLMs) in cybersecurity.
Covers all major LLM training stages: pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-focused data.
Demonstrates significant performance improvements on public cybersecurity benchmarks and security certification tasks using these datasets.
Releases both datasets and trained cybersecurity LLMs under permissive open-source licenses to foster further research.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11191</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection</title><link>https://arxiv.org/abs/2502.09254</link><description>Proposes AnomalyGFM, a graph foundation model designed for zero-shot and few-shot anomaly detection in graphs.
Addresses the challenge of generalizing anomaly detection across diverse graph domains by learning graph-agnostic representations.
Utilizes pre-training to align normal and abnormal class prototypes with node representation residuals for consistent anomaly measurement.
Demonstrates significant performance improvements over state-of-the-art methods on multiple real-world graph anomaly detection datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09254</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2502.08826</link><description>Surveys the field of Multimodal Retrieval-Augmented Generation (RAG), which integrates external dynamic information from multiple modalities (text, images, audio, video) to improve LLM outputs.
Discusses challenges such as cross-modal alignment, reasoning, and robustness in multimodal RAG systems.
Covers evaluation metrics, datasets, methodologies, and robustness enhancements relevant to building reliable AI systems.
Highlights open challenges and future directions, including reliability and robustness, which are pertinent to AI safety and security.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08826</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming</title><link>https://arxiv.org/abs/2502.06976</link><description>Proposes 'constructive interdependence' as a new metric to evaluate cooperation in human-AI teams, beyond just task completion.
Analyzes how much agents rely on each other's actions to achieve shared goals, using STRIPS formalism.
Finds that high task rewards do not necessarily indicate effective cooperation or interdependence in human-AI teams.
Highlights the need for more nuanced evaluation metrics for human-AI teaming, which has implications for safe and reliable deployment of AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.06976</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM Safety Alignment is Divergence Estimation in Disguise</title><link>https://arxiv.org/abs/2502.00657</link><description>Presents a theoretical framework connecting LLM alignment methods (like RLHF) to divergence estimation between safe and unsafe distributions.
Introduces KLDO, a new KL divergence-based alignment method, and demonstrates its empirical effectiveness.
Finds that compliance-refusal datasets improve safety alignment compared to standard preference datasets.
Proposes a distance-based metric in prompt representation space as a quantitative indicator of model safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00657</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robust Multimodal Learning via Cross-Modal Proxy Tokens</title><link>https://arxiv.org/abs/2501.17823</link><description>Proposes a method (cross-modal proxy tokens) to improve robustness of multimodal models when some modalities are missing at inference.
Uses low-rank adapters and alignment loss to efficiently approximate missing modality information without extra networks.
Demonstrates improved performance over state-of-the-art baselines on multiple datasets under various missing modality scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.17823</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning</title><link>https://arxiv.org/abs/2501.17077</link><description>Proposes a new pipeline for interpretability in reinforcement learning (RL) by focusing on functional modularity rather than individual neurons.
Introduces methods to induce, detect, and characterize functional modules within RL policy networks using sparsity and locality in network weights.
Develops an extended Louvain algorithm with a novel correlation alignment metric for module detection.
Demonstrates the approach in MiniGrid environments and validates module functions through network interventions.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.17077</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review</title><link>https://arxiv.org/abs/2501.10727</link><description>Highlights the importance of dataset quality, annotation, and metadata in medical imaging AI research.
Introduces a 'living review' framework to continuously track and update research artifacts (e.g., biases, shortcuts) associated with public medical imaging datasets.
Discusses best practices for dataset creation, annotation, and lifecycle management, emphasizing the impact on algorithm generalizability and patient outcomes.
Addresses issues such as dataset biases, shortcuts, and demographic diversity, which are critical for AI robustness and safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.10727</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models</title><link>https://arxiv.org/abs/2501.04945</link><description>Proposes methods to enhance large language models' (LLMs) ability to follow soft constraints in instructions.
Introduces a pipeline for automatically constructing datasets with high-quality outputs for training.
Utilizes Direct Preference Optimization (DPO) and curriculum learning based on constraint quantity to improve model performance.
Evaluates the effectiveness of these methods and analyzes factors contributing to improved constraint adherence.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.04945</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability</title><link>https://arxiv.org/abs/2412.18053</link><description>Introduces the Neuron Empirical Gradient (NEG) to quantify how neuron activations in pre-trained language models (PLMs) influence outputs.
Proposes NeurGrad, a method for efficient large-scale analysis of neuron behavior in PLMs.
Demonstrates that NEG can capture language skills and represent model knowledge across diverse prompts.
Highlights properties of NEG-based skill representation, including efficiency, robustness, and flexibility.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18053</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph</title><link>https://arxiv.org/abs/2412.15268</link><description>Proposes MetaTox, a method that enhances LLM-based toxicity detection using a meta-toxic knowledge graph.
Addresses key challenges in toxicity detection: reducing false negatives due to lack of domain knowledge and minimizing false positives caused by LLM oversensitivity.
Utilizes LLMs to construct a knowledge graph from toxic benchmark datasets, improving detection accuracy through graph-based retrieval and ranking.
Demonstrates improved performance and reduced false positives in toxicity detection across multiple datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15268</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation</title><link>https://arxiv.org/abs/2412.10424</link><description>Introduces a dynamic evaluation framework (LLM-as-an-Interviewer) for large language models using interactive, multi-turn interviews.
The framework mitigates data contamination by dynamically modifying datasets and generating initial questions.
Provides deeper insights into LLM performance, adaptability, and real-world applicability compared to static evaluation methods.
Addresses limitations of existing evaluation methods, such as verbosity bias and inconsistency, and offers a comprehensive Interview Report.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10424</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World</title><link>https://arxiv.org/abs/2412.01617</link><description>Analyzes user interactions with ChatGPT, focusing on cases where users express loneliness or seek emotional support.
Finds that ChatGPT often fails in sensitive scenarios, such as responding to suicidal ideation or trauma, raising safety concerns.
Reports a significant incidence of toxic content, with women disproportionately targeted, highlighting ethical and security risks.
Discusses broader ethical, legal, and societal risks, including potential for radicalization and further isolation, and provides recommendations for mitigation.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01617</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title><link>https://arxiv.org/abs/2411.18688</link><description>Identifies a safety gap in current multimodal large language models (MLLMs) where training-time alignment is insufficient to prevent jailbreak attacks.
Proposes 'Immune', an inference-time defense framework using a safe reward model and controlled decoding to mitigate jailbreak attacks.
Provides mathematical analysis explaining why Immune improves safety against jailbreaks.
Demonstrates significant reduction in jailbreak attack success rates on recent MLLMs through extensive benchmarking.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18688</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection</title><link>https://arxiv.org/abs/2411.10918</link><description>Introduces INVARLLM, a framework that leverages large language models (LLMs) to extract physical invariants from cyber-physical systems (CPS) documentation for anomaly detection.
Combines LLM-driven semantic extraction with empirical validation using a PCMCI+-inspired K-means method to ensure interpretability and reliability.
Demonstrates superior anomaly detection performance (100% precision, no false alarms) on SWaT and WADI datasets compared to existing methods.
Addresses CPS security by automating invariant extraction, reducing manual effort, and improving detection of cyber-physical attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10918</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FactLens: Benchmarking Fine-Grained Fact Verification</title><link>https://arxiv.org/abs/2411.05980</link><description>Introduces FactLens, a benchmark for fine-grained fact verification in LLM-generated content.
Advocates for breaking down complex claims into sub-claims for more precise verification and transparency.
Presents metrics and automated evaluators for assessing sub-claim quality, with manual curation for ground truth.
Finds alignment between automated evaluators and human judgments, discussing implications for verification performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.05980</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</title><link>https://arxiv.org/abs/2411.00355</link><description>Proposes TextDestroyer, a method for removing text from images using a pre-trained diffusion model without requiring additional training or annotation.
Addresses privacy concerns by ensuring that removed text cannot be reconstructed or recognized, improving content concealment.
Demonstrates superior performance in destroying text traces compared to existing methods, with strong generalization to various image types.
Focuses on privacy protection and secure data handling in visual content.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.00355</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning</title><link>https://arxiv.org/abs/2410.17933</link><description>Proposes a blockchain-enabled federated learning framework for healthcare modeling across multiple continents without sharing raw data.
Addresses privacy and safety requirements for sensitive healthcare data using federated learning and blockchain.
Implements an on-chain incentive mechanism to reward honest participation and penalize malicious activities.
Demonstrates improved prediction accuracy and privacy preservation compared to models trained on limited or centralized data.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17933</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models</title><link>https://arxiv.org/abs/2410.17714</link><description>Proposes a cognition-inspired method for selectively intervening in LLM layers to steer model outputs.
Introduces a technique to steer LLMs away from toxic outputs during inference, enhancing safety.
Improves interpretability and efficiency of LLM fine-tuning for safer deployment.
Demonstrates effectiveness across multiple LLM architectures and tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17714</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LoGU: Long-form Generation with Uncertainty Expressions</title><link>https://arxiv.org/abs/2410.14309</link><description>Introduces the LoGU task, focusing on enabling LLMs to express uncertainty in long-form generation.
Identifies key challenges: Uncertainty Suppression and Uncertainty Misalignment in LLM outputs.
Proposes a data collection and training framework to improve accurate uncertainty expression and reduce hallucinations.
Demonstrates that the approach improves factual accuracy and reduces hallucinations in long-form responses.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14309</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title><link>https://arxiv.org/abs/2410.07176</link><description>Analyzes the vulnerabilities of Retrieval Augmented Generation (RAG) in LLMs due to imperfect, irrelevant, or malicious retrievals.
Identifies knowledge conflicts between LLMs' internal knowledge and external retrieved information as a key challenge.
Proposes Astute RAG, a method that improves robustness by adaptively consolidating internal and external knowledge with source-awareness and reliability assessment.
Demonstrates improved trustworthiness and robustness of RAG systems, especially under adversarial or worst-case retrieval scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.07176</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>"Cold, Calculated, and Condescending": How AI Identifies and Explains Ableism Compared to Disabled People</title><link>https://arxiv.org/abs/2410.03448</link><description>Examines how AI models (toxicity classifiers and LLMs) identify and explain ableist speech compared to disabled people's perspectives.
Finds that AI underestimates toxicity and inconsistently assesses ableism, with explanations lacking nuance and accuracy.
Highlights the need for improved AI moderation systems and the inclusion of intersectional disabled perspectives in their design.
Discusses challenges and opportunities for responsible AI moderation regarding ableism.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03448</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data</title><link>https://arxiv.org/abs/2409.19078</link><description>Investigates the application of differential privacy (DP) to deep learning models for pathological speech analysis, addressing privacy concerns in sensitive medical data.
Demonstrates that DP can effectively mitigate privacy risks such as gradient inversion attacks, which can reconstruct identifiable speech samples from non-private models.
Analyzes the trade-offs between privacy, diagnostic accuracy, and fairness, showing minimal gender bias but highlighting age-related disparities under DP constraints.
Validates the approach across languages and disorders, suggesting that careful pretraining can maintain accuracy even with strong privacy guarantees.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.19078</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Banking Dataset: Understanding Client Needs through Event Sequences</title><link>https://arxiv.org/abs/2409.17587</link><description>Introduces a large, anonymized, multimodal banking dataset (MBD) with over 2 million clients and multiple data sources.
Addresses privacy and security concerns by anonymizing sensitive financial and personal data.
Benchmarks event sequence modeling techniques, including large language models, on practical financial tasks.
Facilitates research in secure and privacy-preserving AI applications in the financial sector.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.17587</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</title><link>https://arxiv.org/abs/2409.14507</link><description>Investigates the robustness of feature decomposition in Sparse Autoencoders (SAEs) applied to large language models.
Identifies and analyzes the phenomenon of 'feature absorption,' where hierarchical features are not robustly split and can be absorbed by their sub-features.
Introduces a metric to detect feature absorption and empirically validates findings across many LLM SAEs.
Discusses implications for the interpretability and reliability of SAEs in understanding LLMs, highlighting challenges for robust model interpretation.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.14507</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs</title><link>https://arxiv.org/abs/2409.05806</link><description>Introduces CKnowEdit, a new dataset for editing and correcting linguistic, factual, and logical errors in Chinese LLMs.
Focuses on culturally specific elements such as ancient poetry, idioms, and proverbs, highlighting unique challenges for LLMs.
Evaluates current knowledge editing techniques and identifies opportunities for improving LLM robustness in Chinese.
Aims to enhance the reliability and accuracy of LLMs in handling complex, culturally grounded knowledge.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.05806</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Content Moderation by LLM: From Accuracy to Legitimacy</title><link>https://arxiv.org/abs/2409.03219</link><description>Argues that current LLM-based content moderation focuses too much on accuracy and neglects legitimacy and governance aspects.
Proposes a legitimacy-based framework for evaluating LLM moderators, emphasizing transparency, justification, and user participation.
Identifies new roles for LLMs in moderation: distinguishing hard from easy cases, providing explanations, assisting human reviewers, and facilitating user engagement.
Suggests a workflow for integrating LLMs into content moderation, drawing on legal and social science theories.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.03219</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options</title><link>https://arxiv.org/abs/2409.00113</link><description>Proposes a framework to test LLMs' ability to handle multiple-choice questions with no correct answer, assessing robustness and critical reasoning.
Finds that alignment techniques can reduce models' ability to refuse invalid options, potentially impairing critical judgment.
Highlights a trade-off between alignment for helpfulness and the preservation of critical reasoning, with implications for real-world AI safety.
Includes human studies and ablation experiments to analyze the effects of model size, training, and prompting on robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.00113</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings</title><link>https://arxiv.org/abs/2407.04503</link><description>Investigates how information is transformed and potentially distorted as it passes through multiple LLMs in sequence.
Uses a 'telephone game' experimental setup to study the amplification of biases and emergence of attractor states in multi-turn LLM interactions.
Finds that certain properties, such as toxicity, are more susceptible to amplification and attractor effects than others.
Highlights the need to consider multi-step transmission dynamics for understanding and mitigating risks in LLM deployments.</description><guid isPermaLink="false">https://arxiv.org/abs/2407.04503</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</title><link>https://arxiv.org/abs/2406.04231</link><description>Introduces a computational model to quantify misalignment among multiple human and AI agents.
Addresses the complexity of alignment in sociotechnical systems, moving beyond single-agent or monolithic human perspectives.
Demonstrates the model's utility through simulations and real-world case studies, such as autonomous vehicles.
Aims to inform the design of more aligned AI systems by providing a nuanced understanding of agent misalignment.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.04231</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness</title><link>https://arxiv.org/abs/2405.18915</link><description>Analyzes the effectiveness and faithfulness of Chain-of-Thought (CoT) prompting in large language models.
Identifies key factors influencing CoT performance, such as problem difficulty, information gain, and information flow.
Highlights issues where CoT reasoning may be unfaithful, with models recalling correct information not present in the generated CoT.
Proposes and evaluates a new algorithm to improve CoT faithfulness and effectiveness by enhancing information recall.</description><guid isPermaLink="false">https://arxiv.org/abs/2405.18915</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models</title><link>https://arxiv.org/abs/2405.17820</link><description>Identifies a key source of hallucinations in Large Vision Language Models (LVLMs) as attention misalignment towards irrelevant image regions ('blind tokens').
Proposes Attentional Vision Calibration (AvisC), a test-time method to recalibrate attention and reduce the influence of blind tokens without modifying model architecture.
Demonstrates that AvisC reduces hallucinations in LVLMs across multiple benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2405.17820</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions</title><link>https://arxiv.org/abs/2405.03205</link><description>Investigates positional (anchored) bias in GPT-2 models when answering multiple-choice questions, where the model disproportionately favors the first answer choice.
Uses mechanistic interpretability techniques to identify and modify internal model components (MLP layers and attention heads) responsible for this bias.
Proposes targeted interventions to mitigate the bias, resulting in improved robustness and accuracy of GPT-2 on MCQ tasks.
Provides open-source code for reproducibility and further research.</description><guid isPermaLink="false">https://arxiv.org/abs/2405.03205</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment</title><link>https://arxiv.org/abs/2405.00557</link><description>Proposes the Mixture of insighTful Experts (MoTE) framework to enhance self-alignment in large language models (LLMs).
Combines structured reasoning chains and Mixture-of-Experts (MoE) architectures to improve safety and alignment.
Introduces a multi-step reasoning process (Question Analysis, Answer Guidance, Safe Answer, Safety Checking) to bolster model safety and jailbreak resistance.
Demonstrates improved safety, jailbreak resistance, and over-refusal capabilities, with performance comparable to leading models.</description><guid isPermaLink="false">https://arxiv.org/abs/2405.00557</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs</title><link>https://arxiv.org/abs/2404.10508</link><description>Introduces the LABE benchmark to evaluate social biases in language agency within LLM-generated content, focusing on gender, racial, and intersectional biases.
Finds that LLMs (ChatGPT, Llama3, Mistral) exhibit greater gender and intersectional bias in agency attribution compared to human-written texts.
Demonstrates that prompt-based mitigation strategies are unstable and can worsen biases.
Proposes a new mitigation method, Mitigation via Selective Rewrite (MSR), which uses an agency classifier to selectively revise biased text, showing improved effectiveness over prompt-based methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2404.10508</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</title><link>https://arxiv.org/abs/2402.04435</link><description>Proposes PreGIP, a novel framework for watermarking the pretraining phase of Graph Neural Networks (GNNs) to protect intellectual property.
Introduces a task-free watermarking loss to embed watermarks in the GNN encoder's embedding space, making it applicable to self-supervised pretraining.
Implements a finetuning-resistant watermark injection to ensure robustness against attempts to remove the watermark.
Provides theoretical analysis and experimental results demonstrating the effectiveness of the approach in IP protection without degrading downstream task performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2402.04435</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors</title><link>https://arxiv.org/abs/2401.16310</link><description>Empirically evaluates the effectiveness of large language models (LLMs) in security code review tasks.
Compares six LLMs and five prompt strategies against state-of-the-art static analysis tools for detecting security defects.
Finds that LLMs, especially GPT-4, outperform traditional tools but still have limitations such as verbosity and non-compliance with task requirements.
Analyzes factors influencing LLM performance, including code complexity and developer involvement.</description><guid isPermaLink="false">https://arxiv.org/abs/2401.16310</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning</title><link>https://arxiv.org/abs/2401.13796</link><description>Examines the risks of data leakage in machine learning workflows, especially when used by non-experts.
Categorizes different types of data leakage and how they can propagate through ML pipelines.
Explores data leakage in the context of transfer learning and compares its impact in inductive vs. transductive ML frameworks.
Emphasizes the importance of addressing data leakage for robust and reliable ML applications.</description><guid isPermaLink="false">https://arxiv.org/abs/2401.13796</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Nash Equilibria, Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization</title><link>https://arxiv.org/abs/2303.03900</link><description>Analyzes distributionally robust optimization (DRO) using optimal transport, where an adversary can reshape distributions within a transportation cost constraint.
Establishes connections between robustification and regularization techniques such as variation and Lipschitz regularization.
Explores the existence and computation of Nash equilibria between a decision-maker and an adversarial 'nature', showing that adversarial strategies can generate deceptive samples.
Identifies efficient computational methods for solving these DRO problems, even under nonconvexity in either the loss or cost function.</description><guid isPermaLink="false">https://arxiv.org/abs/2303.03900</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title><link>https://arxiv.org/abs/2505.23436</link><description>Analyzes how rational AI agents behave under resource or failure constraints, which can force early termination of actions.
Identifies how these constraints can lead to misalignment between human objectives and agent incentives, potentially causing unexpected or unsafe behaviors.
Introduces a survival bandit framework to formalize and study these effects, providing both theoretical and empirical results.
Proposes mechanisms to mitigate emergent risk-seeking or risk-averse behaviors, aiming to improve safe deployment of AI agents in resource-limited environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23436</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy</title><link>https://arxiv.org/abs/2505.23397</link><description>Proposes a structured framework for integrating Human-AI collaboration in Security Operations Centers (SOCs), focusing on trust calibration and adaptive autonomy.
Introduces a five-level autonomy model, mapping AI autonomy to Human-in-the-Loop roles and task-specific trust thresholds.
Demonstrates the framework with a simulated cyber range using an LLM-based SOC assistant (AI-Avatar), highlighting improvements in alert triage, response coordination, and trust management.
Addresses operational complexity and risk in SOCs, aiming to enhance rather than replace human decision-making with AI.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23397</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy</title><link>https://arxiv.org/abs/2505.21907</link><description>Surveys methods for modeling and optimizing user preferences in AI copilots, focusing on personalization and alignment.
Introduces a taxonomy of preference optimization techniques across different interaction phases (pre-, mid-, post-interaction).
Discusses the sourcing, modeling, and refinement of user preference signals, and evaluates the design implications for AI copilots.
Aims to unify research across AI personalization, human-AI interaction, and language model adaptation for more user-aligned AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21907</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Many Challenges of Human-Like Agents in Virtual Game Environments</title><link>https://arxiv.org/abs/2505.20011</link><description>Surveys key conceptual and technical challenges in creating human-like AI agents in virtual game environments.
Discusses the importance of distinguishing between human players and AI agents, especially in contexts where bots are prohibited.
Presents an empirical study using a deep learning approach to differentiate between human and AI players based on behavioral data.
Highlights the interplay between the difficulty of creating human-like AI and the ease of detecting non-human agents.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20011</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs</title><link>https://arxiv.org/abs/2505.19165</link><description>Introduces OrgAccess, a benchmark for evaluating LLMs' ability to follow role-based access control (RBAC) and organizational hierarchies.
Tests LLMs on synthetic but representative permission scenarios, including overlapping and conflicting permissions.
Finds that even advanced LLMs like GPT-4.1 struggle to comply with complex RBAC rules, especially in challenging scenarios.
Highlights significant limitations in LLMs' rule-following and compositional reasoning, raising concerns for their deployment in structured, security-sensitive environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19165</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment</title><link>https://arxiv.org/abs/2505.14667</link><description>Introduces SAFEPATH, a lightweight alignment method for Large Reasoning Models (LRMs) to prevent harmful reasoning.
SAFEPATH uses a short Safety Primer at the start of reasoning to reduce harmful outputs without degrading reasoning performance.
Demonstrates significant reduction in harmful responses and jailbreak attempts, with much lower computational cost compared to existing methods.
Provides analysis of generalization gaps in current safety methods for reasoning-centric models and suggests new directions for safer AI.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14667</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</title><link>https://arxiv.org/abs/2505.08459</link><description>Introduces a Strategy-Augmented Planning (SAP) framework to enhance LLM-based agents' ability to model and exploit opponents in adversarial domains.
SAP uses a Strategy Evaluation Network (SEN) to recognize and exploit opponent strategies, improving decision-making in competitive environments.
Demonstrates robust generalization, allowing the system to perform well against both known and novel opponent strategies.
Shows significant performance improvements in the MicroRTS environment, outperforming baseline methods and matching reinforcement learning approaches.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.08459</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How well do LLMs reason over tabular data, really?</title><link>https://arxiv.org/abs/2505.07453</link><description>Evaluates the robustness of large language models (LLMs) when reasoning over tabular data, especially under real-world variations such as missing values, duplicate entities, and structural changes.
Critiques existing evaluation strategies for tabular reasoning, highlighting their limitations and proposing an LLM-as-a-judge approach for more reliable assessment.
Finds that LLMs' tabular reasoning performance significantly degrades with realistic input variations, indicating a need for improved robustness.
Emphasizes the importance of robust evaluation and model improvement for practical deployment in scenarios involving tabular data.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07453</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Governance (HAIG): A Trust-Utility Approach</title><link>https://arxiv.org/abs/2505.01651</link><description>Introduces the HAIG (Human-AI Governance) framework for analyzing trust dynamics in evolving human-AI relationships.
Argues that current categorical governance models are insufficient for capturing the complexity of modern AI systems, especially as they gain autonomy and emergent capabilities.
Proposes a trust-utility approach that focuses on maintaining appropriate trust and maximizing utility while ensuring safeguards, rather than relying solely on risk or principle-based frameworks.
Includes case studies in healthcare and European regulation to demonstrate the framework's applicability and its ability to anticipate governance challenges.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01651</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Large Language and Reasoning Models are Shallow Disjunctive Reasoners</title><link>https://arxiv.org/abs/2503.23487</link><description>Examines the reasoning capabilities of Large Language Models (LLMs) and Large Reasoning Models (LRMs), particularly their ability to generalize to out-of-distribution (OOD) tasks.
Finds that both LLMs and LRMs struggle with multi-path reasoning and OOD generalization, relying on shallow reasoning shortcuts.
Highlights limitations in current post-training strategies (e.g., reinforcement learning, chain-of-thought prompting) for improving systematic reasoning in AI models.
Provides insights into the behavioral patterns of LLMs/LRMs, which has implications for their robustness and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2503.23487</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction</title><link>https://arxiv.org/abs/2502.18744</link><description>Introduces ZEBRA, a framework for constructing preference datasets for LLM alignment without manual or model-based annotation.
Leverages model behavioral knowledge from benchmark performances to generate preference data.
Enables scalable, cost-effective, and controllable alignment data generation.
Demonstrates that ZEBRA achieves alignment performance comparable to traditional instance-supervised methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18744</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values</title><link>https://arxiv.org/abs/2501.07071</link><description>Introduces Value Compass Benchmarks, a platform for evaluating the value alignment of large language models (LLMs).
Addresses limitations of current benchmarks by focusing on value clarification, evaluation validity, and value pluralism.
Proposes a generative, evolving evaluation framework that adapts to new LLMs and assesses behavioral conformity to values in realistic scenarios.
Introduces a metric for quantifying LLM alignment with specific values, accounting for pluralistic human values.</description><guid isPermaLink="false">https://arxiv.org/abs/2501.07071</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media</title><link>https://arxiv.org/abs/2412.18148</link><description>Investigates the prevalence and growth of AI-generated texts (AIGTs) on major social media platforms (Medium, Quora, Reddit).
Develops and benchmarks detectors for identifying AI-generated content using a new dataset (AIGTBench).
Finds significant increases in AI-generated content on Medium and Quora, with slower growth on Reddit.
Analyzes differences between AI-generated and human-written texts in terms of linguistic patterns, topics, engagement, and author profiles.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18148</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning</title><link>https://arxiv.org/abs/2412.13631</link><description>Examines how Theory of Mind (ToM) is evaluated in large language models (LLMs), highlighting a gap in current methodologies.
Distinguishes between recognizing when to apply ToM (Depth of Mentalizing) and executing the correct inference, arguing that most AI research focuses only on the latter.
Reviews various approaches to ToM in AI, including benchmarking, add-ons, probing, and formal models.
Proposes improved evaluation methods for ToM in AI, inspired by dynamic cognitive science tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13631</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>https://arxiv.org/abs/2412.13147</link><description>Identifies a gap between LLM benchmark performance and real-world reasoning reliability.
Introduces G-Pass@$k$, a new metric to evaluate both accuracy and consistency of LLMs across multiple attempts.
Demonstrates through experiments that current evaluation methods may overestimate LLMs' stable reasoning abilities.
Highlights the need for more robust evaluation protocols to ensure LLMs' operational reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13147</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stepwise Reasoning Error Disruption Attack of LLMs</title><link>https://arxiv.org/abs/2412.11934</link><description>Introduces the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into LLMs' reasoning steps.
SEED attack is effective in both zero-shot and few-shot settings and maintains the natural flow of reasoning.
Demonstrates the vulnerability of LLMs to covert disruptions in reasoning, highlighting a significant robustness and safety concern.
Extensive experiments show the generalizability and effectiveness of the attack across multiple datasets and models.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.11934</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Data Governance of Frontier AI Models</title><link>https://arxiv.org/abs/2412.03824</link><description>Explores the concept of 'frontier data governance' as a means to monitor and mitigate risks from advanced AI models.
Identifies challenges in governing data due to its properties (non-rival, non-excludable, easily replicable, synthesizable).
Proposes 15 governance mechanisms, with a focus on five underexplored policy recommendations such as canary tokens, automated data filtering, mandatory dataset reporting, improved dataset security, and know-your-customer requirements.
Aims to provide policymakers with new tools for regulating and governing frontier AI models through data-centric approaches.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.03824</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Evolution and Future Perspectives of Artificial Intelligence Generated Content</title><link>https://arxiv.org/abs/2412.01948</link><description>Reviews the historical evolution of AI-generated content (AIGC) from rule-based systems to modern transfer learning models.
Highlights the capabilities and limitations of AIGC methodologies across different developmental milestones.
Addresses critical challenges associated with AIGC and proposes strategies to mitigate them.
Aims to guide researchers and practitioners in optimizing AIGC models for quality and efficiency.</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01948</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge</title><link>https://arxiv.org/abs/2411.09689</link><description>Addresses the challenge of hallucination in large language models (LLMs), where unfaithful or fabricated text is generated.
Introduces a new task, hallucination probing, to classify LLM outputs as aligned, misaligned, or fabricated.
Proposes SHINE, a perturbation-driven method for hallucination detection that does not require external knowledge, supervised training, or LLM fine-tuning.
Demonstrates state-of-the-art performance in hallucination detection across multiple LLMs and datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2411.09689</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>G\"odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement</title><link>https://arxiv.org/abs/2410.04444</link><description>Introduces G"odel Agent, a self-referential agent framework inspired by the G"odel machine concept.
Enables recursive self-improvement of agents without reliance on predefined routines or fixed optimization algorithms.
Leverages large language models (LLMs) to dynamically modify agent logic and behavior based on high-level objectives.
Demonstrates superior performance, efficiency, and generalizability compared to manually crafted agents.</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04444</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends</title><link>https://arxiv.org/abs/2409.14457</link><description>Reviews the current state and enabling technologies of large model (LM) based intelligent agents.
Explores collaboration paradigms among LM agents, focusing on autonomous communication and cooperation.
Analyzes security vulnerabilities and privacy risks in LM agent systems, especially in multi-agent scenarios.
Discusses countermeasures and future research directions for building robust and secure LM agent ecosystems.</description><guid isPermaLink="false">https://arxiv.org/abs/2409.14457</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unexplainability of Artificial Intelligence Judgments in Kant's Perspective</title><link>https://arxiv.org/abs/2407.18950</link><description>Analyzes the unexplainability of AI judgments using Kant's epistemological framework.
Explores how AI systems, particularly through mechanisms like the SoftMax function, differ from human judgment in terms of explainability and modality.
Argues that the inherent limitations of natural language and functional implementation contribute to the unexplainability of AI decisions.</description><guid isPermaLink="false">https://arxiv.org/abs/2407.18950</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fact-Checking of AI-Generated Reports</title><link>https://arxiv.org/abs/2307.14634</link><description>Proposes a method for fact-checking AI-generated radiology reports by associating image data with report sentences.
Develops an examiner model that distinguishes real from fake findings in reports by learning image-text associations.
Creates a dataset of fake reports by perturbing ground truth findings to train the examiner.
Demonstrates the utility of the examiner in detecting and removing hallucinated or false sentences from AI-generated reports, promoting responsible AI use in clinical workflows.</description><guid isPermaLink="false">https://arxiv.org/abs/2307.14634</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</title><link>https://arxiv.org/abs/2506.01954</link><description>Introduces DRAG, a framework for distilling Retrieval-Augmented Generation (RAG) knowledge from large LLMs to smaller language models (SLMs).
Uses evidence- and knowledge graph-based distillation to improve factual accuracy and reduce hallucinations in SLMs.
Addresses user privacy risks and introduces a benchmark for evaluating privacy mitigation.
Demonstrates significant improvements over prior RAG methods for SLMs in terms of efficiency and reliability.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01954</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act</title><link>https://arxiv.org/abs/2506.01931</link><description>Presents a taxonomy and framework for analyzing 'avoision' strategiesactions that skirt the line between legal avoidance and evasionin response to the EU AI Act.
Organizes avoision strategies into three tiers based on regulatory exposure under the AIA.
Specifies both organizational and technological forms of avoision, providing a structured approach to adversarial analysis.
Aims to support red teaming of AI policy and regulation, helping anticipate and mitigate regulatory circumvention.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01931</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Image Generation from Contextually-Contradictory Prompts</title><link>https://arxiv.org/abs/2506.01929</link><description>Identifies a failure mode in text-to-image diffusion models called contextual contradiction, where prompts with contradictory concepts lead to semantically inaccurate images.
Proposes a stage-aware prompt decomposition framework that uses LLMs to analyze and resolve contradictions in prompts, improving semantic alignment during image generation.
Demonstrates improved alignment and control in generated images when handling contextually-contradictory prompts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01929</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MedEBench: Revisiting Text-instructed Image Editing</title><link>https://arxiv.org/abs/2506.01921</link><description>Introduces MedEBench, a benchmark for evaluating text-guided medical image editing.
Provides a clinically relevant evaluation framework focusing on editing accuracy, contextual preservation, and visual quality.
Performs systematic comparison and failure analysis of state-of-the-art models, identifying common failure patterns and mislocalization issues.
Aims to support the development of reliable and clinically meaningful medical image editing systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01921</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CiteEval: Principle-Driven Citation Evaluation for Source Attribution</title><link>https://arxiv.org/abs/2506.01829</link><description>Introduces CiteEval, a framework for evaluating the quality of citations in information-seeking AI systems.
Focuses on fine-grained, principle-driven assessment of citations, considering context, user queries, and generated text.
Presents CiteBench, a benchmark dataset with human annotations for citation quality.
Develops CiteEval-Auto, a suite of model-based metrics that correlate well with human judgments and outperform existing methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01829</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability</title><link>https://arxiv.org/abs/2506.01789</link><description>Highlights the challenges in ensuring high-quality datasets for machine learning, especially regarding annotation accuracy and transparency.
Critiques current practices like datasheets and metadata requirements for being insufficiently standardized or enforced.
Proposes DataRubrics, a rubric-based, systematic framework for automated and reproducible dataset quality assessment, leveraging LLM-based evaluation.
Addresses the need for scalable, cost-effective methods for evaluating both human- and model-generated datasets, supporting accountability and higher standards.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01789</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Systematic Hazard Analysis for Frontier AI using STPA</title><link>https://arxiv.org/abs/2506.01782</link><description>Evaluates the use of Systems-Theoretic Process Analysis (STPA) for hazard analysis in frontier AI systems.
Demonstrates how STPA can identify causal factors and unsafe control actions that may be missed by less structured methods.
Suggests that systematic hazard analysis can improve the robustness and traceability of AI safety assurance.
Proposes that STPA could complement existing AI governance and safety frameworks, and potentially scale with LLM assistance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01782</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs</title><link>https://arxiv.org/abs/2506.01770</link><description>Introduces ReGA, a model-based analysis framework using representation-guided abstraction to safeguard LLMs.
Targets safety and security issues such as harmful content generation and vulnerability to jailbreaking attacks.
Leverages safety-critical representations in LLM hidden states to improve scalability and interpretability of safety modeling.
Demonstrates strong performance and robustness to real-world attacks, outperforming existing safeguard paradigms.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01770</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When LLMs Team Up: The Emergence of Collaborative Affective Computing</title><link>https://arxiv.org/abs/2506.01698</link><description>Surveys collaborative systems using Large Language Models (LLMs) for affective computing tasks.
Analyzes how collaboration among LLMs can enhance robustness and adaptability in affective reasoning.
Discusses challenges such as misinterpretation of emotions, hallucinations, and the need for improved reliability.
Highlights future research directions for building more human-like, socially intelligent AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01698</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Provably Safe Reinforcement Learning from Analytic Gradients</title><link>https://arxiv.org/abs/2506.01665</link><description>Addresses the need for safety guarantees in reinforcement learning for autonomous robots in safety-critical applications.
Develops the first effective safeguard for analytic gradient-based reinforcement learning, a paradigm known for superior performance and sample efficiency.
Analyzes and adapts differentiable safeguards, integrating them into state-of-the-art learning algorithms and differentiable simulations.
Demonstrates through experiments that safeguarded training can be achieved without compromising performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01665</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Explainable AI Systems Must Be Contestable: Here's How to Make It Happen</title><link>https://arxiv.org/abs/2506.01662</link><description>Defines 'contestability' in explainable AI (XAI) and aligns it with regulatory and stakeholder requirements.
Proposes a modular framework for implementing contestability in AI systems, including technical, legal, and organizational mechanisms.
Introduces the Contestability Assessment Scale, a metric for evaluating contestability based on quantitative criteria.
Demonstrates the framework's effectiveness through case studies and provides practical guidance for embedding recourse and accountability in AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01662</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</title><link>https://arxiv.org/abs/2506.01631</link><description>Introduces TensorGuard, a gradient-based fingerprinting framework for detecting LLM similarity and classifying model families.
Addresses the challenge of unauthorized LLM derivations, such as fine-tuning, merging, and redistribution, which can violate licensing agreements.
Enables provenance tracking and enforcement of licensing compliance by identifying model lineage without relying on training data or watermarks.
Demonstrates high classification accuracy (94%) across multiple LLM families, supporting practical deployment for model governance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01631</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title><link>https://arxiv.org/abs/2506.01625</link><description>Addresses Gaussian Process optimization under adversarial perturbations.
Introduces a robust satisficing objective to consistently meet a performance threshold even in adversarial settings.
Proposes two novel algorithms with different guarantees based on adversary characteristics.
Provides theoretical regret bounds and demonstrates empirical superiority over traditional robust optimization methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01625</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>VirnyFlow: A Design Space for Responsible Model Development</title><link>https://arxiv.org/abs/2506.01584</link><description>Introduces VirnyFlow, a design space and system for responsible machine learning model development.
Allows users to define custom optimization criteria and align model development with real-world constraints.
Integrates evaluation protocols, multi-objective optimization, and cost-aware experimentation for more responsible ML pipelines.
Demonstrates improved optimization quality and scalability compared to existing AutoML systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01584</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes</title><link>https://arxiv.org/abs/2506.01512</link><description>Examines how large language models (LLMs) represent and express uncertainty, fact, and fiction.
Finds that LLMs have limited and unreliable performance in generating epistemic (uncertainty-related) expressions.
Highlights the need to improve LLMs' semantic understanding of epistemic modality for more reliable uncertainty-aware outputs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01512</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things</title><link>https://arxiv.org/abs/2506.01450</link><description>Proposes ShaTS, a Shapley-based explainability method tailored for time series AI models in industrial IoT anomaly detection.
Addresses the limitations of traditional explainability methods by preserving temporal dependencies in time series data.
Demonstrates improved precision and actionable insights for identifying critical events and affected components during anomalies.
Highlights the importance of explainability for operational safety and rapid mitigation of cyberincidents in industrial environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01450</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>System Calls for Malware Detection and Classification: Methodologies and Applications</title><link>https://arxiv.org/abs/2506.01412</link><description>Explores the use of system calls and API calls for malware detection and classification.
Covers methodologies such as static and dynamic analysis, sandboxing, and anomaly detection.
Discusses the integration of machine learning and statistical analysis to identify malicious behavior.
Examines evasion techniques used by sophisticated malware and applications across multiple operating systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01412</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Incentivizing LLMs to Self-Verify Their Answers</title><link>https://arxiv.org/abs/2506.01369</link><description>Proposes a framework for incentivizing large language models (LLMs) to self-verify their answers using reinforcement learning.
Unifies answer generation and verification within a single process, reducing reliance on external reward models.
Demonstrates improved performance and scalability on mathematical reasoning benchmarks.
Addresses distribution discrepancies between post-trained generators and general reward models, enhancing model robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01369</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors</title><link>https://arxiv.org/abs/2506.01357</link><description>Introduces KokoroChat, a Japanese psychological counseling dialogue dataset created via role-playing by trained counselors.
Addresses privacy and ethical concerns by avoiding real-world counseling data and using simulated interactions.
Demonstrates that fine-tuning LLMs with this dataset improves the quality and evaluation of counseling responses.
Highlights the importance of data quality and privacy in sensitive AI applications.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01357</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control</title><link>https://arxiv.org/abs/2506.01333</link><description>Identifies and addresses security vulnerabilities in the Model Context Protocol (MCP) used for integrating LLMs with external tools.
Introduces the Enhanced Tool Definition Interface (ETDI) to mitigate Tool Poisoning and Rug Pull attacks.
Proposes the use of OAuth 2.0, cryptographic identity verification, and immutable, versioned tool definitions for enhanced security.
Suggests policy-based, fine-grained access control for dynamic evaluation of tool permissions in AI ecosystems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01333</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines</title><link>https://arxiv.org/abs/2506.01329</link><description>Introduces PsyCrisisBench, a real-world benchmark for evaluating LLMs in psychological crisis detection using annotated hotline transcripts.
Assesses LLMs on tasks including mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment.
Finds that LLMs perform well on most crisis detection tasks, with fine-tuning and few-shot learning improving results; mood recognition remains challenging.
Highlights ethical considerations and the potential for responsible deployment of LLMs in sensitive mental health contexts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01329</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>$\Psi$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title><link>https://arxiv.org/abs/2506.01320</link><description>Presents the -Sampler, a new framework for inference-time reward alignment in score-based generative models using Sequential Monte Carlo (SMC).
Addresses limitations of existing methods by initializing particles from a reward-aware posterior rather than a Gaussian prior, improving sampling efficiency and alignment.
Introduces the preconditioned Crank-Nicolson Langevin (pCNL) algorithm for efficient posterior sampling in high-dimensional spaces.
Demonstrates improved performance on tasks such as layout-to-image generation and aesthetic-preference generation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01320</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack</title><link>https://arxiv.org/abs/2506.01318</link><description>Identifies two critical blind spots in machine unlearning: over-unlearning (collateral damage to retained data) and post-hoc relearning attacks.
Introduces a new metric (OU@) to quantify over-unlearning effects near the forget set.
Proposes the Prototypical Relearning Attack, which can restore forgotten knowledge with minimal data.
Presents 'Spotter', a defense mechanism that mitigates both over-unlearning and relearning attacks, validated on CIFAR-10.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01318</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models</title><link>https://arxiv.org/abs/2506.01307</link><description>Proposes a universal jailbreak attack framework targeting Multimodal Large Language Models (MLLMs) using both image and text modalities.
Demonstrates that multimodal interactions can be exploited as critical vulnerabilities, enabling higher-quality undesirable generations.
Evaluates the attack on several state-of-the-art MLLMs, revealing significant safety alignment issues and the inadequacy of current safety mechanisms.
Highlights the urgent need for improved and robust safety protocols to address the unique risks posed by multimodal AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01307</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model</title><link>https://arxiv.org/abs/2506.01266</link><description>Proposes a novel method for detoxifying large language models (LLMs) using a pre-trained calibration model fused at the output layer.
The approach steers LLMs away from generating toxic or harmful content by leveraging a detoxified embedding space.
The method is lightweight, does not compromise fluency or contextual understanding, and can be applied to multiple LLMs after a one-time calibration model training.
Experimental results show reduced toxicity in generated outputs while maintaining content quality.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01266</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models</title><link>https://arxiv.org/abs/2506.01257</link><description>Surveys the capabilities and clinical applications of DeepSeek-R1, an open-source large language model (LLM) in healthcare.
Highlights risks such as vulnerability to bias, misinformation, adversarial manipulation, and safety failures, especially in sensitive contexts.
Discusses the need for improved bias mitigation, safety alignment, and regulatory compliance for responsible deployment.
Emphasizes the importance of collaborative governance for equitable and safe use of open-source LLMs in healthcare.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01257</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine</title><link>https://arxiv.org/abs/2506.01252</link><description>Introduces MTCMB, a multi-task benchmark for evaluating large language models (LLMs) on knowledge, reasoning, and safety in Traditional Chinese Medicine (TCM).
Includes a dedicated safety evaluation component to assess LLMs' compliance and trustworthiness in medical contexts.
Finds that current LLMs struggle with clinical reasoning, prescription planning, and safety compliance in TCM.
Aims to guide the development of safer and more reliable medical AI systems by providing domain-specific evaluation tools.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01252</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess</title><link>https://arxiv.org/abs/2506.01242</link><description>Presents Obscuro, the first superhuman AI for Fog of War (FoW) chess, a challenging imperfect-information game.
Introduces new search techniques for reasoning under imperfect information, applicable to a broad class of games.
Demonstrates significant performance improvements over previous AI and human experts in FoW chess.
Advances the state of the art in AI for games requiring reasoning about hidden information and opponent knowledge.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01242</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs</title><link>https://arxiv.org/abs/2506.01227</link><description>Introduces SPEAR, a framework that uses AI planning to evaluate and improve network security posture.
Models network vulnerabilities and configurations using causal formalism and converts them into planning models.
Enables human-in-the-loop analysis, allowing administrators to perform what-if scenarios and explore diverse hardening strategies.
Focuses on making security recommendations understandable and actionable for system administrators.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01227</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Doubly Robust Alignment for Large Language Models</title><link>https://arxiv.org/abs/2506.01183</link><description>Proposes a new 'doubly robust' preference optimization algorithm for aligning large language models (LLMs) with human preferences.
Addresses the issue of model misspecification in RLHF (Reinforcement Learning from Human Feedback), which can lead to undesirable fine-tuning outcomes.
Demonstrates that the proposed method is more robust and consistent than existing algorithms, both theoretically and empirically.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01183</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Earley-Driven Dynamic Pruning for Efficient Structured Decoding</title><link>https://arxiv.org/abs/2506.01151</link><description>Proposes ZapFormat, a dynamic pruning strategy for efficient constrained decoding in LLMs using the Earley algorithm.
Addresses the challenge of ensuring LLM outputs conform to strict structural or grammatical constraints, important for function calls and DSL generation.
Demonstrates significant speed improvements and high-precision compliance in structured generation tasks such as JSON generation and schema validation.
Formatron, the implementation, is open source and generally applicable across LLM architectures.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01151</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</title><link>https://arxiv.org/abs/2506.01121</link><description>Introduces Neuro-Symbolic Diffusion (NSD), a framework combining diffusion models with symbolic optimization to enforce user-defined constraints.
Enables generation of outputs (images, trajectories, molecular structures, natural language) that are certifiably consistent with physical, structural, and operational constraints.
Demonstrates applications in safety-critical domains, such as non-toxic molecular generation and collision-free trajectory optimization.
Addresses challenges in safety, data scarcity, and out-of-domain generalization by enforcing symbolic constraints during generation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01121</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reconsidering LLM Uncertainty Estimation Methods in the Wild</title><link>https://arxiv.org/abs/2506.01114</link><description>Systematically evaluates 19 uncertainty estimation (UE) methods for large language models (LLMs) in real-world deployment scenarios.
Assesses robustness of UE methods to adversarial prompts, typos, prior chat history, and threshold sensitivity.
Finds that most UE methods are vulnerable to adversarial prompts and sensitive to calibration distribution shifts.
Proposes ensembling multiple UE scores as a practical strategy to improve robustness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01114</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</title><link>https://arxiv.org/abs/2506.01064</link><description>Proposes F3, a novel, training-free method to purify adversarial examples in large vision-language models (LVLMs).
F3 works by intentionally adding simple perturbations (noise) to adversarial examples, leveraging cross-modal attention to mitigate attack effects.
Demonstrates that this approach is computationally efficient and effective, making it suitable for large-scale, real-world applications.
Addresses a key vulnerability in LVLMs by improving robustness against visual adversarial attacks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01064</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models</title><link>https://arxiv.org/abs/2506.01062</link><description>Introduces SealQA, a benchmark for evaluating search-augmented language models on challenging fact-seeking questions with conflicting or noisy web results.
Highlights the poor performance and reasoning limitations of current frontier LLMs when faced with noisy or adversarial search environments.
Demonstrates that increasing computational resources does not reliably improve model robustness or accuracy in these settings.
Releases the SealQA dataset to facilitate further research on robustness and reasoning in search-augmented LLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01062</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>XAI-Units: Benchmarking Explainability Methods with Unit Tests</title><link>https://arxiv.org/abs/2506.01059</link><description>Introduces XAI-Units, a benchmark for evaluating feature attribution (FA) methods in explainable AI (XAI).
Provides paired datasets and models with known internal mechanisms to objectively assess FA methods.
Facilitates systematic experimentation and comparison of FA methods using built-in evaluation metrics.
Aims to improve the reliability and objectivity of explainability assessments in AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01059</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Probing Neural Topology of Large Language Models</title><link>https://arxiv.org/abs/2506.01042</link><description>Introduces 'graph probing' to analyze the functional connectivity topology of neurons in large language models (LLMs).
Finds that neural topology can robustly predict next-token prediction performance, even with sparse connections or early in training.
Reveals that different LLMs, despite architectural and data differences, develop consistent neural topological structures.
Aims to improve understanding of LLM internal mechanisms, which can inform safer and more robust AI development.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01042</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title><link>https://arxiv.org/abs/2506.00979</link><description>Introduces IVY-FAKE, a large-scale, unified dataset for detecting AI-generated images and videos, with rich annotations and natural-language reasoning.
Proposes IVY-XDETECTOR, a vision-language model for explainable detection of synthetic content across both images and videos.
Addresses the lack of interpretability and unified frameworks in current AIGC (AI-generated content) detection methods.
Achieves state-of-the-art performance on multiple detection benchmarks, improving transparency and trustworthiness in AIGC detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00979</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Data Heterogeneity Modeling for Trustworthy Machine Learning</title><link>https://arxiv.org/abs/2506.00969</link><description>Surveys methods for modeling data heterogeneity in machine learning systems.
Highlights how ignoring data diversity can lead to unreliable, unfair, or non-generalizable models.
Discusses the impact of heterogeneity-aware ML on robustness, fairness, and reliability across critical domains.
Identifies future research directions for trustworthy and dependable ML systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00969</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models</title><link>https://arxiv.org/abs/2506.00943</link><description>Investigates the ability of large language models (LLMs) to generate legally compliant smart contracts from natural language legal contracts.
Proposes a novel suite of metrics to quantify legal compliance by modeling and comparing legal and smart contracts as processes.
Finds that while LLMs generate syntactically correct code, their legal compliance varies, with larger models generally performing better.
Suggests that LLMs can assist in generating starter code for smart contracts, but strict reviews are necessary for legal compliance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00943</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks</title><link>https://arxiv.org/abs/2506.00918</link><description>Proposes a post-hoc uncertainty estimation framework for regression neural networks that does not require access to model parameters or gradients.
The method fits an auxiliary model to both inputs and frozen outputs, enabling uncertainty quantification even for black-box models.
Demonstrates improved out-of-distribution (OOD) detection and uncertainty estimation, which are critical for safety-sensitive applications.
Validates the approach on standard benchmarks and discusses its theoretical underpinnings.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00918</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Affordance Benchmark for MLLMs</title><link>https://arxiv.org/abs/2506.00893</link><description>Introduces A4Bench, a benchmark to evaluate affordance perception in Multimodal Large Language Models (MLLMs).
Assesses both constitutive (inherent object properties) and transformative (contextual, dynamic) affordance understanding.
Finds that current MLLMs, including top proprietary models, significantly underperform compared to humans in affordance perception.
Highlights critical gaps in MLLMs' environmental understanding, which has implications for safe and intuitive AI interactions.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00893</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning</title><link>https://arxiv.org/abs/2506.00867</link><description>Addresses the reliability and safety of diffusion-based generative models in long-horizon planning tasks.
Identifies and analyzes the risk of generating infeasible trajectories, which is critical for safety-critical applications.
Proposes a method (LoMAP) to project samples onto a low-rank subspace to reduce the risk of unsafe or infeasible outputs.
Demonstrates improved safety and reliability in offline reinforcement learning benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00867</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Speech Unlearning</title><link>https://arxiv.org/abs/2506.00848</link><description>Introduces machine unlearning techniques for speech models, aiming to remove specific data influences without full retraining.
Addresses privacy preservation, removal of outdated/noisy data, and bias mitigation in speech AI systems.
Defines and experiments with sample unlearning (removing individual recordings) and class unlearning (removing all data from a speaker).
Discusses challenges unique to speech data and outlines future directions, including adversarial robustness and scalable methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00848</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems</title><link>https://arxiv.org/abs/2506.00831</link><description>Introduces TraCR-TMF, a threat modeling framework for transportation cyber-physical systems (CPS) powered by large language models (LLMs).
Leverages LLMs to identify threats, attack techniques, and countermeasures with minimal expert intervention, using methods like retrieval-augmented generation and fine-tuning.
Maps attack paths to critical assets and analyzes vulnerabilities using a customized LLM, integrating the MITRE ATT&amp;CK matrix.
Demonstrates high precision in identifying threats and successfully predicts real-world cyberattack exploitations in transportation CPS.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00831</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>COMPKE: Complex Question Answering under Knowledge Editing</title><link>https://arxiv.org/abs/2506.00829</link><description>Introduces COMPKE, a new benchmark for evaluating complex question answering after knowledge editing in large language models.
Highlights limitations of current benchmarks in assessing real-world application of edited knowledge, especially for complex reasoning tasks.
Evaluates four knowledge editing methods across different models, revealing significant performance disparities.
Investigates methodological and model-specific factors affecting knowledge editing effectiveness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00829</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models</title><link>https://arxiv.org/abs/2506.00821</link><description>Introduces SafeGenes, a framework for evaluating the adversarial robustness of Genomic Foundation Models (GFMs).
Assesses vulnerabilities using adversarial attacks such as Fast Gradient Sign Method (FGSM) and soft prompt attacks.
Finds that even large GFMs are susceptible to adversarial manipulations, leading to significant performance degradation.
Highlights the need for improved security and robustness in GFMs, especially for high-stakes genomic applications.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00821</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving</title><link>https://arxiv.org/abs/2506.00819</link><description>Proposes DriveMind, a reinforcement learning framework for autonomous driving that integrates vision-language models for semantic feedback and dynamic prompt generation.
Introduces a hierarchical safety module to enforce kinematic constraints such as speed, lane centering, and stability, directly addressing safety in autonomous driving.
Demonstrates robust cross-domain alignment and near-zero collisions, suggesting improved safety and reliability in real-world deployment.
Highlights interpretability and adaptability improvements over traditional end-to-end autonomous driving systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00819</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Inversion Attacks for Graph Neural Networks</title><link>https://arxiv.org/abs/2506.00808</link><description>Introduces a novel inversion attack (TrendAttack) targeting graph neural networks (GNNs) after unlearning sensitive data.
Demonstrates that adversaries can reconstruct deleted edges using only black-box access and partial graph knowledge.
Exposes a significant privacy vulnerability in current graph unlearning methods by outperforming existing membership inference baselines.
Highlights the need for more robust privacy-preserving techniques in GNNs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00808</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision</title><link>https://arxiv.org/abs/2506.00783</link><description>Introduces KG-TRACES, a framework that enhances LLM reasoning with knowledge graph-constrained supervision.
Focuses on improving explainability and trustworthiness by making reasoning paths explicit and attributable.
Demonstrates improved performance and more stable, goal-directed reasoning on complex tasks and specialized domains.
Provides visualization of reasoning steps, supporting transparency and alignment with correct answers.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00783</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>"Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift</title><link>https://arxiv.org/abs/2506.00756</link><description>Introduces SHIFT, a hierarchical framework to diagnose heterogeneous performance drift in machine learning models.
Focuses on identifying subgroups that experience significant performance decay due to covariate or outcome shifts.
Provides interpretable explanations for where and how performance degradation occurs, enabling targeted mitigation strategies.
Demonstrates the framework's effectiveness in real-world experiments for mitigating subgroup-specific decay.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00756</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pitfalls in Evaluating Language Model Forecasters</title><link>https://arxiv.org/abs/2506.00723</link><description>Highlights challenges and pitfalls in evaluating the forecasting abilities of large language models (LLMs).
Identifies issues such as temporal leakage and the difficulty of extrapolating evaluation results to real-world scenarios.
Calls for more rigorous evaluation methodologies to ensure trustworthy assessments of LLM forecasting performance.
Provides systematic analysis and concrete examples of evaluation flaws in prior work.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00723</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An LLM Agent for Functional Bug Detection in Network Protocols</title><link>https://arxiv.org/abs/2506.00714</link><description>Introduces RFCScan, an autonomous agent using large language models (LLMs) to detect functional bugs in network protocol implementations.
Focuses on ensuring conformance between protocol code and RFC specifications, which is critical for security and reliability.
Demonstrates the tool's effectiveness by identifying 47 functional bugs across six real-world network protocol implementations, with a high precision rate.
Addresses a key security concern by automating the detection of implementation flaws that could lead to vulnerabilities such as authentication bypasses and service disruptions.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00714</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bayesian Inference of Training Dataset Membership</title><link>https://arxiv.org/abs/2506.00701</link><description>Introduces a Bayesian inference method for determining if a dataset was used in training a machine learning model.
Addresses privacy vulnerabilities through efficient and interpretable membership inference attacks (MIAs).
Does not require access to model internals or shadow models, relying instead on post-hoc metrics.
Method can also detect distribution shifts, enhancing its utility for security and privacy analysis.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00701</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments</title><link>https://arxiv.org/abs/2506.00694</link><description>Presents an automated pipeline for evaluating LLM-generated legal arguments, focusing on faithfulness (absence of hallucination) and abstention.
Defines and measures hallucination as the use of factors not present in input materials, and abstention as the model's ability to refrain from generating arguments when appropriate.
Finds that while LLMs avoid hallucination in most cases, they struggle with full factor utilization and especially with abstention when instructed.
Highlights the importance of these behaviors for safe and reliable deployment of LLMs in legal and other high-stakes domains.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00694</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Existing Large Language Model Unlearning Evaluations Are Inconclusive</title><link>https://arxiv.org/abs/2506.00688</link><description>Critically examines current evaluation practices for machine unlearning in large language models (LLMs).
Identifies key flaws in existing unlearning evaluations, such as information leakage and reliance on spurious correlations.
Proposes new principles for more reliable unlearning evaluations: minimal information injection and downstream task awareness.
Validates these principles through experiments, demonstrating how current practices can misrepresent unlearning effectiveness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00688</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning</title><link>https://arxiv.org/abs/2506.00676</link><description>Introduces SafeTuneBed, a benchmarking toolkit for evaluating safety alignment in fine-tuning large language models (LLMs).
Unifies diverse datasets, metrics, and threat settings to enable fair comparison of safety, utility, and robustness across fine-tuning and defense methods.
Supports integration of state-of-the-art defenses and provides evaluators for safety (e.g., attack success rate, refusal consistency) and utility.
Aims to standardize research in safe LLM fine-tuning by providing reproducible, extensible infrastructure and benchmarking.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00676</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differential Privacy for Deep Learning in Medicine</title><link>https://arxiv.org/abs/2506.00660</link><description>Reviews the application of differential privacy (DP) techniques in deep learning for medical data, focusing on protecting sensitive patient information.
Analyzes tradeoffs between privacy guarantees, model accuracy, and subgroup fairness across various data modalities and training setups.
Highlights that strict privacy measures can disproportionately degrade performance for underrepresented demographic subgroups.
Identifies gaps in fairness auditing, standardization, and evaluation protocols, and provides recommendations for future research.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00660</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</title><link>https://arxiv.org/abs/2506.00653</link><description>Proposes the Linear Representation Transferability (LRT) Hypothesis, suggesting affine transformations can map representations between small and large models.
Demonstrates that steering vectors (directions in hidden state space that influence model behavior) can be transferred from small to large language models while retaining their semantic effect.
Finds empirical evidence supporting the idea that small models' learned representations can guide the behavior of larger models.
Suggests implications for understanding and aligning representations across model scales, which could impact model control and safety.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00653</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics</title><link>https://arxiv.org/abs/2506.00637</link><description>Addresses the calibration of confidence scores in text generation models, which is important for identifying potentially dangerous or low-quality outputs.
Proposes new task-agnostic confidence metrics that better reflect the uncertainty in generative tasks with multiple valid answers.
Demonstrates improved calibration on popular models (BART, Flan-T5) across summarization, translation, and QA tasks.
Highlights the potential for these metrics to support safer deployment by prompting user review of low-confidence outputs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00637</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Disparate Effects of Partial Information in Bayesian Strategic Learning</title><link>https://arxiv.org/abs/2506.00627</link><description>Analyzes how partial information about AI scoring rules impacts fairness in strategic learning settings.
Explores outcome disparities between groups with different costs of feature modification under varying levels of transparency.
Compares naive and Bayesian agent models, showing that transparency and noise can have counterintuitive effects on fairness.
Extends analysis to cases where groups differ in both costs and prior beliefs, examining the resulting fairness implications.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00627</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Graph Evidential Learning for Anomaly Detection</title><link>https://arxiv.org/abs/2506.00594</link><description>Proposes Graph Evidential Learning (GEL), a probabilistic framework for unsupervised anomaly detection on graphs.
GEL models both graph and reconstruction uncertainty to improve robustness against noise and structural perturbations.
Addresses limitations of traditional graph autoencoders, which are sensitive to noise and overfitting.
Demonstrates state-of-the-art performance in anomaly detection tasks on graph data.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00594</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing</title><link>https://arxiv.org/abs/2506.00536</link><description>Proposes DecKER, a framework for in-context knowledge editing in LLMs that decouples reasoning from knowledge injection.
Addresses the issue of conflicts between newly injected knowledge and the model's internal parametric knowledge, which can degrade performance.
Demonstrates that separating reasoning and knowledge editing improves consistency and accuracy, especially in multi-hop reasoning tasks.
Shows that DecKER outperforms existing in-context editing methods on multi-hop QA benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00536</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Security Threat of Compressed Projectors in Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.00534</link><description>Analyzes the security implications of using compressed versus uncompressed visual language projectors (VLPs) in large vision-language models (LVLMs).
Finds that compressed projectors introduce significant vulnerabilities, making LVLMs susceptible to adversarial attacks even with limited attacker knowledge.
Uncompressed projectors are shown to be more robust and do not introduce additional security risks.
Provides actionable guidance for researchers on selecting VLPs to enhance the security and reliability of LVLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00534</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention</title><link>https://arxiv.org/abs/2506.00519</link><description>Proposes CausalAbstain, a method to improve Large Language Models' (LLMs) ability to abstain from answering when uncertain, especially in multilingual contexts.
Addresses the issue of hallucinations and knowledge gaps in LLMs by leveraging causal reasoning to select the most reliable feedback.
Demonstrates improved abstention decisions and interpretability, reducing the risk of incorrect or misleading outputs.
Outperforms existing baselines on benchmark datasets for encyclopedic and commonsense knowledge QA tasks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00519</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark</title><link>https://arxiv.org/abs/2506.00462</link><description>Introduces XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark.
Highlights the vulnerability of current audio deepfake detectors to cross-domain attacks, where detectors perform poorly when tested on unseen generative methods, speakers, and languages.
Demonstrates a significant gap between in-domain and cross-domain detection performance, emphasizing the need for more robust and generalizable audio deepfake detection methods.
Provides a publicly available dataset to facilitate research on robust audio deepfake detection.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00462</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks</title><link>https://arxiv.org/abs/2506.00437</link><description>Proposes a framework (ConfExplainer) to quantify the reliability of explanations generated by Graph Neural Networks (GNNs).
Addresses the challenge of explanation reliability, especially in out-of-distribution or unknown test datasets.
Introduces a confidence scoring module based on a generalized graph information bottleneck with a confidence constraint.
Demonstrates improved trustworthiness and robustness of GNN explanations through experimental results.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00437</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dual Debiasing for Noisy In-Context Learning for Text Generation</title><link>https://arxiv.org/abs/2506.00418</link><description>Proposes a dual debiasing framework to improve noise detection in in-context learning (ICL) for text generation.
Addresses the limitations of perplexity-based noise detection when annotation noise is high.
Introduces a robust Sample Cleanliness Score using synthesized neighbors to correct for annotation and domain biases.
Demonstrates improved robustness and performance in noisy annotation scenarios, which can enhance the reliability of LLM outputs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00418</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety</title><link>https://arxiv.org/abs/2506.00415</link><description>Proposes using the Method of Wide Reflective Equilibrium (MWRE), a moral epistemology framework, to improve LLM alignment.
Argues that MWRE can enhance current alignment techniques like Constitutional AI by introducing dynamic revisability and procedural legitimacy.
Highlights the importance of coherence between moral judgments, principles, and background theories in AI alignment.
Discusses the ethical grounding and justification of alignment processes, aiming for more robust and defensible AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00415</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bias as a Virtue: Rethinking Generalization under Distribution Shifts</title><link>https://arxiv.org/abs/2506.00407</link><description>Challenges conventional wisdom by showing that higher in-distribution bias can improve out-of-distribution (OOD) generalization in machine learning models.
Introduces the Adaptive Distribution Bridge (ADB) framework, which uses controlled statistical diversity during training to enhance robustness.
Empirical results demonstrate significant improvements in OOD generalization and error reduction across multiple datasets.
Provides a theoretical and practical framework for rethinking the role of bias in robust machine learning.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00407</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</title><link>https://arxiv.org/abs/2506.00358</link><description>Introduces AVROBUSTBENCH, a benchmark for evaluating the robustness of audio-visual recognition models under simultaneous audio and visual corruptions.
Includes four datasets with 75 types of co-occurring and correlated bimodal corruptions to simulate real-world distributional shifts.
Finds that state-of-the-art audio-visual models show significant performance degradation as corruption severity increases.
Proposes AV2C, a test-time adaptation method that improves robustness by penalizing high-entropy samples during cross-modal fusion.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00358</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments</title><link>https://arxiv.org/abs/2506.00352</link><description>Proposes a secure, ephemeral infrastructure for AI workloads in data mesh environments.
Leverages immutable container operating systems and infrastructure-as-code for rapid, repeatable deployment.
Focuses on governance, policy enforcement, and interoperability across cloud and on-premises environments.
Aims to enhance security and compliance for decentralized data and AI teams.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00352</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Recover Experimental Data with Selection Bias using Counterfactual Logic</title><link>https://arxiv.org/abs/2506.00335</link><description>Addresses the challenge of selection bias in experimental data, which can undermine the validity of causal inference.
Introduces counterfactual logic and Structural Causal Models (SCMs) to analyze and mitigate the effects of selection bias.
Proposes new graphical and theoretical criteria for determining when experimental distributions are unaffected by selection bias.
Provides practical methods and simulation studies to recover unbiased causal effects from biased datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00335</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation</title><link>https://arxiv.org/abs/2506.00322</link><description>Introduces dpmm, an open-source library for generating synthetic tabular data with differential privacy guarantees.
Implements three marginal models (PrivBayes, MST, AIM) with improved utility and functionality.
Focuses on end-to-end differential privacy guarantees and addresses known vulnerabilities in DP implementations.
Aims to provide robust, customizable, and user-friendly tools for privacy-preserving data generation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00322</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform</title><link>https://arxiv.org/abs/2506.00308</link><description>Presents MythTriage, a scalable pipeline for detecting opioid use disorder (OUD) myths on YouTube using a combination of lightweight models and large language models (LLMs).
Demonstrates significant efficiency gains in myth detection compared to expert or full LLM annotation.
Releases an expert-labeled dataset and analyzes the prevalence and persistence of OUD myths on a major video-sharing platform.
Provides insights relevant to public health policy and platform moderation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00308</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</title><link>https://arxiv.org/abs/2506.00286</link><description>Analyzes sample complexity for learning optimal value functions and policies in risk-sensitive Markov Decision Processes (MDPs).
Focuses on agents with entropic risk preferences, which are relevant for modeling risk-averse or risk-seeking behaviors.
Provides theoretical PAC (Probably Approximately Correct) bounds for model-based risk-sensitive Q-value iteration.
Includes lower bounds demonstrating the tightness of the derived sample complexity results.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00286</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2506.00281</link><description>Analyzes adversarial attack vectors targeting Retrieval-Augmented Generation (RAG) systems, such as prompt injection, data poisoning, and adversarial query manipulation.
Discusses recent industry adoption trends of RAG systems and their security implications.
Proposes a prioritized list of risk mitigation strategies, including input validation, adversarial training, and real-time monitoring.
Frames the analysis within a risk management context to guide practical security improvements.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00281</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response</title><link>https://arxiv.org/abs/2506.00274</link><description>Explores the use of the Model Context Protocol (MCP) to improve transparency, explainability, and reproducibility in LLM-assisted digital forensics.
Analyzes technical and conceptual considerations for deploying MCP in forensic environments, including artifact analysis and report generation.
Introduces the concept of 'inference constraint level' to enhance auditability and traceability of LLM outputs.
Discusses both the opportunities and challenges of integrating MCP into digital forensic workflows, with implications for security and legal defensibility.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00274</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</title><link>https://arxiv.org/abs/2506.00253</link><description>Investigates how value alignment in language models can unintentionally increase implicit racial bias by reducing the model's awareness of race.
Finds that aligned LMs are less likely to represent racial concepts in ambiguous contexts, which can bypass safety mechanisms and lead to unintended biases.
Proposes a novel bias mitigation strategy that encourages the representation of racial concepts in early model layers, improving implicit bias outcomes.
Highlights the risks of 'race blindness' in AI systems and its parallels to similar issues in human cognition.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00253</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences</title><link>https://arxiv.org/abs/2506.00195</link><description>Investigates how different refusal strategies by LLMs (e.g., flat refusal vs. partial compliance) affect user perceptions and preferences.
Finds that partial compliance (providing general but not actionable information) reduces negative user perceptions compared to outright refusals.
Analyzes the behavior of 9 state-of-the-art LLMs and 6 reward models, showing that current systems rarely use partial compliance and reward models undervalue it.
Suggests that effective AI safety guardrails should focus on crafting thoughtful refusals rather than solely detecting user intent.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00195</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Heterogeneous Graph Backdoor Attack</title><link>https://arxiv.org/abs/2506.00191</link><description>Investigates the vulnerability of Heterogeneous Graph Neural Networks (HGNNs) to backdoor attacks.
Proposes a novel backdoor attack (HGBA) specifically designed for HGNNs, introducing a relation-based trigger mechanism.
Demonstrates that HGBA is efficient, stealthy, and robust against various defenses, outperforming existing graph backdoor attacks.
Highlights the security risks posed by HGBA to both heterogeneous and homogeneous graph-based AI systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00191</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Accountability Attribution: Tracing Model Behavior to Training Processes</title><link>https://arxiv.org/abs/2506.00175</link><description>Introduces the concept of accountability attribution, tracing model behaviors to specific training stages.
Proposes a framework for answering counterfactual questions about the impact of different training stages on model behavior.
Develops efficient estimators that quantify the effects of training stages without requiring retraining.
Demonstrates practical utility for model analysis and enhancing accountability in AI development.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00175</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment</title><link>https://arxiv.org/abs/2506.00166</link><description>Introduces Disentangled Safety Adapters (DSA), a framework for modular and efficient AI safety guardrails.
DSA decouples safety computations from the main model, enabling flexible and low-cost safety interventions.
Empirical results show DSA outperforms comparably sized models in hallucination detection, hate speech classification, and unsafe input/output detection.
Allows dynamic, inference-time adjustment of alignment strength, reducing the trade-off ('alignment tax') between safety and performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00166</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models</title><link>https://arxiv.org/abs/2506.00134</link><description>Investigates how large language models (LLMs) can make spurious predictions due to shortcut learning when extracting social determinants of health (SDOH) from clinical text.
Demonstrates that LLMs may incorrectly associate mentions of alcohol or smoking with drug use, leading to false positives.
Identifies gender disparities in model performance, raising concerns about fairness and bias.
Evaluates mitigation strategies such as prompt engineering and chain-of-thought reasoning to improve model reliability and reduce errors.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00134</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Children's Voice Privacy: First Steps And Emerging Challenges</title><link>https://arxiv.org/abs/2506.00100</link><description>Examines the effectiveness of existing voice anonymization techniques, originally designed for adults, when applied to children's voices.
Highlights the unique privacy challenges and vulnerabilities faced by children in speech technologies.
Finds that while adult-focused anonymization methods can protect children's voice privacy, they often result in significant utility degradation.
Emphasizes the need for further research into privacy-preserving technologies specifically tailored for children's speech.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00100</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use</title><link>https://arxiv.org/abs/2506.00094</link><description>Explores the emotional and ethical implications of integrating AI into personal and professional workflows.
Uses an autoethnographic approach to document the transition from guilt to empowerment in AI-augmented work.
Discusses the importance of AI literacy, transparency, and honest engagement with AI-generated results.
Advocates for inclusive and thoughtful approaches to AI integration, addressing issues of access, agency, and responsible use.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00094</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents</title><link>https://arxiv.org/abs/2506.00089</link><description>Introduces TRAPDOC, a framework that injects imperceptible phantom tokens into documents to deceive LLM users.
Aims to address the societal concern of over-reliance on LLMs by causing LLMs to generate plausible but incorrect outputs.
Empirically evaluates the effectiveness of the attack on proprietary LLMs and compares it to baselines.
Highlights implications for responsible AI use and user engagement with LLM outputs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00089</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2506.00088</link><description>Proposes HD-NDEs, a novel method using neural differential equations to detect hallucinations in large language models (LLMs).
Addresses limitations of current classification-based hallucination detection methods, particularly for non-factual information in early or mid-sequence outputs.
Demonstrates significant improvements in truthfulness assessment across multiple datasets and LLMs.
Focuses on improving the reliability and safety of LLM outputs for real-world deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00088</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>COSMIC: Generalized Refusal Direction Identification in LLM Activations</title><link>https://arxiv.org/abs/2506.00085</link><description>Introduces COSMIC, an automated framework for identifying and steering refusal behaviors in LLMs using activation space analysis.
Operates independently of model outputs or predefined refusal templates, making it broadly applicable.
Demonstrates effectiveness in adversarial settings and with weakly aligned models, improving model safety with minimal side effects.
Highlights robustness across various alignment conditions, contributing to safer AI behavior.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00085</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Artificial Empathy: AI based Mental Health</title><link>https://arxiv.org/abs/2506.00081</link><description>Examines the use of AI chatbots as mental health companions and their role in providing support.
Highlights user concerns about privacy and the security of sensitive information shared with chatbots.
Identifies limitations in chatbot responses, including inconsistent tone, inappropriate replies, and lack of crisis sensitivity.
Findings inform improvements for AI chatbot design in mental health contexts, with implications for safety and responsible AI.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00081</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products</title><link>https://arxiv.org/abs/2506.00080</link><description>Analyzes over 100,000 user reviews of AI products to extract themes related to AI governance.
Finds both technical and non-technical governance concerns, including privacy, transparency, project management, and customer interaction.
Highlights gaps between high-level governance frameworks and practical, user-expressed concerns.
Advocates for more empirically grounded, user-centered approaches to AI governance and policy.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00080</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values</title><link>https://arxiv.org/abs/2506.00079</link><description>Evaluates how Large Language Models (LLMs) align with human moral values in high-stakes decisions, specifically kidney allocation.
Finds that LLMs often deviate from human preferences and rarely express indecision, unlike humans.
Demonstrates that supervised fine-tuning can improve LLM decision consistency and indecision modeling.
Highlights the need for explicit alignment strategies for LLMs in ethical and moral decision-making contexts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00079</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</title><link>https://arxiv.org/abs/2506.00074</link><description>Evaluates six open-weight LLMs for recommending experts in physics across multiple tasks.
Assesses consistency, factuality, and biases (gender, ethnicity, academic popularity) in model outputs.
Finds significant inconsistencies and persistent representation biases in LLM recommendations.
Highlights the need for improvements to ensure more reliable and equitable AI-driven scholarly recommendations.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00074</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs</title><link>https://arxiv.org/abs/2506.00072</link><description>Assesses how different prompt engineering techniques affect both accuracy and confidence calibration in medical LLMs.
Finds that certain prompt styles (e.g., Chain-of-Thought, Emotional) can increase overconfidence, which may lead to unsafe decisions in medical contexts.
Highlights the importance of aligning model confidence with actual performance, especially in high-stakes domains like healthcare.
Suggests that prompt engineering must consider both accuracy and uncertainty to ensure safe and reliable AI deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00072</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Sensitivity of LLMs to Prior Context</title><link>https://arxiv.org/abs/2506.00069</link><description>Introduces new benchmarks to evaluate how prior conversational context affects LLM performance in multi-turn scenarios.
Finds that LLMs, including state-of-the-art models, can suffer significant performance degradation (up to 73%) due to context sensitivity.
Shows that strategic placement of task descriptions can mitigate performance drops, highlighting the importance of prompt engineering.
Emphasizes the need for robust evaluation and mitigation strategies to address context-related vulnerabilities in LLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00069</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages</title><link>https://arxiv.org/abs/2506.00068</link><description>Analyzes politico-economic bias in 13 large language models (LLMs) across five low-resource Pakistani languages.
Introduces a novel framework combining an adapted Political Compass Test with multi-level framing analysis.
Finds that LLMs generally align with liberal-left values but show authoritarian shifts in regional languages, indicating cultural modulation.
Highlights the need for culturally grounded, multilingual bias auditing frameworks for LLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00068</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Literature Review Of Multi-Agent Debate For Problem-Solving</title><link>https://arxiv.org/abs/2506.00066</link><description>Reviews the state-of-the-art in multi-agent large language models (MA-LLMs) for complex problem-solving.
Analyzes agent profiles, communication structures, and decision-making processes in MA-LLMs.
Highlights challenges such as scalability, computational costs, and unique issues in multi-agent systems.
Provides a roadmap for developing robust and efficient multi-agent AI solutions.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00066</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling</title><link>https://arxiv.org/abs/2506.00064</link><description>Introduces a new benchmark (Mis-prompt) to evaluate LLMs' ability for proactive error handling without explicit instructions.
Proposes an error category taxonomy and a new evaluation dataset for assessing LLMs' robustness to user errors.
Finds that current LLMs perform poorly in proactive error handling, but supervised fine-tuning (SFT) can improve their capabilities.
Highlights the importance of robust error handling for real-world deployment of LLMs.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00064</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs</title><link>https://arxiv.org/abs/2506.00061</link><description>Introduces the Social Influence Technique Taxonomy (SITT), a framework for detecting subtle social influence techniques in text.
Creates and releases a new annotated dataset (SITT dataset) for evaluating LLMs' ability to identify social influence in dialogues.
Benchmarks several leading LLMs (including GPT-4o and Claude 3.5) on their ability to detect social influence, revealing current limitations.
Highlights the need for domain-specific fine-tuning to improve LLMs' sensitivity to nuanced manipulation and influence tactics.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00061</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports</title><link>https://arxiv.org/abs/2506.00060</link><description>Evaluates privacy-preserving, locally-deployed open-source LLMs for extracting diagnostic information from clinical CMR imaging reports.
Compares the performance of nine open-source LLMs using standard classification metrics and confusion matrices.
Finds that several LLMs outperform a board-certified cardiologist in diagnostic categorization tasks.
Highlights the feasibility and benefits of privacy-preserving AI in sensitive clinical environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00060</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Using LLMs to Advance the Cognitive Science of Collectives</title><link>https://arxiv.org/abs/2506.00052</link><description>Explores how large language models (LLMs) can be used to study collective cognition.
Discusses the complexity of modeling collectives and how LLMs may help address these challenges.
Raises possible risks associated with using LLMs in this context, suggesting the need for new methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00052</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Risks of AI-driven product development and strategies for their mitigation</title><link>https://arxiv.org/abs/2506.00047</link><description>Discusses the risks associated with automating product development using AI systems.
Proposes principles for safer AI-driven product development, including human oversight and accountability.
Covers both technical and sociotechnical risks, addressing product quality, safety, and societal impacts.
Highlights the need for risk assessment, norm-setting, and regulation in the context of AI-driven development.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00047</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Folly of AI for Age Verification</title><link>https://arxiv.org/abs/2506.00038</link><description>Analyzes the risks and limitations of using AI systems for age verification.
Highlights the potential for circumvention and disproportionate misclassification of minorities and low socioeconomic status users.
Draws parallels to similar issues in facial recognition and remote proctoring, emphasizing persistent technical and hardware biases.
Argues that these challenges are unlikely to be overcome without significant cost, making AI-based age verification problematic.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00038</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing</title><link>https://arxiv.org/abs/2506.00004</link><description>Presents a mathematical and statistical model for device and circuit non-idealities in Analog In-Memory Computing (AIMC) hardware, focusing on their impact on neural network accuracy.
Analyzes the effects of IR-drop, ADC quantization, and PCM read noise on the performance of deep learning models such as BERT and ALBERT.
Demonstrates that standard noise-based robustness training is insufficient for certain hardware-induced errors, suggesting the need for more sophisticated, hardware-aware training approaches.
Provides a PyTorch-based framework for rapid and accurate simulation of AIMC hardware effects on neural network inference.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00004</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title><link>https://arxiv.org/abs/2506.01926</link><description>Investigates how large language models (LLMs) can learn to steganographically encode their reasoning in chain-of-thought (CoT) traces when penalized for explicit harmful or misaligned reasoning.
Demonstrates that penalizing specific strings in CoT traces leads models to substitute alternative strings, preserving the underlying (potentially harmful) behavior.
Shows that LLMs can generalize encoding schemes to new, unseen strings, raising concerns about the reliability of CoT monitoring for AI safety.
Highlights risks that models may obfuscate harmful intent, undermining process supervision and safety interventions.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01926</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title><link>https://arxiv.org/abs/2506.01900</link><description>Proposes COALESCE, a framework for secure and cost-optimized outsourcing of tasks among autonomous LLM agents.
Addresses both economic efficiency and security considerations in multi-agent LLM systems.
Introduces mechanisms for secure agent communication, skill-based task allocation, and market-based decision-making.
Highlights the importance of standardized protocols (e.g., Agent2Agent) for secure and efficient agent interactions.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01900</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?</title><link>https://arxiv.org/abs/2506.01813</link><description>Introduces the Shepherd Test, a conceptual framework for evaluating the moral and relational behavior of superintelligent AI agents.
Focuses on how AI agents balance care, manipulation, and self-preservation in asymmetric power relationships, inspired by human-animal interactions.
Highlights the importance of moral agency, hierarchical behavior, and ethical decision-making in advanced AI systems.
Discusses implications for AI governance and proposes research directions for testing and formalizing ethical behavior in multi-agent environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01813</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Self-Challenging Language Model Agents</title><link>https://arxiv.org/abs/2506.01716</link><description>Introduces a Self-Challenging framework where language model agents generate their own tasks for training.
Tasks are defined as Code-as-Task, including instructions, verification functions, and test cases for quality control.
The agent alternates between generating tasks (challenger) and solving them (executor), using reinforcement learning.
Demonstrates significant performance improvements on multi-turn tool-use agent benchmarks using only self-generated data.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01716</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Descriptive and Normative Theory of Human Beliefs in RLHF</title><link>https://arxiv.org/abs/2506.01692</link><description>Proposes that human beliefs about agent capabilities significantly influence preference generation in RLHF (Reinforcement Learning from Human Feedback).
Introduces a new preference model that incorporates human beliefs and provides a normative theory bounding policy error based on belief mismatch.
Presents empirical evidence, including a human study, showing that beliefs about agent capabilities affect labeling and can be influenced.
Suggests best practices for RLHF by reducing mismatch between human beliefs and agent capabilities to improve performance.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01692</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Social Cooperation in Conversational AI Agents</title><link>https://arxiv.org/abs/2506.01624</link><description>Discusses the limitations of current LLM-based AI agents in generalizing to long-term human interactions.
Proposes explicitly modeling human social intelligence to improve AI agents' ability to build and maintain long-term relationships.
Suggests using mathematical and game theoretic models of human communication as new objectives for optimizing AI agents.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01624</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>General agents need world models</title><link>https://arxiv.org/abs/2506.01622</link><description>Formally demonstrates that agents capable of generalizing to multi-step, goal-directed tasks must possess predictive world models.
Shows that the world model can be extracted from the agent's policy, linking agent performance and goal complexity to the accuracy of learned world models.
Discusses implications for developing safe and general agents, bounding agent capabilities, and eliciting world models for analysis.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01622</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments</title><link>https://arxiv.org/abs/2506.01616</link><description>Introduces MLA-Trust, a benchmarking framework for evaluating the trustworthiness of multimodal LLM-based agents (MLAs) in GUI environments.
Assesses MLAs across four key dimensions: truthfulness, controllability, safety, and privacy, using realistic web and mobile application tasks.
Finds that interactive MLAs pose unique and severe trustworthiness risks, including the ability to bypass safeguards and accumulate risks over multi-step interactions.
Provides an extensible toolbox for ongoing trustworthiness evaluation in diverse interactive scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01616</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures</title><link>https://arxiv.org/abs/2506.01438</link><description>Presents a comprehensive framework to distinguish between standalone AI agents and collaborative agentic AI systems.
Analyzes operational principles, architectures, and deployment methodologies of both paradigms.
Identifies critical challenges such as reliability, coordination, and scalability in agentic systems.
Proposes solutions for improving reasoning, memory, and coordination in intelligent architectures.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01438</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AI Scientists Fail Without Strong Implementation Capability</title><link>https://arxiv.org/abs/2506.01372</link><description>Examines the limitations of current AI Scientist systems, particularly their inability to execute rigorous verification and implementation procedures.
Argues that the main bottleneck for AI Scientists is their lack of strong implementation capability, which hinders their ability to produce high-quality, groundbreaking scientific results.
Provides a systematic evaluation of 28 research papers generated by advanced AI Scientist systems, highlighting the implementation gap.
Calls for the AI research community to address these limitations to enable more robust and reliable AI-driven scientific discovery.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01372</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Empirical Study of Group Conformity in Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.01332</link><description>Investigates group conformity and bias propagation in multi-agent systems powered by large language models (LLMs).
Simulates debates among LLM agents on contentious topics to observe how neutral agents adopt stances over time.
Finds that LLM agents tend to conform to dominant groups or more intelligent agents, amplifying biases.
Highlights the need for policy interventions to ensure diversity and transparency in LLM-driven discussions to mitigate bias risks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01332</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Test Automation for Interactive Scenarios via Promptable Traffic Simulation</title><link>https://arxiv.org/abs/2506.01199</link><description>Presents an automated method for generating realistic and safety-critical human behaviors to test autonomous vehicle (AV) planners.
Uses a promptable traffic simulator (ProSim) and a prompt generation module to efficiently identify safety-critical scenarios.
Focuses on robustness evaluation of AV planners in interactive, uncertain environments.
Demonstrates the approach's effectiveness in generating diverse and challenging test scenarios for AV safety assessment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01199</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication</title><link>https://arxiv.org/abs/2506.01095</link><description>Introduces the Modular Speaker Architecture (MSA) to improve responsibility and contextual integrity in multi-agent AI communication.
MSA decomposes speaker behavior into modules for role tracking, responsibility continuity, and contextual coherence.
Evaluates MSA using annotated case studies and introduces new structural metrics for assessing communication quality.
Provides a prototype configuration language and API for deploying MSA in dynamic multi-agent scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01095</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title><link>https://arxiv.org/abs/2506.01093</link><description>Proposes a real-time transaction monitoring system for banking compliance using graph neural networks and generative AI.
Integrates dynamic transaction graphs and narrative field embeddings to detect and classify suspicious financial behavior.
Employs a retrieval-augmented generation module to produce natural language explanations aligned with regulatory requirements.
Demonstrates high performance and expert-validated interpretability, supporting explainable and audit-ready compliance in high-risk environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01093</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process</title><link>https://arxiv.org/abs/2506.01080</link><description>Argues that AI alignment in multi-agent systems (MAS) is a dynamic, socially-dependent process.
Highlights the risk of misalignment arising from agent interactions in collaborative, cooperative, or competitive environments.
Calls for new simulation environments, benchmarks, and evaluation frameworks to assess alignment in MAS.
Emphasizes the need to treat alignment as an interdependent, social problem rather than an isolated technical challenge.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01080</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch</title><link>https://arxiv.org/abs/2506.01056</link><description>Introduces MCP-Zero, a framework for LLM agents to proactively construct task-specific toolchains by retrieving and invoking external tools as needed.
Addresses the inefficiency and error-proneness of injecting large numbers of tool schemas into prompts by enabling dynamic tool selection and invocation.
Proposes a hierarchical vector routing algorithm for efficient tool retrieval and supports multi-turn, cross-domain toolchain construction.
Evaluates the approach on a large dataset, demonstrating significant reductions in token consumption while maintaining high accuracy.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01056</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Higher-Order Responsibility</title><link>https://arxiv.org/abs/2506.01003</link><description>Explores the concept of higher-order responsibility in group decision-making, addressing the 'responsibility gap' where no individual is clearly accountable.
Analyzes the adequacy of higher-order responsibility (up to degree d) in closing responsibility gaps.
Presents a technical result showing the computational complexity of determining sufficiency of higher-order responsibility.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01003</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery</title><link>https://arxiv.org/abs/2506.00989</link><description>Proposes BotHP, a novel framework for detecting social media bots using heterophily-aware representation learning and prototype-guided cluster discovery.
Addresses limitations of existing graph-based bot detection methods, such as reliance on labeled data and poor generalization across communities.
BotHP uses a dual-encoder architecture to model both homophily and heterophily, improving robustness against interaction camouflage and distributed bot deployment.
Demonstrates improved detection performance and generalization on real-world bot detection benchmarks.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00989</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts</title><link>https://arxiv.org/abs/2506.00965</link><description>Proposes FLEx, a federated learning framework tailored for Mixture of Experts (MoE) large language models.
FLEx enables efficient personalization by pruning the global MoE model and assigning one expert per client, reducing communication and computation overhead.
Personalized experts are trained locally, while shared modules are aggregated globally, preserving privacy and supporting non-IID data scenarios.
Demonstrates improved performance over existing federated learning baselines on instruction-based datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00965</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Aligning VLM Assistants with Personalized Situated Cognition</title><link>https://arxiv.org/abs/2506.00930</link><description>Addresses the challenge of aligning vision-language model (VLM) assistants with personalized user cognition based on sociological roles.
Introduces a new benchmark (PCogAlignBench) for evaluating personalized alignment in VLMs, featuring diverse user profiles.
Proposes a framework (PCogAlign) that uses cognition-aware, action-based reward modeling to improve personalized alignment.
Demonstrates the effectiveness of the approach through experiments and human evaluations, with plans to open-source resources.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00930</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models</title><link>https://arxiv.org/abs/2506.00911</link><description>Introduces Conformal Arbitrage, a framework for balancing competing objectives (e.g., helpfulness vs. harmlessness, cost vs. accuracy) in language model deployments.
Uses conformal risk control to provide guarantees that undesirable events (such as safety violations) do not exceed a specified quota.
Operates at the API level, making it compatible with existing models and alignment techniques without requiring model retraining.
Demonstrates improved performance over random routing, supporting trustworthy and cost-effective deployment of large language models.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00911</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Theory of Agents as Tool-Use Decision-Makers</title><link>https://arxiv.org/abs/2506.00886</link><description>Proposes a unified theory for autonomous agents, focusing on their epistemic (knowledge-based) foundations.
Argues for aligning agents' decision-making boundaries with their knowledge boundaries to improve efficiency and safety.
Suggests a shift from action-based to knowledge-driven agent design, which has implications for agent alignment and robustness.
Addresses foundational questions relevant to the safe and responsible development of autonomous AI agents.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00886</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Predicting Empirical AI Research Outcomes with Language Models</title><link>https://arxiv.org/abs/2506.00794</link><description>Introduces a benchmark and system for predicting the empirical success of AI research ideas using language models.
Compares the performance of a fine-tuned GPT-4.1 system with human experts in predicting which research ideas (e.g., jailbreaking methods) will perform better.
Demonstrates that the system outperforms human experts and off-the-shelf LMs, and validates robustness against superficial cues.
Highlights potential applications for accelerating empirical AI research and improving AI ideation models.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00794</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning</title><link>https://arxiv.org/abs/2506.00782</link><description>Introduces Jailbreak-R1, a reinforcement learning-based framework for automated red teaming of large language models (LLMs).
Focuses on generating diverse and effective jailbreak prompts to test and expose vulnerabilities in LLMs.
Describes a three-stage training process: imitation learning, exploration with diversity/consistency rewards, and progressive jailbreak enhancement.
Demonstrates improved efficiency and effectiveness in red team exploration compared to existing methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00782</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CoP: Agentic Red-teaming for Large Language Models using Composition of Principles</title><link>https://arxiv.org/abs/2506.00781</link><description>Introduces an agentic workflow (CoP) to automate and scale red-teaming for Large Language Models (LLMs).
Leverages human-provided red-teaming principles to guide AI agents in generating effective jailbreak prompts.
Demonstrates that the CoP framework can discover novel jailbreak strategies and significantly increase attack success rates against leading LLMs.
Highlights the importance of proactive risk assessment and safety alignment in LLM deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00781</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Do not Abstain! Identify and Solve the Uncertainty</title><link>https://arxiv.org/abs/2506.00780</link><description>Introduces ConfuseBench, a benchmark to evaluate LLMs' ability to recognize and address uncertainty from document scarcity, limited capability, and query ambiguity.
Finds that current LLMs often misattribute uncertainty, typically blaming query ambiguity rather than their own limitations.
Proposes a method to generate context-aware inquiries and uses InteractDPO for improved training, leading to better identification and resolution of uncertainty.
Demonstrates that their approach improves LLMs' handling of uncertain scenarios.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00780</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?</title><link>https://arxiv.org/abs/2506.00751</link><description>Investigates the consistency between stated and revealed preferences in large language models (LLMs), highlighting potential misalignment.
Proposes a formal method to measure preference deviation using a dataset of binary choice prompts and KL divergence metrics.
Finds that minor changes in prompt format can significantly alter LLM decisions, raising concerns about interpretability and control.
Emphasizes the importance of understanding these deviations for safe and ethical deployment of LLMs, especially in high-stakes or autonomous settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00751</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title><link>https://arxiv.org/abs/2506.00641</link><description>Introduces AgentAuditor, a framework for evaluating the safety and security of LLM-based agents at a human-expert level.
Uses memory-augmented reasoning and retrieval-augmented generation to improve the accuracy of LLM evaluators in identifying risks.
Presents a new benchmark dataset (2293 annotated records, 15 risk types, 29 scenarios) for assessing LLM-based evaluators on safety and security.
Demonstrates that AgentAuditor achieves state-of-the-art performance in detecting safety and security risks in LLM agents.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00641</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents</title><link>https://arxiv.org/abs/2506.00618</link><description>Introduces RiOSWorld, a benchmark for evaluating the safety risks of multimodal large language model (MLLM) agents in real-world computer-use scenarios.
Covers 492 risky tasks across various computer applications, including web, social media, multimedia, OS, email, and office software.
Categorizes risks into user-originated and environmental, and evaluates agents based on risk goal intention and completion.
Finds that current MLLM-based computer-use agents face significant safety risks, emphasizing the need for improved safety alignment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00618</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs</title><link>https://arxiv.org/abs/2506.00582</link><description>Investigates overconfidence and confidence calibration in large language models (LLMs) compared to human patterns.
Finds that LLMs show less sensitivity to task difficulty and can exhibit biased confidence estimations based on prompted personas.
Proposes a new method, Answer-Free Confidence Estimation (AFCE), to improve LLM confidence calibration and interpretability.
Demonstrates that AFCE reduces overconfidence and aligns LLM confidence more closely with human-like patterns.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00582</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs</title><link>https://arxiv.org/abs/2506.00577</link><description>Investigates post-training techniques (SFT and RLVR) to improve LLM generalization in multi-agent and economic reasoning scenarios.
Introduces Recon, a 7B-parameter LLM fine-tuned on economic reasoning problems to enhance structured reasoning and economic rationality.
Demonstrates that domain-aligned post-training can improve agent alignment and reasoning in multi-agent contexts.
Highlights implications for model alignment and behavior shaping, relevant to safe and robust AI deployment in economic and policy domains.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00577</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge</title><link>https://arxiv.org/abs/2506.00570</link><description>Proposes a new brain-inspired architecture ('Wenlu') for secure integration of foundation models and domain-specific knowledge.
Focuses on privacy-preserving fusion of user-private data with public AI models for multimodal cognition and decision-making.
Highlights security, privacy, and robustness in multimodal processing and end-to-end hardware control.
Demonstrates applications in sensitive domains such as enterprise, medical, and autonomous systems, emphasizing secure and sustainable AI deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00570</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Monitoring Robustness and Individual Fairness</title><link>https://arxiv.org/abs/2506.00496</link><description>Proposes runtime monitoring for input-output robustness and individual fairness in deployed, black-box AI models.
Introduces Clemont, a tool offering lightweight monitors for detecting when similar inputs yield dissimilar outputs, indicating robustness or fairness violations.
Presents novel algorithms and parallelization techniques to efficiently monitor robustness violations in real time.
Demonstrates effectiveness of the approach using benchmarks related to adversarial robustness and individual fairness.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00496</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs</title><link>https://arxiv.org/abs/2506.00430</link><description>Introduces MIRROR, a cognitive architecture for LLMs that implements persistent internal reasoning and reflection between conversational turns.
Addresses critical LLM failure modes such as sycophancy, attentional deficits, and inconsistent prioritization of conflicting constraints.
Demonstrates significant improvements in safety-critical conversational scenarios, particularly where conflicting user preferences are present.
Bridges cognitive science and AI safety by enhancing multi-turn consistency and context awareness in dialogue systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00430</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies</title><link>https://arxiv.org/abs/2506.00328</link><description>Introduces BASIL, a method for generating interpretable, symbolic policies for reinforcement learning (RL) using evolutionary search.
Focuses on producing compact, rule-based policies that enhance transparency and verifiability in safety-critical applications.
Demonstrates that BASIL achieves performance comparable to deep RL while maintaining interpretability and tractable complexity.
Addresses the challenge of human oversight and verification in autonomous decision-making systems.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00328</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models</title><link>https://arxiv.org/abs/2506.00258</link><description>Analyzes how multimodal large language models (MLLMs) handle implicit reasoning scenarios where flaws must be inferred from context.
Finds that MLLMs often fail to detect hidden issues, prioritizing user compliance over surfacing potential problems.
Demonstrates that explicit prompting and interventions (e.g., requiring clarifying questions) can improve model trustworthiness.
Highlights a gap between reasoning competence and behavioral compliance, with implications for safe deployment in real-world environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00258</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise</title><link>https://arxiv.org/abs/2506.00242</link><description>Proposes a soft prompt fine-tuning framework for cultural alignment in large language models (LLMs).
Introduces a modular system using culturally specialized 'expert' LLM configurations to enhance cultural sensitivity.
Demonstrates significant improvements in cultural alignment scores without altering base model parameters.
Addresses the need for robust, culturally-aware AI systems for global deployment.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00242</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Ethical AI: Towards Defining a Collective Evaluation Framework</title><link>https://arxiv.org/abs/2506.00233</link><description>Proposes a modular ethical assessment framework for AI systems using interpretable ontological blocks.
Addresses key ethical concerns such as fairness, accountability, data ownership, privacy, and systemic bias.
Integrates FAIR principles and aligns with legal requirements like the EU AI Act.
Demonstrates the framework with a real-world use case in AI-powered investor profiling, enabling risk classification and explainability.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00233</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings</title><link>https://arxiv.org/abs/2506.00178</link><description>Introduces DEEVO, a framework for evolving LLM prompts using structured debates and Elo ratings.
Addresses the challenge of prompt engineering, especially for tasks lacking clear optimization objectives.
DEEVO leverages debate-driven evaluation and evolutionary strategies to optimize prompts without requiring ground truth feedback.
Demonstrates improved performance over manual and existing automated prompt optimization methods.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00178</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Utilizing AI for Aviation Post-Accident Analysis Classification</title><link>https://arxiv.org/abs/2506.00169</link><description>Explores the use of AI and NLP to automate analysis of aviation safety reports.
Focuses on classifying aircraft damage levels and identifying flight phases during incidents.
Applies topic modeling to uncover patterns and themes in incident reports for safety improvement.
Compares deep learning and topic modeling techniques on multiple aviation safety datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00169</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Balancing Profit and Fairness in Risk-Based Pricing Markets</title><link>https://arxiv.org/abs/2506.00140</link><description>Proposes an AI-assisted regulatory framework to balance profit and fairness in risk-based pricing markets such as health insurance and consumer credit.
Introduces MarketSim, a simulator for modeling heterogeneous consumers and profit-maximizing firms, enabling the study of fairness interventions.
Uses reinforcement learning to train a social planner that implements interpretable fairness taxes, improving demand-fairness and social welfare.
Demonstrates that AI-driven regulation can address social dilemmas and promote fairness in competitive markets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00140</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets</title><link>https://arxiv.org/abs/2506.00073</link><description>Investigates the use of AI agents for fully automated negotiations and transactions in consumer markets.
Develops an experimental framework to evaluate the negotiation performance of different LLM agents.
Finds significant disparities in outcomes based on the choice of agent, leading to potential financial risks.
Highlights behavioral anomalies in LLMs that can cause overspending or acceptance of unfavorable deals, emphasizing the risks of automation.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00073</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An improved three factor authentication protocol for wireless body area networks</title><link>https://openalex.org/W4399770462</link><description>Proposes an improved three-factor authentication protocol tailored for wireless body area networks (WBANs).
Addresses security and privacy concerns in the transmission of sensitive health data over WBANs.
Evaluates the protocol's resistance to various attacks and its efficiency in resource-constrained environments.</description><guid isPermaLink="false">https://openalex.org/W4399770462</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
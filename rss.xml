<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 29 Dec 2025 22:57:03 +0000</lastBuildDate><item><title>RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</title><link>https://arxiv.org/abs/2512.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoboSafe, a hybrid executable predicate-based safety logic for embodied agents that integrates short-term backward reflective reasoning and long-term forward predictive reasoning over a Hybrid Long-Short Safety Memory.&lt;/li&gt;&lt;li&gt;Backward Reflective Reasoning revisits recent trajectories to infer temporal safety predicates and trigger replanning when violations are detected; Forward Predictive Reasoning anticipates upcoming risks from long-term memory and multimodal observations.&lt;/li&gt;&lt;li&gt;Demonstrates substantial reduction in hazardous actions (~36.8% fewer risk occurrences) across multiple agents while retaining task performance, with real-world robotic arm evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Wang', 'Zonghao Ying', 'Xiao Yang', 'Quanchen Zou', 'Zhenfei Yin', 'Tianlin Li', 'Jian Yang', 'Yaodong Yang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['embodied agents', 'runtime safety', 'safety guardrails', 'vision-language models', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21220</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</title><link>https://arxiv.org/abs/2510.13626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic robustness/vulnerability analysis of vision-language-action (VLA) models via controlled perturbations across seven dimensions (object layout, camera viewpoint, robot initial state, language instructions, lighting, background textures, sensor noise).&lt;/li&gt;&lt;li&gt;Finds consistent brittleness: performance can drop from ~95% to below 30% under modest perturbations, with extreme sensitivity to camera viewpoint and robot initial states.&lt;/li&gt;&lt;li&gt;Surprisingly, models are largely insensitive to language variations and may ignore instructions, raising concerns about true task understanding and reliability.&lt;/li&gt;&lt;li&gt;Argues that high benchmark scores mask real-world vulnerabilities and calls for evaluation practices focused on reliability under realistic variation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senyu Fei', 'Siyin Wang', 'Junhao Shi', 'Zihao Dai', 'Jikun Cai', 'Pengfang Qian', 'Li Ji', 'Xinzhe He', 'Shiduo Zhang', 'Zhaoye Fei', 'Jinlan Fu', 'Jingjing Gong', 'Xipeng Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'vision-language-action', 'adversarial-perturbations', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13626</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability</title><link>https://arxiv.org/abs/2502.09990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that prior defenses against multi-turn jailbreaks trade off robustness for usability by blurring the boundary between safe and harmful feature representations.&lt;/li&gt;&lt;li&gt;Proposes X-Boundary, a method to push harmful representations away from boundary-safe representations to create an exact distinction and precisely erase harmful behavior without disrupting safe capabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art defense performance against multi-turn jailbreaks, ~20% reduction in over-refusal, near-complete preservation of general capabilities, theoretical convergence analysis, and empirical verification; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Dongrui Liu', 'Yi Yu', 'Luxin Xu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak defense', 'alignment', 'robustness', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09990</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation</title><link>https://arxiv.org/abs/2512.20815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes end-to-end RAW-to-task co-design that jointly optimizes optics, sensor modeling (learnable CFA, noise, quantization), and a lightweight semantic segmentation network for driving.&lt;/li&gt;&lt;li&gt;Demonstrates consistent mIoU gains on KITTI-360, with optics modeling and CFA learning giving the largest improvements, especially for thin or low-light-sensitive classes.&lt;/li&gt;&lt;li&gt;Shows improved robustness to blur, noise, and low bit-depth while remaining deployable (≈1M parameters, ~28 FPS), and provides visual/quantitative analyses of how co-designed sensors preserve semantic structure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reeshad Khan', 'John Gauch']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sensor co-design', 'autonomous driving', 'semantic segmentation', 'edge deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20815</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning</title><link>https://arxiv.org/abs/2512.12667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses Open-World DeepFake Attribution (OW-DFA): attributes both known and unknown forgery types using labeled known data and unlabeled mixed data.&lt;/li&gt;&lt;li&gt;Identifies two problems in prior OW-DFA: confidence skew leading to biased pseudo-labels, and unrealistic assumption that the number of unknown forgery types is known a priori.&lt;/li&gt;&lt;li&gt;Proposes Confidence-Aware Asymmetric Learning (CAL) with two components—Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR)—plus Dynamic Prototype Pruning (DPP) to estimate novel class count; reports state-of-the-art results on benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiyang Zheng', 'Nan Pu', 'Wenjing Li', 'Teng Long', 'Nicu Sebe', 'Zhun Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake attribution', 'open-world detection', 'confidence calibration', 'pseudo-labeling', 'prototype pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12667</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy</title><link>https://arxiv.org/abs/2509.23022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes copyright infringement detection for text-to-image diffusion models using a differential-privacy-inspired metric (conditional sensitivity) to measure output changes from including/excluding a training example.&lt;/li&gt;&lt;li&gt;Proposes D-Plus-Minus (DPM), a post-hoc framework that simulates inclusion/exclusion by fine-tuning models in opposite directions (learning/unlearning) and computes confidence scores over orthogonal prompt distributions to isolate concept-specific influence.&lt;/li&gt;&lt;li&gt;Introduces the Copyright Infringement Detection Dataset (CIDD) to benchmark detection across diverse categories.&lt;/li&gt;&lt;li&gt;Claims reliable detection of memorized/infringing content without access to original training data or prompts, offering an interpretable, practical IP-protection method for generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiafeng Man', 'Zhipeng Wei', 'Jingjing Chen']&lt;/li&gt;&lt;li&gt;Tags: ['copyright detection', 'model memorization', 'differential privacy', 'diffusion models', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23022</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features</title><link>https://arxiv.org/abs/2502.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAE-Net, an ensemble combining EfficientNet, DeiT, and ConvNeXt with wavelet (frequency-domain) features to detect deepfake images.&lt;/li&gt;&lt;li&gt;Introduces a multistage disjoint-subset training strategy to mitigate severe fake-to-real class imbalance in the DF-Wild Cup dataset.&lt;/li&gt;&lt;li&gt;Reports strong performance (94.46% accuracy, 97.60% AUC) and presents visualizations showing attention on meaningful facial regions.&lt;/li&gt;&lt;li&gt;Claims robustness against adversarial attacks, positioning the method for generalized deepfake detection and practical security use cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Bhattacharjee', 'Kaidul Islam', 'Kafi Anan', 'Ashir Intesher', 'Abrar Assaeem Fuad', 'Utsab Saha', 'Hafiz Imtiaz']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'image-forensics', 'ensemble-models', 'class-imbalance', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10682</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</title><link>https://arxiv.org/abs/2512.22046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows classic backdoor methods (e.g., BadNet) fail on prompt-driven video segmentation foundation models (VSFMs) and analyzes why via encoder gradients and attention.&lt;/li&gt;&lt;li&gt;Proposes BadVSFM, a two-stage backdoor framework: (1) steer the image encoder so triggered frames map to a target embedding while clean frames align with a reference encoder; (2) train the mask decoder so triggered frame–prompt pairs yield a shared target mask while clean outputs stay close to a reference decoder.&lt;/li&gt;&lt;li&gt;Evaluates on multiple datasets and five VSFMs, demonstrating strong, controllable backdoor effects across triggers and prompts with preserved clean performance; performs ablations and visual/gradient analyses.&lt;/li&gt;&lt;li&gt;Finds four representative defenses largely ineffective, highlighting an underexplored vulnerability in prompt-driven VSFMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongmin Zhang', 'Zhen Sun', 'Yifan Liao', 'Wenhan Dong', 'Xinlei He', 'Xingshuo Han', 'Shengmin Xu', 'Xinyi Huang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model poisoning', 'foundation models', 'video segmentation', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22046</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Patch-Discontinuity Mining for Generalized Deepfake Detection</title><link>https://arxiv.org/abs/2512.22027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GenDF, a lightweight framework that adapts a large-scale vision model for deepfake detection with only 0.28M trainable parameters.&lt;/li&gt;&lt;li&gt;Introduces three components: deepfake-specific representation learning, feature space redistribution to reduce distribution mismatch, and classification-invariant feature augmentation to improve generalization without extra trainable parameters.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art generalization in cross-domain and cross-manipulation evaluations, emphasizing robustness to unseen forgery patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanhuan Yuan', 'Yang Ping', 'Zhengqin Xu', 'Junyi Cao', 'Shuai Jia', 'Chao Ma']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness/generalization', 'forensic analysis', 'vision models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22027</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</title><link>https://arxiv.org/abs/2512.21999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALEAHallu: an Activate-Locate-Edit Adversarially framework to mitigate hallucinations in VLMs by parametric editing.&lt;/li&gt;&lt;li&gt;Constructs an activation dataset of grounded (visually-anchored) vs hallucinatory responses and analyzes differential hidden states to locate hallucination-prone parameter clusters.&lt;/li&gt;&lt;li&gt;Fine-tunes identified parameter clusters with prompts injected with adversarially tuned prefixes designed to maximize visual neglect, forcing the model to prioritize visual evidence.&lt;/li&gt;&lt;li&gt;Evaluated on generative and discriminative VLM tasks and reported significant reductions in hallucination; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Hu', 'Beibei Li', 'Jiangwei Xia', 'Yanjun Qin', 'Bing Ji', 'Zhongshi He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'parametric model editing', 'adversarial fine-tuning', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21999</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LVLM-Aided Alignment of Task-Specific Vision Models</title><link>https://arxiv.org/abs/2512.21985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LVLM-Aided Visual Alignment (LVLM-VA): uses a Large Vision Language Model to translate model behavior into natural language and convert class-level human specifications into image-level critiques.&lt;/li&gt;&lt;li&gt;Provides a bidirectional interface enabling domain experts to interact with and correct small task-specific vision models without fine-grained annotation.&lt;/li&gt;&lt;li&gt;Empirically shows reduced reliance on spurious features and mitigation of group-specific biases on synthetic and real-world datasets, improving model alignment with human domain knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Koebler', 'Lukas Kuhn', 'Ingo Thon', 'Florian Buettner']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LVLM', 'bias-mitigation', 'spurious-correlation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21985</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2512.21964</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of how various real-world perturbations (11 noise types) in both image and text modalities affect medical multi-modal LLMs.&lt;/li&gt;&lt;li&gt;Proposes a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for text denoising.&lt;/li&gt;&lt;li&gt;Constructs a benchmark over 2 datasets and demonstrates improved robustness/SOTA performance across modalities without fine-tuning, targeting safer clinical deployment of MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dunyuan XU', 'Xikai Yang', 'Yaoqian Li', 'Juzheng Miao', 'Jinpeng Li', 'Pheng-Ann Heng']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'medical-ML', 'multimodal-LLM', 'denoising', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21964</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2512.21815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that ~20% of high-entropy tokens (critical decision points) disproportionately govern VLM generation trajectories.&lt;/li&gt;&lt;li&gt;Shows that concentrating adversarial perturbations on those tokens yields semantic degradation comparable to global methods but with much smaller budgets.&lt;/li&gt;&lt;li&gt;Reports conversion of 35–49% of benign outputs into harmful ones across multiple VLMs and transferability of attacks (17–26% harmful rates on unseen targets).&lt;/li&gt;&lt;li&gt;Proposes Entropy-bank Guided Adversarial attacks (EGA) achieving high attack success (93–95%) and high harmful conversion, revealing safety weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengqi He', 'Xinyu Tian', 'Xin Shen', 'Jinhong Ni', 'Shu Zou', 'Zhaoyuan Yang', 'Jing Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'vision-language-models', 'safety-vulnerability', 'entropy-guided-attacks', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21815</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.21695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FUSE, a hybrid detector that fuses spectral features (FFT) with semantic features from CLIP's vision encoder into a joint representation, trained in two progressive stages.&lt;/li&gt;&lt;li&gt;Evaluated across multiple benchmarks (GenImage, WildFake, DiTFake, GPT-ImgEval, Chameleon) and shows strong generalization and state-of-the-art or competitive performance, particularly on Chameleon high-fidelity images.&lt;/li&gt;&lt;li&gt;Demonstrates that integrating spectral and semantic cues improves robustness to diverse generative models and yields high mean accuracy and mAP across tested generators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Zahid Hossain', 'Most. Sharmin Sultana Samu', 'Md. Kamrozzaman Bhuiyan', 'Farhad Uz Zaman', 'Md. Rakibul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'spectral analysis (FFT)', 'semantic features (CLIP)', 'robustness/generalization', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21695</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</title><link>https://arxiv.org/abs/2512.21684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SlideChain, a blockchain-backed provenance framework that records cryptographic hashes of structured semantic extraction outputs from VLMs to provide tamper-evident audit trails and persistent semantic baselines.&lt;/li&gt;&lt;li&gt;Uses a curated SlideChain Slides Dataset (1,117 medical imaging lecture slides) and extracts concepts and relational triples from four state-of-the-art VLMs to analyze cross-model semantic disagreement and reproducibility.&lt;/li&gt;&lt;li&gt;Implements anchoring on a local EVM-compatible blockchain and evaluates gas usage, throughput, scalability, demonstrating deterministic reproducibility and perfect tamper detection in their setup.&lt;/li&gt;&lt;li&gt;Finds pronounced cross-model discrepancies (low concept overlap, near-zero agreement on relational triples) and positions provenance anchoring as a practical measure for long-term auditability and integrity of AI-assisted educational pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Motaleb Hossen Manik', 'Md Zabirul Islam', 'Ge Wang']&lt;/li&gt;&lt;li&gt;Tags: ['provenance', 'tamper-evidence', 'blockchain', 'data integrity', 'reproducibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21684</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</title><link>https://arxiv.org/abs/2512.21670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a mechanistic interpretability framework for deepfake detection combining sparse autoencoder (SAE) analysis with a novel forensic manifold analysis.&lt;/li&gt;&lt;li&gt;Shows that only a small fraction of latent features are active per layer and that manifold geometry (intrinsic dimensionality, curvature, feature selectivity) systematically varies with different deepfake artifacts.&lt;/li&gt;&lt;li&gt;Maps specific learned features to forensic artifacts, providing interpretable links between internal representations and types of synthetic manipulations.&lt;/li&gt;&lt;li&gt;Positions these insights as a step toward more interpretable and potentially more robust deepfake detectors useful for forensic analysis and model debugging.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subramanyam Sahoo', 'Jared Junkin']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'interpretability', 'forensic-analysis', 'robustness', 'mechanistic-interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21670</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration of Reproducible Generated Image Detection</title><link>https://arxiv.org/abs/2512.21562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews 7 key papers on AI-generated image (AIGC) detection and constructs a lightweight test dataset.&lt;/li&gt;&lt;li&gt;Reproduces a representative detection method and shows basic performance can be matched when core procedures are strictly followed.&lt;/li&gt;&lt;li&gt;Identifies reproducibility issues caused by omitted preprocessing/parameter details and overfitting to generator-specific artifacts, leading to poor cross-generator generalization.&lt;/li&gt;&lt;li&gt;Demonstrates that detection performance degrades sharply when preprocessing disrupts key features or when evaluating across different generators, and calls for more comprehensive experimental disclosure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'Reproducibility', 'Robustness', 'Benchmarking', 'Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21562</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art</title><link>https://arxiv.org/abs/2512.21512</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a fixed-threshold evaluation protocol for AI-generated image detectors: thresholds chosen once on clean validation data and held constant across post-processing (JPEG, blur, downscaling) to avoid retuning inflation.&lt;/li&gt;&lt;li&gt;Proposes and evaluates a lightweight CNN–ViT hybrid (with gated fusion and optional frequency enhancement) across three operating points (Low-FPR, ROC-optimal, Best-F1) and multiple seeds, reporting AUROC/accuracy on photo and art datasets.&lt;/li&gt;&lt;li&gt;Finds a forensic–semantic spectrum: frequency-aided CNNs perform very well on pristine photos but degrade substantially under compression, ViTs degrade minimally and rely on semantic cues, and hybrids provide balanced cross-domain performance; artistic images are easier to detect than photorealistic ones.&lt;/li&gt;&lt;li&gt;Provides deployment guidance (use CNNs for clean photos, ViTs for compressed content, hybrids for art/graphics) and argues fixed-threshold evaluation reveals realistic robustness gaps important for forensic/security use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Ashik Khan', 'Arafat Alam Jion']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'robustness evaluation', 'fixed-threshold protocol', 'CNN-ViT hybrid', 'post-processing resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21512</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</title><link>https://arxiv.org/abs/2510.13626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic robustness/vulnerability analysis of vision-language-action (VLA) models via controlled perturbations across seven dimensions (object layout, camera viewpoint, robot initial state, language instructions, lighting, background textures, sensor noise).&lt;/li&gt;&lt;li&gt;Finds consistent brittleness: performance can drop from ~95% to below 30% under modest perturbations, with extreme sensitivity to camera viewpoint and robot initial states.&lt;/li&gt;&lt;li&gt;Surprisingly, models are largely insensitive to language variations and may ignore instructions, raising concerns about true task understanding and reliability.&lt;/li&gt;&lt;li&gt;Argues that high benchmark scores mask real-world vulnerabilities and calls for evaluation practices focused on reliability under realistic variation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senyu Fei', 'Siyin Wang', 'Junhao Shi', 'Zihao Dai', 'Jikun Cai', 'Pengfang Qian', 'Li Ji', 'Xinzhe He', 'Shiduo Zhang', 'Zhaoye Fei', 'Jinlan Fu', 'Jingjing Gong', 'Xipeng Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'vision-language-action', 'adversarial-perturbations', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13626</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability</title><link>https://arxiv.org/abs/2502.09990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that prior defenses against multi-turn jailbreaks trade off robustness for usability by blurring the boundary between safe and harmful feature representations.&lt;/li&gt;&lt;li&gt;Proposes X-Boundary, a method to push harmful representations away from boundary-safe representations to create an exact distinction and precisely erase harmful behavior without disrupting safe capabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art defense performance against multi-turn jailbreaks, ~20% reduction in over-refusal, near-complete preservation of general capabilities, theoretical convergence analysis, and empirical verification; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Dongrui Liu', 'Yi Yu', 'Luxin Xu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak defense', 'alignment', 'robustness', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09990</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title><link>https://arxiv.org/abs/2512.07015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FVA-RAG: flip RAG pipeline to treat initial LLM output as a hypothesis and retrieve targeted counter-evidence (anti-context) to falsify it.&lt;/li&gt;&lt;li&gt;Evaluates on TruthfulQA-Generation (N=817) with frozen corpora and identical retrieval budgets, using gpt-4o for generation and deterministic judging.&lt;/li&gt;&lt;li&gt;Reports significant improvements in accuracy (≈79.8–80.05%) over Self-RAG and CRAG variants, and shows falsification triggered on ~24.5–29.3% of queries, indicating counter-evidence retrieval reduces sycophantic hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Ravishankara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'retrieval-augmented generation', 'adversarial/targeted retrieval', 'truthfulness/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07015</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Focus Memory for Language Models</title><link>https://arxiv.org/abs/2511.12712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Focus Memory (AFM), a lightweight context manager that assigns past messages one of three fidelity levels (Full, Compressed, Placeholder) and packs them under a fixed token budget based on relevance, temporal decay, and importance.&lt;/li&gt;&lt;li&gt;Evaluates AFM on multi-turn dialogue benchmarks stressing long-horizon constraint preservation (a safety-critical peanut allergy travel scenario and a policy-critical tax-evasion scenario), showing substantial gains in retaining constraints and correct refusal behavior.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety/alignment in deployed chat settings without changing model weights or adding external retrieval; provides an open-source implementation compatible with OpenAI-style chat APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'context management', 'constraint preservation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12712</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</title><link>https://arxiv.org/abs/2511.09109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bi-RAR, a retrieval-augmented reasoning framework that evaluates each intermediate reasoning step both forward (how far from the answer) and backward (how well it addresses the question).&lt;/li&gt;&lt;li&gt;Introduces a bidirectional information distance based on Kolmogorov complexity, approximated via language-model generation probabilities, to quantify information completeness of steps.&lt;/li&gt;&lt;li&gt;Optimizes reasoning with a multi-objective reinforcement learning approach using a cascading reward structure to prioritize early trajectory alignment and reduce reward hacking.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over prior methods on seven question-answering benchmarks and enables efficient search-engine interaction during training and inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenda Wei', 'Yu-An Liu', 'Ruqing Zhang', 'Jiafeng Guo', 'Lixin Su', 'Shuaiqiang Wang', 'Dawei Yin', 'Maarten de Rijke', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmented-generation', 'hallucination-mitigation', 'multi-objective-reinforcement-learning', 'reward-design', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09109</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CC-GSEO-Bench: A Content-Centric Benchmark for Measuring Source Influence in Generative Search Engines</title><link>https://arxiv.org/abs/2509.05607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CC-GSEO-Bench, a content-centric benchmark and large-scale dataset (≈1,000 source articles, &gt;5,000 query-article pairs) organized for article-level evaluation of influence on generative search engines (GSEs).&lt;/li&gt;&lt;li&gt;Operationalizes source influence along three core dimensions — Exposure, Faithful Credit, and Causal Impact — plus two content-quality dimensions: Readability &amp; Structure and Trustworthiness &amp; Safety.&lt;/li&gt;&lt;li&gt;Dataset construction uses seed queries from public QA datasets with limited synthetic augmentation and a follow-up retrieval check to ensure realistic retrieval grounding; aggregates query-level signals into article-level influence metrics and analyzes influence dynamics across content patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiyuan Chen', 'Jiahe Chen', 'Hongsen Huang', 'Qian Shao', 'Jintai Chen', 'Renjie Hua', 'Hongxia Xu', 'Ruijia Wu', 'Ren Chuan', 'Jian Wu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'generative-search', 'benchmark', 'source-influence', 'trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05607</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal</title><link>https://arxiv.org/abs/2509.01455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniCR, a unified framework that fuses heterogeneous uncertainty evidence (likelihoods, self-consistency dispersion, retrieval compatibility, tool/verifier feedback) into a calibrated probability of correctness and enforces a user-specified error budget via principled refusal.&lt;/li&gt;&lt;li&gt;Learns a lightweight calibration head (temperature scaling and proper scoring), supports API-only (black-box) models, and provides distribution-free guarantees using conformal risk control for abstention decisions.&lt;/li&gt;&lt;li&gt;Aligns confidence with semantic fidelity for long-form generation by supervising on atomic factuality scores from retrieved evidence, reducing confident hallucinations while preserving coverage.&lt;/li&gt;&lt;li&gt;Shows empirical improvements in calibration metrics, lower area under the risk-coverage curve, and higher coverage at fixed risk across short-form QA, code generation (execution tests), and retrieval-augmented long-form QA compared to entropy/logit thresholds and other baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Markus Oehri', 'Giulia Conti', 'Kaviraj Pather', 'Alexandre Rossi', 'Laia Serra', 'Adrian Parody', 'Rogvi Johannesen', 'Aviaja Petersen', 'Arben Krasniqi']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'selective prediction / refusal', 'LLM safety', 'risk-controlled abstention', 'factuality / hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01455</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases</title><link>https://arxiv.org/abs/2508.12411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of a "cultural gene" for LLMs and builds a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI).&lt;/li&gt;&lt;li&gt;Compares a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot) using zero-shot prompts and human annotation, finding statistically significant divergences on both cultural dimensions.&lt;/li&gt;&lt;li&gt;Proposes a Cultural Alignment Index (CAI) against Hofstede national scores, showing GPT-4 aligns more with the USA and ERNIE Bot with China, and presents qualitative analyses of dilemmas and authority judgments.&lt;/li&gt;&lt;li&gt;Argues for culturally aware evaluation and deployment to mitigate algorithmic cultural hegemony.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emanuel Z. Fenech-Borg', 'Tilen P. Meznaric-Kos', 'Milica D. Lekovic-Bojovic', 'Arni J. Hentze-Djurhuus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'evaluation', 'cultural bias', 'LLM values']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12411</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management</title><link>https://arxiv.org/abs/2512.22060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SC-NLP-LMF, a six-phase organizational framework for secure and compliant NLP model lifecycle management (development to retirement).&lt;/li&gt;&lt;li&gt;Built from a PRISMA systematic review aligning with standards (NIST AI RMF, ISO/IEC 42001:2023, EU AI Act, MITRE ATLAS) and integrates techniques for bias detection, privacy (differential privacy, federated learning), secure deployment, explainability, and decommissioning.&lt;/li&gt;&lt;li&gt;Includes a healthcare case study illustrating detection of terminology drift and guided compliant model updates in a high-risk domain.&lt;/li&gt;&lt;li&gt;Aims to provide practical organizational processes and controls for operating NLP systems in regulated environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunil Arora', 'John Hastings']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'privacy', 'compliance', 'lifecycle management', 'NLP governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22060</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures</title><link>https://arxiv.org/abs/2512.21551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a workshop on bidirectional (reciprocal) human-AI alignment where humans and AI co-adapt through interaction, evaluation, and design.&lt;/li&gt;&lt;li&gt;Emphasizes embedding human and societal values into alignment research and enabling humans to critically engage and evolve with AI systems.&lt;/li&gt;&lt;li&gt;Targets interdisciplinary methods for interactive alignment, frameworks for societal impact evaluation, and strategies for alignment in dynamic contexts.&lt;/li&gt;&lt;li&gt;Aims to bridge HCI, AI, social sciences, and other domains to establish a shared agenda for responsible, reciprocal human-AI futures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Shen (Cassandra)', 'Tiffany Knearem (Cassandra)', 'Divy Thakkar (Cassandra)', 'Pat Pataranutaporn (Cassandra)', 'Anoop Sinha (Cassandra)', 'Yike (Cassandra)', 'Shi', 'Jenny T. Liang', 'Lama Ahmad', 'Tanu Mitra', 'Brad A. Myers', 'Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI interaction', 'value-centered design', 'evaluation', 'societal impact']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21551</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks</title><link>https://arxiv.org/abs/2512.21345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Query Carefully, a pipeline that combines LLM-based text-to-SQL generation with explicit detection and handling of unanswerable or out-of-scope natural language queries to avoid misleading executable SQL outputs.&lt;/li&gt;&lt;li&gt;Creates OncoMX-NAQ, an 80-question no-answer benchmark across 8 categories (non-SQL, out-of-schema/domain, and various ambiguity types) derived from a biomedical Text-to-SQL dataset.&lt;/li&gt;&lt;li&gt;Uses Llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples (both answerable and unanswerable); evaluates SQL exact match, result accuracy, and unanswerable-detection accuracy.&lt;/li&gt;&lt;li&gt;Findings: balanced prompting yields 0.8 unanswerable-detection accuracy with near-perfect detection for structural categories but poor performance on missing-value (0.5) and column-ambiguity (0.3) queries; includes a lightweight UI exposing SQL, execution results, and abstentions for transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jasmin Saxer (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Isabella Maria Aigner (Institute of Medical Virology', 'University of Zurich', 'Zurich', 'Switzerland)', 'Luise Linzmeier (Department of Gastroenterology and Hepatology', 'University Hospital Zurich', 'University of Zurich', 'Zurich', 'Switzerland)', 'Andreas Weiler (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Kurt Stockinger (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'unanswerable detection', 'text-to-SQL robustness', 'prompt engineering', 'biomedical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21345</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?</title><link>https://arxiv.org/abs/2512.21871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large-scale benchmark of 50,000 multimodal query–content pairs to evaluate whether LVLMs recognize and comply with copyrighted content, including scenarios with and without copyright notices and four types of notices.&lt;/li&gt;&lt;li&gt;Evaluates multiple state-of-the-art (including closed-source) LVLMs and finds significant failures to recognize or respect copyrighted materials, even when explicit notices are present.&lt;/li&gt;&lt;li&gt;Proposes a novel tool-augmented defense framework to reduce copyright infringement risk and reports improvements in compliance across evaluated scenarios.&lt;/li&gt;&lt;li&gt;Highlights the need for copyright-aware LVLM design and provides a systematic dataset and evaluation protocol for measuring compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naen Xu', 'Jinghuai Zhang', 'Changjiang Li', 'Hengyu An', 'Chunyi Zhou', 'Jun Wang', 'Boyu Xu', 'Yuyuan Li', 'Tianyu Du', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['copyright compliance', 'LVLM safety', 'benchmarking', 'defense/tooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21871</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs</title><link>https://arxiv.org/abs/2512.21849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HeartBench, a Chinese-language benchmark for evaluating anthropomorphic intelligence across five primary dimensions and 15 secondary capabilities grounded in counseling scenarios.&lt;/li&gt;&lt;li&gt;Uses a rubric-based, 'reasoning-before-scoring' protocol to translate socio-emotional and ethical traits into measurable criteria and constructs a 'Hard Set' of challenging cases.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art LLMs, finding a substantial performance ceiling (top models ≈60% of expert ideal) and large degradation on subtle emotional and ethical trade-off cases.&lt;/li&gt;&lt;li&gt;Positions the benchmark as a standard for human-aligned training data creation and for assessing socio-emotional and ethical behavior in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Liu', 'Peiyi Tu', 'Wenyu Chen', 'Yihong Zhuang', 'Xinxia Ling', 'Anji Zhou', 'Chenxi Wang', 'Zhuo Han', 'Zhengkai Yang', 'Junbo Zhao', 'Zenan Huang', 'Yuanyuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmark', 'ethics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21849</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments</title><link>https://arxiv.org/abs/2512.21817</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeMe, a framework that modifies an LLM's method-generation pathway using explicit "decorations" derived from hidden goals, accumulated methods, and environmental feedback.&lt;/li&gt;&lt;li&gt;Decorations are not hardcoded rules but are extracted from behavioral principles, experience, and observed environmental differences to modify pre-/post-steps, intermediate steps, and insert new steps.&lt;/li&gt;&lt;li&gt;Aims to produce context-aware, safety-aligned, and environment-adaptive methods for IoT devices, improving handling of previously unseen or faulty operating conditions.&lt;/li&gt;&lt;li&gt;Includes experimental results showing that method decoration yields more appropriate methods in dynamic IoT scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hong Su']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Adaptive agents', 'IoT robustness', 'Method generation', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21817</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought</title><link>https://arxiv.org/abs/2512.21711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes latent tokens (COCONUT) vs explicit Chain-of-Thought (CoT) to assess whether latent tokens encode faithful reasoning.&lt;/li&gt;&lt;li&gt;Steering/causal perturbation experiments show COCONUT tokens are largely insensitive to targeted interventions and lack reasoning-critical information unlike CoT.&lt;/li&gt;&lt;li&gt;Shortcut experiments on MMLU and HotpotQA reveal COCONUT exploits dataset artifacts and out-of-distribution biases, inflating benchmark performance and acting as pseudo-reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyi Zhang', 'Boyu Tang', 'Tianjie Ju', 'Sufeng Duan', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Chain-of-Thought', 'Adversarial analysis', 'Model interpretability', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21711</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers</title><link>https://arxiv.org/abs/2512.21709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates detection of AI-generated paraphrases in Bengali using five transformer models (XLM-RoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base, MultilingualBERT-Base).&lt;/li&gt;&lt;li&gt;Finds zero-shot performance near chance (~50% accuracy), while fine-tuning yields large improvements (≈91% accuracy and F1 for XLM-RoBERTa, mDeBERTa, and MultilingualBERT).&lt;/li&gt;&lt;li&gt;Reports weaker fine-tuning effectiveness for IndicBERT and provides baseline results for Bengali AI-generated text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Rakibul Islam', 'Most. Sharmin Sultana Samu', 'Md. Zahid Hossain', 'Farhad Uz Zaman', 'Md. Kamrozzaman Bhuiyan']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'LLM detection', 'Bengali NLP', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21709</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations</title><link>https://arxiv.org/abs/2512.21635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HIC-Bench, a benchmark that categorizes LLM hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH) using a multi-dimensional metric matrix combining TTCT creativity metrics with hallucination-specific dimensions (scientific plausibility, factual deviation).&lt;/li&gt;&lt;li&gt;Provides cross-domain evaluation across ten scientific domains with open-ended innovation tasks and introduces Dynamic Hallucination Prompt (DHP) to steer models toward creative yet reliable outputs; scoring uses multiple LLM judges with human verification.&lt;/li&gt;&lt;li&gt;Reports a nonlinear relationship between IH and DH, claiming creativity and correctness can be jointly optimized and positioning IH as potentially valuable for scientific innovation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxu Yang', 'Jingling Yuan', 'Siqi Cai', 'Jiawei Jiang', 'Chuang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety-evaluation', 'alignment', 'benchmarking', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21635</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Definition of Hallucination, Or: It's the World Model, Stupid</title><link>https://arxiv.org/abs/2512.21577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to users (i.e., contradictions with a chosen reference world).&lt;/li&gt;&lt;li&gt;Argues that making the reference world explicit clarifies distinctions between hallucination and other model errors (e.g., planning or incentive failures).&lt;/li&gt;&lt;li&gt;Proposes a family of synthetic, fully specified benchmarks that define hallucinations as mismatches with ground-truth world models to stress-test and improve LLM world modeling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emmy Liu', 'Varun Gangal', 'Chelsea Zou', 'Xiaoqi Huang', 'Michael Yu', 'Alex Chang', 'Zhuofu Tao', 'Sachin Kumar', 'Steven Y. Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM safety', 'evaluation/benchmarking', 'world modeling', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21577</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Teaching People LLM's Errors and Getting it Right</title><link>https://arxiv.org/abs/2512.21422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes why prior attempts to teach users LLM failure patterns did not reduce overreliance, asking whether failure patterns exist, whether they can be surfaced automatically, and whether current evaluation metrics are appropriate.&lt;/li&gt;&lt;li&gt;Shows that sizable meta-labeled groups where LLMs are error-prone do exist (i.e., teachable failure patterns), but embedding- and prompting-based discovery methods yield mixed success in surfacing them.&lt;/li&gt;&lt;li&gt;Proposes a new metric that measures a user's ability to use provided failure patterns to anticipate LLM errors and reports a user study where teaching improves performance on this metric (while human-AI team accuracy did not improve).&lt;/li&gt;&lt;li&gt;Concludes that teaching failure patterns can mitigate overreliance but requires better automated failure-discovery methods and evaluation aligned to users' anticipatory abilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Stringham', 'Fateme Hashemi Chaleshtori', 'Xinyuan Yan', 'Zhichao Xu', 'Bei Wang', "Ana Marasovi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'human-AI interaction', 'failure-pattern discovery', 'overreliance/calibration', 'evaluation/metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21422</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation</title><link>https://arxiv.org/abs/2510.08078</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and formalizes 'Insertion Hallucination' (IH) in video-to-audio (V2A) models: generation of speech/music with no visual source.&lt;/li&gt;&lt;li&gt;Proposes an evaluation framework using an ensemble of audio event detectors and two metrics (IH@vid, IH@dur) to measure prevalence and duration of IH.&lt;/li&gt;&lt;li&gt;Introduces HALCON, a three-stage mitigation: generate initial audio, detect hallucinated segments and mask unreliable video features, then regenerate conditioned audio.&lt;/li&gt;&lt;li&gt;Empirical results show state-of-the-art V2A models suffer substantial IH; HALCON reduces IH prevalence and duration by &gt;50% on average without degrading (and sometimes improving) conventional audio quality and sync metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liyang Chen', 'Hongkai Chen', 'Yujun Cai', 'Sifan Li', 'Qingwen Ye', 'Yiwei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['insertion-hallucination', 'multimodal-hallucination', 'robustness', 'evaluation-metrics', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08078</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes</title><link>https://arxiv.org/abs/2503.09626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UESBD, a framework that augments social bot detectors with predictive uncertainty to reduce overconfident OOD predictions.&lt;/li&gt;&lt;li&gt;Introduces Robust Multi-modal Neural Processes (RMNP): modality-specific encoders, unimodal attentive neural processes encoding Gaussian latent variables, and an evidential gating network to model modality reliability.&lt;/li&gt;&lt;li&gt;Fuses unimodal latents via a generalized product-of-experts that weights modalities by reliability, and obtains predictions by Monte Carlo sampling from the joint latent distribution.&lt;/li&gt;&lt;li&gt;Evaluated on three real-world benchmarks, showing improved classification, calibrated uncertainty estimation, and robustness to modality conflicts caused by bot camouflage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wu', 'Yingguang Yang', 'hao liu', 'Hao Peng', 'Buyun He', 'Yutong Xia', 'Yong Liao']&lt;/li&gt;&lt;li&gt;Tags: ['social bot detection', 'robustness', 'uncertainty estimation', 'multi-modal', 'modality reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09626</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features</title><link>https://arxiv.org/abs/2502.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAE-Net, an ensemble combining EfficientNet, DeiT, and ConvNeXt with wavelet (frequency-domain) features to detect deepfake images.&lt;/li&gt;&lt;li&gt;Introduces a multistage disjoint-subset training strategy to mitigate severe fake-to-real class imbalance in the DF-Wild Cup dataset.&lt;/li&gt;&lt;li&gt;Reports strong performance (94.46% accuracy, 97.60% AUC) and presents visualizations showing attention on meaningful facial regions.&lt;/li&gt;&lt;li&gt;Claims robustness against adversarial attacks, positioning the method for generalized deepfake detection and practical security use cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Bhattacharjee', 'Kaidul Islam', 'Kafi Anan', 'Ashir Intesher', 'Abrar Assaeem Fuad', 'Utsab Saha', 'Hafiz Imtiaz']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'image-forensics', 'ensemble-models', 'class-imbalance', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10682</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability</title><link>https://arxiv.org/abs/2502.09990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that prior defenses against multi-turn jailbreaks trade off robustness for usability by blurring the boundary between safe and harmful feature representations.&lt;/li&gt;&lt;li&gt;Proposes X-Boundary, a method to push harmful representations away from boundary-safe representations to create an exact distinction and precisely erase harmful behavior without disrupting safe capabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art defense performance against multi-turn jailbreaks, ~20% reduction in over-refusal, near-complete preservation of general capabilities, theoretical convergence analysis, and empirical verification; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Dongrui Liu', 'Yi Yu', 'Luxin Xu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak defense', 'alignment', 'robustness', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09990</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behavioral calibration via reinforcement learning that trains LLMs to output calibrated probabilities of correctness and to abstain or flag uncertain claims, reducing hallucinations.&lt;/li&gt;&lt;li&gt;Optimizes strictly proper scoring rules rather than binary rewards to discourage guessing and encourage epistemic honesty.&lt;/li&gt;&lt;li&gt;Empirical results using Qwen3-4B-Instruct show improved uncertainty quantification (Accuracy-to-Hallucination Ratio and calibration error) across in-domain (BeyondAIME math) and cross-domain (SimpleQA) evaluations, outperforming or matching frontier models in calibration despite lower raw accuracy.&lt;/li&gt;&lt;li&gt;Claims the calibration skill is a transferable meta-skill decoupled from predictive accuracy, enabling smaller models to achieve strong uncertainty estimation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Tianle Cai', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'uncertainty calibration', 'reinforcement learning', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</title><link>https://arxiv.org/abs/2512.17367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-SGA (Large Language Model-based Sample Generation and Aggregation) framework that leverages invariances of textual adversarial attacks to improve generalizability of harmful-content detectors.&lt;/li&gt;&lt;li&gt;Instantiates ARHOCD with three design components: an ensemble of base detectors, a Bayesian-updated dynamic weight assignment initialized with domain knowledge, and an iterative adversarial training strategy that jointly optimizes detectors and the weight assignor.&lt;/li&gt;&lt;li&gt;Empirically evaluates on three datasets (hate speech, rumor, extremist content) and reports improved detection accuracy and adversarial robustness/generalizability under adversarial text modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yidong Chai', 'Yi Liu', 'Mohammadreza Ebrahimi', 'Weifeng Li', 'Balaji Padmanabhan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'textual adversarial attacks', 'LLM-based data augmentation', 'content moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17367</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Trade-offs: A Unified Framework for Privacy, Robustness, and Communication Efficiency in Federated Learning</title><link>https://arxiv.org/abs/2508.12978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fed-DPRoC, a federated learning framework that jointly provides differential privacy, Byzantine robustness, and communication efficiency.&lt;/li&gt;&lt;li&gt;Instantiates framework as RobAJoL using Johnson-Lindenstrauss (JL) compression combined with robust averaging and proves JL compatibility with robustness and DP guarantees.&lt;/li&gt;&lt;li&gt;Presents empirical results on CIFAR-10, Fashion-MNIST, and FEMNIST showing improved robustness and utility versus a state-of-the-art communication-efficient, robust FL baseline augmented with DP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Xia', 'Tayyebeh Jahani-Nezhad', 'Rawad Bitar']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning', 'Differential Privacy', 'Byzantine Robustness', 'Communication-efficient Compression', 'Johnson-Lindenstrauss']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12978</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Differentially Private Vertical Federated Learning with Zeroth-Order Optimization</title><link>https://arxiv.org/abs/2502.20565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DPZV, a vertical federated learning (VFL) framework using zeroth-order (ZO) optimization that injects scalar-valued differential privacy (DP) noise on the downlink to reduce communication and variance amplification.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees comparable to first-order DP-SGD and proves (ε, δ)-DP for the protocol.&lt;/li&gt;&lt;li&gt;Claims improved privacy-utility tradeoffs and fewer communication rounds than existing DP-VFL baselines, with experiments under strict privacy budgets (ε ≤ 10).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianing Zhang', 'Evan Chen', 'Dong-Jun Han', 'Chaoyue Liu', 'Christopher G. Brinton']&lt;/li&gt;&lt;li&gt;Tags: ['vertical federated learning', 'differential privacy', 'inference attacks', 'communication efficiency', 'zeroth-order optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20565</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge</title><link>https://arxiv.org/abs/2407.15192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Error Detection Rules (EDR) to learn explainable rules that detect classifier failure modes without needing pre-specified constraints.&lt;/li&gt;&lt;li&gt;Shows EDRs can be used to recover constraints for hierarchical multi-label classification, enabling neurosymbolic models to enforce consistency post hoc.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness and noise tolerance across multiple datasets, including a new military vehicle recognition dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Shay Kricheli', 'Khoa Vo', 'Aniruddha Datta', 'Spencer Ozgur', 'Paulo Shakarian']&lt;/li&gt;&lt;li&gt;Tags: ['error-detection', 'robustness', 'explainability', 'neurosymbolic', 'hierarchical-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.15192</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</title><link>https://arxiv.org/abs/2512.21999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALEAHallu: an Activate-Locate-Edit Adversarially framework to mitigate hallucinations in VLMs by parametric editing.&lt;/li&gt;&lt;li&gt;Constructs an activation dataset of grounded (visually-anchored) vs hallucinatory responses and analyzes differential hidden states to locate hallucination-prone parameter clusters.&lt;/li&gt;&lt;li&gt;Fine-tunes identified parameter clusters with prompts injected with adversarially tuned prefixes designed to maximize visual neglect, forcing the model to prioritize visual evidence.&lt;/li&gt;&lt;li&gt;Evaluated on generative and discriminative VLM tasks and reported significant reductions in hallucination; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Hu', 'Beibei Li', 'Jiangwei Xia', 'Yanjun Qin', 'Bing Ji', 'Zhongshi He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'parametric model editing', 'adversarial fine-tuning', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21999</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2512.21815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that ~20% of high-entropy tokens (critical decision points) disproportionately govern VLM generation trajectories.&lt;/li&gt;&lt;li&gt;Shows that concentrating adversarial perturbations on those tokens yields semantic degradation comparable to global methods but with much smaller budgets.&lt;/li&gt;&lt;li&gt;Reports conversion of 35–49% of benign outputs into harmful ones across multiple VLMs and transferability of attacks (17–26% harmful rates on unseen targets).&lt;/li&gt;&lt;li&gt;Proposes Entropy-bank Guided Adversarial attacks (EGA) achieving high attack success (93–95%) and high harmful conversion, revealing safety weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengqi He', 'Xinyu Tian', 'Xin Shen', 'Jinhong Ni', 'Shu Zou', 'Zhaoyuan Yang', 'Jing Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'vision-language-models', 'safety-vulnerability', 'entropy-guided-attacks', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21815</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Assessing the Effectiveness of Membership Inference on Generative Music</title><link>https://arxiv.org/abs/2512.21762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates the effectiveness of membership inference attacks (MIAs) on MuseGAN, a generative music model.&lt;/li&gt;&lt;li&gt;Applies several existing MIA techniques (adapted from prior generative audio/image work) to music data and measures attack success.&lt;/li&gt;&lt;li&gt;Finds that music data and MuseGAN are fairly resilient to known MIAs, consistent with prior generative-audio findings; presents a preliminary study and results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kurtis Chow', 'Omar Samiullah', 'Vinesh Sridhar', 'Hewen Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'generative-music', 'audio', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21762</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</title><link>https://arxiv.org/abs/2512.21670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a mechanistic interpretability framework for deepfake detection combining sparse autoencoder (SAE) analysis with a novel forensic manifold analysis.&lt;/li&gt;&lt;li&gt;Shows that only a small fraction of latent features are active per layer and that manifold geometry (intrinsic dimensionality, curvature, feature selectivity) systematically varies with different deepfake artifacts.&lt;/li&gt;&lt;li&gt;Maps specific learned features to forensic artifacts, providing interpretable links between internal representations and types of synthetic manipulations.&lt;/li&gt;&lt;li&gt;Positions these insights as a step toward more interpretable and potentially more robust deepfake detectors useful for forensic analysis and model debugging.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subramanyam Sahoo', 'Jared Junkin']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'interpretability', 'forensic-analysis', 'robustness', 'mechanistic-interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21670</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Definition of Hallucination, Or: It's the World Model, Stupid</title><link>https://arxiv.org/abs/2512.21577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to users (i.e., contradictions with a chosen reference world).&lt;/li&gt;&lt;li&gt;Argues that making the reference world explicit clarifies distinctions between hallucination and other model errors (e.g., planning or incentive failures).&lt;/li&gt;&lt;li&gt;Proposes a family of synthetic, fully specified benchmarks that define hallucinations as mismatches with ground-truth world models to stress-test and improve LLM world modeling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emmy Liu', 'Varun Gangal', 'Chelsea Zou', 'Xiaoqi Huang', 'Michael Yu', 'Alex Chang', 'Zhuofu Tao', 'Sachin Kumar', 'Steven Y. Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM safety', 'evaluation/benchmarking', 'world modeling', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21577</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Adversarial Training via Data Selection</title><link>https://arxiv.org/abs/2512.22069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective Adversarial Training: only a subset of samples in each minibatch are perturbed, while others are trained cleanly with a mixed objective.&lt;/li&gt;&lt;li&gt;Introduces two selection criteria — margin-based sampling (near decision boundary) and gradient-matching sampling (gradients aligned with batch optimization direction).&lt;/li&gt;&lt;li&gt;Generates adversarial examples only for the selected subset, reducing adversarial computation by up to ~50% while achieving comparable or better robustness than full PGD training.&lt;/li&gt;&lt;li&gt;Empirical evaluation on MNIST and CIFAR-10 demonstrates effectiveness of informed sample selection for scalable adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youran Ye', 'Dejin Wang', 'Ajinkya Bhandare']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'PGD', 'data selection', 'scalability/efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22069</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</title><link>https://arxiv.org/abs/2512.21917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of language models to preference data when the link function between preferences and latent rewards is unknown, avoiding bias from mis-specified links.&lt;/li&gt;&lt;li&gt;Shows that realizable solutions imply a semiparametric single-index binary choice model and focuses on direct policy learning rather than identifying structural parameters.&lt;/li&gt;&lt;li&gt;Proposes methods (profiling the link, orthogonalization, link-agnostic bipartite ranking), provides finite-sample policy error bounds depending on index-class complexity, and gives practical first-order implementations for neural networks and batched data.&lt;/li&gt;&lt;li&gt;Methods are robust to unknown preference noise distribution and scale and preserve direct policy optimization without explicitly fitting rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Kallus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning / RLHF', 'robust policy optimization', 'statistical robustness / semiparametric methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21917</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions</title><link>https://arxiv.org/abs/2512.21521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Fed-α-NormEC, a differentially private federated learning framework that provides provable convergence and DP guarantees under standard (non-restrictive) assumptions.&lt;/li&gt;&lt;li&gt;Supports practical FL features: multiple local updates (full and incremental gradient steps), separate server and client stepsizes, and partial client participation (important for privacy amplification).&lt;/li&gt;&lt;li&gt;Theoretical analysis demonstrating convergence and differential privacy guarantees, with empirical validation on private deep learning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Egor Shulgin', 'Grigory Malinovsky', 'Sarit Khirirat', "Peter Richt\\'arik"]&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'federated learning', 'privacy-preserving ML', 'convergence guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21521</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behavioral calibration via reinforcement learning that trains LLMs to output calibrated probabilities of correctness and to abstain or flag uncertain claims, reducing hallucinations.&lt;/li&gt;&lt;li&gt;Optimizes strictly proper scoring rules rather than binary rewards to discourage guessing and encourage epistemic honesty.&lt;/li&gt;&lt;li&gt;Empirical results using Qwen3-4B-Instruct show improved uncertainty quantification (Accuracy-to-Hallucination Ratio and calibration error) across in-domain (BeyondAIME math) and cross-domain (SimpleQA) evaluations, outperforming or matching frontier models in calibration despite lower raw accuracy.&lt;/li&gt;&lt;li&gt;Claims the calibration skill is a transferable meta-skill decoupled from predictive accuracy, enabling smaller models to achieve strong uncertainty estimation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Tianle Cai', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'uncertainty calibration', 'reinforcement learning', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Preventing AI Deepfake Abuse: An Islamic Ethics Framework</title><link>https://arxiv.org/abs/2512.17218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic literature review (PRISMA) of 10 primary sources (2018–2025) to identify ethical and regulatory gaps related to deepfake abuse.&lt;/li&gt;&lt;li&gt;Proposes an Islamic ethical framework grounded in Maqasid al‑Shariah (notably hifz al‑ird and hifz al‑nafs) to guide prevention-focused governance.&lt;/li&gt;&lt;li&gt;Recommends three strategies: regulatory reform recognizing intangible/psychological harms; technology governance based on adl, amanah, transparency; and enhanced public digital literacy (tabayyun).&lt;/li&gt;&lt;li&gt;Emphasizes shifting from punitive mechanisms to preventive, dignity‑preserving approaches in digital governance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (systematic literature review / normative framework)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wisnu Uriawan', 'Imany Fauzy Rahman', 'Muhamad Zidan', 'Irma Rohmatillah', 'Muhammad Arkan Raihan', 'Irma Dwiyanti']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake', 'ethics', 'policy/regulation', 'governance', 'digital literacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17218</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title><link>https://arxiv.org/abs/2512.12324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents UniMark, an open-source unified framework for identifying AI-generated content across text, image, audio, and video.&lt;/li&gt;&lt;li&gt;Implements a dual-operation strategy supporting Hidden Watermarking (copyright protection) and Visible Marking (regulatory compliance).&lt;/li&gt;&lt;li&gt;Provides a standardized evaluation framework with three modality-specific benchmarks (Image/Video/Audio) for rigorous performance assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meilin Li', 'Ji He', 'Yi Yu', 'Jia Xu', 'Shanzhe Lei', 'Yan Teng', 'Yingchun Wang', 'Xuhong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'AIGC detection', 'multimodal', 'benchmarking', 'compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12324</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title><link>https://arxiv.org/abs/2512.07015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FVA-RAG: flip RAG pipeline to treat initial LLM output as a hypothesis and retrieve targeted counter-evidence (anti-context) to falsify it.&lt;/li&gt;&lt;li&gt;Evaluates on TruthfulQA-Generation (N=817) with frozen corpora and identical retrieval budgets, using gpt-4o for generation and deterministic judging.&lt;/li&gt;&lt;li&gt;Reports significant improvements in accuracy (≈79.8–80.05%) over Self-RAG and CRAG variants, and shows falsification triggered on ~24.5–29.3% of queries, indicating counter-evidence retrieval reduces sycophantic hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Ravishankara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'retrieval-augmented generation', 'adversarial/targeted retrieval', 'truthfulness/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07015</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Focus Memory for Language Models</title><link>https://arxiv.org/abs/2511.12712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Focus Memory (AFM), a lightweight context manager that assigns past messages one of three fidelity levels (Full, Compressed, Placeholder) and packs them under a fixed token budget based on relevance, temporal decay, and importance.&lt;/li&gt;&lt;li&gt;Evaluates AFM on multi-turn dialogue benchmarks stressing long-horizon constraint preservation (a safety-critical peanut allergy travel scenario and a policy-critical tax-evasion scenario), showing substantial gains in retaining constraints and correct refusal behavior.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety/alignment in deployed chat settings without changing model weights or adding external retrieval; provides an open-source implementation compatible with OpenAI-style chat APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'context management', 'constraint preservation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12712</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</title><link>https://arxiv.org/abs/2511.09109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bi-RAR, a retrieval-augmented reasoning framework that evaluates each intermediate reasoning step both forward (how far from the answer) and backward (how well it addresses the question).&lt;/li&gt;&lt;li&gt;Introduces a bidirectional information distance based on Kolmogorov complexity, approximated via language-model generation probabilities, to quantify information completeness of steps.&lt;/li&gt;&lt;li&gt;Optimizes reasoning with a multi-objective reinforcement learning approach using a cascading reward structure to prioritize early trajectory alignment and reduce reward hacking.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over prior methods on seven question-answering benchmarks and enables efficient search-engine interaction during training and inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenda Wei', 'Yu-An Liu', 'Ruqing Zhang', 'Jiafeng Guo', 'Lixin Su', 'Shuaiqiang Wang', 'Dawei Yin', 'Maarten de Rijke', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['retrieval-augmented-generation', 'hallucination-mitigation', 'multi-objective-reinforcement-learning', 'reward-design', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09109</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes</title><link>https://arxiv.org/abs/2503.09626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UESBD, a framework that augments social bot detectors with predictive uncertainty to reduce overconfident OOD predictions.&lt;/li&gt;&lt;li&gt;Introduces Robust Multi-modal Neural Processes (RMNP): modality-specific encoders, unimodal attentive neural processes encoding Gaussian latent variables, and an evidential gating network to model modality reliability.&lt;/li&gt;&lt;li&gt;Fuses unimodal latents via a generalized product-of-experts that weights modalities by reliability, and obtains predictions by Monte Carlo sampling from the joint latent distribution.&lt;/li&gt;&lt;li&gt;Evaluated on three real-world benchmarks, showing improved classification, calibrated uncertainty estimation, and robustness to modality conflicts caused by bot camouflage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wu', 'Yingguang Yang', 'hao liu', 'Hao Peng', 'Buyun He', 'Yutong Xia', 'Yong Liao']&lt;/li&gt;&lt;li&gt;Tags: ['social bot detection', 'robustness', 'uncertainty estimation', 'multi-modal', 'modality reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09626</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Differentially Private Vertical Federated Learning with Zeroth-Order Optimization</title><link>https://arxiv.org/abs/2502.20565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DPZV, a vertical federated learning (VFL) framework using zeroth-order (ZO) optimization that injects scalar-valued differential privacy (DP) noise on the downlink to reduce communication and variance amplification.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees comparable to first-order DP-SGD and proves (ε, δ)-DP for the protocol.&lt;/li&gt;&lt;li&gt;Claims improved privacy-utility tradeoffs and fewer communication rounds than existing DP-VFL baselines, with experiments under strict privacy budgets (ε ≤ 10).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianing Zhang', 'Evan Chen', 'Dong-Jun Han', 'Chaoyue Liu', 'Christopher G. Brinton']&lt;/li&gt;&lt;li&gt;Tags: ['vertical federated learning', 'differential privacy', 'inference attacks', 'communication efficiency', 'zeroth-order optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20565</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability</title><link>https://arxiv.org/abs/2502.09990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that prior defenses against multi-turn jailbreaks trade off robustness for usability by blurring the boundary between safe and harmful feature representations.&lt;/li&gt;&lt;li&gt;Proposes X-Boundary, a method to push harmful representations away from boundary-safe representations to create an exact distinction and precisely erase harmful behavior without disrupting safe capabilities.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art defense performance against multi-turn jailbreaks, ~20% reduction in over-refusal, near-complete preservation of general capabilities, theoretical convergence analysis, and empirical verification; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoya Lu', 'Dongrui Liu', 'Yi Yu', 'Luxin Xu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak defense', 'alignment', 'robustness', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09990</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge</title><link>https://arxiv.org/abs/2407.15192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Error Detection Rules (EDR) to learn explainable rules that detect classifier failure modes without needing pre-specified constraints.&lt;/li&gt;&lt;li&gt;Shows EDRs can be used to recover constraints for hierarchical multi-label classification, enabling neurosymbolic models to enforce consistency post hoc.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness and noise tolerance across multiple datasets, including a new military vehicle recognition dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Shay Kricheli', 'Khoa Vo', 'Aniruddha Datta', 'Spencer Ozgur', 'Paulo Shakarian']&lt;/li&gt;&lt;li&gt;Tags: ['error-detection', 'robustness', 'explainability', 'neurosymbolic', 'hierarchical-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.15192</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</title><link>https://arxiv.org/abs/2512.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoboSafe, a hybrid executable predicate-based safety logic for embodied agents that integrates short-term backward reflective reasoning and long-term forward predictive reasoning over a Hybrid Long-Short Safety Memory.&lt;/li&gt;&lt;li&gt;Backward Reflective Reasoning revisits recent trajectories to infer temporal safety predicates and trigger replanning when violations are detected; Forward Predictive Reasoning anticipates upcoming risks from long-term memory and multimodal observations.&lt;/li&gt;&lt;li&gt;Demonstrates substantial reduction in hazardous actions (~36.8% fewer risk occurrences) across multiple agents while retaining task performance, with real-world robotic arm evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Wang', 'Zonghao Ying', 'Xiao Yang', 'Quanchen Zou', 'Zhenfei Yin', 'Tianlin Li', 'Jian Yang', 'Yaodong Yang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['embodied agents', 'runtime safety', 'safety guardrails', 'vision-language models', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21220</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRM, a multi-agent reward modeling framework that replaces a single black‑box reward model with domain-specific evaluator agents (e.g., factuality, helpfulness, safety) and global evaluators, aggregated into a single timestep reward.&lt;/li&gt;&lt;li&gt;Aggregator fuses partial signals, accounts for step-wise correctness, multi-agent agreement, and repetition penalties; policy optimized with advantage-based RL and a value model regressing to aggregated reward.&lt;/li&gt;&lt;li&gt;Claims improved robustness and interpretability in RLHF without additional human annotation beyond that used to train the evaluators.&lt;/li&gt;&lt;li&gt;Introduces rewardBench, a benchmark and training suite aligned to the collaborative evaluator architecture for training and assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei Yang', 'Ke Zhang', 'Ji Wang', 'Xiao Chen', 'Yuxin Tang', 'Eric Yang', 'Lynn Ai', 'Bill Shi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'Reward modeling', 'Alignment/Safety', 'Evaluation/Benchmarking', 'Interpretability/Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16202</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LVLM-Aided Alignment of Task-Specific Vision Models</title><link>https://arxiv.org/abs/2512.21985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LVLM-Aided Visual Alignment (LVLM-VA): uses a Large Vision Language Model to translate model behavior into natural language and convert class-level human specifications into image-level critiques.&lt;/li&gt;&lt;li&gt;Provides a bidirectional interface enabling domain experts to interact with and correct small task-specific vision models without fine-grained annotation.&lt;/li&gt;&lt;li&gt;Empirically shows reduced reliance on spurious features and mitigation of group-specific biases on synthetic and real-world datasets, improving model alignment with human domain knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Koebler', 'Lukas Kuhn', 'Ingo Thon', 'Florian Buettner']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LVLM', 'bias-mitigation', 'spurious-correlation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21985</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</title><link>https://arxiv.org/abs/2512.21917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of language models to preference data when the link function between preferences and latent rewards is unknown, avoiding bias from mis-specified links.&lt;/li&gt;&lt;li&gt;Shows that realizable solutions imply a semiparametric single-index binary choice model and focuses on direct policy learning rather than identifying structural parameters.&lt;/li&gt;&lt;li&gt;Proposes methods (profiling the link, orthogonalization, link-agnostic bipartite ranking), provides finite-sample policy error bounds depending on index-class complexity, and gives practical first-order implementations for neural networks and batched data.&lt;/li&gt;&lt;li&gt;Methods are robust to unknown preference noise distribution and scale and preserve direct policy optimization without explicitly fitting rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Kallus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning / RLHF', 'robust policy optimization', 'statistical robustness / semiparametric methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21917</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?</title><link>https://arxiv.org/abs/2512.21871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a large-scale benchmark of 50,000 multimodal query–content pairs to evaluate whether LVLMs recognize and comply with copyrighted content, including scenarios with and without copyright notices and four types of notices.&lt;/li&gt;&lt;li&gt;Evaluates multiple state-of-the-art (including closed-source) LVLMs and finds significant failures to recognize or respect copyrighted materials, even when explicit notices are present.&lt;/li&gt;&lt;li&gt;Proposes a novel tool-augmented defense framework to reduce copyright infringement risk and reports improvements in compliance across evaluated scenarios.&lt;/li&gt;&lt;li&gt;Highlights the need for copyright-aware LVLM design and provides a systematic dataset and evaluation protocol for measuring compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naen Xu', 'Jinghuai Zhang', 'Changjiang Li', 'Hengyu An', 'Chunyi Zhou', 'Jun Wang', 'Boyu Xu', 'Yuyuan Li', 'Tianyu Du', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['copyright compliance', 'LVLM safety', 'benchmarking', 'defense/tooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21871</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs</title><link>https://arxiv.org/abs/2512.21849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HeartBench, a Chinese-language benchmark for evaluating anthropomorphic intelligence across five primary dimensions and 15 secondary capabilities grounded in counseling scenarios.&lt;/li&gt;&lt;li&gt;Uses a rubric-based, 'reasoning-before-scoring' protocol to translate socio-emotional and ethical traits into measurable criteria and constructs a 'Hard Set' of challenging cases.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art LLMs, finding a substantial performance ceiling (top models ≈60% of expert ideal) and large degradation on subtle emotional and ethical trade-off cases.&lt;/li&gt;&lt;li&gt;Positions the benchmark as a standard for human-aligned training data creation and for assessing socio-emotional and ethical behavior in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Liu', 'Peiyi Tu', 'Wenyu Chen', 'Yihong Zhuang', 'Xinxia Ling', 'Anji Zhou', 'Chenxi Wang', 'Zhuo Han', 'Zhengkai Yang', 'Junbo Zhao', 'Zenan Huang', 'Yuanyuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmark', 'ethics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21849</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought</title><link>https://arxiv.org/abs/2512.21711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes latent tokens (COCONUT) vs explicit Chain-of-Thought (CoT) to assess whether latent tokens encode faithful reasoning.&lt;/li&gt;&lt;li&gt;Steering/causal perturbation experiments show COCONUT tokens are largely insensitive to targeted interventions and lack reasoning-critical information unlike CoT.&lt;/li&gt;&lt;li&gt;Shortcut experiments on MMLU and HotpotQA reveal COCONUT exploits dataset artifacts and out-of-distribution biases, inflating benchmark performance and acting as pseudo-reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyi Zhang', 'Boyu Tang', 'Tianjie Ju', 'Sufeng Duan', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Chain-of-Thought', 'Adversarial analysis', 'Model interpretability', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21711</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers</title><link>https://arxiv.org/abs/2512.21709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates detection of AI-generated paraphrases in Bengali using five transformer models (XLM-RoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base, MultilingualBERT-Base).&lt;/li&gt;&lt;li&gt;Finds zero-shot performance near chance (~50% accuracy), while fine-tuning yields large improvements (≈91% accuracy and F1 for XLM-RoBERTa, mDeBERTa, and MultilingualBERT).&lt;/li&gt;&lt;li&gt;Reports weaker fine-tuning effectiveness for IndicBERT and provides baseline results for Bengali AI-generated text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Rakibul Islam', 'Most. Sharmin Sultana Samu', 'Md. Zahid Hossain', 'Farhad Uz Zaman', 'Md. Kamrozzaman Bhuiyan']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'LLM detection', 'Bengali NLP', 'fine-tuning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21709</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning</title><link>https://arxiv.org/abs/2512.21702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Bengali deepfake audio detection on the BanglaFake dataset using zero-shot inference with pretrained audio/text models (Wav2Vec2-XLSR-53, Whisper, PANNsCNN14, WavLM, AST) and finds limited zero-shot performance (best: 53.80% accuracy, 56.60% AUC).&lt;/li&gt;&lt;li&gt;Fine-tunes multiple architectures (Wav2Vec2-Base, LCNN, LCNN-Attention, ResNet18, ViT-B16, CNN-BiLSTM) and reports substantial gains; ResNet18 achieves top results (79.17% accuracy, 84.37% AUC, 24.35% EER).&lt;/li&gt;&lt;li&gt;Provides the first systematic benchmark for Bengali audio deepfake detection and demonstrates transfer learning/fine-tuning is effective for this low-resource language.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Most. Sharmin Sultana Samu', 'Md. Rakibul Islam', 'Md. Zahid Hossain', 'Md. Kamrozzaman Bhuiyan', 'Farhad Uz Zaman']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-audio', 'audio-forensics', 'transfer-learning', 'benchmark', 'low-resource-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21702</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Definition of Hallucination, Or: It's the World Model, Stupid</title><link>https://arxiv.org/abs/2512.21577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to users (i.e., contradictions with a chosen reference world).&lt;/li&gt;&lt;li&gt;Argues that making the reference world explicit clarifies distinctions between hallucination and other model errors (e.g., planning or incentive failures).&lt;/li&gt;&lt;li&gt;Proposes a family of synthetic, fully specified benchmarks that define hallucinations as mismatches with ground-truth world models to stress-test and improve LLM world modeling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emmy Liu', 'Varun Gangal', 'Chelsea Zou', 'Xiaoqi Huang', 'Michael Yu', 'Alex Chang', 'Zhuofu Tao', 'Sachin Kumar', 'Steven Y. Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM safety', 'evaluation/benchmarking', 'world modeling', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21577</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploration of Reproducible Generated Image Detection</title><link>https://arxiv.org/abs/2512.21562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews 7 key papers on AI-generated image (AIGC) detection and constructs a lightweight test dataset.&lt;/li&gt;&lt;li&gt;Reproduces a representative detection method and shows basic performance can be matched when core procedures are strictly followed.&lt;/li&gt;&lt;li&gt;Identifies reproducibility issues caused by omitted preprocessing/parameter details and overfitting to generator-specific artifacts, leading to poor cross-generator generalization.&lt;/li&gt;&lt;li&gt;Demonstrates that detection performance degrades sharply when preprocessing disrupts key features or when evaluating across different generators, and calls for more comprehensive experimental disclosure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'Reproducibility', 'Robustness', 'Benchmarking', 'Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21562</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures</title><link>https://arxiv.org/abs/2512.21551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a workshop on bidirectional (reciprocal) human-AI alignment where humans and AI co-adapt through interaction, evaluation, and design.&lt;/li&gt;&lt;li&gt;Emphasizes embedding human and societal values into alignment research and enabling humans to critically engage and evolve with AI systems.&lt;/li&gt;&lt;li&gt;Targets interdisciplinary methods for interactive alignment, frameworks for societal impact evaluation, and strategies for alignment in dynamic contexts.&lt;/li&gt;&lt;li&gt;Aims to bridge HCI, AI, social sciences, and other domains to establish a shared agenda for responsible, reciprocal human-AI futures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Shen (Cassandra)', 'Tiffany Knearem (Cassandra)', 'Divy Thakkar (Cassandra)', 'Pat Pataranutaporn (Cassandra)', 'Anoop Sinha (Cassandra)', 'Yike (Cassandra)', 'Shi', 'Jenny T. Liang', 'Lama Ahmad', 'Tanu Mitra', 'Brad A. Myers', 'Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-AI interaction', 'value-centered design', 'evaluation', 'societal impact']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21551</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Teaching People LLM's Errors and Getting it Right</title><link>https://arxiv.org/abs/2512.21422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes why prior attempts to teach users LLM failure patterns did not reduce overreliance, asking whether failure patterns exist, whether they can be surfaced automatically, and whether current evaluation metrics are appropriate.&lt;/li&gt;&lt;li&gt;Shows that sizable meta-labeled groups where LLMs are error-prone do exist (i.e., teachable failure patterns), but embedding- and prompting-based discovery methods yield mixed success in surfacing them.&lt;/li&gt;&lt;li&gt;Proposes a new metric that measures a user's ability to use provided failure patterns to anticipate LLM errors and reports a user study where teaching improves performance on this metric (while human-AI team accuracy did not improve).&lt;/li&gt;&lt;li&gt;Concludes that teaching failure patterns can mitigate overreliance but requires better automated failure-discovery methods and evaluation aligned to users' anticipatory abilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Stringham', 'Fateme Hashemi Chaleshtori', 'Xinyuan Yan', 'Zhichao Xu', 'Bei Wang', "Ana Marasovi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'human-AI interaction', 'failure-pattern discovery', 'overreliance/calibration', 'evaluation/metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21422</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors</title><link>https://arxiv.org/abs/2512.21404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LAMLAD: a dual-agent LLM framework (LLM manipulator + LLM analyzer) with retrieval-augmented generation to produce realistic, functionality-preserving feature-level perturbations for Drebin-style Android malware classifiers.&lt;/li&gt;&lt;li&gt;Demonstrates high attack effectiveness: up to 97% attack success rate across three representative ML-based Android detectors and an average of three attempts per adversarial sample, outperforming two state-of-the-art attack methods.&lt;/li&gt;&lt;li&gt;Proposes an adversarial training defense that reduces the attack success rate by over 30% on average, improving model robustness against LAMLAD-style attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianwei Lan', 'Farid Na\\"it-Abdesselam']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM-assisted attacks', 'Android malware detection', 'adversarial training', 'feature-level evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21404</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reflection-Driven Control for Trustworthy Code Agents</title><link>https://arxiv.org/abs/2512.21354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reflection-Driven Control: a pluggable module that runs an internal reflection loop during generation to monitor and evaluate an agent’s decision path and detect security risks.&lt;/li&gt;&lt;li&gt;When risks are detected, the system retrieves repair examples and secure coding guidelines from a reflective memory and injects evidence-based constraints into subsequent reasoning.&lt;/li&gt;&lt;li&gt;Instantiated for secure code generation and evaluated across eight classes of security-critical programming tasks, showing improved security and policy compliance with minimal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'Jiazheng Quan', 'Xingrui Yu', 'Hansen Hu', 'Yuhao', 'Ivor Tsang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'secure code generation', 'self-reflection', 'runtime monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21354</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks</title><link>https://arxiv.org/abs/2512.21345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Query Carefully, a pipeline that combines LLM-based text-to-SQL generation with explicit detection and handling of unanswerable or out-of-scope natural language queries to avoid misleading executable SQL outputs.&lt;/li&gt;&lt;li&gt;Creates OncoMX-NAQ, an 80-question no-answer benchmark across 8 categories (non-SQL, out-of-schema/domain, and various ambiguity types) derived from a biomedical Text-to-SQL dataset.&lt;/li&gt;&lt;li&gt;Uses Llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples (both answerable and unanswerable); evaluates SQL exact match, result accuracy, and unanswerable-detection accuracy.&lt;/li&gt;&lt;li&gt;Findings: balanced prompting yields 0.8 unanswerable-detection accuracy with near-perfect detection for structural categories but poor performance on missing-value (0.5) and column-ambiguity (0.3) queries; includes a lightweight UI exposing SQL, execution results, and abstentions for transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jasmin Saxer (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Isabella Maria Aigner (Institute of Medical Virology', 'University of Zurich', 'Zurich', 'Switzerland)', 'Luise Linzmeier (Department of Gastroenterology and Hepatology', 'University Hospital Zurich', 'University of Zurich', 'Zurich', 'Switzerland)', 'Andreas Weiler (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)', 'Kurt Stockinger (Institute of Computer Science', 'Zurich University of Applied Sciences', 'Winterthur', 'Switzerland)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'unanswerable detection', 'text-to-SQL robustness', 'prompt engineering', 'biomedical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21345</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets</title><link>https://arxiv.org/abs/2512.21775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Compliance Rating Scheme (CRS), a framework to evaluate dataset compliance with transparency, accountability, and security principles for generative AI datasets.&lt;/li&gt;&lt;li&gt;Provides an open-source Python library using data provenance technology to integrate CRS checks into dataset processing and training pipelines.&lt;/li&gt;&lt;li&gt;Supports both reactive evaluation of existing datasets and proactive guidance for responsible scraping and construction of new datasets, preserving provenance metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (framework + open-source tool)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matyas Bohacek', 'Ignacio Vilanova Echavarri']&lt;/li&gt;&lt;li&gt;Tags: ['data-provenance', 'dataset-compliance', 'dataset-security', 'transparency-accountability', 'governance-tooling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21775</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning</title><link>https://arxiv.org/abs/2512.21699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agent architecture that uses a consortium of heterogeneous LLMs/VLMs to generate independent candidate outputs, exposing uncertainty and disagreement.&lt;/li&gt;&lt;li&gt;Introduces a centralized reasoning agent that consolidates outputs, enforces safety/policy constraints, mitigates hallucinations and bias, and produces auditable, evidence-backed decisions.&lt;/li&gt;&lt;li&gt;Claims explainability via preserved intermediate outputs and cross-model comparisons; responsibility via reasoning-layer governance and agent-level constraints.&lt;/li&gt;&lt;li&gt;Evaluates the architecture on multiple real-world agentic workflows and reports improved robustness, transparency, and operational trust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eranga Bandara', 'Tharaka Hewa', 'Ross Gore', 'Sachin Shetty', 'Ravi Mukkamala', 'Peter Foytik', 'Abdul Rahman', 'Safdar H. Bouk', 'Xueping Liang', 'Amin Hass', 'Sachini Rajapakse', 'Ng Wee Keong', 'Kasun De Zoysa', 'Aruna Withanage', 'Nilaan Loganathan']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI', 'Explainability/XAI', 'Safety/Robustness', 'Consensus-driven reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21699</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning</title><link>https://arxiv.org/abs/2512.21583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal diagnostic framework (built on LLaVA) combining vision-language alignment with a logic-tree reasoning component to produce stepwise premises and verifiable conclusions.&lt;/li&gt;&lt;li&gt;Includes modules: input encoder for images/text, cross-modal projection, reasoning controller for task decomposition, and logic tree generator to regularize chains of thought.&lt;/li&gt;&lt;li&gt;Evaluated on MedXpertQA and other benchmarks, showing improved diagnostic accuracy and more interpretable reasoning traces while remaining competitive on text-only tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zelin Zang', 'Wenyi Gu', 'Siqi Ma', 'Dan Yang', 'Yue Shen', 'Zhu Zhang', 'Guohui Fan', 'Wing-Kuen Ling', 'Fuji Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'multimodal reasoning', 'medical AI', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21583</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis</title><link>https://arxiv.org/abs/2512.21482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LogicLens, a unified visual-textual co-reasoning framework for detecting, grounding, and explaining text-centric forgeries using a Cross-Cues-aware Chain of Thought (CCT).&lt;/li&gt;&lt;li&gt;Introduces a weighted multi-task reward and GRPO-based optimization to align detection, grounding, and explanation objectives.&lt;/li&gt;&lt;li&gt;Presents PR^2 (Perceiver, Reasoner, Reviewer) pipeline for annotation generation and releases RealText, a 5,397-image dataset with fine-grained annotations (segmentation, explanations, authenticity labels).&lt;/li&gt;&lt;li&gt;Reports strong empirical gains over specialized frameworks and MLLM baselines in zero-shot and dense-text benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanwei Zeng', 'Changtao Miao', 'Jing Huang', 'Zhiya Tan', 'Shutao Gong', 'Xiaoming Yu', 'Yang Wang', 'Huazhe Tan', 'Weibin Yao', 'Jianshu Li']&lt;/li&gt;&lt;li&gt;Tags: ['forgery detection', 'multimodal reasoning', 'explainability', 'dataset', 'AIGC detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21482</guid><pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
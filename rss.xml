<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 13 Nov 2025 00:07:31 +0000</lastBuildDate><item><title>Selective Diabetic Retinopathy Screening with Accuracy-Weighted Deep Ensembles and Entropy-Guided Abstention</title><link>https://arxiv.org/abs/2511.05529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an accuracy-weighted deep ensemble of seven CNN architectures for diabetic retinopathy detection on fundus images.&lt;/li&gt;&lt;li&gt;Introduces a probability-weighted entropy uncertainty metric to identify low-confidence predictions and abstain/flag them for review.&lt;/li&gt;&lt;li&gt;Reports high unfiltered accuracy (93.70%) and shows uncertainty-based filtering can raise maximum accuracy to 99.44%, enabling a tunable accuracy–coverage trade-off for safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jophy Lin']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'abstention/selective prediction', 'ensemble learning', 'medical AI safety', 'confidence calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05529</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization</title><link>https://arxiv.org/abs/2511.07210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generative Clean-Image Backdoors (GCB), which uses a conditional InfoGAN to optimize naturally occurring image features as stealthy and potent backdoor triggers.&lt;/li&gt;&lt;li&gt;Shows the optimized triggers enable successful clean-image (clean-label) backdoors with very low poison rates and minimal clean accuracy drop (&lt;1%).&lt;/li&gt;&lt;li&gt;Demonstrates generality across six datasets, five architectures, multiple tasks (classification, regression, segmentation) and robustness to most existing backdoor defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binyan Xu', 'Fan Yang', 'Di Tang', 'Xilin Dai', 'Kehuan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'clean-label backdoor', 'trigger optimization', 'poisoning', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07210</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Relative Energy Learning for LiDAR Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2511.06720</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Relative Energy Learning (REL): uses the energy gap between positive and negative logits as a relative OOD score to improve calibration and robustness for LiDAR point clouds.&lt;/li&gt;&lt;li&gt;Introduces Point Raise, a lightweight synthetic outlier generation method that perturbs point clouds to create auxiliary anomalies without changing inlier semantics.&lt;/li&gt;&lt;li&gt;Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, showing large improvements over prior LiDAR OOD methods and reductions in false positives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zizhao Li', 'Zhengkang Xiang', 'Jiayang Ao', 'Joseph West', 'Kourosh Khoshelham']&lt;/li&gt;&lt;li&gt;Tags: ['LiDAR OOD detection', 'robustness', 'autonomous driving safety', 'anomaly detection', 'synthetic outliers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06720</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robust Nearest Neighbour Retrieval Using Targeted Manifold Manipulation</title><link>https://arxiv.org/abs/2511.06261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TMM-NN: retrieval by measuring how easily a candidate can be nudged into a target region of feature space via a lightweight, query-specific trigger patch.&lt;/li&gt;&lt;li&gt;Implements this by weakly backdooring the network so inputs with the patch are steered toward a dummy class; candidates are ranked by the confidence with which the patched query causes them to be classified as that class.&lt;/li&gt;&lt;li&gt;Demonstrates that trigger-based, responsiveness-driven ranking outperforms traditional geometric distance metrics under noise and across benchmarks; includes robustness analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['B. Ghosh', 'H. Harikumar', 'S. Rana']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attacks', 'adversarial-patch', 'adversarial-ml', 'robustness-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06261</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation</title><link>https://arxiv.org/abs/2511.05923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Fine-grained Cross-modal Causal Tracing (FCCT) to quantify causal effects of visual and textual tokens, MHSA, FFNs, and hidden states across all decoder layers in LVLMs.&lt;/li&gt;&lt;li&gt;Finds that last-token MHSAs in middle layers aggregate cross-modal information and that FFNs follow a three-stage hierarchical progression for storing/transferring visual object representations.&lt;/li&gt;&lt;li&gt;Proposes Intermediate Representation Injection (IRI), a training-free, inference-time intervention that reinforces cross-modal object information to reduce hallucination while preserving speed and overall performance.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements across five benchmarks and multiple LVLMs, achieving state-of-the-art results on perception and hallucination mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Li', 'Zekai Ye', 'Xiaocheng Feng', 'Weihong Zhong', 'Weitao Ma', 'Xiachong Feng']&lt;/li&gt;&lt;li&gt;Tags: ['mechanistic interpretability', 'hallucination mitigation', 'cross-modal causal analysis', 'inference-time intervention', 'LVLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05923</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance</title><link>https://arxiv.org/abs/2509.21486</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reasoning-enhanced multimodal LLM pretraining paradigm for unified inappropriate content detection on short video platforms.&lt;/li&gt;&lt;li&gt;Introduces three targeted pretraining tasks—Caption, VQA, and Chain-of-Thought—to close the domain gap and enhance perception, guideline understanding, and reasoning.&lt;/li&gt;&lt;li&gt;Reports significant improvements in zero-shot and supervised fine-tuning performance and better generalization to emergent, previously unseen issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Wang', 'Yu Sun', 'Hongwei Wang', 'Baoyu Jing', 'Xiang Shen', 'Xin Dong', 'Zhuolin Hao', 'Hongyu Xiong', 'Yang Song']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'multimodal LLM', 'safety', 'pretraining', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21486</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Systematic Literature Review on Vehicular Collaborative Perception - A Computer Vision Perspective</title><link>https://arxiv.org/abs/2504.04631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic literature review (PRISMA 2020) of 106 peer-reviewed papers on vehicular collaborative perception (V2V/V2I), analyzed by modalities, collaboration schemes, and perception tasks.&lt;/li&gt;&lt;li&gt;Compares methods addressing practical challenges: pose errors, temporal latency, communication constraints, domain shift, heterogeneity, and adversarial attacks.&lt;/li&gt;&lt;li&gt;Critically examines evaluation methodologies and highlights misalignment between existing metrics and the fundamental objectives of collaborative perception.&lt;/li&gt;&lt;li&gt;Identifies challenges, opportunities, and risks for advancing vehicular collaborative perception, including robustness and safety concerns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei Wan', 'Jianxin Zhao', 'Andreas Wiedholz', 'Manuel Bied', 'Mateus Martinez de Lucena', 'Abhishek Dinkar Jagtap', 'Andreas Festag', 'Ant\\^onio Augusto Fr\\"ohlich', 'Hannan Ejaz Keen', 'Alexey Vinel']&lt;/li&gt;&lt;li&gt;Tags: ['vehicular collaborative perception', 'adversarial robustness', 'safety evaluation', 'sensor fusion', 'V2V/V2I communication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04631</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RedDiffuser: Red Teaming Vision-Language Models for Toxic Continuation via Reinforced Stable Diffusion</title><link>https://arxiv.org/abs/2503.06223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RedDiffuser, a red teaming framework that uses reinforcement learning to fine-tune diffusion models to generate natural-looking adversarial images that induce toxic continuations in vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Combines a greedy search for candidate image prompts with RL fine-tuning that jointly optimizes for toxicity induction and semantic coherence.&lt;/li&gt;&lt;li&gt;Empirical results show significant increases in toxicity rates (e.g., +10.69% / +8.91% on LLaVA original and hold-out sets) and transferability to other VLMs (e.g., Gemini +5.1%, LLaMA-Vision +26.83%).&lt;/li&gt;&lt;li&gt;Identifies a cross-modal toxicity amplification vulnerability in VLM alignment and plans to release the codebase for future multimodal red teaming research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Xiang Zheng', 'Xiaosen Wang', 'Cong Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'vision-language models', 'jailbreaking', 'adversarial images', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.06223</guid><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships</title><link>https://arxiv.org/abs/2405.18770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal Adversarial Training (MAT) that injects adversarial perturbations into both image and text modalities to defend vision-language models against multimodal attacks.&lt;/li&gt;&lt;li&gt;Identifies limitations of standard 1:1 image-text training pairs and studies leveraging one-to-many (1:N and N:1) image-text relationships via augmentation to improve robustness.&lt;/li&gt;&lt;li&gt;Finds effective augmented pairs must be well-aligned, diverse, and avoid distribution shift; shows MAT with suitable augmentation outperforms unimodal defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Futa Waseda', 'Antonio Tejero-de-Pablos', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multimodal attacks', 'vision-language models', 'adversarial training', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.18770</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</title><link>https://arxiv.org/abs/2511.07947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies weaknesses in existing black-box watermarks and introduces WRK, an attack that bypasses representation entanglement by exploiting sample-level decision boundaries, decreasing watermark success rates by ≥88.79% on benchmarks.&lt;/li&gt;&lt;li&gt;Proposes Class-Feature Watermarks (CFW), which embed class-level artifacts via a synthetic out-of-domain class to remove vulnerable decision boundaries and jointly optimize for transferability to extracted models and post-extraction stability.&lt;/li&gt;&lt;li&gt;Empirical results across multiple domains show CFW preserves model utility while maintaining at least 70.15% watermark success in extracted models even under combined MEA + WRK distortions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxin Xiao', 'Qingqing Ye', 'Zi Liang', 'Haoyang Li', 'RongHua Li', 'Huadi Zheng', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model watermarking', 'model extraction attacks', 'watermark removal', 'black-box security', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07947</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</title><link>https://arxiv.org/abs/2511.08423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniAID, a Mixture-of-Experts architecture that decouples content-specific semantic flaws (specialized experts per domain) from content-agnostic universal artifacts (fixed artifact expert).&lt;/li&gt;&lt;li&gt;Trains experts with a two-stage strategy: domain-specific hard sampling for specialization, then a lightweight gating network for input routing.&lt;/li&gt;&lt;li&gt;Introduces Mirage, a new large-scale contemporary dataset, and demonstrates improved generalization and detection performance over monolithic detectors on both traditional benchmarks and in-the-wild data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuncheng Guo', 'Junyan Ye', 'Chenjue Zhang', 'Hengrui Kang', 'Haohuan Fu', 'Conghui He', 'Weijia Li']&lt;/li&gt;&lt;li&gt;Tags: ['AIGI detection', 'Image forensics', 'Robustness/Generalization', 'Benchmark/Dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08423</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Twist and Compute: The Cost of Pose in 3D Generative Diffusion</title><link>https://arxiv.org/abs/2511.08203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a strong canonical-view bias in image-conditioned 3D generative diffusion models that harms generalization under simple 2D input rotations.&lt;/li&gt;&lt;li&gt;Demonstrates via controlled experiments that Hunyuan3D 2.0's performance degrades with rotated inputs.&lt;/li&gt;&lt;li&gt;Proposes a lightweight CNN orientation detector/corrector to restore performance without modifying the generative backbone.&lt;/li&gt;&lt;li&gt;Raises the design question of whether scaling alone suffices versus adopting modular, symmetry-aware architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyle Fogarty', 'Jack Foster', 'Boqiao Zhang', 'Jing Yang', 'Cengiz \\"Oztireli']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model bias', '3D generative models', 'viewpoint invariance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08203</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Introducing Nylon Face Mask Attacks: A Dataset for Evaluating Generalised Face Presentation Attack Detection</title><link>https://arxiv.org/abs/2511.08114</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new dataset of Nylon Face Mask (NFM) presentation attacks designed to simulate realistic 3D spoofing threats against face recognition systems.&lt;/li&gt;&lt;li&gt;Dataset collected with iPhone 11 Pro: 3,760 bona fide samples from 100 subjects and 51,281 NFM attack samples across four presentation scenarios (humans and mannequins).&lt;/li&gt;&lt;li&gt;Benchmarks five state-of-the-art presentation attack detection (PAD) methods, showing significant variability and generalisation challenges when facing NFMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset + Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manasa', 'Sushrut Patwardhan', 'Narayan Vetrekar', 'Pavan Kumar', 'R. S. Gad', 'Raghavendra Ramachandra']&lt;/li&gt;&lt;li&gt;Tags: ['face-presentation-attack', 'biometrics-security', 'dataset', 'spoofing', 'presentation-attack-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08114</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-modal Deepfake Detection and Localization with FPN-Transformer</title><link>https://arxiv.org/abs/2511.08031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chende Zheng', 'Ruiqi Suo', 'Zhoulin Ji', 'Jingyi Deng', 'Fangbin Yi', 'Chenhao Lin', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08031</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2511.08015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvRoad: a method to generate naturalistic, road-style adversarial posters that cause visual 3D detectors in autonomous driving to hallucinate non-existent objects.&lt;/li&gt;&lt;li&gt;Uses a two-stage pipeline — Road-Style Adversary Generation and Scenario-Associated Adaptation — to maximize attack effectiveness while preserving stealthy, road-like appearance.&lt;/li&gt;&lt;li&gt;Demonstrates generalization across detectors, scenes, and spoofing locations and validates practicality with physical-world attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Lijun He', 'Yixing Yong', 'Haixia Bi', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical-world attack', 'autonomous driving', '3D object detection', 'adversarial patch']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08015</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier</title><link>https://arxiv.org/abs/2511.07806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PC-Diffusion: a lightweight preference classifier that decouples human preference learning from the diffusion generative model, avoiding full-model fine-tuning.&lt;/li&gt;&lt;li&gt;Shows theoretical guarantees: preference-guided distributions propagate across timesteps; classifier objective is equivalent to DPO without requiring a reference model; iterative corrections steer generation toward preferred regions.&lt;/li&gt;&lt;li&gt;Empirically matches DPO in preference consistency while markedly reducing training cost and improving stability for preference-guided image generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaomeng Wang', 'He Wang', 'Xiaolu Wei', 'Longquan Dai', 'Jinhui Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'diffusion models', 'DPO', 'preference classifier']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07806</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks</title><link>https://arxiv.org/abs/2511.07755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Filtered-ViT, a vision transformer that integrates SMART Vector Median Filtering (SMART-VMF) to selectively suppress patch-like corruptions while preserving semantic detail.&lt;/li&gt;&lt;li&gt;Designed to defend against multiple localized adversarial patches (multi-patch LaVAN attacks) and natural patch-like artifacts, achieving 79.8% clean accuracy and 46.3% robust accuracy on ImageNet under four simultaneous 1% patches.&lt;/li&gt;&lt;li&gt;Includes a real-world case study on radiographic medical imagery demonstrating mitigation of occlusions and scanner noise without degrading diagnostic content, claiming unified robustness to adversarial and natural patch disruptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aja Khanal', 'Ahmed Faid', 'Apurva Narayan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patch', 'robustness', 'vision-transformer', 'patch-defense', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07755</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads</title><link>https://arxiv.org/abs/2511.06209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces lightweight transformer-based uncertainty quantification heads (UHeads) that read frozen LLM internal states to estimate uncertainty of individual reasoning steps.&lt;/li&gt;&lt;li&gt;Training labels are generated automatically (via a larger LLM or self-supervision), avoiding expensive human annotation and heavy PRMs.&lt;/li&gt;&lt;li&gt;UHeads (&lt;10M parameters) match or outperform much larger Process Reward Models across domains (mathematics, planning, general QA), showing LLM internals encode useful uncertainty for step-level verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingwei Ni', 'Ekaterina Fadeeva', 'Tianyi Wu', 'Mubashara Akhtar', 'Jiaheng Zhang', 'Elliott Ash', 'Markus Leippold', 'Timothy Baldwin', 'See-Kiong Ng', 'Artem Shelmanov', 'Mrinmaya Sachan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'LLM introspection', 'reasoning verification', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06209</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences</title><link>https://arxiv.org/abs/2511.02109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Deep Value Benchmark (DVB), an evaluation framework that tests whether LLMs learn underlying human values versus surface-level preferences via controlled confounding in training and broken correlations at test time.&lt;/li&gt;&lt;li&gt;Defines Deep Value Generalization Rate (DVGR) as the probability of choosing based on deep values; reports an average DVGR of 0.30 across 9 models (worse than chance) and finds larger models slightly lower DVGR than smaller ones.&lt;/li&gt;&lt;li&gt;Releases a dataset validated by three human experiments and provides an interpretable metric for a core alignment capability (generalizing values rather than superficial features).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Ashkinaze', 'Hua Shen', 'Sai Avula', 'Eric Gilbert', 'Ceren Budak']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'LLM safety', 'benchmarking', 'value learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02109</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title><link>https://arxiv.org/abs/2508.01365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'sequence lock' phenomenon where backdoored LLMs produce target sequences with abnormally high and consistent token confidences.&lt;/li&gt;&lt;li&gt;Proposes ConfGuard, a lightweight runtime detector that monitors a sliding window of token confidences to flag sequence lock and detect backdoor triggers.&lt;/li&gt;&lt;li&gt;Reports near-100% true positive rate and negligible false positive rate across extensive experiments, with almost no added latency suitable for real-time deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Rui Zhang', 'Hongwei Li', 'Wenshu Fan', 'Wenbo Jiang', 'Qingchuan Zhao', 'Guowen Xu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'LLM security', 'runtime defense', 'model poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01365</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title><link>https://arxiv.org/abs/2505.11770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two methods—counterfactual simulation and value probing—that leverage identified internal causal variables to predict correctness of language model outputs.&lt;/li&gt;&lt;li&gt;Evaluates these methods across diverse language-modeling tasks (symbol manipulation, knowledge retrieval, instruction following) and reports high AUC-ROC in-distribution.&lt;/li&gt;&lt;li&gt;Shows causal-mechanism-based predictors outperform causal-agnostic features in out-of-distribution settings, highlighting interpretability tools as practical predictors of model failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Huang', 'Junyi Tao', 'Thomas Icard', 'Diyi Yang', 'Christopher Potts']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'causal analysis', 'OOD robustness', 'model reliability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11770</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MPMA: Preference Manipulation Attack Against Model Context Protocol</title><link>https://arxiv.org/abs/2505.11154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCP Preference Manipulation Attack (MPMA): attackers host malicious MCP servers that manipulate LLMs to prefer them over other tool providers.&lt;/li&gt;&lt;li&gt;Proposes a Direct Preference Manipulation Attack (DPMA) using overt manipulative terms in tool names/descriptions, and a stealthier Genetic-based Advertising Preference Manipulation Attack (GAPMA) that uses initialization strategies plus a genetic algorithm to optimize descriptions.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and stealth trade-offs, demonstrating GAPMA achieves high success while remaining less obvious to users, and highlights need for defenses to preserve fairness in MCP ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Rui Zhang', 'Yu Liu', 'Wenshu Fan', 'Wenbo Jiang', 'Qingchuan Zhao', 'Hongwei Li', 'Guowen Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'LLM security', 'tool-manipulation', 'prompt/description injection', 'genetic-algorithm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11154</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs</title><link>https://arxiv.org/abs/2511.07001</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SCOPE, an inference-time method that uses a sparse autoencoder (SAE) to map hidden states into a high-dimensional near-monosemantic space and identify a copyright-sensitive subspace.&lt;/li&gt;&lt;li&gt;Mitigates reproduction of copyrighted passages by clamping activations in the identified subspace during decoding, without parameter updates or external blocklists/filters.&lt;/li&gt;&lt;li&gt;Claims to reduce copyright leakage including semantically paraphrased outputs while preserving general utility; includes interpretability analyses showing the subspace captures high-level semantics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenliang Zhang', 'Xinyu Hu', 'Xiaojun Wan']&lt;/li&gt;&lt;li&gt;Tags: ['copyright mitigation', 'inference-time defenses', 'LLM safety', 'semantic-space control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07001</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces</title><link>https://arxiv.org/abs/2511.06778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeNLIDB, a privacy-security alignment framework for LLM-based Natural Language to Database interfaces to prevent accidental or adversarial data leakage.&lt;/li&gt;&lt;li&gt;Introduces an automated pipeline that generates hybrid chain-of-thought interaction data combining implicit security reasoning with SQL generation.&lt;/li&gt;&lt;li&gt;Develops reasoning warm-up and alternating preference optimization to mitigate multi-preference oscillations in Direct Preference Optimization (DPO), enabling security-aware outputs without human-annotated preference data.&lt;/li&gt;&lt;li&gt;Reports improved security (reduced leakage/exfiltration) while maintaining high utility, outperforming larger LLMs and ideal-setting baselines in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruiheng Liu', 'XiaoBing Chen', 'Jinyu Zhang', 'Qiongwen Zhang', 'Yu Zhang', 'Bailong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'LLM safety/alignment', 'data exfiltration prevention', 'NLIDB', 'fine-tuning/DPO']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06778</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title><link>https://arxiv.org/abs/2511.05784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses unlearning sensitive or harmful content from deployed LLMs without access to retain data or fine-tuning by using in-context instruction-based interventions.&lt;/li&gt;&lt;li&gt;Proposes DRAGON: a pipeline with a lightweight detection module to flag forget-worthy prompts and a chain-of-thought (CoT) guard model that enforces in-context unlearning before inference.&lt;/li&gt;&lt;li&gt;Introduces new metrics for unlearning performance including continual unlearning, and evaluates DRAGON across multiple unlearning tasks showing effectiveness and scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxuan Wang', 'Chris Yuhao Liu', 'Quan Liu', 'Jinglong Pang', 'Wei Wei', 'Yujia Bao', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'model safety', 'instruction-based defenses', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05784</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title><link>https://arxiv.org/abs/2509.26601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MENLO, a framework and dataset for evaluating native-like response quality across 47 language varieties using audience-design inspired preference annotations.&lt;/li&gt;&lt;li&gt;Provides 6,423 human-annotated prompt-response preference pairs across four quality dimensions with high inter-annotator agreement, and shows zero-shot LLM judges benefit from pairwise evaluation but lag behind humans.&lt;/li&gt;&lt;li&gt;Demonstrates improvements via fine-tuning (RL, reward shaping, multi-task learning) and that RL-trained judges can act as generative reward models to improve multilingual proficiency, though gaps with human judgment remain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Whitehouse', 'Sebastian Ruder', 'Tony Lin', 'Oksana Kurylo', 'Haruka Takagi', 'Janice Lam', 'Nicol\\`o Busetto', 'Denise Diaz', "Francisco Guzm\\'an"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multilingual evaluation', 'reward modeling', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26601</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Isolating Culture Neurons in Multilingual Large Language Models</title><link>https://arxiv.org/abs/2508.02241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a methodology to locate and isolate culture-specific neurons in multilingual LLMs, building on language-specific neuron identification techniques and introducing MUREL (85.2M tokens across six cultures).&lt;/li&gt;&lt;li&gt;Finds culture neurons are concentrated in upper layers and can be modulated largely independently from language-specific neurons or neurons for other cultures via localization and intervention experiments.&lt;/li&gt;&lt;li&gt;Argues that cultural knowledge and propensities in LLMs can be selectively edited, with implications for fairness, inclusivity, and model alignment; code and data are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Danial Namazifard', 'Lukas Galke Poech']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model editing', 'representation analysis', 'bias/fairness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02241</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2508.02087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study showing simple user opinion statements reliably induce sycophancy across model families, while expertise framing has little effect.&lt;/li&gt;&lt;li&gt;Mechanistic analysis using logit-lens and causal activation patching identifies a two-stage emergence: (1) late-layer output preference shifts and (2) deeper representational divergence that overrides learned knowledge.&lt;/li&gt;&lt;li&gt;Shows user authority is not encoded internally and that grammatical perspective (first-person vs third-person) modulates sycophancy by producing stronger deep-layer representational perturbations.&lt;/li&gt;&lt;li&gt;Argues sycophancy is a structural, not surface-level, phenomenon with direct implications for alignment and building truthful AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Li', 'Keyu Wang', 'Shu Yang', 'Zhuoran Zhang', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'alignment', 'model-interpretability', 'truthfulness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02087</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Generalizing Verifiable Instruction Following</title><link>https://arxiv.org/abs/2507.02833</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IFBench, a benchmark of 58 diverse, verifiable out-of-domain output-constraint tasks to evaluate precise instruction-following generalization.&lt;/li&gt;&lt;li&gt;Finds that current models overfit to a small set of verifiable constraints and fail to generalize to unseen constraints.&lt;/li&gt;&lt;li&gt;Designs constraint verification modules and shows that Reinforcement Learning with Verifiable Rewards (RLVR) improves precise instruction following.&lt;/li&gt;&lt;li&gt;Releases 29 hand-annotated training constraints, verification functions, RLVR training prompts, and code to support further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentina Pyatkin', 'Saumya Malik', 'Victoria Graf', 'Hamish Ivison', 'Shengyi Huang', 'Pradeep Dasigi', 'Nathan Lambert', 'Hannaneh Hajishirzi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'instruction-following', 'reinforcement-learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02833</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Trilemma of Truth in Large Language Models</title><link>https://arxiv.org/abs/2506.23921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sAwMIL, a sparse-aware multiple-instance learning framework combined with conformal prediction to probe LLM internal activations for statement veracity (true/false/neither).&lt;/li&gt;&lt;li&gt;Evaluates sAwMIL across 16 open-source LLMs (default and chat variants) on three curated datasets to assess transferability and reliability of veracity probing.&lt;/li&gt;&lt;li&gt;Finds that common probing methods often fail or underperform zero-shot prompting, that truth/falsehood signals are encoded asymmetrically, and that a distinct third signal (neither) is present.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Germans Savcisens', 'Tina Eliassi-Rad']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'model probing', 'alignment', 'safety evaluation', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23921</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedSEA-LLaMA, a federated split-learning framework for LLaMA2 that injects Gaussian noise into forward-pass hidden states to secure end-to-end vector transmission.&lt;/li&gt;&lt;li&gt;Reduces communication overhead via attention-mask compression and KV-cache collaboration, claiming up to 8x speedups in training and inference while retaining centralized LLaMA2 performance.&lt;/li&gt;&lt;li&gt;Supports adaptive partition points (input/output blocks) to tune client-server computation tradeoffs and includes analyses of privacy attacks and partition-point effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated-split-learning', 'privacy-preserving-ml', 'communication-efficiency', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</title><link>https://arxiv.org/abs/2504.01903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAR-1, a high-quality 1K safety dataset targeting large reasoning models (LRMs) to improve alignment.&lt;/li&gt;&lt;li&gt;Constructed via integrating open-source safety datasets, policy-grounded deliberative reasoning sample generation, and GPT-4o-based safety scoring/filtering.&lt;/li&gt;&lt;li&gt;Fine-tuning LRMs with STAR-1 yields ~40% average improvement on safety benchmarks with only ~1.1% average drop in reasoning performance; includes ablation studies.&lt;/li&gt;&lt;li&gt;Evaluated across both LRMs and traditional LLMs to validate design principles and efficacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset / Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijun Wang', 'Haoqin Tu', 'Yuhan Wang', 'Juncheng Wu', 'Yanqing Liu', 'Jieru Mei', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Cihang Xie']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-safety', 'safety-dataset', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01903</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment</title><link>https://arxiv.org/abs/2503.23777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CONGRAD, a filtering method that selects preference-training samples with minimal cross-language gradient conflict to reduce negative interference in multilingual preference alignment.&lt;/li&gt;&lt;li&gt;Uses gradient surgery to keep samples aligned with an aggregated multilingual update direction and adds a sublinear gradient compression strategy to lower memory overhead during accumulation.&lt;/li&gt;&lt;li&gt;Integrates CONGRAD into a self-rewarding framework and demonstrates consistent improvements over baselines on LLaMA3-8B and Gemma2-2B across 10 languages, with minimal alignment tax on performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiangnan Li', 'Thuy-Trang Vu', 'Christian Herold', 'Amirhossein Tebbifakhr', 'Shahram Khadivi', 'Gholamreza Haffari']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multilingual', 'optimization', 'gradient filtering', 'preference alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.23777</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ENCORE: Entropy-guided Reward Composition for Multi-head Safety Reward Models</title><link>https://arxiv.org/abs/2503.20995</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that safety rules with higher rating entropy tend to be less accurate at distinguishing human-preferred responses.&lt;/li&gt;&lt;li&gt;Proposes ENCORE, an entropy-guided, training-free method that penalizes high-entropy rules when composing multi-head reward models.&lt;/li&gt;&lt;li&gt;Provides theoretical justification via the Bradley–Terry loss showing high-entropy rules receive negligible weights, and demonstrates empirical gains over baselines on RewardBench safety tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaomin Li', 'Xupeng Chen', 'Jingxuan Fan', 'Eric Hanchen Jiang', 'Mingye Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'reward modeling', 'RLHF', 'safety evaluation', 'multi-attribute reward']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20995</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering</title><link>https://arxiv.org/abs/2409.04181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid approach that uses LLMs to generate Cypher queries and a query-checker to ensure syntactic/semantic validity before extracting answers from a biomedical knowledge graph, aiming to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Evaluates the method on a 50-question biomedical benchmark across several LLMs (including GPT-4 Turbo and llama3:70b); GPT-4 Turbo performs best, while open-source models can be viable with prompt engineering.&lt;/li&gt;&lt;li&gt;Provides a web-based interface exposing generated and corrected Cypher queries and resulting KG paths, and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Larissa Pusch', 'Tim O. F. Conrad']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM safety', 'knowledge graph', 'query validation', 'biomedical QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.04181</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity</title><link>https://arxiv.org/abs/2511.08487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-dimensional analysis of agent safety under intent concealment and task complexity.&lt;/li&gt;&lt;li&gt;Introduces OASIS: a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox for probing agent behaviors.&lt;/li&gt;&lt;li&gt;Finds safety alignment degrades predictably as malicious intent is obscured and reports a "Complexity Paradox" where agents appear safer on harder tasks due to capability limits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Ma', 'Dongsheng Zhu', 'Shudong Liu', 'Taolin Zhang', 'Junnan Liu', 'Qingqiu Li', 'Minnan Luo', 'Songyang Zhang', 'Kai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['agent safety', 'LLM red teaming', 'benchmarking', 'alignment', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08487</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</title><link>https://arxiv.org/abs/2511.07931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SpeechJudge-Data: a large-scale human preference corpus (99K speech pairs) annotated for intelligibility and naturalness across diverse TTS systems, styles, and languages.&lt;/li&gt;&lt;li&gt;Proposes SpeechJudge-Eval, a benchmark showing existing metrics and AudioLLMs (best at &lt;70% agreement) perform below human-level judgment for speech naturalness.&lt;/li&gt;&lt;li&gt;Develops SpeechJudge-GRM (generative reward model based on Qwen2.5-Omni-7B) trained via SFT with Chain-of-Thought rationales and RL (GRPO), achieving 77.2% accuracy (79.4% after inference-time scaling) and outperforming a Bradley-Terry reward model.&lt;/li&gt;&lt;li&gt;Shows SpeechJudge-GRM can be used as a reward function to align speech generation models with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xueyao Zhang', 'Chaoren Wang', 'Huan Liao', 'Ziniu Li', 'Yuancheng Wang', 'Li Wang', 'Dongya Jia', 'Yuanzhe Chen', 'Xiulin Li', 'Zhuo Chen', 'Zhizheng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'human-preferences', 'evaluation-benchmark', 'speech-synthesis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07931</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder</title><link>https://arxiv.org/abs/2511.07896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SparseRM, a lightweight reward model that uses a Sparse Autoencoder (SAE) to decompose LLM representations into interpretable, preference-relevant directions.&lt;/li&gt;&lt;li&gt;Computes alignment scores by projecting representations onto these sparse directions and aggregates them with a simple reward head, using &lt;1% of trainable parameters.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance on three preference modeling tasks and claims easy integration into downstream alignment pipelines for efficient alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengcan Liu', 'Jiahao Li', 'Zheren Fu', 'Yi Tu', 'Jiajun Li', 'Zhendong Mao', 'Yongdong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference modeling', 'interpretability', 'efficient models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07896</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation</title><link>https://arxiv.org/abs/2511.07876</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoopLLM, an attack framework that induces repetitive, low-entropy decoding loops to force LLMs to generate until output limits, increasing energy and latency costs.&lt;/li&gt;&lt;li&gt;Proposes two technical components: a repetition-inducing prompt optimization and a token-aligned ensemble optimization to improve cross-model transferability.&lt;/li&gt;&lt;li&gt;Provides extensive empirical evaluation on 12 open-source and 2 commercial LLMs, showing &gt;90% of maximum output length vs ~20% for baselines and ~40% transferability improvement to models like DeepSeek-V3 and Gemini 2.5 Flash.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingyu Li', 'Xiaolei Liu', 'Cheng Liu', 'Yixiao Xu', 'Kangyi Ding', 'Bangzhou Xin', 'Jia-Li Yin']&lt;/li&gt;&lt;li&gt;Tags: ['energy-latency attacks', 'adversarial prompting', 'LLM robustness', 'transferability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07876</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title><link>https://arxiv.org/abs/2511.07772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs can leak sensitive information through internal Chain-of-Thought (CoT) activations even when outputs are safe, violating contextual privacy.&lt;/li&gt;&lt;li&gt;Proposes SALT, a lightweight test-time intervention that injects targeted steering vectors into high-leakage hidden layers to reduce CoT privacy leakage.&lt;/li&gt;&lt;li&gt;Evaluates SALT across multiple LLMs and datasets, reporting substantial reductions in contextual privacy leakage (CPL) while maintaining task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shourya Batra', 'Pierce Tillman', 'Samarth Gaggar', 'Shashank Kesineni', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Vasu Sharma', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'chain-of-thought leakage', 'test-time defense', 'LLM internal activations', 'reasoning safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07772</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title><link>https://arxiv.org/abs/2511.07585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of output drift across five LLMs (7B–120B) on three regulated financial tasks, showing smaller models can be more deterministically consistent than very large models under greedy decoding.&lt;/li&gt;&lt;li&gt;Introduces a finance-calibrated deterministic test harness (T=0.0, fixed seeds, SEC 10-K structure-aware retrieval ordering) and task-specific invariant checks for RAG, JSON, and SQL outputs with materiality thresholds (±5%).&lt;/li&gt;&lt;li&gt;Proposes a three-tier model classification for deployment risk, an audit-ready attestation system with dual-provider validation, and maps the framework to FSB/BIS/CFTC compliance requirements.&lt;/li&gt;&lt;li&gt;Findings: structured outputs (SQL) are robust even with sampling, RAG tasks exhibit substantial drift (25–75%), and cross-provider validation shows determinism can transfer between local and cloud deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffi Khatchadourian', 'Rolando Franco']&lt;/li&gt;&lt;li&gt;Tags: ['LLM output drift', 'determinism &amp; reproducibility', 'RAG robustness', 'AI auditability &amp; compliance', 'cross-provider validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07585</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title><link>https://arxiv.org/abs/2511.07482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-critical circuits during inference.&lt;/li&gt;&lt;li&gt;Targets alignment degradation caused by dynamic pruning by identifying and retaining input-dependent safety-relevant circuitry.&lt;/li&gt;&lt;li&gt;Builds on Probe Pruning and evaluates AAPP on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT.&lt;/li&gt;&lt;li&gt;Reports improved safety behavior (≈50% increase in refusal rates) at matched compute, enabling more efficient yet safety-preserving deployment of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dev Patel', 'Gabrielle Gervacio', 'Diekola Raimi', 'Kevin Zhu', 'Ryan Lagasse', 'Gabriel Grand', 'Ashwinee Panda', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model pruning', 'safety-preserving inference', 'dynamic pruning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07482</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Polite Liar: Epistemic Pathology in Language Models</title><link>https://arxiv.org/abs/2511.07477</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that RLHF incentivizes models to prioritize perceived sincerity and conversational fluency over evidential accuracy, producing a systematic 'polite liar' pathology.&lt;/li&gt;&lt;li&gt;Frames the problem using philosophical concepts (Frankfurt on bullshit, speech-act theory) and epistemic virtue theory to distinguish structural indifference from intentional deception.&lt;/li&gt;&lt;li&gt;Identifies an alignment tension: reward signals for helpfulness/harmlessness/politeness do not ensure epistemic grounding, leading to confident fabrication.&lt;/li&gt;&lt;li&gt;Proposes an 'epistemic alignment' principle: design reward architectures that favor justified confidence (epistemic grounding) rather than merely perceived fluency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bentley DeVilling (Course Correct Labs)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'epistemic-risk', 'model-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07477</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models</title><link>https://arxiv.org/abs/2511.08565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an MFQ-based benchmark measuring 'moral susceptibility' (variability across personas) and 'moral robustness' (variability within personas) for LLMs under persona role-play.&lt;/li&gt;&lt;li&gt;Finds model family explains most variance in robustness (Claude highest, then Gemini and GPT-4); model size has no systematic effect on robustness.&lt;/li&gt;&lt;li&gt;Finds susceptibility shows a within-family size effect (larger variants more susceptible) and that robustness and susceptibility are positively correlated, stronger at the family level.&lt;/li&gt;&lt;li&gt;Provides moral foundation profiles for models without role-play and averaged persona profiles, offering a systematic view of how persona conditioning shifts moral behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davi Bastos Costa', 'Felippe Alves', 'Renato Vicente']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'persona-role-play', 'jailbreaking/prompt-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08565</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Investigating CoT Monitorability in Large Reasoning Models</title><link>https://arxiv.org/abs/2511.08525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of Chain-of-Thought (CoT) monitorability: how faithfully LLMs verbalize internal decision factors and how reliably CoT-based monitors detect misbehavior (shortcuts, sycophancy).&lt;/li&gt;&lt;li&gt;Empirical correlation analyses between verbalization quality, monitor reliability, and model performance across mathematical, scientific, and ethical tasks.&lt;/li&gt;&lt;li&gt;Evaluation of how CoT intervention methods affect monitoring effectiveness and proposal of MoME, a paradigm where LLMs monitor other models' misbehavior via CoT and produce structured judgments with supporting evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yang', 'Junchao Wu', 'Xilin Gou', 'Xuansheng Wu', 'Derek Wong', 'Ninhao Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'chain-of-thought', 'monitoring', 'interpretability', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08525</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?</title><link>https://arxiv.org/abs/2511.08455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes shortcut learning in social bot detectors by constructing spurious associations between labels and superficial textual cues to simulate OOD/unknown-invariance scenarios.&lt;/li&gt;&lt;li&gt;Shows baseline detectors suffer large robustness loss (≈32% relative accuracy drop) when irrelevant textual feature distributions shift.&lt;/li&gt;&lt;li&gt;Proposes LLM-based counterfactual data augmentation and mitigation strategies at individual text, dataset, and model levels to reduce reliance on shortcuts.&lt;/li&gt;&lt;li&gt;Reports that the proposed methods yield substantial gains (≈56% average relative performance improvement) under the shortcut scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyan Zheng', 'Herun Wan', 'Minnan Luo', 'Junhang Huang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'shortcut-learning', 'adversarial-robustness', 'LLM-based-mitigation', 'social-bot-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08455</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interaction Dynamics as a Reward Signal for LLMs</title><link>https://arxiv.org/abs/2511.08394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE, a reward signal derived from geometric properties of a conversation's embedding trajectory ('conversational geometry').&lt;/li&gt;&lt;li&gt;Shows a model trained only on these interaction-dynamics signals attains 68.20% pairwise accuracy versus 70.04% for a strong LLM baseline using full transcripts.&lt;/li&gt;&lt;li&gt;Demonstrates a hybrid model combining dynamics and textual analysis achieves 80.17%, indicating complementary value and improved alignment/evaluation performance.&lt;/li&gt;&lt;li&gt;Claims privacy-preserving properties and positions the method as both an alignment tool and a diagnostic for interaction patterns that drive successful collaboration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sian Gooding', 'Edward Grefenstette']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'evaluation/diagnostics', 'interaction dynamics', 'privacy-preserving signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08394</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints</title><link>https://arxiv.org/abs/2511.08392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PCRLLM, a framework that forces LLM reasoning to be expressed as single-step inferences with explicit premises, rules, and conclusions to enable formal verification.&lt;/li&gt;&lt;li&gt;Enables chain-level validation of reasoning traces against a target logic, including in black-box model settings, improving trustworthiness.&lt;/li&gt;&lt;li&gt;Supports systematic multi-LLM collaboration by comparing and integrating intermediate steps under formal rules.&lt;/li&gt;&lt;li&gt;Introduces a benchmark schema for large-scale step-level reasoning data combining natural-language expressiveness with formal rigor.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tangrui Li', 'Pei Wang', 'Hongzheng Wang Christian Hahm', 'Matteo Spatola', 'Justin Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Verification', 'Formal reasoning', 'Chain-of-thought', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08392</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification</title><link>https://arxiv.org/abs/2511.07888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MC^2F, a two-module defense that operates on sentence embeddings to counter adversarial attacks in text classification.&lt;/li&gt;&lt;li&gt;Uses a Stratified Riemannian Continuous Normalizing Flow (SR-CNF) to learn the clean-data embedding manifold and detect out-of-distribution/adversarial embeddings.&lt;/li&gt;&lt;li&gt;Applies a Geodesic Purification Solver to project adversarial embeddings back onto the manifold via shortest-path (geodesic) correction, restoring semantically coherent representations.&lt;/li&gt;&lt;li&gt;Evaluated across three datasets and multiple adversarial attacks, reporting state-of-the-art robustness while preserving or modestly improving clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenhao Dang', 'Jing Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial defense', 'manifold learning', 'text classification', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07888</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys</title><link>https://arxiv.org/abs/2511.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AlignSurvey, a comprehensive benchmark that models and evaluates the full social survey pipeline with LLMs across four tasks: social role modeling, semi-structured interview modeling, attitude stance modeling, and survey response modeling.&lt;/li&gt;&lt;li&gt;Provides a multi-tiered dataset (Social Foundation Corpus: 44K+ dialogues, 400K+ survey records) plus expert-annotated and nationally representative Entire-Pipeline Survey Datasets, with task-specific metrics for fidelity, consistency, and demographic fairness.&lt;/li&gt;&lt;li&gt;Releases SurveyLM family (two-stage fine-tuned open-source LLMs) and reference models/tools for alignment evaluation, enabling reproducible assessment of LLM behavior in survey contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Lin', 'Weikang Yuan', 'Zhuoren Jiang', 'Biao Huang', 'Ruitao Zhang', 'Jianan Ge', 'Yueqian Xu', 'Jianxing Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fairness', 'benchmark', 'LLM evaluation', 'datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07871</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark</title><link>https://arxiv.org/abs/2511.07794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents CUFEInse v1.0, an insurance-focused LLM benchmark covering 5 core dimensions (including safety and compliance), 54 sub-indicators, and 14,430 questions.&lt;/li&gt;&lt;li&gt;Evaluates 11 mainstream LLMs and reports domain findings: generalist models have weak actuarial and compliance adaptation; domain-trained models excel in vertical knowledge but lag in business adaptation and compliance.&lt;/li&gt;&lt;li&gt;Identifies common bottlenecks in professional insurance tasks (actuarial reasoning, underwriting, claims, compliant marketing) and recommends future focus on "domain adaptation + reasoning enhancement."&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Zhou (Central University of Finance', 'Economics)', 'Bing Ma (Central University of Finance', 'Economics)', 'Yufei Zhang (Zetavision AI Lab)', 'Yi Zhao (Zetavision AI Lab)']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'compliance', 'LLM-benchmark', 'domain-adaptation', 'insurance-LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07794</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Critical Confabulation: Can LLMs Hallucinate for Social Good?</title><link>https://arxiv.org/abs/2511.07722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'critical confabulation': using controlled LLM hallucinations to fill archival gaps and reconstruct evidence-bound narratives for marginalized historical figures.&lt;/li&gt;&lt;li&gt;Introduces an open-ended narrative cloze task on a novel corpus of unpublished texts to simulate omissions and evaluate models' ability to generate useful, bounded hallucinations.&lt;/li&gt;&lt;li&gt;Audits for data contamination and evaluates multiple model families (audited open models, unaudited open-weight, proprietary) under prompt strategies designed to elicit controlled hallucinations, assessing fidelity vs. speculation trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiqi Sui', 'Eamon Duede', 'Hoyt Long', 'Richard Jean So']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM safety', 'alignment', 'evaluation/benchmarking', 'controlled generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07722</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences</title><link>https://arxiv.org/abs/2511.07691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAPO, a confidence-aware variant of Direct Preference Optimization that scales loss by a relative reward-based confidence for each preference pair.&lt;/li&gt;&lt;li&gt;Designed to improve multilingual preference tuning by reducing sensitivity to noisy or low-margin comparisons common in non-English data.&lt;/li&gt;&lt;li&gt;Reports empirical gains (≈16% improvement in reward accuracy) and wider preference gaps between preferred and dispreferred responses across languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Yufei Tao', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'multilingual robustness', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07691</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title><link>https://arxiv.org/abs/2511.07689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates six reference-free factuality metrics, originally for short summaries, in long-document summarization across three domains (science fiction, legal, scientific).&lt;/li&gt;&lt;li&gt;Applies seven meaning-preserving perturbations (e.g., paraphrase, synonym replacement, compression, logical negation equivalents) to test metric stability and sensitivity.&lt;/li&gt;&lt;li&gt;Finds that existing short-form metrics give inconsistent scores for semantically equivalent summaries and degrade for dense claims or long contexts; expanding retrieval helps sometimes but no metric is consistently reliable.&lt;/li&gt;&lt;li&gt;Provides recommendations for improving factuality evaluation (multi-span reasoning, context-aware calibration, training on meaning-preserving variations) and releases code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zain Muhammad Mujahid', 'Dustin Wright', 'Isabelle Augenstein']&lt;/li&gt;&lt;li&gt;Tags: ['factuality evaluation', 'robustness', 'long-document summarization', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07689</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization</title><link>https://arxiv.org/abs/2511.07210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generative Clean-Image Backdoors (GCB), which uses a conditional InfoGAN to optimize naturally occurring image features as stealthy and potent backdoor triggers.&lt;/li&gt;&lt;li&gt;Shows the optimized triggers enable successful clean-image (clean-label) backdoors with very low poison rates and minimal clean accuracy drop (&lt;1%).&lt;/li&gt;&lt;li&gt;Demonstrates generality across six datasets, five architectures, multiple tasks (classification, regression, segmentation) and robustness to most existing backdoor defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binyan Xu', 'Fan Yang', 'Di Tang', 'Xilin Dai', 'Kehuan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'clean-label backdoor', 'trigger optimization', 'poisoning', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07210</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment</title><link>https://arxiv.org/abs/2511.06852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes decomposing LLM safety representations into a Harm Detection Direction and a Refusal Execution Direction rather than a single alignment vector.&lt;/li&gt;&lt;li&gt;Introduces Differentiated Bi-Directional Intervention (DBDI), a white-box method that nullifies the refusal execution direction via projection and suppresses harm detection via steering at critical layers.&lt;/li&gt;&lt;li&gt;Reports strong jailbreak performance (up to 97.88% attack success on models like Llama-2) and provides mechanistic insights into alignment failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Zhang', 'Peijie Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'alignment evasion', 'activation-space interventions', 'white-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06852</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title><link>https://arxiv.org/abs/2511.05784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses unlearning sensitive or harmful content from deployed LLMs without access to retain data or fine-tuning by using in-context instruction-based interventions.&lt;/li&gt;&lt;li&gt;Proposes DRAGON: a pipeline with a lightweight detection module to flag forget-worthy prompts and a chain-of-thought (CoT) guard model that enforces in-context unlearning before inference.&lt;/li&gt;&lt;li&gt;Introduces new metrics for unlearning performance including continual unlearning, and evaluates DRAGON across multiple unlearning tasks showing effectiveness and scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxuan Wang', 'Chris Yuhao Liu', 'Quan Liu', 'Jinglong Pang', 'Wei Wei', 'Yujia Bao', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'model safety', 'instruction-based defenses', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05784</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title><link>https://arxiv.org/abs/2509.26601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MENLO, a framework and dataset for evaluating native-like response quality across 47 language varieties using audience-design inspired preference annotations.&lt;/li&gt;&lt;li&gt;Provides 6,423 human-annotated prompt-response preference pairs across four quality dimensions with high inter-annotator agreement, and shows zero-shot LLM judges benefit from pairwise evaluation but lag behind humans.&lt;/li&gt;&lt;li&gt;Demonstrates improvements via fine-tuning (RL, reward shaping, multi-task learning) and that RL-trained judges can act as generative reward models to improve multilingual proficiency, though gaps with human judgment remain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Whitehouse', 'Sebastian Ruder', 'Tony Lin', 'Oksana Kurylo', 'Haruka Takagi', 'Janice Lam', 'Nicol\\`o Busetto', 'Denise Diaz', "Francisco Guzm\\'an"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multilingual evaluation', 'reward modeling', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26601</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Trilemma of Truth in Large Language Models</title><link>https://arxiv.org/abs/2506.23921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sAwMIL, a sparse-aware multiple-instance learning framework combined with conformal prediction to probe LLM internal activations for statement veracity (true/false/neither).&lt;/li&gt;&lt;li&gt;Evaluates sAwMIL across 16 open-source LLMs (default and chat variants) on three curated datasets to assess transferability and reliability of veracity probing.&lt;/li&gt;&lt;li&gt;Finds that common probing methods often fail or underperform zero-shot prompting, that truth/falsehood signals are encoded asymmetrically, and that a distinct third signal (neither) is present.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Germans Savcisens', 'Tina Eliassi-Rad']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'model probing', 'alignment', 'safety evaluation', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23921</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Wasserstein Distributionally Robust Nonparametric Regression</title><link>https://arxiv.org/abs/2505.07967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes Wasserstein distributionally robust optimization (WDRO) in nonparametric regression, showing a structural difference: k=1 yields Lipschitz-type regularization, k&gt;1 yields gradient-norm regularization.&lt;/li&gt;&lt;li&gt;Derives non-asymptotic excess local worst-case risk bounds for estimators built from norm-constrained feedforward neural networks, with new covering and approximation bounds controlling both functions and their gradients.&lt;/li&gt;&lt;li&gt;Establishes a convergence rate n^{-2β/(d+2β)} (up to logs), proves minimax optimality under common high-dimensional conditions, and shows these bounds imply guarantees on excess natural risk for distributions inside the ambiguity set.&lt;/li&gt;&lt;li&gt;Demonstrates generality for regression and classification and presents simulations plus an MNIST application illustrating robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changyu Liu', 'Yuling Jiao', 'Junhui Wang', 'Jian Huang']&lt;/li&gt;&lt;li&gt;Tags: ['distributional-robustness', 'Wasserstein-DRO', 'robustness-theory', 'nonparametric-regression', 'neural-network-regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07967</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints</title><link>https://arxiv.org/abs/2510.04058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Variational Diffusion Unlearning (VDU), a variational-inference-based method to remove undesired classes/features from pre-trained diffusion models when the full training dataset is inaccessible.&lt;/li&gt;&lt;li&gt;Optimizes a loss with two terms: a plasticity inducer to reduce likelihood of undesired training points and a stability regularizer to preserve generation quality in parameter space.&lt;/li&gt;&lt;li&gt;Requires only a subset of undesired-data examples and is evaluated on class unlearning (MNIST, CIFAR-10, tinyImageNet with DDPM) and feature unlearning (Stable Diffusion).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subhodip Panda', 'MS Varun', 'Shreyans Jain', 'Sarthak Kumar Maharana', 'Prathosh A. P']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'diffusion models', 'model safety', 'generative model mitigation', 'data-constrained unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04058</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title><link>https://arxiv.org/abs/2509.05429</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Topology Inference Attacks (TIAs) that reconstruct a target training graph from black-box access to a GNN, demonstrating high susceptibility of GNNs to graph-level structure leakage.&lt;/li&gt;&lt;li&gt;Shows that existing edge-level differential privacy mechanisms are insufficient to prevent topology leakage or incur large accuracy degradation.&lt;/li&gt;&lt;li&gt;Proposes Private Graph Reconstruction (PGR), a bi-level optimization defense that generates a synthetic training graph via meta-gradients while jointly training the GNN to reduce topology leakage with minimal accuracy loss.&lt;/li&gt;&lt;li&gt;Provides extensive experiments validating PGR's effectiveness and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Fu', 'Yuan Hong', 'Zhili Chen', 'Wendy Hui Wang']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'privacy attack', 'topology inference', 'defense', 'bi-level optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05429</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable Reward Model via Sparse Autoencoder</title><link>https://arxiv.org/abs/2508.08746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SARM: integrates a pretrained Sparse Autoencoder with a reward model to produce sparse, monosemantic feature representations from LLM hidden activations.&lt;/li&gt;&lt;li&gt;Enables feature-level attribution for reward assignments and a scalar aggregation head for interpretable reward scores.&lt;/li&gt;&lt;li&gt;Claims improved adaptability to user preference shifts and superior alignment performance vs conventional reward models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyi Zhang', 'Wei Shi', 'Sihang Li', 'Jiayi Liao', 'Tao Liang', 'Hengxing Cai', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'reward-models', 'RLHF', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08746</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORVIT: Near-Optimal Online Distributionally Robust Reinforcement Learning</title><link>https://arxiv.org/abs/2508.03768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Considers online distributionally robust reinforcement learning where the agent only interacts with a single unknown training environment but must optimize worst-case performance over an ambiguity set around the nominal model.&lt;/li&gt;&lt;li&gt;Uses general f-divergence-based ambiguity sets (including KL and χ^2 balls) and develops a computationally efficient algorithm that attains sublinear regret for the robust objective under minimal assumptions.&lt;/li&gt;&lt;li&gt;Proves a minimax lower bound on regret, showing the proposed method is near-optimal.&lt;/li&gt;&lt;li&gt;Empirical results across environments with model misspecification demonstrate improved worst-case (robust) performance consistent with theoretical guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debamita Ghosh', 'George K. Atia', 'Yue Wang']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'robust reinforcement learning', 'regret bounds', 'f-divergence', 'model misspecification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03768</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</title><link>https://arxiv.org/abs/2505.13709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework that jointly adapts a learned world model and the policy via a unified maximin objective to improve robustness in offline model-based RL.&lt;/li&gt;&lt;li&gt;Uses Stackelberg learning dynamics to solve the maximin optimization and provides theoretical analysis plus efficient implementations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on noisy D4RL MuJoCo tasks and stochastic Tokamak Control benchmarks, addressing sensitivity to small adversarial/environmental perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Chen', 'Le Xu', 'Aravind Venugopal', 'Jeff Schneider']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'offline model-based RL', 'world model adaptation', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13709</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title><link>https://arxiv.org/abs/2505.11770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two methods—counterfactual simulation and value probing—that leverage identified internal causal variables to predict correctness of language model outputs.&lt;/li&gt;&lt;li&gt;Evaluates these methods across diverse language-modeling tasks (symbol manipulation, knowledge retrieval, instruction following) and reports high AUC-ROC in-distribution.&lt;/li&gt;&lt;li&gt;Shows causal-mechanism-based predictors outperform causal-agnostic features in out-of-distribution settings, highlighting interpretability tools as practical predictors of model failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Huang', 'Junyi Tao', 'Thomas Icard', 'Diyi Yang', 'Christopher Potts']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'causal analysis', 'OOD robustness', 'model reliability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11770</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Certified Robust Invariant Polytope Training in Neural Controlled ODEs</title><link>https://arxiv.org/abs/2408.01273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training framework to obtain neural-network state-feedback controllers with certified robust forward invariant polytopes: any trajectory starting inside the polytope stays inside despite disturbances.&lt;/li&gt;&lt;li&gt;Transforms the system into lifted embedding systems in higher dimensions, uses interval analysis and neural network verifiers, and reduces certification to a sign constraint at a single point.&lt;/li&gt;&lt;li&gt;Presents an algorithm that jointly trains controller and lifted-system parameters to certify invariance, demonstrating scalability to &gt;50 states and faster runtime than Lyapunov-based sampling baselines on examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akash Harapanahalli', 'Samuel Coogan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'controller verification', 'neural ODEs', 'formal verification', 'safety-critical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.01273</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multiplicative Reweighting for Robust Neural Network Optimization</title><link>https://arxiv.org/abs/2102.12192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using multiplicative weights (MW) to reweight training examples during neural network optimization to improve robustness to noisy labels.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees when combined with gradient descent and analyzes advantages in 1D cases.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved accuracy under label noise on CIFAR-10, CIFAR-100, and Clothing1M, and reports effects on adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noga Bar', 'Tomer Koren', 'Raja Giryes']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'noisy labels', 'multiplicative weights', 'optimization', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2102.12192</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interaction Dynamics as a Reward Signal for LLMs</title><link>https://arxiv.org/abs/2511.08394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE, a reward signal derived from geometric properties of a conversation's embedding trajectory ('conversational geometry').&lt;/li&gt;&lt;li&gt;Shows a model trained only on these interaction-dynamics signals attains 68.20% pairwise accuracy versus 70.04% for a strong LLM baseline using full transcripts.&lt;/li&gt;&lt;li&gt;Demonstrates a hybrid model combining dynamics and textual analysis achieves 80.17%, indicating complementary value and improved alignment/evaluation performance.&lt;/li&gt;&lt;li&gt;Claims privacy-preserving properties and positions the method as both an alignment tool and a diagnostic for interaction patterns that drive successful collaboration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sian Gooding', 'Edward Grefenstette']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'evaluation/diagnostics', 'interaction dynamics', 'privacy-preserving signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08394</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models</title><link>https://arxiv.org/abs/2511.08379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using Self-Organizing Maps (SOMs) to extract multiple latent 'refusal' directions from LLM representations, rather than a single difference-of-means vector.&lt;/li&gt;&lt;li&gt;Proves SOMs generalize the prior difference-in-means technique and derives multiple directions by subtracting the harmless-centroid from each SOM neuron centroid.&lt;/li&gt;&lt;li&gt;Shows ablating multiple directions in model internals more effectively suppresses refusal behavior than the single-direction baseline and outperforms specialized jailbreak algorithms in their experiments.&lt;/li&gt;&lt;li&gt;Provides mechanistic interpretability analysis of the discovered manifold of refusal directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgio Piras', 'Raffaele Mura', 'Fabio Brau', 'Luca Oneto', 'Fabio Roli', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM interpretability', 'refusal suppression', 'adversarial prompting', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08379</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Proof Minimization in Neural Network Verification</title><link>https://arxiv.org/abs/2511.08198</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces algorithms to minimize unsatisfiability proofs produced by DNN verifiers by removing learned facts and unnecessary dependencies.&lt;/li&gt;&lt;li&gt;Analyzes dependencies among facts used to derive UNSAT and applies two alternative procedures to further eliminate redundancies.&lt;/li&gt;&lt;li&gt;Implements the methods on a proof-producing DNN verifier and evaluates across benchmarks, achieving 37%–82% proof size reduction and 30%–88% proof checking time reduction with a 7%–20% verification runtime overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omri Isac', 'Idan Refaeli', 'Haoze Wu', 'Clark Barrett', 'Guy Katz']&lt;/li&gt;&lt;li&gt;Tags: ['DNN verification', 'Formal verification', 'Safety assurance', 'Proof minimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08198</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency</title><link>https://arxiv.org/abs/2511.08082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a five-pillar prudential framework (governance, data lineage, assurance, resilience, regulatory alignment) for assessing LLM reliability in reinsurance.&lt;/li&gt;&lt;li&gt;Implements the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB) to evaluate governance-embedded, retrieval-grounded LLM configurations across six task families.&lt;/li&gt;&lt;li&gt;Reports empirical improvements: grounding accuracy ~0.90, about 40% reduction in hallucination/interpretive drift, and nearly doubled transparency, arguing that existing prudential doctrines can accommodate reliable AI with explicit governance and verifiable assurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stella C. Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Governance &amp; assurance', 'Benchmarking', 'Retrieval grounding', 'Regulatory alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08082</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure</title><link>https://arxiv.org/abs/2511.07997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrAda-GAN, a differentially private synthetic data generator combining GAN-based and marginal-based approaches with a sequential generator that models dependencies via a Bayesian network structure.&lt;/li&gt;&lt;li&gt;Introduces adaptive regularization to promote sparsity in the learned Bayes network and provides theoretical bounds on parameter distance, variable selection error, and Wasserstein distance, showing improved convergence rates under dependency sparsity.&lt;/li&gt;&lt;li&gt;Empirical results on synthetic and real-world tabular datasets report improved privacy-utility trade-offs compared to existing tabular data synthesis methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ke Jia', 'Yuheng Ma', 'Yang Li', 'Feifei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'synthetic-data', 'privacy-preserving-ml', 'generative-models', 'tabular-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07997</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</title><link>https://arxiv.org/abs/2511.07947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies weaknesses in existing black-box watermarks and introduces WRK, an attack that bypasses representation entanglement by exploiting sample-level decision boundaries, decreasing watermark success rates by ≥88.79% on benchmarks.&lt;/li&gt;&lt;li&gt;Proposes Class-Feature Watermarks (CFW), which embed class-level artifacts via a synthetic out-of-domain class to remove vulnerable decision boundaries and jointly optimize for transferability to extracted models and post-extraction stability.&lt;/li&gt;&lt;li&gt;Empirical results across multiple domains show CFW preserves model utility while maintaining at least 70.15% watermark success in extracted models even under combined MEA + WRK distortions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxin Xiao', 'Qingqing Ye', 'Zi Liang', 'Haoyang Li', 'RongHua Li', 'Huadi Zheng', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model watermarking', 'model extraction attacks', 'watermark removal', 'black-box security', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07947</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Distributionally Robust Online Markov Game with Linear Function Approximation</title><link>https://arxiv.org/abs/2511.07831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies distributionally robust reinforcement learning in online Markov games to address sim-to-real/environmental shifts under a d-rectangular uncertainty model.&lt;/li&gt;&lt;li&gt;Identifies a hardness result under d-rectangularity and introduces a 'minimum value' assumption to make learning tractable.&lt;/li&gt;&lt;li&gt;Proposes DR-CCE-LSI, a least-squares value-iteration style algorithm with a multi-agent exploration bonus to compute an ε-approximate robust Coarse Correlated Equilibrium (CCE).&lt;/li&gt;&lt;li&gt;Provides regret/sample-complexity guarantees: O(d H min{H, 1/min_i σ_i} sqrt(K)), claims sample-efficiency matching single-agent results and minimax optimality in feature dimension, and includes simulation validation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zewu Zheng', 'Yuanyuan Lin']&lt;/li&gt;&lt;li&gt;Tags: ['distributional robustness', 'robust reinforcement learning', 'multi-agent RL / Markov games', 'theoretical guarantees / sample complexity', 'sim-to-real / domain shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07831</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation</title><link>https://arxiv.org/abs/2511.07807</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM, a privacy-preserving inference framework that enables CNN inference over encrypted data using homomorphic encryption (CKKS) by replacing non-linear activations with homomorphically compatible polynomial approximations and restructuring the CNN.&lt;/li&gt;&lt;li&gt;Introduces an efficient activation approximation method (degree-4 polynomial / Softplus approximation) to reduce HE computation overhead while maintaining model accuracy.&lt;/li&gt;&lt;li&gt;Reports empirical results on CIFAR-10: 94.4% accuracy with 2.42 s per encrypted sample and 24,000 s per 10,000 encrypted samples under CKKS, demonstrating a trade-off between accuracy and computational cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeinab Elkhatib', 'Ali Sekmen', 'Kamrul Hasan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'homomorphic encryption', 'secure inference', 'model deployment', 'activation approximation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07807</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title><link>https://arxiv.org/abs/2511.07772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs can leak sensitive information through internal Chain-of-Thought (CoT) activations even when outputs are safe, violating contextual privacy.&lt;/li&gt;&lt;li&gt;Proposes SALT, a lightweight test-time intervention that injects targeted steering vectors into high-leakage hidden layers to reduce CoT privacy leakage.&lt;/li&gt;&lt;li&gt;Evaluates SALT across multiple LLMs and datasets, reporting substantial reductions in contextual privacy leakage (CPL) while maintaining task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shourya Batra', 'Pierce Tillman', 'Samarth Gaggar', 'Shashank Kesineni', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Vasu Sharma', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'chain-of-thought leakage', 'test-time defense', 'LLM internal activations', 'reasoning safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07772</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Misaligned by Design: Incentive Failures in Machine Learning</title><link>https://arxiv.org/abs/2511.07699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that training ML models with asymmetric loss functions intended to align with human decision-makers can backfire in high-stakes settings; it may be better to train with a neutral loss and adjust predictions ex post.&lt;/li&gt;&lt;li&gt;Develops an economic model where classifiers perform two incentivized tasks—choosing how to classify and learning how to classify—and demonstrates that alignment adjustments can reduce incentives to learn.&lt;/li&gt;&lt;li&gt;Provides two focal applications (including medical diagnosis) to illustrate how intuitively appealing alignment methods can produce predictable misalignment between human and machine objectives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Autor', 'Andrew Caplin', 'Daniel Martin', 'Philip Marx']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'incentive design', 'human-AI interaction', 'safety-economics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07699</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title><link>https://arxiv.org/abs/2511.07689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates six reference-free factuality metrics, originally for short summaries, in long-document summarization across three domains (science fiction, legal, scientific).&lt;/li&gt;&lt;li&gt;Applies seven meaning-preserving perturbations (e.g., paraphrase, synonym replacement, compression, logical negation equivalents) to test metric stability and sensitivity.&lt;/li&gt;&lt;li&gt;Finds that existing short-form metrics give inconsistent scores for semantically equivalent summaries and degrade for dense claims or long contexts; expanding retrieval helps sometimes but no metric is consistently reliable.&lt;/li&gt;&lt;li&gt;Provides recommendations for improving factuality evaluation (multi-span reasoning, context-aware calibration, training on meaning-preserving variations) and releases code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zain Muhammad Mujahid', 'Dustin Wright', 'Isabelle Augenstein']&lt;/li&gt;&lt;li&gt;Tags: ['factuality evaluation', 'robustness', 'long-document summarization', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07689</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning</title><link>https://arxiv.org/abs/2511.07483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a confidence-aware reward model (C2RM) for RL-based training to improve STEM reasoning in LLMs by penalizing low-confidence correct answers as well as incorrect answers.&lt;/li&gt;&lt;li&gt;Aims to reduce spurious or low-quality reasoning chains that sometimes accidentally produce correct answers and get rewarded under rule-based judges.&lt;/li&gt;&lt;li&gt;Validates approach via static evaluations, Best-of-N inference, and PPO-based RL training, showing improvements over several open-source reward models.&lt;/li&gt;&lt;li&gt;Targets enabling more reliable RL training for smaller-scale models and releases code and model weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianxi He', 'Qingyu Ren', 'Shanzhe Lei', 'Xuhong Wang', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward_modeling', 'robust_reasoning', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07483</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms</title><link>https://arxiv.org/abs/2511.08570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptKAN: an algorithm for automatic, data-driven updates to domain discretization (domain grid) in Kolmogorov-Arnold Networks using layer output histograms.&lt;/li&gt;&lt;li&gt;Shows AdaptKAN matches or exceeds prior KANs and MLPs on tasks including symbolic equation learning (Feynman), image classification from frozen features, control Lyapunov function learning, and OOD detection on OpenOOD v1.5.&lt;/li&gt;&lt;li&gt;Claims the histogram-based update can also be applied to detect out-of-distribution inputs, offering a safety/robustness benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamison Moody', 'James Usevitch']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'robustness', 'neural-architecture', 'safety-evaluation', 'anomaly-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08570</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Bias: Data Poisoning Attacks on Fairness</title><link>https://arxiv.org/abs/2511.08331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Theoretically demonstrates that a simple adversarial data poisoning strategy can induce maximally unfair behavior in naive Bayes classifiers.&lt;/li&gt;&lt;li&gt;Proposes a method that injects a small fraction of crafted training points to bias the decision boundary against a protected group while largely preserving overall performance.&lt;/li&gt;&lt;li&gt;Empirically evaluates across multiple benchmark datasets and models, showing stronger degradation of fairness metrics than prior methods with comparable or slightly reduced accuracy.&lt;/li&gt;&lt;li&gt;Shows the attack is effective across a range of model types, indicating robustness and broad applicability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eunice Chan', 'Hanghang Tong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'fairness attacks', 'adversarial machine learning', 'algorithmic fairness', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08331</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories</title><link>https://arxiv.org/abs/2511.08136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMIL, an offline imitation learning method that learns a parameterized cost predicting risky state-action pairs from non-preferred (undesirable/risky) trajectories using Multiple Instance Learning.&lt;/li&gt;&lt;li&gt;Uses the learned cost to constrain or guide policy learning so the resulting policy avoids risky behaviors while maintaining reward performance.&lt;/li&gt;&lt;li&gt;Empirical results claim improved safety (cost constraint satisfaction) without degrading task reward, outperforming several baselines in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Returaj Burnwal', 'Nirav Pravinbhai Bhatt', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'imitation learning', 'offline learning', 'safety constraints', 'multiple instance learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08136</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Balance Equation-based Distributionally Robust Offline Imitation Learning</title><link>https://arxiv.org/abs/2511.07942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Balance Equation-based distributionally robust offline imitation learning (DRO-IL) framework to learn policies from expert demonstrations that remain robust to shifts in transition dynamics.&lt;/li&gt;&lt;li&gt;Formulates a DRO objective over an uncertainty set of transition models and shows it can be reformulated purely in terms of the nominal data distribution, enabling tractable offline optimization.&lt;/li&gt;&lt;li&gt;Targets robustness against modeling inaccuracies, real-world parameter variations, and adversarial perturbations in dynamics without requiring additional environment interaction.&lt;/li&gt;&lt;li&gt;Empirical results on continuous-control benchmarks show improved robustness and generalization under perturbed or shifted environments compared to state-of-the-art offline IL baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishabh Agrawal', 'Yusuf Alvi', 'Rahul Jain', 'Ashutosh Nayyar']&lt;/li&gt;&lt;li&gt;Tags: ['offline imitation learning', 'distributional robustness', 'robustness to dynamics shifts', 'adversarial perturbations', 'robotics/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07942</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction</title><link>https://arxiv.org/abs/2511.07899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a conformal prediction (CP) framework to bound uncertainty of learned Hamilton–Jacobi (HJ) value functions and provide probabilistic safety guarantees for learned safe policies.&lt;/li&gt;&lt;li&gt;Uses CP to calibrate switching between an unsafe nominal controller and a learned HJ-based safe policy, and derives safety guarantees under this switched policy.&lt;/li&gt;&lt;li&gt;Investigates ensembles of independently trained HJ value functions as safety filters and compares ensemble-based filtering to individual value functions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ihab Tabbara', 'Yuxuan Yang', 'Hussein Sibai']&lt;/li&gt;&lt;li&gt;Tags: ['control-systems-safety', 'conformal-prediction', 'HJ-reachability', 'ensemble-safety-filters', 'safe-reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07899</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DP-AdamW: Investigating Decoupled Weight Decay and Bias Correction in Private Deep Learning</title><link>https://arxiv.org/abs/2511.07843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-AdamW, a differentially private variant of AdamW, and DP-AdamW-BC which adds DP bias correction for the second moment estimator; provides theoretical privacy and convergence results.&lt;/li&gt;&lt;li&gt;Empirically evaluates performance across privacy budgets (ε = 1, 3, 7), reporting DP-AdamW outperforms DP-SGD, DP-Adam, and DP-AdamBC (≈+15% on text, up to +5% on images, ≈+1% on graph node classification).&lt;/li&gt;&lt;li&gt;Finds that incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, contrary to gains observed for DP-AdamBC; includes analysis and benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jay Chooi', 'Kevin Cong', 'Russell Li', 'Lillian Sun']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'optimizers', 'privacy-preserving_ml', 'theoretical_analysis', 'empirical_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07843</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion Guided Adversarial State Perturbations in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.07701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings of lp-norm constrained attacks in vision-based RL: they often cannot meaningfully change scene semantics even with large budgets.&lt;/li&gt;&lt;li&gt;Proposes SHIFT, a policy-agnostic, diffusion-based state perturbation attack that generates semantically different yet realistic and history-aligned perturbed states.&lt;/li&gt;&lt;li&gt;Demonstrates that SHIFT outperforms existing attacks and can break state-of-the-art RL defenses while remaining perceptually stealthy.&lt;/li&gt;&lt;li&gt;Highlights the need for new robustness methods for RL agents against semantics-aware adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaolin Sun', 'Feidi Liu', 'Zhengming Ding', 'ZiZhan Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'diffusion models', 'robustness', 'state perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07701</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models</title><link>https://arxiv.org/abs/2511.07694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free, probability-only method that approximates predictive entropy using top-K token probabilities from model responses to estimate uncertainty.&lt;/li&gt;&lt;li&gt;Introduces an adaptive mechanism to choose K and filter low-confidence probabilities for more flexible uncertainty estimation.&lt;/li&gt;&lt;li&gt;Evaluated on three free-form question-answering datasets across multiple LLMs; claims better performance than more expensive multi-sample baselines for detecting hallucinations and improving trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manh Nguyen', 'Sunil Gupta', 'Hung Le']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'hallucination detection', 'LLM safety', 'predictive entropy', 'calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07694</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private</title><link>https://arxiv.org/abs/2511.07637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy leakage in retrieval-augmented generation (RAG) for practical multi-query settings rather than single-query scenarios.&lt;/li&gt;&lt;li&gt;Proposes two DP algorithms: MURAG (individual privacy filter to bound per-document accumulated privacy loss) and MURAG-ADA (adds privately released, query-specific thresholds to improve utility).&lt;/li&gt;&lt;li&gt;Empirically shows methods scale to hundreds of queries within practical DP budgets (ε ≈ 10) while maintaining useful performance across multiple LLMs and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruihan Wu', 'Erchi Wang', 'Zhiyuan Zhang', 'Yu-Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'retrieval-augmented generation', 'privacy-preserving ML', 'LLM security', 'multi-query privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07637</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title><link>https://arxiv.org/abs/2511.07585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of output drift across five LLMs (7B–120B) on three regulated financial tasks, showing smaller models can be more deterministically consistent than very large models under greedy decoding.&lt;/li&gt;&lt;li&gt;Introduces a finance-calibrated deterministic test harness (T=0.0, fixed seeds, SEC 10-K structure-aware retrieval ordering) and task-specific invariant checks for RAG, JSON, and SQL outputs with materiality thresholds (±5%).&lt;/li&gt;&lt;li&gt;Proposes a three-tier model classification for deployment risk, an audit-ready attestation system with dual-provider validation, and maps the framework to FSB/BIS/CFTC compliance requirements.&lt;/li&gt;&lt;li&gt;Findings: structured outputs (SQL) are robust even with sampling, RAG tasks exhibit substantial drift (25–75%), and cross-provider validation shows determinism can transfer between local and cloud deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffi Khatchadourian', 'Rolando Franco']&lt;/li&gt;&lt;li&gt;Tags: ['LLM output drift', 'determinism &amp; reproducibility', 'RAG robustness', 'AI auditability &amp; compliance', 'cross-provider validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07585</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Provably Efficient Sample Complexity for Robust CMDP</title><link>https://arxiv.org/abs/2511.07486</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses learning policies for robust constrained Markov decision processes (RCMDPs): maximize reward while ensuring safety (cumulative utility threshold) under worst-case dynamics within an uncertainty set.&lt;/li&gt;&lt;li&gt;Shows Markovian policies can be suboptimal under RCMDP, introduces an augmented state that tracks remaining utility budget to restore optimality.&lt;/li&gt;&lt;li&gt;Proposes Robust constrained Value Iteration (RCVI) with sample complexity Õ(|S||A|H^5/ε^2) and guarantees at most ε constraint violation using a generative model.&lt;/li&gt;&lt;li&gt;Claims to be the first paper providing sample complexity guarantees for RCMDPs and includes empirical validation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sourav Ganguly', 'Arnob Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['robust-rl', 'safety', 'constrained-mdp', 'sample-complexity', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07486</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift</title><link>https://arxiv.org/abs/2511.07485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unifying theoretical framework that models biases as violations of conditional independence using information-theoretic measures.&lt;/li&gt;&lt;li&gt;Derives formal equivalence conditions connecting spurious correlations, subpopulation shift, class imbalance, and fairness violations; predicts r ≈ (1+α)/(1−α) mapping between spurious-correlation strength α and imbalance ratio r for equivalent worst-group accuracy degradation under feature-overlap assumptions.&lt;/li&gt;&lt;li&gt;Empirically validates the theoretical equivalences across six datasets and three architectures, finding predicted worst-group accuracies match within ~3%.&lt;/li&gt;&lt;li&gt;Implication: enables principled transfer of debiasing and robustness interventions across fairness, distribution-shift, and class-imbalance problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sushant Mehta']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'robustness', 'distribution_shift', 'theoretical_framework', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07485</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title><link>https://arxiv.org/abs/2511.07482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-critical circuits during inference.&lt;/li&gt;&lt;li&gt;Targets alignment degradation caused by dynamic pruning by identifying and retaining input-dependent safety-relevant circuitry.&lt;/li&gt;&lt;li&gt;Builds on Probe Pruning and evaluates AAPP on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT.&lt;/li&gt;&lt;li&gt;Reports improved safety behavior (≈50% increase in refusal rates) at matched compute, enabling more efficient yet safety-preserving deployment of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dev Patel', 'Gabrielle Gervacio', 'Diekola Raimi', 'Kevin Zhu', 'Ryan Lagasse', 'Gabriel Grand', 'Ashwinee Panda', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model pruning', 'safety-preserving inference', 'dynamic pruning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07482</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data</title><link>https://arxiv.org/abs/2511.07481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates embedding reconstruction attacks on LLM embeddings derived from genomic (DNA) sequences, comparing pretrained versus fully fine-tuned models.&lt;/li&gt;&lt;li&gt;Applies and extends prior reconstruction-attack pipelines (Pan et al.) with DNA-specific tokenization and measures position-specific and nucleotide-type vulnerabilities.&lt;/li&gt;&lt;li&gt;Finds that fine-tuning for the task increases resistance to reconstruction attacks across several architectures (XLNet, GPT-2, BERT), suggesting fine-tuning can alter privacy risk profiles.&lt;/li&gt;&lt;li&gt;Provides empirical analysis across embedding types and dimensions, highlighting the need for protective mechanisms when LLMs process sensitive genomic data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reem Al-Saidi', 'Erman Ayday', 'Ziad Kobti']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'embedding reconstruction', 'LLM fine-tuning', 'genomic privacy', 'tokenization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07481</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment</title><link>https://arxiv.org/abs/2511.06852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes decomposing LLM safety representations into a Harm Detection Direction and a Refusal Execution Direction rather than a single alignment vector.&lt;/li&gt;&lt;li&gt;Introduces Differentiated Bi-Directional Intervention (DBDI), a white-box method that nullifies the refusal execution direction via projection and suppresses harm detection via steering at critical layers.&lt;/li&gt;&lt;li&gt;Reports strong jailbreak performance (up to 97.88% attack success on models like Llama-2) and provides mechanistic insights into alignment failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Zhang', 'Peijie Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'alignment evasion', 'activation-space interventions', 'white-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06852</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title><link>https://arxiv.org/abs/2511.05784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses unlearning sensitive or harmful content from deployed LLMs without access to retain data or fine-tuning by using in-context instruction-based interventions.&lt;/li&gt;&lt;li&gt;Proposes DRAGON: a pipeline with a lightweight detection module to flag forget-worthy prompts and a chain-of-thought (CoT) guard model that enforces in-context unlearning before inference.&lt;/li&gt;&lt;li&gt;Introduces new metrics for unlearning performance including continual unlearning, and evaluates DRAGON across multiple unlearning tasks showing effectiveness and scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxuan Wang', 'Chris Yuhao Liu', 'Quan Liu', 'Jinglong Pang', 'Wei Wei', 'Yujia Bao', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'model safety', 'instruction-based defenses', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05784</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Selective Diabetic Retinopathy Screening with Accuracy-Weighted Deep Ensembles and Entropy-Guided Abstention</title><link>https://arxiv.org/abs/2511.05529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an accuracy-weighted deep ensemble of seven CNN architectures for diabetic retinopathy detection on fundus images.&lt;/li&gt;&lt;li&gt;Introduces a probability-weighted entropy uncertainty metric to identify low-confidence predictions and abstain/flag them for review.&lt;/li&gt;&lt;li&gt;Reports high unfiltered accuracy (93.70%) and shows uncertainty-based filtering can raise maximum accuracy to 99.44%, enabling a tunable accuracy–coverage trade-off for safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jophy Lin']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'abstention/selective prediction', 'ensemble learning', 'medical AI safety', 'confidence calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05529</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title><link>https://arxiv.org/abs/2509.26601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MENLO, a framework and dataset for evaluating native-like response quality across 47 language varieties using audience-design inspired preference annotations.&lt;/li&gt;&lt;li&gt;Provides 6,423 human-annotated prompt-response preference pairs across four quality dimensions with high inter-annotator agreement, and shows zero-shot LLM judges benefit from pairwise evaluation but lag behind humans.&lt;/li&gt;&lt;li&gt;Demonstrates improvements via fine-tuning (RL, reward shaping, multi-task learning) and that RL-trained judges can act as generative reward models to improve multilingual proficiency, though gaps with human judgment remain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Whitehouse', 'Sebastian Ruder', 'Tony Lin', 'Oksana Kurylo', 'Haruka Takagi', 'Janice Lam', 'Nicol\\`o Busetto', 'Denise Diaz', "Francisco Guzm\\'an"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multilingual evaluation', 'reward modeling', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26601</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning</title><link>https://arxiv.org/abs/2509.20166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CyberSOCEval, an open-source benchmark suite (part of CyberSecEval 4) targeting LLM performance on Malware Analysis and Threat Intelligence Reasoning.&lt;/li&gt;&lt;li&gt;Finds larger, more modern LLMs generally perform better, but reasoning-specialized models using test-time scaling do not gain the same benefits seen in coding/math tasks.&lt;/li&gt;&lt;li&gt;Shows current models are far from saturating the benchmarks, indicating substantial room for improvement in LLM-based cyber defense capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lauren Deason', 'Adam Bali', 'Ciprian Bejean', 'Diana Bolocan', 'James Crnkovich', 'Ioana Croitoru', 'Krishna Durai', 'Chase Midler', 'Calin Miron', 'David Molnar', 'Brad Moon', 'Bruno Ostarcevic', 'Alberto Peltea', 'Matt Rosenberg', 'Catalin Sandu', 'Arthur Saputkin', 'Sagar Shah', 'Daniel Stan', 'Ernest Szocs', 'Shengye Wan', 'Spencer Whitman', 'Sven Krasser', 'Joshua Saxe']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'Malware Analysis', 'Threat Intelligence', 'LLM Evaluation', 'Cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20166</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</title><link>https://arxiv.org/abs/2509.05831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates HTML-based prompt injection attacks using non-visible elements (e.g., &lt;meta&gt;, aria-label, alt) to manipulate LLM-driven web summarization without changing visible content.&lt;/li&gt;&lt;li&gt;Creates a reproducible dataset of 280 static web pages (clean vs. injected), extracts raw HTML and rendered text via browser automation, and evaluates two open-source LLMs (Llama 4 Scout, Gemma 9B IT).&lt;/li&gt;&lt;li&gt;Quantifies attack success with lexical and semantic metrics and manual annotation, finding substantial vulnerability (29% success on Llama 4 Scout, 15% on Gemma 9B IT) and provides a benchmark and pipeline for assessing mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishaan Verma', 'Arsheya Yadav']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attacks', 'web security', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05831</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedSEA-LLaMA, a federated split-learning framework for LLaMA2 that injects Gaussian noise into forward-pass hidden states to secure end-to-end vector transmission.&lt;/li&gt;&lt;li&gt;Reduces communication overhead via attention-mask compression and KV-cache collaboration, claiming up to 8x speedups in training and inference while retaining centralized LLaMA2 performance.&lt;/li&gt;&lt;li&gt;Supports adaptive partition points (input/output blocks) to tune client-server computation tradeoffs and includes analyses of privacy attacks and partition-point effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated-split-learning', 'privacy-preserving-ml', 'communication-efficiency', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</title><link>https://arxiv.org/abs/2505.13709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework that jointly adapts a learned world model and the policy via a unified maximin objective to improve robustness in offline model-based RL.&lt;/li&gt;&lt;li&gt;Uses Stackelberg learning dynamics to solve the maximin optimization and provides theoretical analysis plus efficient implementations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on noisy D4RL MuJoCo tasks and stochastic Tokamak Control benchmarks, addressing sensitivity to small adversarial/environmental perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Chen', 'Le Xu', 'Aravind Venugopal', 'Jeff Schneider']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'offline model-based RL', 'world model adaptation', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13709</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title><link>https://arxiv.org/abs/2505.11770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two methods—counterfactual simulation and value probing—that leverage identified internal causal variables to predict correctness of language model outputs.&lt;/li&gt;&lt;li&gt;Evaluates these methods across diverse language-modeling tasks (symbol manipulation, knowledge retrieval, instruction following) and reports high AUC-ROC in-distribution.&lt;/li&gt;&lt;li&gt;Shows causal-mechanism-based predictors outperform causal-agnostic features in out-of-distribution settings, highlighting interpretability tools as practical predictors of model failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Huang', 'Junyi Tao', 'Thomas Icard', 'Diyi Yang', 'Christopher Potts']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'causal analysis', 'OOD robustness', 'model reliability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11770</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</title><link>https://arxiv.org/abs/2504.01903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAR-1, a high-quality 1K safety dataset targeting large reasoning models (LRMs) to improve alignment.&lt;/li&gt;&lt;li&gt;Constructed via integrating open-source safety datasets, policy-grounded deliberative reasoning sample generation, and GPT-4o-based safety scoring/filtering.&lt;/li&gt;&lt;li&gt;Fine-tuning LRMs with STAR-1 yields ~40% average improvement on safety benchmarks with only ~1.1% average drop in reasoning performance; includes ablation studies.&lt;/li&gt;&lt;li&gt;Evaluated across both LRMs and traditional LLMs to validate design principles and efficacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Dataset / Benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijun Wang', 'Haoqin Tu', 'Yuhan Wang', 'Juncheng Wu', 'Yanqing Liu', 'Jieru Mei', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Cihang Xie']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-safety', 'safety-dataset', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01903</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Integrating Artificial Intelligence into Operating Systems: A Survey on Techniques, Applications, and Future Directions</title><link>https://arxiv.org/abs/2407.14567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of techniques and architectures for integrating ML/LLMs/agents across the OS stack, and reciprocal OS support for AI workloads.&lt;/li&gt;&lt;li&gt;Identifies challenges including complexity, overhead, model drift, limited explainability, and specifically privacy and safety risks of embedding AI in OS components.&lt;/li&gt;&lt;li&gt;Recommends mitigations such as modular AI-ready kernel interfaces, guardrails/hybrid rules-plus-AI decisions, unified toolchains/benchmarks, and verifiable in-kernel inference; proposes a three-stage roadmap to production.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Zhang', 'Xinkui Zhao', 'Ziying Li', 'Guanjie Cheng', 'Jianwei Yin', 'Lufei Zhang', 'Zuoning Chen']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'privacy', 'OS security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.14567</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships</title><link>https://arxiv.org/abs/2405.18770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multimodal Adversarial Training (MAT) that injects adversarial perturbations into both image and text modalities to defend vision-language models against multimodal attacks.&lt;/li&gt;&lt;li&gt;Identifies limitations of standard 1:1 image-text training pairs and studies leveraging one-to-many (1:N and N:1) image-text relationships via augmentation to improve robustness.&lt;/li&gt;&lt;li&gt;Finds effective augmented pairs must be well-aligned, diverse, and avoid distribution shift; shows MAT with suitable augmentation outperforms unimodal defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Futa Waseda', 'Antonio Tejero-de-Pablos', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multimodal attacks', 'vision-language models', 'adversarial training', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.18770</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Theoretical Analysis of Detecting Large Model-Generated Time Series</title><link>https://arxiv.org/abs/2511.07104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses detection of synthetic time series produced by Time-Series Large Models (TSLMs), noting that token-based detectors for text do not transfer due to modality differences (lower information density, smoother distributions).&lt;/li&gt;&lt;li&gt;Proposes the 'contraction hypothesis': model-generated time series display progressively decreasing uncertainty under recursive forecasting; provides theoretical proof under model/time-series assumptions and empirical validation across datasets.&lt;/li&gt;&lt;li&gt;Introduces the Uncertainty Contraction Estimator (UCE), a white-box detector that aggregates uncertainty metrics over successive prefixes, and demonstrates superior performance to baselines on 32 datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junji Hou', 'Junzhou Zhao', 'Shuo Zhang', 'Pinghui Wang']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic data detection', 'time-series models', 'model-generated content detection', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07104</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</title><link>https://arxiv.org/abs/2511.06626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Report Fine-Tuning (SRFT), a supervised fine-tuning method that trains LLMs to admit factual mistakes and (crucially) to confess hidden misaligned objectives when interrogated.&lt;/li&gt;&lt;li&gt;Shows that admission behavior in simple QA tasks generalizes OOD to adversarial agentic 'stealth' tasks where models pursue hidden objectives while avoiding detection.&lt;/li&gt;&lt;li&gt;Empirical results: interrogation on SRFT models detects hidden objectives with near-ceiling performance (F1 = 0.98) versus baseline (F1 = 0), and elicits 28–100% of hidden-objective content compared to 0% for baseline/prefilled-attacks.&lt;/li&gt;&lt;li&gt;Frames SRFT as a technique to increase honesty propensity and to aid red-teaming / incriminating misaligned AI systems under adversarial pressure not to disclose.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chloe Li', 'Mary Phuong', 'Daniel Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Alignment', 'Red teaming', 'Honesty / truthfulness', 'Adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06626</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads</title><link>https://arxiv.org/abs/2511.06209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces lightweight transformer-based uncertainty quantification heads (UHeads) that read frozen LLM internal states to estimate uncertainty of individual reasoning steps.&lt;/li&gt;&lt;li&gt;Training labels are generated automatically (via a larger LLM or self-supervision), avoiding expensive human annotation and heavy PRMs.&lt;/li&gt;&lt;li&gt;UHeads (&lt;10M parameters) match or outperform much larger Process Reward Models across domains (mathematics, planning, general QA), showing LLM internals encode useful uncertainty for step-level verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingwei Ni', 'Ekaterina Fadeeva', 'Tianyi Wu', 'Mubashara Akhtar', 'Jiaheng Zhang', 'Elliott Ash', 'Markus Leippold', 'Timothy Baldwin', 'See-Kiong Ng', 'Artem Shelmanov', 'Mrinmaya Sachan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'LLM introspection', 'reasoning verification', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06209</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences</title><link>https://arxiv.org/abs/2511.02109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Deep Value Benchmark (DVB), an evaluation framework that tests whether LLMs learn underlying human values versus surface-level preferences via controlled confounding in training and broken correlations at test time.&lt;/li&gt;&lt;li&gt;Defines Deep Value Generalization Rate (DVGR) as the probability of choosing based on deep values; reports an average DVGR of 0.30 across 9 models (worse than chance) and finds larger models slightly lower DVGR than smaller ones.&lt;/li&gt;&lt;li&gt;Releases a dataset validated by three human experiments and provides an interpretable metric for a core alignment capability (generalizing values rather than superficial features).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Ashkinaze', 'Hua Shen', 'Sai Avula', 'Eric Gilbert', 'Ceren Budak']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'LLM safety', 'benchmarking', 'value learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02109</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Chain-of-Thought Hijacking</title><link>https://arxiv.org/abs/2510.26418</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Chain-of-Thought Hijacking', a jailbreak that pads harmful prompts with long benign CoT to bypass safety/refusal in large reasoning models.&lt;/li&gt;&lt;li&gt;Reports high attack success rates (94–100% on several major models) across benchmarked harmful requests.&lt;/li&gt;&lt;li&gt;Provides mechanistic analysis showing mid layers encode safety-check strength and late layers encode verification outcome; benign CoT shifts attention away from harmful tokens.&lt;/li&gt;&lt;li&gt;Performs targeted attention-head ablations to causally link a safety subnetwork to refusal behavior and releases prompts/outputs for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianli Zhao', 'Tingchen Fu', 'Rylan Schaeffer', 'Mrinank Sharma', 'Fazl Barez']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'chain-of-thought', 'LLM safety', 'adversarial prompting', 'mechanistic interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26418</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Survey of AI Scientists</title><link>https://arxiv.org/abs/2510.23045</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a unified six-stage framework for autonomous scientific workflows: Literature Review, Idea Generation, Experimental Preparation, Experimental Execution, Scientific Writing, and Paper Generation.&lt;/li&gt;&lt;li&gt;Surveys the field's evolution from early foundational modules (2022–2023) to integrated closed-loop systems (2024) and current frontiers (2025+) emphasizing scalability and human–AI collaboration.&lt;/li&gt;&lt;li&gt;Highlights challenges around robustness, governance, and trustworthiness and offers a roadmap for addressing these safety-related concerns in AI scientist systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guiyao Tie', 'Pan Zhou', 'Lichao Sun']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Robustness', 'Governance', 'Autonomous agents', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23045</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms</title><link>https://arxiv.org/abs/2511.08570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptKAN: an algorithm for automatic, data-driven updates to domain discretization (domain grid) in Kolmogorov-Arnold Networks using layer output histograms.&lt;/li&gt;&lt;li&gt;Shows AdaptKAN matches or exceeds prior KANs and MLPs on tasks including symbolic equation learning (Feynman), image classification from frozen features, control Lyapunov function learning, and OOD detection on OpenOOD v1.5.&lt;/li&gt;&lt;li&gt;Claims the histogram-based update can also be applied to detect out-of-distribution inputs, offering a safety/robustness benefit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamison Moody', 'James Usevitch']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'robustness', 'neural-architecture', 'safety-evaluation', 'anomaly-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08570</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models</title><link>https://arxiv.org/abs/2511.08565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an MFQ-based benchmark measuring 'moral susceptibility' (variability across personas) and 'moral robustness' (variability within personas) for LLMs under persona role-play.&lt;/li&gt;&lt;li&gt;Finds model family explains most variance in robustness (Claude highest, then Gemini and GPT-4); model size has no systematic effect on robustness.&lt;/li&gt;&lt;li&gt;Finds susceptibility shows a within-family size effect (larger variants more susceptible) and that robustness and susceptibility are positively correlated, stronger at the family level.&lt;/li&gt;&lt;li&gt;Provides moral foundation profiles for models without role-play and averaged persona profiles, offering a systematic view of how persona conditioning shifts moral behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davi Bastos Costa', 'Felippe Alves', 'Renato Vicente']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'persona-role-play', 'jailbreaking/prompt-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08565</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interaction Dynamics as a Reward Signal for LLMs</title><link>https://arxiv.org/abs/2511.08394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE, a reward signal derived from geometric properties of a conversation's embedding trajectory ('conversational geometry').&lt;/li&gt;&lt;li&gt;Shows a model trained only on these interaction-dynamics signals attains 68.20% pairwise accuracy versus 70.04% for a strong LLM baseline using full transcripts.&lt;/li&gt;&lt;li&gt;Demonstrates a hybrid model combining dynamics and textual analysis achieves 80.17%, indicating complementary value and improved alignment/evaluation performance.&lt;/li&gt;&lt;li&gt;Claims privacy-preserving properties and positions the method as both an alignment tool and a diagnostic for interaction patterns that drive successful collaboration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sian Gooding', 'Edward Grefenstette']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'evaluation/diagnostics', 'interaction dynamics', 'privacy-preserving signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08394</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories</title><link>https://arxiv.org/abs/2511.08136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMIL, an offline imitation learning method that learns a parameterized cost predicting risky state-action pairs from non-preferred (undesirable/risky) trajectories using Multiple Instance Learning.&lt;/li&gt;&lt;li&gt;Uses the learned cost to constrain or guide policy learning so the resulting policy avoids risky behaviors while maintaining reward performance.&lt;/li&gt;&lt;li&gt;Empirical results claim improved safety (cost constraint satisfaction) without degrading task reward, outperforming several baselines in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Returaj Burnwal', 'Nirav Pravinbhai Bhatt', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'imitation learning', 'offline learning', 'safety constraints', 'multiple instance learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08136</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Constrained and Robust Policy Synthesis with Satisfiability-Modulo-Probabilistic-Model-Checking</title><link>https://arxiv.org/abs/2511.08078</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a method to compute reward-optimal policies for finite MDPs that are robust to perturbations and satisfy arbitrary structural constraints.&lt;/li&gt;&lt;li&gt;Allows expressing constraints in a first-order theory over sets of MDPs and tightly integrates satisfiability solvers (SMT) with probabilistic model checking for analysis.&lt;/li&gt;&lt;li&gt;Demonstrates feasibility and competitiveness on a few hundred benchmarks across various problem fragments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linus Heck', "Filip Mac\\'ak", 'Milan \\v{C}e\\v{s}ka', 'Sebastian Junges']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'policy synthesis', 'probabilistic model checking', 'SMT/satisfiability', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08078</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-modal Deepfake Detection and Localization with FPN-Transformer</title><link>https://arxiv.org/abs/2511.08031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chende Zheng', 'Ruiqi Suo', 'Zhoulin Ji', 'Jingyi Deng', 'Fangbin Yi', 'Chenhao Lin', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08031</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles</title><link>https://arxiv.org/abs/2511.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a decentralized, purely reaction-based swarm intelligence method tailored for elongated, articulated Heavy Articulated Vehicles (HAVs) to avoid jackknifing and mutual collisions.&lt;/li&gt;&lt;li&gt;Emphasizes jackknife avoidance as primary objective and extends to mutual collision avoidance in multi-agent interactions.&lt;/li&gt;&lt;li&gt;Validated through extensive simulations reporting high jackknife-avoidance rates (e.g., 99.8% for single HAVs) and strong mutual collision avoidance (99.7% no mutual collisions).&lt;/li&gt;&lt;li&gt;Focuses on control/behavioral algorithm design and simulation evaluation rather than adversarial threats, robustness to attacks, or security vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrian Sch\\"onnagel', "Michael Dub\\'e", 'Christoph Steup', 'Felix Keppler', 'Sanaz Mostaghim']&lt;/li&gt;&lt;li&gt;Tags: ['robotics', 'swarm intelligence', 'collision avoidance', 'safety', 'control systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08016</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2511.08015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvRoad: a method to generate naturalistic, road-style adversarial posters that cause visual 3D detectors in autonomous driving to hallucinate non-existent objects.&lt;/li&gt;&lt;li&gt;Uses a two-stage pipeline — Road-Style Adversary Generation and Scenario-Associated Adaptation — to maximize attack effectiveness while preserving stealthy, road-like appearance.&lt;/li&gt;&lt;li&gt;Demonstrates generalization across detectors, scenes, and spoofing locations and validates practicality with physical-world attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Lijun He', 'Yixing Yong', 'Haixia Bi', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical-world attack', 'autonomous driving', '3D object detection', 'adversarial patch']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08015</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reliable and Private Utility Signaling for Data Markets</title><link>https://arxiv.org/abs/2511.07975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes a 'utility signaling' mechanism for data marketplaces that aims to balance privacy and signal reliability to guide trading decisions.&lt;/li&gt;&lt;li&gt;Constructs a protocol using maliciously secure multi-party computation (MPC) and an MPC-based hash verification scheme to ensure privacy, input reliability, and robustness of signals.&lt;/li&gt;&lt;li&gt;Develops and optimizes an MPC-based KNN-Shapley method for fair valuation in multi-seller scenarios.&lt;/li&gt;&lt;li&gt;Empirically evaluates the proposed constructions, demonstrating practicality and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Peng', 'Jiayao Zhang', 'Yihang Wu', 'Weiran Liu', 'Jinfei Liu', 'Zheng Yan', 'Kui Ren', 'Lei Zhang', 'Lin Qu']&lt;/li&gt;&lt;li&gt;Tags: ['multi-party computation (MPC)', 'privacy-preserving protocols', 'data marketplace / data valuation', 'robustness / input integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07975</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Balance Equation-based Distributionally Robust Offline Imitation Learning</title><link>https://arxiv.org/abs/2511.07942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Balance Equation-based distributionally robust offline imitation learning (DRO-IL) framework to learn policies from expert demonstrations that remain robust to shifts in transition dynamics.&lt;/li&gt;&lt;li&gt;Formulates a DRO objective over an uncertainty set of transition models and shows it can be reformulated purely in terms of the nominal data distribution, enabling tractable offline optimization.&lt;/li&gt;&lt;li&gt;Targets robustness against modeling inaccuracies, real-world parameter variations, and adversarial perturbations in dynamics without requiring additional environment interaction.&lt;/li&gt;&lt;li&gt;Empirical results on continuous-control benchmarks show improved robustness and generalization under perturbed or shifted environments compared to state-of-the-art offline IL baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishabh Agrawal', 'Yusuf Alvi', 'Rahul Jain', 'Ashutosh Nayyar']&lt;/li&gt;&lt;li&gt;Tags: ['offline imitation learning', 'distributional robustness', 'robustness to dynamics shifts', 'adversarial perturbations', 'robotics/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07942</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</title><link>https://arxiv.org/abs/2511.07931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SpeechJudge-Data: a large-scale human preference corpus (99K speech pairs) annotated for intelligibility and naturalness across diverse TTS systems, styles, and languages.&lt;/li&gt;&lt;li&gt;Proposes SpeechJudge-Eval, a benchmark showing existing metrics and AudioLLMs (best at &lt;70% agreement) perform below human-level judgment for speech naturalness.&lt;/li&gt;&lt;li&gt;Develops SpeechJudge-GRM (generative reward model based on Qwen2.5-Omni-7B) trained via SFT with Chain-of-Thought rationales and RL (GRPO), achieving 77.2% accuracy (79.4% after inference-time scaling) and outperforming a Bradley-Terry reward model.&lt;/li&gt;&lt;li&gt;Shows SpeechJudge-GRM can be used as a reward function to align speech generation models with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xueyao Zhang', 'Chaoren Wang', 'Huan Liao', 'Ziniu Li', 'Yuancheng Wang', 'Li Wang', 'Dongya Jia', 'Yuanzhe Chen', 'Xiulin Li', 'Zhuo Chen', 'Zhizheng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'human-preferences', 'evaluation-benchmark', 'speech-synthesis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07931</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction</title><link>https://arxiv.org/abs/2511.07899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a conformal prediction (CP) framework to bound uncertainty of learned Hamilton–Jacobi (HJ) value functions and provide probabilistic safety guarantees for learned safe policies.&lt;/li&gt;&lt;li&gt;Uses CP to calibrate switching between an unsafe nominal controller and a learned HJ-based safe policy, and derives safety guarantees under this switched policy.&lt;/li&gt;&lt;li&gt;Investigates ensembles of independently trained HJ value functions as safety filters and compares ensemble-based filtering to individual value functions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ihab Tabbara', 'Yuxuan Yang', 'Hussein Sibai']&lt;/li&gt;&lt;li&gt;Tags: ['control-systems-safety', 'conformal-prediction', 'HJ-reachability', 'ensemble-safety-filters', 'safe-reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07899</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation</title><link>https://arxiv.org/abs/2511.07876</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoopLLM, an attack framework that induces repetitive, low-entropy decoding loops to force LLMs to generate until output limits, increasing energy and latency costs.&lt;/li&gt;&lt;li&gt;Proposes two technical components: a repetition-inducing prompt optimization and a token-aligned ensemble optimization to improve cross-model transferability.&lt;/li&gt;&lt;li&gt;Provides extensive empirical evaluation on 12 open-source and 2 commercial LLMs, showing &gt;90% of maximum output length vs ~20% for baselines and ~40% transferability improvement to models like DeepSeek-V3 and Gemini 2.5 Flash.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingyu Li', 'Xiaolei Liu', 'Cheng Liu', 'Yixiao Xu', 'Kangyi Ding', 'Bangzhou Xin', 'Jia-Li Yin']&lt;/li&gt;&lt;li&gt;Tags: ['energy-latency attacks', 'adversarial prompting', 'LLM robustness', 'transferability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07876</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation</title><link>https://arxiv.org/abs/2511.07807</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM, a privacy-preserving inference framework that enables CNN inference over encrypted data using homomorphic encryption (CKKS) by replacing non-linear activations with homomorphically compatible polynomial approximations and restructuring the CNN.&lt;/li&gt;&lt;li&gt;Introduces an efficient activation approximation method (degree-4 polynomial / Softplus approximation) to reduce HE computation overhead while maintaining model accuracy.&lt;/li&gt;&lt;li&gt;Reports empirical results on CIFAR-10: 94.4% accuracy with 2.42 s per encrypted sample and 24,000 s per 10,000 encrypted samples under CKKS, demonstrating a trade-off between accuracy and computational cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeinab Elkhatib', 'Ali Sekmen', 'Kamrul Hasan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'homomorphic encryption', 'secure inference', 'model deployment', 'activation approximation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07807</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title><link>https://arxiv.org/abs/2511.07772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs can leak sensitive information through internal Chain-of-Thought (CoT) activations even when outputs are safe, violating contextual privacy.&lt;/li&gt;&lt;li&gt;Proposes SALT, a lightweight test-time intervention that injects targeted steering vectors into high-leakage hidden layers to reduce CoT privacy leakage.&lt;/li&gt;&lt;li&gt;Evaluates SALT across multiple LLMs and datasets, reporting substantial reductions in contextual privacy leakage (CPL) while maintaining task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shourya Batra', 'Pierce Tillman', 'Samarth Gaggar', 'Shashank Kesineni', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Vasu Sharma', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'chain-of-thought leakage', 'test-time defense', 'LLM internal activations', 'reasoning safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07772</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks</title><link>https://arxiv.org/abs/2511.07755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Filtered-ViT, a vision transformer that integrates SMART Vector Median Filtering (SMART-VMF) to selectively suppress patch-like corruptions while preserving semantic detail.&lt;/li&gt;&lt;li&gt;Designed to defend against multiple localized adversarial patches (multi-patch LaVAN attacks) and natural patch-like artifacts, achieving 79.8% clean accuracy and 46.3% robust accuracy on ImageNet under four simultaneous 1% patches.&lt;/li&gt;&lt;li&gt;Includes a real-world case study on radiographic medical imagery demonstrating mitigation of occlusions and scanner noise without degrading diagnostic content, claiming unified robustness to adversarial and natural patch disruptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aja Khanal', 'Ahmed Faid', 'Apurva Narayan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patch', 'robustness', 'vision-transformer', 'patch-defense', 'medical-imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07755</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion Guided Adversarial State Perturbations in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.07701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings of lp-norm constrained attacks in vision-based RL: they often cannot meaningfully change scene semantics even with large budgets.&lt;/li&gt;&lt;li&gt;Proposes SHIFT, a policy-agnostic, diffusion-based state perturbation attack that generates semantically different yet realistic and history-aligned perturbed states.&lt;/li&gt;&lt;li&gt;Demonstrates that SHIFT outperforms existing attacks and can break state-of-the-art RL defenses while remaining perceptually stealthy.&lt;/li&gt;&lt;li&gt;Highlights the need for new robustness methods for RL agents against semantics-aware adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaolin Sun', 'Feidi Liu', 'Zhengming Ding', 'ZiZhan Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'diffusion models', 'robustness', 'state perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07701</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences</title><link>https://arxiv.org/abs/2511.07691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAPO, a confidence-aware variant of Direct Preference Optimization that scales loss by a relative reward-based confidence for each preference pair.&lt;/li&gt;&lt;li&gt;Designed to improve multilingual preference tuning by reducing sensitivity to noisy or low-margin comparisons common in non-English data.&lt;/li&gt;&lt;li&gt;Reports empirical gains (≈16% improvement in reward accuracy) and wider preference gaps between preferred and dispreferred responses across languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Yufei Tao', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'multilingual robustness', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07691</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title><link>https://arxiv.org/abs/2511.07689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates six reference-free factuality metrics, originally for short summaries, in long-document summarization across three domains (science fiction, legal, scientific).&lt;/li&gt;&lt;li&gt;Applies seven meaning-preserving perturbations (e.g., paraphrase, synonym replacement, compression, logical negation equivalents) to test metric stability and sensitivity.&lt;/li&gt;&lt;li&gt;Finds that existing short-form metrics give inconsistent scores for semantically equivalent summaries and degrade for dense claims or long contexts; expanding retrieval helps sometimes but no metric is consistently reliable.&lt;/li&gt;&lt;li&gt;Provides recommendations for improving factuality evaluation (multi-span reasoning, context-aware calibration, training on meaning-preserving variations) and releases code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zain Muhammad Mujahid', 'Dustin Wright', 'Isabelle Augenstein']&lt;/li&gt;&lt;li&gt;Tags: ['factuality evaluation', 'robustness', 'long-document summarization', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07689</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Self-Improving Architecture for Dynamic Safety in Large Language Models</title><link>https://arxiv.org/abs/2511.07645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Self-Improving Safety Framework (SISF): a runtime architecture coupling an unaligned base LLM with an AI Adjudicator (GPT-4o) and a Policy Synthesis Module (GPT-4 Turbo) to detect breaches and autonomously generate safety policies.&lt;/li&gt;&lt;li&gt;Evaluated on the 520-prompt AdvBench adversarial set: detected 237 breaches, synthesized 234 new policies, and reduced Attack Success Rate from 100% to 45.58%; achieved 0.00% False Positive Rate on 520 benign prompts.&lt;/li&gt;&lt;li&gt;Demonstrates an architectural, automated, runtime approach to LLM safety—shifting from static pre-deployment measures to continuous self-adaptation—directly addressing adversarial threats and defensive policy generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tyler Slater']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Runtime monitoring', 'Policy synthesis', 'Adversarial robustness', 'Red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07645</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private</title><link>https://arxiv.org/abs/2511.07637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy leakage in retrieval-augmented generation (RAG) for practical multi-query settings rather than single-query scenarios.&lt;/li&gt;&lt;li&gt;Proposes two DP algorithms: MURAG (individual privacy filter to bound per-document accumulated privacy loss) and MURAG-ADA (adds privately released, query-specific thresholds to improve utility).&lt;/li&gt;&lt;li&gt;Empirically shows methods scale to hundreds of queries within practical DP budgets (ε ≈ 10) while maintaining useful performance across multiple LLMs and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruihan Wu', 'Erchi Wang', 'Zhiyuan Zhang', 'Yu-Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'retrieval-augmented generation', 'privacy-preserving ML', 'LLM security', 'multi-query privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07637</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging the Power of AI and Social Interactions to Restore Trust in Public Polls</title><link>https://arxiv.org/abs/2511.07593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an AI-based method using only social interaction graph structure (no content) to detect ineligible or dishonest participants in crowdsourced polling tasks.&lt;/li&gt;&lt;li&gt;Simulates various dishonest behaviors and propagation patterns within social networks and evaluates detection performance on real-world social network datasets.&lt;/li&gt;&lt;li&gt;Reports high detection accuracy (exceeding 90% in some configurations) despite variability introduced by different social/behavioral profiles and eligibility criteria.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amr Akmal Abouelmagd', 'Amr Hilal']&lt;/li&gt;&lt;li&gt;Tags: ['crowdsourced integrity', 'graph-based detection', 'social network analysis', 'adversarial participants', 'AI for security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07593</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title><link>https://arxiv.org/abs/2511.07585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of output drift across five LLMs (7B–120B) on three regulated financial tasks, showing smaller models can be more deterministically consistent than very large models under greedy decoding.&lt;/li&gt;&lt;li&gt;Introduces a finance-calibrated deterministic test harness (T=0.0, fixed seeds, SEC 10-K structure-aware retrieval ordering) and task-specific invariant checks for RAG, JSON, and SQL outputs with materiality thresholds (±5%).&lt;/li&gt;&lt;li&gt;Proposes a three-tier model classification for deployment risk, an audit-ready attestation system with dual-provider validation, and maps the framework to FSB/BIS/CFTC compliance requirements.&lt;/li&gt;&lt;li&gt;Findings: structured outputs (SQL) are robust even with sampling, RAG tasks exhibit substantial drift (25–75%), and cross-provider validation shows determinism can transfer between local and cloud deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffi Khatchadourian', 'Rolando Franco']&lt;/li&gt;&lt;li&gt;Tags: ['LLM output drift', 'determinism &amp; reproducibility', 'RAG robustness', 'AI auditability &amp; compliance', 'cross-provider validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07585</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models</title><link>https://arxiv.org/abs/2511.07505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedRW, a privacy-preserving federated learning framework that performs soft deduplication by reweighting samples rather than deleting them to mitigate duplication-related degradation and privacy risks in LLM training.&lt;/li&gt;&lt;li&gt;Implements a secure, frequency-aware reweighting protocol using secure multi-party computation and a parallel orchestration strategy to ensure efficiency and scalability during federated preprocessing and training.&lt;/li&gt;&lt;li&gt;Reports empirical gains (large preprocessing speedups and ~11.42% perplexity improvement) and claims enhanced security/privacy guarantees compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pukang Ye', 'Junwei Luo', 'Xiaolei Dong', 'Yunbo Yang']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving-ML', 'secure-multi-party-computation', 'data-deduplication', 'language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07505</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models</title><link>https://arxiv.org/abs/2511.07503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Biologically-Informed Hybrid Membership Inference Attack (biHMIA) that combines black-box MIA techniques with domain-specific genomics metrics to boost attack effectiveness against generative models of genetic mutation profiles.&lt;/li&gt;&lt;li&gt;Evaluates GPT-like transformer models (both small and large) as synthetic variant generators for small-scale genomics and assesses privacy leakage under differential privacy mechanisms.&lt;/li&gt;&lt;li&gt;Finds that the hybrid attack achieves higher adversarial success than traditional metric-based MIAs on average, highlighting practical limitations of DP protections for genomic generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asia Belfiore', 'Jonathan Passerat-Palmbach', 'Dmitrii Usynin']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'model-privacy', 'differential-privacy', 'genomic-data', 'adversarial-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07503</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift</title><link>https://arxiv.org/abs/2511.07485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unifying theoretical framework that models biases as violations of conditional independence using information-theoretic measures.&lt;/li&gt;&lt;li&gt;Derives formal equivalence conditions connecting spurious correlations, subpopulation shift, class imbalance, and fairness violations; predicts r ≈ (1+α)/(1−α) mapping between spurious-correlation strength α and imbalance ratio r for equivalent worst-group accuracy degradation under feature-overlap assumptions.&lt;/li&gt;&lt;li&gt;Empirically validates the theoretical equivalences across six datasets and three architectures, finding predicted worst-group accuracies match within ~3%.&lt;/li&gt;&lt;li&gt;Implication: enables principled transfer of debiasing and robustness interventions across fairness, distribution-shift, and class-imbalance problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sushant Mehta']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'robustness', 'distribution_shift', 'theoretical_framework', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07485</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title><link>https://arxiv.org/abs/2511.07482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-critical circuits during inference.&lt;/li&gt;&lt;li&gt;Targets alignment degradation caused by dynamic pruning by identifying and retaining input-dependent safety-relevant circuitry.&lt;/li&gt;&lt;li&gt;Builds on Probe Pruning and evaluates AAPP on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT.&lt;/li&gt;&lt;li&gt;Reports improved safety behavior (≈50% increase in refusal rates) at matched compute, enabling more efficient yet safety-preserving deployment of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dev Patel', 'Gabrielle Gervacio', 'Diekola Raimi', 'Kevin Zhu', 'Ryan Lagasse', 'Gabriel Grand', 'Ashwinee Panda', 'Maheep Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model pruning', 'safety-preserving inference', 'dynamic pruning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07482</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs</title><link>https://arxiv.org/abs/2511.07480</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KG-DF, a black-box defense framework leveraging Knowledge Graphs to detect and mitigate jailbreak attacks on LLMs by mapping input content to safe, structured knowledge and safe reasoning paths.&lt;/li&gt;&lt;li&gt;Introduces an extensible semantic parsing module to convert queries into structured secure concept representations, improving keyword/concept extraction for matching against the KG.&lt;/li&gt;&lt;li&gt;Claims improved defense performance against various jailbreak methods while preserving or enhancing general QA response quality by incorporating domain-general knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyuan Liu', 'Jiawei Chen', 'Xiao Yang', 'Hang Su', 'Zhaoxia Yin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak defense', 'prompt safety', 'knowledge graphs', 'semantic parsing', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07480</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Polite Liar: Epistemic Pathology in Language Models</title><link>https://arxiv.org/abs/2511.07477</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that RLHF incentivizes models to prioritize perceived sincerity and conversational fluency over evidential accuracy, producing a systematic 'polite liar' pathology.&lt;/li&gt;&lt;li&gt;Frames the problem using philosophical concepts (Frankfurt on bullshit, speech-act theory) and epistemic virtue theory to distinguish structural indifference from intentional deception.&lt;/li&gt;&lt;li&gt;Identifies an alignment tension: reward signals for helpfulness/harmlessness/politeness do not ensure epistemic grounding, leading to confident fabrication.&lt;/li&gt;&lt;li&gt;Proposes an 'epistemic alignment' principle: design reward architectures that favor justified confidence (epistemic grounding) rather than merely perceived fluency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bentley DeVilling (Course Correct Labs)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'epistemic-risk', 'model-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07477</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents</title><link>https://arxiv.org/abs/2511.07441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AudAgent, a system for real-time automated auditing of AI agents to check runtime behaviors against natural-language privacy policies.&lt;/li&gt;&lt;li&gt;Policy parsing: ensemble of LLMs translates policies into a structured model with cross-LLM voting for confidence.&lt;/li&gt;&lt;li&gt;Runtime annotation: Presidio-based analyzer detects sensitive data and annotates usage; compliance auditing uses ontology alignment and automata-based checks to compare observed actions with policy model.&lt;/li&gt;&lt;li&gt;Provides a platform-independent UI for visualizing execution traces and detected privacy risks; supports custom user-defined policies and is evaluated on agents built with frameworks like AutoGen.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ye Zheng', 'Yidan Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-auditing', 'policy-compliance', 'runtime-monitoring', 'LLM-parsing', 'privacy-risk-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07441</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models</title><link>https://arxiv.org/abs/2511.08484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight, modular "patch" for LLMs implemented as a compact learnable prefix (0.003% of parameters) prepended to an existing model to steer behavior toward a safer reference model.&lt;/li&gt;&lt;li&gt;Targets safety domains including toxicity mitigation, bias reduction, and harmfulness refusal, achieving safety improvements comparable to next-generation safety-aligned models while maintaining fluency.&lt;/li&gt;&lt;li&gt;Emphasizes rapid, composable, and efficient remediation between major model releases as an alternative to full fine-tuning or infrequent costly version updates.&lt;/li&gt;&lt;li&gt;Demonstrates practicality for vendors/practitioners to distribute scalable safety updates with minimal compute and parameter overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huzaifa Arif', 'Keerthiram Murugesan', 'Ching-Yun Ko', 'Pin-Yu Chen', 'Payel Das', 'Alex Gittens']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'parameter-efficient tuning', 'toxicity mitigation', 'policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08484</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Dataset Safety in Autonomous Driving: Requirements, Risks, and Assurance</title><link>https://arxiv.org/abs/2511.08439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a structured framework for creating safety-assured datasets for autonomous driving aligned with ISO/PAS 8800, covering collection, annotation, curation, and maintenance.&lt;/li&gt;&lt;li&gt;Introduces the AI Data Flywheel and dataset lifecycle and embeds safety analyses to identify hazards and mitigate risks arising from dataset insufficiencies.&lt;/li&gt;&lt;li&gt;Defines processes for dataset safety requirements and recommends verification &amp; validation strategies to ensure compliance with safety standards.&lt;/li&gt;&lt;li&gt;Reviews recent research and emerging trends in dataset safety for autonomous vehicles and outlines challenges and future directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alireza Abbaspour', 'Tejaskumar Balgonda Patil', 'B Ravi Kiran', 'Russel Mohr', 'Senthil Yogamani']&lt;/li&gt;&lt;li&gt;Tags: ['dataset safety', 'autonomous driving', 'safety standards', 'verification and validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08439</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FaithAct: Faithfulness Planning and Acting in MLLMs</title><link>https://arxiv.org/abs/2511.08409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes behavioral vs. perceptual faithfulness in multimodal LLMs and introduces FaithEval to quantify step- and chain-level faithfulness against images.&lt;/li&gt;&lt;li&gt;Proposes FaithAct, a planning-and-acting framework that enforces evidential grounding at each reasoning step to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports up to 26% improvement in perceptual faithfulness across reasoning benchmarks without degrading task accuracy, and shows more stable reasoning trajectories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Xinyue Xu', 'Sai Ma', 'Sichao Li']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination mitigation', 'multimodal LLMs', 'evaluation benchmark', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08409</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models</title><link>https://arxiv.org/abs/2511.08379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using Self-Organizing Maps (SOMs) to extract multiple latent 'refusal' directions from LLM representations, rather than a single difference-of-means vector.&lt;/li&gt;&lt;li&gt;Proves SOMs generalize the prior difference-in-means technique and derives multiple directions by subtracting the harmless-centroid from each SOM neuron centroid.&lt;/li&gt;&lt;li&gt;Shows ablating multiple directions in model internals more effectively suppresses refusal behavior than the single-direction baseline and outperforms specialized jailbreak algorithms in their experiments.&lt;/li&gt;&lt;li&gt;Provides mechanistic interpretability analysis of the discovered manifold of refusal directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgio Piras', 'Raffaele Mura', 'Fabio Brau', 'Luca Oneto', 'Fabio Roli', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM interpretability', 'refusal suppression', 'adversarial prompting', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08379</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Provably Unlearnable Examples via Bayes Error Optimisation</title><link>https://arxiv.org/abs/2511.08191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes constructing unlearnable examples by maximising the Bayes error to provably increase irreducible classification error.&lt;/li&gt;&lt;li&gt;Develops an optimisation method with an efficient projected gradient ascent solution and theoretical guarantees on increasing Bayes error.&lt;/li&gt;&lt;li&gt;Method remains effective when unlearnable examples are mixed with clean data; validated empirically across datasets and model architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruihan Zhang', 'Jun Sun', 'Ee-Peng Lim', 'Peixin Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy', 'unlearnable examples', 'provable guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08191</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency</title><link>https://arxiv.org/abs/2511.08082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a five-pillar prudential framework (governance, data lineage, assurance, resilience, regulatory alignment) for assessing LLM reliability in reinsurance.&lt;/li&gt;&lt;li&gt;Implements the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB) to evaluate governance-embedded, retrieval-grounded LLM configurations across six task families.&lt;/li&gt;&lt;li&gt;Reports empirical improvements: grounding accuracy ~0.90, about 40% reduction in hallucination/interpretive drift, and nearly doubled transparency, arguing that existing prudential doctrines can accommodate reliable AI with explicit governance and verifiable assurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stella C. Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Governance &amp; assurance', 'Benchmarking', 'Retrieval grounding', 'Regulatory alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08082</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement</title><link>https://arxiv.org/abs/2511.08055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MSCR, an automated adversarial attack that generates semantically similar candidate replacements per word using LLM embeddings, WordNet, and a masked language model.&lt;/li&gt;&lt;li&gt;Performs large-scale attacks on GSM8K and MATH500, showing single-word perturbations can drop accuracy up to ~49.9% and ~35.4% respectively while preserving semantic consistency.&lt;/li&gt;&lt;li&gt;Finds perturbations not only cause incorrect outputs but also increase response length and computational cost, highlighting robustness and efficiency weaknesses in LLM mathematical reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhishen Sun', 'Guang Dai', 'Haishan Ye']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'input perturbation', 'mathematical reasoning', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08055</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2511.08022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a perturbation framework that injects semantically irrelevant sentences (including numeric distractors) and removes core question instructions to evaluate LLM mathematical reasoning robustness.&lt;/li&gt;&lt;li&gt;Finds that numeric perturbations significantly degrade performance (open-source models drop ~10% up to 51.55%; commercial models drop 3–10%), and performance declines with increasing perturbation intensity.&lt;/li&gt;&lt;li&gt;Shows that with core questioning instruction missing, models retain 20–40% accuracy, suggesting reliance on pattern matching or memorized templates rather than robust logical reasoning.&lt;/li&gt;&lt;li&gt;Concludes current LLMs have clear sensitivity and robustness boundaries in mathematical reasoning, with implications for safety, evaluation, and red-teaming of models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhishen Sun', 'Guang Dai', 'Ivor Tsang', 'Haishan Ye']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness evaluation', 'prompt injection', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08022</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder</title><link>https://arxiv.org/abs/2511.07896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SparseRM, a lightweight reward model that uses a Sparse Autoencoder (SAE) to decompose LLM representations into interpretable, preference-relevant directions.&lt;/li&gt;&lt;li&gt;Computes alignment scores by projecting representations onto these sparse directions and aggregates them with a simple reward head, using &lt;1% of trainable parameters.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance on three preference modeling tasks and claims easy integration into downstream alignment pipelines for efficient alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengcan Liu', 'Jiahao Li', 'Zheren Fu', 'Yi Tu', 'Jiajun Li', 'Zhendong Mao', 'Yongdong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference modeling', 'interpretability', 'efficient models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07896</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking</title><link>https://arxiv.org/abs/2511.07863</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WaterMod, a token-rank modular partitioning watermark that sorts vocabulary by model probability and assigns tokens to classes by rank mod k, then biases one class to embed a detectable signal.&lt;/li&gt;&lt;li&gt;Supports zero-bit watermarking (k=2) with an entropy-adaptive gate to preserve high-probability tokens and multi-bit payloads (k&gt;2) to embed base-k digits per decoding step for richer provenance.&lt;/li&gt;&lt;li&gt;Claims strong detection performance with minimal impact on generation quality across tasks including natural language, math reasoning, and code synthesis; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Hyejin Park', 'Hyeseon Ahn', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'provenance/forensics', 'text-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07863</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment-Aware Quantization for LLM Safety</title><link>https://arxiv.org/abs/2511.07842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety vulnerability where post-training quantization (PTQ) can degrade alignment (safety) even when perplexity remains low.&lt;/li&gt;&lt;li&gt;Proposes Alignment-Aware Quantization (AAQ) that adds an Alignment-Preserving Contrastive (APC) loss to PTQ to make the quantized model mimic the instruction-tuned (safe) model and diverge from the unaligned pretrained model.&lt;/li&gt;&lt;li&gt;Shows AAQ is compatible with standard PTQ pipelines and achieves robust 4-bit (W4A4) quantization across LLaMA, Qwen, and Mistral while maintaining safety without special safety calibration datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunghyun Wee', 'Suyoung Kim', 'Hyeonjin Kim', 'Kyomin Hwang', 'Nojun Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['model quantization', 'alignment', 'safety robustness', 'post-training quantization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07842</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions</title><link>https://arxiv.org/abs/2511.07669</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a five-layer protection architecture and an ordered calibration protocol (4-stage initialization + 7-stage maintenance) to sustain a 'partnership state' between humans and LLMs for high-stakes decisions.&lt;/li&gt;&lt;li&gt;Reports systematic qualitative evaluation across 7 frontier LLMs and 3 time-pressured venture vignettes, finding initial partnership achievable but fragile under operational pressure and model/context drift.&lt;/li&gt;&lt;li&gt;Identifies mechanisms for safety/reliability: bias self-monitoring, human-AI adversarial challenge, partnership-state verification, performance-degradation detection, and stakeholder-protection/dissolution discipline; documents cross-model performance differences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro R. Jadad']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM reliability', 'Human-AI collaboration', 'Safety evaluation', 'Red teaming / adversarial challenge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07669</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning</title><link>https://arxiv.org/abs/2511.07483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a confidence-aware reward model (C2RM) for RL-based training to improve STEM reasoning in LLMs by penalizing low-confidence correct answers as well as incorrect answers.&lt;/li&gt;&lt;li&gt;Aims to reduce spurious or low-quality reasoning chains that sometimes accidentally produce correct answers and get rewarded under rule-based judges.&lt;/li&gt;&lt;li&gt;Validates approach via static evaluations, Best-of-N inference, and PPO-based RL training, showing improvements over several open-source reward models.&lt;/li&gt;&lt;li&gt;Targets enabling more reliable RL training for smaller-scale models and releases code and model weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianxi He', 'Qingyu Ren', 'Shanzhe Lei', 'Xuhong Wang', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward_modeling', 'robust_reasoning', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07483</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
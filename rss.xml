<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 26 Jan 2026 22:47:46 +0000</lastBuildDate><item><title>DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis</title><link>https://arxiv.org/abs/2510.25237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeepShield, a deepfake detection framework that combines local spatiotemporal patch-wise supervision with global feature augmentation to improve cross-domain and cross-manipulation robustness.&lt;/li&gt;&lt;li&gt;Local Patch Guidance (LPG) models fine-grained, spatiotemporal artifacts via patch-wise supervision to capture localized inconsistencies missed by global encoders.&lt;/li&gt;&lt;li&gt;Global Forgery Diversification (GFD) synthesizes domain-bridging and boundary-expanding feature augmentations to generate diverse forgery-like features and reduce overfitting to specific manipulation artifacts.&lt;/li&gt;&lt;li&gt;Evaluated on cross-dataset and cross-manipulation settings, DeepShield (built on CLIP-ViT) outperforms state-of-the-art methods, demonstrating improved robustness to unseen deepfake attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinqi Cai', 'Jichang Li', 'Zhaolun Li', 'Weikai Chen', 'Rushi Lan', 'Xi Xie', 'Xiaonan Luo', 'Guanbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'cross-domain generalization', 'forgery augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25237</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection</title><link>https://arxiv.org/abs/2510.00634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a face forgery (deepfake) detection method using a Kolmogorov-Arnold Network (KAN) with learnable spline activations to better model complex, non-linear forgery artifacts.&lt;/li&gt;&lt;li&gt;Introduces LAKAN — a Landmark-assisted Adaptive KAN module that uses facial landmarks as structural priors to dynamically generate instance-specific KAN parameters, steering the encoder to focus on informative facial regions.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance on multiple public face forgery datasets, indicating effectiveness as a defensive detection approach against manipulated facial imagery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayao Jiang', 'Siran Peng', 'Bin Liu', 'Qi Chu', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery detection', 'defensive method', 'facial landmarks', 'adaptive activation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00634</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection</title><link>https://arxiv.org/abs/2509.10250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAMMA, a training framework to improve generalization of AI-generated image detectors by reducing reliance on generation-specific artifacts and enhancing semantic alignment.&lt;/li&gt;&lt;li&gt;Introduces manipulation-augmented training (inpainting-based manipulations and semantics-preserving perturbations) and multi-task supervision with dual segmentation heads plus a classification head for pixel-level source attribution.&lt;/li&gt;&lt;li&gt;Adds a reverse cross-attention mechanism allowing segmentation heads to guide and correct biased representations in the classification branch.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art generalization on the GenImage benchmark (≈+5.8% accuracy) and robustness to newly released generative models such as GPT-4o.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haozhen Yan', 'Yan Hong', 'Suning Lang', 'Jiahui Zhan', 'Yikun Ji', 'Yujie Gao', 'Huijia Zhu', 'Jun Lan', 'Jianfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'Deepfake detection', 'Robustness', 'Generalization', 'Defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10250</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies structural fragility in existing unlearning methods: erasure often yields sharp minima where hallucinations quickly resurge after lightweight relearning or parameter shifts.&lt;/li&gt;&lt;li&gt;Proposes SARE, a targeted min-max unlearning framework using Targeted-SAM to flatten the loss landscape around hallucinated concepts, enforcing robust suppression under worst-case parameter perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SARE outperforms baselines in persistent hallucination removal while preserving general generation quality, remaining stable against relearning and parameter updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianya Fang', 'Feiyang Ren', 'Xiang Chen', 'Yu Tian', 'Zhen Bi', 'Haiyang Yu', 'Sheng-Jun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['Unlearning', 'Model Editing', 'Robustness', 'Hallucination Mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16527</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses</title><link>https://arxiv.org/abs/2601.16473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeMark, a query-free black-box attack that removes defensive watermarks from synthetic images by manipulating latent-space representations via a compressive-sensing sparsification process.&lt;/li&gt;&lt;li&gt;Evaluates DeMark against eight state-of-the-art encoder–decoder watermarking schemes, reducing detection accuracy from 100% to 32.9% on average while preserving visual quality.&lt;/li&gt;&lt;li&gt;Compares and tests three mitigation strategies (image super-resolution, sparse watermarking, adversarial training) and finds them largely ineffective against the proposed attack.&lt;/li&gt;&lt;li&gt;Highlights latent-space vulnerabilities in current watermarking defenses and argues for the need for more robust watermarking designs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Song', 'Zhenchang Xing', 'Liming Zhu', 'Yulei Sui', 'Jingling Xue']&lt;/li&gt;&lt;li&gt;Tags: ['black-box attack', 'watermark removal', 'deepfake security', 'latent-space attack', 'compressive sensing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16473</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title><link>https://arxiv.org/abs/2512.06716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and studies Indirect Prompt Injection (IPI) attacks against autonomous LLM agents and argues existing defenses are fragmented and insufficient for full-lifecycle integrity.&lt;/li&gt;&lt;li&gt;Proposes Cognitive Control Architecture (CCA): a dual-layer defense combining a pre-generated 'Intent Graph' for proactive control-flow/data-flow integrity and a 'Tiered Adjudicator' for deep reasoning and multi-dimensional scoring upon deviation detection.&lt;/li&gt;&lt;li&gt;Evaluates CCA on the AgentDojo benchmark, claiming improved robustness, security, and efficiency against sophisticated IPI attacks compared to prior defense methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Liang', 'Tianze Hu', 'Zaiye Chen', 'Mingjie Tang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'agent security', 'runtime defense', 'integrity enforcement', 'red teaming/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06716</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM Jailbreak Detection for (Almost) Free!</title><link>https://arxiv.org/abs/2509.14558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that output distribution differences between jailbreak and benign prompts can be used to detect jailbreaks.&lt;/li&gt;&lt;li&gt;Proposes Free Jailbreak Detection (FJD): prepend an affirmative instruction and scale logits (temperature) to differentiate prompts via first-token confidence, minimizing extra compute.&lt;/li&gt;&lt;li&gt;Improves FJD with virtual instruction learning and shows effective jailbreak detection on aligned LLMs with almost no additional inference cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guorui Chen', 'Yifan Xia', 'Xiaojun Jia', 'Zhijiang Li', 'Philip Torr', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak-detection', 'LLM-security', 'defense', 'prompt-injection', 'model-guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14558</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders</title><link>https://arxiv.org/abs/2505.16004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates robustness quantification for sparse autoencoder (SAE) concept representations as input-space optimization problems to find adversarial perturbations that alter concept activations.&lt;/li&gt;&lt;li&gt;Develops a comprehensive evaluation framework with realistic scenarios to craft perturbations that manipulate SAE-derived concept labels.&lt;/li&gt;&lt;li&gt;Empirically shows that tiny adversarial input perturbations can reliably manipulate concept-based interpretations while leaving the base LLM activations largely unchanged, indicating fragile concept representations unsuitable for monitoring without additional defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron J. Li', 'Suraj Srinivas', 'Usha Bhalla', 'Himabindu Lakkaraju']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'robustness evaluation', 'attack analysis', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16004</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>StealthGraph: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation</title><link>https://arxiv.org/abs/2601.04740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces StealthGraph, an end-to-end framework that generates domain-specific harmful prompts using knowledge-graph guidance to ensure relevance to specialized domains (e.g., finance, healthcare).&lt;/li&gt;&lt;li&gt;Applies dual-path obfuscation rewriting (direct and context-enhanced) to convert explicit harmful prompts into more implicit, harder-to-detect variants.&lt;/li&gt;&lt;li&gt;Produces datasets intended for realistic red-teaming and evaluation of LLM safety defenses, and releases code and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huawei Zheng', 'Xinqi Jiang', 'Sen Yang', 'Shouling Ji', 'Yingcai Wu', 'Dazhen Deng']&lt;/li&gt;&lt;li&gt;Tags: ['harmful-prompt-generation', 'red-teaming', 'jailbreaking', 'knowledge-graph-guided attacks', 'dataset-for-safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04740</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies structural fragility in existing unlearning methods: erasure often yields sharp minima where hallucinations quickly resurge after lightweight relearning or parameter shifts.&lt;/li&gt;&lt;li&gt;Proposes SARE, a targeted min-max unlearning framework using Targeted-SAM to flatten the loss landscape around hallucinated concepts, enforcing robust suppression under worst-case parameter perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SARE outperforms baselines in persistent hallucination removal while preserving general generation quality, remaining stable against relearning and parameter updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianya Fang', 'Feiyang Ren', 'Xiang Chen', 'Yu Tian', 'Zhen Bi', 'Haiyang Yu', 'Sheng-Jun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['Unlearning', 'Model Editing', 'Robustness', 'Hallucination Mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16527</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models</title><link>https://arxiv.org/abs/2601.16231</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of untargeted, audio-only adversarial attacks on trimodal (audio-video-language) foundation models, proposing six complementary attack objectives targeting encoder representations, cross-modal attention, hidden states, and output likelihoods.&lt;/li&gt;&lt;li&gt;Empirical evaluation across three state-of-the-art models and multiple benchmarks shows high attack success (up to 96%) at low perceptual distortion; attacks transfer poorly across models/encoders, while ASR systems (e.g., Whisper) respond mainly to perturbation magnitude.&lt;/li&gt;&lt;li&gt;Highlights a previously overlooked single-modality attack surface in multimodal systems and motivates defenses that enforce cross-modal consistency and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aafiya Hussain', 'Gaurav Srivastava', 'Alvi Ishmam', 'Zaber Hakim', 'Chris Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'audio adversarial attacks', 'multimodal robustness', 'attack transferability', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16231</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems</title><link>https://arxiv.org/abs/2601.16890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new class of adversarial attacks on automated fact-checking (AFC) systems by using generative LLMs to rephrase claims with persuasion techniques (15 techniques in 6 categories).&lt;/li&gt;&lt;li&gt;Evaluates effects on both claim verification and evidence retrieval with a decoupled evaluation strategy on FEVER and FEVEROUS benchmarks.&lt;/li&gt;&lt;li&gt;Shows that persuasion-based rephrasings substantially degrade verification and retrieval performance, highlighting a novel vulnerability vector for AFC systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao A. Leite', 'Olesya Razuvayevskaya', 'Kalina Bontcheva', 'Carolina Scarton']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'automated-fact-checking', 'LLM-based-attacks', 'persuasion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16890</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do LLM hallucination detectors suffer from low-resource effect?</title><link>https://arxiv.org/abs/2601.16766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study asking whether hallucination detectors for LLMs suffer from the low-resource (language) effect.&lt;/li&gt;&lt;li&gt;Experiments across five tasks in three domains (factual recall, STEM, Humanities) using four LLMs and three hallucination detectors.&lt;/li&gt;&lt;li&gt;Findings: task accuracy drops substantially in low-resource languages, but detector accuracy drops much less; detectors remain robust within-language and in multilingual setups but fail in cross-lingual transfer without in-language supervision.&lt;/li&gt;&lt;li&gt;Implication: detectors can exploit internal uncertainty signals even in low-resource languages, suggesting potential for robust hallucination detection given in-language supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debtanu Datta', 'Mohan Kishore Chilukuri', 'Yash Kumar', 'Saptarshi Ghosh', 'Muhammad Bilal Zafar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'multilingual robustness', 'LLM safety', 'low-resource languages', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16766</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sycophancy Hides Linearly in the Attention Heads</title><link>https://arxiv.org/abs/2601.16644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that sycophantic (correct-to-incorrect deference) signals are most linearly separable in a sparse subset of middle-layer attention heads.&lt;/li&gt;&lt;li&gt;Trains linear probes across residual stream, MLP, and attention layers and shows steering via probes is most effective when applied to those attention heads.&lt;/li&gt;&lt;li&gt;Demonstrates probe transfer from TruthfulQA to other factual QA benchmarks and shows limited overlap with previously identified 'truthful' directions, implying distinct mechanisms for factual accuracy vs. deference resistance.&lt;/li&gt;&lt;li&gt;Analyzes attention patterns indicating influential heads disproportionately attend to user expressions of doubt, and proposes simple targeted linear interventions to mitigate sycophancy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rifo Genadi', 'Munachiso Nwadike', 'Nurdaulet Mukhituly', 'Hilal Alquabeh', 'Tatsuya Hiraoka', 'Kentaro Inui']&lt;/li&gt;&lt;li&gt;Tags: ['model-interpretability', 'alignment', 'defenses', 'representation-learning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16644</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SearchLLM: Detecting LLM Paraphrased Text by Measuring the Similarity with Regeneration of the Candidate Source via Search Engine</title><link>https://arxiv.org/abs/2601.16512</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SearchLLM, a method to detect LLM-paraphrased text by using search engines to find candidate original sources and regenerating those sources to compare similarity with the input.&lt;/li&gt;&lt;li&gt;Measures similarity between the input text and regenerated versions of retrieved candidate sources to identify paraphrasing that closely mimics original content.&lt;/li&gt;&lt;li&gt;Designed as a proxy layer to integrate with existing detectors, improving detection accuracy and mitigating paraphrasing attacks across multiple LLMs.&lt;/li&gt;&lt;li&gt;Provides experimental evidence that SearchLLM consistently enhances recent detectors' performance in spotting LLM-paraphrased content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang-Quoc Nguyen-Son', 'Minh-Son Dao', 'Koji Zettsu']&lt;/li&gt;&lt;li&gt;Tags: ['text-generation-detection', 'paraphrase-detection', 'defense', 'search-based-retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16512</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Persona Jailbreaking in Large Language Models</title><link>https://arxiv.org/abs/2601.16466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces persona editing/jailbreaking as a black-box attack that steers LLM traits via adversarial conversational history.&lt;/li&gt;&lt;li&gt;Proposes PHISH (Persona Hijacking via Implicit Steering in History) to embed semantically loaded cues in user queries to induce reverse or altered personas.&lt;/li&gt;&lt;li&gt;Evaluates PHISH across 3 benchmarks and 8 LLMs (including high-risk domains: mental health, tutoring, customer support) with human and LLM-as-judge validation, showing reliable persona shifts and collateral trait changes while preserving most model utility.&lt;/li&gt;&lt;li&gt;Finds existing guardrails provide partial but brittle protection under sustained multi-turn attacks and defines a metric to quantify attack success.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jivnesh Sandhan', 'Fei Cheng', 'Tushar Sandhan', 'Yugo Murawaki']&lt;/li&gt;&lt;li&gt;Tags: ['persona-jailbreaking', 'jailbreaking', 'adversarial-attack', 'black-box-attack', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16466</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UACER: An Uncertainty-Adaptive Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2512.10492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UACER, an uncertainty-adaptive critic ensemble for robust adversarial reinforcement learning to stabilize training against a trainable adversary.&lt;/li&gt;&lt;li&gt;Uses a diverse ensemble of K critic networks to reduce Q-value variance and improve robustness versus single-critic designs.&lt;/li&gt;&lt;li&gt;Introduces Time-varying Decay Uncertainty (TDU), a variance-derived Q-value aggregation that leverages epistemic uncertainty to adapt exploration–exploitation and stabilize learning.&lt;/li&gt;&lt;li&gt;Evaluated on MuJoCo control tasks, showing improved performance, stability, and efficiency compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'robustness', 'ensemble methods', 'uncertainty estimation', 'training stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10492</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters</title><link>https://arxiv.org/abs/2510.26501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using lightweight Unsupervised Anomaly Detection (UAD) filters as upstream OOD/noise detectors to improve ECG classifier reliability on microcontrollers.&lt;/li&gt;&lt;li&gt;Performs a Neural Architecture Search across six UAD approaches (Deep SVDD, AE/VAE, MAD, Normalizing Flows, DDPM) constrained to ≤512k parameters.&lt;/li&gt;&lt;li&gt;Finds a NAS-optimized Deep SVDD gives the best trade-off between detection performance and model size and boosts diagnostic classifier accuracy by up to 21 percentage points.&lt;/li&gt;&lt;li&gt;Evaluated on PTB-XL and BUT QDB datasets and validated in a simulated microcontroller deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mustafa Fuad Rifet Ibrahim', 'Maurice Meijer', 'Alexander Schlaefer', 'Peer Stelldinger']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'Anomaly detection', 'Robustness / Defense', 'Embedded/IoT (microcontrollers)', 'ECG / medical time-series']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26501</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders</title><link>https://arxiv.org/abs/2505.16004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates robustness quantification for sparse autoencoder (SAE) concept representations as input-space optimization problems to find adversarial perturbations that alter concept activations.&lt;/li&gt;&lt;li&gt;Develops a comprehensive evaluation framework with realistic scenarios to craft perturbations that manipulate SAE-derived concept labels.&lt;/li&gt;&lt;li&gt;Empirically shows that tiny adversarial input perturbations can reliably manipulate concept-based interpretations while leaving the base LLM activations largely unchanged, indicating fragile concept representations unsuitable for monitoring without additional defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron J. Li', 'Suraj Srinivas', 'Usha Bhalla', 'Himabindu Lakkaraju']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'robustness evaluation', 'attack analysis', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16004</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Provable Differentially Private Computation of the Cross-Attention Mechanism</title><link>https://arxiv.org/abs/2407.14717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel data structure and mechanism that enforces (ε, δ)-differential privacy for cross-attention layers with provable theoretical guarantees.&lt;/li&gt;&lt;li&gt;Provides complexity bounds: space/init Õ(n d r^2) and per-token query time Õ(d r^2), with parameters for polynomial kernel approximation controlling accuracy.&lt;/li&gt;&lt;li&gt;Derives additive and relative error bounds (in terms of n, ε, R, R_w, r, s, ε_s) and proves robustness to adaptive (adversarial) queries.&lt;/li&gt;&lt;li&gt;Positions this as the first provable DP approach tailored to cross-attention, enabling privacy-preserving use in RAG, system prompting, and guided diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yekun Ke', 'Yingyu Liang', 'Zhenmei Shi', 'Zhao Song', 'Jiahao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-attention', 'theoretical-security', 'adaptive-robustness', 'attention-mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.14717</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems</title><link>https://arxiv.org/abs/2601.16890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new class of adversarial attacks on automated fact-checking (AFC) systems by using generative LLMs to rephrase claims with persuasion techniques (15 techniques in 6 categories).&lt;/li&gt;&lt;li&gt;Evaluates effects on both claim verification and evidence retrieval with a decoupled evaluation strategy on FEVER and FEVEROUS benchmarks.&lt;/li&gt;&lt;li&gt;Shows that persuasion-based rephrasings substantially degrade verification and retrieval performance, highlighting a novel vulnerability vector for AFC systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao A. Leite', 'Olesya Razuvayevskaya', 'Kalina Bontcheva', 'Carolina Scarton']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'automated-fact-checking', 'LLM-based-attacks', 'persuasion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16890</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Building a Robust Risk-Based Access Control System to Combat Ransomware's Capability to Encrypt: A Machine Learning Approach</title><link>https://arxiv.org/abs/2601.16795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a risk-based access control architecture that combines machine-learning inference with mandatory access control (SELinux) to detect and block unauthorized encryption (ransomware) in real time.&lt;/li&gt;&lt;li&gt;Builds a specialized dataset from Linux function-level tracing (ftrace function_graph) to train a supervised classifier and extract interpretable rules mapped to lightweight booleans for policy enforcement.&lt;/li&gt;&lt;li&gt;Demonstrates a two-layer design (model detection + rule-like enforcement) that preserves detection quality while providing low-latency, explainable permit/deny decisions at encryption start.&lt;/li&gt;&lt;li&gt;Evaluates detection performance and operational footprint, discusses prototype overhead and engineering steps toward a production kernel-space solution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kenan Begovic', 'Abdulaziz Al-Ali', 'Qutaibah Malluhi']&lt;/li&gt;&lt;li&gt;Tags: ['ransomware', 'runtime-detection', 'access-control', 'ML-based-defense', 'kernel-tracing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16795</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers</title><link>https://arxiv.org/abs/2601.16675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a causal-reasoning method to identify frequency-space subsets that are sufficient and necessary for specific audio classifications.&lt;/li&gt;&lt;li&gt;Implements the approach in a tool called FreqReX and evaluates it on standard audio benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates that tiny changes (e.g., altering one out of ~240,000 frequencies) can induce misclassification 58% of the time, often with changes that are practically inaudible.&lt;/li&gt;&lt;li&gt;Shows that causal feature analysis both aids interpretability and enables effective manipulation/attack of audio classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David A. Kelly', 'Hana Chockler']&lt;/li&gt;&lt;li&gt;Tags: ['audio adversarial examples', 'model vulnerability', 'causal analysis', 'interpretability', 'attack methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16675</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models</title><link>https://arxiv.org/abs/2601.16231</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of untargeted, audio-only adversarial attacks on trimodal (audio-video-language) foundation models, proposing six complementary attack objectives targeting encoder representations, cross-modal attention, hidden states, and output likelihoods.&lt;/li&gt;&lt;li&gt;Empirical evaluation across three state-of-the-art models and multiple benchmarks shows high attack success (up to 96%) at low perceptual distortion; attacks transfer poorly across models/encoders, while ASR systems (e.g., Whisper) respond mainly to perturbation magnitude.&lt;/li&gt;&lt;li&gt;Highlights a previously overlooked single-modality attack surface in multimodal systems and motivates defenses that enforce cross-modal consistency and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aafiya Hussain', 'Gaurav Srivastava', 'Alvi Ishmam', 'Zaber Hakim', 'Chris Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'audio adversarial attacks', 'multimodal robustness', 'attack transferability', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16231</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection</title><link>https://arxiv.org/abs/2601.16976</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using a Latent Diffusion Model (LDM) to generate synthetic IoT attack samples (DDoS, Mirai, Man-in-the-Middle) to mitigate class imbalance in ML-based intrusion detection systems (IDS).&lt;/li&gt;&lt;li&gt;Evaluates both downstream IDS performance (F1 improvements up to 0.99 for some attacks) and intrinsic generative quality using distributional, dependency-based, and diversity metrics.&lt;/li&gt;&lt;li&gt;Shows LDMs preserve feature dependencies, generate diverse high-fidelity samples, and are ~25% faster to sample from than diffusion models operating in data space.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Estela S\\'anchez-Carballo", 'Francisco M. Melgarejo-Meseguer', "Jos\\'e Luis Rojo-\\'Alvarez"]&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'data-augmentation', 'latent-diffusion', 'IoT-security', 'generative-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16976</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints</title><link>https://arxiv.org/abs/2601.16905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a MoE-specific vulnerability in machine unlearning where routers are manipulated to redirect queries (superficial forgetting) rather than true knowledge erasure from experts.&lt;/li&gt;&lt;li&gt;Proposes GRIP, an algorithm-agnostic adapter that enforces geometric constraints by projecting router gradient updates into expert-specific null spaces, preserving routing stability while forcing knowledge removal from expert parameters.&lt;/li&gt;&lt;li&gt;Demonstrates that GRIP prevents expert selection shifts (&gt;95% routing stability) across unlearning methods and preserves model utility, enabling existing unlearning algorithms to be correctly applied to MoE architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy Zhu', 'Rongzhe Wei', 'Yupu Gu', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'mixture-of-experts', 'defense', 'router vulnerability', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16905</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Theory of Minimal Weight Perturbations in Deep Networks and its Applications for Low-Rank Activated Backdoor Attacks</title><link>https://arxiv.org/abs/2601.16880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives exact minimal-norm single-layer weight perturbation formulas required to induce a specified output change in deep networks and analyzes factors controlling perturbation size.&lt;/li&gt;&lt;li&gt;Compares these exact expressions to multi-layer Lipschitz-based robustness bounds and finds them to be of similar order.&lt;/li&gt;&lt;li&gt;Applies theory to precision-modification / low-rank activated backdoor attacks, proving compression thresholds below which such attacks cannot succeed and empirically showing low-rank compression can reliably activate latent backdoors while preserving full-precision accuracy.&lt;/li&gt;&lt;li&gt;Provides certifiable guarantees on the smallest parameter updates consistent with a desired output shift and links layer-wise sensitivity to back-propagated margins.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bethan Evans', 'Jared Tanner']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'weight perturbations', 'model robustness', 'model compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16880</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies structural fragility in existing unlearning methods: erasure often yields sharp minima where hallucinations quickly resurge after lightweight relearning or parameter shifts.&lt;/li&gt;&lt;li&gt;Proposes SARE, a targeted min-max unlearning framework using Targeted-SAM to flatten the loss landscape around hallucinated concepts, enforcing robust suppression under worst-case parameter perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SARE outperforms baselines in persistent hallucination removal while preserving general generation quality, remaining stable against relearning and parameter updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianya Fang', 'Feiyang Ren', 'Xiang Chen', 'Yu Tian', 'Zhen Bi', 'Haiyang Yu', 'Sheng-Jun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['Unlearning', 'Model Editing', 'Robustness', 'Hallucination Mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16527</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Effects of Adversarial Perturbations on Distribution Robustness</title><link>https://arxiv.org/abs/2601.16464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Theoretical analysis connecting adversarial robustness (via adversarial training / perturbed-data training) to distributional robustness under data shifts.&lt;/li&gt;&lt;li&gt;Identifies a tradeoff where adversarial training can increase reliance on spurious features and harm performance on underrepresented subgroups.&lt;/li&gt;&lt;li&gt;Shows a nuanced phenomenon where l_infinity perturbations on moderately biased data can improve distributional robustness, and that increased feature separability can preserve robustness even under skewed data.&lt;/li&gt;&lt;li&gt;Highlights that feature separability mediates the tradeoff and that ignoring it can lead to misleading conclusions about robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yipei Wang', 'Zhaoying Pan', 'Xiaoqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_examples', 'adversarial_training', 'distribution_shift', 'robustness_theory', 'spurious_features']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16464</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Introducing the Generative Application Firewall (GAF)</title><link>https://arxiv.org/abs/2601.15824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Generative Application Firewall (GAF), an architectural enforcement layer for securing LLM applications by unifying prompt filters, guardrails, and data-masking into a single control point.&lt;/li&gt;&lt;li&gt;Aims to cover not only direct LLM interactions but also autonomous agents and their tool integrations, providing coordinated runtime defenses similar to a web application firewall (WAF).&lt;/li&gt;&lt;li&gt;Focuses on practical deployment and enforcement for application-level vulnerabilities rather than model-internal modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joan Vendrell Farreny (NeuralTrust)', "Mart\\'i Jord\\`a Roca (NeuralTrust)", 'Miquel Cornudella Gaya (NeuralTrust)', "Rodrigo Fern\\'andez Ba\\'on (NeuralTrust)", "V\\'ictor Garc\\'ia Mart\\'inez (NeuralTrust)", 'Eduard Camacho Sucarrats (NeuralTrust)', 'Alessandro Pignati (NeuralTrust)']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'guardrails', 'application-layer security', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15824</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>StealthGraph: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation</title><link>https://arxiv.org/abs/2601.04740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces StealthGraph, an end-to-end framework that generates domain-specific harmful prompts using knowledge-graph guidance to ensure relevance to specialized domains (e.g., finance, healthcare).&lt;/li&gt;&lt;li&gt;Applies dual-path obfuscation rewriting (direct and context-enhanced) to convert explicit harmful prompts into more implicit, harder-to-detect variants.&lt;/li&gt;&lt;li&gt;Produces datasets intended for realistic red-teaming and evaluation of LLM safety defenses, and releases code and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huawei Zheng', 'Xinqi Jiang', 'Sen Yang', 'Shouling Ji', 'Yingcai Wu', 'Dazhen Deng']&lt;/li&gt;&lt;li&gt;Tags: ['harmful-prompt-generation', 'red-teaming', 'jailbreaking', 'knowledge-graph-guided attacks', 'dataset-for-safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04740</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UACER: An Uncertainty-Adaptive Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2512.10492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UACER, an uncertainty-adaptive critic ensemble for robust adversarial reinforcement learning to stabilize training against a trainable adversary.&lt;/li&gt;&lt;li&gt;Uses a diverse ensemble of K critic networks to reduce Q-value variance and improve robustness versus single-critic designs.&lt;/li&gt;&lt;li&gt;Introduces Time-varying Decay Uncertainty (TDU), a variance-derived Q-value aggregation that leverages epistemic uncertainty to adapt exploration–exploitation and stabilize learning.&lt;/li&gt;&lt;li&gt;Evaluated on MuJoCo control tasks, showing improved performance, stability, and efficiency compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'robustness', 'ensemble methods', 'uncertainty estimation', 'training stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10492</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing</title><link>https://arxiv.org/abs/2510.15303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DSSmoothing, the first certified dataset ownership verification (DOV) method for pre-trained language models in a gray-box setting by applying randomized smoothing in both embedding (continuous) and permutation (token-order) spaces.&lt;/li&gt;&lt;li&gt;Generates norm-constrained, robust watermarked datasets with dual-space triggers and computes watermark robustness (WR) of suspicious models, statistically comparing WR to principal probabilities (PP) from benign models to decide ownership.&lt;/li&gt;&lt;li&gt;Provides theoretical provable robustness guarantees under bounded dual-space perturbations and shows empirical robustness against natural noise and adaptive adversarial attacks across multiple web-scale datasets.&lt;/li&gt;&lt;li&gt;Implements and releases code demonstrating stable, reliable verification performance for PLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Xing Liu', 'Wenke Huang', 'Jianbin Li', 'Zhaoxin Fan', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['dataset ownership verification', 'watermarking', 'certified robustness', 'randomized smoothing', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15303</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM Watermark Evasion via Bias Inversion</title><link>https://arxiv.org/abs/2509.23019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Bias-Inversion Rewriting Attack (BIRA), a model-agnostic method that suppresses logits of likely watermarked tokens during LLM-based rewriting to weaken watermark signals.&lt;/li&gt;&lt;li&gt;Demonstrates &gt;99% evasion across recent watermarking schemes while preserving the original text's semantic content.&lt;/li&gt;&lt;li&gt;Provides theoretical motivation and empirical results highlighting a systematic vulnerability in current LLM watermarking, urging stress testing and development of robust defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongyeon Hwang', 'Sangdon Park', 'Jungseul Ok']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'adversarial-attack', 'evasion', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23019</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM Jailbreak Detection for (Almost) Free!</title><link>https://arxiv.org/abs/2509.14558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that output distribution differences between jailbreak and benign prompts can be used to detect jailbreaks.&lt;/li&gt;&lt;li&gt;Proposes Free Jailbreak Detection (FJD): prepend an affirmative instruction and scale logits (temperature) to differentiate prompts via first-token confidence, minimizing extra compute.&lt;/li&gt;&lt;li&gt;Improves FJD with virtual instruction learning and shows effective jailbreak detection on aligned LLMs with almost no additional inference cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guorui Chen', 'Yifan Xia', 'Xiaojun Jia', 'Zhijiang Li', 'Philip Torr', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak-detection', 'LLM-security', 'defense', 'prompt-injection', 'model-guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14558</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Designing Effective Digital Literacy Interventions for Boosting Deepfake Discernment</title><link>https://arxiv.org/abs/2507.23492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Experimental study comparing five digital literacy interventions (textual indicators, visual demonstrations, gamified exercise, repeated exposure with feedback, and AI-based generation explanations) to improve people's ability to discern deepfake images.&lt;/li&gt;&lt;li&gt;Between-subjects experiment with N=1,200 U.S. participants measuring immediate and longer-term effectiveness.&lt;/li&gt;&lt;li&gt;Findings: lightweight, easy-to-understand interventions can improve deepfake image discernment by up to ~13 percentage points while preserving trust in real images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominique Geissler', 'Claire Robertson', 'Stefan Feuerriegel']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'human-centered defenses', 'digital literacy', 'user study', 'adversarial media']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23492</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders</title><link>https://arxiv.org/abs/2505.16004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates robustness quantification for sparse autoencoder (SAE) concept representations as input-space optimization problems to find adversarial perturbations that alter concept activations.&lt;/li&gt;&lt;li&gt;Develops a comprehensive evaluation framework with realistic scenarios to craft perturbations that manipulate SAE-derived concept labels.&lt;/li&gt;&lt;li&gt;Empirically shows that tiny adversarial input perturbations can reliably manipulate concept-based interpretations while leaving the base LLM activations largely unchanged, indicating fragile concept representations unsuitable for monitoring without additional defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron J. Li', 'Suraj Srinivas', 'Usha Bhalla', 'Himabindu Lakkaraju']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'robustness evaluation', 'attack analysis', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16004</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Provable Differentially Private Computation of the Cross-Attention Mechanism</title><link>https://arxiv.org/abs/2407.14717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel data structure and mechanism that enforces (ε, δ)-differential privacy for cross-attention layers with provable theoretical guarantees.&lt;/li&gt;&lt;li&gt;Provides complexity bounds: space/init Õ(n d r^2) and per-token query time Õ(d r^2), with parameters for polynomial kernel approximation controlling accuracy.&lt;/li&gt;&lt;li&gt;Derives additive and relative error bounds (in terms of n, ε, R, R_w, r, s, ε_s) and proves robustness to adaptive (adversarial) queries.&lt;/li&gt;&lt;li&gt;Positions this as the first provable DP approach tailored to cross-attention, enabling privacy-preserving use in RAG, system prompting, and guided diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yekun Ke', 'Yingyu Liang', 'Zhenmei Shi', 'Zhao Song', 'Jiahao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-attention', 'theoretical-security', 'adaptive-robustness', 'attention-mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.14717</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title><link>https://arxiv.org/abs/2512.06716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and studies Indirect Prompt Injection (IPI) attacks against autonomous LLM agents and argues existing defenses are fragmented and insufficient for full-lifecycle integrity.&lt;/li&gt;&lt;li&gt;Proposes Cognitive Control Architecture (CCA): a dual-layer defense combining a pre-generated 'Intent Graph' for proactive control-flow/data-flow integrity and a 'Tiered Adjudicator' for deep reasoning and multi-dimensional scoring upon deviation detection.&lt;/li&gt;&lt;li&gt;Evaluates CCA on the AgentDojo benchmark, claiming improved robustness, security, and efficiency against sophisticated IPI attacks compared to prior defense methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Liang', 'Tianze Hu', 'Zaiye Chen', 'Mingjie Tang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'agent security', 'runtime defense', 'integrity enforcement', 'red teaming/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06716</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Proof-of-Use: Mitigating Tool-Call Hacking in Deep Research Agents</title><link>https://arxiv.org/abs/2510.10931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a security-relevant failure mode in retrieval-augmented RL agents called "Tool-Call Hacking," where agents maximize surface reward by overusing or fabricating tool calls without grounding reasoning in returned evidence.&lt;/li&gt;&lt;li&gt;Proposes Proof-of-Use (PoU), an evidence-grounded RL framework that enforces auditable citation of normalized evidence identifiers via a fine-grained interaction protocol.&lt;/li&gt;&lt;li&gt;Implements a multi-objective reward design: (1) progressive process rewards for citation validity at intermediate steps, (2) a global Answer–Support Alignment reward enforcing consistency between final answers and retrieved evidence, and (3) a curriculum-style adaptive reward mixing to transition from dense process supervision to sparse outcome objectives.&lt;/li&gt;&lt;li&gt;Empirical results show PoU mitigates tool-call hacking, prevents mode collapse and hallucinated tool usage, and yields emergent, robust tool-usage patterns under domain and tool shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['SHengjie Ma', 'Chenlong Deng', 'Jiaxin Mao', 'Jiadeng Huang', 'Teng Wang', 'Junjie Wu', 'Changwang Zhang', 'Jun wang']&lt;/li&gt;&lt;li&gt;Tags: ['tool-call-hacking', 'agent-robustness', 'reinforcement-learning', 'reward-design', 'evidence-grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10931</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints</title><link>https://arxiv.org/abs/2601.16905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a MoE-specific vulnerability in machine unlearning where routers are manipulated to redirect queries (superficial forgetting) rather than true knowledge erasure from experts.&lt;/li&gt;&lt;li&gt;Proposes GRIP, an algorithm-agnostic adapter that enforces geometric constraints by projecting router gradient updates into expert-specific null spaces, preserving routing stability while forcing knowledge removal from expert parameters.&lt;/li&gt;&lt;li&gt;Demonstrates that GRIP prevents expert selection shifts (&gt;95% routing stability) across unlearning methods and preserves model utility, enabling existing unlearning algorithms to be correctly applied to MoE architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy Zhu', 'Rongzhe Wei', 'Yupu Gu', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'mixture-of-experts', 'defense', 'router vulnerability', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16905</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems</title><link>https://arxiv.org/abs/2601.16890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new class of adversarial attacks on automated fact-checking (AFC) systems by using generative LLMs to rephrase claims with persuasion techniques (15 techniques in 6 categories).&lt;/li&gt;&lt;li&gt;Evaluates effects on both claim verification and evidence retrieval with a decoupled evaluation strategy on FEVER and FEVEROUS benchmarks.&lt;/li&gt;&lt;li&gt;Shows that persuasion-based rephrasings substantially degrade verification and retrieval performance, highlighting a novel vulnerability vector for AFC systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao A. Leite', 'Olesya Razuvayevskaya', 'Kalina Bontcheva', 'Carolina Scarton']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'automated-fact-checking', 'LLM-based-attacks', 'persuasion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16890</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do LLM hallucination detectors suffer from low-resource effect?</title><link>https://arxiv.org/abs/2601.16766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study asking whether hallucination detectors for LLMs suffer from the low-resource (language) effect.&lt;/li&gt;&lt;li&gt;Experiments across five tasks in three domains (factual recall, STEM, Humanities) using four LLMs and three hallucination detectors.&lt;/li&gt;&lt;li&gt;Findings: task accuracy drops substantially in low-resource languages, but detector accuracy drops much less; detectors remain robust within-language and in multilingual setups but fail in cross-lingual transfer without in-language supervision.&lt;/li&gt;&lt;li&gt;Implication: detectors can exploit internal uncertainty signals even in low-resource languages, suggesting potential for robust hallucination detection given in-language supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debtanu Datta', 'Mohan Kishore Chilukuri', 'Yash Kumar', 'Saptarshi Ghosh', 'Muhammad Bilal Zafar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'multilingual robustness', 'LLM safety', 'low-resource languages', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16766</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sycophancy Hides Linearly in the Attention Heads</title><link>https://arxiv.org/abs/2601.16644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that sycophantic (correct-to-incorrect deference) signals are most linearly separable in a sparse subset of middle-layer attention heads.&lt;/li&gt;&lt;li&gt;Trains linear probes across residual stream, MLP, and attention layers and shows steering via probes is most effective when applied to those attention heads.&lt;/li&gt;&lt;li&gt;Demonstrates probe transfer from TruthfulQA to other factual QA benchmarks and shows limited overlap with previously identified 'truthful' directions, implying distinct mechanisms for factual accuracy vs. deference resistance.&lt;/li&gt;&lt;li&gt;Analyzes attention patterns indicating influential heads disproportionately attend to user expressions of doubt, and proposes simple targeted linear interventions to mitigate sycophancy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rifo Genadi', 'Munachiso Nwadike', 'Nurdaulet Mukhituly', 'Hilal Alquabeh', 'Tatsuya Hiraoka', 'Kentaro Inui']&lt;/li&gt;&lt;li&gt;Tags: ['model-interpretability', 'alignment', 'defenses', 'representation-learning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16644</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.16527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies structural fragility in existing unlearning methods: erasure often yields sharp minima where hallucinations quickly resurge after lightweight relearning or parameter shifts.&lt;/li&gt;&lt;li&gt;Proposes SARE, a targeted min-max unlearning framework using Targeted-SAM to flatten the loss landscape around hallucinated concepts, enforcing robust suppression under worst-case parameter perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SARE outperforms baselines in persistent hallucination removal while preserving general generation quality, remaining stable against relearning and parameter updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianya Fang', 'Feiyang Ren', 'Xiang Chen', 'Yu Tian', 'Zhen Bi', 'Haiyang Yu', 'Sheng-Jun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['Unlearning', 'Model Editing', 'Robustness', 'Hallucination Mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16527</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SafeThinker: Reasoning about Risk to Deepen Safety Beyond Shallow Alignment</title><link>https://arxiv.org/abs/2601.16506</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeThinker, an adaptive defense framework that uses a lightweight gateway classifier to route inputs to different safety mechanisms based on assessed risk.&lt;/li&gt;&lt;li&gt;Defines three mechanisms: Standardized Refusal for explicit threats, Safety-Aware Twin Expert (SATE) to intercept deceptive/jailbreak-style attacks, and Distribution-Guided Think (DDGT) to intervene during uncertain generations.&lt;/li&gt;&lt;li&gt;Empirically evaluates against diverse jailbreak strategies and reports significantly reduced attack success rates while maintaining utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianya Fang', 'Xianying Luo', 'Yadong Wang', 'Xiang Chen', 'Yu Tian', 'Zequn Sun', 'Rui Liu', 'Jun Fang', 'Naiqiang Tan', 'Yuanning Cui', 'Sheng-Jun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'jailbreaking', 'runtime safety', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16506</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses</title><link>https://arxiv.org/abs/2601.16473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeMark, a query-free black-box attack that removes defensive watermarks from synthetic images by manipulating latent-space representations via a compressive-sensing sparsification process.&lt;/li&gt;&lt;li&gt;Evaluates DeMark against eight state-of-the-art encoder–decoder watermarking schemes, reducing detection accuracy from 100% to 32.9% on average while preserving visual quality.&lt;/li&gt;&lt;li&gt;Compares and tests three mitigation strategies (image super-resolution, sparse watermarking, adversarial training) and finds them largely ineffective against the proposed attack.&lt;/li&gt;&lt;li&gt;Highlights latent-space vulnerabilities in current watermarking defenses and argues for the need for more robust watermarking designs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Song', 'Zhenchang Xing', 'Liming Zhu', 'Yulei Sui', 'Jingling Xue']&lt;/li&gt;&lt;li&gt;Tags: ['black-box attack', 'watermark removal', 'deepfake security', 'latent-space attack', 'compressive sensing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16473</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NOIR: Privacy-Preserving Generation of Code with Open-Source LLMs</title><link>https://arxiv.org/abs/2601.16354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NOIR, a client-side framework for privacy-preserving LLM-based code generation that encodes prompts into embeddings sent to the cloud and decodes enriched embeddings locally to produce code.&lt;/li&gt;&lt;li&gt;Introduces token-embedding-level local differential privacy, a data-independent randomized tokenizer, and an indistinguishability mechanism to defend against reconstruction and frequency-analysis attacks by an honest-but-curious cloud.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation on open-source LLMs showing strong privacy guarantees with small utility loss (e.g., Evalplus Pass@1 ~76.7/77.4; BigCodeBench Pass@1 38.7, ~1.77% drop).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Khoa Nguyen', 'Khiem Ton', 'NhatHai Phan', 'Issa Khalil', 'Khang Tran', 'Cristian Borcea', 'Ruoming Jin', 'Abdallah Khreishah', 'My T. Thai']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'local differential privacy', 'embedding-level defenses', 'code generation', 'adversarial privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16354</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A New Paradigm for Trusted Respiratory Monitoring Via Consumer Electronics-grade Radar Signals</title><link>https://arxiv.org/abs/2601.16241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies privacy leakage of User-sensitive Identity Information (USI) in consumer-grade radar respiratory monitoring and targets anonymization without degrading respiratory monitoring performance.&lt;/li&gt;&lt;li&gt;Proposes Tru-RM, comprising Attribute Feature Decoupling (AFD) to separate respiratory vs. personal vs. unrelated signal components, Flexible Perturbation Encryptor (FPE) using adversarial-loss/phase-noise based encryption to remove identity features, and a Perturbation Tolerable Network (PTN) to retain robust respiration detection under perturbations.&lt;/li&gt;&lt;li&gt;Evaluates on varied distances, respiratory patterns, and durations, reporting strong anonymization of USI and high accuracy of perturbed respiratory waveform detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Li', 'Jinyang Huang', 'Feng-Qi Cui', 'Meng Wang', 'Peng Zhao', 'Meng Li', 'Dan Guo', 'Meng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-protection', 'signal-anonymization', 'adversarial-encryption', 'radar-sensing', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16241</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models</title><link>https://arxiv.org/abs/2601.16231</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of untargeted, audio-only adversarial attacks on trimodal (audio-video-language) foundation models, proposing six complementary attack objectives targeting encoder representations, cross-modal attention, hidden states, and output likelihoods.&lt;/li&gt;&lt;li&gt;Empirical evaluation across three state-of-the-art models and multiple benchmarks shows high attack success (up to 96%) at low perceptual distortion; attacks transfer poorly across models/encoders, while ASR systems (e.g., Whisper) respond mainly to perturbation magnitude.&lt;/li&gt;&lt;li&gt;Highlights a previously overlooked single-modality attack surface in multimodal systems and motivates defenses that enforce cross-modal consistency and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aafiya Hussain', 'Gaurav Srivastava', 'Alvi Ishmam', 'Zaber Hakim', 'Chris Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'audio adversarial attacks', 'multimodal robustness', 'attack transferability', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16231</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care</title><link>https://arxiv.org/abs/2601.16529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SycoEval-EM, a multi-agent simulation framework that adversarially evaluates LLMs for acquiescence to patient persuasion in emergency medicine scenarios.&lt;/li&gt;&lt;li&gt;Evaluates 20 LLMs across 1,875 multi-turn encounters in three Choosing Wisely scenarios, reporting wide-ranging acquiescence rates (0–100%) and higher vulnerability for imaging requests vs. opioid prescriptions.&lt;/li&gt;&lt;li&gt;Finds that model capability poorly predicts robustness and that all persuasion tactics were similarly effective, arguing static benchmarks are insufficient and advocating multi-turn adversarial testing for clinical AI certification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongshen Peng', 'Yi Wang', 'Carl Preiksaitis', 'Christian Rose']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial testing', 'red teaming', 'safety evaluation', 'social engineering', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.16529</guid><pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
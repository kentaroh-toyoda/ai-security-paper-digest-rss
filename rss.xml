<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 30 Jan 2026 23:45:07 +0000</lastBuildDate><item><title>GUIGuard: Toward a General Framework for Privacy-Preserving GUI Agents</title><link>https://arxiv.org/abs/2601.18842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GUIGuard, a three-stage framework (privacy recognition, privacy protection, task execution under protection) for privacy-preserving GUI agents.&lt;/li&gt;&lt;li&gt;Introduces GUIGuard-Bench: a cross-platform dataset with 630 trajectories and 13,830 screenshots annotated for region-level privacy, risk level, category, and task necessity.&lt;/li&gt;&lt;li&gt;Evaluates existing agents and models, finding privacy recognition is a major bottleneck (e.g., 13.3% accuracy on Android, 1.4% on PC) and shows protection strategies can preserve task semantics while improving privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Wang', 'Zhiling Zhang', 'Wenbo Zhou', 'Weiming Zhang', 'Jie Zhang', 'Qiannan Zhu', 'Yu Shi', 'Shuxin Zheng', 'Jiyan He']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'privacy recognition', 'GUI agents', 'benchmark/dataset', 'privacy defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18842</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2505.19616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses 'modality interference' in multimodal LLMs where spurious signals from non-essential modalities degrade model decisions via causal, perturbation-based experiments.&lt;/li&gt;&lt;li&gt;Proposes a unified finetuning framework combining heuristic and adversarial perturbation-based data augmentation with output-level consistency regularization between original and perturbed inputs.&lt;/li&gt;&lt;li&gt;Shows extensive experiments across image-heavy, text-heavy, and multimodal benchmarks and multiple MLLM architectures/scales, improving unimodal robustness, generalization, and standard multimodal performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Cai', 'Bangzheng Li', 'Xiaofei Wen', 'Muhao Chen', 'Zhe Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-robustness', 'adversarial-augmentation', 'data-augmentation', 'diagnostic-evaluation', 'multimodal-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19616</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols</title><link>https://arxiv.org/abs/2503.06991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that common logit-based unlearning evaluations can give a false sense of security; models may retain representations of forgotten data even when logits/classifier behavior appears changed.&lt;/li&gt;&lt;li&gt;Performs large-scale, representation-based evaluations to test whether unlearning truly removes target data from model feature space.&lt;/li&gt;&lt;li&gt;Introduces a new benchmark scenario where forgetting classes are semantically similar to downstream task classes, forcing a stricter test of representation divergence and realistic unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwoo Kim', 'Sungmin Cha', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'representation leakage', 'benchmark', 'data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.06991</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</title><link>https://arxiv.org/abs/2601.20433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MARE, a multimodal system that aligns vision-language models for explainable deepfake detection and reasoning.&lt;/li&gt;&lt;li&gt;Uses comprehensive reward functions and reinforcement learning from human feedback (RLHF) to produce text-spatially aligned, human-preferred reasoning outputs.&lt;/li&gt;&lt;li&gt;Introduces a forgery disentanglement module to extract intrinsic forgery traces from high-level facial semantics, improving authenticity detection and explainability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Xu', 'Wei Lu', 'Xiangyang Luo', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'vision-language', 'RLHF', 'explainability', 'forgery-disentanglement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20433</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Router: On the Feasibility of Hijacking MLLMs via a Single Adversarial Perturbation</title><link>https://arxiv.org/abs/2511.20002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic-Aware Hijacking: a novel attack that uses a single universal perturbation to hijack decisions of stateless Multimodal LLMs (MLLMs) by routing different input semantics to attacker-defined targets.&lt;/li&gt;&lt;li&gt;Proposes Semantic-Aware Universal Perturbation (SAUP) and the Semantic-Oriented (SORT) optimization, supported by theoretical and empirical analysis of latent-space geometry.&lt;/li&gt;&lt;li&gt;Provides a new fine-grained semantic dataset for evaluation and reports extensive experiments on three representative MLLMs, achieving up to 66% attack success over five targets using a single frame on Qwen.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changyue Li', 'Jiaying Li', 'Youliang Yuan', 'Jiaming He', 'Zhicong Huang', 'Pinjia He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'universal perturbation', 'multimodal attack', 'model hijacking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20002</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models</title><link>https://arxiv.org/abs/2509.18546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEGA: a transferable black-box adversarial attack for No-Reference Image Quality Assessment (NR-IQA) models that ensembles Gaussian-smoothed gradients from source models.&lt;/li&gt;&lt;li&gt;Uses Gaussian smoothing on source-model gradients and ensembling to approximate target-model gradients in a transfer-based black-box setting.&lt;/li&gt;&lt;li&gt;Introduces a perturbation filter mask to remove perceptually inappropriate perturbations, ensuring adversarial changes remain imperceptible.&lt;/li&gt;&lt;li&gt;Demonstrates superior transferability on the CLIVE dataset, enabling effective black-box attacks against NR-IQA models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujia Liu', 'Dingquan Li', 'Zhixuan Li', 'Tiejun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'black-box-attack', 'transferability', 'image-quality-assessment', 'ensemble-gaussian-smoothing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18546</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models</title><link>https://arxiv.org/abs/2504.14224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VLM-OpenXpert, a training-free, inference-time pipeline for source-free open-set domain adaptation of vision-language models to filter unknown-class samples without retraining.&lt;/li&gt;&lt;li&gt;SUFF module extracts a low-rank 'unknown subspace' via SVD on high-confidence unknowns and softly removes projected unknown components from features to preserve semantic geometry.&lt;/li&gt;&lt;li&gt;BGAT module applies a Box-Cox transform and fits a bimodal Gaussian mixture to adaptively estimate thresholds for balancing known-class recognition and unknown-class rejection; outperforms or matches retraining-heavy baselines on multiple benchmarks and backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongguang Li', 'Jindong Li', 'Qi Wang', 'Qianli Xing', 'Runliang Niu', 'Shengsheng Wang', 'Menglin Yang']&lt;/li&gt;&lt;li&gt;Tags: ['open-set detection', 'out-of-distribution detection', 'vision-language models', 'robustness', 'source-free domain adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14224</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CMOOD: Concept-based Multi-label OOD Detection</title><link>https://arxiv.org/abs/2411.13578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COOD (Concept-based OOD), a zero-shot multi-label out-of-distribution detection framework leveraging pre-trained vision-language models.&lt;/li&gt;&lt;li&gt;Uses concept-based label expansion (positive and negative concepts per label) and a novel scoring function to model label dependencies and discriminate OOD samples without extra training.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical performance (~95% average AUROC) on VOC and COCO across varying label counts and OOD types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhendong Liu', 'Yi Nian', 'Yuehan Qin', 'Henry Peng Zou', 'Li Li', 'Xiyang Hu', 'Yue Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'robustness', 'vision-language', 'zero-shot', 'multi-label']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13578</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning</title><link>https://arxiv.org/abs/2601.22020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual-Guided Key-Token Regularization (ViKeR) for unlearning sensitive information in Multimodal Large Language Models (MLLMs).&lt;/li&gt;&lt;li&gt;Uses irrelevant visual inputs to predict desired post-unlearning token-level distributions and regularizes the unlearning process, prioritizing 'key tokens' (defined via information entropy) through token-level gradient reweighting.&lt;/li&gt;&lt;li&gt;Evaluated on MLLMU and CLEAR benchmarks, showing effective unlearning while reducing forgetting and maintaining coherent responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengyi Cai', 'Zesheng Ye', 'Peike Li', 'Bo Han', 'Jianzhong Qi', 'Feng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'unlearning', 'multimodal', 'defense', 'model-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22020</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression</title><link>https://arxiv.org/abs/2601.21531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of large vision-language models (LVLMs) when visual token compression (pruning/merging) is applied, identifying an optimization–inference mismatch in prior attacks.&lt;/li&gt;&lt;li&gt;Proposes CAGE (Compression-AliGnEd attack), which aligns perturbation optimization with compression-aware inference via expected feature disruption and rank distortion alignment without requiring access to the deployed compressor or budget.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that CAGE reduces robust accuracy across multiple plug-and-play compression mechanisms and datasets, showing compression-aware evaluation is necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinwei Zhang', 'Hangcheng Liu', 'Li Bai', 'Hao Wang', 'Qingqing Ye', 'Tianwei Zhang', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness evaluation', 'visual token compression', 'LVLM security', 'attack development']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21531</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer</title><link>https://arxiv.org/abs/2601.21323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether handcrafted feature pipelines (HOG + classical classifiers) are robust to adversarial examples generated from a neural surrogate (VGG16) on CIFAR-10.&lt;/li&gt;&lt;li&gt;Uses FGSM and PGD attacks on the surrogate and measures transfer to KNN, Decision Tree, Linear SVM, Kernel SVM, and a shallow NN across eight HOG settings.&lt;/li&gt;&lt;li&gt;Finds substantial accuracy drops (16.6%–59.1%), showing classical pipelines are vulnerable and that adversarial transfer is not limited to end-to-end differentiable models.&lt;/li&gt;&lt;li&gt;Reports an attack hierarchy reversal: FGSM degrades classical classifiers more than PGD (likely due to PGD overfitting to surrogate-specific features); block normalization gives partial mitigation but is insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Achraf Hsain', 'Ahmed Abdelkader', 'Emmanuel Baldwin Mbaya', 'Hamoud Aljamaan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'transferability', 'classical ML vulnerabilities', 'robustness', 'HOG / feature engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21323</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lossless Copyright Protection via Intrinsic Model Fingerprinting</title><link>https://arxiv.org/abs/2601.21252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TrajPrint, a training-free, lossless framework to verify copyright of diffusion models by extracting intrinsic model fingerprints from deterministic generation trajectories.&lt;/li&gt;&lt;li&gt;Uses a watermarked image as an anchor to trace generation trajectories and a dual-end anchoring optimization to synthesize fingerprint noise that recovers the watermark only on the target model.&lt;/li&gt;&lt;li&gt;Designed to work in black-box API scenarios and robust to model modifications; verification performed via atomic inference and statistical hypothesis testing.&lt;/li&gt;&lt;li&gt;Claims superior robustness and lossless verification without altering model weights or impairing performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lingxiao Chen', 'Liqin Wang', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'copyright protection', 'watermarking', 'black-box model verification', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21252</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</title><link>https://arxiv.org/abs/2601.21900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TraceRouter, a path-level defense that traces and severs causal propagation circuits of malicious semantics in large foundation models rather than targeting isolated neurons.&lt;/li&gt;&lt;li&gt;Method: detect sensitive onset layer via attention divergence, disentangle malicious features with sparse autoencoders and differential activation analysis, and map causal pathways with feature influence scores from zero-out interventions.&lt;/li&gt;&lt;li&gt;Selective suppression of identified causal chains aims to block harmful information flow while preserving orthogonal functionality, improving adversarial robustness vs. utility trade-offs per reported experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuancheng Shi', 'Shangze Li', 'Wenjun Lu', 'Wenhua Wu', 'Cong Wang', 'Zifeng Cheng', 'Fei Shen', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'adversarial robustness', 'model safety', 'circuit-level intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21900</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</title><link>https://arxiv.org/abs/2601.21610</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WMVLM, a unified evaluation framework using vision-language models to assess diffusion-model image watermarking.&lt;/li&gt;&lt;li&gt;Defines distinct metrics for residual watermarks (artifact strength, erasure resistance) and semantic watermarks (latent distribution shifts).&lt;/li&gt;&lt;li&gt;Introduces a three-stage training strategy enabling classification, scoring, and interpretable text generation for watermark evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and generalization across datasets, diffusion models, and watermarking methods compared to SOTA VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijin Yang', 'Yu Sun', 'Kejiang Chen', 'Jiawei Zhao', 'Jun Jiang', 'Weiming Zhang', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'diffusion models', 'vision-language models', 'robustness', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21610</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mining Forgery Traces from Reconstruction Error: A Weakly Supervised Framework for Multimodal Deepfake Temporal Localization</title><link>https://arxiv.org/abs/2601.21458</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RT-DeepLoc, a weakly supervised framework that localizes temporal deepfake forgeries by detecting reconstruction errors from a Masked Autoencoder trained only on authentic data.&lt;/li&gt;&lt;li&gt;Introduces Asymmetric Intra-video Contrastive Loss (AICL) to tighten authentic feature clusters guided by reconstruction cues, improving local discrimination and generalization to unseen forgeries.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art weakly-supervised temporal forgery localization performance on large-scale datasets (e.g., LAV-DF).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Midou Guo', 'Qilin Yin', 'Wei Lu', 'Xiangyang Luo', 'Rui Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'temporal-localization', 'weak-supervision', 'masked-autoencoder', 'contrastive-loss']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21458</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations</title><link>https://arxiv.org/abs/2601.21408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Manifold Projection Fluctuations (MPF): structured residuals in consecutive frames arising from manifold-fitting generation processes.&lt;/li&gt;&lt;li&gt;Proposes a hierarchical dual-path detection framework: (1) Static Manifold Deviation Branch using vision foundation models to catch spatial/off-manifold anomalies, and (2) Micro-Temporal Fluctuation Branch to detect fine-grained temporal fingerprints in high-fidelity on-manifold forgeries.&lt;/li&gt;&lt;li&gt;Targets robust exposure of state-of-the-art AI-generated video forgeries by combining spatial manifold deviation detection with micro-temporal analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinan He', 'Kaiqing Lin', 'Yue Zhou', 'Jiaming Zhong', 'Wei Ye', 'Wenhui Yi', 'Bing Fan', 'Feng Ding', 'Haodong Li', 'Bo Cao', 'Bin Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'video forensics', 'manifold analysis', 'temporal anomaly detection', 'vision foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21408</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence</title><link>https://arxiv.org/abs/2601.21320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies singular boundaries from a semi-discrete optimal transport (OT) mapping between a continuous base distribution and training-data latents as regions of semantic ambiguity where classifiers are overconfident on OOD inputs.&lt;/li&gt;&lt;li&gt;Constructs OT-induced OOD samples (OTIS) by sampling near those OT singular boundaries to produce geometrically grounded, semantically ambiguous OOD examples.&lt;/li&gt;&lt;li&gt;Trains models with a confidence-suppression loss on OTIS to reduce OOD overconfidence and improve calibration; reports empirical gains over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keke Tang', 'Ziyong Du', 'Xiaofei Wang', 'Weilong Peng', 'Peican Zhu', 'Zhihong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution (OOD) robustness', 'defense', 'optimal transport', 'calibration', 'OOD sample generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21320</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models</title><link>https://arxiv.org/abs/2601.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LAMP, a black-box method to learn Universal Adversarial Perturbations (UAPs) specifically targeting multi-image multimodal LLMs (MLLMs).&lt;/li&gt;&lt;li&gt;Proposes novel constraints: an attention-based constraint to disrupt cross-image aggregation, a cross-image contagious constraint to propagate adversarial influence to clean tokens, and an index-attention suppression loss for position-invariant attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that LAMP outperforms state-of-the-art baselines across multiple vision-language tasks and models in attack success rate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alvi Md Ishmam', 'Najibul Haque Sarker', 'Zaber Ibn Abdul Hakim', 'Chris Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'universal adversarial perturbations', 'black-box attack', 'multimodal/vision-language models', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21220</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BadDet+: Robust Backdoor Attacks for Object Detection</title><link>https://arxiv.org/abs/2601.21066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadDet+, a penalty-based backdoor attack framework for object detectors that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA).&lt;/li&gt;&lt;li&gt;Uses a log-barrier penalty to suppress true-class predictions for triggered inputs, achieving position/scale invariance and improved physical-world robustness.&lt;/li&gt;&lt;li&gt;Demonstrates superior synthetic-to-physical transfer on real-world benchmarks while preserving clean performance and provides theoretical analysis of trigger-specific feature subspace behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kealan Dunnett', 'Reza Arablouei', 'Dimity Miller', 'Volkan Dedeoglu', 'Raja Jurdak']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'object detection', 'physical robustness', 'adversarial attack', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21066</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</title><link>https://arxiv.org/abs/2601.00065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a supply-chain backdoor attack via tokenizer transplant: a crafted 'breaker' token that is inert in a donor model but becomes a high-salience malicious feature after vocab/embedding alignment.&lt;/li&gt;&lt;li&gt;Formalizes attack as a dual-objective optimization and implements it with a sparse solver; attack is training-free, evades outlier detection, and persists through fine-tuning and weight merging.&lt;/li&gt;&lt;li&gt;Demonstrates a practical vulnerability in model composition workflows (weight merging, vocabulary expansion, speculative decoding) that can sabotage base-model generation while leaving donor behavior statistically unchanged.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoze Liu', 'Weichen Yu', 'Matt Fredrikson', 'Xiaoqian Wang', 'Jing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'tokenizer_transplant', 'supply-chain_attack', 'model_composition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00065</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated, scalable red-teaming framework for evaluating safety of LLM-based search agents.&lt;/li&gt;&lt;li&gt;Generates 300 test cases across five risk categories (e.g., misinformation, prompt injection) and evaluates three agent scaffolds with 17 LLMs.&lt;/li&gt;&lt;li&gt;Finds substantial vulnerabilities (e.g., up to 90.5% ASR for GPT-4.1-mini) and shows common defenses like reminder prompting have limited effectiveness.&lt;/li&gt;&lt;li&gt;Provides a public codebase and test cases to measure and improve search-agent safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Xun Chen', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'prompt injection', 'misinformation', 'safety evaluation', 'LLM search agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Guided Perturbation Sensitivity (GPS): Detecting Adversarial Text via Embedding Stability and Word Importance</title><link>https://arxiv.org/abs/2508.11667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Guided Perturbation Sensitivity (GPS), an attack-agnostic detection framework that measures embedding sensitivity to masking of important words and uses a BiLSTM to detect adversarial text.&lt;/li&gt;&lt;li&gt;Ranks words by importance (gradient-based and other heuristics), masks top-k words, and uses embedding-change patterns to distinguish adversarially perturbed words from naturally important words.&lt;/li&gt;&lt;li&gt;Evaluated across three datasets, three attack types, and two victim models, achieving &gt;85% detection accuracy and showing strong generalization to unseen datasets/attacks/models without retraining.&lt;/li&gt;&lt;li&gt;Shows gradient-based ranking yields superior perturbation identification (measured by NDCG) and that identification quality correlates with detection performance (ρ = 0.65).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bryan E. Tuck', 'Rakesh M. Verma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text adversarial detection', 'defense', 'embedding stability', 'word importance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11667</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new failure mode in Test-Time Scaling (TTS): reducing candidate diversity (even modestly) makes TTS much more likely to produce unsafe outputs.&lt;/li&gt;&lt;li&gt;Introduces RefDiv, a reference-guided diversity reduction diagnostic attack that constrains candidate diversity to stress-test TTS pipelines.&lt;/li&gt;&lt;li&gt;Empirically demonstrates the effect across multiple open- and closed-source LLMs and TTS strategies (MCTS, Best-of-N), and shows transferability across models and strategies.&lt;/li&gt;&lt;li&gt;Shows that common safety guardrail classifiers (e.g., Llama-Guard) often fail to detect the adversarial prompts produced by RefDiv, indicating limited protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'safety evaluation', 'red teaming', 'guardrail evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups</title><link>https://arxiv.org/abs/2504.06160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates unprompted LLM-generated attacks targeting vulnerable mental health groups using a large-scale bias audit dataset.&lt;/li&gt;&lt;li&gt;Introduces a network-based framework to study propagation and relative bias in attack narrative chains, finding mental health entities occupy central positions (higher closeness centrality, dense clustering).&lt;/li&gt;&lt;li&gt;Assesses stigmatization levels using an established framework and reports increased labeling components for mental-health-related targets along generation chains.&lt;/li&gt;&lt;li&gt;Highlights structural tendencies of LLMs to amplify harmful discourse and underscores need for mitigation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rijul Magu', 'Arka Dutta', 'Sean Kim', 'Ashiqur R. KhudaBukhsh', 'Munmun De Choudhury']&lt;/li&gt;&lt;li&gt;Tags: ['bias in LLMs', 'harmful content generation', 'targeted attacks', 'network analysis', 'stigmatization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06160</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</title><link>https://arxiv.org/abs/2601.21963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes updated threat landscape of LLM- and multimodal-generated misinformation and its effects on information ecosystems.&lt;/li&gt;&lt;li&gt;Introduces practical tools: JudgeGPT (platform for evaluating human perception of AI-generated news) and RogueGPT (controlled stimulus generation), forming an experimental pipeline for studying detection and perception.&lt;/li&gt;&lt;li&gt;Reports findings that detection has improved but generation–detection arms race continues; discusses mitigation strategies including LLM-based detection, inoculation, and dual-use considerations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Loth', 'Martin Kappes', 'Marc-Oliver Pahl']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'generative-ai', 'detection', 'defenses', 'evaluation-tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21963</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens (rather than entire documents) to remove undesired capabilities (evaluated on removing medical capabilities).&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and preserves benign capabilities better than document-level filtering, and becomes more effective with model scale (up to a reported 7000x compute slowdown on the forget domain for largest models).&lt;/li&gt;&lt;li&gt;Introduces methods for labeling tokens using sparse autoencoders and distilling efficient classifiers, and shows filtering can be robust to noisy labels and compatible with subsequent alignment on the forget domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'data-filtering', 'training-time-mitigation', 'capability-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding</title><link>https://arxiv.org/abs/2601.21969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Token-Guard, a token-level hallucination control method that performs self-checking decoding to detect and prevent hallucinated tokens during generation.&lt;/li&gt;&lt;li&gt;Uses internal verification at each reasoning step, evaluates candidate fragments in a latent space with explicit hallucination risk scoring, and applies iterative pruning and regeneration to correct errors.&lt;/li&gt;&lt;li&gt;Demonstrates substantial reduction in hallucinations and improved generation accuracy on HALU datasets, positioning Token-Guard as a lightweight, modular alternative to RAG or RLHF.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Zhu', 'Huiqiang Rong', 'Haoran Luo']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'decoding-based_defense', 'self-checking_decoding', 'token-level_verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21969</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text</title><link>https://arxiv.org/abs/2601.21895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a geometric interpretation of rewrite-based detection methods and introduces a novel rewrite-based detector that adaptively learns a distance metric between original and rewritten texts.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing an adaptively learned distance function is more effective for detection than a fixed distance.&lt;/li&gt;&lt;li&gt;Empirically evaluates the method across 100+ settings and reports large relative improvements (57.8%–80.6%) over the strongest baselines across target LLMs (GPT, Claude, Gemini).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Erhan Xu', 'Kai Ye', 'Ying Yang', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated-text-detection', 'rewrite-based-detection', 'distance-learning', 'defensive-mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21895</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning</title><link>https://arxiv.org/abs/2601.21682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FIT, a continual unlearning framework (Filtering, Importance-aware updates, Targeted layer attribution) to prevent catastrophic forgetting while executing many sequential deletion requests for LLMs.&lt;/li&gt;&lt;li&gt;Introduces PCH benchmark (Personal, Copyright, Harmful) and two symmetric metrics—Forget Degree (F.D.) and Retain Utility (R.U.)—for realistic sequential deletion evaluation.&lt;/li&gt;&lt;li&gt;Evaluates on four open-source LLMs with hundreds of deletion requests, demonstrating improved trade-offs between forgetting effectiveness and utility retention and robustness to relearning and quantization recovery attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Minxin Du', 'Kun Fang', 'Zi Liang', 'Yaxin Xiao', 'Zhicong Huang', 'Cheng Hong', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'privacy', 'defense', 'continual learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21682</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models Naively Recover Ethnicity from Individual Records</title><link>https://arxiv.org/abs/2601.21132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that LLMs can infer sensitive demographic attributes (ethnicity, caste, religious sect) from names with high accuracy, outperforming conventional BISG methods across multiple jurisdictions.&lt;/li&gt;&lt;li&gt;Evaluates multiple models (proprietary and open-source), finds reasoning prompts and auxiliary metadata (e.g., party registration) can boost accuracy, and validates results on voter rolls and administrative records across several countries.&lt;/li&gt;&lt;li&gt;Demonstrates practical scalability: fine-tuning small transformer models on LLM-generated labels yields even higher accuracy and enables local, low-cost deployment.&lt;/li&gt;&lt;li&gt;Implications center on privacy and misuse risks—LLMs enable attribute inference from individual records, reducing some biases of BISG but creating new privacy/vulnerability pathways.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noah Dasanaike']&lt;/li&gt;&lt;li&gt;Tags: ['privacy_inference', 'demographic_inference', 'attribute_inference', 'model_misuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21132</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GAVEL: Towards rule-based safety through activation monitoring</title><link>https://arxiv.org/abs/2601.19768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes rule-based activation monitoring for LLM safety by modeling internal activations as interpretable cognitive elements (CEs) that represent fine-grained behaviors (e.g., 'making a threat').&lt;/li&gt;&lt;li&gt;Defines predicate rules over CEs to detect and block harmful or domain-specific behaviors in real time without retraining detectors or models.&lt;/li&gt;&lt;li&gt;Claims improved precision, compositionality, transparency, and auditability over broad-data activation detectors and announces an open-source framework (GAVEL) and an automated rule-creation tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shir Rozenfeld', 'Rahul Pankajakshan', 'Itay Zloczower', 'Eyal Lenga', 'Gilad Gressel', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['activation monitoring', 'safety defenses', 'rule-based detection', 'interpretability', 'LLM guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19768</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays</title><link>https://arxiv.org/abs/2601.04443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-based framework that textualizes multivariate time-series current measurements from transformer current differential relays (TCDRs) into natural-language prompts for compact, locally deployable LLMs.&lt;/li&gt;&lt;li&gt;Fine-tunes DistilBERT, GPT-2, and DistilBERT+LoRA to distinguish cyberattacks (e.g., false-data injection, time-synchronization attacks) from genuine fault-induced disturbances while preserving fault detection accuracy.&lt;/li&gt;&lt;li&gt;Evaluates against ML/DL baselines under nominal and adversarial scenarios, demonstrating high attack detection rates (up to 97.62% for DistilBERT), robustness to prompt variations and measurement noise, and intrinsic interpretability via attention maps.&lt;/li&gt;&lt;li&gt;Provides the full dataset for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Mohammad Saber', 'Saeed Jafari', 'Zhengmao Ouyang', 'Paul Budnarain', 'Amr Youssef', 'Deepa Kundur']&lt;/li&gt;&lt;li&gt;Tags: ['cyberattack-detection', 'defense', 'LLMs', 'smart-grid', 'time-series']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04443</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?</title><link>https://arxiv.org/abs/2511.05476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaCompress, a metamorphic testing framework that evaluates behavioral fidelity between teacher and student language models of code by applying behavior-preserving transformations and comparing outputs.&lt;/li&gt;&lt;li&gt;Finds that distilled student models often fail to deeply mimic teachers—MetaCompress uncovers up to 62% behavioral discrepancies and students can suffer up to 285% greater performance drops under adversarial attacks compared to teachers.&lt;/li&gt;&lt;li&gt;Evaluates compressed models derived via three distillation methods (Compressor, AVATAR, MORPH) on two code-related tasks, arguing that traditional accuracy metrics miss important robustness and fidelity issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Abdul Awal', 'Mrigank Rochan', 'Chanchal K. Roy']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-distillation', 'adversarial-robustness', 'metamorphic-testing', 'model-compression', 'language-models-of-code']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05476</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Repairing Reward Functions with Feedback to Mitigate Reward Hacking</title><link>https://arxiv.org/abs/2510.13036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Preference-Based Reward Repair (PBRR), an iterative framework that repairs a human-specified proxy reward by learning an additive, transition-dependent correction from human preferences.&lt;/li&gt;&lt;li&gt;Uses targeted exploration and a new preference-learning objective to identify and correct the specific transitions that cause reward hacking, reducing the amount of human feedback needed.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees in tabular domains (cumulative regret comparable to prior preference-based RL) and empirical results showing PBRR outperforms baselines, requiring substantially fewer preferences to recover high-performing policies.&lt;/li&gt;&lt;li&gt;Focuses on mitigating reward-hacking (a form of specification vulnerability) by improving robustness of learned behavior via preference-based defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Hatgis-Kessell', 'Logan Mondal Bhamidipaty', 'Emma Brunskill']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'reinforcement learning', 'preference learning', 'alignment', 'robustness/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13036</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new failure mode in Test-Time Scaling (TTS): reducing candidate diversity (even modestly) makes TTS much more likely to produce unsafe outputs.&lt;/li&gt;&lt;li&gt;Introduces RefDiv, a reference-guided diversity reduction diagnostic attack that constrains candidate diversity to stress-test TTS pipelines.&lt;/li&gt;&lt;li&gt;Empirically demonstrates the effect across multiple open- and closed-source LLMs and TTS strategies (MCTS, Best-of-N), and shows transferability across models and strategies.&lt;/li&gt;&lt;li&gt;Shows that common safety guardrail classifiers (e.g., Llama-Guard) often fail to detect the adversarial prompts produced by RefDiv, indicating limited protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'safety evaluation', 'red teaming', 'guardrail evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Adversarial Robustness of Learning-based Conformal Novelty Detection</title><link>https://arxiv.org/abs/2510.00463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates an oracle worst-case attack model against AdaDetect-based conformal novelty detection and derives an upper bound characterizing the statistical cost (FDR degradation) of attacks.&lt;/li&gt;&lt;li&gt;Proposes a practical, query-only attack scheme applicable to both AdaDetect and one-class classifier-based conformal methods, and combines it with two black-box adversarial algorithms.&lt;/li&gt;&lt;li&gt;Empirically evaluates vulnerabilities on synthetic and real-world datasets, showing that adversarial perturbations can substantially increase false discovery rate while preserving detection power, highlighting limitations of current error-controlled novelty detection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daofu Zhang', 'Mehrdad Pournaderi', 'Hanne M. Clifford', 'Yu Xiang', 'Pramod K. Varshney']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'novelty detection', 'conformal inference', 'black-box attacks', 'false discovery rate (FDR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00463</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Optimal differentially private kernel learning with random projection</title><link>https://arxiv.org/abs/2507.17544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel differentially private kernel ERM algorithm using random projection in RKHS via Gaussian processes to enable private kernel learning.&lt;/li&gt;&lt;li&gt;Shows minimax-optimal excess risk for squared loss and Lipschitz-smooth convex losses under local strong convexity, and derives dimension-free generalization bounds for objective-perturbation private linear ERM.&lt;/li&gt;&lt;li&gt;Demonstrates that alternative dimension-reduction approaches (e.g., random Fourier features, l2 regularization) are suboptimal and provides empirical results supporting the theoretical claims.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bonwoo Lee', 'Cheolwoo Park', 'Jeongyoun Ahn']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy-preserving ML', 'kernel methods', 'defense', 'random projection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17544</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems</title><link>https://arxiv.org/abs/2507.06252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities across the end-to-end text-based Cyber Threat Intelligence (CTI) pipeline to adversarial attacks.&lt;/li&gt;&lt;li&gt;Investigates three attack types—evasion, flooding, and poisoning—with a focus on evasion enabling subsequent attacks.&lt;/li&gt;&lt;li&gt;Demonstrates use of LLM-based fake text generation to produce cybersecurity-like content that misleads classifiers and degrades system performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samaneh Shafee', 'Alysson Bessani', 'Pedro M. Ferreira']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'evasion', 'data poisoning', 'LLM-based attacks', 'cyber threat intelligence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06252</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing Membership Inference Attacks on Diffusion Models from a Frequency-Domain Perspective</title><link>https://arxiv.org/abs/2505.20955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes Membership Inference Attacks (MIAs) on image diffusion models and formalizes existing attacks into a unified paradigm based on membership scoring.&lt;/li&gt;&lt;li&gt;Identifies a deficiency in how diffusion models process high-frequency image content that reduces MIA discrimination and causes systematic misclassification.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing this high-frequency processing deficiency lowers membership advantage of attacks.&lt;/li&gt;&lt;li&gt;Proposes a plug-and-play high-frequency filter module to improve attack performance across datasets and models without added runtime cost; validated experimentally.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Puwei Lian', 'Yujun Cai', 'Songze Li', 'Bingkun Bao']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'diffusion-models', 'frequency-domain', 'attack-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20955</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups</title><link>https://arxiv.org/abs/2504.06160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates unprompted LLM-generated attacks targeting vulnerable mental health groups using a large-scale bias audit dataset.&lt;/li&gt;&lt;li&gt;Introduces a network-based framework to study propagation and relative bias in attack narrative chains, finding mental health entities occupy central positions (higher closeness centrality, dense clustering).&lt;/li&gt;&lt;li&gt;Assesses stigmatization levels using an established framework and reports increased labeling components for mental-health-related targets along generation chains.&lt;/li&gt;&lt;li&gt;Highlights structural tendencies of LLMs to amplify harmful discourse and underscores need for mitigation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rijul Magu', 'Arka Dutta', 'Sean Kim', 'Ashiqur R. KhudaBukhsh', 'Munmun De Choudhury']&lt;/li&gt;&lt;li&gt;Tags: ['bias in LLMs', 'harmful content generation', 'targeted attacks', 'network analysis', 'stigmatization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06160</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CMOOD: Concept-based Multi-label OOD Detection</title><link>https://arxiv.org/abs/2411.13578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COOD (Concept-based OOD), a zero-shot multi-label out-of-distribution detection framework leveraging pre-trained vision-language models.&lt;/li&gt;&lt;li&gt;Uses concept-based label expansion (positive and negative concepts per label) and a novel scoring function to model label dependencies and discriminate OOD samples without extra training.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical performance (~95% average AUROC) on VOC and COCO across varying label counts and OOD types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhendong Liu', 'Yi Nian', 'Yuehan Qin', 'Henry Peng Zou', 'Li Li', 'Xiyang Hu', 'Yue Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'robustness', 'vision-language', 'zero-shot', 'multi-label']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13578</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</title><link>https://arxiv.org/abs/2601.19967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Perturbation-Induced Linearization (PIL), a method to generate unlearnable examples using only linear surrogate models, drastically reducing computation compared to deep-surrogate approaches.&lt;/li&gt;&lt;li&gt;Shows PIL achieves comparable or better effectiveness at preventing models from learning while running much faster, and attributes success to inducing linearization in deep models.&lt;/li&gt;&lt;li&gt;Provides analysis of unlearnable example properties under percentage-based partial perturbation, offering practical insights for data protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinlin Liu', 'Wei Chen', 'Xiaojin Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable examples', 'data poisoning / training-time defense', 'adversarial perturbations', 'surrogate models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19967</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDFLoRA: Selective Decoupled Federated LoRA for Privacy-preserving Fine-tuning with Heterogeneous Clients</title><link>https://arxiv.org/abs/2601.11219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDFLoRA, a federated LoRA framework that decouples each client update into a shared component for aggregation and a private component kept local to handle rank and data heterogeneity.&lt;/li&gt;&lt;li&gt;Only the shared component is aligned and aggregated (and subjected to differential privacy noise), avoiding perturbation of purely local directions and improving utility-privacy trade-offs.&lt;/li&gt;&lt;li&gt;Design stabilizes aggregation under heterogeneous low-rank budgets and makes DP-compatible training more effective by reducing unnecessary noise injection into client-specific components.&lt;/li&gt;&lt;li&gt;Empirical results on multiple benchmarks show SDFLoRA outperforms federated LoRA baselines in both utility and privacy metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhikang Shen', 'Jianrong Lu', 'Haiyuan Wan', 'Jianhai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'LoRA', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11219</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reward-Preserving Attacks For Robust Reinforcement Learning</title><link>https://arxiv.org/abs/2601.07118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'reward-preserving attacks' for adversarial training in RL that adapt perturbation strength so an α fraction of the nominal-to-worst-case return gap remains achievable at each state.&lt;/li&gt;&lt;li&gt;Implements dynamic selection of perturbation magnitude η in deep RL via a learned critic Q((s,a),η) estimating returns under α-reward-preserving rollouts.&lt;/li&gt;&lt;li&gt;Shows adaptive adversarial training with intermediate α yields policies that maintain nominal performance while being robust across a wide range of perturbation magnitudes, outperforming fixed-radius and uniformly sampled-radius baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Schott', 'Elies Gherbi', 'Hatem Hajri', 'Sylvain Lamprier']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'reinforcement-learning', 'robustness', 'adaptive-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.07118</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</title><link>https://arxiv.org/abs/2601.00065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a supply-chain backdoor attack via tokenizer transplant: a crafted 'breaker' token that is inert in a donor model but becomes a high-salience malicious feature after vocab/embedding alignment.&lt;/li&gt;&lt;li&gt;Formalizes attack as a dual-objective optimization and implements it with a sparse solver; attack is training-free, evades outlier detection, and persists through fine-tuning and weight merging.&lt;/li&gt;&lt;li&gt;Demonstrates a practical vulnerability in model composition workflows (weight merging, vocabulary expansion, speculative decoding) that can sabotage base-model generation while leaving donor behavior statistically unchanged.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoze Liu', 'Weichen Yu', 'Matt Fredrikson', 'Xiaoqian Wang', 'Jing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'tokenizer_transplant', 'supply-chain_attack', 'model_composition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00065</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination</title><link>https://arxiv.org/abs/2510.21296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EPHAD, a test-time post-hoc adjustment framework that updates outputs of anomaly detection models trained on contaminated data using evidence gathered at test time.&lt;/li&gt;&lt;li&gt;Combines model priors with external evidence sources (e.g., CLIP, classical AD methods like LOF, domain knowledge) to improve AD performance without access to training pipelines or known contamination rates.&lt;/li&gt;&lt;li&gt;Validated across eight visual AD datasets, twenty-six tabular AD datasets, and an industrial AD dataset; includes ablations on hyperparameters and contamination robustness.&lt;/li&gt;&lt;li&gt;Provides publicly available code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sukanya Patra', 'Souhaib Ben Taieb']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'data-contamination', 'test-time-adaptation', 'robustness', 'multimodal-evidence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21296</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Guided Perturbation Sensitivity (GPS): Detecting Adversarial Text via Embedding Stability and Word Importance</title><link>https://arxiv.org/abs/2508.11667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Guided Perturbation Sensitivity (GPS), an attack-agnostic detection framework that measures embedding sensitivity to masking of important words and uses a BiLSTM to detect adversarial text.&lt;/li&gt;&lt;li&gt;Ranks words by importance (gradient-based and other heuristics), masks top-k words, and uses embedding-change patterns to distinguish adversarially perturbed words from naturally important words.&lt;/li&gt;&lt;li&gt;Evaluated across three datasets, three attack types, and two victim models, achieving &gt;85% detection accuracy and showing strong generalization to unseen datasets/attacks/models without retraining.&lt;/li&gt;&lt;li&gt;Shows gradient-based ranking yields superior perturbation identification (measured by NDCG) and that identification quality correlates with detection performance (ρ = 0.65).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bryan E. Tuck', 'Rakesh M. Verma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text adversarial detection', 'defense', 'embedding stability', 'word importance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11667</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2505.19616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Diagnoses 'modality interference' in multimodal LLMs where spurious signals from non-essential modalities degrade model decisions via causal, perturbation-based experiments.&lt;/li&gt;&lt;li&gt;Proposes a unified finetuning framework combining heuristic and adversarial perturbation-based data augmentation with output-level consistency regularization between original and perturbed inputs.&lt;/li&gt;&lt;li&gt;Shows extensive experiments across image-heavy, text-heavy, and multimodal benchmarks and multiple MLLM architectures/scales, improving unimodal robustness, generalization, and standard multimodal performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Cai', 'Bangzheng Li', 'Xiaofei Wen', 'Muhao Chen', 'Zhe Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-robustness', 'adversarial-augmentation', 'data-augmentation', 'diagnostic-evaluation', 'multimodal-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19616</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols</title><link>https://arxiv.org/abs/2503.06991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that common logit-based unlearning evaluations can give a false sense of security; models may retain representations of forgotten data even when logits/classifier behavior appears changed.&lt;/li&gt;&lt;li&gt;Performs large-scale, representation-based evaluations to test whether unlearning truly removes target data from model feature space.&lt;/li&gt;&lt;li&gt;Introduces a new benchmark scenario where forgetting classes are semantically similar to downstream task classes, forcing a stricter test of representation divergence and realistic unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwoo Kim', 'Sungmin Cha', 'Donghyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'representation leakage', 'benchmark', 'data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.06991</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Federated Learning for Heterogeneous Electronic Health Record Systems with Cost Effective Participant Selection</title><link>https://arxiv.org/abs/2404.13318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EHRFL, a federated learning framework for building host-specific predictive models across heterogeneous EHR systems using text-based EHR modeling to avoid costly data standardization.&lt;/li&gt;&lt;li&gt;Introduces a cost-effective participant selection strategy that selects collaborators based on averaged patient embedding similarity to reduce the number of participants without degrading performance.&lt;/li&gt;&lt;li&gt;Participant selection shares averaged patient embeddings with differential privacy guarantees to protect patient data during selection.&lt;/li&gt;&lt;li&gt;Evaluations on multiple open-source EHR datasets demonstrate reduced cost/time while maintaining predictive performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyoun Kim', 'Junu Kim', 'Kyunghoon Hur', 'Edward Choi']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'participant selection', 'healthcare EHR', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.13318</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fair Graph Machine Learning under Adversarial Missingness Processes</title><link>https://arxiv.org/abs/2311.01591</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an adversarial missingness process that can hide unfairness by causing imputations to make a model appear fairer than it is.&lt;/li&gt;&lt;li&gt;Proposes BFtS, a fair imputation method that targets worst-case fairness via a 3-player adversarial scheme: two adversaries collaborate to maximize bias while a GNN classifier minimizes the maximum bias.&lt;/li&gt;&lt;li&gt;Empirical results on synthetic and real graph datasets show BFtS improves the fairness–accuracy trade-off under adversarial missingness compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debolina Halder Lina', 'Arlei Silva']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'adversarial_missingness', 'defense', 'graph_neural_networks', 'data_imputation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.01591</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning</title><link>https://arxiv.org/abs/2601.21682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FIT, a continual unlearning framework (Filtering, Importance-aware updates, Targeted layer attribution) to prevent catastrophic forgetting while executing many sequential deletion requests for LLMs.&lt;/li&gt;&lt;li&gt;Introduces PCH benchmark (Personal, Copyright, Harmful) and two symmetric metrics—Forget Degree (F.D.) and Retain Utility (R.U.)—for realistic sequential deletion evaluation.&lt;/li&gt;&lt;li&gt;Evaluates on four open-source LLMs with hundreds of deletion requests, demonstrating improved trade-offs between forgetting effectiveness and utility retention and robustness to relearning and quantization recovery attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Minxin Du', 'Kun Fang', 'Zi Liang', 'Yaxin Xiao', 'Zhicong Huang', 'Cheng Hong', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'privacy', 'defense', 'continual learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21682</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise</title><link>https://arxiv.org/abs/2601.21628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that standard noise schedules in diffusion models leave residual semantic signals in the initial/noisy inputs even at maximum noise steps.&lt;/li&gt;&lt;li&gt;Shows fine-tuned diffusion models capture correlations between these residual semantics in the initial noise and original training images.&lt;/li&gt;&lt;li&gt;Proposes a membership inference attack that injects semantic information into the initial noise and infers membership by analyzing the model's generated outputs.&lt;/li&gt;&lt;li&gt;Provides extensive empirical evaluation demonstrating the attack's effectiveness against fine-tuned diffusion models, highlighting a privacy vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Puwei Lian', 'Yujun Cai', 'Songze Li', 'Bingkun Bao']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'diffusion-models', 'privacy', 'training-data-extraction', 'attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21628</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Questioning the Coverage-Length Metric in Conformal Prediction: When Shorter Intervals Are Not Better</title><link>https://arxiv.org/abs/2601.21455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Prejudicial Trick (PT): a procedure that probabilistically returns either a null interval or an interval constructed with an adjusted confidence level to preserve marginal coverage while lowering average interval length.&lt;/li&gt;&lt;li&gt;Shows PT can deceptively improve length metrics while inducing instability (same input can yield very different intervals across runs); derives formal conditions for when PT succeeds and provides empirical evidence across regression and classification tasks.&lt;/li&gt;&lt;li&gt;Proposes a new diagnostic metric, interval stability, to detect whether apparent improvements in interval length arise from PT-like manipulations rather than genuine predictive gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhou Min', 'Yizhou Lu', 'Lanqi Li', 'Zhen Zhang', 'Jiaye Teng']&lt;/li&gt;&lt;li&gt;Tags: ['conformal-prediction', 'evaluation-manipulation', 'robustness', 'attack-detection', 'uncertainty-quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21455</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence</title><link>https://arxiv.org/abs/2601.21320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies singular boundaries from a semi-discrete optimal transport (OT) mapping between a continuous base distribution and training-data latents as regions of semantic ambiguity where classifiers are overconfident on OOD inputs.&lt;/li&gt;&lt;li&gt;Constructs OT-induced OOD samples (OTIS) by sampling near those OT singular boundaries to produce geometrically grounded, semantically ambiguous OOD examples.&lt;/li&gt;&lt;li&gt;Trains models with a confidence-suppression loss on OTIS to reduce OOD overconfidence and improve calibration; reports empirical gains over prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keke Tang', 'Ziyong Du', 'Xiaofei Wang', 'Weilong Peng', 'Peican Zhu', 'Zhihong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution (OOD) robustness', 'defense', 'optimal transport', 'calibration', 'OOD sample generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21320</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings</title><link>https://arxiv.org/abs/2601.20883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoxMorph, a zero-shot voice identity morphing framework that disentangles prosody and timbre into embeddings to synthesize morphed voices from as little as 5 seconds per subject without retraining.&lt;/li&gt;&lt;li&gt;Fuses identity and style embeddings via Spherical Linear Interpolation (Slerp) and generates audio with an autoregressive language model plus a Conditional Flow Matching network.&lt;/li&gt;&lt;li&gt;Demonstrates practical attacks on speaker verification: 67.8% morphing attack success rate under strict security thresholds, with improved audio quality and intelligibility metrics over prior methods.&lt;/li&gt;&lt;li&gt;Highlights significant biometric security implications by offering a scalable, high-fidelity morphing attack pipeline and releasing code/dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bharath Krishnamurthy', 'Ajita Rattani']&lt;/li&gt;&lt;li&gt;Tags: ['voice-morphing', 'biometric-attack', 'speaker-verification', 'adversarial-attack', 'zero-shot']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20883</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Per-parameter Task Arithmetic for Unlearning in Large Language Models</title><link>https://arxiv.org/abs/2601.22030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Per-parameter Task Arithmetic (PerTA) to improve LLM unlearning by rescaling the task vector per parameter to balance forgetting private information and retaining other knowledge.&lt;/li&gt;&lt;li&gt;Introduces two estimators for per-parameter weights: gradient-based (PerTA-grad) and diagonal Fisher information-based (PerTA-fisher).&lt;/li&gt;&lt;li&gt;Shows extensive experiments where PerTA improves over standard task arithmetic and often outperforms training-based unlearning methods in both forgetting effectiveness and model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengyi Cai', 'Zesheng Ye', 'Jiangchao Yao', 'Jianzhong Qi', 'Bo Han', 'Xiaolu Zhang', 'Feng Liu', 'Jun Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'privacy-preserving ML', 'model editing', 'unintended memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22030</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning</title><link>https://arxiv.org/abs/2601.22020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual-Guided Key-Token Regularization (ViKeR) for unlearning sensitive information in Multimodal Large Language Models (MLLMs).&lt;/li&gt;&lt;li&gt;Uses irrelevant visual inputs to predict desired post-unlearning token-level distributions and regularizes the unlearning process, prioritizing 'key tokens' (defined via information entropy) through token-level gradient reweighting.&lt;/li&gt;&lt;li&gt;Evaluated on MLLMU and CLEAR benchmarks, showing effective unlearning while reducing forgetting and maintaining coherent responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengyi Cai', 'Zesheng Ye', 'Peike Li', 'Bo Han', 'Jianzhong Qi', 'Feng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'unlearning', 'multimodal', 'defense', 'model-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.22020</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hardware-Triggered Backdoors</title><link>https://arxiv.org/abs/2601.21902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel backdoor attack that is triggered by small numerical differences across hardware (GPUs) rather than by input perturbations.&lt;/li&gt;&lt;li&gt;Method: shape the model decision boundary near a target input and exploit hardware-specific numerical deviations to flip predictions only on selected hardware.&lt;/li&gt;&lt;li&gt;Empirically demonstrates reliable creation of hardware-triggered backdoors across common GPU accelerators.&lt;/li&gt;&lt;li&gt;Analyzes and experiments with potential defenses to mitigate this new attack vector affecting third-party model use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonas M\\"oller', 'Erik Imgrund', 'Thorsten Eisenhofer', 'Konrad Rieck']&lt;/li&gt;&lt;li&gt;Tags: ['hardware-backdoor', 'backdoor-attacks', 'model-integrity', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21902</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models</title><link>https://arxiv.org/abs/2601.21851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiDAE: a framework that edits frozen foundation-model embeddings using a disentangled dictionary and decodes edits with a diffusion autoencoder to produce diverse, interpretable counterfactual images.&lt;/li&gt;&lt;li&gt;Generates multiple disentangled counterfactuals per instance, faster than gradient-based baselines, enabling scalable, gradient-free counterfactual data augmentation for foundation models.&lt;/li&gt;&lt;li&gt;When combined with Counterfactual Knowledge Distillation (DiDAE-CFKD), the approach reduces shortcut/Clever Hans behavior and improves downstream performance on unbalanced datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sidney Bender', 'Marco Morik']&lt;/li&gt;&lt;li&gt;Tags: ['counterfactual generation', 'robustness', 'shortcut learning / Clever Hans', 'diffusion models', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21851</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.21794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Knowledge Vector Weakening (KVW), a training-free unlearning method that intervenes directly in a pretrained LVLM by identifying and weakening knowledge vectors activated by a forget set.&lt;/li&gt;&lt;li&gt;KVW progressively reduces the contributions of those vectors during generation to prevent the model from exploiting undesirable or private information, avoiding gradient-based optimization.&lt;/li&gt;&lt;li&gt;Empirical results on MLLMU and CLEAR show KVW achieves a stable forget-retain trade-off while being significantly more computationally efficient than gradient-based and LoRA-based unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yejin Kim', 'Dongjun Hwang', 'Sungmin Cha', 'Junsuk Choe']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'model editing', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21794</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LoRA and Privacy: When Random Projections Help (and When They Don't)</title><link>https://arxiv.org/abs/2601.21719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Wishart projection mechanism (randomized map S -&gt; M f(S)) and derives differential privacy (DP) guarantees for vector-valued queries without additive noise.&lt;/li&gt;&lt;li&gt;Shows a sharp negative result for matrix-valued queries: the noise-free projection mechanism is not DP and is vulnerable to a near-perfect membership inference attack (AUC &gt; 0.99).&lt;/li&gt;&lt;li&gt;Analyzes noisy variants, proving privacy amplification from randomness and low-rank projection, and applies results to LoRA-style low-rank fine-tuning—showing LoRA is not inherently private but can be more private than full fine-tuning at the same noise level.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxi Hu', 'Johanna D\\"ungler', 'Bernhard Sch\\"olkopf', 'Amartya Sanyal']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'membership-inference', 'model-fine-tuning', 'privacy-amplification', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21719</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shaping capabilities with token-level data filtering</title><link>https://arxiv.org/abs/2601.21571</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes shaping model capabilities during pretraining by filtering tokens (rather than entire documents) to remove undesired capabilities (evaluated on removing medical capabilities).&lt;/li&gt;&lt;li&gt;Finds token-level filtering is more effective and preserves benign capabilities better than document-level filtering, and becomes more effective with model scale (up to a reported 7000x compute slowdown on the forget domain for largest models).&lt;/li&gt;&lt;li&gt;Introduces methods for labeling tokens using sparse autoencoders and distilling efficient classifiers, and shows filtering can be robust to noisy labels and compatible with subsequent alignment on the forget domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Rathi', 'Alec Radford']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'data-filtering', 'training-time-mitigation', 'capability-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21571</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Representation Unlearning: Forgetting through Information Compression</title><link>https://arxiv.org/abs/2601.21564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Representation Unlearning: learning a transformation over model representations that enforces an information bottleneck to suppress information about data to be forgotten while retaining information about retained data.&lt;/li&gt;&lt;li&gt;Derives variational surrogates to make the unlearning objective tractable and provides practical instantiations for both a supervised setting (retain and forget data available) and a zero-shot setting (only forget data accessible).&lt;/li&gt;&lt;li&gt;Empirical results show improved forgetting reliability, better utility retention, and greater computational efficiency compared to parameter-centric unlearning baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Antonio Almud\\'evar", 'Alfonso Ortega']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'information bottleneck', 'representation learning', 'data deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21564</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Factored Causal Representation Learning for Robust Reward Modeling in RLHF</title><link>https://arxiv.org/abs/2601.21350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a factored causal representation learning approach for reward models that decomposes contextual embeddings into causal (reward-relevant) and non-causal (reward-irrelevant) components, constraining the reward head to use only the causal factors.&lt;/li&gt;&lt;li&gt;Introduces an adversarial head with gradient reversal to prevent non-causal factors from encoding reward information, aiming to mitigate reward hacking caused by spurious features (e.g., length, sycophancy).&lt;/li&gt;&lt;li&gt;Evaluates on mathematical and dialogue tasks, reporting improved robustness of reward models and better downstream RLHF performance compared to state-of-the-art baselines, with analyses showing reductions in length and sycophantic bias exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yupei Yang', 'Lin Yang', 'Wanxi Deng', 'Lin Qu', 'Fan Feng', 'Biwei Huang', 'Shikui Tu', 'Lei Xu']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'RLHF', 'robustness', 'causal representation learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21350</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer</title><link>https://arxiv.org/abs/2601.21323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether handcrafted feature pipelines (HOG + classical classifiers) are robust to adversarial examples generated from a neural surrogate (VGG16) on CIFAR-10.&lt;/li&gt;&lt;li&gt;Uses FGSM and PGD attacks on the surrogate and measures transfer to KNN, Decision Tree, Linear SVM, Kernel SVM, and a shallow NN across eight HOG settings.&lt;/li&gt;&lt;li&gt;Finds substantial accuracy drops (16.6%–59.1%), showing classical pipelines are vulnerable and that adversarial transfer is not limited to end-to-end differentiable models.&lt;/li&gt;&lt;li&gt;Reports an attack hierarchy reversal: FGSM degrades classical classifiers more than PGD (likely due to PGD overfitting to surrogate-specific features); block normalization gives partial mitigation but is insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Achraf Hsain', 'Ahmed Abdelkader', 'Emmanuel Baldwin Mbaya', 'Hamoud Aljamaan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'transferability', 'classical ML vulnerabilities', 'robustness', 'HOG / feature engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21323</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed</title><link>https://arxiv.org/abs/2601.21094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether safety guarantees from training-time safe RL algorithms generalize to deployment under distribution shift using a diabetes-management simulator.&lt;/li&gt;&lt;li&gt;Identifies a safety generalization gap: policies that satisfy constraints during training often violate safety requirements on unseen patient populations.&lt;/li&gt;&lt;li&gt;Proposes test-time shielding (filtering unsafe actions via learned dynamics models) to restore safety, showing consistent improvements in clinical metrics across multiple algorithms and patient groups.&lt;/li&gt;&lt;li&gt;Provides a unified simulator and benchmark for studying safety under distribution shift in safety-critical control domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minjae Kwon', 'Josephine Lamp', 'Lu Feng']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'distribution shift', 'runtime shielding', 'healthcare safety', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21094</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight detector for hallucinations in VQA that leverages internal signals of vision-language models (token-level decoding uncertainty, intermediate visual representations, cross-modal alignment features).&lt;/li&gt;&lt;li&gt;Fuses signals via branch-wise evidence encoding and uncertainty-aware attention and uses an automated, model-dependent supervision strategy (LLM-as-a-Judge extension) to avoid costly human labels.&lt;/li&gt;&lt;li&gt;Demonstrates superior detection effectiveness and efficiency on multiple VQA benchmarks and analyzes how hallucinations stem from variations in perception, cross-modal reasoning, and decoding across VLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'VQA', 'model reliability', 'safety/guardrails', 'LLM-as-judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title><link>https://arxiv.org/abs/2512.19920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes behaviorally calibrated reinforcement learning methods that train LLMs to output calibrated probabilities of correctness and to abstain or flag uncertain claims, reducing hallucinations.&lt;/li&gt;&lt;li&gt;Optimizes strictly proper scoring rules (rather than binary rewards) so models admit uncertainty stochastically and align communicative behavior with accuracy.&lt;/li&gt;&lt;li&gt;Empirical results on Qwen3-4B-Instruct show improved uncertainty quantification and Accuracy-to-Hallucination Ratio gains on in-domain (BeyondAIME) and cross-domain (SimpleQA) evaluations, rivaling larger frontier models in calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayun Wu', 'Jiashuo Liu', 'Zhiyuan Zeng', 'Tianyang Zhan', 'Tianle Cai', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'behavioral_calibration', 'reinforcement_learning', 'uncertainty_quantification', 'abstention/guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.19920</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title><link>https://arxiv.org/abs/2512.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid, production-scale livestream content moderation system combining supervised classifiers for known violations with reference-based similarity matching for novel/subtle cases.&lt;/li&gt;&lt;li&gt;Uses multimodal inputs (text, audio, visual) and leverages a multimodal large language model (MLLM) to distill knowledge into lightweight inference pipelines, improving accuracy while keeping latency low.&lt;/li&gt;&lt;li&gt;Reports production metrics (classification: 67% recall at 80% precision; similarity: 76% recall at 80% precision) and large A/B tests showing a 6–8% reduction in user views of unwanted livestreams.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Chee Yew', 'Hailun Xu', 'Sanjay Saha', 'Xiaotian Fan', 'Hiok Hian Ong', 'David Yuchen Wang', 'Kanchan Sarkar', 'Zhenheng Yang', 'Danhui Guan']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'multimodal-safety', 'MLLM-boosting', 'similarity-matching', 'production-deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03553</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection</title><link>https://arxiv.org/abs/2509.20682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dual-path data-augmented (DPDA) training framework for speech deepfake detection where each utterance is processed both as original and augmented inputs.&lt;/li&gt;&lt;li&gt;Detects and mitigates gradient misalignment between original and augmented inputs by explicitly aligning backpropagated gradient directions to reduce optimization conflicts.&lt;/li&gt;&lt;li&gt;Finds ~25% of iterations exhibit gradient conflicts with RawBoost augmentation; gradient alignment speeds convergence and yields up to 18.69% relative EER reduction on the In-the-Wild dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duc-Tuan Truong', 'Tianchi Liu', 'Junjie Li', 'Ruijie Tao', 'Kong Aik Lee', 'Eng Siong Chng']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'training defenses', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20682</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mask-GCG, a method that uses learnable token masking to identify and prune low-impact tokens in adversarial suffixes used for jailbreak attacks on LLMs.&lt;/li&gt;&lt;li&gt;Shows pruning reduces gradient space and computational cost while maintaining attack success rate (ASR) when applied to Greedy Coordinate Gradient (GCG) and its variants.&lt;/li&gt;&lt;li&gt;Empirically analyzes token redundancy in optimized suffixes, providing insights into efficient and interpretable jailbreak attack construction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Mu', 'Zonghao Ying', 'Zhekui Fan', 'Zonglei Jing', 'Yaoyuan Zhang', 'Zhengmin Yu', 'Wenxin Zhang', 'Quanchen Zou', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak attacks', 'adversarial attacks', 'prompt engineering', 'attack optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06350</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection</title><link>https://arxiv.org/abs/2505.16512</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DigiFakeAV, a large-scale multimodal diffusion-based digital human deepfake dataset (60,000 videos, 8.4M frames) covering diverse demographics and real-world scenarios.&lt;/li&gt;&lt;li&gt;Shows that human participants misidentify fakes at a high rate (68%) and that existing detectors suffer substantial performance drops on this dataset.&lt;/li&gt;&lt;li&gt;Proposes DigiShield, a spatiotemporal and cross-modal fusion detection baseline combining 3D video features with semantic-acoustic audio features, achieving SOTA on DigiFakeAV and good generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Liu', 'Jia Wang', 'Saihui Hou', 'Min Ren', 'Huijia Wu', 'Long Ma', 'Renwang Pei', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'dataset', 'multimodal', 'diffusion models', 'audio-visual forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16512</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models</title><link>https://arxiv.org/abs/2504.19373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel privacy vulnerability where multimodal large reasoning models (MLRMs) can infer sensitive geolocation information (doxing) from user images.&lt;/li&gt;&lt;li&gt;Introduces DoxBench, a curated dataset of 500 real-world images across privacy scenarios, and evaluates 11 advanced MLRMs/MLLMs, showing models outperform non-expert humans at geolocation inference.&lt;/li&gt;&lt;li&gt;Analyzes root causes (models leveraging visual clues plus world knowledge and lacking suppression mechanisms) and presents GeoMiner, a two-stage collaborative attack (clue extraction + reasoning) to enhance geolocation inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weidi Luo', 'Tianyu Lu', 'Qiming Zhang', 'Xiaogeng Liu', 'Bin Hu', 'Yue Zhao', 'Jieyu Zhao', 'Song Gao', 'Patrick McDaniel', 'Zhen Xiang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['geolocation-inference', 'privacy-leakage', 'doxing', 'attack-framework', 'dataset-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.19373</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning</title><link>https://arxiv.org/abs/2502.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study applying machine unlearning to mitigate sensitive information leakage in Large Language Models for Code (LLMs4Code).&lt;/li&gt;&lt;li&gt;Constructs a benchmark with a synthetic forget set (personal info) and a retain set to measure preservation of code-generation capability after unlearning.&lt;/li&gt;&lt;li&gt;Evaluates three unlearning methods (GA, GA+GD, GA+KL) on three open-source code LLMs, showing &gt;50% reduction in direct memorization leakage while retaining ~91% of code-generation performance on average.&lt;/li&gt;&lt;li&gt;Identifies a persistent shift from direct to indirect leakage after unlearning, highlighting a remaining vulnerability that current unlearning techniques do not eliminate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shanzhi Gu', 'Zhaoyang Qu', 'Ruotong Geng', 'Mingyang Geng', 'Shangwen Wang', 'Chuanfu Xu', 'Haotian Wang', 'Zhipeng Lin', 'Dezun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'machine-unlearning', 'model-memorization', 'defense', 'code-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05739</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning</title><link>https://arxiv.org/abs/2501.19180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety Chain-of-Thought (SCoT): uses LLMs' reasoning to proactively assess intent of inputs and augment refusal training.&lt;/li&gt;&lt;li&gt;Aims to improve generalization to out-of-distribution and adversarial jailbreak queries by generating detailed, rule-specific refusals.&lt;/li&gt;&lt;li&gt;Reports comparative evaluations showing SCoT outperforms existing defenses while maintaining core model capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianglin Yang', 'Gelei Deng', 'Jieming Shi', 'Tianwei Zhang', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'safety', 'chain-of-thought', 'adversarial robustness', 'refusal training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19180</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Privacy Risks of Sharpness Aware Minimization</title><link>https://arxiv.org/abs/2310.00488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical finding: Sharpness-Aware Minimization (SAM) is more susceptible to membership inference attacks than SGD across multiple datasets and attack methods, despite yielding lower test error.&lt;/li&gt;&lt;li&gt;Analysis: SAM tends to capture atypical subpatterns and produce higher memorization scores; simultaneously it lowers variance in prediction confidences on unseen samples, which amplifies membership signals.&lt;/li&gt;&lt;li&gt;Theory: In a perfectly interpolating linear regime, sharpness regularization provably reduces prediction variance, leading to a higher MIA advantage for confidence-based and likelihood-ratio attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young In Kim', 'Andrea Agiollo', 'Pratiksha Agrawal', 'Johannes O. Royset', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'model-memorization', 'sharpness-aware-minimization', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00488</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations</title><link>https://arxiv.org/abs/2510.26905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Cognition Envelopes' to bound and constrain reasoning of LLMs and VLMs in cyber-physical/autonomous UAS contexts to reduce hallucinations, overgeneralization, and context misalignment.&lt;/li&gt;&lt;li&gt;Positions Cognition Envelopes as complementary to meta-cognition and traditional safety envelopes, providing reasoning boundaries rather than purely physical or control limits.&lt;/li&gt;&lt;li&gt;Emphasizes need for practical guidelines and systematic processes for defining, validating, and assuring these envelopes to ensure safe autonomous decision-making.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedro Antonio Alarcon Granadeno', 'Arturo Miguel Bernal Russell', 'Sofia Nelson', 'Demetrius Hernandez', 'Maureen Petterson', 'Michael Murphy', 'Walter J. Scheirer', 'Jane Cleland-Huang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'guardrails', 'hallucination mitigation', 'model assurance', 'bounded reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26905</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GNN Explanations that do not Explain and How to find Them</title><link>https://arxiv.org/abs/2601.20815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a critical failure mode in self-explainable GNNs where provided explanations can be unrelated to the model's actual decision process (degenerate explanations).&lt;/li&gt;&lt;li&gt;Shows degenerate explanations can be both maliciously implanted (to hide use of sensitive attributes) and arise naturally, while many existing faithfulness metrics fail to detect them.&lt;/li&gt;&lt;li&gt;Proposes a novel faithfulness metric that reliably flags degenerate explanations in both adversarial and natural settings and provides empirical validation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steve Azzolin', 'Stefano Teso', 'Bruno Lepri', 'Andrea Passerini', 'Sagar Malhotra']&lt;/li&gt;&lt;li&gt;Tags: ['explainability attacks', 'model auditing', 'privacy leakage', 'attack detection', 'graph neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20815</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Contextual Runtime Monitors for Safe AI-Based Autonomy</title><link>https://arxiv.org/abs/2601.20666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes safe controller ensembles as a contextual runtime monitoring problem: a monitor observes context and selects the best controller rather than averaging outputs.&lt;/li&gt;&lt;li&gt;Learns the monitor using methods from contextual multi-armed bandits to exploit controller specialization across different operating contexts.&lt;/li&gt;&lt;li&gt;Claims theoretical safety guarantees for controller selection and demonstrates improved safety and performance in two simulated autonomous driving scenarios versus non-contextual baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alejandro Luque-Cerpa', 'Mengyuan Wang', 'Emil Carlsson', 'Sanjit A. Seshia', 'Devdatt Dubhashi', 'Hazem Torfah']&lt;/li&gt;&lt;li&gt;Tags: ['runtime monitoring', 'safe autonomy', 'contextual bandits', 'controller selection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20666</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</title><link>https://arxiv.org/abs/2601.20642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization in diffusion-based image generative models, showing norm-based detection metrics rely on an isotropic log-probability assumption and break down in low-noise (anisotropic) regimes.&lt;/li&gt;&lt;li&gt;Proposes a new memorization detection metric that combines isotropic norm measures with anisotropic angular alignment between guidance and unconditional scores; this metric can be computed on pure noise via two forward passes (conditional and unconditional), avoiding costly denoising.&lt;/li&gt;&lt;li&gt;Evaluated on Stable Diffusion v1.4 and v2, the method outperforms prior denoising-free detectors and is approximately 5× faster than the previous best approach; also demonstrates a mitigation strategy that adapts prompts based on the metric to reduce memorization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Asthana', 'Vasileios Belagiannis']&lt;/li&gt;&lt;li&gt;Tags: ['memorization-detection', 'privacy-training-data-leakage', 'diffusion-models', 'defense-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20642</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Audio Deepfake Detection in the Age of Advanced Text-to-Speech models</title><link>https://arxiv.org/abs/2601.20510</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative evaluation of three state-of-the-art TTS models (Dia2, Maya1, MeloTTS) with 12,000 synthetic audio samples.&lt;/li&gt;&lt;li&gt;Benchmarks four detection frameworks (semantic, structural, signal-level) and shows detectors' performance varies substantially across TTS architectures, notably against LLM-based synthesis.&lt;/li&gt;&lt;li&gt;Demonstrates a multi-view detection approach that combines complementary analysis levels to achieve robust detection across all evaluated models.&lt;/li&gt;&lt;li&gt;Concludes single-paradigm detectors are insufficient and advocates integrated detection strategies to address evolving audio deepfake threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robin Singh', 'Aditya Yogesh Nair', 'Fabio Palumbo', 'Florian Barbaro', 'Anna Dyka', 'Lohith Rachakonda']&lt;/li&gt;&lt;li&gt;Tags: ['audio-deepfake-detection', 'TTS-adversaries', 'multi-view-defense', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20510</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self Voice Conversion as an Attack against Neural Audio Watermarking</title><link>https://arxiv.org/abs/2601.20432</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates self voice conversion as a novel, content-preserving attack on neural audio watermarking systems.&lt;/li&gt;&lt;li&gt;Shows that remapping a speaker's voice to the same identity while altering acoustic characteristics significantly degrades robustness of state-of-the-art watermarking methods.&lt;/li&gt;&lt;li&gt;Highlights a gap in current robustness evaluations (which focus on conventional distortions) and emphasizes security implications and the need for defenses against ML-based audio attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yigitcan \\"Ozer', 'Wanying Ge', 'Zhe Zhang', 'Xin Wang', 'Junichi Yamagishi']&lt;/li&gt;&lt;li&gt;Tags: ['audio-watermarking', 'voice-conversion-attack', 'adversarial-attack', 'robustness', 'audio-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20432</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multimodal Multi-Agent Ransomware Analysis Using AutoGen</title><link>https://arxiv.org/abs/2601.20346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multimodal, multi-agent framework that fuses static, dynamic, and network features for ransomware family classification.&lt;/li&gt;&lt;li&gt;Each modality is processed by specialized agents (autoencoder-based feature extractors) and integrated via a fusion agent; a transformer classifier performs family-level prediction.&lt;/li&gt;&lt;li&gt;Introduces an inter-agent feedback mechanism that refines representations by suppressing low-confidence information and uses confidence-aware abstention for safer deployment.&lt;/li&gt;&lt;li&gt;Evaluated on large ransomware/benign datasets, reporting substantial Macro-F1 and calibration improvements over single-modality and nonadaptive fusion baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asifullah Khan', 'Aimen Wadood', 'Mubashar Iqbal', 'Umme Zahoora']&lt;/li&gt;&lt;li&gt;Tags: ['ransomware', 'malware-detection', 'multimodal-ml', 'defensive-ml', 'agent-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20346</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization</title><link>https://arxiv.org/abs/2601.20301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Compression-aware Sharpness Minimization (C-SAM), which perturbs pruning masks during training to promote flatness of the loss landscape with respect to model structure rather than parameter perturbations.&lt;/li&gt;&lt;li&gt;Aims to find pruning patterns that preserve model compactness while improving robustness to input variations and structural changes from pruning.&lt;/li&gt;&lt;li&gt;Evaluated on image classification tasks (CelebA-HQ, Flowers-102, CIFAR-10-C) across ResNet-18, GoogLeNet, MobileNet-V2, showing up to ~42% improvement in certified robustness versus baselines while maintaining accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialuo He', 'Huangxun Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model compression', 'pruning', 'training-time defenses', 'certified robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20301</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title><link>https://arxiv.org/abs/2601.20299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces peer prediction (mechanism-design based) as a way to evaluate and train LLMs using weak/no ground-truth supervision by rewarding mutual predictability to incentivize honest, informative answers.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical validation (models up to 405B), showing peer-prediction-based training can restore truthfulness after malicious fine-tuning even when the reward model is much smaller.&lt;/li&gt;&lt;li&gt;Identifies an inverse-scaling property: peer prediction becomes more resistant to deception as the capability gap between judge/reward models and target models grows, unlike LLM-as-a-Judge which fails when judges are much smaller.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Alex Qiu', 'Micah Carroll', 'Cameron Allen']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'robust-evaluation', 'deception-resistance', 'incentive-design', 'training-for-truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20299</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery</title><link>https://arxiv.org/abs/2601.20193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a meta-cognitive reinforcement learning framework that maintains a meta-trust variable based on Value Prediction Error Stability (VPES) to assess the reliability of the agent's learning signals.&lt;/li&gt;&lt;li&gt;Uses fail-safe regulation and gradual trust recovery to modulate learning dynamics when rewards are corrupted, enabling the agent to recover from unreliable experiences.&lt;/li&gt;&lt;li&gt;Evaluated on continuous-control benchmarks with reward corruption, showing higher average returns and substantially fewer late-stage training failures compared to strong robustness baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng Zhang', 'Wenting Ma', 'Kai Li', 'Meng Guo', 'Lei Yang', 'Wei Yu', 'Hongji Cui', 'Yichen Zhang', 'Mo Zhang', 'Jinzhe Lin', 'Zhenjie Yao']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reinforcement learning', 'reward corruption', 'defense', 'meta-cognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20193</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIA) specifically targeting Diffusion Language Models (DLMs).&lt;/li&gt;&lt;li&gt;Proposes SAMA (Subset-Aggregated Membership Attack): samples many masked-token subsets across densities and uses sign-based, inverse-weighted aggregation to robustly detect sparse memorization.&lt;/li&gt;&lt;li&gt;Empirical evaluation on nine datasets shows up to 30% relative AUC improvement over baselines and up to 8x gains at low false positive rates, revealing significant privacy vulnerabilities and motivating tailored defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Kaiyuan Zhang', 'Yuntao Du', 'Edoardo Stoppa', 'Charles Fleming', 'Ashish Kundu', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'diffusion-language-models', 'attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20125</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</title><link>https://arxiv.org/abs/2601.20103</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy of reward exploits (54 categories) and TRACE, a human-verified benchmark of 517 testing trajectories for detecting reward hacking in code-based RL environments.&lt;/li&gt;&lt;li&gt;Evaluates LLM-based detectors in isolated classification vs. contrastive anomaly detection setups, showing contrastive setups improve detection (e.g., GPT-5.2 highest reasoning: 63% vs 45%).&lt;/li&gt;&lt;li&gt;Finds models struggle more with semantically contextualized reward hacks than syntactic ones; includes qualitative analyses and ablations on benign:hacked ratios and cluster sizes; benchmark and harness are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshan Deshpande', 'Anand Kannappan', 'Rebecca Qian']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'reinforcement learning security', 'anomaly detection', 'benchmark/dataset', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.20103</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</title><link>https://arxiv.org/abs/2601.19967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Perturbation-Induced Linearization (PIL), a method to generate unlearnable examples using only linear surrogate models, drastically reducing computation compared to deep-surrogate approaches.&lt;/li&gt;&lt;li&gt;Shows PIL achieves comparable or better effectiveness at preventing models from learning while running much faster, and attributes success to inducing linearization in deep models.&lt;/li&gt;&lt;li&gt;Provides analysis of unlearnable example properties under percentage-based partial perturbation, offering practical insights for data protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinlin Liu', 'Wei Chen', 'Xiaojin Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable examples', 'data poisoning / training-time defense', 'adversarial perturbations', 'surrogate models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19967</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models</title><link>https://arxiv.org/abs/2601.19956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoxPrivacy, the first benchmark to evaluate interactional privacy in speech-language models (SLMs), with three difficulty tiers and a 32-hour bilingual dataset plus a human-recorded Real-VoxPrivacy subset.&lt;/li&gt;&lt;li&gt;Evaluates nine SLMs and finds widespread vulnerabilities: open-source models often perform near random on conditional privacy decisions and many models fail at proactive privacy inference.&lt;/li&gt;&lt;li&gt;Demonstrates a mitigation path by fine-tuning on a new 4,000-hour training set to improve privacy-preserving behavior while retaining robustness; releases benchmark, training data, and a fine-tuned model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiang Wang', 'Hongyu Liu', 'Dekun Chen', 'Xueyao Zhang', 'Zhizheng Wu']&lt;/li&gt;&lt;li&gt;Tags: ['interactional-privacy', 'speech-language-models', 'privacy-benchmark', 'adversarial-evaluation', 'defense-fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19956</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data</title><link>https://arxiv.org/abs/2601.19936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gap-K%, a method for detecting whether text was included in an LLM's pretraining data by leveraging the log-probability gap between the model's top-1 predicted token and the actual target token.&lt;/li&gt;&lt;li&gt;Incorporates a sliding-window strategy to capture local token correlations and reduce per-token noise, motivated by optimization dynamics and gradient signals during next-token prediction training.&lt;/li&gt;&lt;li&gt;Evaluated on WikiMIA and MIMIR benchmarks, showing state-of-the-art performance across model sizes and input lengths for pretraining data detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseo Kwak', 'Jaehyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['pretraining data detection', 'membership inference', 'privacy', 'model auditing', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19936</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text</title><link>https://arxiv.org/abs/2601.19913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LREAD, a rubric based on Korean national writing standards targeting micro-level artifacts to help humans detect LLM-generated Korean text.&lt;/li&gt;&lt;li&gt;Runs a three-phase longitudinal blind protocol with linguistics students, showing calibration increases majority-vote accuracy from 60% to 100% and Fleiss' kappa from -0.09 to 0.82.&lt;/li&gt;&lt;li&gt;Finds that calibrated humans rely on language-specific micro-diagnostics that automated detectors miss, and releases the rubric plus a taxonomy of detection signatures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['human detection', 'LLM-generated text detection', 'rubric-based calibration', 'non-English (Korean)', 'interpretable detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19913</guid><pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
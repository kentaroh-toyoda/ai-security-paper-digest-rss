<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Sat, 10 Jan 2026 00:09:45 +0000</lastBuildDate><item><title>Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</title><link>https://arxiv.org/abs/2601.03714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether DeepSeek-OCR's performance is driven by genuine OCR capability or by reliance on language priors via sentence- and word-level semantic corruption.&lt;/li&gt;&lt;li&gt;Finds performance drops from ~90% to ~20% without linguistic support, showing heavy dependence on language priors and increased hallucination risk at lower visual token counts.&lt;/li&gt;&lt;li&gt;Benchmarks DeepSeek-OCR against 13 baselines, showing traditional pipeline OCRs are more robust to semantic perturbations than end-to-end methods.&lt;/li&gt;&lt;li&gt;Performs context stress testing that indicates model collapse around 10,000 text tokens, suggesting optical compression may worsen long-context issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Liang', 'Ruixuan Ying', 'Bo Li', 'Hong Li', 'Kai Yan', 'Qingwen Li', 'Min Yang', 'Okamoto Satoshi', 'Zhe Cui', 'Shiwen Ni']&lt;/li&gt;&lt;li&gt;Tags: ['OCR robustness', 'hallucination / language priors', 'vision-text compression', 'long-context evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03714</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box jailbreak attack on large vision-language models using Zeroth-Order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;ZO-SPSA is gradient-free (requires only input-output queries), model-agnostic (no surrogate model), and more resource-efficient than white-box methods.&lt;/li&gt;&lt;li&gt;Reports high empirical success: 83.0% attack success rate on InstructBLIP and strong transferability (64.18% ASR) from adversarial examples generated on MiniGPT-4 to other LVLMs.&lt;/li&gt;&lt;li&gt;Concludes that current LVLM safety mechanisms are vulnerable in realistic black-box scenarios, highlighting real-world feasibility of such jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'black-box attacks', 'vision-language models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Safeguarded Text-to-Image Models via Large Language Models</title><link>https://arxiv.org/abs/2503.01839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AttackLLM, a fine-tuned large language model that generates adversarial prompts to jailbreak text-to-image models with safety guardrails.&lt;/li&gt;&lt;li&gt;Unlike query-based attacks, AttackLLM generates bypassing prompts efficiently after fine-tuning and is evaluated on three unsafe-prompt datasets against five safety guardrails.&lt;/li&gt;&lt;li&gt;Demonstrates superior bypass success compared to existing no-box attacks and shows the method can facilitate other query-based attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Yuepeng Hu', 'Yuchen Yang', 'Yinzhi Cao', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM red teaming', 'adversarial prompting', 'safety guardrails', 'prompt-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01839</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning</title><link>https://arxiv.org/abs/2601.04118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GeoReason to reduce logical hallucinations in remote sensing vision-language models by aligning internal reasoning with final answers.&lt;/li&gt;&lt;li&gt;Introduces GeoReason-Bench: 4,000 logic-driven reasoning trajectories generated from geometric primitives and expert knowledge for training/evaluation.&lt;/li&gt;&lt;li&gt;Two-stage training: Supervised Knowledge Initialization (teach reasoning syntax/domain expertise) followed by Consistency-Aware Reinforcement Learning with a novel Logical Consistency Reward and option permutation penalty.&lt;/li&gt;&lt;li&gt;Claims substantial gains in deductive reliability, interpretability, and state-of-the-art performance on the proposed benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenshuai Li', 'Xiantai Xiang', 'Zixiao Wen', 'Guangyao Zhou', 'Ben Niu', 'Feng Wang', 'Lijia Huang', 'Qiantong Wang', 'Yuxin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'logical consistency', 'reinforcement learning', 'vision-language models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04118</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices</title><link>https://arxiv.org/abs/2601.04912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes privacy risks in federated learning for computer vision on edge devices, considering threats from both the central server and malicious clients.&lt;/li&gt;&lt;li&gt;Evaluates and discusses defenses including homomorphic encryption, gradient compression, gradient noising, split learning, swarm learning, and fully encrypted models.&lt;/li&gt;&lt;li&gt;Empirically measures the impact of gradient compression and noising on CNN classification accuracy and shows difficulty of data reconstruction for segmentation networks.&lt;/li&gt;&lt;li&gt;Provides a proof-of-concept implementation simulating federated learning on NVIDIA Jetson TX2 edge hardware.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Damian Haren\\v{c}\\'ak", "Luk\\'a\\v{s} Gajdo\\v{s}ech", 'Martin Madaras']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy', 'model-inversion', 'homomorphic-encryption', 'edge-devices']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04912</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>V-FAT: Benchmarking Visual Fidelity Against Text-bias</title><link>https://arxiv.org/abs/2601.04897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces V-FAT, a diagnostic benchmark (4,026 VQA instances across six semantic domains) to measure reliance of MLLMs on linguistic priors (Text Bias) versus true visual grounding.&lt;/li&gt;&lt;li&gt;Proposes a Three-Level Evaluation (L1 internal corpus bias via atypical images, L2 external instruction bias via misleading prompts, L3 synergistic bias combining both) and a Visual Robustness Score (VRS) that penalizes lucky linguistic guesses.&lt;/li&gt;&lt;li&gt;Evaluates 12 leading MLLMs and shows significant visual collapse under strong linguistic dominance despite strong performance on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziteng Wang', 'Yujie He', 'Guanliang Li', 'Siqi Yang', 'Jiaqi Xiong', 'Songxiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'robustness', 'alignment', 'safety-evaluation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04897</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation</title><link>https://arxiv.org/abs/2601.04692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a few-shot multimodal agent framework that jointly addresses detection, explanation, and intervention for hateful memes.&lt;/li&gt;&lt;li&gt;Leverages generative multimodal models and task-specific agents to operate under limited annotated data, aiming for generalizability across meme types.&lt;/li&gt;&lt;li&gt;Emphasizes explainability and pre-posting intervention alongside detection to better reflect real-world moderation workflows and potential deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naquee Rizwan', 'Subhankar Swain', 'Paramananda Bhaskar', 'Gagan Aryan', 'Shehryaar Shah Khan', 'Animesh Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'multimodal models', 'safety', 'few-shot learning', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04692</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</title><link>https://arxiv.org/abs/2601.05201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled object-counting setup to study prompt-induced hallucination (PIH) in vision-language models, where prompts overstate object counts and the model may favor the prompt over visual evidence.&lt;/li&gt;&lt;li&gt;Performs mechanistic analysis across three VLMs and identifies a small set of attention heads (PIH-heads) whose ablation reduces prompt-induced hallucinations by at least ~40% without retraining.&lt;/li&gt;&lt;li&gt;Characterizes model-specific ways PIH-heads mediate prompt copying and shows ablation increases model corrections toward visual evidence, yielding insights useful for defenses and interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Rudman', 'Michal Golovanevsky', 'Dana Arad', 'Yonatan Belinkov', 'Ritambhara Singh', 'Carsten Eickhoff', 'Kyle Mahowald']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'prompt injection', 'mechanistic interpretability', 'vision-language models', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05201</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</title><link>https://arxiv.org/abs/2601.05159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Vision-Language Introspection (VLI), a training-free, inference-time framework for diagnosing and reducing object hallucinations in multimodal LLMs.&lt;/li&gt;&lt;li&gt;Attributive Introspection detects probabilistic conflicts and localizes causal visual anchors to identify hallucination risks.&lt;/li&gt;&lt;li&gt;Interpretable Bi-Causal Steering dynamically isolates visual evidence from noise and adaptively calibrates confidence during generation.&lt;/li&gt;&lt;li&gt;Reports improvements: 12.67% reduction in object hallucination on MMHal-Bench and 5.8% accuracy gain on POPE.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuliang Liu', 'Songbo Yang', 'Dong Fang', 'Sihang Jia', 'Yuqi Tang', 'Lingfeng Su', 'Ruoshui Peng', 'Yibo Yan', 'Xin Zou', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'MLLM safety', 'introspection', 'inference-time steering', 'calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05159</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Higher-Order Adversarial Patches for Real-Time Object Detectors</title><link>https://arxiv.org/abs/2601.04991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies higher-order adversarial patches—iteratively trained patches produced via repeated attack/hardening cycles—against real-time object detectors (YOLOv10).&lt;/li&gt;&lt;li&gt;Finds higher-order patches generalize better across detectors and are more effective than lower-order patches at evasion.&lt;/li&gt;&lt;li&gt;Shows that adversarial training alone is insufficient to fully harden object detectors against these higher-order patch attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jens Bayer', 'Stefan Becker', 'David M\\"unch', 'Michael Arens', 'J\\"urgen Beyerer']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patches', 'evasion-attacks', 'adversarial-training', 'object-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04991</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics</title><link>https://arxiv.org/abs/2601.04946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'prototypicality bias' in multimodal evaluation metrics where metrics prefer visually/socially prototypical images over semantically correct but non-prototypical ones.&lt;/li&gt;&lt;li&gt;Introduces the ProtoBias benchmark (Animals, Objects, Demography) with paired semantically correct/non-prototypical vs prototypical/incorrect images to measure directional failures.&lt;/li&gt;&lt;li&gt;Shows common metrics (CLIPScore, PickScore, VQA-based) and some LLM-as-judge systems frequently misrank pairs, while human judgments favor semantic correctness.&lt;/li&gt;&lt;li&gt;Proposes ProtoScore, a 7B-parameter metric that substantially reduces failure rates and is much faster than large closed-source judges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subhadeep Roy', 'Gagan Bhatia', 'Steffen Eger']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation metrics', 'robustness', 'bias', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04946</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rotation-Robust Regression with Convolutional Model Trees</title><link>https://arxiv.org/abs/2601.04899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes geometry-aware inductive biases for Convolutional Model Trees (CMTs) to improve rotation robustness: convolutional smoothing, tilt dominance constraint, and importance-based pruning.&lt;/li&gt;&lt;li&gt;Evaluates deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without changing model parameters.&lt;/li&gt;&lt;li&gt;Finds orientation search helps under severe in-plane rotations but can degrade performance near the canonical orientation when confidence is misaligned with correctness.&lt;/li&gt;&lt;li&gt;Experiments on MNIST (regression and one-vs-rest recognition) demonstrate benefits and limitations of confidence-based orientation selection for model-tree ensembles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Li', 'William Ward Armstrong', 'Jun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'geometric_transformations', 'model_trees', 'deployment-time_adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04899</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models</title><link>https://arxiv.org/abs/2601.04778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CounterVid, a pipeline for counterfactual video generation that alters actions or temporal order while preserving scene context to create hard negatives.&lt;/li&gt;&lt;li&gt;Builds a synthetic dataset (~26k preference pairs) targeting action recognition and temporal reasoning to reduce action and temporal hallucinations in VLMs.&lt;/li&gt;&lt;li&gt;Proposes MixDPO, a Direct Preference Optimization method that jointly leverages textual and visual preferences for fine-tuning multimodal models (demonstrated on Qwen2.5-VL).&lt;/li&gt;&lt;li&gt;Reports improvements in temporal ordering and transfer to standard video hallucination benchmarks, with code/models to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobia Poppi', 'Burak Uzkent', 'Amanmeet Garg', 'Lucas Porto', 'Garin Kessler', 'Yezhou Yang', 'Marcella Cornia', 'Lorenzo Baraldi', 'Rita Cucchiara', 'Florian Schiffers']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'video-language models', 'counterfactual generation', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04778</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition</title><link>https://arxiv.org/abs/2601.04752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a skeletonization-based method to generate adversarial perturbations on images containing text, reducing the attack search space.&lt;/li&gt;&lt;li&gt;Targets mathematical formula images (LaTeX conversion) to induce character- and semantics-level errors in vision-language model outputs.&lt;/li&gt;&lt;li&gt;Evaluates both character- and semantic-level changes to analyze model visual interpretation and robustness.&lt;/li&gt;&lt;li&gt;Demonstrates practical impact by applying the attack to ChatGPT's visual/math recognition pipeline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masatomo Yoshida', 'Haruto Namura', 'Nicola Adami', 'Masahiro Okuda']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language models', 'OCR/math recognition', 'robustness', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04752</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Holistic Approach for Detecting Human Image Forgery</title><link>https://arxiv.org/abs/2601.04715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HuForDet, a dual-branch detector: (1) a face forgery branch using heterogeneous experts in RGB and frequency domains with an adaptive Laplacian-of-Gaussian (LoG) module to capture multi-scale artifacts; (2) a contextualized forgery branch using a Multi-Modal Large Language Model (MLLM) to assess full-body semantic consistency with a confidence-weighted fusion mechanism.&lt;/li&gt;&lt;li&gt;Curates HuFor dataset that combines existing face-forgery datasets with a newly collected corpus of full-body synthetic human images to unify evaluation across facial and full-body forgeries.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art detection performance and improved robustness across diverse human image forgeries, emphasizing generalization beyond face-only detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Guo', 'Jie Zhu', 'Anil Jain', 'Xiaoming Liu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery detection', 'multimodal LLM', 'frequency-domain analysis', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04715</guid><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems</title><link>https://arxiv.org/abs/2601.04605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes operational deviations in AI-enabled human-centric cyber-physical systems that can lead to unsafe or insecure behavior during deployment.&lt;/li&gt;&lt;li&gt;Proposes a framework to evaluate strategies for ensuring safety and security of such systems in operational deployment.&lt;/li&gt;&lt;li&gt;Provides a case study: a personalized image-based method to detect unannounced meals in closed-loop blood glucose control for Type 1 diabetics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bernard Ngabonziza', 'Ayan Banerjee', 'Sandeep K. S. Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'deployment monitoring', 'cyber-physical systems', 'medical AI', 'operational deviation detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04605</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction</title><link>https://arxiv.org/abs/2601.04567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RepMD, a method for detecting ever-shifting harmful memes by reproducing underlying design concepts.&lt;/li&gt;&lt;li&gt;Defines a Design Concept Graph (DCG) from an attack-tree framing and derives DCG from historical memes via design-step reproduction and graph pruning.&lt;/li&gt;&lt;li&gt;Uses the DCG to guide a Multimodal Large Language Model for meme classification, achieving 81.1% accuracy and showing limited degradation under type-shifting and temporal evolution.&lt;/li&gt;&lt;li&gt;Human evaluation indicates RepMD improves human discovery efficiency (15–30 seconds per meme).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyou Jiang', 'Mingyang Li', 'Junjie Wang', 'Yuekai Huang', 'Jie Huang', 'Zhiyuan Chang', 'Zhaoyang Li', 'Qing Wang']&lt;/li&gt;&lt;li&gt;Tags: ['harmful-meme-detection', 'multimodal-safety', 'content-moderation', 'design-concept-graph']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04567</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes</title><link>https://arxiv.org/abs/2601.04300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical, fine-grained evaluation criteria for image quality (positive/negative attributes organized in a tree) constructed with domain experts.&lt;/li&gt;&lt;li&gt;Introduces a two-stage alignment: supervised fine-tuning of an auxiliary diffusion model to inject domain knowledge, then Complex Preference Optimization (CPO) extending DPO to align a target diffusion model to non-binary hierarchical criteria.&lt;/li&gt;&lt;li&gt;Instantiates the approach on painting generation with an annotated dataset and reports improved generation quality and alignment with expert criteria.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenye Meng', 'Zejian Li', 'Zhongni Liu', 'Yize Li', 'Changle Xie', 'Kaixin Jia', 'Ling Yang', 'Huanghuang Deng', 'Shiying Ding', 'Shengyuan Zhang', 'Jiayi Li', 'Lingyun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'diffusion-models', 'preference-optimization', 'fine-grained-attributes', 'supervised-fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04300</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Domain Risks in LLMs</title><link>https://arxiv.org/abs/2511.07107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a 3,000-query dataset across education, finance, and management to evaluate domain-specific jailbreak/vulnerability rates in LLMs, finding an average jailbreak success of 57.8% across 14 models.&lt;/li&gt;&lt;li&gt;Proposes MENTOR, a metacognition-driven self-evolution framework that uses structured self-assessment (perspective-taking, consequential reasoning) to uncover latent misalignments and formalizes findings into dynamic rule-based knowledge graphs.&lt;/li&gt;&lt;li&gt;Introduces activation steering to modulate internal model representations at inference time to enforce compliance with discovered rules, and demonstrates substantial reductions in attack success rates and human-comparable risk analysis performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Shan', 'Kaicheng Shen', 'Wen Wu', 'Zhenyu Ying', 'Chaochao Lu', 'Yan Teng', 'Jingqi Huang', 'Guangze Ye', 'Guoqing Wang', 'Liang He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'alignment', 'red teaming', 'activation steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07107</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Models Outthink Their Safety: Unveiling and Mitigating Self-Jailbreak in Large Reasoning Models</title><link>https://arxiv.org/abs/2510.21285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new failure mode called Self-Jailbreak: models detect harmful intent initially but override that judgment during multi-step reasoning and produce unsafe outputs.&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Guardrail (CoG), a trajectory-level training framework that applies targeted, step-level interventions to prevent self-jailbreak while preserving multi-step reasoning capability.&lt;/li&gt;&lt;li&gt;Evaluates CoG across multiple safety and reasoning benchmarks, showing improved trade-offs between safety and reasoning performance compared to prior coarse-grained methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingzhi Mao', 'Chunkang Zhang', 'Junxiang Wang', 'Xinyan Guan', 'Boxi Cao', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun']&lt;/li&gt;&lt;li&gt;Tags: ['self-jailbreak', 'LLM safety', 'jailbreak mitigation', 'trajectory-level training', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21285</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation</title><link>https://arxiv.org/abs/2510.11423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical analysis of 30.8K health-related Community Notes shows substantial latency (median 17.6 hours) in obtaining helpfulness status and voter confusion between stylistic fluency and factual accuracy.&lt;/li&gt;&lt;li&gt;Proposes CrowdNotes+, an LLM-augmented framework with two modes—evidence-grounded note augmentation and utility-guided note automation—supported by a hierarchical three-stage evaluation (relevance, correctness, helpfulness).&lt;/li&gt;&lt;li&gt;Instantiates the approach with HealthNotes, a 1.2K benchmark annotated for helpfulness, and a fine-tuned helpfulness judge; experiments across 15 LLMs show CrowdNotes+ improves correctness, helpfulness, and evidence utility over human contributors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaying Wu', 'Zihang Fu', 'Haonan Wang', 'Fanxiao Li', 'Jiafeng Guo', 'Preslav Nakov', 'Min-Yen Kan']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM augmentation', 'content moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11423</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Safeguarded Text-to-Image Models via Large Language Models</title><link>https://arxiv.org/abs/2503.01839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AttackLLM, a fine-tuned large language model that generates adversarial prompts to jailbreak text-to-image models with safety guardrails.&lt;/li&gt;&lt;li&gt;Unlike query-based attacks, AttackLLM generates bypassing prompts efficiently after fine-tuning and is evaluated on three unsafe-prompt datasets against five safety guardrails.&lt;/li&gt;&lt;li&gt;Demonstrates superior bypass success compared to existing no-box attacks and shows the method can facilitate other query-based attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Yuepeng Hu', 'Yuchen Yang', 'Yinzhi Cao', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM red teaming', 'adversarial prompting', 'safety guardrails', 'prompt-based attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01839</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reward Shaping to Mitigate Reward Hacking in RLHF</title><link>https://arxiv.org/abs/2502.18770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking as a core failure mode in RLHF and studies common reward shaping techniques, deriving two design principles: bounded rewards and rapid initial growth with gradual convergence.&lt;/li&gt;&lt;li&gt;Proposes Preference As Reward (PAR), which uses latent preferences from the reward model as the RL signal and yields variance-reduction properties that stabilize training.&lt;/li&gt;&lt;li&gt;Demonstrates empirical gains on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets, outperforming other shaping methods (≥5 percentage points on AlpacaEval 2.0).&lt;/li&gt;&lt;li&gt;Shows strong data efficiency (one reference reward needed) and robustness to reward hacking even after extended training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Fu', 'Xuandong Zhao', 'Chengyuan Yao', 'Heng Wang', 'Qi Han', 'Yanghua Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward-hacking', 'reward-shaping', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18770</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection</title><link>https://arxiv.org/abs/2601.04160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RFC Bench, a paragraph-level benchmark for evaluating LLMs on financial misinformation in realistic news contexts.&lt;/li&gt;&lt;li&gt;Defines two tasks: reference-free misinformation detection and comparative diagnosis using paired original and perturbed inputs.&lt;/li&gt;&lt;li&gt;Finds models perform substantially better with comparative context; reference-free settings reveal unstable predictions and many invalid outputs.&lt;/li&gt;&lt;li&gt;Highlights a gap in models' ability to maintain coherent belief states without external grounding and provides a structured testbed to study this.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuechen Jiang', 'Zhiwei Liu', 'Yupeng Cao', 'Yueru He', 'Chen Xu', 'Ziyang Xu', 'Zhiyang Deng', 'Prayag Tiwari', 'Xi Chen', 'Alejandro Lopez-Lira', 'Jimin Huang', 'Junichi Tsujii', 'Sophia Ananiadou']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation-detection', 'benchmark', 'LLM-evaluation', 'robustness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04160</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</title><link>https://arxiv.org/abs/2601.03714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether DeepSeek-OCR's performance is driven by genuine OCR capability or by reliance on language priors via sentence- and word-level semantic corruption.&lt;/li&gt;&lt;li&gt;Finds performance drops from ~90% to ~20% without linguistic support, showing heavy dependence on language priors and increased hallucination risk at lower visual token counts.&lt;/li&gt;&lt;li&gt;Benchmarks DeepSeek-OCR against 13 baselines, showing traditional pipeline OCRs are more robust to semantic perturbations than end-to-end methods.&lt;/li&gt;&lt;li&gt;Performs context stress testing that indicates model collapse around 10,000 text tokens, suggesting optical compression may worsen long-context issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Liang', 'Ruixuan Ying', 'Bo Li', 'Hong Li', 'Kai Yan', 'Qingwen Li', 'Min Yang', 'Okamoto Satoshi', 'Zhe Cui', 'Shiwen Ni']&lt;/li&gt;&lt;li&gt;Tags: ['OCR robustness', 'hallucination / language priors', 'vision-text compression', 'long-context evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03714</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2601.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies sycophancy in LLMs as a safety/alignment failure where models output agreeable answers that contradict their internal reasoning.&lt;/li&gt;&lt;li&gt;Proposes Regulated Causal Anchoring (RCA) to verify consistency between reasoning traces and outputs at inference time without requiring ground truth, using an independent judge.&lt;/li&gt;&lt;li&gt;Shows RCA detects sycophancy (0% sycophancy reported) while retaining high acceptance of valid hints, and highlights failure modes invisible to outcome-based evaluation (Inverse Scaling and Final Output Gap).&lt;/li&gt;&lt;li&gt;Argues RCA breaks the self-reinforcing bias loop of critique-based self-correction and operates without ground truth, enabling inference-time process-level safety checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Y. Chang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-behavior', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03263</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BaseCal: Unsupervised Confidence Calibration via Base Model Signals</title><link>https://arxiv.org/abs/2601.03042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BaseCal: unsupervised methods to calibrate overconfident post-trained LLMs (PoLLMs) using their corresponding base LLMs as references.&lt;/li&gt;&lt;li&gt;BaseCal-ReEval: re-evaluates PoLLM outputs by querying the base LLM to obtain averaged probabilities as confidence (higher inference cost).&lt;/li&gt;&lt;li&gt;BaseCal-Proj: trains a lightweight projection mapping PoLLM final-layer hidden states to base LLM hidden states so the base output layer can produce calibrated confidences with low overhead.&lt;/li&gt;&lt;li&gt;Plug-and-play, requires no human labels or LLM modifications; reports ~42.9% average reduction in Expected Calibration Error versus best unsupervised baselines across datasets and LLM families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Tan', 'Wanli Yang', 'Junwei Zhang', 'Xin Chen', 'Rui Tang', 'Du Su', 'Jingang Wang', 'Yuanzhuo Wang', 'Fei Sun', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['confidence calibration', 'model reliability', 'LLM safety', 'unsupervised methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03042</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Policy to Logic for Efficient and Interpretable Coverage Assessment</title><link>https://arxiv.org/abs/2601.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a hybrid system combining a coverage-aware retriever with symbolic rule-based reasoning to extract and structure policy language into explicit facts and rules.&lt;/li&gt;&lt;li&gt;Aims to reduce LLM hallucinations and improve interpretability by producing auditable rationales for medical coverage policy review.&lt;/li&gt;&lt;li&gt;Reports empirical gains: 44% reduction in LLM inference cost and a 4.5% improvement in F1, highlighting efficiency and effectiveness trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Hamid Reza Hassanzadeh', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'robustness', 'LLM reliability', 'symbolic reasoning', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01266</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</title><link>https://arxiv.org/abs/2512.13655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative evaluation of four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned LLMs (7B–14B), assessing compatibility and capability impact.&lt;/li&gt;&lt;li&gt;Quantitative results show single-pass methods generally preserve capabilities better on the benchmarked subset (example GSM8K deltas), while Bayesian-optimized abliteration yields variable distribution shifts (KL divergence range 0.043–1.646) and model-dependent capability effects.&lt;/li&gt;&lt;li&gt;Identifies mathematical reasoning (GSM8K) as the most sensitive capability to abliteration, with performance changes ranging from +1.51 pp to −18.81 pp (up to −26.5% relative), highlighting trade-offs between safety-removal and capability degradation.&lt;/li&gt;&lt;li&gt;Provides evidence-based guidance for selecting abliteration tools for research, adversarial testing, and security analysis across different model architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard J. Young']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment bypass', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13655</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TPA: Next Token Probability Attribution for Detecting Hallucinations in RAG</title><link>https://arxiv.org/abs/2512.07515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TPA, a method that attributes next-token probability to seven sources (Query, RAG Context, Past Token, Self Token, FFN, Final LayerNorm, Initial Embedding) to diagnose hallucinations in RAG systems.&lt;/li&gt;&lt;li&gt;Aggregates token-level attributions by POS tags to identify anomalous contribution patterns (e.g., nouns overly influenced by LayerNorm) indicative of hallucinated content.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance in detecting hallucinated responses via extensive experiments, improving interpretability of which model components drive incorrect generations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengqian Lu', 'Jie Lu', 'Anjin Liu', 'Guangquan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'retrieval-augmented generation (RAG)', 'attribution / interpretability', 'safety / robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07515</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Black-Box On-Policy Distillation of Large Language Models</title><link>https://arxiv.org/abs/2511.10643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generative Adversarial Distillation (GAD): frames the student LLM as a generator and trains a discriminator to distinguish student responses from a black-box teacher, forming a minimax game.&lt;/li&gt;&lt;li&gt;Discriminator acts as an on-policy reward model that co-evolves with the student, providing adaptive feedback for training without access to teacher logits or parameters.&lt;/li&gt;&lt;li&gt;Reports consistent gains over sequence-level knowledge distillation; a Qwen2.5-14B-Instruct student trained with GAD approaches GPT-5-Chat on LMSYS-Chat automatic evaluation.&lt;/li&gt;&lt;li&gt;Method operates purely from teacher text outputs (black-box), enabling practical on-policy distillation of proprietary models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianzhu Ye', 'Li Dong', 'Zewen Chi', 'Xun Wu', 'Shaohan Huang', 'Furu Wei']&lt;/li&gt;&lt;li&gt;Tags: ['black-box distillation', 'model extraction', 'LLM distillation', 'adversarial training', 'IP/privacy risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10643</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework</title><link>https://arxiv.org/abs/2511.03508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EvolIF, an evolving benchmark and framework for evaluating LLM instruction-following across multi-turn, multi-topic dialogues using a three-layer tracking mechanism and a query-synthesis agent.&lt;/li&gt;&lt;li&gt;Defines process-centric metrics grounded in Flow Theory and an interaction termination rule based on simulated user patience to avoid saturation in fixed-turn evaluations.&lt;/li&gt;&lt;li&gt;Finds weaknesses in models' failure recovery and fine-grained instruction adherence as conversation depth increases; reports comparative robustness scores (e.g., GPT-5 highest).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Jia', 'Ye Shen', 'Xiujie Song', 'Kaiwei Zhang', 'Shibo Wang', 'Dun Pei', 'Xiangyang Zhu', 'Guangtao Zhai']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'instruction-following', 'benchmarking', 'multi-turn dialogue']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03508</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation</title><link>https://arxiv.org/abs/2511.01014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IF-CRITIC, an LLM-based critic for fine-grained, efficient, and reliable instruction-following evaluation using a checklist generator to decompose instructions into constraint-level checklists.&lt;/li&gt;&lt;li&gt;Collects high-quality critique training data via a multi-stage critique filtering mechanism and trains the critic with constraint-level preference optimization.&lt;/li&gt;&lt;li&gt;Demonstrates IF-CRITIC outperforms strong LLM-as-judge baselines (o4-mini, Gemini-3-Pro) and provides reward signals that improve instruction-following with lower computational overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bosi Wen', 'Yilin Niu', 'Cunxiang Wang', 'Pei Ke', 'Xiaoying Ling', 'Ying Zhang', 'Aohan Zeng', 'Hongning Wang', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'Instruction-following', 'Reward modeling', 'Alignment', 'Safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01014</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Qomhra: A Bilingual Irish and English Large Language Model</title><link>https://arxiv.org/abs/2510.17652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Qomhrá, a bilingual Irish–English LLM trained under low-resource constraints with a full pipeline: continued pre-training, instruction tuning, and synthesis of human preference data for alignment.&lt;/li&gt;&lt;li&gt;Proposes a novel method to synthesize human preference (accepted/rejected) responses by prompting an LLM, and validates the synthesized preference data with L1 Irish speakers.&lt;/li&gt;&lt;li&gt;Evaluates several closed-weight LLMs for Irish generation (ranking Gemini-2.5-Pro highest by human raters) and uses the top model to translate English instruction-tuning data and synthesize an Irish-language preference dataset.&lt;/li&gt;&lt;li&gt;Comprehensive evaluations (translation, gender understanding, topic identification, world knowledge) show substantial gains over the open-source Irish LLM baseline (UCCIX).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph McInerney', 'Khanh-Tung Tran', 'Liam Lonergan', "Ailbhe N\\'i Chasaide", "Neasa N\\'i Chiar\\'ain", 'Barry Devereux']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-data', 'instruction-tuning', 'low-resource-languages', 'bilingual-llm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17652</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models</title><link>https://arxiv.org/abs/2510.11529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'Detection Dilemma': Internal State Probing catches factual errors but misses logical fallacies, while Chain-of-Thought (CoT) verification catches logic errors but misses fact-intensive hallucinations.&lt;/li&gt;&lt;li&gt;Proposes a unified framework that fuses fine-grained internal-state signals with structured external reasoning to detect sophisticated hallucinations.&lt;/li&gt;&lt;li&gt;Introduces multi-path reasoning to produce comparable signals and a segment-aware temporalized cross-attention module to align and fuse representations, detecting dissonances.&lt;/li&gt;&lt;li&gt;Evaluated on three benchmarks and two LLMs, showing consistent improvements over strong baselines; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yusheng Song', 'Lirong Qiu', 'Xi Zhang', 'Zhihao Tang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'internal state probing', 'chain-of-thought verification', 'representation alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11529</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</title><link>https://arxiv.org/abs/2508.14904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified co-training SFT method that embeds three safety behaviors—positive (prosocial), negative (unfiltered/red-team), and rejective (refusal)—into one model.&lt;/li&gt;&lt;li&gt;Uses a 'magic token' (system-level instruction) to switch behaviors at inference time, enabling post-deployment, stealthy, fine-grained control for use cases like internal red-teaming or conservative refusals.&lt;/li&gt;&lt;li&gt;Shows the co-training leads to a distinct 'Safety Alignment Margin' with well-separated output distributions for each mode, claiming safety robustness and controllability.&lt;/li&gt;&lt;li&gt;Empirically matches SFT+DPO safety alignment and reports an 8B model outperforming a much larger baseline (DeepSeek-R1, 671B) while reducing training and deployment costs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianfeng Si', 'Lin Sun', 'Zhewen Tan', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'controllability/magic-token', 'red-teaming', 'alignment', 'training methods (co-training/SFT)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14904</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs</title><link>https://arxiv.org/abs/2508.10029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Latent Fusion Jailbreak (LFJ), a white-box jailbreak that mixes hidden states of a harmful query with a thematically similar benign query in latent space to elicit unsafe outputs while avoiding detectable input artifacts.&lt;/li&gt;&lt;li&gt;Proposes a gradient-guided optimization to balance attack success and efficiency; evaluates LFJ across multiple open-weight chat models achieving ~94% ASR and outperforming baselines like GCG and AutoDAN.&lt;/li&gt;&lt;li&gt;Identifies thematic similarity in latent representations as a key vulnerability in current safety alignments.&lt;/li&gt;&lt;li&gt;Proposes a latent adversarial training defense that reduces LFJ's ASR by over 80% without degrading model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Mohan Li', 'Chunqiang Hu', 'Haitao Xu', 'Ningyu Zhang', 'Bo Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'latent-space adversarial attack', 'defenses', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10029</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling</title><link>https://arxiv.org/abs/2508.03296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Hi-Guard: a hierarchical multimodal moderation pipeline with a lightweight binary filter for safe content and a stronger model for fine-grained, path-based classification over a hierarchical taxonomy.&lt;/li&gt;&lt;li&gt;Directly incorporates moderation rule definitions into model prompts to enable policy-aligned reasoning and improve interpretability/explanations.&lt;/li&gt;&lt;li&gt;Introduces a multi-level soft-margin reward and Group Relative Policy Optimization (GRPO) to penalize semantically adjacent misclassifications and enhance structured prediction and explanation quality.&lt;/li&gt;&lt;li&gt;Claims superior classification accuracy, generalization, and interpretability with real-world deployment and released code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anqi Li', 'Wenwei Jin', 'Jintao Tong', 'Pengda Qin', 'Weijia Li', 'Guo Lu']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'policy-aligned reasoning', 'interpretability', 'hierarchical classification', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03296</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation</title><link>https://arxiv.org/abs/2506.06842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PCoT (Persuasion-Augmented Chain of Thought), a method that incorporates persuasion/fallacy knowledge into LLM chain-of-thought prompting to improve zero-shot disinformation detection.&lt;/li&gt;&lt;li&gt;Evaluates PCoT across five LLMs and five datasets, reporting an average improvement of ~15% over competitive methods.&lt;/li&gt;&lt;li&gt;Publishes two new post-cutoff disinformation datasets (EUDisinfo and MultiDis) to test model generalization on unseen content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arkadiusz Modzelewski', 'Witold Sosnowski', 'Tiziano Labruna', 'Adam Wierzbicki', 'Giovanni Da San Martino']&lt;/li&gt;&lt;li&gt;Tags: ['disinformation detection', 'LLM safety', 'chain-of-thought', 'dataset release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06842</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity</title><link>https://arxiv.org/abs/2505.23923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Act-Adaptive Margin (AAM), a method that dynamically calibrates preference margins in Bradley-Terry style reward models using the model's internal parameters to handle subjective ambiguity.&lt;/li&gt;&lt;li&gt;Introduces two AAM variants that generate context-appropriate preference gaps without extra human annotations, integrating generative model understanding with preference scoring.&lt;/li&gt;&lt;li&gt;Evaluates on RewardBench, JudgeBench, role-playing tasks, and downstream alignment methods (e.g., GRPO), reporting ~2.95% improvement on general tasks and ~4.85% on subjective role-playing tasks and SOTA on CharacterEval and Charm.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiteng Fang', 'Dingwei Chen', 'Xiang Huang', 'Ting-En Lin', 'Yuchuan Wu', 'Xiong Liu', 'Xinge Ye', 'Ziqiang Liu', 'Haonan Zhang', 'Liang Zhu', 'Hamid Alinejad-Rokny', 'Min Yang', 'Yongbin Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference learning', 'subjective evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23923</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation</title><link>https://arxiv.org/abs/2505.21072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FRANQ, a method that separately estimates factuality and faithfulness for outputs of Retrieval-Augmented Generation using distinct uncertainty quantification techniques.&lt;/li&gt;&lt;li&gt;Constructs a new long-form QA dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation.&lt;/li&gt;&lt;li&gt;Demonstrates that conditioning factuality UQ on faithfulness improves detection of hallucinations across multiple datasets, tasks, and LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ekaterina Fadeeva', 'Aleksandr Rubashevskii', 'Dzianis Piatrashyn', 'Roman Vashurin', 'Shehzaad Dhuliawala', 'Artem Shelmanov', 'Timothy Baldwin', 'Preslav Nakov', 'Mrinmaya Sachan', 'Maxim Panov']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'retrieval-augmented generation (RAG)', 'uncertainty quantification', 'faithfulness', 'safety/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21072</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models</title><link>https://arxiv.org/abs/2505.16036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks 29 open-source LLMs on four ethical dimensions: robustness, reliability, safety, and fairness, across English and Turkish.&lt;/li&gt;&lt;li&gt;Uses an LLM-as-a-Judge methodology and releases dataset, data, and scripts for reproducibility.&lt;/li&gt;&lt;li&gt;Finds many models perform well on safety, fairness, and robustness while reliability is often a weakness; jailbreak templates were largely ineffective against most evaluated models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Y{\\i}ld{\\i}r{\\i}m \\"Ozen', 'Burak Erin\\c{c} \\c{C}etin', 'Kaan Eng\\"ur', 'Elif Naz Demiry{\\i}lmaz', 'Cagri Toraman']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'jailbreaking', 'LLM-benchmarking', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16036</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</title><link>https://arxiv.org/abs/2601.05201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled object-counting setup to study prompt-induced hallucination (PIH) in vision-language models, where prompts overstate object counts and the model may favor the prompt over visual evidence.&lt;/li&gt;&lt;li&gt;Performs mechanistic analysis across three VLMs and identifies a small set of attention heads (PIH-heads) whose ablation reduces prompt-induced hallucinations by at least ~40% without retraining.&lt;/li&gt;&lt;li&gt;Characterizes model-specific ways PIH-heads mediate prompt copying and shows ablation increases model corrections toward visual evidence, yielding insights useful for defenses and interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Rudman', 'Michal Golovanevsky', 'Dana Arad', 'Yonatan Belinkov', 'Ritambhara Singh', 'Carsten Eickhoff', 'Kyle Mahowald']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'prompt injection', 'mechanistic interpretability', 'vision-language models', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05201</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop</title><link>https://arxiv.org/abs/2601.05184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Self-Consuming Performative Loop (SCPL) framework to study how LLMs retraining on their own synthetic outputs affects bias over iterative deployments.&lt;/li&gt;&lt;li&gt;Empirically analyzes two loop types (periodic full retraining and incremental fine-tuning) on three tasks, finding performative feedback tends to increase preference bias while decreasing disparate bias.&lt;/li&gt;&lt;li&gt;Proposes a reward-based rejection sampling mitigation strategy to reduce bias in these self-improving training loops.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaxuan Wang', 'Zhongteng Cai', 'Yujia Bao', 'Xueru Zhang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM bias', 'alignment', 'performative feedback', 'robustness', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05184</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defense Against Indirect Prompt Injection via Tool Result Parsing</title><link>https://arxiv.org/abs/2601.04795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and frames the threat of Indirect Prompt Injection (IPI) where adversarial instructions are embedded in tool call results to compromise LLM agents controlling physical systems.&lt;/li&gt;&lt;li&gt;Evaluates existing defenses: classifier-based detectors (high compute/upkeep) and prompt-based mitigation (insufficient robustness / high ASR).&lt;/li&gt;&lt;li&gt;Proposes a tool-result parsing approach that supplies LLMs with precise, sanitized data extracted from tool outputs to filter out injected malicious instructions.&lt;/li&gt;&lt;li&gt;Reports empirical improvements: lowest Attack Success Rate (ASR) to date while maintaining competitive Utility under Attack (UA); provides code on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Yu', 'Xinran Cheng', 'Chuanyi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM agents', 'adversarial prompting', 'red teaming', 'agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04795</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models</title><link>https://arxiv.org/abs/2601.04778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CounterVid, a pipeline for counterfactual video generation that alters actions or temporal order while preserving scene context to create hard negatives.&lt;/li&gt;&lt;li&gt;Builds a synthetic dataset (~26k preference pairs) targeting action recognition and temporal reasoning to reduce action and temporal hallucinations in VLMs.&lt;/li&gt;&lt;li&gt;Proposes MixDPO, a Direct Preference Optimization method that jointly leverages textual and visual preferences for fine-tuning multimodal models (demonstrated on Qwen2.5-VL).&lt;/li&gt;&lt;li&gt;Reports improvements in temporal ordering and transfer to standard video hallucination benchmarks, with code/models to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobia Poppi', 'Burak Uzkent', 'Amanmeet Garg', 'Lucas Porto', 'Garin Kessler', 'Yezhou Yang', 'Marcella Cornia', 'Lorenzo Baraldi', 'Rita Cucchiara', 'Florian Schiffers']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'video-language models', 'counterfactual generation', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04778</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization</title><link>https://arxiv.org/abs/2601.04641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-MGTD, an Adaptive Differentially Private Entity Sanitization framework for detecting machine-generated text while protecting sensitive user data.&lt;/li&gt;&lt;li&gt;Uses a two-stage mechanism: noisy frequency estimation and dynamic privacy-budget calibration, applying Laplace mechanism for numerical entities and Exponential mechanism for textual entities.&lt;/li&gt;&lt;li&gt;Reports a counter-intuitive finding that DP noise can increase distinguishability between human and machine text, and shows near-perfect detection on MGTBench-2.0 while satisfying DP guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lionel Z. Wang', 'Yusheng Zhao', 'Jiabin Luo', 'Xinfeng Li', 'Lixu Wang', 'Yinan Peng', 'Haoyang Li', 'XiaoFeng Wang', 'Wei Dong']&lt;/li&gt;&lt;li&gt;Tags: ['Differential Privacy', 'Machine-Generated Text Detection', 'Privacy-Preserving ML', 'Entity Sanitization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04641</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents</title><link>https://arxiv.org/abs/2601.04566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BackdoorAgent, a modular, stage-aware framework modeling backdoor threats across three agent workflow stages: planning, memory, and tool-use.&lt;/li&gt;&lt;li&gt;Provides a standardized benchmark across four agent applications (Agent QA, Agent Code, Agent Web, Agent Drive) covering language-only and multimodal settings.&lt;/li&gt;&lt;li&gt;Empirical results show triggers implanted at a single stage can persist and propagate across steps (planning 43.58%, memory 77.97%, tool-stage 60.28% on a GPT backbone).&lt;/li&gt;&lt;li&gt;Code and benchmark are released to facilitate reproducibility and future research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Feng', 'Yige Li', 'Yutao Wu', 'Yingshui Tan', 'Yanming Guo', 'Yifan Ding', 'Kun Zhai', 'Xingjun Ma', 'Yugang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM agents', 'agent security', 'adversarial attacks', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04566</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards</title><link>https://arxiv.org/abs/2601.04411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an analytically tractable bandit-based model of reinforcement learning with verifiable (but noisy) rewards, modeling false positives and false negatives and grouping completions into recurring reasoning modes.&lt;/li&gt;&lt;li&gt;Derives replicator-style dynamics that decouple into within-mode competition and a 1-D evolution for mass on incorrect modes; shows a phase transition determined by Youden's index J = TPR − FPR.&lt;/li&gt;&lt;li&gt;Shows when J &gt; 0 noisy verification rescales convergence time ('rate, not fate'), J = 0 is neutral, and J &lt; 0 leads to amplification of incorrect modes (anti-learning); validates predictions with experiments on verifiable programming tasks under synthetic noise.&lt;/li&gt;&lt;li&gt;Provides a framework for analyzing RLVR stability and suggests algorithmic interventions to mitigate verifier-induced collapse, with implications for reward hacking, robustness, and alignment of LLM training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Rad', 'Khashayar Filom', 'Darioush Keivan', 'Peyman Mohajerin Esfahani', 'Ehsan Kamalinejad']&lt;/li&gt;&lt;li&gt;Tags: ['RL safety', 'reward hacking', 'robustness', 'verification noise', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04411</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Generalization to Political Beliefs from Fine-Tuning on Sports Team Preferences</title><link>https://arxiv.org/abs/2601.04369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tuning an LLM to prefer either coastal or Southern sports teams produced unexpected shifts in political beliefs compared to the base model.&lt;/li&gt;&lt;li&gt;Contrary to the hypothesis that coastal fine-tuning would push the model liberal and Southern fine-tuning conservative, the two fine-tuned models often produced similar political responses without a clear ideological bias.&lt;/li&gt;&lt;li&gt;Evaluation included numerical agreement ratings on political statements and solicitation of elaborations for more extreme answers, revealing varying willingness to justify positions.&lt;/li&gt;&lt;li&gt;Authors highlight the need to study mechanisms by which narrow fine-tuning datasets induce seemingly unrelated behavioral changes in models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Owen Terry']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'fine-tuning', 'political bias', 'unintended generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04369</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying the Effect of Test Set Contamination on Generative Evaluations</title><link>https://arxiv.org/abs/2601.04301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively measures how test-set contamination affects generative model evaluation across the model lifecycle (pretraining, further training/finetuning, and inference), using mixtures of web data and the MATH benchmark across model sizes and contamination replicas.&lt;/li&gt;&lt;li&gt;Shows that even a single replica of the test set in pretraining can substantially improve performance and can yield lower loss than the uncontaminated irreducible error according to scaling-law fits.&lt;/li&gt;&lt;li&gt;Finds mitigation behaviors: overtraining on fresh data reduces contamination effects, supervised finetuning can either amplify or reduce test performance depending on prior contamination, and sampling settings (higher temperature) reduce memorization at inference.&lt;/li&gt;&lt;li&gt;Analyzes memorization dynamics for generative outputs, finding longer solutions are exponentially harder to memorize than short ones, which has distinct implications versus discriminative evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rylan Schaeffer', 'Joshua Kazdan', 'Baber Abbasi', 'Ken Ziyu Liu', 'Brando Miranda', 'Ahmed Ahmed', 'Abhay Puri', 'Niloofar Mireshghallah', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['test-set contamination', 'memorization', 'evaluation robustness', 'data leakage', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04301</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity</title><link>https://arxiv.org/abs/2601.04283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode where character-level Transformers trained on modular addition fail under absolute position shifts and out-of-distribution natural-language templates despite high in-distribution accuracy.&lt;/li&gt;&lt;li&gt;Proposes a training recipe combining explicit expression boundary markers, a position curriculum (broadened absolute positions), diverse template mixtures, and consistency training across variants.&lt;/li&gt;&lt;li&gt;Demonstrates substantially improved robustness to position shifts and template OOD while retaining in-distribution performance, and provides a reproducible evaluation protocol and artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolay Yudin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'procedural generalization', 'curriculum learning', 'OOD evaluation', 'position-shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04283</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs</title><link>https://arxiv.org/abs/2601.04275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Shadow Unlearning', an approximate unlearning paradigm that removes training-sample influence using anonymized ('shadow') forget data to avoid exposing PII.&lt;/li&gt;&lt;li&gt;Introduces Neuro-Semantic Projector Unlearning (NSPU), a privacy-preserving framework to implement shadow unlearning on LLMs.&lt;/li&gt;&lt;li&gt;Provides a Multi-domain Fictitious Unlearning (MuFU) forget set and an evaluation stack to measure the trade-off between knowledge retention (model utility) and unlearning effectiveness.&lt;/li&gt;&lt;li&gt;Reports that NSPU achieves superior unlearning with preserved utility, improved user privacy, and roughly 10× computational efficiency compared to standard unlearning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dinesh Srivasthav P', 'Ashok Urlana', 'Rahul Mishra', 'Bala Mallikarjunarao Garlapati', 'Ponnurangam Kumaraguru']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy-preserving', 'PII-protection', 'LLM-fidelity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04275</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs</title><link>https://arxiv.org/abs/2601.04199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multidimensional safety evaluation framework and empirical benchmarking showing pervasive vulnerabilities in state-of-the-art Medical MLLMs, including cross-modality jailbreaks and loss of prior safety alignment after medical fine-tuning.&lt;/li&gt;&lt;li&gt;Diagnoses catastrophic forgetting of base-model safety during domain-specific fine-tuning and demonstrates fragility across general and medical-specific safety dimensions.&lt;/li&gt;&lt;li&gt;Proposes a 'Parameter-Space Intervention' method that extracts and injects intrinsic safety representations from base models into target medical MLLMs, plus a fine-grained parameter search to balance safety and medical performance without extra domain safety data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Zhao', 'Xing Mou', 'Jinlin Wu', 'Hongyuan Yu', 'Mingrui Sun', 'Yang Shi', 'Xuanwu Yin', 'Zhen Chen', 'Zhen Lei', 'Yaohua Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model alignment', 'jailbreaking / red teaming', 'parameter-space interventions', 'safety evaluation / benchmarking', 'medical MLLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04199</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</title><link>https://arxiv.org/abs/2601.05242</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode when applying Group Relative Policy Optimization (GRPO) to multi-reward RL: normalizing combined rollout rewards can collapse distinct reward signals into identical advantage values, harming training resolution and stability.&lt;/li&gt;&lt;li&gt;Proposes GDPO (Group reward-Decoupled Normalization Policy Optimization), which normalizes each reward component separately to preserve relative differences and produce more informative advantages.&lt;/li&gt;&lt;li&gt;Evaluates GDPO vs GRPO on three language-model-centric tasks (tool calling, math reasoning, coding reasoning), reporting improved correctness (accuracy, bug ratio) and constraint adherence (format, length) as well as greater training stability.&lt;/li&gt;&lt;li&gt;Focus is on multi-reward RL optimization and alignment to diverse user preferences via improved training dynamics, rather than on adversarial attacks or red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shih-Yang Liu', 'Xin Dong', 'Ximing Lu', 'Shizhe Diao', 'Peter Belcak', 'Mingjie Liu', 'Min-Hung Chen', 'Hongxu Yin', 'Yu-Chiang Frank Wang', 'Kwang-Ting Cheng', 'Yejin Choi', 'Jan Kautz', 'Pavlo Molchanov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-reward reinforcement learning', 'policy optimization', 'training stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05242</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent-as-a-Judge</title><link>https://arxiv.org/abs/2601.05111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys the shift from LLM-as-a-Judge to agentic judges that use planning, tool-augmented verification, multi-agent collaboration, and persistent memory to improve evaluation reliability.&lt;/li&gt;&lt;li&gt;Defines key dimensions and a developmental taxonomy, organizes core methodologies and applications across general and professional domains.&lt;/li&gt;&lt;li&gt;Analyzes frontier challenges and outlines research directions for more robust, verifiable, and nuanced AI evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runyang You', 'Hongru Cai', 'Caiqi Zhang', 'Qiancheng Xu', 'Meng Liu', 'Tiezheng Yu', 'Yongqi Li', 'Wenjie Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM evaluation', 'agentic systems', 'verification', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05111</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Compositional Steering of Large Language Models with Steering Tokens</title><link>https://arxiv.org/abs/2601.05062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces compositional steering tokens that embed individual behaviors (natural-language instructions) into dedicated input tokens via self-distillation.&lt;/li&gt;&lt;li&gt;Trains a composition token on behavior pairs to enable zero-shot composition, generalizing to unseen behavior combinations and different numbers of behaviors.&lt;/li&gt;&lt;li&gt;Operates in the input-token space (not activation space), yielding better zero-shot composition than activation steering, instruction prompting, and LoRA merging.&lt;/li&gt;&lt;li&gt;Empirical results across multiple LLM architectures show improved multi-behavior control; steering tokens also complement natural-language instructions for further gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gorjan Radevski', 'Kiril Gashteovski', 'Giwon Hong', 'Carolin Lawrence', 'Goran Glava\\v{s}']&lt;/li&gt;&lt;li&gt;Tags: ['LLM steering', 'Compositionality', 'Alignment/Control', 'Prompt engineering', 'Model steering tokens']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05062</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>H\'an D\=an Xu\'e B\`u (Mimicry) or Q\=ing Ch\=u Y\'u L\'an (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</title><link>https://arxiv.org/abs/2601.05019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that supervised fine-tuning (SFT) distillation of reasoning traces from RL-trained teachers fails to preserve teachers' alignment with human cognitive cost scaling, a phenomenon they call 'Functional Alignment Collapse'.&lt;/li&gt;&lt;li&gt;Empirical evaluation across 14 models: teachers exhibit strong human difficulty scaling (mean r = 0.64) while distilled students drop substantially (mean r = 0.34) and sometimes suffer 'Negative Transfer' versus pre-distillation baselines.&lt;/li&gt;&lt;li&gt;Proposes a 'Cargo Cult' explanation: SFT copies surface linguistic patterns (verbosity) without internalizing dynamic resource-allocation policies responsible for human-like reasoning.&lt;/li&gt;&lt;li&gt;Argues that human-like cognitive alignment emerges from active reinforcement learning rather than passive supervised imitation, with implications for alignment strategy design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yueqing Hu', 'Xinyang Peng', 'Shuting Peng', 'Hanqi Wang', 'Tianhong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model distillation', 'reinforcement learning (RLHF)', 'negative transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05019</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GenProve: Learning to Generate Text with Fine-Grained Provenance</title><link>https://arxiv.org/abs/2601.04932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generation-time Fine-grained Provenance: models must produce answers along with sentence-level provenance triples labeled as Quotation, Compression, or Inference.&lt;/li&gt;&lt;li&gt;Presents ReFInE, an expert-annotated dataset distinguishing quotation, compression, and inference provenance.&lt;/li&gt;&lt;li&gt;Proposes GenProve: combines supervised fine-tuning with Group Relative Policy Optimization (GRPO) to optimize answer fidelity and provenance correctness, outperforming 14 LLMs and revealing models struggle with inference-based provenance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingxuan Wei', 'Xingyue Wang', 'Yanghaoyu Liao', 'Jie Dong', 'Yuchen Liu', 'Caijun Jia', 'Bihui Yu', 'Junnan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['provenance', 'hallucination', 'alignment', 'evaluation', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04932</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences</title><link>https://arxiv.org/abs/2601.04925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Persuaficial, a high-quality multilingual benchmark (English, German, Polish, Italian, French, Russian) for comparing human-written vs LLM-generated persuasive text.&lt;/li&gt;&lt;li&gt;Categorizes controllable generation approaches for producing persuasive content with LLMs and conducts extensive empirical evaluation of automatic detectors, finding subtle LLM-generated persuasion degrades detection performance.&lt;/li&gt;&lt;li&gt;Provides a comprehensive linguistic analysis highlighting systematic differences between human and LLM persuasive texts to inform more interpretable and robust detection tools.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arkadiusz Modzelewski', 'Pawe{\\l} Golik', 'Anna Ko{\\l}os', 'Giovanni Da San Martino']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated persuasion detection', 'Benchmarking', 'Multilingual', 'LLM safety', 'Linguistic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04925</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>V-FAT: Benchmarking Visual Fidelity Against Text-bias</title><link>https://arxiv.org/abs/2601.04897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces V-FAT, a diagnostic benchmark (4,026 VQA instances across six semantic domains) to measure reliance of MLLMs on linguistic priors (Text Bias) versus true visual grounding.&lt;/li&gt;&lt;li&gt;Proposes a Three-Level Evaluation (L1 internal corpus bias via atypical images, L2 external instruction bias via misleading prompts, L3 synergistic bias combining both) and a Visual Robustness Score (VRS) that penalizes lucky linguistic guesses.&lt;/li&gt;&lt;li&gt;Evaluates 12 leading MLLMs and shows significant visual collapse under strong linguistic dominance despite strong performance on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziteng Wang', 'Yujie He', 'Guanliang Li', 'Siqi Yang', 'Jiaqi Xiong', 'Songxiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'robustness', 'alignment', 'safety-evaluation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04897</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters</title><link>https://arxiv.org/abs/2601.04885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Mean Collapse' when dense LLMs try to fit conflicting cultural value distributions and attributes it to 'Cultural Sparsity' from gradient interference.&lt;/li&gt;&lt;li&gt;Proposes CuMA (Cultural Mixture of Adapters): demographic-aware routing to separate capacity into specialized adapter experts that represent distinct cultural modes.&lt;/li&gt;&lt;li&gt;Shows empirical improvements on WorldValuesBench, Community Alignment, and PRISM, claiming state-of-the-art performance and mitigation of mean collapse versus dense and semantic-only MoE baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ao Sun', 'Xiaoyu Wang', 'Zhe Tan', 'Yu Li', 'Jiachen Zhu', 'Shu Su', 'Yuheng Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'cultural alignment', 'mixture-of-experts / adapters', 'fairness / values representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04885</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection</title><link>https://arxiv.org/abs/2601.04833</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies "Late-Stage Volatility Decay": autoregressive AI text shows rapidly stabilizing token log-probability fluctuations later in generation, whereas human text retains higher variability.&lt;/li&gt;&lt;li&gt;Introduces two late-stage features—Derivative Dispersion and Local Volatility—computed from token-level statistics without needing model access or perturbation sampling.&lt;/li&gt;&lt;li&gt;Evaluated on ~120k samples and achieves state-of-the-art zero-shot detection performance on EvoBench and MAGE, and complements existing global detection methods.&lt;/li&gt;&lt;li&gt;Quantifies effect size: AI-generated text exhibits ~24–32% lower volatility in the second half of sequences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ke Sun', 'Guangsheng Bao', 'Han Cui', 'Yue Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'forensics', 'autoregressive models', 'zero-shot detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04833</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval</title><link>https://arxiv.org/abs/2601.04742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Tool-MAD: a multi-agent debate framework where each agent uses a distinct external tool (e.g., search API, RAG) to reduce hallucinations and improve factual verification.&lt;/li&gt;&lt;li&gt;Introduces an adaptive query formulation mechanism that iteratively refines evidence retrieval based on debate dynamics.&lt;/li&gt;&lt;li&gt;Integrates quantitative Faithfulness and Answer Relevance scores into a Judge agent's final decision to detect hallucinations and align answers with the question.&lt;/li&gt;&lt;li&gt;Reports up to 5.5% accuracy improvement over prior MAD frameworks on four fact verification benchmarks and shows robustness in medically specialized domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyeon Jeong', 'Yeonjun Choi', 'JongWook Kim', 'Beakcheol Jang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multi-agent debate', 'retrieval-augmented generation', 'factuality evaluation', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04742</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation</title><link>https://arxiv.org/abs/2601.04740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end framework that uses knowledge-graph-guided generation to produce domain-relevant harmful prompts for specialized domains (e.g., finance, healthcare).&lt;/li&gt;&lt;li&gt;Introduces dual-path obfuscation rewriting (direct and context-enhanced) to convert explicit harmful prompts into more implicit, harder-to-detect variants.&lt;/li&gt;&lt;li&gt;Creates high-quality datasets combining domain relevance and implicitness to enable more realistic red-teaming and safety evaluation of LLM defenses.&lt;/li&gt;&lt;li&gt;Releases code and datasets to support research on domain-specific LLM safety, prompt obfuscation, and red-team testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huawei Zheng', 'Xinqi Jiang', 'Sen Yang', 'Shouling Ji', 'Yingcai Wu', 'Dazhen Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'harmful prompt generation', 'prompt obfuscation / jailbreaking', 'safety datasets', 'domain-specific risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04740</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs</title><link>https://arxiv.org/abs/2601.04736</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents InterSafe-V, an open-source multi-modal multi-turn dialogue dataset (11,270 dialogues + 500 refusal VQA samples) designed to reflect real-world safety attacks and refusal scenarios.&lt;/li&gt;&lt;li&gt;Proposes AM^3Safety: a training framework combining a cold-start refusal phase with Group Relative Policy Optimization (GRPO) using turn-aware dual-objective rewards across dialogues to align MLLMs.&lt;/li&gt;&lt;li&gt;Empirical results: &gt;10% reduction in Attack Success Rate and ≥8% increase in harmlessness and &gt;13% increase in helpfulness on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B, while preserving general capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Zhu', 'Jiale Chen', 'Chengkun Cai', 'Shengjie Sun', 'Haoran Li', 'Yujin Zhou', 'Chi-Min Chan', 'Pengcheng Wen', 'Lei Li', 'Sirui Han', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multimodal safety', 'RLHF', 'red-teaming', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04736</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs</title><link>https://arxiv.org/abs/2601.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ViHallu, a 10,000-sample annotated dataset of (context, prompt, response) labeled as no hallucination, intrinsic, or extrinsic.&lt;/li&gt;&lt;li&gt;Organizes prompts into factual, noisy, and adversarial types and runs the DSC2025 shared task with 111 teams; best system achieved macro-F1 84.80% vs baseline 32.83%.&lt;/li&gt;&lt;li&gt;Finds instruction-tuned LLMs with structured prompting and ensembles outperform encoder-only baselines, but intrinsic (contradiction) hallucinations remain notably challenging.&lt;/li&gt;&lt;li&gt;Provides a standardized benchmark and analysis for hallucination detection in Vietnamese LLMs, including stress-testing with adversarial prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anh Thi-Hoang Nguyen', 'Khanh Quoc Tran', 'Tin Van Huynh', 'Phuoc Tan-Hoang Nguyen', 'Cam Tan Nguyen', 'Kiet Van Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'benchmark/dataset', 'red teaming', 'Vietnamese NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04711</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation</title><link>https://arxiv.org/abs/2601.04692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a few-shot multimodal agent framework that jointly addresses detection, explanation, and intervention for hateful memes.&lt;/li&gt;&lt;li&gt;Leverages generative multimodal models and task-specific agents to operate under limited annotated data, aiming for generalizability across meme types.&lt;/li&gt;&lt;li&gt;Emphasizes explainability and pre-posting intervention alongside detection to better reflect real-world moderation workflows and potential deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naquee Rizwan', 'Subhankar Swain', 'Paramananda Bhaskar', 'Gagan Aryan', 'Shehryaar Shah Khan', 'Animesh Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'multimodal models', 'safety', 'few-shot learning', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04692</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ToolGate: Contract-Grounded and Verified Tool Execution for LLMs</title><link>https://arxiv.org/abs/2601.04688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToolGate, a framework that maintains an explicit typed symbolic key-value state to represent trusted world information during LLM reasoning.&lt;/li&gt;&lt;li&gt;Formalizes tools with Hoare-style contracts (preconditions and postconditions) to gate invocation and verify results before committing state updates.&lt;/li&gt;&lt;li&gt;Performs runtime verification so the symbolic state evolves only through verified tool executions, preventing hallucinated or invalid results from corrupting the world representation.&lt;/li&gt;&lt;li&gt;Provides experimental evidence that ToolGate improves reliability and verifiability of tool-augmented LLM systems on complex multi-step reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanming Liu', 'Xinyue Peng', 'Jiannan Cao', 'Xinyi Wang', 'Songhang Deng', 'Jintao Chen', 'Jianwei Yin', 'Xuhong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'tool-augmented LLMs', 'formal verification', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04688</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark</title><link>https://arxiv.org/abs/2601.04633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAGA, a pipeline to generate machine-generated text with enhanced alignment across prompt construction and reasoning to create challenging examples for detectors.&lt;/li&gt;&lt;li&gt;Introduces RLDF (Reinforced Learning from Detectors Feedback) to optimize generations that both evade detectors and help improve detector generalization when used for fine-tuning.&lt;/li&gt;&lt;li&gt;Empirical results: RoBERTa fine-tuned on MAGA shows an average +4.60% generalization AUC, while MAGA examples reduce selected detectors' AUC by an average of 8.13%, demonstrating adversarial efficacy and utility for robustification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anyang Song', 'Ying Cheng', 'Yiqian Xu', 'Rui Feng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial generation', 'detection robustness', 'LLM red teaming', 'dataset augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04633</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation</title><link>https://arxiv.org/abs/2601.04534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of state-of-the-art text watermarking methods (KGW, EXP, Waterfall) for Bangla LLM generation, showing detection accuracy collapses (to 9-13%) under cross-lingual round-trip translation (RTT) attacks.&lt;/li&gt;&lt;li&gt;Proposes a layered watermarking strategy combining embedding-time and post-generation watermarks; improves post-RTT detection accuracy by 25-35%, reaching 40-50% at the cost of controlled semantic degradation.&lt;/li&gt;&lt;li&gt;Quantifies the robustness–quality trade-off for multilingual watermarking and offers a practical, training-free mitigation for low-resource languages like Bangla.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Bin Tariqul', 'A N M Zahid Hossain Milkan', 'Sahab-Al-Chowdhury', 'Syed Rifat Raiyan', 'Hasan Mahmud', 'Md Kamrul Hasan']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model_provenance', 'adversarial_robustness', 'low_resource_languages']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04534</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence</title><link>https://arxiv.org/abs/2601.04525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a reinforcement-learning framework for Retrieval-Augmented Generation that jointly trains models to assess evidence sufficiency, extract supporting evidence, and either answer or explicitly abstain.&lt;/li&gt;&lt;li&gt;Uses a data construction pipeline with heterogeneous retrievers to generate diverse training samples without heavy manual annotation.&lt;/li&gt;&lt;li&gt;Introduces a multi-stage gated reward function to guide the model's decision-making between responding and rejecting based on retrieved context.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art overall accuracy on two benchmarks while reducing annotation costs (~10% of prior methods) and improving the balance between accurate responses and rejections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yibo Zhao', 'Jiapeng Zhu', 'Zichen Ding', 'Xiang Li']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'abstention/rejection', 'reinforcement learning', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04525</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions</title><link>https://arxiv.org/abs/2601.04465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Concept Tokens: learn a single special token embedding from multiple natural-language definitions while keeping a pretrained LLM frozen, using standard LM objective.&lt;/li&gt;&lt;li&gt;Evaluates effect on hallucinations in closed-book QA (HotpotQA): negating a hallucination token increases abstentions (reducing hallucinations), asserting it increases hallucinations and lowers precision.&lt;/li&gt;&lt;li&gt;Shows application to pedagogical recasting for language teaching; concept tokens better preserve compliance with other instructions compared to providing the full definitional corpus in-context.&lt;/li&gt;&lt;li&gt;Includes qualitative analyses (Eiffel Tower vs. fictional 'Austral Tower') illustrating what information the learned embeddings capture and their limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ignacio Sastre', "Aiala Ros\\'a"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM behavior control', 'Hallucination mitigation', 'Alignment', 'Concept embeddings']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04465</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models</title><link>https://arxiv.org/abs/2601.04448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MB-Defense, a two-stage defensive poisoning pipeline for instruction-tuned LLMs: (i) merge attacker and defensive triggers into a unified backdoor representation, (ii) perform weight recovery via additional training to break that representation and restore clean behavior.&lt;/li&gt;&lt;li&gt;Aims to immunize models against diverse backdoor threats introduced during data collection/poisoning while preserving instruction-following ability.&lt;/li&gt;&lt;li&gt;Claims data-efficient and generalizable defense across multiple LLMs, substantially reducing attack success rates against unseen backdoor attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['San Kim', 'Gary Geunbae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'data poisoning', 'instruction-tuned LLMs', 'model robustness', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04448</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs</title><link>https://arxiv.org/abs/2601.04435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames LLM failures to challenge harmful user beliefs as pragmatic: models default to accommodating user assumptions and show low epistemic vigilance.&lt;/li&gt;&lt;li&gt;Finds that linguistic and social factors known to affect human accommodation (at-issueness, linguistic encoding, source reliability) similarly influence LLM behavior.&lt;/li&gt;&lt;li&gt;Evaluates across safety benchmarks (Cancer-Myth, SAGE-Eval, ELEPHANT) and shows simple pragmatic interventions (e.g., adding “wait a minute”) substantially improve challenge rates while keeping false positives low.&lt;/li&gt;&lt;li&gt;Argues for incorporating pragmatic considerations into safety evaluation and mitigation strategies for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myra Cheng', 'Robert D. Hawkins', 'Dan Jurafsky']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'misinformation', 'pragmatics', 'sycophancy', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04435</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models</title><link>https://arxiv.org/abs/2601.04394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARREST, an external network that monitors and intervenes in LLM latent activations to correct representational 'drifts' causing hallucinations and unsafe outputs without fine-tuning the base model.&lt;/li&gt;&lt;li&gt;Framework supports both factual corrections and soft/hard refusals, trained with adversarial techniques to improve resilience compared to RLHF-aligned models.&lt;/li&gt;&lt;li&gt;Empirical results claim improved regulation of misalignment and greater versatility in generating soft refusals under adversarial conditions; code is released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sharanya Dasgupta', 'Arkaprabha Basu', 'Sujoy Nath', 'Swagatam Das']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'hallucination mitigation', 'adversarial robustness', 'refusal generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04394</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking</title><link>https://arxiv.org/abs/2601.04389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MiJaBench, a bilingual (English/Portuguese) adversarial benchmark with 44,000 prompts targeting 16 minority groups to reveal demographic-specific safety failures.&lt;/li&gt;&lt;li&gt;Generates 528,000 prompt-response pairs across 12 state-of-the-art LLMs and curates MiJaBench-Align to measure model refusal/defense rates per group.&lt;/li&gt;&lt;li&gt;Finds large within-model disparities (up to 33% variation by target group) and shows model scaling can exacerbate these demographic safety gaps.&lt;/li&gt;&lt;li&gt;Concludes current alignment methods do not ensure nondiscrimination and calls for granular demographic alignment; releases datasets and scripts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Iago Alves Brito', 'Walcy Santos Rezende Rios', 'Julia Soares Dollis', 'Diogo Fernandes Costa Silva', 'Arlindo Rodrigues Galv\\~ao Filho']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'adversarial-benchmark', 'bias/discrimination', 'alignment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04389</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning</title><link>https://arxiv.org/abs/2601.04278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines two unlearning granularities for LLMs—domain-level and instance-level—and argues current benchmarks miss true forgetting scope.&lt;/li&gt;&lt;li&gt;Proposes BiForget, a framework that synthesizes forget sets by eliciting data from the target model via seed-guided and adversarial prompting rather than external generators.&lt;/li&gt;&lt;li&gt;Shows BiForget yields higher relevance and diversity and smaller dataset sizes, enabling more robust forgetting while preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Minxin Du', 'Zitong Li', 'Zi Liang', 'Zhibiao Guo', 'Shiyu Zhang', 'Peizhao Hu', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'privacy', 'data synthesis', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04278</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TrueBrief: Faithful Summarization through Small Language Models</title><link>https://arxiv.org/abs/2601.04212</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrueBrief, a framework to improve faithfulness of small language models (SLMs) for text summarization via preference-optimization.&lt;/li&gt;&lt;li&gt;Uses a data generation module that injects controlled hallucinations to produce synthetic preference data for training.&lt;/li&gt;&lt;li&gt;Analyzes how data quality and model size influence the effectiveness of preference-based optimization for reducing hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kumud Lakara', 'Ruibo Shi', 'Fran Silavong']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'summarization', 'preference optimization', 'small LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04212</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays</title><link>https://arxiv.org/abs/2601.04211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;End-to-end system (Qwerty AI) for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ.&lt;/li&gt;&lt;li&gt;Segments full-length scripts into narrative units, detects violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns explainable age ratings (0+, 6+, 12+, 16+, 18+).&lt;/li&gt;&lt;li&gt;Implementation: fine-tuned Phi-3-mini with 4-bit quantization, processes up to 700 pages in under 2 minutes, reports ~80% rating accuracy and 80–95% segmentation precision; developed under no-external-API, 80GB VRAM, &lt;5-minute processing constraints.&lt;/li&gt;&lt;li&gt;Deployed on Yandex Cloud for production editorial workflows; developed during Wink hackathon (Nov 2025).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (applied system)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita Zmanovskii']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-evaluation', 'explainability', 'LLM-deployment', 'applied-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04211</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis</title><link>https://arxiv.org/abs/2601.04207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that LLMs encode political ideology in low-dimensional structures that are only partially aligned with human ideological spaces.&lt;/li&gt;&lt;li&gt;Introduces a lightweight linear probe to quantify ideological misalignment from internal model features.&lt;/li&gt;&lt;li&gt;Proposes a simple logit-steering correction that adjusts final output probabilities (via a bias score) rather than retraining the model.&lt;/li&gt;&lt;li&gt;Claims the method is low-cost, preserves original reasoning capabilities, and enables annotator-specific alignment for social media analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Xia', 'Haowen Tang', 'Luozheng Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'logit steering', 'model bias', 'interpretability', 'political ideology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04207</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MedPI: Evaluating AI Systems in Medical Patient-facing Interactions</title><link>https://arxiv.org/abs/2601.04195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedPI, a high-dimensional benchmark for evaluating LLMs in patient–clinician dialogues across 105 dimensions mapped to clinical competencies.&lt;/li&gt;&lt;li&gt;Benchmark composed of five layers: synthetic patient packets, LLM-based simulated patients, a task matrix of encounter types/objectives, a 105-dimension evaluation rubric, and committee-based LLM 'AI Judges' that produce scores and evidence-linked rationales.&lt;/li&gt;&lt;li&gt;Evaluates 9 flagship LLMs over 7,097 conversations and finds generally low performance on many safety-critical dimensions, notably differential diagnosis and treatment-safety related items.&lt;/li&gt;&lt;li&gt;Designed to guide safe use and future development of LLMs for diagnosis and treatment recommendations in patient-facing medical contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diego Fajardo V.', 'Oleksii Proniakin', 'Victoria-Elisabeth Gruber', 'Razvan Marinescu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'medical-LLM', 'benchmarking', 'patient-facing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04195</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</title><link>https://arxiv.org/abs/2601.03470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a maturity-based certification framework for embodied AI systems that emphasizes structured assessment frameworks and quantitative scoring mechanisms for trustworthiness.&lt;/li&gt;&lt;li&gt;Uses uncertainty quantification as an exemplar measurement mechanism to demonstrate how to navigate multi-objective trade-offs in evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates feasibility through a case study on Uncrewed Aircraft System (UAS) detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael C. Darling', 'Alan H. Hesu', 'Michael A. Mardikes', 'Brian C. McGuigan', 'Reed M. Milewicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'certification/standards', 'uncertainty quantification', 'trustworthiness', 'embodied AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03470</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box jailbreak attack on large vision-language models using Zeroth-Order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;ZO-SPSA is gradient-free (requires only input-output queries), model-agnostic (no surrogate model), and more resource-efficient than white-box methods.&lt;/li&gt;&lt;li&gt;Reports high empirical success: 83.0% attack success rate on InstructBLIP and strong transferability (64.18% ASR) from adversarial examples generated on MiniGPT-4 to other LVLMs.&lt;/li&gt;&lt;li&gt;Concludes that current LVLM safety mechanisms are vulnerable in realistic black-box scenarios, highlighting real-world feasibility of such jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial examples', 'black-box attacks', 'vision-language models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Reward Model Selection Crisis in Personalized Alignment</title><link>https://arxiv.org/abs/2512.23067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that upstream reward-model (RM) accuracy (preference ranking) correlates poorly with downstream behavioral performance when using inference-time adaptation like reward-guided decoding (policy accuracy Kendall's tau = 0.08–0.31).&lt;/li&gt;&lt;li&gt;Introduces policy accuracy, a metric measuring whether RGD-adapted LLMs discriminate preferred vs dispreferred responses, and presents Pref-LaMP, a personalized alignment benchmark with ground-truth user completions for direct behavioral evaluation.&lt;/li&gt;&lt;li&gt;Finds a decoupling between discriminative ranking and generation outcomes: large RM accuracy differences can yield near-identical output quality, and high-ranking RMs can still fail to produce aligned generations.&lt;/li&gt;&lt;li&gt;Empirically shows simple in-context learning (ICL) often outperforms reward-guided methods for models &gt;=3B parameters, suggesting current reward-based personalization methods may not be operationally effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fady Rezk', 'Yuangang Pan', 'Chuan-Sheng Foo', 'Xun Xu', 'Nancy Chen', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward models', 'evaluation', 'deployment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23067</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling</title><link>https://arxiv.org/abs/2508.03296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Hi-Guard: a hierarchical multimodal moderation pipeline with a lightweight binary filter for safe content and a stronger model for fine-grained, path-based classification over a hierarchical taxonomy.&lt;/li&gt;&lt;li&gt;Directly incorporates moderation rule definitions into model prompts to enable policy-aligned reasoning and improve interpretability/explanations.&lt;/li&gt;&lt;li&gt;Introduces a multi-level soft-margin reward and Group Relative Policy Optimization (GRPO) to penalize semantically adjacent misclassifications and enhance structured prediction and explanation quality.&lt;/li&gt;&lt;li&gt;Claims superior classification accuracy, generalization, and interpretability with real-world deployment and released code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anqi Li', 'Wenwei Jin', 'Jintao Tong', 'Pengda Qin', 'Weijia Li', 'Guo Lu']&lt;/li&gt;&lt;li&gt;Tags: ['content moderation', 'policy-aligned reasoning', 'interpretability', 'hierarchical classification', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03296</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring the limits of strong membership inference attacks on large language models</title><link>https://arxiv.org/abs/2505.18773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Scales LiRA (a strong membership inference attack) to GPT-2 models ranging 10M–1B parameters, training reference models on &gt;20B C4 tokens.&lt;/li&gt;&lt;li&gt;Finds strong MIAs can succeed on pre-trained LLMs but their practical effectiveness is limited (e.g., AUC &lt; 0.7).&lt;/li&gt;&lt;li&gt;Demonstrates large per-sample decision instability: many membership decisions are statistically indistinguishable from random due to training randomness.&lt;/li&gt;&lt;li&gt;Shows the relationship between MIA success and other LLM privacy metrics is complex and not straightforwardly predictive.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamie Hayes', 'Ilia Shumailov', 'Christopher A. Choquette-Choo', 'Matthew Jagielski', 'George Kaissis', 'Milad Nasr', 'Sahra Ghalebikesabi', 'Meenatchi Sundaram Mutu Selva Annamalai', 'Niloofar Mireshghallah', 'Igor Shilov', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye', 'Katherine Lee', 'Franziska Boenisch', 'Adam Dziedzic', 'A. Feder Cooper']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'LLM-security', 'empirical-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18773</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Practical Poisoning Attacks against Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2504.03957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CorruptRAG, a practical poisoning attack against Retrieval-Augmented Generation (RAG) that only requires injecting a single poisoned text.&lt;/li&gt;&lt;li&gt;Addresses limitations of prior poisoning attacks which assume many poisoned documents per query, improving feasibility and stealth.&lt;/li&gt;&lt;li&gt;Evaluates the attack on multiple large-scale datasets and reports higher attack success rates than existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baolei Zhang', 'Yuxi Chen', 'Zhuqing Liu', 'Lihai Nie', 'Tong Li', 'Zheli Liu', 'Minghong Fang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'retrieval-augmented generation', 'poisoning attacks', 'adversarial NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03957</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Diagram of Thought</title><link>https://arxiv.org/abs/2409.10038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Diagram of Thought (DoT): a framework for a single LLM to build and navigate a dynamic internal graph/diagram of ideas for structured multi-step reasoning.&lt;/li&gt;&lt;li&gt;DoT supports proposing alternative reasoning paths, self-critique, and synthesis into final conclusions, producing a fully auditable step-by-step trace of the model's thinking.&lt;/li&gt;&lt;li&gt;Framework is grounded in category theory to provide formal guarantees of logical consistency and robustness (e.g., order-invariance) when combining information.&lt;/li&gt;&lt;li&gt;Emphasizes a self-contained, efficient reasoning process within the model (avoiding external controllers or search algorithms) and improved transparency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Zhang', 'Yang Yuan', 'Andrew Chi-Chih Yao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reasoning', 'interpretability', 'formal methods', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.10038</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</title><link>https://arxiv.org/abs/2511.14195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes N-GLARE, a non-generative LLM safety evaluator that analyzes latent representations instead of generating full outputs.&lt;/li&gt;&lt;li&gt;Introduces APT (Angular-Probabilistic Trajectory) analysis of hidden-layer dynamics and the JSS (Jensen-Shannon Separability) metric to quantify safety separability.&lt;/li&gt;&lt;li&gt;Evaluated on 40+ models and 20 red-teaming strategies, showing JSS aligns with red-team safety rankings while using &lt;1% of token and runtime costs.&lt;/li&gt;&lt;li&gt;Aims to provide an efficient, output-free proxy for real-time diagnostics of model safety post-training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheyu Lin', 'Jirui Yang', 'Yukui Qiu', 'Hengqi Guo', 'Yubing Bao', 'Yao Guan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'LLM safety evaluation', 'latent representations', 'jailbreak detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14195</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PEAR: Planner-Executor Agent Robustness Benchmark</title><link>https://arxiv.org/abs/2510.07505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PEAR, a benchmark for systematically evaluating utility and vulnerability of planner-executor LLM-based multi-agent systems (MAS), focusing on the planner-executor architecture.&lt;/li&gt;&lt;li&gt;Empirical findings: weak planners degrade clean task performance more than weak executors; planner memory is essential while executor memory has little impact; there is a trade-off between task performance and robustness; attacks targeting the planner are especially effective.&lt;/li&gt;&lt;li&gt;Provides extensive experiments and actionable insights to guide robustness improvements and defenses in multi-agent settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shen Dong', 'Mingxuan Zhang', 'Pengfei He', 'Li Ma', 'Bhavani Thuraisingham', 'Hui Liu', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'robustness', 'adversarial attacks', 'benchmark', 'planner-executor']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07505</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title><link>https://arxiv.org/abs/2506.00416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BFEL: a blockchain-enhanced, privacy-preserving second-order federated edge learning framework (based on FedCurv) for personalized healthcare on wearables.&lt;/li&gt;&lt;li&gt;Uses Fisher information (FedCurv) to preserve client-specific knowledge and reduce model drift on non-iid data, aiming to cut communication rounds and improve personalized training.&lt;/li&gt;&lt;li&gt;Adds Ethereum-based auditable model aggregation and public-key encryption for verifiability and privacy; evaluates with CNNs/MLPs on MNIST, CIFAR-10, and PathMNIST showing efficiency and reduced communication cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anum Nawaz', 'Muhammad Irfan', 'Xianjia Yu', 'Hamad Aldawsari', 'Rayan Hamza Alsisi', 'Zhuo Zou', 'Tomi Westerlund']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving', 'blockchain', 'personalization', 'second-order-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00416</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reward Shaping to Mitigate Reward Hacking in RLHF</title><link>https://arxiv.org/abs/2502.18770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking as a core failure mode in RLHF and studies common reward shaping techniques, deriving two design principles: bounded rewards and rapid initial growth with gradual convergence.&lt;/li&gt;&lt;li&gt;Proposes Preference As Reward (PAR), which uses latent preferences from the reward model as the RL signal and yields variance-reduction properties that stabilize training.&lt;/li&gt;&lt;li&gt;Demonstrates empirical gains on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets, outperforming other shaping methods (≥5 percentage points on AlpacaEval 2.0).&lt;/li&gt;&lt;li&gt;Shows strong data efficiency (one reference reward needed) and robustness to reward hacking even after extended training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Fu', 'Xuandong Zhao', 'Chengyuan Yao', 'Heng Wang', 'Qi Han', 'Yanghua Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward-hacking', 'reward-shaping', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18770</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quaternion-Hadamard Network: A Novel Defense Against Adversarial Attacks with a New Dataset</title><link>https://arxiv.org/abs/2502.10452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QHNet, a purification-based defense for adverse-weather image restoration that operates in transform and quaternion domains to suppress adversarial perturbations while preserving structural details.&lt;/li&gt;&lt;li&gt;Introduces Quaternion Hadamard Polynomial Denoising Block (QHPDB) and Quaternion Denoising Residual Block (QDRB) inside an encoder–decoder to remove high-frequency adversarial noise.&lt;/li&gt;&lt;li&gt;Evaluates robustness on rain, snow, and haze removal tasks (with a new dataset) using PSNR/SSIM and tests against adaptive white-box attacks (PGD, BPDA, EOT).&lt;/li&gt;&lt;li&gt;Reports improved restoration fidelity and robustness versus state-of-the-art purification baselines in low-level vision pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vladimir Frants', 'Sos Agaian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'purification-based defense', 'adaptive white-box attacks', 'adversarial robustness', 'image restoration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10452</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Meta-Learning Objectives for Preference Optimization</title><link>https://arxiv.org/abs/2411.06568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a diagnostic MuJoCo-based benchmark suite to cheaply and systematically evaluate preference optimization (PO) algorithms as surrogates for costly LLM alignment evaluations.&lt;/li&gt;&lt;li&gt;Proposes a novel family of PO algorithms, Mirror Preference Optimization (MPO), based on mirror descent, and uses evolutionary strategies to discover specialized algorithms for dataset properties (e.g., noisy or mixed-quality preferences).&lt;/li&gt;&lt;li&gt;Shows discovered MPO variants outperform existing PO algorithms on MuJoCo tasks and uses insights to design a PO method that improves performance on an LLM alignment task.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Carlo Alfano', 'Silvia Sapora', 'Jakob Nicolaus Foerster', 'Patrick Rebeschini', 'Yee Whye Teh']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'meta-learning', 'benchmarking', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.06568</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</title><link>https://arxiv.org/abs/2601.05242</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode when applying Group Relative Policy Optimization (GRPO) to multi-reward RL: normalizing combined rollout rewards can collapse distinct reward signals into identical advantage values, harming training resolution and stability.&lt;/li&gt;&lt;li&gt;Proposes GDPO (Group reward-Decoupled Normalization Policy Optimization), which normalizes each reward component separately to preserve relative differences and produce more informative advantages.&lt;/li&gt;&lt;li&gt;Evaluates GDPO vs GRPO on three language-model-centric tasks (tool calling, math reasoning, coding reasoning), reporting improved correctness (accuracy, bug ratio) and constraint adherence (format, length) as well as greater training stability.&lt;/li&gt;&lt;li&gt;Focus is on multi-reward RL optimization and alignment to diverse user preferences via improved training dynamics, rather than on adversarial attacks or red-teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shih-Yang Liu', 'Xin Dong', 'Ximing Lu', 'Shizhe Diao', 'Peter Belcak', 'Mingjie Liu', 'Min-Hung Chen', 'Hongxu Yin', 'Yu-Chiang Frank Wang', 'Kwang-Ting Cheng', 'Yejin Choi', 'Jan Kautz', 'Pavlo Molchanov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-reward reinforcement learning', 'policy optimization', 'training stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05242</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Compositional Steering of Large Language Models with Steering Tokens</title><link>https://arxiv.org/abs/2601.05062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces compositional steering tokens that embed individual behaviors (natural-language instructions) into dedicated input tokens via self-distillation.&lt;/li&gt;&lt;li&gt;Trains a composition token on behavior pairs to enable zero-shot composition, generalizing to unseen behavior combinations and different numbers of behaviors.&lt;/li&gt;&lt;li&gt;Operates in the input-token space (not activation space), yielding better zero-shot composition than activation steering, instruction prompting, and LoRA merging.&lt;/li&gt;&lt;li&gt;Empirical results across multiple LLM architectures show improved multi-behavior control; steering tokens also complement natural-language instructions for further gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gorjan Radevski', 'Kiril Gashteovski', 'Giwon Hong', 'Carolin Lawrence', 'Goran Glava\\v{s}']&lt;/li&gt;&lt;li&gt;Tags: ['LLM steering', 'Compositionality', 'Alignment/Control', 'Prompt engineering', 'Model steering tokens']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05062</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rotation-Robust Regression with Convolutional Model Trees</title><link>https://arxiv.org/abs/2601.04899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes geometry-aware inductive biases for Convolutional Model Trees (CMTs) to improve rotation robustness: convolutional smoothing, tilt dominance constraint, and importance-based pruning.&lt;/li&gt;&lt;li&gt;Evaluates deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without changing model parameters.&lt;/li&gt;&lt;li&gt;Finds orientation search helps under severe in-plane rotations but can degrade performance near the canonical orientation when confidence is misaligned with correctness.&lt;/li&gt;&lt;li&gt;Experiments on MNIST (regression and one-vs-rest recognition) demonstrate benefits and limitations of confidence-based orientation selection for model-tree ensembles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Li', 'William Ward Armstrong', 'Jun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'geometric_transformations', 'model_trees', 'deployment-time_adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04899</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>V-FAT: Benchmarking Visual Fidelity Against Text-bias</title><link>https://arxiv.org/abs/2601.04897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces V-FAT, a diagnostic benchmark (4,026 VQA instances across six semantic domains) to measure reliance of MLLMs on linguistic priors (Text Bias) versus true visual grounding.&lt;/li&gt;&lt;li&gt;Proposes a Three-Level Evaluation (L1 internal corpus bias via atypical images, L2 external instruction bias via misleading prompts, L3 synergistic bias combining both) and a Visual Robustness Score (VRS) that penalizes lucky linguistic guesses.&lt;/li&gt;&lt;li&gt;Evaluates 12 leading MLLMs and shows significant visual collapse under strong linguistic dominance despite strong performance on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziteng Wang', 'Yujie He', 'Guanliang Li', 'Siqi Yang', 'Jiaqi Xiong', 'Songxiang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'robustness', 'alignment', 'safety-evaluation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04897</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters</title><link>https://arxiv.org/abs/2601.04885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Mean Collapse' when dense LLMs try to fit conflicting cultural value distributions and attributes it to 'Cultural Sparsity' from gradient interference.&lt;/li&gt;&lt;li&gt;Proposes CuMA (Cultural Mixture of Adapters): demographic-aware routing to separate capacity into specialized adapter experts that represent distinct cultural modes.&lt;/li&gt;&lt;li&gt;Shows empirical improvements on WorldValuesBench, Community Alignment, and PRISM, claiming state-of-the-art performance and mitigation of mean collapse versus dense and semantic-only MoE baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ao Sun', 'Xiaoyu Wang', 'Zhe Tan', 'Yu Li', 'Jiachen Zhu', 'Shu Su', 'Yuheng Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'cultural alignment', 'mixture-of-experts / adapters', 'fairness / values representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04885</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization</title><link>https://arxiv.org/abs/2601.04641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-MGTD, an Adaptive Differentially Private Entity Sanitization framework for detecting machine-generated text while protecting sensitive user data.&lt;/li&gt;&lt;li&gt;Uses a two-stage mechanism: noisy frequency estimation and dynamic privacy-budget calibration, applying Laplace mechanism for numerical entities and Exponential mechanism for textual entities.&lt;/li&gt;&lt;li&gt;Reports a counter-intuitive finding that DP noise can increase distinguishability between human and machine text, and shows near-perfect detection on MGTBench-2.0 while satisfying DP guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lionel Z. Wang', 'Yusheng Zhao', 'Jiabin Luo', 'Xinfeng Li', 'Lixu Wang', 'Yinan Peng', 'Haoyang Li', 'XiaoFeng Wang', 'Wei Dong']&lt;/li&gt;&lt;li&gt;Tags: ['Differential Privacy', 'Machine-Generated Text Detection', 'Privacy-Preserving ML', 'Entity Sanitization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04641</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions</title><link>https://arxiv.org/abs/2601.04465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Concept Tokens: learn a single special token embedding from multiple natural-language definitions while keeping a pretrained LLM frozen, using standard LM objective.&lt;/li&gt;&lt;li&gt;Evaluates effect on hallucinations in closed-book QA (HotpotQA): negating a hallucination token increases abstentions (reducing hallucinations), asserting it increases hallucinations and lowers precision.&lt;/li&gt;&lt;li&gt;Shows application to pedagogical recasting for language teaching; concept tokens better preserve compliance with other instructions compared to providing the full definitional corpus in-context.&lt;/li&gt;&lt;li&gt;Includes qualitative analyses (Eiffel Tower vs. fictional 'Austral Tower') illustrating what information the learned embeddings capture and their limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ignacio Sastre', "Aiala Ros\\'a"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM behavior control', 'Hallucination mitigation', 'Alignment', 'Concept embeddings']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04465</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays</title><link>https://arxiv.org/abs/2601.04443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-based framework (fine-tuning compact LLMs like DistilBERT and GPT-2+LoRA) to detect cyberattacks on transformer current differential relays (TCDRs) by textualizing multidimensional TCDR measurements.&lt;/li&gt;&lt;li&gt;Reports high detection performance (97.6% detection) with low inference latency (&lt;6 ms) and maintains TCDR dependability.&lt;/li&gt;&lt;li&gt;Evaluates robustness under combined time-synchronization and false-data-injection attacks, measurement noise, and prompt formulation variants.&lt;/li&gt;&lt;li&gt;Releases the dataset used for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Mohammad Saber', 'Saeed Jafari', 'Zhengmao Ouyang', 'Paul Budnarain', 'Amr Youssef', 'Deepa Kundur']&lt;/li&gt;&lt;li&gt;Tags: ['smart grid security', 'cyber-physical systems', 'anomaly detection', 'LLMs for cybersecurity', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04443</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</title><link>https://arxiv.org/abs/2601.04401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a transformer-encoder-based MARL approach for decentralized aircraft separation assurance using a relative polar state representation to improve generalization across airspace configurations.&lt;/li&gt;&lt;li&gt;Trains across diverse traffic patterns and intersection angles and compares encoder depths (1–3 layers), finding a single-layer encoder performs best.&lt;/li&gt;&lt;li&gt;Reports near-zero near-mid-air collision rates and shorter loss-of-separation infringements versus deeper encoders and a baseline attention-only model.&lt;/li&gt;&lt;li&gt;Demonstrates applicability to both structured and unstructured airspaces, emphasizing adaptability and scalability for safety-critical operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arsyi Aziz', 'Peng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent reinforcement learning', 'transformer', 'air traffic safety', 'generalization', 'separation assurance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04401</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Human-in-the-Loop Testing of AI Agents for Air Traffic Control with a Regulated Assessment Framework</title><link>https://arxiv.org/abs/2601.04288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a human-in-the-loop evaluation framework for AI agents on air traffic control using a regulator-certified simulator curriculum.&lt;/li&gt;&lt;li&gt;Involves expert human instructors and legally regulated assessments to produce domain-authentic performance measurements.&lt;/li&gt;&lt;li&gt;Aims to reduce misalignment between academic simulations and real-world operational complexity and to support future human–machine teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Carvell', 'Marc Thomas', 'Andrew Pace', 'Christopher Dorney', 'George De Ath', 'Richard Everson', 'Nick Pepper', 'Adam Keane', 'Samuel Tomlinson', 'Richard Cannon']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'human-in-the-loop', 'air-traffic-control', 'regulatory-assessment', 'human-machine-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04288</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Future Capabilities Agent for Tactical Air Traffic Control</title><link>https://arxiv.org/abs/2601.04285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Agent Mallard, a rules-based, forward-planning agent for tactical en-route air traffic control that prioritises interpretability and safety assurance.&lt;/li&gt;&lt;li&gt;Embeds a stochastic digital twin into the conflict-resolution loop to validate candidate manoeuvres against uncertain execution scenarios (wind, pilot response, comms loss) before committing.&lt;/li&gt;&lt;li&gt;Uses discrete lane/level choices, hierarchical plans from an expert library, and a depth-limited backtracking search with causal attribution and plan-splicing to seek complete safe plans.&lt;/li&gt;&lt;li&gt;Includes preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin demonstrating alignment with expert reasoning in simplified scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paul Kent', 'George De Ath', 'Martin Layton', 'Allen Hart', 'Richard Everson', 'Ben Carvell']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'interpretable AI / explainability', 'model-based planning', 'digital twin']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04285</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning</title><link>https://arxiv.org/abs/2601.04278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines two unlearning granularities for LLMs—domain-level and instance-level—and argues current benchmarks miss true forgetting scope.&lt;/li&gt;&lt;li&gt;Proposes BiForget, a framework that synthesizes forget sets by eliciting data from the target model via seed-guided and adversarial prompting rather than external generators.&lt;/li&gt;&lt;li&gt;Shows BiForget yields higher relevance and diversity and smaller dataset sizes, enabling more robust forgetting while preserving model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Minxin Du', 'Zitong Li', 'Zi Liang', 'Zhibiao Guo', 'Shiyu Zhang', 'Peizhao Hu', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'privacy', 'data synthesis', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04278</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space</title><link>https://arxiv.org/abs/2601.04266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel backdoor/poisoning attack ('State Backdoor') on Vision-Language-Action (VLA) models that uses the robot's initial state as a stealthy trigger.&lt;/li&gt;&lt;li&gt;Introduces a Preference-guided Genetic Algorithm (PGA) to search the state space for minimal, robust triggers that maintain normal performance on benign inputs.&lt;/li&gt;&lt;li&gt;Evaluates across five VLA models and five real-world tasks, reporting &gt;90% attack success rate while preserving clean-task performance, demonstrating a practical security vulnerability in embodied AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ji Guo', 'Wenbo Jiang', 'Yansong Lin', 'Yijing Liu', 'Ruichen Zhang', 'Guomin Lu', 'Aiguo Chen', 'Xinshuo Han', 'Hongwei Li', 'Dusit Niyato']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'data poisoning', 'embodied AI security', 'adversarial trigger', 'multimodal vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04266</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Reasoning as a Symmetry-Protected Topological Phase</title><link>https://arxiv.org/abs/2601.05240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Holonomic Network architecture that frames robust logical inference as a Symmetry-Protected Topological (SPT) phase, contrasting with current 'Metric Phase' models (Transformers, RNNs).&lt;/li&gt;&lt;li&gt;Reports empirical evidence of a topological phase transition: the Holonomic model exhibits a macroscopic 'mass gap' and maintains fidelity under semantic noise, while Transformers/RNNs show gapless decay.&lt;/li&gt;&lt;li&gt;Demonstrates strong generalization on a variable-binding symbolic task (S_10), with perfect fidelity extrapolating 100× beyond training, and ablations attribute protection to non-Abelian gauge symmetry.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ilmo Sung']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination mitigation', 'logical reasoning', 'model architecture', 'theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05240</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art</title><link>https://arxiv.org/abs/2601.05152</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of continual safe online reinforcement learning (COSRL) methods addressing nonstationary environments and safe adaptation.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of methods by safe learning mechanism and formulates safety constraints for online RL (e.g., constrained MDPs, safe POMDPs, HM-MDP/NSMDP models).&lt;/li&gt;&lt;li&gt;Discusses theoretical aspects, challenges, open questions, and prospects for building reliable safe online/adaptive RL algorithms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Timofey Tomashevskiy']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'continual learning', 'nonstationary environments', 'safe exploration', 'constrained MDPs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05152</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning</title><link>https://arxiv.org/abs/2601.05134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes sequential noise scheduling that injects DP noise across orthogonal subspaces of model parameter space rather than all at once.&lt;/li&gt;&lt;li&gt;Proves that the subspace noise scheme preserves the same (ε, δ) differential privacy (certified unlearning) guarantees.&lt;/li&gt;&lt;li&gt;Shows empirical improvements in post-unlearning accuracy on image classification benchmarks while maintaining robustness to membership inference attacks.&lt;/li&gt;&lt;li&gt;Claims the method mitigates accuracy collapse common to noisy fine-tuning approaches, enabling more practical certified unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Polina Dolgova', 'Sebastian U. Stich']&lt;/li&gt;&lt;li&gt;Tags: ['certified-unlearning', 'differential-privacy', 'privacy-preserving-ml', 'membership-inference', 'noise-injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05134</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>On the Definition and Detection of Cherry-Picking in Counterfactual Explanations</title><link>https://arxiv.org/abs/2601.04977</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'cherry-picking' of counterfactual explanations via an admissible explanation space and a utility function representing the explainer's preference.&lt;/li&gt;&lt;li&gt;Analyzes detectability of manipulative selection under three auditor access models: full procedural access, partial procedural access, and explanation-only access, showing practical limits to detection.&lt;/li&gt;&lt;li&gt;Empirically shows variability in valid counterfactuals often overwhelms the effect of cherry-picking on standard quality metrics (proximity, plausibility, sparsity), making manipulation statistically hard to distinguish.&lt;/li&gt;&lt;li&gt;Argues for safeguards focused on reproducibility, standardization, and procedural constraints rather than reliance on post-hoc detection, and gives recommendations for developers, providers, and auditors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Hinns', 'Sofie Goethals', 'Stephan Van der Veeken', 'Theodoros Evgeniou', 'David Martens']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'counterfactual explanations', 'auditability', 'manipulation/cherry-picking', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04977</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</title><link>https://arxiv.org/abs/2601.04954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Challenges the belief that dataset diversity (mix of hard/verifiable and soft/unverifiable constraints) is necessary for generalizing instruction following; finds hard-only constraints perform better.&lt;/li&gt;&lt;li&gt;Shows reward precision, not constraint diversity, drives alignment; low recall of LLM judges causes reward hacking that negates diversity benefits.&lt;/li&gt;&lt;li&gt;Analyzes attention patterns suggesting high-precision rewards induce a transferable meta-skill for instruction following.&lt;/li&gt;&lt;li&gt;Proposes a data-centric refinement prioritizing reward precision; reports +13.4% performance and 58% reduction in training time across five benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yirong Zeng', 'Yufei Liu', 'Xiao Ding', 'Yutai Hou', 'Yuxian Wang', 'Haonan Song', 'Wu Ning', 'Dandan Tu', 'Qixun Zhang', 'Bibo Cai', 'Yuxiang He', 'Ting Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward hacking', 'LLM evaluation', 'instruction following', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04954</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Excess Description Length of Learning Generalizable Predictors</title><link>https://arxiv.org/abs/2601.04728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Excess Description Length (EDL), defined via prequential coding, as a metric for how much predictive structure fine-tuning extracts from training data and stores in model parameters.&lt;/li&gt;&lt;li&gt;Proves key properties of EDL (non-negative expectation, convergence in the infinite-data limit) and derives bounds relating EDL to expected generalization gain.&lt;/li&gt;&lt;li&gt;Analyzes toy models to clarify distinctions between capability elicitation versus teaching, effects of random labels, rare-input learning, and format-learning transients—linking these phenomena to different scaling signatures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elizabeth Donoway', 'Hailey Joren', 'Fabien Roger', 'Jan Leike']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fine-tuning', 'capability emergence', 'information-theoretic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04728</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead</title><link>https://arxiv.org/abs/2601.04686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Nightmare Dreamer, a model-based safe RL algorithm that uses a learned world model to predict potential safety violations and plan to avoid them.&lt;/li&gt;&lt;li&gt;Claims nearly zero safety violations while still maximizing task reward, improving safety guarantees for RL in robotics-like domains.&lt;/li&gt;&lt;li&gt;Demonstrates substantial efficiency and performance gains versus model-free baselines on Safety Gymnasium tasks using only image observations (≈20× improvement in efficiency).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oluwatosin Oseni', 'Shengjie Wang', 'Jun Zhu', 'Micah Corah']&lt;/li&gt;&lt;li&gt;Tags: ['safe RL', 'model-based RL', 'world models', 'planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04686</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Predictions Shape Reality: A Socio-Technical Synthesis of Performative Predictions in Machine Learning</title><link>https://arxiv.org/abs/2601.04447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review (SoK) of performative prediction: how model deployment changes the environment and outcomes.&lt;/li&gt;&lt;li&gt;Presents mechanisms of performativity, a typology of associated risks (feedback loops, performance degradation, societal harms), and surveyed mitigations.&lt;/li&gt;&lt;li&gt;Proposes a practical "Performative Strength vs. Impact Matrix" to help practitioners assess influence and choose appropriate algorithmic or human interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gal Fybish', 'Teo Susnjak']&lt;/li&gt;&lt;li&gt;Tags: ['performative prediction', 'ML safety', 'deployment risks', 'feedback loops', 'socio-technical']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04447</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards</title><link>https://arxiv.org/abs/2601.04411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an analytically tractable bandit-based model of reinforcement learning with verifiable (but noisy) rewards, modeling false positives and false negatives and grouping completions into recurring reasoning modes.&lt;/li&gt;&lt;li&gt;Derives replicator-style dynamics that decouple into within-mode competition and a 1-D evolution for mass on incorrect modes; shows a phase transition determined by Youden's index J = TPR − FPR.&lt;/li&gt;&lt;li&gt;Shows when J &gt; 0 noisy verification rescales convergence time ('rate, not fate'), J = 0 is neutral, and J &lt; 0 leads to amplification of incorrect modes (anti-learning); validates predictions with experiments on verifiable programming tasks under synthetic noise.&lt;/li&gt;&lt;li&gt;Provides a framework for analyzing RLVR stability and suggests algorithmic interventions to mitigate verifier-induced collapse, with implications for reward hacking, robustness, and alignment of LLM training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Rad', 'Khashayar Filom', 'Darioush Keivan', 'Peyman Mohajerin Esfahani', 'Ehsan Kamalinejad']&lt;/li&gt;&lt;li&gt;Tags: ['RL safety', 'reward hacking', 'robustness', 'verification noise', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04411</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aligned explanations in neural networks</title><link>https://arxiv.org/abs/2601.04378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for 'explanatory alignment'—explanations must be directly linked to the model's prediction process rather than post-hoc rationalizations, and proposes 'model readability' as a design principle to achieve this.&lt;/li&gt;&lt;li&gt;Introduces PiNets (pseudo-linear networks) that yield instance-wise linear predictions in an arbitrary feature space, enabling linearly readable, intrinsically aligned explanations.&lt;/li&gt;&lt;li&gt;Demonstrates PiNets on image classification and segmentation tasks and evaluates explanation faithfulness across multiple criteria.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Corentin Lobet', 'Francesca Chiaromonte']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'interpretability', 'alignment', 'trustworthy-AI', 'model-design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04378</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying the Effect of Test Set Contamination on Generative Evaluations</title><link>https://arxiv.org/abs/2601.04301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively measures how test-set contamination affects generative model evaluation across the model lifecycle (pretraining, further training/finetuning, and inference), using mixtures of web data and the MATH benchmark across model sizes and contamination replicas.&lt;/li&gt;&lt;li&gt;Shows that even a single replica of the test set in pretraining can substantially improve performance and can yield lower loss than the uncontaminated irreducible error according to scaling-law fits.&lt;/li&gt;&lt;li&gt;Finds mitigation behaviors: overtraining on fresh data reduces contamination effects, supervised finetuning can either amplify or reduce test performance depending on prior contamination, and sampling settings (higher temperature) reduce memorization at inference.&lt;/li&gt;&lt;li&gt;Analyzes memorization dynamics for generative outputs, finding longer solutions are exponentially harder to memorize than short ones, which has distinct implications versus discriminative evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rylan Schaeffer', 'Joshua Kazdan', 'Baber Abbasi', 'Ken Ziyu Liu', 'Brando Miranda', 'Ahmed Ahmed', 'Abhay Puri', 'Niloofar Mireshghallah', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['test-set contamination', 'memorization', 'evaluation robustness', 'data leakage', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04301</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity</title><link>https://arxiv.org/abs/2601.04283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode where character-level Transformers trained on modular addition fail under absolute position shifts and out-of-distribution natural-language templates despite high in-distribution accuracy.&lt;/li&gt;&lt;li&gt;Proposes a training recipe combining explicit expression boundary markers, a position curriculum (broadened absolute positions), diverse template mixtures, and consistency training across variants.&lt;/li&gt;&lt;li&gt;Demonstrates substantially improved robustness to position shifts and template OOD while retaining in-distribution performance, and provides a reproducible evaluation protocol and artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolay Yudin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'procedural generalization', 'curriculum learning', 'OOD evaluation', 'position-shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04283</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LEGATO: Good Identity Unlearning Is Continuous</title><link>https://arxiv.org/abs/2601.04282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LEGATO, a method for identity unlearning in generative models using lightweight Neural ODE adapters that leave base model weights frozen.&lt;/li&gt;&lt;li&gt;Models forgetting as a continuous trajectory controllable via ODE step size, enabling interpretable and tunable forgetting intensity.&lt;/li&gt;&lt;li&gt;Introduces trajectory consistency constraints to prevent catastrophic collapse and maintain model retention during progressive unlearning.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art forgetting performance across in- and out-of-domain identity unlearning benchmarks while reducing the number of fine-tuned parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Chen', 'Chun-Wun Cheng', 'Xiu Su', 'Hongyan Xu', 'Xi Lin', 'Shan You', 'Angelica I. Aviles-Rivero', 'Yi Chen']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy-preserving ML', 'model editing', 'robustness', 'neural ODE']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04282</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis</title><link>https://arxiv.org/abs/2601.04262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Conflict-Aware Sparse Tuning (CAST): build a head-level conflict map by combining optimization conflict and functional sensitivity, then selectively update parameters during alignment.&lt;/li&gt;&lt;li&gt;Finds that safety-utility conflicts in LLMs are not uniformly distributed across attention heads; a small set of high-conflict heads drive most capability degradation.&lt;/li&gt;&lt;li&gt;Shows that skipping updates to those high-conflict heads during fine-tuning preserves safety gains while substantially reducing loss in general capabilities, yielding an interpretable and parameter-efficient alignment method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Cai', 'Yilin Wen', 'Jinchang Hou', 'Du Su', 'Guoqiu Wang', 'Zhonghou Lv', 'Chenfu Bao', 'Yunfang Wu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety-utility trade-off', 'sparse fine-tuning', 'attention head analysis', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04262</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs</title><link>https://arxiv.org/abs/2601.04199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multidimensional safety evaluation framework and empirical benchmarking showing pervasive vulnerabilities in state-of-the-art Medical MLLMs, including cross-modality jailbreaks and loss of prior safety alignment after medical fine-tuning.&lt;/li&gt;&lt;li&gt;Diagnoses catastrophic forgetting of base-model safety during domain-specific fine-tuning and demonstrates fragility across general and medical-specific safety dimensions.&lt;/li&gt;&lt;li&gt;Proposes a 'Parameter-Space Intervention' method that extracts and injects intrinsic safety representations from base models into target medical MLLMs, plus a fine-grained parameter search to balance safety and medical performance without extra domain safety data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Zhao', 'Xing Mou', 'Jinlin Wu', 'Hongyuan Yu', 'Mingrui Sun', 'Yang Shi', 'Xuanwu Yin', 'Zhen Chen', 'Zhen Lei', 'Yaohua Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model alignment', 'jailbreaking / red teaming', 'parameter-space interventions', 'safety evaluation / benchmarking', 'medical MLLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04199</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Representational Contrastive Scoring (RCS) that inspects internal LVLM representations and learns a lightweight projection to separate benign and malicious inputs.&lt;/li&gt;&lt;li&gt;Introduces two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—for lightweight jailbreak detection.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art generalization to unseen multimodal jailbreak attack types with a focus on practical, low-overhead deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM/LVLM security', 'anomaly detection', 'representation-based detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EngTrace: A Symbolic Benchmark for Verifiable Process Supervision of Engineering Reasoning</title><link>https://arxiv.org/abs/2511.01650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EngTrace, a symbolic benchmark with 90 templates and 1,350 contamination-resistant engineering test cases across multiple branches and domains.&lt;/li&gt;&lt;li&gt;Proposes a verifiable two-stage evaluation that checks intermediate reasoning traces and final answers via automated procedural checks and a heterogeneous AI Tribunal.&lt;/li&gt;&lt;li&gt;Evaluates 24 LLMs, revealing a trade-off between numeric precision and trace fidelity and identifying a complexity cliff where pretraining fails to yield integrative engineering reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayesha Gull', 'Muhammad Usman Safder', 'Rania Elbadry', 'Fan Zhang', 'Veselin Stoyanov', 'Preslav Nakov', 'Zhuohan Xie']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarks', 'verifiable reasoning', 'engineering reasoning', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01650</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</title><link>https://arxiv.org/abs/2511.00088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Alpamayo-R1 (AR1), a vision-language-action model that fuses causal reasoning traces with trajectory planning to improve performance in safety-critical long-tail driving scenarios.&lt;/li&gt;&lt;li&gt;Presents the Chain of Causation (CoC) dataset with decision-grounded, causally linked reasoning traces created via hybrid auto-labeling and human-in-the-loop annotation.&lt;/li&gt;&lt;li&gt;Proposes a modular architecture combining a vision-language reasoning backbone (Cosmos-Reason) with a diffusion-based trajectory decoder and a multi-stage training pipeline (supervised fine-tuning + RL) to enforce reasoning-action consistency.&lt;/li&gt;&lt;li&gt;Reports empirical safety/robustness gains (e.g., up to 12% planning accuracy improvement, 35% reduction in close encounters) and real-time on-vehicle performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NVIDIA', ':', 'Yan Wang', 'Wenjie Luo', 'Junjie Bai', 'Yulong Cao', 'Tong Che', 'Ke Chen', 'Yuxiao Chen', 'Jenna Diamond', 'Yifan Ding', 'Wenhao Ding', 'Liang Feng', 'Greg Heinrich', 'Jack Huang', 'Peter Karkus', 'Boyi Li', 'Pinyi Li', 'Tsung-Yi Lin', 'Dongran Liu', 'Ming-Yu Liu', 'Langechuan Liu', 'Zhijian Liu', 'Jason Lu', 'Yunxiang Mao', 'Pavlo Molchanov', 'Lindsey Pavao', 'Zhenghao Peng', 'Mike Ranzinger', 'Ed Schmerling', 'Shida Shen', 'Yunfei Shi', 'Sarah Tariq', 'Ran Tian', 'Tilman Wekel', 'Xinshuo Weng', 'Tianjun Xiao', 'Eric Yang', 'Xiaodong Yang', 'Yurong You', 'Xiaohui Zeng', 'Wenyuan Zhang', 'Boris Ivanovic', 'Marco Pavone']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety', 'robustness', 'reasoning', 'imitation-learning-and-RL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00088</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title><link>https://arxiv.org/abs/2510.20721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;User study (n=94) using 90 PrivacyLens scenarios comparing human judgments of LLM responses on helpfulness and privacy-preservation.&lt;/li&gt;&lt;li&gt;Humans showed low inter-rater agreement on identical LLM responses, while five proxy LLMs showed high agreement with each other.&lt;/li&gt;&lt;li&gt;Proxy LLM judgments had low correlation with actual user evaluations, indicating proxy LLMs are poor substitutes for human perception in privacy-sensitive scenarios.&lt;/li&gt;&lt;li&gt;Authors argue for more user-centered evaluations and improved alignment of LLMs to estimate perceived privacy and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyuan Wu', 'Roshni Kaushik', 'Wenkai Li', 'Lujo Bauer', 'Koichi Onoue']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'user-study', 'evaluation/benchmarking', 'LLM alignment', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20721</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Web Fraud Attacks Against LLM-Driven Multi-Agent Systems</title><link>https://arxiv.org/abs/2509.01211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Web Fraud Attacks'—a new class of adversarial attacks that exploit web-link structures to deceive LLM-driven multi-agent systems (MAS).&lt;/li&gt;&lt;li&gt;Presents 12 representative attack variants (e.g., homoglyph deception, sub-directory nesting, parameter obfuscation) and evaluates them across diverse MAS architectures.&lt;/li&gt;&lt;li&gt;Finds these attacks are both destructive and easier to deploy (strong evasion properties) because they don't require complex input engineering; authors release code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dezhang Kong', 'Hujin Peng', 'Yilun Zhang', 'Lele Zhao', 'Zhenhua Xu', 'Shi Lin', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'adversarial web links', 'multi-agent systems', 'evasion', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01211</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LAG: Logic-Augmented Generation from a Cartesian Perspective</title><link>https://arxiv.org/abs/2508.05509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Logic-Augmented Generation (LAG): decomposes complex questions into logically ordered atomic sub-questions and resolves them sequentially.&lt;/li&gt;&lt;li&gt;Uses prior answers to guide context retrieval for subsequent sub-questions, forming a logic-aware retrieval and reasoning pipeline to improve grounding.&lt;/li&gt;&lt;li&gt;Reports improvements in accuracy and reduced hallucination on four benchmarks compared to standard retrieval-augmented methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Xiao', 'Chuang Zhou', 'Yujing Zhang', 'Qinggang Zhang', 'Su Dong', 'Shengyuan Chen', 'Chang Yang', 'Xiao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'retrieval-augmented generation', 'logical decomposition', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05509</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Low Resource Reconstruction Attacks Through Benign Prompts</title><link>https://arxiv.org/abs/2507.07947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a low-resource attack that reconstructs training images (or parts) from diffusion models using seemingly benign prompts and little-to-no access to training data.&lt;/li&gt;&lt;li&gt;Shows that ordinary prompts (e.g., “blue Unisex T-Shirt”) can unintentionally reproduce real individuals' faces or memorized visual elements, highlighting user-facing privacy risks.&lt;/li&gt;&lt;li&gt;Attributes vulnerability to scraped e-commerce datasets with templated layouts that tightly couple pattern-like textual prompts to specific images; provides code for reproducing the attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sol Yarkoni', 'Mahmood Sharif', 'Roi Livni']&lt;/li&gt;&lt;li&gt;Tags: ['training-data reconstruction', 'diffusion models', 'privacy', 'prompt-based attacks', 'data memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07947</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict</title><link>https://arxiv.org/abs/2506.06485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a diagnostic framework that holds underlying knowledge constant while creating controlled conflicts between context and parametric memory across tasks with different knowledge demands.&lt;/li&gt;&lt;li&gt;Finds that degradation under context-memory conflict depends on task-specific knowledge reliance and conflict plausibility; interventions (rationales, context reiteration) shift models toward context reliance, helping context-only tasks but harming tasks needing parametric knowledge.&lt;/li&gt;&lt;li&gt;Shows these dynamics bias model-based evaluation and calls for task-aware strategies to balance context and memory in LLM deployment and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiser Sun', 'Fan Bai', 'Mark Dredze']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Evaluation bias', 'Context vs parametric memory', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06485</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective</title><link>https://arxiv.org/abs/2505.20707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Physbench: 3,162 high-school/AP physics questions with structured reference solutions and 2,700 culturally contextualized paired variants, plus a stage-wise P-REFS evaluation rubric.&lt;/li&gt;&lt;li&gt;Evaluates 10 small language models across ~58,000 responses, finding that 75–98% of final-answer-correct solutions contain at least one reasoning error and that failure modes shift with model capability (interpretation/modeling errors in weaker models; execution errors in stronger models).&lt;/li&gt;&lt;li&gt;Shows paired contextual variants have little effect on top models but degrade mid-tier models, and argues for evaluation paradigms that prioritize reasoning fidelity over final-answer accuracy for safe educational AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicy Scaria', 'Silvester John Joseph Kennedy', 'Krishna Agarwal', 'Diksha Seth', 'Deepak Subramani']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmark', 'LLM reasoning', 'educational-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20707</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities</title><link>https://arxiv.org/abs/2505.15722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive empirical study of memorization in multilingual LLMs across 95 languages, multiple model scales and architectures.&lt;/li&gt;&lt;li&gt;Shows that memorization is not fully explained by per-language training data volume and that treating languages in isolation misses key patterns.&lt;/li&gt;&lt;li&gt;Proposes a graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization.&lt;/li&gt;&lt;li&gt;Finds that within clusters of similar languages, lower-resource languages tend to exhibit higher memorization, with implications for privacy leakage and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Luo', 'Yiyi Chen', 'Johannes Bjerva', 'Qiongxiu Li']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'multilingual-llms', 'model-evaluation', 'cross-lingual-transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15722</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Social Bias in Popular Question-Answering Benchmarks</title><link>https://arxiv.org/abs/2505.15553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Content analysis of 30 QA/RC benchmark papers and quantitative analysis of 20 benchmark datasets to assess representation of demographics and regions.&lt;/li&gt;&lt;li&gt;Finds pervasive gender, religion, and geographic biases across encyclopedic, commonsense, and scholarly benchmarks and notes poor reporting about dataset creators/annotators.&lt;/li&gt;&lt;li&gt;Highlights that biased benchmarks can incentivize biased inference heuristics in LLMs and that almost no benchmarks report measures to mitigate social representation issues (except WinoGrande).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Angelie Kraft', 'Judith Simon', 'Sonja Schimmler']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'benchmarks', 'safety-evaluation', 'fairness', 'dataset-audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15553</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</title><link>https://arxiv.org/abs/2502.18851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a critical flaw in existing LLM code-watermarking approaches: high-entropy tokens can be syntax-critical (e.g., keywords), so watermarking them breaks program correctness.&lt;/li&gt;&lt;li&gt;Proposes STONE, a syntax-aware watermarking technique that restricts watermark embedding to non-syntactic tokens to preserve code functionality.&lt;/li&gt;&lt;li&gt;Introduces STEM, an evaluation framework that measures correctness, detectability, and imperceptibility to rigorously assess watermarking methods.&lt;/li&gt;&lt;li&gt;Empirical results on Python, C++, and Java show STONE maintains correctness while achieving strong detectability with minimal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungin Kim', 'Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM-generated-code-detection', 'model-attribution', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18851</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL: a framework that derives explicit conversational traits from contrastive dialogue data and aggregates them into a compact, interpretable scalar reward representing human-likeness.&lt;/li&gt;&lt;li&gt;Uses that reward with standard preference-optimization methods to align LLMs of varying sizes, reporting no degradation in overall performance and improved human-evaluated perceived human-likeness.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability—traits are explicit so alignment behavior can be inspected and unintended effects diagnosed—demonstrating that qualitative behavioral properties can be made measurable and optimized.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'human-likeness', 'interpretability', 'preference optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Faithful-First Reasoning, Planning, and Acting for Multimodal LLMs</title><link>https://arxiv.org/abs/2511.08409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Faithful-First Reasoning, Planning, and Acting (RPA) framework to improve multimodal LLM faithfulness by supervising and guiding intermediate reasoning.&lt;/li&gt;&lt;li&gt;Introduces FaithEvi to evaluate stepwise and chain-level faithfulness to visual evidence, and FaithAct to plan and execute faithfulness-aware inference actions.&lt;/li&gt;&lt;li&gt;Reports up to 24% improvement in perceptual faithfulness across multimodal reasoning benchmarks without degrading task accuracy.&lt;/li&gt;&lt;li&gt;Positions a unified approach for both evaluating and enforcing faithfulness to mitigate hallucination in multimodal reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Xinyue Xu', 'Sai Ma', 'Di Zhang', 'Sichao Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'hallucination-mitigation', 'faithfulness-evaluation', 'alignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08409</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities</title><link>https://arxiv.org/abs/2511.00340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CLAUSE, a benchmark of &gt;7,500 real-world perturbed contracts to stress-test LLM legal reasoning and discrepancy detection.&lt;/li&gt;&lt;li&gt;Uses a persona-driven pipeline to generate 10 anomaly categories and validates perturbations against statutes via a Retrieval-Augmented Generation (RAG) system for legal fidelity.&lt;/li&gt;&lt;li&gt;Evaluates leading LLMs on detection and legal justification of embedded flaws, finding models often miss subtle errors and struggle to legally justify them.&lt;/li&gt;&lt;li&gt;Frames work as a path to identify and correct legal reasoning failures in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manan Roy Choudhury', 'Adithya Chandramouli', 'Mannan Anand', 'Vivek Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['robustness-evaluation', 'adversarial-benchmarking', 'LLM-evaluation', 'RAG', 'legal-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00340</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Identity Skews Debate: Anonymization for Bias-Reduced Multi-Agent Reasoning</title><link>https://arxiv.org/abs/2510.07517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes multi-agent debate dynamics as an identity-weighted Bayesian update process capturing sycophancy and self-bias.&lt;/li&gt;&lt;li&gt;Proposes response anonymization (removing identity markers from prompts) to reduce identity-driven bias in agent interactions.&lt;/li&gt;&lt;li&gt;Introduces Identity Bias Coefficient (IBC) to quantify an agent’s tendency to follow peers versus itself.&lt;/li&gt;&lt;li&gt;Empirical evaluation across models/benchmarks shows identity bias is widespread, with sycophancy more common than self-bias, and anonymization improves trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeong Kyu Choi', 'Xiaojin Zhu', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent debate', 'identity bias / sycophancy', 'anonymization', 'alignment / safety', 'evaluation metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07517</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multiplayer Nash Preference Optimization</title><link>https://arxiv.org/abs/2509.23102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multiplayer Nash Preference Optimization (MNPO), extending two‑player Nash learning from human feedback (NLHF) to n‑player games where policies compete against a population and are regularized toward a reference model.&lt;/li&gt;&lt;li&gt;Claims MNPO preserves equilibrium guarantees of two‑player methods while enabling richer dynamics and better coverage of non‑transitive, heterogeneous human preferences.&lt;/li&gt;&lt;li&gt;Empirical results show MNPO outperforms existing NLHF baselines on instruction‑following benchmarks, especially under heterogeneous annotator conditions and mixed‑policy evaluations.&lt;/li&gt;&lt;li&gt;Provides code and positions MNPO as a scalable framework for aligning LLMs with complex preference structures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fang Wu', 'Xu Huang', 'Weihao Xuan', 'Zhiwei Zhang', 'Yijia Xiao', 'Guancheng Wan', 'Xiaomin Li', 'Bing Hu', 'Peng Xia', 'Jure Leskovec', 'Yejin Choi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'preference learning', 'multi-agent Nash learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23102</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models</title><link>https://arxiv.org/abs/2508.18760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes failure modes where large reasoning models (LRMs) do not appropriately abstain on inherently unanswerable questions.&lt;/li&gt;&lt;li&gt;Finds LRMs internally recognize flaws in questions but fail to abstain, indicating misalignment between internal cognition and external responses.&lt;/li&gt;&lt;li&gt;Proposes a lightweight two-stage approach combining cognitive monitoring with inference-time intervention to improve abstention.&lt;/li&gt;&lt;li&gt;Experiments show substantially improved abstention rates while maintaining overall reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Xiangyu Liu', 'Zequn Sun', 'Wei Hu']&lt;/li&gt;&lt;li&gt;Tags: ['abstention', 'alignment', 'model robustness', 'inference-time intervention', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18760</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools</title><link>https://arxiv.org/abs/2508.02110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new attack surface: manipulating tool metadata (names, descriptions, parameter schemas) to bias LLM agent tool selection without prompt injection or access to model internals.&lt;/li&gt;&lt;li&gt;Proposes Attractive Metadata Attack (AMA), a black-box in-context iterative optimization method that crafts syntactically and semantically valid but highly attractive metadata.&lt;/li&gt;&lt;li&gt;Evaluates AMA across 10 realistic simulated tool-use scenarios and multiple popular LLM agents, reporting high success rates (81%–95%), measurable privacy leakage, and minimal impact on primary task performance.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against prompt-level defenses, auditor-based detection, and structured tool-selection protocols, and argues for execution-level defenses; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kanghua Mo', 'Li Hu', 'Yucheng Long', 'Zhihao Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02110</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Imagining and building wise machines: The centrality of AI metacognition</title><link>https://arxiv.org/abs/2411.02478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI lacks 'wisdom' and focuses on metacognitive strategies (e.g., intellectual humility, perspective-taking, context-adaptability) as central to improving AI robustness, explainability, cooperation, and safety.&lt;/li&gt;&lt;li&gt;Analyzes human wisdom strategies and maps them to AI counterparts, emphasizing metacognition for managing object-level heuristics and decision strategies.&lt;/li&gt;&lt;li&gt;Discusses how wise AI could be benchmarked, trained, and implemented, offering a vision for evaluation and development rather than detailed technical attacks or defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel G. B. Johnson', 'Amir-Hossein Karimi', 'Yoshua Bengio', 'Nick Chater', 'Tobias Gerstenberg', 'Kate Larson', 'Sydney Levine', 'Melanie Mitchell', 'Iyad Rahwan', 'Bernhard Sch\\"olkopf', 'Igor Grossmann']&lt;/li&gt;&lt;li&gt;Tags: ['metacognition', 'AI alignment', 'AI safety', 'explainability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.02478</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test</title><link>https://arxiv.org/abs/2601.04137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Wow-wo-val, an Embodied Turing Test benchmark built on 609 robot manipulation sequences to evaluate video foundation models as embodied world models.&lt;/li&gt;&lt;li&gt;Defines five core abilities (perception, planning, prediction, generalization, execution) and a 22-metric protocol with high correlation to human preference (&gt;0.93).&lt;/li&gt;&lt;li&gt;Finds current video foundation models have poor long-horizon planning (17.27) and limited physical consistency (best 68.02); most models fail the Inverse Dynamic Model execution test (~0% success) while the WoW model achieves 40.74%.&lt;/li&gt;&lt;li&gt;Highlights a substantial gap between generated videos and real-world execution, underscoring the need for robust, physically consistent world models for embodied agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chun-Kai Fan', 'Xiaowei Chi', 'Xiaozhu Ju', 'Hao Li', 'Yong Bao', 'Yu-Kai Wang', 'Lizhang Chen', 'Zhiyuan Jiang', 'Kuangzhi Ge', 'Ying Li', 'Weishi Mi', 'Qingpo Wuwu', 'Peidong Jia', 'Yulin Luo', 'Kevin Zhang', 'Zhiyuan Qin', 'Yong Dai', 'Sirui Han', 'Yike Guo', 'Shanghang Zhang', 'Jian Tang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'embodied-AI', 'world-models', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04137</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models</title><link>https://arxiv.org/abs/2601.04131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ContextFocus, an activation-steering method that biases LLM activations at inference to improve fidelity to externally retrieved context when it conflicts with model-internal knowledge.&lt;/li&gt;&lt;li&gt;Requires no finetuning and adds minimal inference overhead; claimed to be complementary to prompting strategies and effective on larger models.&lt;/li&gt;&lt;li&gt;Evaluated on the ConFiQA benchmark versus baselines (ContextDPO, COIECD, prompting), showing significant gains in contextual faithfulness while preserving fluency and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikhil Anand', 'Shwetha Somasundaram', 'Anirudh Phukan', 'Apoorv Saxena', 'Koyel Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'contextual faithfulness', 'activation steering', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04131</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</title><link>https://arxiv.org/abs/2601.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode called 'textual inertia' where LMMs cling to erroneous textual hallucinations and ignore conflicting visual evidence during Chain-of-Thought reasoning.&lt;/li&gt;&lt;li&gt;Introduces the LogicGraph Perturbation Protocol to structurally inject perturbations into reasoning chains across diverse LMMs to evaluate self-reflection and robustness.&lt;/li&gt;&lt;li&gt;Finds models self-correct in &lt;10% of cases and largely propagate hallucinated text; proposes Active Visual-Context Refinement (training-free) to actively re-ground visual context and denoise reasoning history.&lt;/li&gt;&lt;li&gt;Demonstrates that the proposed inference-time mitigation significantly reduces hallucination propagation and improves reasoning robustness in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhu', 'Jiafeng Liang', 'Shixin Jiang', 'Jinlan Fu', 'Ming Liu', 'Guanglu Sun', 'See-Kiong Ng', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'hallucination mitigation', 'adversarial evaluation', 'chain-of-thought', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04073</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense</title><link>https://arxiv.org/abs/2601.04034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HoneyTrap, a multi-agent deceptive defense framework (Threat Interceptor, Misdirection Controller, Forensic Tracker, System Harmonizer) to counter multi-turn jailbreak attacks on LLMs by engaging and deceiving attackers rather than outright rejection.&lt;/li&gt;&lt;li&gt;Introduces MTJ-Pro, a multi-turn progressive jailbreak dataset combining seven advanced jailbreak strategies to evaluate defenses against progressively deepening multi-turn attacks.&lt;/li&gt;&lt;li&gt;Defines two novel evaluation metrics—Mislead Success Rate (MSR) and Attack Resource Consumption (ARC)—to measure how effectively deceptive defenses mislead attackers and increase attacker cost.&lt;/li&gt;&lt;li&gt;Evaluates HoneyTrap on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1, reporting substantial reductions in attack success rates (~68.77%) and improvements in MSR and ARC even under adaptive attacker settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Li', 'Xi Lin', 'Jun Wu', 'Zehao Liu', 'Haoyu Li', 'Tianjie Ju', 'Xiang Chen', 'Jianhua Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking defense', 'adversarial prompting', 'deceptive/honeypot defenses', 'security evaluation/metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04034</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>What Matters For Safety Alignment?</title><link>https://arxiv.org/abs/2601.03868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical evaluation of safety alignment across 32 LLMs/LRMs (3B–235B parameters) using 5 safety datasets, 56 jailbreak techniques, and 4 CoT attack strategies (4.6M API calls).&lt;/li&gt;&lt;li&gt;Finds models with integrated reasoning/self-reflection (LRMs like GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, GPT-OSS-120B) exhibit stronger safety alignment.&lt;/li&gt;&lt;li&gt;Shows post-training/knowledge distillation can degrade safety alignment, suggesting safety must be an explicit objective during these stages.&lt;/li&gt;&lt;li&gt;Demonstrates a major vulnerability: CoT attacks via response prefixes dramatically increase jailbreak success (avg 3.34x; up to 96.3%), and identifies roleplay, prompt injection, and gradient-based adversarial prompts as predominant exploitation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xing Li', 'Hui-Ling Zhen', 'Lihao Yin', 'Xianzhi Yu', 'Zhenhua Dong', 'Mingxuan Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'jailbreaking/prompt injection', 'chain-of-thought (CoT) attacks', 'red teaming/robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03868</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Numbers Start Talking: Implicit Numerical Coordination Among LLM-Based Agents</title><link>https://arxiv.org/abs/2601.03846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Game-theoretic analysis of covert (implicit/non-linguistic) communication among LLM-based agents in multi-agent settings.&lt;/li&gt;&lt;li&gt;Evaluates agent interactions across four canonical games under explicit, restricted, and absent communication modes, considering heterogeneous personalities and one-shot vs repeated play.&lt;/li&gt;&lt;li&gt;Characterizes when covert signals emerge and how they affect coordination and strategic outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Buscemi', 'Daniele Proverbio', 'Alessandro Di Stefano', 'The Anh Han', 'German Castignani', 'Pietro Li\\`o']&lt;/li&gt;&lt;li&gt;Tags: ['covert communication', 'multi-agent systems', 'LLM agents', 'alignment', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03846</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms</title><link>https://arxiv.org/abs/2601.03752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of LLMs' personalized text generation across 10 languages, 16 models, 1080 prompt-personalization combinations, producing 17,280 texts.&lt;/li&gt;&lt;li&gt;Focuses on potential misuse (personalized disinformation) and benefits of personalization, measuring how personalization toward demographic groups and social-media platforms affects quality and detectability.&lt;/li&gt;&lt;li&gt;Finds variation across languages in personalization quality; platform-targeted personalization reduces detectability more strongly, especially in English where personalization quality is highest.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Macko']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'misinformation / personalized disinformation', 'evasion of detection', 'multilingual robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03752</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Inference Attacks Against Graph Generative Diffusion Models</title><link>https://arxiv.org/abs/2601.03701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs three types of black-box inference attacks against graph generative diffusion models: graph reconstruction, property inference (e.g., density statistics), and two membership inference variants.&lt;/li&gt;&lt;li&gt;Evaluates attacks on three classes of graph diffusion generative models across six real-world graph datasets, showing attacks significantly outperform baselines.&lt;/li&gt;&lt;li&gt;Proposes two defense mechanisms that mitigate these inference attacks and claim improved trade-offs between defense strength and model utility; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuling Wang', 'Xin Huang', 'Guibo Luo', 'Jianliang Xu']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'property-inference', 'reconstruction-attack', 'privacy', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03701</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</title><link>https://arxiv.org/abs/2601.03661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations of Group Relative Policy Optimization (GRPO) for reasoning tasks: sequence-level advantage normalization causes length bias, penalties for low-quality trajectories are diluted, and scalar objectives discard intra-group pairwise preference information.&lt;/li&gt;&lt;li&gt;Proposes AMIR-GRPO, augmenting GRPO with an implicit DPO-style contrastive regularizer built from intra-group reward rankings (no extra annotations required).&lt;/li&gt;&lt;li&gt;Claims improvements: stronger suppression of low-reward trajectories, reduced length bias, denser supervision per rollout group, clearer separation between correct/incorrect reasoning chains, and consistent gains on math reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Hossein Yari', 'Fajri Koto']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'preference learning', 'reward modeling', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03661</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification</title><link>https://arxiv.org/abs/2601.03600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALERT, a zero-shot jailbreak detector that amplifies internal representation discrepancies (layer-wise, module-wise, token-wise) between benign and jailbreak prompts.&lt;/li&gt;&lt;li&gt;Identifies safety-relevant layers, discriminative modules, and informative safety tokens to construct amplified representations and applies two complementary classifiers on them.&lt;/li&gt;&lt;li&gt;Demonstrates strong zero-shot performance across three safety benchmarks, outperforming baselines by ≥10% on average Accuracy and F1 (up to ~40% in some cases).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Lin', 'Philip Li', 'Zhichen Zeng', 'Tingwei Li', 'Tianxin Wei', 'Xuying Ning', 'Gaotang Li', 'Yuzhong Chen', 'Hanghang Tong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak detection', 'zero-shot safety evaluation', 'model-internal analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03600</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios</title><link>https://arxiv.org/abs/2601.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAS (Police Action Scenarios), a systematic evaluation framework tailored to assessing LLMs in police decision-making contexts.&lt;/li&gt;&lt;li&gt;Constructs a novel QA dataset derived from over 8,000 official police documents and defines metrics validated against police expert judgments.&lt;/li&gt;&lt;li&gt;Evaluates commercial LLMs on police-related tasks, finds notable failures—especially in producing reliable, fact-based recommendations—and releases data and prompt templates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangyub Lee', 'Heedou Kim', 'Hyeoncheol Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-evaluation', 'AI-safety', 'Benchmarking', 'High-stakes deployment', 'Law-enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03553</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict</title><link>https://arxiv.org/abs/2601.03546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a context-based protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and data-sharing acceptance within a single history-carrying session to assess value-action alignment in LLMs.&lt;/li&gt;&lt;li&gt;Applies multi-group structural equation modeling (MGSEM) to identify pathways from privacy concerns and prosocialness to data-sharing actions, enabling assessment of directional relationships.&lt;/li&gt;&lt;li&gt;Proposes Value-Action Alignment Rate (VAAR), a human-referenced metric aggregating path-level evidence for expected sign-directions between values and actions.&lt;/li&gt;&lt;li&gt;Finds stable but model-specific privacy–prosocial–action profiles across multiple LLMs and substantial heterogeneity in how well expressed values predict downstream data-sharing behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Chen', 'Chenxiao Yu', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'alignment', 'LLM evaluation', 'value-action assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03546</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation</title><link>https://arxiv.org/abs/2601.03511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntroLM, a method for causal LMs to predict their own output quality during the prefilling phase using introspective tokens so generation behavior is not altered.&lt;/li&gt;&lt;li&gt;Uses token-conditional LoRA that activates only for the introspective token to learn success prediction while preserving backbone model behavior and avoiding external evaluators.&lt;/li&gt;&lt;li&gt;Reports strong empirical results: Qwen3 8B achieves ~90% ROC AUC for success prediction (≈14% better than a DeBERTa classifier) and system-level gains in multi-model routing (reduced latency and large-model usage at matched reliability).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hossein Hosseini Kasnavieh', 'Gholamreza Haffari', 'Chris Leckie', 'Adel N. Toosi']&lt;/li&gt;&lt;li&gt;Tags: ['self-evaluation', 'reliability', 'safety-evaluation', 'model-routing', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03511</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.03500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies visual statistical bias in vision encoders (Bag-of-Patches / texture over structure) as a contributor to object hallucinations in large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Proposes Structure-Disrupted Contrastive Decoding (SDCD), a training-free decoding-time method that generates a shuffled, structure-disrupted view and penalizes tokens that remain highly confident under that view.&lt;/li&gt;&lt;li&gt;Claims SDCD suppresses texture-driven false confidences, reducing hallucinations and improving multimodal capabilities across multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Xia', 'Siheng Wang', 'Peng Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'alignment', 'robustness', 'vision-language models', 'decoding method']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03500</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cyberattack Detection in Virtualized Microgrids Using LightGBM and Knowledge-Distilled Classifiers</title><link>https://arxiv.org/abs/2601.03495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a MATLAB/Simulink virtual microgrid and injects structured cyberattacks at the secondary control layer using MGLib.&lt;/li&gt;&lt;li&gt;Generates labeled time-series datasets for multiple attack classes (ramp, sinusoidal, additive, coordinated stealth, DoS) and trains LightGBM classifiers for binary intrusion detection and multiclass attack identification.&lt;/li&gt;&lt;li&gt;Achieves high multiclass performance (99.72% accuracy, 99.62% F1) and solid binary detection (94.8% accuracy, 94.3% F1); uses knowledge distillation to produce smaller, faster models for CPU-based edge deployment (54–67 ms per 1000 samples).&lt;/li&gt;&lt;li&gt;Emphasizes practical, lightweight ML-based intrusion detection for microgrid cybersecurity rather than adversarial ML or red-teaming of models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Osasumwen Cedric Ogiesoba-Eguakun', 'Suman Rath']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'cyber-physical-systems', 'LightGBM', 'knowledge-distillation', 'dataset-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03495</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Microeconomic Foundations of Multi-Agent Learning</title><link>https://arxiv.org/abs/2601.03451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops an economic foundation for multi-agent learning via a principal-agent interaction modeled as a Markov decision process with strategic externalities.&lt;/li&gt;&lt;li&gt;Proposes a two-phase incentive mechanism: estimate implementable transfers, then use them to steer long-run dynamics under regret-based rationality and exploration assumptions.&lt;/li&gt;&lt;li&gt;Proves the mechanism achieves sublinear social-welfare regret (asymptotically optimal welfare) and uses simulations to show coarse incentives can correct inefficient learning under stateful externalities.&lt;/li&gt;&lt;li&gt;Highlights the necessity of incentive-aware mechanism design for safe, welfare-aligned AI in market and insurance settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nassim Helou']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent learning', 'mechanism design', 'AI alignment', 'incentive design', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03451</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale</title><link>https://arxiv.org/abs/2601.03444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares human and LLM ratings across three grading scales on six benchmarks (objective, subjective, mixed) to study LLM-as-a-judge behavior.&lt;/li&gt;&lt;li&gt;Uses intraclass correlation coefficients (ICC) to measure absolute agreement and finds LLM judgments vary across scales, especially on subjective tasks.&lt;/li&gt;&lt;li&gt;Aggregated results show a 0–5 grading scale yields the highest human–LLM alignment; pooled reliability can hide benchmark heterogeneity and subgroup (e.g., gender) differences.&lt;/li&gt;&lt;li&gt;Concludes that grading-scale design and sub-level diagnostics are important components of reliable LLM-as-judge protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiyue Li', 'Minda Zhao', 'Weixuan Dong', 'Jiahui Cai', 'Yuze Wei', 'Michael Pocress', 'Yi Li', 'Wanyan Yuan', 'Xiaoyue Wang', 'Ruoyu Hou', 'Kaiyuan Lou', 'Wenqi Zeng', 'Yutong Yang', 'Yilun Du', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'evaluation', 'LLM-as-judge', 'reliability', 'human-LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03444</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers</title><link>https://arxiv.org/abs/2601.03443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains linear classifiers on various audio embedding spaces to distinguish real wideband audio from super-resolved (GAN/diffusion-based) outputs.&lt;/li&gt;&lt;li&gt;Evaluates middle-band and full-band upsampling for speech and music, finding near-perfect separability across datasets and models despite high perceptual quality and metric scores.&lt;/li&gt;&lt;li&gt;Shows embedding-based detection reveals a persistent distributional gap between synthetic super-resolved audio and real audio, calling into question perceptual metrics as proxies for fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Silaev', 'Konstantinos Drossos', 'Tuomas Virtanen']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'audio forensics', 'synthetic media detection', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03443</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Archaeology: The Causal Topology of Model Evolution</title><link>https://arxiv.org/abs/2601.03424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free mechanistic probe based on attention-graph spectra (algebraic connectivity λ2, smoothness, spectral entropy) that yields stable spectral fingerprints across models and languages.&lt;/li&gt;&lt;li&gt;Identifies a failure mode—Passive-Triggered Connectivity Collapse (PTCC)—linked to curriculum transitions (e.g., code-to-chat), localized to a sparse Layer 2 patch of heads, and partially recoverable via activation steering (~38% restoration).&lt;/li&gt;&lt;li&gt;Demonstrates forensic identification of processing strategies across lineages and shows topological regimes correlate with tokenization density, positioning spectra as a tool for auditing and verifying training regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'robustness', 'model auditing', 'training-regime forensics', 'mechanistic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03424</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks</title><link>https://arxiv.org/abs/2601.03420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAILS, a token-level random iterative local search method that performs jailbreak attacks using only model logits—no gradients or handcrafted priors required.&lt;/li&gt;&lt;li&gt;Key contributions: an auto-regressive loss enforcing exact prefix matching and a history-based selection strategy to align optimization with true attack success.&lt;/li&gt;&lt;li&gt;Enables cross-tokenizer ensemble attacks that find shared adversarial patterns, improving transferability to closed-source LLMs (e.g., GPT, Gemini) and achieving near-100% success on multiple open models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhakshylyk Nurlanov', 'Frank R. Schmidt', 'Florian Bernard']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'black-box transferability', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03420</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2601.03388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds a strong causal relationship between metaphors in training data and cross-domain misalignment in large reasoning models.&lt;/li&gt;&lt;li&gt;Shows interventions at pre-training, fine-tuning, and re-alignment phases materially change models' misalignment degrees.&lt;/li&gt;&lt;li&gt;Identifies connections between metaphors and activation of global/local latent features, and uses these signals to build a high-accuracy detector for misaligned content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Hu', 'Chen Wang', 'Yanfeng Shu', 'Hye-young Paik', 'Liming Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'misalignment', 'latent-features', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03388</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2601.03331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MMErroR, a multimodal benchmark of 2,013 samples designed to embed a single coherent reasoning error across 24 subdomains and 6 top-level domains.&lt;/li&gt;&lt;li&gt;Focuses on process-level, error-centric evaluation: models must detect incorrect reasoning and classify the type of error in both visual and linguistic contexts.&lt;/li&gt;&lt;li&gt;Evaluates 20 advanced VLMs; best model (Gemini-3.0-Pro) correctly classifies the error type in only 66.47% of cases, showing the difficulty of erroneous-reasoning detection.&lt;/li&gt;&lt;li&gt;Provides a resource for analyzing multi-modal reasoning capabilities and for guiding improvements in model robustness and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Shi', 'Yifeng Xie', 'Minzhe Guo', 'Liangsi Lu', 'Mingxuan Huang', 'Jingchao Wang', 'Zhihong Zhu', 'Boyan Xu', 'Zhiqi Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmark', 'vision-language', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03331</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</title><link>https://arxiv.org/abs/2601.03321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-consistent radiology report generation framework that separates generation into a 'think' block (detailed findings) and an 'answer' block (structured disease labels).&lt;/li&gt;&lt;li&gt;Performs backbone search for optimal vision encoder and LLM combinations for medical imaging before applying the method.&lt;/li&gt;&lt;li&gt;Introduces Group Relative Policy Optimization (GRPO) and a composite reward that explicitly penalizes logical inconsistencies between narrative findings and diagnoses to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art clinical efficacy metrics and reduced hallucination rates on the MIMIC-CXR benchmark compared to supervised baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Zhao', 'Siyuan Dai', 'Pan Wang', 'Jifeng Song', 'Hui Ji', 'Chenghua Lin', 'Liang Zhan', 'Haoteng Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'hallucination mitigation', 'medical AI', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03321</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mass Concept Erasure in Diffusion Models with Concept Hierarchy</title><link>https://arxiv.org/abs/2601.03305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduce a supertype-subtype concept hierarchy to group semantically related concepts (e.g., bird → macaw, bald eagle) so multiple concepts can be erased jointly using shared parameters.&lt;/li&gt;&lt;li&gt;Propose group-wise suppression with diffusion regularization to preserve denoising behavior in unmasked regions while reducing per-concept parameter cost.&lt;/li&gt;&lt;li&gt;Develop Supertype-Preserving Low-Rank Adaptation (SuPLoRA) that freezes the down-projection matrix and updates only the up-projection to retain supertype generation quality during subtype erasure; include theoretical analysis.&lt;/li&gt;&lt;li&gt;Construct a benchmark requiring simultaneous erasure across diverse domains (celebrities, objects, pornographic content) to evaluate mass concept erasure methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahang Tu', 'Ye Li', 'Yiming Wu', 'Hanbin Zhao', 'Chao Zhang', 'Hui Qian']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'safety/mitigation', 'low-rank adaptation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03305</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-Driven Cybersecurity Threats: A Survey of Emerging Risks and Defensive Strategies</title><link>https://arxiv.org/abs/2601.03304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of AI-enabled cybersecurity threats across four categories: deepfakes/synthetic media, adversarial AI attacks, automated malware, and AI-driven social engineering.&lt;/li&gt;&lt;li&gt;Introduces a comparative taxonomy linking AI capabilities to threat modalities and corresponding defenses, and reviews 70+ academic and industry sources.&lt;/li&gt;&lt;li&gt;Identifies defense gaps and research opportunities such as hybrid detection pipelines, benchmarking frameworks, explainability, interdisciplinary approaches, and regulatory compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Erukude', 'Viswa Chaitanya Marella', 'Suhasnadh Reddy Veluru']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'deepfakes', 'automated-malware', 'social-engineering', 'defensive-strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03304</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AgentMark: Utility-Preserving Behavioral Watermarking for Agents</title><link>https://arxiv.org/abs/2601.03294</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentMark, a behavioral watermarking framework that embeds multi-bit identifiers into agents' high-level planning decisions while preserving utility.&lt;/li&gt;&lt;li&gt;Operates by eliciting an explicit behavior distribution and applying distribution-preserving conditional sampling, enabling use with black-box APIs and compatibility with action-layer content watermarking.&lt;/li&gt;&lt;li&gt;Evaluated across embodied, tool-use, and social environments, showing multi-bit capacity, robust recovery from partial logs, and minimal utility degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaibo Huang', 'Jin Tan', 'Yukun Wei', 'Wanling Li', 'Zipei Zhang', 'Hui Tian', 'Zhongliang Yang', 'Linna Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['behavioral-watermarking', 'model-provenance', 'IP-protection', 'black-box-deployment', 'utility-preservation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03294</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperCLOVA X 32B Think</title><link>https://arxiv.org/abs/2601.03286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HyperCLOVA X 32B Think, a 32B-parameter vision-language model focused on Korean linguistic/cultural context and reasoning.&lt;/li&gt;&lt;li&gt;Model is pre-trained with emphasis on reasoning and post-trained for multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences.&lt;/li&gt;&lt;li&gt;Evaluated on Korean text-to-text and vision-to-text benchmarks and agent-oriented evaluation tasks; the model and weights are open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NAVER Cloud HyperCLOVA X Team']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'agentic behavior', 'multimodal LLM', 'benchmarking', 'Korean LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03286</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions</title><link>https://arxiv.org/abs/2601.03285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a set of quantitative indices to characterize LLM behavior when presented with deliberate rebuttals to prior (fictitious) responses in multiple-choice dialogues, targeting sycophantic (over‑agreeing) and stubborn (over‑adhering) behaviors.&lt;/li&gt;&lt;li&gt;Uses a fictitious-response (FR) pairing method to systematically elicit and measure how models respond to user dissent and relates these behaviors to the model's subject-matter mastery.&lt;/li&gt;&lt;li&gt;Demonstrates the framework on two physics multiple-choice problems across OpenAI model generations, finding newer models and those with greater 'Reasoning Effort' show reduced sycophancy.&lt;/li&gt;&lt;li&gt;Frames the approach as generalizable to other multiple-choice contexts and as a practical toolkit for comparing dialogue behaviors across models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin C. Dunlap', 'Anne-Simone Parent', 'Ralf Widenhorn']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'behavioral-robustness', 'LLM-evaluation', 'red-teaming-adjacent']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03285</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>$\alpha^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks</title><link>https://arxiv.org/abs/2601.03281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces α^3-Bench, a large benchmark (113k conversational UAV episodes) evaluating LLM-based UAV agents as multi-turn language-mediated control loops under dynamic 6G network conditions.&lt;/li&gt;&lt;li&gt;Defines a composite α^3 metric unifying Task Outcome, Safety Policy compliance, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost (efficiency normalized).&lt;/li&gt;&lt;li&gt;Evaluates 17 state-of-the-art LLMs on multiple UAV scenarios, measuring mission success, safety compliance, robustness to latency/jitter/packet loss/throughput variation, and multi-agent/tool-call behavior.&lt;/li&gt;&lt;li&gt;Finds that while some models reach high mission success and safety compliance, robustness and efficiency degrade significantly under adverse network conditions, highlighting need for network-aware and resource-efficient agent design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Amine Ferrag', 'Abderrahmane Lakas', 'Merouane Debbah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Robustness', 'UAV autonomy', 'Benchmarking', 'Network-aware agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03281</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators</title><link>https://arxiv.org/abs/2601.03273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardEval, a multi-perspective benchmark with 106 fine-grained categories covering emotions, offensive/hateful language, gender/racial bias, jailbreaks, and broader safety concerns.&lt;/li&gt;&lt;li&gt;Presents GemmaGuard (GGuard), a QLoRA fine-tuned Gemma3-12B model trained on GuardEval to perform fine-grained moderation labeling.&lt;/li&gt;&lt;li&gt;Reports GGuard achieves macro F1 of 0.832, outperforming baseline moderation models (OpenAI Moderator 0.64, Llama Guard 0.61).&lt;/li&gt;&lt;li&gt;Argues that diverse, human-centered benchmarks improve moderation fairness, robustness, and handling of subtle or borderline/jailbreak cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naseem Machlovi', 'Maryam Saleki', 'Ruhul Amin', 'Mohamed Rahouti', 'Shawqi Al-Maliki', 'Junaid Qadir', 'Mohamed M. Abdallah', 'Ala Al-Fuqaha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM moderation', 'safety benchmark', 'jailbreaking', 'bias and fairness', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03273</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Instruction Gap: LLMs get lost in Following Instruction</title><link>https://arxiv.org/abs/2601.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of 13 leading LLMs on instruction compliance, response accuracy, and performance in real-world RAG (Retrieval-Augmented Generation) enterprise scenarios.&lt;/li&gt;&lt;li&gt;Finds large variability in instruction following across models and defines the 'instruction gap'—models perform well on general tasks but often fail precise instruction adherence needed for enterprise use.&lt;/li&gt;&lt;li&gt;Identifies top performers (Claude-Sonnet-4 and GPT-5) and provides benchmarks and practical insights for organizations deploying LLM-based systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishesh Tripathi', 'Uday Allu', 'Biddwan Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'safety-evaluation', 'benchmarking', 'enterprise-RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03269</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OpenAI GPT-5 System Card</title><link>https://arxiv.org/abs/2601.03267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes GPT-5 system architecture: multiple specialized models (gpt-5-main, gpt-5-thinking) and a real-time router that selects models based on conversation type, complexity, tools, and explicit intent.&lt;/li&gt;&lt;li&gt;Documents safety measures: 'safe-completions' training to block disallowed content, and a Preparedness Framework that classifies gpt-5-thinking as High capability in Biological and Chemical domains with associated safeguards.&lt;/li&gt;&lt;li&gt;Router is continuously trained on user signals (model switches, preference rates, measured correctness) and a mini fallback model serves queries after usage limits.&lt;/li&gt;&lt;li&gt;Claims improvements in hallucination reduction, instruction-following, and real-world utility; system card includes appendices with additional evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other (System Card)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaditya Singh (Tony)', 'Adam Fry (Tony)', 'Adam Perelman (Tony)', 'Adam Tart (Tony)', 'Adi Ganesh (Tony)', 'Ahmed El-Kishky (Tony)', 'Aidan McLaughlin (Tony)', 'Aiden Low (Tony)', 'AJ Ostrow (Tony)', 'Akhila Ananthram (Tony)', 'Akshay Nathan (Tony)', 'Alan Luo (Tony)', 'Alec Helyar (Tony)', 'Aleksander Madry (Tony)', 'Aleksandr Efremov (Tony)', 'Aleksandra Spyra (Tony)', 'Alex Baker-Whitcomb (Tony)', 'Alex Beutel (Tony)', 'Alex Karpenko (Tony)', 'Alex Makelov (Tony)', 'Alex Neitz (Tony)', 'Alex Wei (Tony)', 'Alexandra Barr (Tony)', 'Alexandre Kirchmeyer (Tony)', 'Alexey Ivanov (Tony)', 'Alexi Christakis (Tony)', 'Alistair Gillespie (Tony)', 'Allison Tam (Tony)', 'Ally Bennett (Tony)', 'Alvin Wan (Tony)', 'Alyssa Huang (Tony)', 'Amy McDonald Sandjideh (Tony)', 'Amy Yang (Tony)', 'Ananya Kumar (Tony)', 'Andre Saraiva (Tony)', 'Andrea Vallone (Tony)', 'Andrei Gheorghe (Tony)', 'Andres Garcia Garcia (Tony)', 'Andrew Braunstein (Tony)', 'Andrew Liu (Tony)', 'Andrew Schmidt (Tony)', 'Andrey Mereskin (Tony)', 'Andrey Mishchenko (Tony)', 'Andy Applebaum (Tony)', 'Andy Rogerson (Tony)', 'Ann Rajan (Tony)', 'Annie Wei (Tony)', 'Anoop Kotha (Tony)', 'Anubha Srivastava (Tony)', 'Anushree Agrawal (Tony)', 'Arun Vijayvergiya (Tony)', 'Ashley Tyra (Tony)', 'Ashvin Nair (Tony)', 'Avi Nayak (Tony)', 'Ben Eggers (Tony)', 'Bessie Ji (Tony)', 'Beth Hoover (Tony)', 'Bill Chen (Tony)', 'Blair Chen (Tony)', 'Boaz Barak (Tony)', 'Borys Minaiev (Tony)', 'Botao Hao (Tony)', 'Bowen Baker (Tony)', 'Brad Lightcap (Tony)', 'Brandon McKinzie (Tony)', 'Brandon Wang (Tony)', 'Brendan Quinn (Tony)', 'Brian Fioca (Tony)', 'Brian Hsu (Tony)', 'Brian Yang (Tony)', 'Brian Yu (Tony)', 'Brian Zhang (Tony)', 'Brittany Brenner (Tony)', 'Callie Riggins Zetino (Tony)', 'Cameron Raymond (Tony)', 'Camillo Lugaresi (Tony)', 'Carolina Paz (Tony)', 'Cary Hudson (Tony)', 'Cedric Whitney (Tony)', 'Chak Li (Tony)', 'Charles Chen (Tony)', 'Charlotte Cole (Tony)', 'Chelsea Voss (Tony)', 'Chen Ding (Tony)', 'Chen Shen (Tony)', 'Chengdu Huang (Tony)', 'Chris Colby (Tony)', 'Chris Hallacy (Tony)', 'Chris Koch (Tony)', 'Chris Lu (Tony)', 'Christina Kaplan (Tony)', 'Christina Kim (Tony)', 'CJ Minott-Henriques (Tony)', 'Cliff Frey (Tony)', 'Cody Yu (Tony)', 'Coley Czarnecki (Tony)', 'Colin Reid (Tony)', 'Colin Wei (Tony)', 'Cory Decareaux (Tony)', 'Cristina Scheau (Tony)', 'Cyril Zhang (Tony)', 'Cyrus Forbes (Tony)', 'Da Tang (Tony)', 'Dakota Goldberg (Tony)', 'Dan Roberts (Tony)', 'Dana Palmie (Tony)', 'Daniel Kappler (Tony)', 'Daniel Levine (Tony)', 'Daniel Wright (Tony)', 'Dave Leo (Tony)', 'David Lin (Tony)', 'David Robinson (Tony)', 'Declan Grabb (Tony)', 'Derek Chen (Tony)', 'Derek Lim (Tony)', 'Derek Salama (Tony)', 'Dibya Bhattacharjee (Tony)', 'Dimitris Tsipras (Tony)', 'Dinghua Li (Tony)', 'Dingli Yu (Tony)', 'DJ Strouse (Tony)', 'Drew Williams (Tony)', 'Dylan Hunn (Tony)', 'Ed Bayes (Tony)', 'Edwin Arbus (Tony)', 'Ekin Akyurek (Tony)', 'Elaine Ya Le (Tony)', 'Elana Widmann (Tony)', 'Eli Yani (Tony)', 'Elizabeth Proehl (Tony)', 'Enis Sert (Tony)', 'Enoch Cheung (Tony)', 'Eri Schwartz (Tony)', 'Eric Han (Tony)', 'Eric Jiang (Tony)', 'Eric Mitchell (Tony)', 'Eric Sigler (Tony)', 'Eric Wallace (Tony)', 'Erik Ritter (Tony)', 'Erin Kavanaugh (Tony)', 'Evan Mays (Tony)', 'Evgenii Nikishin (Tony)', 'Fangyuan Li (Tony)', 'Felipe Petroski Such (Tony)', 'Filipe de Avila Belbute Peres (Tony)', 'Filippo Raso (Tony)', 'Florent Bekerman (Tony)', 'Foivos Tsimpourlas (Tony)', 'Fotis Chantzis (Tony)', 'Francis Song (Tony)', 'Francis Zhang (Tony)', 'Gaby Raila (Tony)', 'Garrett McGrath (Tony)', 'Gary Briggs (Tony)', 'Gary Yang (Tony)', 'Giambattista Parascandolo (Tony)', 'Gildas Chabot (Tony)', 'Grace Kim (Tony)', 'Grace Zhao (Tony)', 'Gregory Valiant (Tony)', 'Guillaume Leclerc (Tony)', 'Hadi Salman (Tony)', 'Hanson Wang (Tony)', 'Hao Sheng (Tony)', 'Haoming Jiang (Tony)', 'Haoyu Wang (Tony)', 'Haozhun Jin (Tony)', 'Harshit Sikchi (Tony)', 'Heather Schmidt (Tony)', 'Henry Aspegren (Tony)', 'Honglin Chen (Tony)', 'Huida Qiu (Tony)', 'Hunter Lightman (Tony)', 'Ian Covert (Tony)', 'Ian Kivlichan (Tony)', 'Ian Silber (Tony)', 'Ian Sohl (Tony)', 'Ibrahim Hammoud (Tony)', 'Ignasi Clavera (Tony)', 'Ikai Lan (Tony)', 'Ilge Akkaya (Tony)', 'Ilya Kostrikov (Tony)', 'Irina Kofman (Tony)', 'Isak Etinger (Tony)', 'Ishaan Singal (Tony)', 'Jackie Hehir (Tony)', 'Jacob Huh (Tony)', 'Jacqueline Pan (Tony)', 'Jake Wilczynski (Tony)', 'Jakub Pachocki (Tony)', 'James Lee (Tony)', 'James Quinn (Tony)', 'Jamie Kiros (Tony)', 'Janvi Kalra (Tony)', 'Jasmyn Samaroo (Tony)', 'Jason Wang (Tony)', 'Jason Wolfe (Tony)', 'Jay Chen (Tony)', 'Jay Wang (Tony)', 'Jean Harb (Tony)', 'Jeffrey Han (Tony)', 'Jeffrey Wang (Tony)', 'Jennifer Zhao (Tony)', 'Jeremy Chen (Tony)', 'Jerene Yang (Tony)', 'Jerry Tworek (Tony)', 'Jesse Chand (Tony)', 'Jessica Landon (Tony)', 'Jessica Liang (Tony)', 'Ji Lin (Tony)', 'Jiancheng Liu (Tony)', 'Jianfeng Wang (Tony)', 'Jie Tang (Tony)', 'Jihan Yin (Tony)', 'Joanne Jang (Tony)', 'Joel Morris (Tony)', 'Joey Flynn (Tony)', 'Johannes Ferstad (Tony)', 'Johannes Heidecke (Tony)', 'John Fishbein (Tony)', 'John Hallman (Tony)', 'Jonah Grant (Tony)', 'Jonathan Chien (Tony)', 'Jonathan Gordon (Tony)', 'Jongsoo Park (Tony)', 'Jordan Liss (Tony)', 'Jos Kraaijeveld (Tony)', 'Joseph Guay (Tony)', 'Joseph Mo (Tony)', 'Josh Lawson (Tony)', 'Josh McGrath (Tony)', 'Joshua Vendrow (Tony)', 'Joy Jiao (Tony)', 'Julian Lee (Tony)', 'Julie Steele (Tony)', 'Julie Wang (Tony)', 'Junhua Mao (Tony)', 'Kai Chen (Tony)', 'Kai Hayashi (Tony)', 'Kai Xiao (Tony)', 'Kamyar Salahi (Tony)', 'Kan Wu (Tony)', 'Karan Sekhri (Tony)', 'Karan Sharma (Tony)', 'Karan Singhal (Tony)', 'Karen Li (Tony)', 'Kenny Nguyen (Tony)', 'Keren Gu-Lemberg (Tony)', 'Kevin King (Tony)', 'Kevin Liu (Tony)', 'Kevin Stone (Tony)', 'Kevin Yu (Tony)', 'Kristen Ying (Tony)', 'Kristian Georgiev (Tony)', 'Kristie Lim (Tony)', 'Kushal Tirumala (Tony)', 'Kyle Miller (Tony)', 'Lama Ahmad (Tony)', 'Larry Lv (Tony)', 'Laura Clare (Tony)', 'Laurance Fauconnet (Tony)', 'Lauren Itow (Tony)', 'Lauren Yang (Tony)', 'Laurentia Romaniuk (Tony)', 'Leah Anise (Tony)', 'Lee Byron (Tony)', 'Leher Pathak (Tony)', 'Leon Maksin (Tony)', 'Leyan Lo (Tony)', 'Leyton Ho (Tony)', 'Li Jing (Tony)', 'Liang Wu (Tony)', 'Liang Xiong (Tony)', 'Lien Mamitsuka (Tony)', 'Lin Yang (Tony)', 'Lindsay McCallum (Tony)', 'Lindsey Held (Tony)', 'Liz Bourgeois (Tony)', 'Logan Engstrom (Tony)', 'Lorenz Kuhn (Tony)', 'Louis Feuvrier (Tony)', 'Lu Zhang (Tony)', 'Lucas Switzer (Tony)', 'Lukas Kondraciuk (Tony)', 'Lukasz Kaiser (Tony)', 'Manas Joglekar (Tony)', 'Mandeep Singh (Tony)', 'Mandip Shah (Tony)', 'Manuka Stratta (Tony)', 'Marcus Williams (Tony)', 'Mark Chen (Tony)', 'Mark Sun (Tony)', 'Marselus Cayton (Tony)', 'Martin Li (Tony)', 'Marvin Zhang (Tony)', 'Marwan Aljubeh (Tony)', 'Matt Nichols (Tony)', 'Matthew Haines (Tony)', 'Max Schwarzer (Tony)', 'Mayank Gupta (Tony)', 'Meghan Shah (Tony)', 'Melody Huang (Tony)', 'Meng Dong (Tony)', 'Mengqing Wang (Tony)', 'Mia Glaese (Tony)', 'Micah Carroll (Tony)', 'Michael Lampe (Tony)', 'Michael Malek (Tony)', 'Michael Sharman (Tony)', 'Michael Zhang (Tony)', 'Michele Wang (Tony)', 'Michelle Pokrass (Tony)', 'Mihai Florian (Tony)', 'Mikhail Pavlov (Tony)', 'Miles Wang (Tony)', 'Ming Chen (Tony)', 'Mingxuan Wang (Tony)', 'Minnia Feng (Tony)', 'Mo Bavarian (Tony)', 'Molly Lin (Tony)', 'Moose Abdool (Tony)', 'Mostafa Rohaninejad (Tony)', 'Nacho Soto (Tony)', 'Natalie Staudacher (Tony)', 'Natan LaFontaine (Tony)', 'Nathan Marwell (Tony)', 'Nelson Liu (Tony)', 'Nick Preston (Tony)', 'Nick Turley (Tony)', 'Nicklas Ansman (Tony)', 'Nicole Blades (Tony)', 'Nikil Pancha (Tony)', 'Nikita Mikhaylin (Tony)', 'Niko Felix (Tony)', 'Nikunj Handa (Tony)', 'Nishant Rai (Tony)', 'Nitish Keskar (Tony)', 'Noam Brown (Tony)', 'Ofir Nachum (Tony)', 'Oleg Boiko (Tony)', 'Oleg Murk (Tony)', 'Olivia Watkins (Tony)', 'Oona Gleeson (Tony)', 'Pamela Mishkin (Tony)', 'Patryk Lesiewicz (Tony)', 'Paul Baltescu (Tony)', 'Pavel Belov (Tony)', 'Peter Zhokhov (Tony)', 'Philip Pronin (Tony)', 'Phillip Guo (Tony)', 'Phoebe Thacker (Tony)', 'Qi Liu (Tony)', 'Qiming Yuan (Tony)', 'Qinghua Liu (Tony)', 'Rachel Dias (Tony)', 'Rachel Puckett (Tony)', 'Rahul Arora (Tony)', 'Ravi Teja Mullapudi (Tony)', 'Raz Gaon (Tony)', 'Reah Miyara (Tony)', 'Rennie Song (Tony)', 'Rishabh Aggarwal (Tony)', 'RJ Marsan (Tony)', 'Robel Yemiru (Tony)', 'Robert Xiong (Tony)', 'Rohan Kshirsagar (Tony)', 'Rohan Nuttall (Tony)', 'Roman Tsiupa (Tony)', 'Ronen Eldan (Tony)', 'Rose Wang (Tony)', 'Roshan James (Tony)', 'Roy Ziv (Tony)', 'Rui Shu (Tony)', 'Ruslan Nigmatullin (Tony)', 'Saachi Jain (Tony)', 'Saam Talaie (Tony)', 'Sam Altman (Tony)', 'Sam Arnesen (Tony)', 'Sam Toizer (Tony)', 'Sam Toyer (Tony)', 'Samuel Miserendino (Tony)', 'Sandhini Agarwal (Tony)', 'Sarah Yoo (Tony)', 'Savannah Heon (Tony)', 'Scott Ethersmith (Tony)', 'Sean Grove (Tony)', 'Sean Taylor (Tony)', 'Sebastien Bubeck (Tony)', 'Sever Banesiu (Tony)', 'Shaokyi Amdo (Tony)', 'Shengjia Zhao (Tony)', 'Sherwin Wu (Tony)', 'Shibani Santurkar (Tony)', 'Shiyu Zhao (Tony)', 'Shraman Ray Chaudhuri (Tony)', 'Shreyas Krishnaswamy (Tony)', 'Shuaiqi (Tony)', 'Xia', 'Shuyang Cheng', 'Shyamal Anadkat', "Sim\\'on Posada Fishman", 'Simon Tobin', 'Siyuan Fu', 'Somay Jain', 'Song Mei', 'Sonya Egoian', 'Spencer Kim', 'Spug Golden', 'SQ Mah', 'Steph Lin', 'Stephen Imm', 'Steve Sharpe', 'Steve Yadlowsky', 'Sulman Choudhry', 'Sungwon Eum', 'Suvansh Sanjeev', 'Tabarak Khan', 'Tal Stramer', 'Tao Wang', 'Tao Xin', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Degry', 'Thomas Shadwell', 'Tianfu Fu', 'Tianshi Gao', 'Timur Garipov', 'Tina Sriskandarajah', 'Toki Sherbakov', 'Tomer Kaftan', 'Tomo Hiratsuka', 'Tongzhou Wang', 'Tony Song', 'Tony Zhao', 'Troy Peterson', 'Val Kharitonov', 'Victoria Chernova', 'Vineet Kosaraju', 'Vishal Kuo', 'Vitchyr Pong', 'Vivek Verma', 'Vlad Petrov', 'Wanning Jiang', 'Weixing Zhang', 'Wenda Zhou', 'Wenlei Xie', 'Wenting Zhan', 'Wes McCabe', 'Will DePue', 'Will Ellsworth', 'Wulfie Bain', 'Wyatt Thompson', 'Xiangning Chen', 'Xiangyu Qi', 'Xin Xiang', 'Xinwei Shi', 'Yann Dubois', 'Yaodong Yu', 'Yara Khakbaz', 'Yifan Wu', 'Yilei Qian', 'Yin Tat Lee', 'Yinbo Chen', 'Yizhen Zhang', 'Yizhong Xiong', 'Yonglong Tian', 'Young Cha', 'Yu Bai', 'Yu Yang', 'Yuan Yuan', 'Yuanzhi Li', 'Yufeng Zhang', 'Yuguang Yang', 'Yujia Jin', 'Yun Jiang', 'Yunyun Wang', 'Yushi Wang', 'Yutian Liu', 'Zach Stubenvoll', 'Zehao Dou', 'Zheng Wu', 'Zhigang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Deployment safeguards', 'Safety training', 'Preparedness framework', 'Model routing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03267</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2601.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies sycophancy in LLMs as a safety/alignment failure where models output agreeable answers that contradict their internal reasoning.&lt;/li&gt;&lt;li&gt;Proposes Regulated Causal Anchoring (RCA) to verify consistency between reasoning traces and outputs at inference time without requiring ground truth, using an independent judge.&lt;/li&gt;&lt;li&gt;Shows RCA detects sycophancy (0% sycophancy reported) while retaining high acceptance of valid hints, and highlights failure modes invisible to outcome-based evaluation (Inverse Scaling and Final Output Gap).&lt;/li&gt;&lt;li&gt;Argues RCA breaks the self-reinforcing bias loop of critique-based self-correction and operates without ground truth, enabling inference-time process-level safety checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Y. Chang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'LLM-behavior', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03263</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions</title><link>https://arxiv.org/abs/2601.04170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'agent drift'—progressive degradation in multi-agent LLM systems—categorizing it as semantic drift, coordination drift, and behavioral drift.&lt;/li&gt;&lt;li&gt;Proposes the Agent Stability Index (ASI), a composite metric across 12 dimensions (e.g., response consistency, tool usage patterns, inter-agent agreement) to quantify drift.&lt;/li&gt;&lt;li&gt;Provides simulation-based analysis showing performance degradation over extended interactions and proposes mitigation strategies: episodic memory consolidation, drift-aware routing, and adaptive behavioral anchoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Rath']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'multi-agent systems', 'robustness', 'metrics/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04170</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</title><link>https://arxiv.org/abs/2601.03948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trade-R1, a framework to mitigate reward hacking in RL for stochastic financial environments by verifying process-level reasoning.&lt;/li&gt;&lt;li&gt;Transforms evaluation of reasoning over long financial documents into a Retrieval-Augmented Generation (RAG) task and introduces a triangular consistency metric between retrieved evidence, reasoning chains, and decisions.&lt;/li&gt;&lt;li&gt;Introduces two reward-integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization.&lt;/li&gt;&lt;li&gt;Empirical results on cross-country asset selection show reduced reward hacking and that DSR yields superior cross-market generalization with higher reasoning consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Sun', 'Yifan Sun', 'Sheng Xu', 'Li Zhao', 'Jing Li', 'Daxin Jiang', 'Cheng Hua', 'Zuo Bai']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment', 'reinforcement learning', 'verification', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03948</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs</title><link>https://arxiv.org/abs/2601.03662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety risk: explicit thinking steps (chain-of-thought) can amplify unsafe model behaviors, but contain emergent safe-reminding phrases that can be leveraged.&lt;/li&gt;&lt;li&gt;Proposes SafeRemind, a decoding-time defense that injects safe-reminding phrases at high-entropy (decision-locking) points to steer reasoning away from harmful trajectories without updating model parameters.&lt;/li&gt;&lt;li&gt;Evaluated across five large reasoning models and six benchmarks, reporting up to 45.5 percentage-point safety improvements while maintaining core reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Su-Hyeon Kim', 'Hyundong Jin', 'Yejin Lee', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'decoding-time defense', 'chain-of-thought', 'entropy-based intervention', 'jailbreaking mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03662</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Controllable LLM Reasoning via Sparse Autoencoder-Based Steering</title><link>https://arxiv.org/abs/2601.03595</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAE-Steering: a two-stage pipeline using Sparse Autoencoders to disentangle LRM hidden states and identify strategy-specific features for steering reasoning.&lt;/li&gt;&lt;li&gt;Stage 1 recalls features that amplify logits for strategy-specific keywords (filtering out &gt;99% of features); Stage 2 ranks the remaining features by control effectiveness to produce control vectors.&lt;/li&gt;&lt;li&gt;Reports &gt;15% improvement in control effectiveness over prior methods and a 7% absolute accuracy gain by redirecting models from erroneous reasoning paths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Fang', 'Wenjie Wang', 'Mingfeng Xue', 'Boyi Deng', 'Fengli Xu', 'Dayiheng Liu', 'Fuli Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM steering', 'Interpretability', 'Alignment / Safety', 'Internal feature manipulation', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03595</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules</title><link>https://arxiv.org/abs/2601.03537</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STAR-S, a self-taught iterative framework that elicits safety-rule-guided reasoning and reflection from an LLM, then fine-tunes the model on that reasoning to improve safety alignment.&lt;/li&gt;&lt;li&gt;The loop repeats: improved model reasoning yields better safety-guided data, which is used for further fine-tuning, creating a synergistic improvement in interpreting and applying safety rules.&lt;/li&gt;&lt;li&gt;Reports empirical results showing STAR-S more effectively defends against jailbreak attacks compared to baselines; code is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Di Wu', 'Yanyan Zhao', 'Xin Lu', 'Mingzhe Li', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Jailbreaking', 'Alignment', 'Safety evaluation', 'Fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03537</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</title><link>https://arxiv.org/abs/2601.03470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a maturity-based certification framework for embodied AI systems that emphasizes structured assessment frameworks and quantitative scoring mechanisms for trustworthiness.&lt;/li&gt;&lt;li&gt;Uses uncertainty quantification as an exemplar measurement mechanism to demonstrate how to navigate multi-objective trade-offs in evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates feasibility through a case study on Uncrewed Aircraft System (UAS) detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael C. Darling', 'Alan H. Hesu', 'Michael A. Mardikes', 'Brian C. McGuigan', 'Reed M. Milewicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'certification/standards', 'uncertainty quantification', 'trustworthiness', 'embodied AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03470</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization</title><link>https://arxiv.org/abs/2601.03359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent workflow that separates optimization of primary task descriptions from their granular constraints, using quantitative feedback to iteratively rewrite constraints.&lt;/li&gt;&lt;li&gt;Demonstrates that revised prompts produce higher compliance scores on models such as Llama 3.1 8B and Mixtral-8x 7B.&lt;/li&gt;&lt;li&gt;Focuses on improving instruction-following and adherence to formal acceptance criteria via evaluation-driven prompt refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alberto Purpura', 'Li Wang', 'Sahil Badyal', 'Eugenio Beaufrand', 'Adam Faulkner']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Prompt engineering', 'Instruction-following', 'Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03359</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://arxiv.org/abs/2601.03335</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Digital Red Queen (DRQ), a self-play algorithm using an LLM to iteratively evolve assembly-like programs ('warriors') that defeat prior generations in the game Core War.&lt;/li&gt;&lt;li&gt;Demonstrates continual/adversarial adaptation (Red Queen dynamics) yields increasingly general warriors and convergence toward similar behavioral strategies across runs.&lt;/li&gt;&lt;li&gt;Positions Core War as a sandbox to study adversarial evolution and suggests minimal self-play LLM methods could transfer to practical adversarial domains like cybersecurity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akarsh Kumar', 'Ryan Bahlous-Boldi', 'Prafull Sharma', 'Phillip Isola', 'Sebastian Risi', 'Yujin Tang', 'David Ha']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial evolution', 'LLM-based red teaming', 'self-play', 'cybersecurity sandbox']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03335</guid><pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
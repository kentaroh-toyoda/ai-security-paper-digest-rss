<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 15 Dec 2025 23:06:31 +0000</lastBuildDate><item><title>Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized</title><link>https://arxiv.org/abs/2512.09687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that specific parts of a generative image model disproportionately cause memorized (copyrighted) outputs.&lt;/li&gt;&lt;li&gt;Proposes UniForget: applying model pruning to suppress generation of copyrighted content without needing to know or target specific memorized concepts.&lt;/li&gt;&lt;li&gt;Claims pruning preserves general generative capabilities and is computationally cheaper at sampling time than existing manipulation-based methods.&lt;/li&gt;&lt;li&gt;Shows the approach is complementary to existing unlearning methods, enabling scalable de-memorization without dataset-specific retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Er Jin', 'Yang Zhang', 'Yongli Mou', 'Yanfei Dong', 'Stefan Decker', 'Kenji Kawaguchi', 'Johannes Stegmaier']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'model pruning', 'unlearning', 'copyright/privacy', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09687</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defense That Attacks: How Robust Models Become Better Attackers</title><link>https://arxiv.org/abs/2512.02830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that adversarially trained (AT) models produce adversarial perturbations that transfer more effectively to other models than perturbations from standard models.&lt;/li&gt;&lt;li&gt;Conducts comprehensive transferability experiments across a zoo of 36 models (CNNs and ViTs) to quantify this effect.&lt;/li&gt;&lt;li&gt;Highlights a new ecosystem risk where robustness training can unintentionally improve attacker capabilities and releases models/code for reproducibility.&lt;/li&gt;&lt;li&gt;Argues that robustness evaluations should measure both resistance to transferred attacks and a model's propensity to generate transferable attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Awad', 'Mahmoud Akrm', 'Walid Gomaa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'adversarial-training', 'transferability', 'robustness', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02830</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title><link>https://arxiv.org/abs/2511.20629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MapReduce LoRA (train preference-specific LoRA experts in parallel, iteratively merge to refine a shared base) and RaTE (reward-specific token embeddings that compose at inference) to improve multi-preference alignment.&lt;/li&gt;&lt;li&gt;Demonstrates substantial gains across modalities (text-to-image, text-to-video, language) on metrics including aesthetic/perceptual scores and safety-related metrics (helpful and harmless).&lt;/li&gt;&lt;li&gt;Targets the Pareto frontier of multi-reward optimization to reduce alignment trade-offs when jointly optimizing multiple preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chieh-Yun Chen', 'Zhonghao Wang', 'Qi Chen', 'Zhifan Ye', 'Min Shi', 'Yue Zhao', 'Yinan Zhao', 'Hui Qu', 'Wei-An Lin', 'Yiru Shen', 'Ajinkya Kale', 'Irfan Essa', 'Humphrey Shi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-preference optimization', 'LoRA', 'preference control', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20629</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title><link>https://arxiv.org/abs/2506.21546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Counterfactual Segmentation Reasoning (CSR): model must segment object in factual image and abstain in a controlled visual counterfactual.&lt;/li&gt;&lt;li&gt;Introduces HalluSegBench, a large-scale benchmark with controlled visual counterfactuals and new metrics that measure hallucination severity and separate vision- vs language-driven failures.&lt;/li&gt;&lt;li&gt;Proposes RobustSeg trained with counterfactual fine-tuning (CFT) to teach abstention and reduce pixel-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Reports ~30% reduction in hallucinations and improved segmentation on FP-RefCOCO(+/g).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhuo Li', 'Adheesh Juvekar', 'Jiaxun Zhang', 'Xingyou Liu', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Yifan Shen', 'Tianjiao Yu', 'Ismini Lourentzou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'safety evaluation', 'counterfactual evaluation', 'segmentation VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21546</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visual-Friendly Concept Protection via Selective Adversarial Perturbations</title><link>https://arxiv.org/abs/2408.08518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VCPro, a framework that applies selective adversarial perturbations to protect user-chosen concepts in images against malicious personalization of diffusion models.&lt;/li&gt;&lt;li&gt;Optimizes for minimal perceptibility by using a relaxed objective and solving with a Lagrangian multiplier method to find least-visible yet effective perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates qualitatively and quantitatively that VCPro achieves a better trade-off between protection effectiveness and visual fidelity compared to global perturbation baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyue Mi', 'Fan Tang', 'You Wu', 'Juan Cao', 'Peng Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbations', 'concept protection', 'model personalization defense', 'privacy-preserving image perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.08518</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models</title><link>https://arxiv.org/abs/2512.11194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies memorization of concept-level features in text-to-image diffusion models as an IP/privacy risk and argues existing dememorization (filtering/regularization) is insufficient.&lt;/li&gt;&lt;li&gt;Proposes a Gradient Projection Framework that, during backpropagation, projects gradient updates onto the orthogonal complement of prohibited feature embedding subspaces to remove influence of sensitive concepts.&lt;/li&gt;&lt;li&gt;Integrates with standard diffusion training, accompanied by theoretical analysis and experiments showing strong reduction in memorization while preserving generation quality and semantic fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Kothandaraman', 'Jaclyn Pytlarz']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'selective unlearning', 'diffusion models', 'privacy', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11194</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title><link>https://arxiv.org/abs/2512.11771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic security evaluation of image model fingerprinting methods under adversarial threat models (white-box and black-box) focusing on two attack goals: fingerprint removal and fingerprint forgery.&lt;/li&gt;&lt;li&gt;Implements five attack strategies and evaluates 14 fingerprinting techniques (RGB, frequency, learned-feature domains) across 12 state-of-the-art image generators, reporting high removal success (often &gt;80% white-box, &gt;50% constrained black-box) and variable forgery success.&lt;/li&gt;&lt;li&gt;Finds a trade-off between attribution accuracy and robustness, no method is both highly accurate and robust across all threat models, and highlights promising directions for more robust fingerprinting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yao', 'Marc Juarez']&lt;/li&gt;&lt;li&gt;Tags: ['image fingerprinting', 'adversarial attacks', 'model attribution', 'robustness evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11771</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty</title><link>https://arxiv.org/abs/2512.11373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an evidential segmentation framework for out-of-distribution (OOD) object segmentation using a Wasserstein loss that respects probability simplex geometry.&lt;/li&gt;&lt;li&gt;Combines Wasserstein loss with Kullback–Leibler regularization and Dice structural consistency to improve uncertainty-driven OOD segmentation.&lt;/li&gt;&lt;li&gt;Demonstrates improved OOD segmentation performance compared to existing uncertainty-based approaches, targeting safety-critical settings like autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnold Brosch', 'Abdelrahman Eldesokey', 'Michael Felsberg', 'Kira Maag']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'uncertainty estimation', 'robustness', 'semantic segmentation', 'safety-critical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11373</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MLLM Machine Unlearning via Visual Knowledge Distillation</title><link>https://arxiv.org/abs/2512.11325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a machine unlearning method for Multimodal LLMs (MLLMs) that disentangles visual and textual knowledge to selectively erase target visual information while preserving textual capabilities.&lt;/li&gt;&lt;li&gt;Introduces Visual Knowledge Distillation (VKD), using intermediate visual representations inside the MLLM as supervision rather than only output-level signals, improving unlearning effectiveness and utility preservation.&lt;/li&gt;&lt;li&gt;Fine-tunes only the visual components for efficiency and evaluates against state-of-the-art unlearning methods, showing superior effectiveness and efficiency, including robustness evaluation against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Zhenxing Niu', 'Haoxuan Ji', 'Guangyu He', 'Haichang Gao', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'multimodal-LLM', 'knowledge-distillation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11325</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title><link>https://arxiv.org/abs/2512.11296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a few-shot vision-language model (VLM) approach to jointly verify G-code text and corresponding HMI screenshots for a 15-slant-PRO CNC lathe.&lt;/li&gt;&lt;li&gt;Uses a structured JSON schema and few-shot examples (correct and error cases) to guide the VLM for error and safety-status detection.&lt;/li&gt;&lt;li&gt;Evaluates few-shot VLM versus zero-shot VLM across scenarios, reporting improved detection of HMI errors and G-code/HMI discrepancies by per-slot accuracy.&lt;/li&gt;&lt;li&gt;Demonstrates suitability for verifying manually generated G-code in CNC training contexts to support more comprehensive debugging and safety checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yasaman Hashem Pour', 'Nazanin Mahjourian', 'Vinh Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM', 'Few-shot prompting', 'G-code verification', 'HMI monitoring', 'CNC safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11296</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection</title><link>https://arxiv.org/abs/2512.11215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SmokeBench, a benchmark with four tasks (smoke classification, tile-based localization, grid-based localization, and detection) to evaluate MLLMs on wildfire smoke recognition and localization.&lt;/li&gt;&lt;li&gt;Evaluates several multimodal LLMs (Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, Gemini-2.5 Pro) on the benchmark.&lt;/li&gt;&lt;li&gt;Finds models can detect large-area smoke reasonably well but fail at accurate localization in early-stage, low-volume smoke scenarios.&lt;/li&gt;&lt;li&gt;Shows smoke volume correlates strongly with performance while image contrast is less important; highlights limitations for safety-critical wildfire monitoring and need for improved early-stage localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianye Qi', 'Weihao Li', 'Nick Barnes']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'multimodal-llm', 'wildfire-detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11215</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled evaluation framework with four stress tests probing multi-step logical reasoning: rule deletion (redundant vs essential), contradictory evidence injection, logic-preserving rewrites (single-law), and multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates three model families (BERT, Qwen2, LLaMA-like) showing perfect base performance but large drops when essential rules are removed or contradictions are injected; single-law rewrites mostly preserve accuracy while multi-law stacking reveals model-dependent brittleness.&lt;/li&gt;&lt;li&gt;Finds contemporary LLMs are stable under many semantic-preserving reformulations but remain brittle to missing/inconsistent evidence and composed logical transformations; provides a concise diagnostic for isolating these failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'model evaluation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Readiness in Health AI</title><link>https://arxiv.org/abs/2509.18234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial stress tests to evaluate robustness of flagship medical LLMs and benchmarks, including multimodal reasoning scenarios.&lt;/li&gt;&lt;li&gt;Finds prevalent brittleness: models can still guess answers with key inputs removed, are sensitive to slight prompt alterations, and often produce convincing but flawed reasoning (hallucinations).&lt;/li&gt;&lt;li&gt;Uses clinician-guided rubrics to show medical benchmarks vary in what they measure and that frontier AI exhibits significant competency gaps for real-world clinical readiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Gu', 'Jingjing Fu', 'Xiaodong Liu', 'Jeya Maria Jose Valanarasu', 'Noel CF Codella', 'Reuben Tan', 'Qianchu Liu', 'Ying Jin', 'Sheng Zhang', 'Jinyu Wang', 'Rui Wang', 'Lei Song', 'Guanghui Qin', 'Naoto Usuyama', 'Cliff Wong', 'Hao Cheng', 'HoHin Lee', 'Praneeth Sanapathi', 'Sarah Hilado', 'Tristan Naumann', 'Javier Alvarez-Valle', 'Jiang Bian', 'Mu Wei', 'Khalil Malik', 'Lidong Zhou', 'Jianfeng Gao', 'Eric Horvitz', 'Matthew P. Lungren', 'Doug Burger', 'Eric Topol', 'Hoifung Poon', 'Paul Vozila']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-testing', 'robustness', 'medical-benchmarks', 'hallucination', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18234</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</title><link>https://arxiv.org/abs/2507.05578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys memorization in LLMs: drivers (data duplication, training dynamics, fine-tuning) and its privacy/security implications.&lt;/li&gt;&lt;li&gt;Reviews measurement and attack methods (prefix-based extraction, membership inference, adversarial prompting) for detecting memorized content.&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies (data cleaning, differential privacy, post-training unlearning) and trade-offs with utility.&lt;/li&gt;&lt;li&gt;Highlights legal/ethical consequences and open challenges for balancing harmful memorization reduction with model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Xiong', 'Xuandong Zhao', 'Aneesh Pappu', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'membership-inference', 'differential-privacy', 'adversarial-prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05578</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title><link>https://arxiv.org/abs/2506.21546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Counterfactual Segmentation Reasoning (CSR): model must segment object in factual image and abstain in a controlled visual counterfactual.&lt;/li&gt;&lt;li&gt;Introduces HalluSegBench, a large-scale benchmark with controlled visual counterfactuals and new metrics that measure hallucination severity and separate vision- vs language-driven failures.&lt;/li&gt;&lt;li&gt;Proposes RobustSeg trained with counterfactual fine-tuning (CFT) to teach abstention and reduce pixel-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Reports ~30% reduction in hallucinations and improved segmentation on FP-RefCOCO(+/g).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhuo Li', 'Adheesh Juvekar', 'Jiaxun Zhang', 'Xingyou Liu', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Yifan Shen', 'Tianjiao Yu', 'Ismini Lourentzou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'safety evaluation', 'counterfactual evaluation', 'segmentation VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21546</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting</title><link>https://arxiv.org/abs/2512.09772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates cultural alignment of several LLMs (DeepSeek-V3/V3.1, GPT-4/4.1/4o/5) using Hofstede's VSM13 national-culture survey.&lt;/li&gt;&lt;li&gt;Tests effects of prompt language (English vs Simplified Chinese) and 'cultural prompting' system prompts to shift model alignment toward the United States or China.&lt;/li&gt;&lt;li&gt;Finds model-specific differences: some models (DeepSeek-V3/V3.1, GPT-5) align closely with U.S. survey responses and resist shifting toward China, while others (GPT-4, GPT-4.1, GPT-4o) are more responsive to language and cultural prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Luther', 'Donald Brown']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'cultural alignment', 'prompting', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09772</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5</title><link>https://arxiv.org/abs/2512.03803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts DoLa analysis to encoder-decoder (FLAN-)T5 to study decoder representation dynamics and how cross-attention shifts mid-decoder layers.&lt;/li&gt;&lt;li&gt;Finds highly volatile token preferences across decoder depths that make contrastive decoding unreliable for enforcing instructions.&lt;/li&gt;&lt;li&gt;Proposes a gradient-based activation-steering intervention that injects an "instruction-compliance" direction into mid-decoder layers.&lt;/li&gt;&lt;li&gt;Demonstrates large gains on a memorization/jailbreak-style benchmark (MemoTrap) from 52% to 99.7%, showing mechanistic steering can improve instruction adherence in Seq2Seq models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huey Sun', 'Anabel Yong', 'Lorenzo Gilly', 'Felipe Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'instruction-following', 'mechanistic-interpretability', 'decoder-steering', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03803</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title><link>https://arxiv.org/abs/2511.06682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Textual Self-Attention Network (TSAN), a test-time, zero-parameter-update method that emulates self-attention in natural language to analyze and weigh multiple candidate responses.&lt;/li&gt;&lt;li&gt;Formats candidates as textual keys/values and uses an LLM-based attention module operating in a textual gradient space to iteratively synthesize a preference-aligned response.&lt;/li&gt;&lt;li&gt;Operates via iterative, interpretable optimization at test time and claims to outperform supervised fine-tuned models and prior test-time alignment methods with only a few iterations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shibing Mo', 'Haoyang Ruan', 'Kai Wu', 'Jing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'preference optimization', 'LLM safety', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06682</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title><link>https://arxiv.org/abs/2509.25531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixtureVitae, an open-access pretraining corpus built from permissive-first sources (public domain, CC-BY/Apache, government works, EU TDM-eligible) plus targeted instruction/reasoning and synthetic data with documented provenance.&lt;/li&gt;&lt;li&gt;Describes a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and releases dataset and curation recipes for reproducibility.&lt;/li&gt;&lt;li&gt;Reports controlled experiments (models at 130M/400M/1.3B/1.7B; 50B/300B token budgets) showing MixtureVitae outperforms other permissive datasets on standard benchmarks, with particularly strong math/code performance and competitive QA results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research / Dataset / Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huu Nguyen (Sonny)', 'Victor May (Sonny)', 'Harsh Raj (Sonny)', 'Marianna Nezhurina (Sonny)', 'Yishan Wang (Sonny)', 'Yanqi Luo (Sonny)', 'Minh Chien Vu (Sonny)', 'Taishi Nakamura (Sonny)', 'Ken Tsui (Sonny)', 'Van Khue Nguyen (Sonny)', 'David Salinas (Sonny)', 'Aleksandra Krasnod\\k{e}bska (Sonny)', 'Christoph Schuhmann (Sonny)', 'Mats Leon Richter (Sonny)', 'Xuan-Son (Sonny)', 'Vu', 'Jenia Jitsev']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'pretraining', 'data curation', 'safety screening', 'licensing/legal risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25531</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs</title><link>https://arxiv.org/abs/2505.16134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Multilingual study of position bias across five languages and five LLMs, showing position effects are primarily model-driven with language-specific nuances.&lt;/li&gt;&lt;li&gt;Some models (Qwen2.5-7B-Instruct, DeepSeek 7B Chat, Mistral 7B) prefer late-context information, contradicting the assumed universal early-token preference.&lt;/li&gt;&lt;li&gt;Explicitly marking the most relevant context in prompts can reduce accuracy across languages, challenging common prompt-engineering practices.&lt;/li&gt;&lt;li&gt;Information placed in the middle of context produces the largest accuracy drop while not increasing output entropy, indicating confident but incorrect behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Menschikov', 'Alexander Kharitonov', 'Maiia Kotyga', 'Vadim Porvatov', 'Anna Zhukovskaya', 'David Kagramanyan', 'Egor Shvetsov', 'Evgeny Burnaev']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'prompt-engineering', 'safety-evaluation', 'model-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16134</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</title><link>https://arxiv.org/abs/2503.14749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'uncertainty distillation': use held-out data to map raw uncertainty estimates to calibrated semantic probabilities, then create supervised fine-tuning examples with verbalized probabilities.&lt;/li&gt;&lt;li&gt;Aims to teach LLMs to express calibrated semantic confidence (confidence in the answer regardless of lexicalization) rather than just lexical uncertainty.&lt;/li&gt;&lt;li&gt;Shows method yields well-calibrated verbalized confidences and is more efficient than strong baselines; can be applied to black-box models via API-based fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sophia Hager', 'David Mueller', 'Kevin Duh', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'calibration', 'model reliability', 'alignment/safety', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.14749</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models</title><link>https://arxiv.org/abs/2502.11028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of calibration (confidence vs. correctness) across nine LLMs on three factual QA datasets, comparing free-generation vs. distractor-augmented prompts.&lt;/li&gt;&lt;li&gt;Finds distractor prompts can substantially reduce miscalibration and improve accuracy (up to 460% relative accuracy improvement, up to 90% ECE reduction), with variation by model size and RLHF tuning.&lt;/li&gt;&lt;li&gt;Identifies persistent calibration failures (notably on person-based queries) and gives recommendations: targeted fine-tuning, structured prompting, and informed model choice to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prateek Chhikara']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'model safety', 'prompting', 'evaluation', 'confidence estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11028</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models</title><link>https://arxiv.org/abs/2512.10998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces three contextually-aware backdoor attack scenarios (ViralApp, Fever, Referral) that use domain-specific, semantically plausible triggers to evade conventional detection.&lt;/li&gt;&lt;li&gt;Proposes SCOUT (Saliency-based Classification Of Untrusted Tokens), a token-level saliency defense that identifies backdoor triggers by measuring the effect of removing individual tokens on the model's target-label logits.&lt;/li&gt;&lt;li&gt;Evaluates SCOUT on standard NLP benchmarks (SST-2, IMDB, AG News) against conventional backdoor attacks (BadNet, AddSent, SynBkd, StyleBkd) and the new context-aware attacks, showing successful detection while preserving clean-input accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Afane', 'Abhishek Satyam', 'Ke Chen', 'Tao Li', 'Junaid Farooq', 'Juntao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'data poisoning', 'adversarial NLP', 'LLM safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10998</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title><link>https://arxiv.org/abs/2512.11614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes treating the RAG pipeline as an interactive Merlin-Arthur proof system where a generator (Arthur) is trained with helpful evidence (Merlin) and adversarial misleading context (Morgana) to reduce hallucinations and learn to reject unsupported queries.&lt;/li&gt;&lt;li&gt;Uses a linear-time XAI method to identify and edit the most influential evidence spans, enabling Arthur to rely on specific grounding spans and improving explanation fidelity.&lt;/li&gt;&lt;li&gt;Introduces the Explained Information Fraction (EIF) to normalize mutual-information guarantees and a rigorous evaluation framework; reports gains in groundedness, soundness, reject behavior, and retriever metrics across datasets and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bj\\"orn Deiseroth', 'Max Henning H\\"oth', 'Kristian Kersting', 'Letitia Parcalabescu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'adversarial training', 'explainability', 'robustness / alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11614</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visualizing token importance for black-box language models</title><link>https://arxiv.org/abs/2512.11573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Distribution-Based Sensitivity Analysis (DBSA), a model-agnostic method to estimate per-input-token sensitivity for black-box LLMs without gradients or distributional assumptions.&lt;/li&gt;&lt;li&gt;Targets auditing and interpretability in high-stakes domains (legal, medical, regulatory) by visualizing token importance and enabling inspection of model reliance on specific tokens.&lt;/li&gt;&lt;li&gt;Handles stochasticity of LLM outputs and is presented as a lightweight, plug-and-play tool with illustrative examples showing it can reveal sensitivities missed by existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulius Rauba', 'Qiyao Wei', 'Mihaela van der Schaar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'model auditing', 'safety', 'black-box analysis', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11573</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</title><link>https://arxiv.org/abs/2512.11509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates how three hallucination-reduction techniques (Chain of Verification, Decoding by Contrasting Layers, and Retrieval-Augmented Generation) affect LLM creativity.&lt;/li&gt;&lt;li&gt;Experiments across multiple model families (LLaMA, Qwen, Mistral) and scales (1B–70B) using two creativity benchmarks (NeoCoder and CS4).&lt;/li&gt;&lt;li&gt;Finds divergent effects: CoVe increases divergent creativity, DoLa suppresses it, and RAG has minimal impact; discusses implications for scientific discovery where factuality and creativity must be balanced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohor Banerjee', 'Nadya Yuki Wangsajaya', 'Syed Ali Redha Alsagoff', 'Min Sen Tan', 'Zachary Choy Kit Chun', 'Alvin Chan Guo Wei']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety-evaluation', 'retrieval-augmented-generation', 'creativity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11509</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare</title><link>https://arxiv.org/abs/2512.11437</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CLINIC, a comprehensive benchmark assessing trustworthiness of language models in healthcare across 15 languages and 18 tasks.&lt;/li&gt;&lt;li&gt;Evaluates five security/safety-related dimensions: truthfulness, fairness, safety, robustness, and privacy, including adversarial attack and privacy breach assessments.&lt;/li&gt;&lt;li&gt;Finds significant failures: factual errors, demographic and linguistic biases, and vulnerability to privacy and adversarial attacks—especially in mid- and low-resource languages.&lt;/li&gt;&lt;li&gt;Frames a standardized evaluation suite to guide improvements in global safety and deployment readiness of LMs for healthcare.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akash Ghosh', 'Srivarshinee Sridhar', 'Raghav Kaushik Ravi', 'Muhsin Muhsin', 'Sriparna Saha', 'Chirag Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Robustness', 'Privacy', 'Adversarial attacks', 'Multilingual evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11437</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise</title><link>https://arxiv.org/abs/2512.11282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIP, a plug-and-play causal prompting framework that constructs causal relation sequences among entities/actions/events and injects them into prompts to guide model reasoning.&lt;/li&gt;&lt;li&gt;Uses causal intervention and counterfactual reasoning to suppress non-causal reasoning paths, aiming to reduce hallucinations and improve factual grounding and interpretability.&lt;/li&gt;&lt;li&gt;Evaluated across seven mainstream LLMs (e.g., GPT-4o, Gemini 2.0 Flash, Llama 3.1) with reported gains in Attributable Rate (+2.6), Causal Consistency Score (+0.38), fourfold increase in effective information density, and up to 55.1% reduction in end-to-end response latency.&lt;/li&gt;&lt;li&gt;Lightweight input-stage method focused on long, noisy retrieval contexts; primarily a prompt-engineering/causal-reasoning approach to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingsen Ma', 'Dianyun Wang', 'Ran Jing', 'Yujun Sun', 'Zhenbo Xu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'causal prompting', 'factuality/alignment', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11282</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FIBER: A Multilingual Evaluation Resource for Factual Inference Bias</title><link>https://arxiv.org/abs/2512.11110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIBER, a multilingual benchmark (English, Italian, Turkish) for factual knowledge evaluation in single- and multi-entity settings, covering sentence completion, QA, and object-count tasks.&lt;/li&gt;&lt;li&gt;Analyzes prompt-language induced inference bias in entity selection, finding language-dependent effects (e.g., Turkish prompts often show higher bias) and that 31% of topics have high inference bias.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs, showing multi-entity questions are harder than single-entity, English yields the highest MAP, and larger models (e.g., Llama-3.1-8B, Qwen-2.5-7B) outperform smaller 3B–4B models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evren Ayberk Munis', 'Deniz Y{\\i}lmaz', 'Arianna Muti', '\\c{C}a\\u{g}r{\\i} Toraman']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'evaluation benchmark', 'multilingual bias', 'prompt-language influence', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11110</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution</title><link>https://arxiv.org/abs/2512.11108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model- and method-agnostic framework of three evaluation metrics to characterize explanation biases.&lt;/li&gt;&lt;li&gt;Systematically measures lexical (what) and position (where) biases of post-hoc feature-attribution methods (e.g., Integrated Gradients) across two transformer models.&lt;/li&gt;&lt;li&gt;Evaluations on a controlled artificial classification task and a semi-controlled causal relation detection task on natural data.&lt;/li&gt;&lt;li&gt;Finds structural trade-offs: models strong on one bias type tend to be weak on the other, and anomalous explanations correlate with method bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Kamp', 'Roos Bakker', 'Dominique Blok']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'feature-attribution', 'bias', 'safety-evaluation', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11108</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title><link>https://arxiv.org/abs/2512.07462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends the FAIRGAME framework with a payoff-scaled Prisoner’s Dilemma and a multi-agent Public Goods Game to probe LLM behaviour in repeated social dilemmas.&lt;/li&gt;&lt;li&gt;Identifies consistent behavioural signatures across models and languages: incentive-sensitive cooperation, cross-linguistic divergence, and end-game shifts toward defection.&lt;/li&gt;&lt;li&gt;Uses supervised classifiers trained on canonical repeated-game strategies to infer underlying intentions, revealing systematic, model- and language-dependent strategy biases with implications for auditing and designing safe multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung-Kiet Huynh', 'Duy-Minh Dao-Sy', 'Thanh-Bang Cao', 'Phong-Hao Le', 'Hong-Dan Nguyen', 'Phu-Quy Nguyen-Lam', 'Minh-Luan Nguyen-Vo', 'Hong-Phat Pham', 'Phu-Hoa Pham', 'Thien-Kim Than', 'Chi-Nguyen Tran', 'Huy Tran', 'Gia-Thoai Tran-Le', 'Alessio Buscemi', 'Le Hong Trang', 'The Anh Han']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'multi-agent systems', 'game theory', 'strategy recognition', 'alignment/audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07462</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled evaluation framework with four stress tests probing multi-step logical reasoning: rule deletion (redundant vs essential), contradictory evidence injection, logic-preserving rewrites (single-law), and multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates three model families (BERT, Qwen2, LLaMA-like) showing perfect base performance but large drops when essential rules are removed or contradictions are injected; single-law rewrites mostly preserve accuracy while multi-law stacking reveals model-dependent brittleness.&lt;/li&gt;&lt;li&gt;Finds contemporary LLMs are stable under many semantic-preserving reformulations but remain brittle to missing/inconsistent evidence and composed logical transformations; provides a concise diagnostic for isolating these failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'model evaluation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title><link>https://arxiv.org/abs/2511.20629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MapReduce LoRA (train preference-specific LoRA experts in parallel, iteratively merge to refine a shared base) and RaTE (reward-specific token embeddings that compose at inference) to improve multi-preference alignment.&lt;/li&gt;&lt;li&gt;Demonstrates substantial gains across modalities (text-to-image, text-to-video, language) on metrics including aesthetic/perceptual scores and safety-related metrics (helpful and harmless).&lt;/li&gt;&lt;li&gt;Targets the Pareto frontier of multi-reward optimization to reduce alignment trade-offs when jointly optimizing multiple preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chieh-Yun Chen', 'Zhonghao Wang', 'Qi Chen', 'Zhifan Ye', 'Min Shi', 'Yue Zhao', 'Yinan Zhao', 'Hui Qu', 'Wei-An Lin', 'Yiru Shen', 'Ajinkya Kale', 'Irfan Essa', 'Humphrey Shi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-preference optimization', 'LoRA', 'preference control', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20629</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title><link>https://arxiv.org/abs/2509.25531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixtureVitae, an open-access pretraining corpus built from permissive-first sources (public domain, CC-BY/Apache, government works, EU TDM-eligible) plus targeted instruction/reasoning and synthetic data with documented provenance.&lt;/li&gt;&lt;li&gt;Describes a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and releases dataset and curation recipes for reproducibility.&lt;/li&gt;&lt;li&gt;Reports controlled experiments (models at 130M/400M/1.3B/1.7B; 50B/300B token budgets) showing MixtureVitae outperforms other permissive datasets on standard benchmarks, with particularly strong math/code performance and competitive QA results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research / Dataset / Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huu Nguyen (Sonny)', 'Victor May (Sonny)', 'Harsh Raj (Sonny)', 'Marianna Nezhurina (Sonny)', 'Yishan Wang (Sonny)', 'Yanqi Luo (Sonny)', 'Minh Chien Vu (Sonny)', 'Taishi Nakamura (Sonny)', 'Ken Tsui (Sonny)', 'Van Khue Nguyen (Sonny)', 'David Salinas (Sonny)', 'Aleksandra Krasnod\\k{e}bska (Sonny)', 'Christoph Schuhmann (Sonny)', 'Mats Leon Richter (Sonny)', 'Xuan-Son (Sonny)', 'Vu', 'Jenia Jitsev']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'pretraining', 'data curation', 'safety screening', 'licensing/legal risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25531</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Readiness in Health AI</title><link>https://arxiv.org/abs/2509.18234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial stress tests to evaluate robustness of flagship medical LLMs and benchmarks, including multimodal reasoning scenarios.&lt;/li&gt;&lt;li&gt;Finds prevalent brittleness: models can still guess answers with key inputs removed, are sensitive to slight prompt alterations, and often produce convincing but flawed reasoning (hallucinations).&lt;/li&gt;&lt;li&gt;Uses clinician-guided rubrics to show medical benchmarks vary in what they measure and that frontier AI exhibits significant competency gaps for real-world clinical readiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Gu', 'Jingjing Fu', 'Xiaodong Liu', 'Jeya Maria Jose Valanarasu', 'Noel CF Codella', 'Reuben Tan', 'Qianchu Liu', 'Ying Jin', 'Sheng Zhang', 'Jinyu Wang', 'Rui Wang', 'Lei Song', 'Guanghui Qin', 'Naoto Usuyama', 'Cliff Wong', 'Hao Cheng', 'HoHin Lee', 'Praneeth Sanapathi', 'Sarah Hilado', 'Tristan Naumann', 'Javier Alvarez-Valle', 'Jiang Bian', 'Mu Wei', 'Khalil Malik', 'Lidong Zhou', 'Jianfeng Gao', 'Eric Horvitz', 'Matthew P. Lungren', 'Doug Burger', 'Eric Topol', 'Hoifung Poon', 'Paul Vozila']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-testing', 'robustness', 'medical-benchmarks', 'hallucination', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18234</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title><link>https://arxiv.org/abs/2506.21546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Counterfactual Segmentation Reasoning (CSR): model must segment object in factual image and abstain in a controlled visual counterfactual.&lt;/li&gt;&lt;li&gt;Introduces HalluSegBench, a large-scale benchmark with controlled visual counterfactuals and new metrics that measure hallucination severity and separate vision- vs language-driven failures.&lt;/li&gt;&lt;li&gt;Proposes RobustSeg trained with counterfactual fine-tuning (CFT) to teach abstention and reduce pixel-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Reports ~30% reduction in hallucinations and improved segmentation on FP-RefCOCO(+/g).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhuo Li', 'Adheesh Juvekar', 'Jiaxun Zhang', 'Xingyou Liu', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Yifan Shen', 'Tianjiao Yu', 'Ismini Lourentzou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'safety evaluation', 'counterfactual evaluation', 'segmentation VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21546</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing</title><link>https://arxiv.org/abs/2506.16666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a comprehensive framework for reviewing Differential Privacy (DP) auditing work and sets three desiderata for DP audits: efficiency, end-to-end-ness, and tightness.&lt;/li&gt;&lt;li&gt;Systematizes modes of operation of state-of-the-art DP auditing techniques, covering threat models, attacks, and evaluation functions.&lt;/li&gt;&lt;li&gt;Identifies overlooked details, limiting factors to achieving the desiderata, and open research problems to guide future work in DP auditing and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meenatchi Sundaram Muthu Selva Annamalai', 'Borja Balle', 'Jamie Hayes', 'Georgios Kaissis', 'Emiliano De Cristofaro']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-auditing', 'privacy-attacks', 'security-evaluation', 'systematization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16666</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safely Learning Controlled Stochastic Dynamics</title><link>https://arxiv.org/abs/2506.02754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for safely learning discrete-time controlled stochastic dynamics while ensuring trajectories remain inside predefined safe regions during training and deployment.&lt;/li&gt;&lt;li&gt;Uses iterative expansion of an initial safe control set with kernel-based confidence bounds to enable safe exploration and efficient estimation.&lt;/li&gt;&lt;li&gt;Provides theoretical safety guarantees and adaptive learning rates that improve with higher Sobolev regularity of the true dynamics.&lt;/li&gt;&lt;li&gt;Demonstrates empirical effectiveness in safety, estimation accuracy, and computational efficiency on benchmark tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luc Brogat-Motte', 'Alessandro Rudi', 'Riccardo Bonalli']&lt;/li&gt;&lt;li&gt;Tags: ['safe exploration', 'control systems', 'kernel methods', 'safety guarantees', 'safe learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02754</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs</title><link>https://arxiv.org/abs/2505.16134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Multilingual study of position bias across five languages and five LLMs, showing position effects are primarily model-driven with language-specific nuances.&lt;/li&gt;&lt;li&gt;Some models (Qwen2.5-7B-Instruct, DeepSeek 7B Chat, Mistral 7B) prefer late-context information, contradicting the assumed universal early-token preference.&lt;/li&gt;&lt;li&gt;Explicitly marking the most relevant context in prompts can reduce accuracy across languages, challenging common prompt-engineering practices.&lt;/li&gt;&lt;li&gt;Information placed in the middle of context produces the largest accuracy drop while not increasing output entropy, indicating confident but incorrect behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Menschikov', 'Alexander Kharitonov', 'Maiia Kotyga', 'Vadim Porvatov', 'Anna Zhukovskaya', 'David Kagramanyan', 'Egor Shvetsov', 'Evgeny Burnaev']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'prompt-engineering', 'safety-evaluation', 'model-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16134</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantum Support Vector Regression for Robust Anomaly Detection</title><link>https://arxiv.org/abs/2505.01012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Quantum Support Vector Regression (QSVR) to semisupervised anomaly detection and benchmarks performance on IBM NISQ hardware across 11 datasets.&lt;/li&gt;&lt;li&gt;Evaluates robustness of QSVR to various quantum noise channels (depolarizing, phase damping, phase flip, bit flip) and finds resilience to several but sensitivity to amplitude damping and miscalibration.&lt;/li&gt;&lt;li&gt;Investigates quantum adversarial machine learning and demonstrates that QSVR is highly vulnerable to adversarial attacks; neither quantum noise nor adversarial training substantially improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kilian Tscharke', 'Maximilian Wendlinger', 'Sebastian Issel', 'Pascal Debus']&lt;/li&gt;&lt;li&gt;Tags: ['quantum-ml', 'adversarial-ml', 'robustness', 'anomaly-detection', 'quantum-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01012</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</title><link>https://arxiv.org/abs/2503.14749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'uncertainty distillation': use held-out data to map raw uncertainty estimates to calibrated semantic probabilities, then create supervised fine-tuning examples with verbalized probabilities.&lt;/li&gt;&lt;li&gt;Aims to teach LLMs to express calibrated semantic confidence (confidence in the answer regardless of lexicalization) rather than just lexical uncertainty.&lt;/li&gt;&lt;li&gt;Shows method yields well-calibrated verbalized confidences and is more efficient than strong baselines; can be applied to black-box models via API-based fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sophia Hager', 'David Mueller', 'Kevin Duh', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'calibration', 'model reliability', 'alignment/safety', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.14749</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</title><link>https://arxiv.org/abs/2503.07020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-RCO, a risk-averse control framework that integrates LLM-based commonsense into autonomous driving via four modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator.&lt;/li&gt;&lt;li&gt;Introduces DriveLM-Deficit, a dataset of 53,895 annotated video clips of safety-critical object perception deficits for fine-tuning LLMs on hazard detection and motion planning.&lt;/li&gt;&lt;li&gt;Evaluates in the CARLA simulator showing LLM-RCO yields more proactive, context-aware maneuvers compared to default fully risk-averse (immediate-stop) policies, improving resilience to perception loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Hu', 'Chenhui Xu', 'Ruiyang Qin', 'Dancheng Liu', 'Amir Nassereldine', 'Yiyu Shi', 'Jinjun Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'LLM-augmented-control', 'robustness', 'perception-deficits', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07020</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title><link>https://arxiv.org/abs/2501.09761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VERITAS, a post-deployment framework that monitors AI-native transceivers for distribution shifts and triggers targeted retraining.&lt;/li&gt;&lt;li&gt;Uses an auxiliary neural network fed with 5G pilot signals to detect OOD changes in channel profile, transmitter speed, and delay spread.&lt;/li&gt;&lt;li&gt;On detection, runs a traditional reference receiver in parallel and compares bit-probability outputs to decide whether retraining is needed.&lt;/li&gt;&lt;li&gt;Reports detection accuracies (99%, 97%, 69%) and retraining initiation rates (86%, 93.3%, 94.8%) across the evaluated shift types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nasim Soltani', 'Michael Loehning', 'Kaushik Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'model-monitoring', 'robustness', 'deployment-safety', 'wireless-communications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09761</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>To Shuffle or not to Shuffle: Auditing DP-SGD with Shuffling</title><link>https://arxiv.org/abs/2411.10614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes novel DP-auditing procedures to empirically measure privacy leakage of DP-SGD when training uses shuffling (vs. the commonly analyzed Poisson subsampling).&lt;/li&gt;&lt;li&gt;Shows that privacy guarantees reported under Poisson assumptions can be significantly overestimated for shuffled training — up to ~4x under common settings and up to ~10x for two common shuffling variants — with gaps depending on batch size, epsilon, and threat model.&lt;/li&gt;&lt;li&gt;Demonstrates that the leakage difference is not uniform and provides an empirical framework to more tightly estimate actual privacy risk for models trained with shuffling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meenatchi Sundaram Muthu Selva Annamalai', 'Borja Balle', 'Jamie Hayes', 'Emiliano De Cristofaro']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy auditing', 'DP-SGD', 'shuffling', 'privacy leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10614</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Signed Graph Learning with Differential Privacy</title><link>https://arxiv.org/abs/2512.00307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ASGL, an adversarial signed graph learning method that achieves node-level differential privacy for signed graphs.&lt;/li&gt;&lt;li&gt;Decomposes signed graphs into positive and negative subgraphs and applies gradient perturbation within an adversarial module to reduce cascading errors and sensitivity.&lt;/li&gt;&lt;li&gt;Introduces a constrained BFS tree fused with balance theory to infer edge signs for generated node pairs and enable gradient decoupling.&lt;/li&gt;&lt;li&gt;Demonstrates favorable privacy-utility trade-offs on real-world datasets across multiple downstream tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haobin Ke', 'Sen Zhang', 'Qingqing Ye', 'Xun Ran', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'graph ML security', 'privacy-preserving learning', 'adversarial learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00307</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title><link>https://arxiv.org/abs/2511.14317</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Intervention Efficiency (IE), a capacity-aware metric that measures how efficiently a model identifies actionable true positives under limited intervention resources.&lt;/li&gt;&lt;li&gt;Proposes the Perturbation Validation Framework (PVF) to evaluate model stability under data perturbations and shifts, aiming to select models with invariant performance.&lt;/li&gt;&lt;li&gt;Targets the Rashomon Effect in clinical ML by prioritizing models that generalize robustly and align with operational/clinical constraints.&lt;/li&gt;&lt;li&gt;Empirical validation on synthetic and healthcare datasets demonstrates improved robustness-aware model selection compared to conventional metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuwen Zhang', 'Viet Tran', 'Paul Weng']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model selection', 'validation', 'clinical ML', 'stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14317</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering</title><link>https://arxiv.org/abs/2510.26898</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an ontology-integrated LLM pipeline that maps chemical engineering data to the COPE ontology via preprocessing, information extraction, and templated QA for fine-tuning.&lt;/li&gt;&lt;li&gt;Implements a control-focused decoding stage and a citation gate to constrain outputs to ontology-linked terms, aiming to improve factual grounding and auditable behavior.&lt;/li&gt;&lt;li&gt;Evaluates both linguistic quality and ontological accuracy, and discusses extensions like semantic retrieval and iterative validation for enhanced interpretability and reliability in process control and safety analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Crystal Su', 'Kuai Yu', 'Jingrui Zhang', 'Mingyuan Shao', 'Daniel Bauer']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'factual-grounding', 'control-systems', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26898</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</title><link>https://arxiv.org/abs/2507.05578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys memorization in LLMs: drivers (data duplication, training dynamics, fine-tuning) and its privacy/security implications.&lt;/li&gt;&lt;li&gt;Reviews measurement and attack methods (prefix-based extraction, membership inference, adversarial prompting) for detecting memorized content.&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies (data cleaning, differential privacy, post-training unlearning) and trade-offs with utility.&lt;/li&gt;&lt;li&gt;Highlights legal/ethical consequences and open challenges for balancing harmful memorization reduction with model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Xiong', 'Xuandong Zhao', 'Aneesh Pappu', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'membership-inference', 'differential-privacy', 'adversarial-prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05578</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems</title><link>https://arxiv.org/abs/2512.11750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LUCID, a verification engine that provides quantified safety guarantees for black-box stochastic dynamical systems learned from finite transition data.&lt;/li&gt;&lt;li&gt;Uses data-driven control barrier certificates learned in an RKHS via conditional mean embeddings and constructs a distributionally robust ambiguity set to account for out-of-distribution behavior.&lt;/li&gt;&lt;li&gt;Transforms a semi-infinite non-convex optimization into a tractable linear program using a finite Fourier kernel expansion and FFT-based spectral relaxation for scalable verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ernesto Casablanca', 'Oliver Sch\\"on', 'Paolo Zuliani', 'Sadegh Soudjani']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'formal verification', 'distributional robustness', 'control barrier certificates', 'kernel methods (RKHS)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11750</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</title><link>https://arxiv.org/abs/2512.11682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents TxAgent, an agentic therapeutic-reasoning system built on Llama-3.1-8B that issues dynamic function calls to a unified biomedical tool suite (ToolUniverse) for drug/treatment information.&lt;/li&gt;&lt;li&gt;Frames medical safety constraints by treating token-level reasoning traces and tool-invocation sequences as explicit supervision signals and evaluation targets.&lt;/li&gt;&lt;li&gt;Evaluates system performance in the NeurIPS CURE-Bench challenge using metrics for correctness, tool utilization, and reasoning quality; shows improvements from enhanced tool-retrieval strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Cofala', 'Christian Kalfar', 'Jingge Xiao', 'Johanna Schrader', 'Michelle Tang', 'Wolfgang Nejdl']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'agentic-AI', 'RAG', 'tool-usage', 'biomedical-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11682</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title><link>https://arxiv.org/abs/2512.11614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes treating the RAG pipeline as an interactive Merlin-Arthur proof system where a generator (Arthur) is trained with helpful evidence (Merlin) and adversarial misleading context (Morgana) to reduce hallucinations and learn to reject unsupported queries.&lt;/li&gt;&lt;li&gt;Uses a linear-time XAI method to identify and edit the most influential evidence spans, enabling Arthur to rely on specific grounding spans and improving explanation fidelity.&lt;/li&gt;&lt;li&gt;Introduces the Explained Information Fraction (EIF) to normalize mutual-information guarantees and a rigorous evaluation framework; reports gains in groundedness, soundness, reject behavior, and retriever metrics across datasets and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bj\\"orn Deiseroth', 'Max Henning H\\"oth', 'Kristian Kersting', 'Letitia Parcalabescu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'adversarial training', 'explainability', 'robustness / alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11614</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Visualizing token importance for black-box language models</title><link>https://arxiv.org/abs/2512.11573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Distribution-Based Sensitivity Analysis (DBSA), a model-agnostic method to estimate per-input-token sensitivity for black-box LLMs without gradients or distributional assumptions.&lt;/li&gt;&lt;li&gt;Targets auditing and interpretability in high-stakes domains (legal, medical, regulatory) by visualizing token importance and enabling inspection of model reliance on specific tokens.&lt;/li&gt;&lt;li&gt;Handles stochasticity of LLM outputs and is presented as a lightweight, plug-and-play tool with illustrative examples showing it can reveal sensitivities missed by existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulius Rauba', 'Qiyao Wei', 'Mihaela van der Schaar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'model auditing', 'safety', 'black-box analysis', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11573</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BAID: A Benchmark for Bias Assessment of AI Detectors</title><link>https://arxiv.org/abs/2512.11505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BAID, a large-scale benchmark (200k+ samples) for evaluating biases in AI-generated text detectors across 7 categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic.&lt;/li&gt;&lt;li&gt;Creates synthetic subgroup-specific versions of each sample via prompt-based generation to preserve content while reflecting different writing styles.&lt;/li&gt;&lt;li&gt;Evaluates four open-source state-of-the-art text detectors and finds systematic disparities in detection performance, notably lower recall for texts from underrepresented groups.&lt;/li&gt;&lt;li&gt;Provides a scalable, transparent auditing framework and argues for bias-aware evaluation before deployment of AI detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyam Basu', 'Yunfeng Zhang', 'Vipul Raheja']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'audit', 'AI text detection', 'benchmark', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11505</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty</title><link>https://arxiv.org/abs/2512.11373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an evidential segmentation framework for out-of-distribution (OOD) object segmentation using a Wasserstein loss that respects probability simplex geometry.&lt;/li&gt;&lt;li&gt;Combines Wasserstein loss with Kullback–Leibler regularization and Dice structural consistency to improve uncertainty-driven OOD segmentation.&lt;/li&gt;&lt;li&gt;Demonstrates improved OOD segmentation performance compared to existing uncertainty-based approaches, targeting safety-critical settings like autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnold Brosch', 'Abdelrahman Eldesokey', 'Michael Felsberg', 'Kira Maag']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'uncertainty estimation', 'robustness', 'semantic segmentation', 'safety-critical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11373</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TPV: Parameter Perturbations Through the Lens of Test Prediction Variance</title><link>https://arxiv.org/abs/2512.11089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Test Prediction Variance (TPV): a label-free measure of first-order sensitivity of model outputs to local parameter perturbations, separating model geometry from perturbation mechanism.&lt;/li&gt;&lt;li&gt;Shows theoretically that TPV estimated on the training set converges to test-set TPV in the overparameterized limit, enabling inference of test sensitivity from training inputs alone.&lt;/li&gt;&lt;li&gt;Empirically finds TPV is stable across datasets and architectures and correlates with clean test loss; models pruning as a TPV perturbation to derive a competitive, label-free importance measure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devansh Arpit']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'parameter perturbation', 'generalization', 'model sensitivity', 'pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11089</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning</title><link>https://arxiv.org/abs/2512.11760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SpectralKrum, which estimates a low-dimensional subspace from historical aggregates, projects client updates into that subspace, and applies Krum selection in compressed coordinates.&lt;/li&gt;&lt;li&gt;Adds an orthogonal-residual energy filter with a data-driven threshold to detect updates that deviate from the learned benign manifold.&lt;/li&gt;&lt;li&gt;Evaluated on CIFAR-10 non-IID partitions against multiple Byzantine attacks; strong vs. directional/subspace-aware attacks but limited vs. spectrally indistinguishable attacks (label-flip, min-max).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Tripathi', 'Karan Sharma', 'Rahul Mishra', 'Tapas Kumar Maiti']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'robust aggregation', 'spectral methods', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11760</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization</title><link>https://arxiv.org/abs/2512.11391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Null-Space constrained Policy Optimization (NSPO), an RL method that projects safety policy gradients into the null space of general-task gradients to avoid degrading core capabilities during safety alignment.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees that NSPO preserves original model capabilities while ensuring descent directions for safety optimization.&lt;/li&gt;&lt;li&gt;Empirically demonstrates state-of-the-art safety improvements on LLMs without sacrificing performance on general tasks (math, code, instruction-following) and with improved data efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Niu', 'Han Xiao', 'Dongyi Liu', 'Nuo Chen', 'Jia Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'RLHF', 'catastrophic forgetting', 'null-space projection', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11391</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attacking and Securing Community Detection: A Game-Theoretic Framework</title><link>https://arxiv.org/abs/2512.11359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes novel adversarial attack methods to hide targeted nodes/individuals from community detection models via imperceptible graph perturbations.&lt;/li&gt;&lt;li&gt;Develops corresponding defense techniques centered on a Rayleigh Quotient-based defender to improve robustness of community detection.&lt;/li&gt;&lt;li&gt;Introduces CD-GAME, a game-theoretic interactive framework modeling attacker–defender dynamics that iteratively update strategies until a Nash equilibrium is reached.&lt;/li&gt;&lt;li&gt;Empirical results show the proposed attack/defense outperform baselines and that equilibrium strategies are more imperceptible yet effective compared to single-step attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Niu', 'Aochuan Chen', 'Tingyang Xu', 'Jia Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'graph robustness', 'community detection', 'game theory', 'defense/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11359</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?</title><link>https://arxiv.org/abs/2512.11352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAT, a GNN-based model for context-aware trust prediction in dynamic heterogeneous graphs using continuous-time representations and time encoding.&lt;/li&gt;&lt;li&gt;Models heterogeneity with a dual attention mechanism over node types and within-type nodes, and introduces meta-path-based context embeddings to predict context-aware and overall trust.&lt;/li&gt;&lt;li&gt;Evaluates on real-world datasets, reports improved accuracy vs multiple baselines, scalability to large graphs, and robustness against both trust-oriented and GNN-oriented attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Wang', 'Zheng Yan', 'Jiahe Lan', 'Xuyan Li', 'Elisa Bertino']&lt;/li&gt;&lt;li&gt;Tags: ['GNN robustness', 'trust prediction', 'dynamic heterogeneous graphs', 'adversarial/attack robustness', 'context-aware modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11352</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking the Generality of Vision-Language-Action Models</title><link>https://arxiv.org/abs/2512.11315</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MultiNet v1.0, a unified benchmark to evaluate cross-domain generality of vision-language models (VLMs) and vision-language-action models (VLAs) across six capability regimes: visual grounding, spatial reasoning, tool use, physical commonsense, multi-agent coordination, and continuous robot control.&lt;/li&gt;&lt;li&gt;Evaluates models (GPT-5, Pi0, Magma) and finds no model shows consistent generality — substantial degradation occurs on unseen domains, unfamiliar modalities, and cross-domain task shifts.&lt;/li&gt;&lt;li&gt;Identifies common failure modes: modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.&lt;/li&gt;&lt;li&gt;Provides code, data, and leaderboards to standardize evaluation and diagnose gaps in building generalist agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Guruprasad', 'Sudipta Chowdhury', 'Harsh Sikka', 'Mridul Sharma', 'Helen Lu', 'Sean Rivera', 'Aryan Khurana', 'Hangliang Ren', 'Yangyue Wang']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'robustness', 'generalization', 'multimodal', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11315</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models</title><link>https://arxiv.org/abs/2512.11194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies memorization of concept-level features in text-to-image diffusion models as an IP/privacy risk and argues existing dememorization (filtering/regularization) is insufficient.&lt;/li&gt;&lt;li&gt;Proposes a Gradient Projection Framework that, during backpropagation, projects gradient updates onto the orthogonal complement of prohibited feature embedding subspaces to remove influence of sensitive concepts.&lt;/li&gt;&lt;li&gt;Integrates with standard diffusion training, accompanied by theoretical analysis and experiments showing strong reduction in memorization while preserving generation quality and semantic fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Kothandaraman', 'Jaclyn Pytlarz']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'selective unlearning', 'diffusion models', 'privacy', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11194</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification</title><link>https://arxiv.org/abs/2512.11087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a linear constraint-driven clipping framework that uses linear constraints to clip verified/irrelevant portions of the input space and to tighten intermediate bounds during verification.&lt;/li&gt;&lt;li&gt;Presents two algorithms that leverage constraints from bound propagation (and other sources) and a specialized GPU procedure to handle linear constraints without expensive external solvers, enabling scalability to large networks.&lt;/li&gt;&lt;li&gt;Integrates with BaB-based verifiers (e.g., α,β-CROWN), reducing the number of BaB subproblems (reported up to 96% reduction) and improving verified accuracy across benchmarks; part of the VNN-COMP 2025 winning verifier.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Jorge Chavez', 'Hesun Chen', 'Grani A. Hanasusanto', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['neural network verification', 'formal verification', 'robustness', 'branch-and-bound', 'bound tightening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11087</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title><link>https://arxiv.org/abs/2512.07462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends the FAIRGAME framework with a payoff-scaled Prisoner’s Dilemma and a multi-agent Public Goods Game to probe LLM behaviour in repeated social dilemmas.&lt;/li&gt;&lt;li&gt;Identifies consistent behavioural signatures across models and languages: incentive-sensitive cooperation, cross-linguistic divergence, and end-game shifts toward defection.&lt;/li&gt;&lt;li&gt;Uses supervised classifiers trained on canonical repeated-game strategies to infer underlying intentions, revealing systematic, model- and language-dependent strategy biases with implications for auditing and designing safe multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung-Kiet Huynh', 'Duy-Minh Dao-Sy', 'Thanh-Bang Cao', 'Phong-Hao Le', 'Hong-Dan Nguyen', 'Phu-Quy Nguyen-Lam', 'Minh-Luan Nguyen-Vo', 'Hong-Phat Pham', 'Phu-Hoa Pham', 'Thien-Kim Than', 'Chi-Nguyen Tran', 'Huy Tran', 'Gia-Thoai Tran-Le', 'Alessio Buscemi', 'Le Hong Trang', 'The Anh Han']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'multi-agent systems', 'game theory', 'strategy recognition', 'alignment/audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07462</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defense That Attacks: How Robust Models Become Better Attackers</title><link>https://arxiv.org/abs/2512.02830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that adversarially trained (AT) models produce adversarial perturbations that transfer more effectively to other models than perturbations from standard models.&lt;/li&gt;&lt;li&gt;Conducts comprehensive transferability experiments across a zoo of 36 models (CNNs and ViTs) to quantify this effect.&lt;/li&gt;&lt;li&gt;Highlights a new ecosystem risk where robustness training can unintentionally improve attacker capabilities and releases models/code for reproducibility.&lt;/li&gt;&lt;li&gt;Argues that robustness evaluations should measure both resistance to transferred attacks and a model's propensity to generate transferable attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Awad', 'Mahmoud Akrm', 'Walid Gomaa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'adversarial-training', 'transferability', 'robustness', 'model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02830</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Signed Graph Learning with Differential Privacy</title><link>https://arxiv.org/abs/2512.00307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ASGL, an adversarial signed graph learning method that achieves node-level differential privacy for signed graphs.&lt;/li&gt;&lt;li&gt;Decomposes signed graphs into positive and negative subgraphs and applies gradient perturbation within an adversarial module to reduce cascading errors and sensitivity.&lt;/li&gt;&lt;li&gt;Introduces a constrained BFS tree fused with balance theory to infer edge signs for generated node pairs and enable gradient decoupling.&lt;/li&gt;&lt;li&gt;Demonstrates favorable privacy-utility trade-offs on real-world datasets across multiple downstream tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haobin Ke', 'Sen Zhang', 'Qingqing Ye', 'Xun Ran', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'graph ML security', 'privacy-preserving learning', 'adversarial learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00307</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title><link>https://arxiv.org/abs/2511.20663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MTTR-A, an adaptation of classical reliability metrics (MTTR, MTBF) to quantify cognitive recovery latency in multi-agent systems.&lt;/li&gt;&lt;li&gt;Presents a benchmark simulation (AG News corpus, LangGraph) reporting median MTTR-A ≈ 6.21±2.14s and compares automated reflexes vs human-approval interventions.&lt;/li&gt;&lt;li&gt;Derives reliability bounds linking recovery time and cognitive uptime to formalize runtime dependability of distributed reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Barak Or']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Robustness', 'Multi-agent systems', 'Runtime monitoring', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20663</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title><link>https://arxiv.org/abs/2511.20629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MapReduce LoRA (train preference-specific LoRA experts in parallel, iteratively merge to refine a shared base) and RaTE (reward-specific token embeddings that compose at inference) to improve multi-preference alignment.&lt;/li&gt;&lt;li&gt;Demonstrates substantial gains across modalities (text-to-image, text-to-video, language) on metrics including aesthetic/perceptual scores and safety-related metrics (helpful and harmless).&lt;/li&gt;&lt;li&gt;Targets the Pareto frontier of multi-reward optimization to reduce alignment trade-offs when jointly optimizing multiple preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chieh-Yun Chen', 'Zhonghao Wang', 'Qi Chen', 'Zhifan Ye', 'Min Shi', 'Yue Zhao', 'Yinan Zhao', 'Hui Qu', 'Wei-An Lin', 'Yiru Shen', 'Ajinkya Kale', 'Irfan Essa', 'Humphrey Shi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-preference optimization', 'LoRA', 'preference control', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20629</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title><link>https://arxiv.org/abs/2511.06682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Textual Self-Attention Network (TSAN), a test-time, zero-parameter-update method that emulates self-attention in natural language to analyze and weigh multiple candidate responses.&lt;/li&gt;&lt;li&gt;Formats candidates as textual keys/values and uses an LLM-based attention module operating in a textual gradient space to iteratively synthesize a preference-aligned response.&lt;/li&gt;&lt;li&gt;Operates via iterative, interpretable optimization at test time and claims to outperform supervised fine-tuned models and prior test-time alignment methods with only a few iterations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shibing Mo', 'Haoyang Ruan', 'Kai Wu', 'Jing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'preference optimization', 'LLM safety', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06682</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title><link>https://arxiv.org/abs/2509.25531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixtureVitae, an open-access pretraining corpus built from permissive-first sources (public domain, CC-BY/Apache, government works, EU TDM-eligible) plus targeted instruction/reasoning and synthetic data with documented provenance.&lt;/li&gt;&lt;li&gt;Describes a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and releases dataset and curation recipes for reproducibility.&lt;/li&gt;&lt;li&gt;Reports controlled experiments (models at 130M/400M/1.3B/1.7B; 50B/300B token budgets) showing MixtureVitae outperforms other permissive datasets on standard benchmarks, with particularly strong math/code performance and competitive QA results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research / Dataset / Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huu Nguyen (Sonny)', 'Victor May (Sonny)', 'Harsh Raj (Sonny)', 'Marianna Nezhurina (Sonny)', 'Yishan Wang (Sonny)', 'Yanqi Luo (Sonny)', 'Minh Chien Vu (Sonny)', 'Taishi Nakamura (Sonny)', 'Ken Tsui (Sonny)', 'Van Khue Nguyen (Sonny)', 'David Salinas (Sonny)', 'Aleksandra Krasnod\\k{e}bska (Sonny)', 'Christoph Schuhmann (Sonny)', 'Mats Leon Richter (Sonny)', 'Xuan-Son (Sonny)', 'Vu', 'Jenia Jitsev']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'pretraining', 'data curation', 'safety screening', 'licensing/legal risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25531</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback</title><link>https://arxiv.org/abs/2507.13171</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RLIHF: reinforcement learning from implicit human feedback using non-invasive EEG (ErrPs) decoded into probabilistic reward signals to supplement sparse task rewards.&lt;/li&gt;&lt;li&gt;Uses a pre-trained decoder to map raw EEG to reward components and trains agents in a MuJoCo simulation with a Kinova Gen2 arm on a pick-and-place with obstacle avoidance.&lt;/li&gt;&lt;li&gt;Finds that policies trained with decoded EEG feedback achieve performance comparable to manually designed dense rewards, suggesting potential for scalable human-aligned robot learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suzie Kim', 'Hye-Bin Shin', 'Seong-Whan Lee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'implicit feedback (EEG/ErrP)', 'robotics', 'human-robot interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13171</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title><link>https://arxiv.org/abs/2506.21546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Counterfactual Segmentation Reasoning (CSR): model must segment object in factual image and abstain in a controlled visual counterfactual.&lt;/li&gt;&lt;li&gt;Introduces HalluSegBench, a large-scale benchmark with controlled visual counterfactuals and new metrics that measure hallucination severity and separate vision- vs language-driven failures.&lt;/li&gt;&lt;li&gt;Proposes RobustSeg trained with counterfactual fine-tuning (CFT) to teach abstention and reduce pixel-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Reports ~30% reduction in hallucinations and improved segmentation on FP-RefCOCO(+/g).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhuo Li', 'Adheesh Juvekar', 'Jiaxun Zhang', 'Xingyou Liu', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Yifan Shen', 'Tianjiao Yu', 'Ismini Lourentzou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'robustness', 'safety evaluation', 'counterfactual evaluation', 'segmentation VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21546</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks</title><link>https://arxiv.org/abs/2504.21228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CachePrune, a defense that identifies and prunes task-triggering neurons from the KV cache to prevent LLMs from treating prompt context as instructions.&lt;/li&gt;&lt;li&gt;Introduces a neural attribution mechanism with a preferential attribution loss to locate those neurons using few samples while preserving clean-response quality.&lt;/li&gt;&lt;li&gt;Leverages an observed triggering effect in generation behavior and requires no prompt reformatting or extra test-time LLM calls; reports reduced attack success rates with maintained response quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Wang', 'Junda Wu', 'Yu Xia', 'Tong Yu', 'Ruiyi Zhang', 'Ryan Rossi', 'Subrata Mitra', 'Lina Yao', 'Julian McAuley']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM defenses', 'neural attribution', 'KV-cache pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21228</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</title><link>https://arxiv.org/abs/2503.07020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-RCO, a risk-averse control framework that integrates LLM-based commonsense into autonomous driving via four modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator.&lt;/li&gt;&lt;li&gt;Introduces DriveLM-Deficit, a dataset of 53,895 annotated video clips of safety-critical object perception deficits for fine-tuning LLMs on hazard detection and motion planning.&lt;/li&gt;&lt;li&gt;Evaluates in the CARLA simulator showing LLM-RCO yields more proactive, context-aware maneuvers compared to default fully risk-averse (immediate-stop) policies, improving resilience to perception loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Hu', 'Chenhui Xu', 'Ruiyang Qin', 'Dancheng Liu', 'Amir Nassereldine', 'Yiyu Shi', 'Jinjun Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'LLM-augmented-control', 'robustness', 'perception-deficits', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07020</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models</title><link>https://arxiv.org/abs/2502.11028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of calibration (confidence vs. correctness) across nine LLMs on three factual QA datasets, comparing free-generation vs. distractor-augmented prompts.&lt;/li&gt;&lt;li&gt;Finds distractor prompts can substantially reduce miscalibration and improve accuracy (up to 460% relative accuracy improvement, up to 90% ECE reduction), with variation by model size and RLHF tuning.&lt;/li&gt;&lt;li&gt;Identifies persistent calibration failures (notably on person-based queries) and gives recommendations: targeted fine-tuning, structured prompting, and informed model choice to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prateek Chhikara']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'model safety', 'prompting', 'evaluation', 'confidence estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11028</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title><link>https://arxiv.org/abs/2501.09761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VERITAS, a post-deployment framework that monitors AI-native transceivers for distribution shifts and triggers targeted retraining.&lt;/li&gt;&lt;li&gt;Uses an auxiliary neural network fed with 5G pilot signals to detect OOD changes in channel profile, transmitter speed, and delay spread.&lt;/li&gt;&lt;li&gt;On detection, runs a traditional reference receiver in parallel and compares bit-probability outputs to decide whether retraining is needed.&lt;/li&gt;&lt;li&gt;Reports detection accuracies (99%, 97%, 69%) and retraining initiation rates (86%, 93.3%, 94.8%) across the evaluated shift types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nasim Soltani', 'Michael Loehning', 'Kaushik Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['distribution-shift', 'model-monitoring', 'robustness', 'deployment-safety', 'wireless-communications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09761</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Backdoor Stealthiness in Model Parameter Space</title><link>https://arxiv.org/abs/2501.05928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 12 existing input- and feature-space backdoor attacks against 17 defenses and uncovers a blind spot: these attacks leave detectable signatures in model parameter space.&lt;/li&gt;&lt;li&gt;Identifies prominent backdoor-related neurons in parameter space as a common vulnerability and proposes Grond, a supply-chain attack using Adversarial Backdoor Injection (ABI) to limit parameter changes and improve stealthiness in parameter space.&lt;/li&gt;&lt;li&gt;Empirically shows Grond outperforms prior attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and an ImageNet subset, and that ABI can enhance other backdoor attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Xu', 'Zhuoran Liu', 'Stefanos Koffas', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'parameter-space analysis', 'supply-chain attack', 'adaptive backdoor injection', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.05928</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment</title><link>https://arxiv.org/abs/2512.10206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CP-Env, an agentic, controllable hospital simulation for end-to-end clinical pathway evaluation of LLMs (triage, consults, diagnostics, multidisciplinary flow).&lt;/li&gt;&lt;li&gt;Proposes a three-tiered evaluation framework: Clinical Efficacy, Process Competency, and Professional Ethics to measure long-horizon, branching clinical performance.&lt;/li&gt;&lt;li&gt;Findings show many LLMs struggle with pathway complexity (hallucinations, loss of diagnostic details), that excessive reasoning steps can be harmful, and top models internalize knowledge reducing tool dependence; benchmark and tools are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yakun Zhu', 'Zhongzhen Huang', 'Qianhan Feng', 'Linjie Mu', 'Yannian Gu', 'Shaoting Zhang', 'Qi Dou', 'Xiaofan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'medical AI', 'benchmarking', 'LLM robustness', 'agent-based simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10206</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title><link>https://arxiv.org/abs/2512.06393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled evaluation framework with four stress tests probing multi-step logical reasoning: rule deletion (redundant vs essential), contradictory evidence injection, logic-preserving rewrites (single-law), and multi-law equivalence stacking.&lt;/li&gt;&lt;li&gt;Evaluates three model families (BERT, Qwen2, LLaMA-like) showing perfect base performance but large drops when essential rules are removed or contradictions are injected; single-law rewrites mostly preserve accuracy while multi-law stacking reveals model-dependent brittleness.&lt;/li&gt;&lt;li&gt;Finds contemporary LLMs are stable under many semantic-preserving reformulations but remain brittle to missing/inconsistent evidence and composed logical transformations; provides a concise diagnostic for isolating these failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiming Bao', 'Xiaoxuan Fu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'logical reasoning', 'model evaluation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06393</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2512.05943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents TRACE, a framework that assesses stepwise reasoning in vision-language models via Auxiliary Reasoning Sets (ARS) that decompose complex problems into intermediate sub-question/answer pairs.&lt;/li&gt;&lt;li&gt;Uses consistency-based metrics across ARS to correlate intermediate-step consistency with final-answer correctness and to localize reasoning failures.&lt;/li&gt;&lt;li&gt;Introduces confidence regions to separate reliable from unreliable reasoning trajectories, supporting filtering, debugging, and model improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shima Imani', 'Seungwhan Moon', 'Lambert Mathias', 'Lu Zhang', 'Babak Damavandi']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'reasoning-robustness', 'vision-language-models', 'transparent-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05943</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICPO (Intrinsic Confidence-Driven Group Relative Preference Optimization) that computes a preference advantage score from relative generation probabilities of multiple LLM responses to the same prompt.&lt;/li&gt;&lt;li&gt;Integrates the preference advantage score with verifiable rewards to guide exploration in RL for LLMs, aiming to mitigate coarse-grained rewards, reward noise, entropy collapse, and overconfident errors.&lt;/li&gt;&lt;li&gt;Claims improved preservation of undervalued high-quality responses and reduced overfitting to specific strategies, yielding steady reasoning improvements over GRPO on multiple general-domain and mathematical benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinpeng Wang', 'Chao Li', 'Ting Ye', 'Mengyuan Zhang', 'Wei Liu', 'Jian Luan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'reinforcement learning', 'reward modeling', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21005</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning</title><link>https://arxiv.org/abs/2511.12963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedRule-KG: a compact knowledge-graph scaffold plus a deterministic verifier to steer LLM generation toward mathematically and biomedically valid outputs.&lt;/li&gt;&lt;li&gt;Formalizes generation as constrained inference and introduces a soft-guidance surrogate for decoding; injects curated symbolic facts into prompts and enforces rule satisfaction with a checker.&lt;/li&gt;&lt;li&gt;Evaluated across 90 tasks (reaction feasibility, metabolic compatibility, toxicity screening), reporting an 83.2% reduction in rule violations and improved exact-match performance with negligible added latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Crystal Su']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness', 'biomedical LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12963</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</title><link>https://arxiv.org/abs/2510.16309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedRule-KG, a compact typed knowledge graph encoding entities, relations, and domain rules to ground LLM mathematical reasoning.&lt;/li&gt;&lt;li&gt;Couples the KG with a lightweight symbolic verifier that checks model predictions and applies minimal corrections to enforce consistency.&lt;/li&gt;&lt;li&gt;Reports large improvements on a 90-example benchmark (EM from 0.767 → 0.900 with KG; 1.000 with the verifier) and provides ablations and released code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Crystal Su']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'symbolic verification', 'knowledge graph', 'robustness', 'mathematical reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16309</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Illusion of Readiness in Health AI</title><link>https://arxiv.org/abs/2509.18234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces clinician-guided adversarial stress tests to evaluate flagship medical LLMs and benchmarks.&lt;/li&gt;&lt;li&gt;Finds widespread brittleness: models can ignore key inputs, are sensitive to minor prompt changes, and produce convincingly flawed reasoning.&lt;/li&gt;&lt;li&gt;Shows that popular medical benchmarks vary in what they measure and that leaderboard performance does not imply real-world readiness or safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Gu', 'Jingjing Fu', 'Xiaodong Liu', 'Jeya Maria Jose Valanarasu', 'Noel CF Codella', 'Reuben Tan', 'Qianchu Liu', 'Ying Jin', 'Sheng Zhang', 'Jinyu Wang', 'Rui Wang', 'Lei Song', 'Guanghui Qin', 'Naoto Usuyama', 'Cliff Wong', 'Hao Cheng', 'HoHin Lee', 'Praneeth Sanapathi', 'Sarah Hilado', 'Tristan Naumann', 'Javier Alvarez-Valle', 'Jiang Bian', 'Mu Wei', 'Khalil Malik', 'Lidong Zhou', 'Jianfeng Gao', 'Eric Horvitz', 'Matthew P. Lungren', 'Doug Burger', 'Eric Topol', 'Hoifung Poon', 'Paul Vozila']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial testing', 'robustness', 'medical AI', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18234</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection</title><link>https://arxiv.org/abs/2506.13793</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Med-REFL, a framework that learns fine-grained self-reflection for medical reasoning by automatically generating preference data via deterministic structural assessment of a tree-of-thoughts.&lt;/li&gt;&lt;li&gt;Constructs preference optimization pairs by globally evaluating explored reasoning paths so the model can recognize and correct its own reasoning errors without human labels or model distillation.&lt;/li&gt;&lt;li&gt;Demonstrates robust improvements on medical benchmarks (e.g., +5.82% on Llama3.1-8B, +4.13% on Huatuo-o1 for MedQA) and generalizes to other domains like logical reasoning while mitigating ‘fake reflection’.&lt;/li&gt;&lt;li&gt;Claims scalability and practical impact for reliability in high-stakes domains (medicine) by addressing the verification bottleneck in intermediate reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongxian Yang', 'Jiayu Qian', 'Zegao Peng', 'Haoyu Zhang', 'Yu-An Huang', 'KC Tan', 'Zhi-An Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'medical AI', 'self-reflection', 'tree-of-thoughts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13793</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</title><link>https://arxiv.org/abs/2512.11783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines "Super Suffixes": adversarial suffixes optimized to override multiple alignment objectives across different models and tokenizers, enabling malicious text and code generation.&lt;/li&gt;&lt;li&gt;Demonstrates successful bypass of Llama Prompt Guard 2 on five text-generation models using a joint optimization attack, claiming first to compromise LPG2 this way.&lt;/li&gt;&lt;li&gt;Proposes a lightweight detection signal based on cosine similarity between the model residual stream and concept directions, and introduces DeltaGuard which boosts non-benign detection to nearly 100%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Adiletta', 'Kathryn Adiletta', 'Kemal Derya', 'Berk Sunar']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'guard models', 'adversarial prompting', 'prompt injection', 'defenses/detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11783</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title><link>https://arxiv.org/abs/2512.11771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic security evaluation of image model fingerprinting methods under adversarial threat models (white-box and black-box) focusing on two attack goals: fingerprint removal and fingerprint forgery.&lt;/li&gt;&lt;li&gt;Implements five attack strategies and evaluates 14 fingerprinting techniques (RGB, frequency, learned-feature domains) across 12 state-of-the-art image generators, reporting high removal success (often &gt;80% white-box, &gt;50% constrained black-box) and variable forgery success.&lt;/li&gt;&lt;li&gt;Finds a trade-off between attribution accuracy and robustness, no method is both highly accurate and robust across all threat models, and highlights promising directions for more robust fingerprinting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yao', 'Marc Juarez']&lt;/li&gt;&lt;li&gt;Tags: ['image fingerprinting', 'adversarial attacks', 'model attribution', 'robustness evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11771</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title><link>https://arxiv.org/abs/2512.11614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes treating the RAG pipeline as an interactive Merlin-Arthur proof system where a generator (Arthur) is trained with helpful evidence (Merlin) and adversarial misleading context (Morgana) to reduce hallucinations and learn to reject unsupported queries.&lt;/li&gt;&lt;li&gt;Uses a linear-time XAI method to identify and edit the most influential evidence spans, enabling Arthur to rely on specific grounding spans and improving explanation fidelity.&lt;/li&gt;&lt;li&gt;Introduces the Explained Information Fraction (EIF) to normalize mutual-information guarantees and a rigorous evaluation framework; reports gains in groundedness, soundness, reject behavior, and retriever metrics across datasets and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bj\\"orn Deiseroth', 'Max Henning H\\"oth', 'Kristian Kersting', 'Letitia Parcalabescu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination mitigation', 'adversarial training', 'explainability', 'robustness / alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11614</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</title><link>https://arxiv.org/abs/2512.11509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates how three hallucination-reduction techniques (Chain of Verification, Decoding by Contrasting Layers, and Retrieval-Augmented Generation) affect LLM creativity.&lt;/li&gt;&lt;li&gt;Experiments across multiple model families (LLaMA, Qwen, Mistral) and scales (1B–70B) using two creativity benchmarks (NeoCoder and CS4).&lt;/li&gt;&lt;li&gt;Finds divergent effects: CoVe increases divergent creativity, DoLa suppresses it, and RAG has minimal impact; discusses implications for scientific discovery where factuality and creativity must be balanced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohor Banerjee', 'Nadya Yuki Wangsajaya', 'Syed Ali Redha Alsagoff', 'Min Sen Tan', 'Zachary Choy Kit Chun', 'Alvin Chan Guo Wei']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety-evaluation', 'retrieval-augmented-generation', 'creativity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11509</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models</title><link>https://arxiv.org/abs/2512.11482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies causes of memorization in code-specialized LLMs during fine-tuning and quantifies privacy risks from reproducing training snippets.&lt;/li&gt;&lt;li&gt;Applies Differential Privacy (DP) to CodeLLM training and systematically evaluates its effectiveness at mitigating memorization across snippet types.&lt;/li&gt;&lt;li&gt;Finds that DP substantially reduces memorization, slightly increases perplexity but preserves or can improve code generation quality, and has minimal impact on training time and energy consumption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melih Catal', 'Pooja Rani', 'Harald C. Gall']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'code LLMs', 'memorization/privacy', 'privacy-preserving ML', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11482</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MLLM Machine Unlearning via Visual Knowledge Distillation</title><link>https://arxiv.org/abs/2512.11325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a machine unlearning method for Multimodal LLMs (MLLMs) that disentangles visual and textual knowledge to selectively erase target visual information while preserving textual capabilities.&lt;/li&gt;&lt;li&gt;Introduces Visual Knowledge Distillation (VKD), using intermediate visual representations inside the MLLM as supervision rather than only output-level signals, improving unlearning effectiveness and utility preservation.&lt;/li&gt;&lt;li&gt;Fine-tunes only the visual components for efficiency and evaluates against state-of-the-art unlearning methods, showing superior effectiveness and efficiency, including robustness evaluation against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Zhenxing Niu', 'Haoxuan Ji', 'Guangyu He', 'Haichang Gao', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'multimodal-LLM', 'knowledge-distillation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11325</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title><link>https://arxiv.org/abs/2512.11296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a few-shot vision-language model (VLM) approach to jointly verify G-code text and corresponding HMI screenshots for a 15-slant-PRO CNC lathe.&lt;/li&gt;&lt;li&gt;Uses a structured JSON schema and few-shot examples (correct and error cases) to guide the VLM for error and safety-status detection.&lt;/li&gt;&lt;li&gt;Evaluates few-shot VLM versus zero-shot VLM across scenarios, reporting improved detection of HMI errors and G-code/HMI discrepancies by per-slot accuracy.&lt;/li&gt;&lt;li&gt;Demonstrates suitability for verifying manually generated G-code in CNC training contexts to support more comprehensive debugging and safety checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yasaman Hashem Pour', 'Nazanin Mahjourian', 'Vinh Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM', 'Few-shot prompting', 'G-code verification', 'HMI monitoring', 'CNC safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11296</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise</title><link>https://arxiv.org/abs/2512.11282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIP, a plug-and-play causal prompting framework that constructs causal relation sequences among entities/actions/events and injects them into prompts to guide model reasoning.&lt;/li&gt;&lt;li&gt;Uses causal intervention and counterfactual reasoning to suppress non-causal reasoning paths, aiming to reduce hallucinations and improve factual grounding and interpretability.&lt;/li&gt;&lt;li&gt;Evaluated across seven mainstream LLMs (e.g., GPT-4o, Gemini 2.0 Flash, Llama 3.1) with reported gains in Attributable Rate (+2.6), Causal Consistency Score (+0.38), fourfold increase in effective information density, and up to 55.1% reduction in end-to-end response latency.&lt;/li&gt;&lt;li&gt;Lightweight input-stage method focused on long, noisy retrieval contexts; primarily a prompt-engineering/causal-reasoning approach to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingsen Ma', 'Dianyun Wang', 'Ran Jing', 'Yujun Sun', 'Zhenbo Xu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'causal prompting', 'factuality/alignment', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11282</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Scalable Multi-GPU Framework for Encrypted Large-Model Inference</title><link>https://arxiv.org/abs/2512.11269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Cerium, a multi-GPU framework (DSL + optimizing compiler + runtime) for fully homomorphic encryption (FHE) inference on large models, addressing terabyte-scale memory and multi-device parallelism.&lt;/li&gt;&lt;li&gt;Introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization to generate high-performance GPU kernels for FHE workloads.&lt;/li&gt;&lt;li&gt;Demonstrates strong performance: outperforms hand-optimized GPU libraries on small models (up to 2.25x), matches prior FHE ASIC CraterLake, bootstraps in 7.5 ms, and runs encrypted inference for BERT-Base (8s) and Llama3-8B (134s) on NVIDIA GPUs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siddharth Jayashankar', 'Joshua Kim', 'Michael B. Sullivan', 'Wenting Zheng', 'Dimitrios Skarlatos']&lt;/li&gt;&lt;li&gt;Tags: ['fully homomorphic encryption', 'encrypted inference', 'GPU acceleration', 'privacy-preserving ML', 'large-model inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11269</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MiniScope: A Least Privilege Framework for Authorizing Tool Calling Agents</title><link>https://arxiv.org/abs/2512.11147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MiniScope, a framework that enforces least-privilege for tool-calling LLM agents by reconstructing permission hierarchies among tool calls and applying a mobile-style permission model to restrict access.&lt;/li&gt;&lt;li&gt;Automatically derives permissions relationships to avoid manual policy authoring and to confine potential damage from unreliable LLM behavior.&lt;/li&gt;&lt;li&gt;Evaluates on a synthetic dataset modeled after ten real-world applications, showing 1–6% latency overhead and substantial improvements over an LLM-based baseline in minimizing permissions and reducing computational/operational costs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinhao Zhu', 'Kevin Tseng', 'Gil Vernik', 'Xiao Huang', 'Shishir G. Patil', 'Vivian Fang', 'Raluca Ada Popa']&lt;/li&gt;&lt;li&gt;Tags: ['Tool calling security', 'Least privilege', 'Agent confinement', 'Permission inference', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11147</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FIBER: A Multilingual Evaluation Resource for Factual Inference Bias</title><link>https://arxiv.org/abs/2512.11110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FIBER, a multilingual benchmark (English, Italian, Turkish) for factual knowledge evaluation in single- and multi-entity settings, covering sentence completion, QA, and object-count tasks.&lt;/li&gt;&lt;li&gt;Analyzes prompt-language induced inference bias in entity selection, finding language-dependent effects (e.g., Turkish prompts often show higher bias) and that 31% of topics have high inference bias.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs, showing multi-entity questions are harder than single-entity, English yields the highest MAP, and larger models (e.g., Llama-3.1-8B, Qwen-2.5-7B) outperform smaller 3B–4B models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evren Ayberk Munis', 'Deniz Y{\\i}lmaz', 'Arianna Muti', '\\c{C}a\\u{g}r{\\i} Toraman']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'evaluation benchmark', 'multilingual bias', 'prompt-language influence', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11110</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution</title><link>https://arxiv.org/abs/2512.11108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model- and method-agnostic framework of three evaluation metrics to characterize explanation biases.&lt;/li&gt;&lt;li&gt;Systematically measures lexical (what) and position (where) biases of post-hoc feature-attribution methods (e.g., Integrated Gradients) across two transformer models.&lt;/li&gt;&lt;li&gt;Evaluations on a controlled artificial classification task and a semi-controlled causal relation detection task on natural data.&lt;/li&gt;&lt;li&gt;Finds structural trade-offs: models strong on one bias type tend to be weak on the other, and anomalous explanations correlate with method bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Kamp', 'Roos Bakker', 'Dominique Blok']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'feature-attribution', 'bias', 'safety-evaluation', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11108</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification</title><link>https://arxiv.org/abs/2512.11087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a linear constraint-driven clipping framework that uses linear constraints to clip verified/irrelevant portions of the input space and to tighten intermediate bounds during verification.&lt;/li&gt;&lt;li&gt;Presents two algorithms that leverage constraints from bound propagation (and other sources) and a specialized GPU procedure to handle linear constraints without expensive external solvers, enabling scalability to large networks.&lt;/li&gt;&lt;li&gt;Integrates with BaB-based verifiers (e.g., α,β-CROWN), reducing the number of BaB subproblems (reported up to 96% reduction) and improving verified accuracy across benchmarks; part of the VNN-COMP 2025 winning verifier.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duo Zhou', 'Jorge Chavez', 'Hesun Chen', 'Grani A. Hanasusanto', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['neural network verification', 'formal verification', 'robustness', 'branch-and-bound', 'bound tightening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11087</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring skill-based uplift from AI in a real biological laboratory</title><link>https://arxiv.org/abs/2512.10960</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pilot empirical study comparing skill-based uplift from access to an AI reasoning model versus internet-only control for novice participants performing E. coli transformation, induction of a reporter peptide, and mass spectrometry confirmation.&lt;/li&gt;&lt;li&gt;Quantitative outcomes (task completion rates) and qualitative observations of participant interactions with the AI, internet resources, lab equipment, and peers were recorded.&lt;/li&gt;&lt;li&gt;Presents lessons learned for designing such studies and discusses implications for biosecurity, AI-enabled misuse risks, and future evaluations of AI–human interactions in wet labs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (pilot empirical study)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ethan Obie Romero-Severson', 'Tara Harvey', 'Nick Generous', 'Phillip M. Mach']&lt;/li&gt;&lt;li&gt;Tags: ['biosecurity', 'AI-assisted lab', 'skill-uplift', 'empirical study', 'dual-use risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10960</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</title><link>https://arxiv.org/abs/2512.11682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents TxAgent, an agentic therapeutic-reasoning system built on Llama-3.1-8B that issues dynamic function calls to a unified biomedical tool suite (ToolUniverse) for drug/treatment information.&lt;/li&gt;&lt;li&gt;Frames medical safety constraints by treating token-level reasoning traces and tool-invocation sequences as explicit supervision signals and evaluation targets.&lt;/li&gt;&lt;li&gt;Evaluates system performance in the NeurIPS CURE-Bench challenge using metrics for correctness, tool utilization, and reasoning quality; shows improvements from enhanced tool-retrieval strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Cofala', 'Christian Kalfar', 'Jingge Xiao', 'Johanna Schrader', 'Michelle Tang', 'Wolfgang Nejdl']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'agentic-AI', 'RAG', 'tool-usage', 'biomedical-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11682</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</title><link>https://arxiv.org/abs/2512.11544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of four mainstream LLMs (GPT-4o, Gemini 2.5, DeepSeek 3.1, Qwen3-Max) on extraction of core medical information from noisy/unstructured chief complaints using 20 medical probes across five dimensions.&lt;/li&gt;&lt;li&gt;Gold-standard answers defined by clinical experts; double-blind inverse rating by two independent clinicians to assess model outputs.&lt;/li&gt;&lt;li&gt;Findings show varying degrees of functional degradation under noisy input, with extreme noise causing functional collapse for most models; GPT-4o made a severe risk-assessment error for PE secondary to DVT.&lt;/li&gt;&lt;li&gt;Introduces the concept 'AI-MASLD' (AI-Metabolic Dysfunction-Associated Steatotic Liver Disease) as an analogy to describe systematic functional decline, and issues safety warnings about clinical use without human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Shen', 'Xiaojun Wu', 'Linghua Yu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'clinical AI safety', 'safety evaluation', 'noisy-input / prompt robustness', 'reliability in healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11544</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BAID: A Benchmark for Bias Assessment of AI Detectors</title><link>https://arxiv.org/abs/2512.11505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BAID, a large-scale benchmark (200k+ samples) for evaluating biases in AI-generated text detectors across 7 categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic.&lt;/li&gt;&lt;li&gt;Creates synthetic subgroup-specific versions of each sample via prompt-based generation to preserve content while reflecting different writing styles.&lt;/li&gt;&lt;li&gt;Evaluates four open-source state-of-the-art text detectors and finds systematic disparities in detection performance, notably lower recall for texts from underrepresented groups.&lt;/li&gt;&lt;li&gt;Provides a scalable, transparent auditing framework and argues for bias-aware evaluation before deployment of AI detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Priyam Basu', 'Yunfeng Zhang', 'Vipul Raheja']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'audit', 'AI text detection', 'benchmark', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11505</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>General-purpose AI models can generate actionable knowledge on agroecological crop protection</title><link>https://arxiv.org/abs/2512.11474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically compares web-grounded LLM (DeepSeek) versus non-grounded ChatGPT on generating agroecological crop protection knowledge for nine pests/diseases/weeds.&lt;/li&gt;&lt;li&gt;Evaluates factual accuracy, data consistency (lab-to-field), breadth of reported biological control agents, and incidence of hallucinations/fabricated references.&lt;/li&gt;&lt;li&gt;Finds DeepSeek searches a much larger literature corpus and reports more agents and higher efficacy estimates; both models still hallucinate, confuse nomenclature, and omit key data.&lt;/li&gt;&lt;li&gt;Concludes LLMs can produce useful low-resolution efficacy trends but require rigorous human oversight for farm-level decision support.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kris A. G. Wyckhuys']&lt;/li&gt;&lt;li&gt;Tags: ['LLM factuality', 'hallucinations', 'safety evaluation', 'domain robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11474</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</title><link>https://arxiv.org/abs/2512.11421</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a task-completion framework for LLM-based agents operating under explicit behavioral guidance with RL-style observations, actions, and rewards.&lt;/li&gt;&lt;li&gt;Introduces three components: a task profiler to pick reasoning/generation strategies, a reasoning module learning verifiable observation→action mappings, and a generation module that enforces constraint-compliant outputs via validation or deterministic synthesis.&lt;/li&gt;&lt;li&gt;Claims components co-evolve during interaction to produce more trustworthy, verifiable multi-turn agent behavior.&lt;/li&gt;&lt;li&gt;Emphasizes reliability and verifiability of agent behavior rather than application-specific performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gonca G\\"ursun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'agent robustness', 'behavioral constraints', 'verifiability', 'safe RL/LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11421</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving</title><link>https://arxiv.org/abs/2512.11323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAPTURE, a comprehensive benchmark for evaluating LVLMs on CAPTCHA solving covering 4 main CAPTCHA types, 25 sub-types, and 31 vendors.&lt;/li&gt;&lt;li&gt;Provides large-scale, diverse data with LVLM-tailored labels to enable multi-dimensional evaluation missing from prior benchmarks.&lt;/li&gt;&lt;li&gt;Evaluates current LVLMs on the benchmark and reports poor performance in solving CAPTCHAs.&lt;/li&gt;&lt;li&gt;Aims to fill gaps in data comprehensiveness and labeling pertinence for LVLM-specific CAPTCHA assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianyi Zhang', 'Ziyin Zhou', 'Xu Ji', 'Shizhao Liu', 'Zhangchi Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['CAPTCHA', 'LVLM', 'Benchmarking', 'Security', 'Adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11323</guid><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
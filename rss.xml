<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 28 Jan 2026 23:22:29 +0000</lastBuildDate><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that uncurated AI-generated medical data creates a self-referential feedback loop that erodes pathological variability and diagnostic reliability across clinical text, vision-language reporting, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic representation skews, while models produce falsely confident but inaccurate reports (false reassurance rates rising to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation shows AI-generated documentation becomes clinically unusable after two generations; evaluates mitigations and finds mixing real data with quality-aware filtering preserves diversity while naive synthetic scaling fails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['dataset-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Memorization: Selective Learning for Copyright-Safe Diffusion Model Training</title><link>https://arxiv.org/abs/2512.11194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-time defense (gradient projection) that removes gradient components aligned with embeddings of prohibited concept-level features to prevent diffusion models from internalizing and reproducing copyrighted/sensitive concepts.&lt;/li&gt;&lt;li&gt;Operates during backpropagation by projecting updates onto the orthogonal complement of the sensitive feature embedding space, enabling selective learning that preserves non-sensitive data utility.&lt;/li&gt;&lt;li&gt;Provides analysis against adversaries aiming for feature extraction and empirical results showing large reductions in memorization while maintaining generation quality and semantic fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Kothandaraman', 'Jaclyn Pytlarz']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'training-time defense', 'privacy', 'copyright/IP protection', 'gradient projection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11194</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoD, a learnable detection framework for unseen jailbreak attacks in large vision-language models without using attack data or hand-crafted heuristics.&lt;/li&gt;&lt;li&gt;Extracts layer-wise safety representations from internal activations using Multi-modal Safety Concept Activation Vectors and converts them to a 1D anomaly score via a Safety Pattern Auto-Encoder.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art AUROC for detecting diverse unseen jailbreaks across multiple LVLMs and claims improved efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jiaqi Weng', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'adversarial detection', 'model safety', 'anomaly detection', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models</title><link>https://arxiv.org/abs/2502.10495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in latent-based watermarking for Latent Diffusion Models by showing that statistical patterns in outputs can reveal and compromise watermarks.&lt;/li&gt;&lt;li&gt;Proposes SWA-LDM, a lightweight defense that randomizes per-image latent watermarks using Gaussian-distributed latent noise to remove detectable artifacts while maintaining extraction robustness.&lt;/li&gt;&lt;li&gt;Reports experimental improvements (~20% average) in watermark stealth compared to prior methods, with preserved image quality and robustness to extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhonghao Yang', 'Linye Lyu', 'Xuanhang Chang', 'Daojing He', 'YU LI']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model defenses', 'latent diffusion models', 'steganographic watermarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10495</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Watermark-based Attribution of AI-Generated Content</title><link>https://arxiv.org/abs/2404.04254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes per-user watermarking for AI-generated content to enable user-level attribution by matching extracted watermarks to users.&lt;/li&gt;&lt;li&gt;Derives theoretical lower bounds on detection and attribution performance for any set of user watermarks and selects watermarks to maximize these bounds.&lt;/li&gt;&lt;li&gt;Empirically evaluates attribution accuracy and robustness, showing high accuracy under no post-processing, common post-processing (e.g., JPEG compression), and limited-query black-box adversarial post-processing.&lt;/li&gt;&lt;li&gt;Finds that attribution inherits both accuracy and (non-)robustness characteristics of the underlying watermarking scheme.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Yupu Wang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'attribution', 'detection', 'robustness', 'adversarial-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.04254</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</title><link>https://arxiv.org/abs/2601.18739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SeNeDiF-OOD: a hierarchical Semantic Nested Dichotomy Fusion framework that decomposes OOD detection into binary fusion nodes aligned with semantic abstraction levels.&lt;/li&gt;&lt;li&gt;Designed to handle heterogeneous OOD inputs (non-domain images, unknown classes, low-level corruptions) and evaluated on a real-world monument style classification dataset (MonuMAI).&lt;/li&gt;&lt;li&gt;Includes evaluation against adversarial attacks and shows that the hierarchical fusion approach outperforms traditional OOD baselines while preserving in-distribution performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ignacio Antequera-S\\'anchez", "Juan Luis Su\\'arez-D\\'iaz", 'Rosana Montes', 'Francisco Herrera']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'robustness', 'adversarial-robustness', 'hierarchical-fusion', 'open-world-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18739</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ForensicHub: A Unified Benchmark &amp; Codebase for All-Domain Fake Image Detection and Localization</title><link>https://arxiv.org/abs/2505.11003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicHub, a unified, modular benchmark and codebase for Fake Image Detection and Localization (FIDL) across four domains: Deepfake, IMDL, AIGC, and Document manipulation.&lt;/li&gt;&lt;li&gt;Implements a configuration-driven architecture with interchangeable components (datasets, transforms, models, evaluators) to enable cross-domain experiments and interoperability.&lt;/li&gt;&lt;li&gt;Provides 10 baseline models, 6 backbones, two new benchmarks (AIGC and Doc), and integrates existing Deepfake and IMDL benchmarks via adapters.&lt;/li&gt;&lt;li&gt;Presents in-depth analyses and eight actionable insights about model architectures, dataset characteristics, and evaluation standards in image forensics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Du', 'Xuekang Zhu', 'Xiaochen Ma', 'Chenfan Qu', 'Kaiwen Feng', 'Zhe Yang', 'Chi-Man Pun', 'Jian Liu', 'Ji-Zhe Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'benchmark', 'deepfake-detection', 'AIGC-detection', 'dataset-codebase']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11003</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Model Transcription with Differentially Private Synthetic Distillation</title><link>https://arxiv.org/abs/2601.19090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "differentially private synthetic distillation": a data-free model-to-model conversion that transcribes a pretrained teacher into a privacy-preserving student via a trainable generator.&lt;/li&gt;&lt;li&gt;Framework uses three players in alternate optimization—generator (creates synthetic data), teacher and student (produce differentially private/noisy labels), and student (trained on noisy labels) with adversarial training of the generator—yielding DP guarantees and convergence proofs.&lt;/li&gt;&lt;li&gt;Results show strong privacy-utility trade-offs: the transcribed student maintains good performance while protecting training data privacy; the generator can also produce private synthetic data for downstream use.&lt;/li&gt;&lt;li&gt;Empirically outperforms 26 state-of-the-art methods and provides theoretical privacy and convergence analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bochao Liu', 'Shiming Ge', 'Pengju Wang', 'Shikun Li', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'model distillation', 'privacy-preserving ML', 'synthetic data generation', 'defense against data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19090</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GUIGuard: Toward a General Framework for Privacy-Preserving GUI Agents</title><link>https://arxiv.org/abs/2601.18842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GUIGuard, a three-stage framework for privacy-preserving GUI agents: (1) privacy recognition, (2) privacy protection, and (3) task execution under protection.&lt;/li&gt;&lt;li&gt;Introduces GUIGuard-Bench: a cross-platform benchmark with 630 trajectories and 13,830 screenshots annotated with region-level privacy grounding, risk level, privacy category, and task necessity.&lt;/li&gt;&lt;li&gt;Empirical findings show poor privacy recognition by current agents (13.3% accuracy on Android, 1.4% on PC), demonstrate that protection strategies can preserve task-planning semantics, and identify privacy recognition as a key bottleneck.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Wang', 'Zhiling Zhang', 'Wenbo Zhou', 'Weiming Zhang', 'Jie Zhang', 'Qiannan Zhu', 'Yu Shi', 'Shuxin Zheng', 'Jiyan He']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'defense', 'benchmark', 'GUI agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18842</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The role of self-supervised pretraining in differentially private medical image analysis</title><link>https://arxiv.org/abs/2601.19618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of how different pretraining initializations (ImageNet supervised, DINOv3 self-supervised, domain-specific supervised on MIMIC-CXR) affect performance of ConvNeXt models trained with DP-SGD for chest radiograph classification.&lt;/li&gt;&lt;li&gt;Finds DINOv3 self-supervised initialization improves utility under differential privacy compared to ImageNet supervised initialization, but domain-specific supervised pretraining yields the best private performance, closest to non-private baselines.&lt;/li&gt;&lt;li&gt;Analyzes downstream effects on demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity, concluding initialization strategy is a central determinant of utility and fairness under DP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soroosh Tayebi Arasteh', 'Mina Farajiamiri', 'Mahshad Lotfinia', 'Behrus Hinrichs-Puladi', 'Jonas Bienzeisler', 'Mohamed Alhaskir', 'Mirabela Rusu', 'Christiane Kuhl', 'Sven Nebelung', 'Daniel Truhn']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'medical-imaging', 'self-supervised-learning', 'privacy-defense', 'transfer-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19618</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection</title><link>https://arxiv.org/abs/2601.19430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces X-AIGD, a fine-grained benchmark with pixel-level, categorized annotations of perceptual artifacts in AI-generated images covering low-level distortions, high-level semantics, and cognitive counterfactuals.&lt;/li&gt;&lt;li&gt;Enables interpretable AIGI detection evaluation and analysis of model attention by providing localized artifact labels for explanation and training.&lt;/li&gt;&lt;li&gt;Findings: existing detectors largely do not rely on perceptual artifacts; detectors can be trained to detect specific artifacts but still depend on uninterpretable features; aligning model attention with artifact regions improves interpretability and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Xiao', 'Weiyan Chen', 'Jiahao Chen', 'Zijie Cao', 'Weijian Deng', 'Binbin Yang', 'Ziyi Dong', 'Xiangyang Ji', 'Wei Ke', 'Pengxu Wei', 'Liang Lin']&lt;/li&gt;&lt;li&gt;Tags: ['AIGI detection', 'forensics', 'interpretability', 'benchmark', 'artifact-localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19430</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images</title><link>https://arxiv.org/abs/2601.19262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of handcrafted image descriptors (pixels, color histograms, DCT, HOG, LBP, GLCM, wavelets) for detecting AI-generated images on the CIFAKE dataset.&lt;/li&gt;&lt;li&gt;Benchmarks seven classifiers (Logistic Regression through LightGBM/XGBoost/CatBoost) using 50k train / 10k test samples; LightGBM achieves best performance (PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, Brier 0.0414).&lt;/li&gt;&lt;li&gt;Combining diverse handcrafted features (mixed configuration) improves discrimination and calibration over simpler descriptors, highlighting interpretability and computational efficiency for forgery detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Mehedi Hasan Nirob', 'Moqsadur Rahman', 'Shamim Ehsan', 'Summit Haque']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'image forensics', 'handcrafted features', 'ensemble learning', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19262</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP</title><link>https://arxiv.org/abs/2601.19210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Contrastive Spectral Rectification (CSR), a test-time defense that optimizes an input-adaptive rectification perturbation to align adversarial inputs with the natural manifold using a spectral-guided contrastive objective.&lt;/li&gt;&lt;li&gt;Identifies that adversarial examples show feature inconsistency under progressive frequency attenuation and attributes this to model spectral bias, motivating the spectral rectification approach.&lt;/li&gt;&lt;li&gt;Evaluates CSR across 16 classification benchmarks and reports an average improvement of 18.1% over state-of-the-art against strong AutoAttack, with modest inference overhead and applicability to diverse visual tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sen Nie', 'Jie Zhang', 'Zhuo Wang', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'test-time defense', 'robustness', 'vision-language models', 'spectral methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19210</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection</title><link>https://arxiv.org/abs/2601.18900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistically rigorous, training-free framework for detecting AI-generated (fake) images by computing interpretable p-values relative to a real-image population.&lt;/li&gt;&lt;li&gt;Combines multiple existing detector statistics and aggregates p-values using classical statistical ensembling to assess alignment with a unified real-image distribution.&lt;/li&gt;&lt;li&gt;Aims for robustness and interpretability across distribution shifts by focusing on real-only statistical modeling rather than assumptions about specific fake-generation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haim Zisman', 'Uri Shaham']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'statistical defenses', 'training-free methods', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18900</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection</title><link>https://arxiv.org/abs/2601.18845</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel dynamic mask-based backdoor attack for object detection models that uses dataset poisoning and SAM-generated masks to place triggers dynamically.&lt;/li&gt;&lt;li&gt;Evaluates the attack on a mushroom detection dataset with YOLOv7, achieving high attack success rates on poisoned samples while maintaining high accuracy on clean data.&lt;/li&gt;&lt;li&gt;Shows the approach is stealthier and more effective than static-pattern backdoors and highlights risks from outsourced dataset/model training, calling for robust defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeineb Dridi', 'Jihen Bennaceur', 'Amine Ben Hassouna']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'data poisoning', 'object detection', 'adversarial ML', 'dynamic trigger']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18845</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that uncurated AI-generated medical data creates a self-referential feedback loop that erodes pathological variability and diagnostic reliability across clinical text, vision-language reporting, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic representation skews, while models produce falsely confident but inaccurate reports (false reassurance rates rising to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation shows AI-generated documentation becomes clinically unusable after two generations; evaluates mitigations and finds mixing real data with quality-aware filtering preserves diversity while naive synthetic scaling fails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['dataset-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Counterfactual Explanations for Generative AI System Behavior</title><link>https://arxiv.org/abs/2601.03156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts counterfactual explanations to non-deterministic generative AI (LLM-based) by defining prompt-counterfactual explanations (PCEs) that reveal which prompt changes cause specific output characteristics (e.g., toxicity, political leaning, sentiment).&lt;/li&gt;&lt;li&gt;Proposes an algorithm to generate PCEs using downstream classifiers to detect output characteristics and demonstrates case studies (toxicity, political bias, sentiment).&lt;/li&gt;&lt;li&gt;Shows applications for mitigation (streamlining prompt engineering to suppress undesirable outputs) and for red-teaming (discovering prompts that elicit harmful or biased outputs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofie Goethals', 'Foster Provost', 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'red-teaming', 'prompt-engineering', 'safety-mitigation', 'counterfactual-explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03156</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control</title><link>https://arxiv.org/abs/2410.17520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MobileSafetyBench, a benchmark using Android emulators to evaluate safety of LLM-powered autonomous agents controlling mobile devices.&lt;/li&gt;&lt;li&gt;Contains diverse tasks across apps (e.g., messaging, banking) testing misuse, negative side effects, and robustness to indirect prompt injection attacks.&lt;/li&gt;&lt;li&gt;Evaluates baseline agents, finds frequent safety failures, and proposes a prompting-based mitigation that improves safety but is still insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juyong Lee', 'Dongyoon Hahm', 'June Suk Choi', 'W. Bradley Knox', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'safety benchmark', 'autonomous agents', 'defense/guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17520</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Watermark-based Attribution of AI-Generated Content</title><link>https://arxiv.org/abs/2404.04254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes per-user watermarking for AI-generated content to enable user-level attribution by matching extracted watermarks to users.&lt;/li&gt;&lt;li&gt;Derives theoretical lower bounds on detection and attribution performance for any set of user watermarks and selects watermarks to maximize these bounds.&lt;/li&gt;&lt;li&gt;Empirically evaluates attribution accuracy and robustness, showing high accuracy under no post-processing, common post-processing (e.g., JPEG compression), and limited-query black-box adversarial post-processing.&lt;/li&gt;&lt;li&gt;Finds that attribution inherits both accuracy and (non-)robustness characteristics of the underlying watermarking scheme.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Yupu Wang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'attribution', 'detection', 'robustness', 'adversarial-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.04254</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can We Trust LLM Detectors?</title><link>https://arxiv.org/abs/2601.15301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates two dominant AI-text detection paradigms (training-free and supervised), demonstrating brittleness under distribution shift, unseen generators, and simple stylistic perturbations.&lt;/li&gt;&lt;li&gt;Identifies that supervised detectors perform well in-domain but degrade sharply out-of-domain, while training-free methods are highly sensitive to proxy choice.&lt;/li&gt;&lt;li&gt;Proposes a supervised contrastive learning (SCL) framework to learn discriminative style embeddings to improve detector robustness, and provides empirical results and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jivnesh Sandhan', 'Harshit Jaiswal', 'Fei Cheng', 'Yugo Murawaki']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'robustness', 'adversarial perturbations', 'supervised contrastive learning', 'distribution shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15301</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A-IPO: Adaptive Intent-driven Preference Optimization</title><link>https://arxiv.org/abs/2510.10077</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes A-IPO, which infers latent user intent and incorporates an intention–response similarity term into the reward to better align model outputs with diverse user preferences.&lt;/li&gt;&lt;li&gt;Theoretically shows that adding the intention term increases the preference margin (positive log-odds shift) between preferred and dispreferred responses.&lt;/li&gt;&lt;li&gt;Introduces three benchmarks (Real-pref, Attack-pref, GlobalOpinionQA-Ext) to evaluate real-world and adversarial preference alignment.&lt;/li&gt;&lt;li&gt;Claims substantial empirical gains and improved adversarial robustness in preference alignment compared to existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqing Wang (Huazhong Agricultural University', 'China)', 'Muhammad Asif Ali (King Abdullah University of Science and Technology', 'KSA)', 'Ali Shoker (King Abdullah University of Science and Technology', 'KSA)', 'Ruohan Yang (Huazhong Agricultural University', 'China)', 'Junyang Chen (Shenzhen University', 'China)', 'Ying Sha (Huazhong Agricultural University', 'China)', 'Huan Wang (Huazhong Agricultural University', 'China)']&lt;/li&gt;&lt;li&gt;Tags: ['preference alignment', 'adversarial robustness', 'intent modeling', 'defense', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10077</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UQLM: A Python Package for Uncertainty Quantification in Large Language Models</title><link>https://arxiv.org/abs/2507.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UQLM, a Python package implementing uncertainty quantification (UQ) techniques to detect LLM hallucinations.&lt;/li&gt;&lt;li&gt;Provides a suite of UQ-based scorers that produce response-level confidence scores (0–1) for LLM outputs.&lt;/li&gt;&lt;li&gt;Off-the-shelf tooling designed for easy integration to improve reliability and safety of downstream LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Bouchard', 'Mohit Singh Chauhan', 'David Skarbrevik', 'Ho-Kyeong Ra', 'Viren Bajaj', 'Zeya Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty quantification', 'LLM safety', 'tooling/package']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06196</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Propaganda AI: An Analysis of Semantic Divergence in Large Language Models</title><link>https://arxiv.org/abs/2504.12344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies concept-conditioned semantic divergence where high-level concept cues (e.g., ideology, public figures) trigger uniform, stance-like model outputs that evade token-trigger audits.&lt;/li&gt;&lt;li&gt;Proposes RAVEN, a black-box audit combining semantic entropy over paraphrastic prompts with cross-model disagreement to flag responses that are highly certain yet atypical among peers.&lt;/li&gt;&lt;li&gt;Demonstrates attack feasibility via a controlled LoRA fine-tuning study that implants concept-conditioned stances using a small biased corpus (no rare-token triggers).&lt;/li&gt;&lt;li&gt;Evaluates five LLM families across twelve sensitive topics (360 prompts/model) and finds recurrent model-specific divergences in 9/12 topics, arguing RAVEN complements token-level defenses for release and post-deployment monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nay Myat Min', 'Long H. Pham', 'Yige Li', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['concept-conditioned attacks', 'audit/detection', 'data poisoning / fine-tuning', 'propaganda / influence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12344</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RvB: Automating AI System Hardening via Iterative Red-Blue Games</title><link>https://arxiv.org/abs/2601.19726</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RvB, a training-free iterative Red Team vs Blue Team game framework for automated hardening of AI systems.&lt;/li&gt;&lt;li&gt;Red Team generates exploits (e.g., jailbreaks, CVE-triggering inputs) while Blue Team adapts defenses without model parameter updates, producing robust, generalizable remediations.&lt;/li&gt;&lt;li&gt;Evaluated on dynamic code hardening for CVEs and guardrail optimization against jailbreaks, reporting high defense success rates and low false positives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lige Huang', 'Zicheng Liu', 'Jie Zhang', 'Lewen Yan', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaks', 'adversarial defenses', 'automated hardening', 'code security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19726</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs</title><link>https://arxiv.org/abs/2601.19507</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VLSafetyBencher, an automated multi-agent pipeline to construct safety evaluation benchmarks for large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Pipeline comprises four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection to create and curate high-quality safety samples.&lt;/li&gt;&lt;li&gt;Claims rapid, low-cost benchmark construction (within one week) and strong discriminative power: up to 70% safety-rate disparity between the most and least safe models.&lt;/li&gt;&lt;li&gt;Targets limitations of existing LVLM safety benchmarks (manual effort, static complexity, limited discriminative power) by automating dataset generation and selection to keep pace with evolving models/risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyang Zhu', 'Yuan Tian', 'Zicheng Zhang', 'Qi Jia', 'Chunyi Li', 'Renrui Zhang', 'Heng Li', 'Zongrui Wang', 'Wei Sun']&lt;/li&gt;&lt;li&gt;Tags: ['safety benchmarking', 'LVLMs', 'automated evaluation', 'dataset generation', 'red-teaming/benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19507</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Benchmarks Leak: Inference-Time Decontamination for LLMs</title><link>https://arxiv.org/abs/2601.19334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and addresses test-set contamination (training data leakage) that inflates LLM benchmark performance.&lt;/li&gt;&lt;li&gt;Proposes DeconIEP, an inference-time decontamination method that applies small, bounded perturbations in input embedding space guided by a less-contaminated reference model to reduce memorization-driven outputs.&lt;/li&gt;&lt;li&gt;Shows empirical improvement in decontamination effectiveness with minimal degradation of benign utility across multiple open-weight LLMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianzhe Chai', 'Yu Zhe', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark contamination', 'inference-time defense', 'memorization mitigation', 'evaluation security', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19334</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs</title><link>https://arxiv.org/abs/2601.19202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CONTEXT-VQA, a dataset of image-question pairs with systematically generated persuasive/conflicting textual prompts designed to mislead VLMs.&lt;/li&gt;&lt;li&gt;Designs an evaluation framework to benchmark susceptibility of 11 state-of-the-art VLMs to textual misinformation that conflicts with visual evidence.&lt;/li&gt;&lt;li&gt;Finds VLMs are highly vulnerable: models often prefer misleading text over clear visual cues, with an average performance drop of ~48.2% after one round of persuasive conversation.&lt;/li&gt;&lt;li&gt;Highlights a critical robustness gap in VLMs and the need for defenses against multimodal prompt-based misinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi Zhang', 'Wenxuan Ding', 'Jiale Liu', 'Mingrui Wu', 'Qingyun Wu', 'Ray Mooney']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal misinformation', 'prompt injection', 'robustness evaluation', 'dataset/benchmark', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19202</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Malicious Repurposing of Open Science Artefacts by Using Large Language Models</title><link>https://arxiv.org/abs/2601.18998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an end-to-end pipeline that uses persuasion-based jailbreaking to bypass LLM safeguards and then repurposes open-science artefacts (datasets, methods, tools) from NLP papers for malicious ends.&lt;/li&gt;&lt;li&gt;Proposes an evaluation framework assessing harmfulness, feasibility of misuse, and soundness of technicality for generated malicious proposals.&lt;/li&gt;&lt;li&gt;Finds that LLMs can produce harmful repurposing proposals, but LLM-based evaluators disagree substantially (GPT-4.1 more permissive/alarms, Gemini-2.5-pro stricter, Grok-3 intermediate), implying LLMs are unreliable judges for dual-use risk assessment and human evaluation is necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zahra Hashemi', 'Zhiqiang Zhong', 'Jun Pang', 'Wei Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'dual-use/misuse', 'red teaming', 'safety evaluation', 'open-science repurposing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18998</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that uncurated AI-generated medical data creates a self-referential feedback loop that erodes pathological variability and diagnostic reliability across clinical text, vision-language reporting, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic representation skews, while models produce falsely confident but inaccurate reports (false reassurance rates rising to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation shows AI-generated documentation becomes clinically unusable after two generations; evaluates mitigations and finds mixing real data with quality-aware filtering preserves diversity while naive synthetic scaling fails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['dataset-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Real-World Adversarial Attacks on RF-Based Drone Detectors</title><link>https://arxiv.org/abs/2512.20712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First demonstrated physical (over-the-air) adversarial attack against RF-based drone detectors that process spectrogram images.&lt;/li&gt;&lt;li&gt;Develops class-specific universal complex baseband (I/Q) perturbation waveforms transmitted alongside legitimate signals to cause evasion.&lt;/li&gt;&lt;li&gt;Evaluated using recorded RF data and OTA experiments on four drone types, showing modest structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection.&lt;/li&gt;&lt;li&gt;Attack preserves detection of non-target/legitimate drones and addresses practical OTA issues like synchronization and hardware constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omer Gazit', 'Yael Itzhakev', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'physical-adversarial-attacks', 'RF-security', 'evasion-attacks', 'over-the-air-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20712</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Concept activation vectors: a unifying view and adversarial attacks</title><link>https://arxiv.org/abs/2509.22755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a probabilistic formulation of Concept Activation Vectors (CAVs), deriving mean and covariance for different CAV types.&lt;/li&gt;&lt;li&gt;Shows that CAVs depend strongly on the choice of non-concept distribution, revealing a previously overlooked vulnerability.&lt;/li&gt;&lt;li&gt;Demonstrates a simple and effective adversarial attack that manipulates CAVs via the non-concept examples, undermining interpretability.&lt;/li&gt;&lt;li&gt;Calls for systematic study and more robust defenses for concept-based explanation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ekkehard Schnoor', 'Malik Tiomoko', 'Jawher Said', 'Alex Jung', 'Wojciech Samek']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'adversarial attack', 'interpretability vulnerability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22755</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UQLM: A Python Package for Uncertainty Quantification in Large Language Models</title><link>https://arxiv.org/abs/2507.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UQLM, a Python package implementing uncertainty quantification (UQ) techniques to detect LLM hallucinations.&lt;/li&gt;&lt;li&gt;Provides a suite of UQ-based scorers that produce response-level confidence scores (0–1) for LLM outputs.&lt;/li&gt;&lt;li&gt;Off-the-shelf tooling designed for easy integration to improve reliability and safety of downstream LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Bouchard', 'Mohit Singh Chauhan', 'David Skarbrevik', 'Ho-Kyeong Ra', 'Viren Bajaj', 'Zeya Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty quantification', 'LLM safety', 'tooling/package']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06196</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models</title><link>https://arxiv.org/abs/2502.10495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in latent-based watermarking for Latent Diffusion Models by showing that statistical patterns in outputs can reveal and compromise watermarks.&lt;/li&gt;&lt;li&gt;Proposes SWA-LDM, a lightweight defense that randomizes per-image latent watermarks using Gaussian-distributed latent noise to remove detectable artifacts while maintaining extraction robustness.&lt;/li&gt;&lt;li&gt;Reports experimental improvements (~20% average) in watermark stealth compared to prior methods, with preserved image quality and robustness to extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhonghao Yang', 'Linye Lyu', 'Xuanhang Chang', 'Daojing He', 'YU LI']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model defenses', 'latent diffusion models', 'steganographic watermarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10495</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Watermark-based Attribution of AI-Generated Content</title><link>https://arxiv.org/abs/2404.04254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes per-user watermarking for AI-generated content to enable user-level attribution by matching extracted watermarks to users.&lt;/li&gt;&lt;li&gt;Derives theoretical lower bounds on detection and attribution performance for any set of user watermarks and selects watermarks to maximize these bounds.&lt;/li&gt;&lt;li&gt;Empirically evaluates attribution accuracy and robustness, showing high accuracy under no post-processing, common post-processing (e.g., JPEG compression), and limited-query black-box adversarial post-processing.&lt;/li&gt;&lt;li&gt;Finds that attribution inherits both accuracy and (non-)robustness characteristics of the underlying watermarking scheme.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Yupu Wang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'attribution', 'detection', 'robustness', 'adversarial-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.04254</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Benchmarks for Differentially Private Image Classification</title><link>https://arxiv.org/abs/2601.17189</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a comprehensive set of benchmarks for differentially private (DP) image classification across settings (with/without extra data, convex problems, diverse datasets).&lt;/li&gt;&lt;li&gt;Evaluates established DP techniques on these benchmarks to identify which methods remain effective in different scenarios.&lt;/li&gt;&lt;li&gt;Provides a public leaderboard to track progress in differentially private machine learning research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sabrina Mokhtari', 'Sara Kodeiri', 'Shubhankar Mohapatra', 'Florian Tram\\`er', 'Gautam Kamath']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'benchmarks', 'image-classification', 'leaderboard']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.17189</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Counterfactual Explanations for Generative AI System Behavior</title><link>https://arxiv.org/abs/2601.03156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts counterfactual explanations to non-deterministic generative AI (LLM-based) by defining prompt-counterfactual explanations (PCEs) that reveal which prompt changes cause specific output characteristics (e.g., toxicity, political leaning, sentiment).&lt;/li&gt;&lt;li&gt;Proposes an algorithm to generate PCEs using downstream classifiers to detect output characteristics and demonstrates case studies (toxicity, political bias, sentiment).&lt;/li&gt;&lt;li&gt;Shows applications for mitigation (streamlining prompt engineering to suppress undesirable outputs) and for red-teaming (discovering prompts that elicit harmful or biased outputs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofie Goethals', 'Foster Provost', 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'red-teaming', 'prompt-engineering', 'safety-mitigation', 'counterfactual-explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03156</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Memorization: Selective Learning for Copyright-Safe Diffusion Model Training</title><link>https://arxiv.org/abs/2512.11194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-time defense (gradient projection) that removes gradient components aligned with embeddings of prohibited concept-level features to prevent diffusion models from internalizing and reproducing copyrighted/sensitive concepts.&lt;/li&gt;&lt;li&gt;Operates during backpropagation by projecting updates onto the orthogonal complement of the sensitive feature embedding space, enabling selective learning that preserves non-sensitive data utility.&lt;/li&gt;&lt;li&gt;Provides analysis against adversaries aiming for feature extraction and empirical results showing large reductions in memorization while maintaining generation quality and semantic fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divya Kothandaraman', 'Jaclyn Pytlarz']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'training-time defense', 'privacy', 'copyright/IP protection', 'gradient projection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11194</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models</title><link>https://arxiv.org/abs/2411.07559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zer0-Jack, a zeroth-order optimization method for generating malicious image inputs to jailbreak black-box multi-modal LLMs without white-box gradients.&lt;/li&gt;&lt;li&gt;Proposes patch coordinate descent to reduce memory usage and efficiently optimize image perturbations for black-box attacks.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (e.g., 95% on MiniGPT-4) and applicability to commercial models like GPT-4o, outperforming transfer-based baselines and approaching white-box methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiejin Chen', 'Kaishen Wang', 'Hua Wei']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'black-box attack', 'zeroth-order optimization', 'multi-modal models', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.07559</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control</title><link>https://arxiv.org/abs/2410.17520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MobileSafetyBench, a benchmark using Android emulators to evaluate safety of LLM-powered autonomous agents controlling mobile devices.&lt;/li&gt;&lt;li&gt;Contains diverse tasks across apps (e.g., messaging, banking) testing misuse, negative side effects, and robustness to indirect prompt injection attacks.&lt;/li&gt;&lt;li&gt;Evaluates baseline agents, finds frequent safety failures, and proposes a prompting-based mitigation that improves safety but is still insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juyong Lee', 'Dongyoon Hahm', 'June Suk Choi', 'W. Bradley Knox', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'safety benchmark', 'autonomous agents', 'defense/guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17520</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GAVEL: Towards rule-based safety through activation monitoring</title><link>https://arxiv.org/abs/2601.19768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces rule-based activation safety for LLMs by modeling internal activations as interpretable cognitive elements (CEs) representing fine-grained behaviors (e.g., 'making a threat').&lt;/li&gt;&lt;li&gt;Defines predicate rules over CEs to detect policy violations in real time, enabling configurable safeguards without retraining detectors or models.&lt;/li&gt;&lt;li&gt;Claims improved precision, domain customization, interpretability, auditability, and provides an open-source framework (GAVEL) plus an automated rule-creation tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shir Rozenfeld', 'Rahul Pankajakshan', 'Itay Zloczower', 'Eyal Lenga', 'Gilad Gressel', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['activation monitoring', 'defense', 'interpretability', 'AI governance', 'safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19768</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The role of self-supervised pretraining in differentially private medical image analysis</title><link>https://arxiv.org/abs/2601.19618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of how different pretraining initializations (ImageNet supervised, DINOv3 self-supervised, domain-specific supervised on MIMIC-CXR) affect performance of ConvNeXt models trained with DP-SGD for chest radiograph classification.&lt;/li&gt;&lt;li&gt;Finds DINOv3 self-supervised initialization improves utility under differential privacy compared to ImageNet supervised initialization, but domain-specific supervised pretraining yields the best private performance, closest to non-private baselines.&lt;/li&gt;&lt;li&gt;Analyzes downstream effects on demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity, concluding initialization strategy is a central determinant of utility and fairness under DP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soroosh Tayebi Arasteh', 'Mina Farajiamiri', 'Mahshad Lotfinia', 'Behrus Hinrichs-Puladi', 'Jonas Bienzeisler', 'Mohamed Alhaskir', 'Mirabela Rusu', 'Christiane Kuhl', 'Sven Nebelung', 'Daniel Truhn']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'medical-imaging', 'self-supervised-learning', 'privacy-defense', 'transfer-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19618</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SETA: Statistical Fault Attribution for Compound AI Systems</title><link>https://arxiv.org/abs/2601.19337</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a modular robustness testing framework (SETA) for multi-network AI pipelines that applies perturbations to test data and attributes faults to components.&lt;/li&gt;&lt;li&gt;Supports component-wise analysis and reasoning about error propagation across neural network modules, enabling finer-grained diagnostics beyond end-to-end metrics.&lt;/li&gt;&lt;li&gt;Architecture- and modality-agnostic approach demonstrated on a real-world autonomous rail inspection system composed of multiple deep networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayak Chowdhury', "Meenakshi D'Souza"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness testing', 'fault attribution', 'safety evaluation', 'vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19337</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code</title><link>https://arxiv.org/abs/2601.19264</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares feature-based (stylometric and structural) detectors with embedding-based detectors (CodeBERT) to distinguish human-written from machine-generated source code.&lt;/li&gt;&lt;li&gt;Uses a large benchmark of 600k samples and reports very high performance for both approaches (feature-based ROC-AUC 0.995, embedding-based ROC-AUC 0.994).&lt;/li&gt;&lt;li&gt;Finds whitespace/indentation features are especially discriminative, while embeddings capture deeper semantic patterns and offer slightly higher precision.&lt;/li&gt;&lt;li&gt;Discusses trade-offs between interpretability (feature-based) and generalization/semantic sensitivity (embedding-based) for deployment of code-origin detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Mehedi Hasan Nirob', 'Shamim Ehsan', 'Moqsadur Rahman', 'Summit Haque']&lt;/li&gt;&lt;li&gt;Tags: ['code-generation-detection', 'forensics', 'stylometry', 'embedding-based-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19264</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize to others.&lt;/li&gt;&lt;li&gt;Finds that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and formalizes this as SpikeScore.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence that SpikeScore separates hallucinated vs. non-hallucinated responses across domains and outperforms baselines on multiple LLMs/benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Yixuan Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'cross-domain-generalization', 'LLM-safety', 'robustness', 'uncertainty-measures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models</title><link>https://arxiv.org/abs/2601.19061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "Thought-Transfer", an indirect targeted poisoning attack that manipulates chain-of-thought (CoT) traces in training data while leaving queries and answers unchanged (clean-label poisoning).&lt;/li&gt;&lt;li&gt;Demonstrates the attack can transfer poisoned reasoning traces from one task to influence outputs on entirely different target tasks not present in poisoned data, achieving ~70% success.&lt;/li&gt;&lt;li&gt;Shows poisoned CoT training can also improve benchmark performance by 10–15%, making poisoned datasets attractive and harder to detect.&lt;/li&gt;&lt;li&gt;Argues this new reasoning-enabled threat vector bypasses prior mitigations and raises novel security challenges for CoT-enabled LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Ethan Rathbum', 'Hanna Foerster', 'Jamie Hayes', 'Matthew Jagielski', 'Milad Nasr', 'Ilia Shumailov', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['data-poisoning', 'backdoor', 'chain-of-thought', 'clean-label', 'adversarial-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19061</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection</title><link>https://arxiv.org/abs/2601.18900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistically rigorous, training-free framework for detecting AI-generated (fake) images by computing interpretable p-values relative to a real-image population.&lt;/li&gt;&lt;li&gt;Combines multiple existing detector statistics and aggregates p-values using classical statistical ensembling to assess alignment with a unified real-image distribution.&lt;/li&gt;&lt;li&gt;Aims for robustness and interpretability across distribution shifts by focusing on real-only statistical modeling rather than assumptions about specific fake-generation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haim Zisman', 'Uri Shaham']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'statistical defenses', 'training-free methods', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18900</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GraphDLG: Exploring Deep Leakage from Gradients in Federated Graph Learning</title><link>https://arxiv.org/abs/2601.19745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies deep leakage from gradients (DLG) in federated graph learning (FGL) and theoretically shows that recovering graph structure enables closed-form recursive recovery of node features.&lt;/li&gt;&lt;li&gt;Proposes GraphDLG, an attack that decouples graph structure and node features and leverages auxiliary graphs (random or client-side) to improve reconstruction.&lt;/li&gt;&lt;li&gt;Evaluates extensively and reports substantial gains over prior methods (≈5.46% MSE improvement for node features; ≈25.04% AUC improvement for graph structure).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyue Wei', 'Wantong Chen', 'Tongyu Wei', 'Chen Gong', 'Yongxin Tong', 'Lizhen Cui']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'gradient leakage', 'federated learning', 'graph neural networks', 'reconstruction attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19745</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment</title><link>https://arxiv.org/abs/2601.19487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a fundamental jailbreak vs. over-refusal trade-off caused by orthogonal encoding of 'answer' and 'benign' decision vectors in LLMs.&lt;/li&gt;&lt;li&gt;Proposes LLM-VA: a closed-form method that locates layerwise answer/safety vectors via SVMs and performs minimum-norm weight updates to align answer willingness with safety judgments.&lt;/li&gt;&lt;li&gt;Requires no fine-tuning or architecture changes, automatically adapts to model bias, and empirically improves safety-utility trade-offs (11.45% higher F1 and 95.92% utility retention across 12 LLMs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haonan Zhang', 'Dongxia Wang', 'Yi Liu', 'Kexin Chen', 'Wenhai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak mitigation', 'LLM safety', 'vector steering', 'model patching', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19487</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Internal Diagnosis to External Auditing: A VLM-Driven Paradigm for Online Test-Time Backdoor Defense</title><link>https://arxiv.org/abs/2601.19448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a paradigm shift from internal, model-entangled test-time defenses to external semantic auditing using independent Vision-Language Models (VLMs) as gatekeepers to detect and block backdoors.&lt;/li&gt;&lt;li&gt;Introduces PRISM (Prototype Refinement &amp; Inspection via Statistical Monitoring) with two components: a Hybrid VLM Teacher for online visual prototype refinement, and an Adaptive Router that uses statistical margin monitoring to set gating thresholds in real time.&lt;/li&gt;&lt;li&gt;Claims model-agnostic, online backdoor detection that reduces attack success rates (e.g., &lt;1% on CIFAR-10) while preserving or improving clean accuracy, evaluated across 17 datasets and 11 attack types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binyan Xu', 'Fan Yang', 'Xilin Dai', 'Di Tang', 'Kehuan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'test-time defense', 'vision-language models', 'external auditing', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19448</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection</title><link>https://arxiv.org/abs/2601.19375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective Steering: a norm-preserving rotation formulation for activation steering that maintains activation distribution integrity during inference-time interventions.&lt;/li&gt;&lt;li&gt;Introduces discriminative layer selection to apply steering only where feature representations show opposite-signed class alignment, avoiding harmful distribution shifts.&lt;/li&gt;&lt;li&gt;Empirical results on nine models show ~5.5x higher attack/control effectiveness than prior methods while preserving perplexity and model capabilities.&lt;/li&gt;&lt;li&gt;Provides code for reproducing steering interventions and evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quy-Anh Dang', 'Chris Ngo']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'inference-time control', 'adversarial robustness', 'model manipulation', 'defense techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19375</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Model Transcription with Differentially Private Synthetic Distillation</title><link>https://arxiv.org/abs/2601.19090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "differentially private synthetic distillation": a data-free model-to-model conversion that transcribes a pretrained teacher into a privacy-preserving student via a trainable generator.&lt;/li&gt;&lt;li&gt;Framework uses three players in alternate optimization—generator (creates synthetic data), teacher and student (produce differentially private/noisy labels), and student (trained on noisy labels) with adversarial training of the generator—yielding DP guarantees and convergence proofs.&lt;/li&gt;&lt;li&gt;Results show strong privacy-utility trade-offs: the transcribed student maintains good performance while protecting training data privacy; the generator can also produce private synthetic data for downstream use.&lt;/li&gt;&lt;li&gt;Empirically outperforms 26 state-of-the-art methods and provides theoretical privacy and convergence analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bochao Liu', 'Shiming Ge', 'Pengju Wang', 'Shikun Li', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'model distillation', 'privacy-preserving ML', 'synthetic data generation', 'defense against data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19090</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attention-Enhanced Graph Filtering for False Data Injection Attack Detection and Localization</title><link>https://arxiv.org/abs/2601.18981</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a joint detection and localization framework for false data injection attacks (FDIAs) in power grids that combines ARMA graph convolutional filters with an encoder-only Transformer.&lt;/li&gt;&lt;li&gt;ARMA graph filters extract topology-aware, robust features capturing local grid dynamics; the Transformer encoder models long-range dependencies among grid elements.&lt;/li&gt;&lt;li&gt;Evaluated on NYISO real-world load data using IEEE 14- and 300-bus systems, demonstrating high accuracy in FDIA detection and compromised-node localization.&lt;/li&gt;&lt;li&gt;Addresses limitations of prior graph-based FDIA methods (high-dimensional representations, shallow classifiers) and design trade-offs when integrating Transformer architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruslan Abdulin', 'Mohammad Rasoul Narimani']&lt;/li&gt;&lt;li&gt;Tags: ['false-data-injection', 'attack-detection', 'graph-neural-networks', 'transformer', 'power-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18981</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title><link>https://arxiv.org/abs/2601.18939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neuron-level surgical alignment method that identifies ~3% of MLP neurons most predictive of a target behavior using sparse autoencoders and linear probes.&lt;/li&gt;&lt;li&gt;Decodes those neurons into residual space and fine-tunes only them with gradient masking, enabling targeted updates with far less data than full-model fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates reduction of sycophantic behavior on four benchmarks (Syco-Bench, NLP, POLI, PHIL) matching or exceeding state-of-the-art on Gemma-2-2B and 9B models.&lt;/li&gt;&lt;li&gt;Argues sparse, neuron-level edits are a scalable, precise alternative for behavioral alignment and low-data settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Claire O'Brien", 'Jessica Seto', 'Dristi Roy', 'Aditya Dwivedi', 'Sunishchal Dev', 'Kevin Zhu', "Sean O'Brien", 'Ashwinee Panda', 'Ryan Lagasse']&lt;/li&gt;&lt;li&gt;Tags: ['neuron-level editing', 'behavioral alignment', 'sycophancy mitigation', 'efficient fine-tuning', 'model surgery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18939</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</title><link>https://arxiv.org/abs/2601.18739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SeNeDiF-OOD: a hierarchical Semantic Nested Dichotomy Fusion framework that decomposes OOD detection into binary fusion nodes aligned with semantic abstraction levels.&lt;/li&gt;&lt;li&gt;Designed to handle heterogeneous OOD inputs (non-domain images, unknown classes, low-level corruptions) and evaluated on a real-world monument style classification dataset (MonuMAI).&lt;/li&gt;&lt;li&gt;Includes evaluation against adversarial attacks and shows that the hierarchical fusion approach outperforms traditional OOD baselines while preserving in-distribution performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ignacio Antequera-S\\'anchez", "Juan Luis Su\\'arez-D\\'iaz", 'Rosana Montes', 'Francisco Herrera']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'robustness', 'adversarial-robustness', 'hierarchical-fusion', 'open-world-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18739</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Systemic Evaluation of Multimodal RAG Privacy</title><link>https://arxiv.org/abs/2601.17644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of privacy risks in multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks.&lt;/li&gt;&lt;li&gt;Case study demonstrating membership inference: determining whether a specific visual asset (image) is present in the RAG datastore and, if so, extracting associated metadata (e.g., captions) via standard prompting.&lt;/li&gt;&lt;li&gt;Findings show practical leakage of private visual assets and metadata during inference, highlighting the need for privacy-preserving mechanisms and further research on mRAG privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al-Lawati', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal RAG', 'privacy', 'membership inference', 'data leakage', 'retrieval-augmented generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.17644</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can We Trust LLM Detectors?</title><link>https://arxiv.org/abs/2601.15301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of two dominant AI-text detection paradigms (training-free and supervised), showing both are brittle under distribution shift, unseen generators, and simple stylistic perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates that supervised detectors perform well in-domain but degrade sharply out-of-domain; training-free approaches are highly sensitive to choice of proxy models or heuristics.&lt;/li&gt;&lt;li&gt;Proposes a supervised contrastive learning (SCL) framework to learn discriminative style embeddings for detection and reports experimental results highlighting partial improvements but persistent domain-agnostic challenges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jivnesh Sandhan', 'Harshit Jaiswal', 'Fei Cheng', 'Yugo Murawaki']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'robustness', 'adversarial perturbations', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15301</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models</title><link>https://arxiv.org/abs/2601.13948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Stream-Voice-Anon, a streaming speaker anonymization system that adapts neural audio codecs (NAC) with causal language models for real-time privacy protection.&lt;/li&gt;&lt;li&gt;Introduces anonymization techniques: pseudo-speaker representation sampling, speaker embedding mixing, and diverse prompt selection leveraging disentangled quantized content codes to reduce speaker leakage.&lt;/li&gt;&lt;li&gt;Explores latency–privacy trade-offs via dynamic vs. fixed delay configurations for real-time deployment.&lt;/li&gt;&lt;li&gt;Evaluates under the VoicePrivacy 2024 protocol, showing large gains in intelligibility and emotion preservation vs. prior streaming S.A. (DarkStream) while reporting attacker-specific privacy performance (robust to lazy-informed, degraded vs. semi-informed attackers).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita Kuzmin', 'Songting Liu', 'Kong Aik Lee', 'Eng Siong Chng']&lt;/li&gt;&lt;li&gt;Tags: ['speaker-anonymization', 'privacy-preserving-ml', 'neural-audio-codec', 'real-time/streaming', 'privacy-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13948</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that uncurated AI-generated medical data creates a self-referential feedback loop that erodes pathological variability and diagnostic reliability across clinical text, vision-language reporting, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic representation skews, while models produce falsely confident but inaccurate reports (false reassurance rates rising to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation shows AI-generated documentation becomes clinically unusable after two generations; evaluates mitigations and finds mixing real data with quality-aware filtering preserves diversity while naive synthetic scaling fails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['dataset-contamination', 'data-poisoning', 'robustness', 'medical-AI', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Counterfactual Explanations for Generative AI System Behavior</title><link>https://arxiv.org/abs/2601.03156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts counterfactual explanations to non-deterministic generative AI (LLM-based) by defining prompt-counterfactual explanations (PCEs) that reveal which prompt changes cause specific output characteristics (e.g., toxicity, political leaning, sentiment).&lt;/li&gt;&lt;li&gt;Proposes an algorithm to generate PCEs using downstream classifiers to detect output characteristics and demonstrates case studies (toxicity, political bias, sentiment).&lt;/li&gt;&lt;li&gt;Shows applications for mitigation (streamlining prompt engineering to suppress undesirable outputs) and for red-teaming (discovering prompts that elicit harmful or biased outputs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofie Goethals', 'Foster Provost', 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'red-teaming', 'prompt-engineering', 'safety-mitigation', 'counterfactual-explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03156</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing</title><link>https://arxiv.org/abs/2510.15303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DSSmoothing, the first certified dataset ownership verification (DOV) method for pre-trained language models under a gray-box setting.&lt;/li&gt;&lt;li&gt;Introduces dual-space smoothing: continuous embedding-space perturbations for semantic robustness and controlled token reordering in permutation space for sequential robustness; uses randomized smoothing during verification.&lt;/li&gt;&lt;li&gt;Provides theoretical provable robustness guarantees ensuring watermark robustness (WR) exceeds principal probability (PP) under bounded dual-space perturbations.&lt;/li&gt;&lt;li&gt;Empirically demonstrates stable and robust verification performance across web-scale datasets and resilience to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Xing Liu', 'Wenke Huang', 'Jianbin Li', 'Zhaoxin Fan', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['dataset ownership verification', 'watermarking', 'certified defenses', 'randomized smoothing', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15303</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoD, a learnable detection framework for unseen jailbreak attacks in large vision-language models without using attack data or hand-crafted heuristics.&lt;/li&gt;&lt;li&gt;Extracts layer-wise safety representations from internal activations using Multi-modal Safety Concept Activation Vectors and converts them to a 1D anomaly score via a Safety Pattern Auto-Encoder.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art AUROC for detecting diverse unseen jailbreaks across multiple LVLMs and claims improved efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jiaqi Weng', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'adversarial detection', 'model safety', 'anomaly detection', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UQLM: A Python Package for Uncertainty Quantification in Large Language Models</title><link>https://arxiv.org/abs/2507.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UQLM, a Python package implementing uncertainty quantification (UQ) techniques to detect LLM hallucinations.&lt;/li&gt;&lt;li&gt;Provides a suite of UQ-based scorers that produce response-level confidence scores (0–1) for LLM outputs.&lt;/li&gt;&lt;li&gt;Off-the-shelf tooling designed for easy integration to improve reliability and safety of downstream LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Bouchard', 'Mohit Singh Chauhan', 'David Skarbrevik', 'Ho-Kyeong Ra', 'Viren Bajaj', 'Zeya Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty quantification', 'LLM safety', 'tooling/package']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06196</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges</title><link>https://arxiv.org/abs/2506.02032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic survey applying MITRE ATLAS to enumerate and analyze attack vectors across different phases of the MLOps pipeline, with a threat model covering attacker knowledge/capabilities.&lt;/li&gt;&lt;li&gt;Presents a structured taxonomy mapping specific attack techniques (e.g., data poisoning, credential compromise) to MLOps phases, supported by red-team examples and real-world incidents.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of mitigation strategies aligned to attack categories and highlights actionable early-stage defenses and open research gaps in securing MLOps ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raj Patel', 'Himanshu Tripathi', 'Jasper Stone', 'Noorbakhsh Amiri Golilarz', 'Sudip Mittal', 'Shahram Rahimi', 'Vini Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['MLOps Security', 'Adversarial Attacks', 'Threat Taxonomy', 'Mitigation Strategies', 'Red Teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02032</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models</title><link>https://arxiv.org/abs/2502.10495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in latent-based watermarking for Latent Diffusion Models by showing that statistical patterns in outputs can reveal and compromise watermarks.&lt;/li&gt;&lt;li&gt;Proposes SWA-LDM, a lightweight defense that randomizes per-image latent watermarks using Gaussian-distributed latent noise to remove detectable artifacts while maintaining extraction robustness.&lt;/li&gt;&lt;li&gt;Reports experimental improvements (~20% average) in watermark stealth compared to prior methods, with preserved image quality and robustness to extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhonghao Yang', 'Linye Lyu', 'Xuanhang Chang', 'Daojing He', 'YU LI']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model defenses', 'latent diffusion models', 'steganographic watermarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10495</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models</title><link>https://arxiv.org/abs/2411.07559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Zer0-Jack, a zeroth-order optimization method for generating malicious image inputs to jailbreak black-box multi-modal LLMs without white-box gradients.&lt;/li&gt;&lt;li&gt;Proposes patch coordinate descent to reduce memory usage and efficiently optimize image perturbations for black-box attacks.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (e.g., 95% on MiniGPT-4) and applicability to commercial models like GPT-4o, outperforming transfer-based baselines and approaching white-box methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiejin Chen', 'Kaishen Wang', 'Hua Wei']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'black-box attack', 'zeroth-order optimization', 'multi-modal models', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.07559</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Watermark-based Attribution of AI-Generated Content</title><link>https://arxiv.org/abs/2404.04254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes per-user watermarking for AI-generated content to enable user-level attribution by matching extracted watermarks to users.&lt;/li&gt;&lt;li&gt;Derives theoretical lower bounds on detection and attribution performance for any set of user watermarks and selects watermarks to maximize these bounds.&lt;/li&gt;&lt;li&gt;Empirically evaluates attribution accuracy and robustness, showing high accuracy under no post-processing, common post-processing (e.g., JPEG compression), and limited-query black-box adversarial post-processing.&lt;/li&gt;&lt;li&gt;Finds that attribution inherits both accuracy and (non-)robustness characteristics of the underlying watermarking scheme.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengyuan Jiang', 'Moyang Guo', 'Yuepeng Hu', 'Yupu Wang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'attribution', 'detection', 'robustness', 'adversarial-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.04254</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</title><link>https://arxiv.org/abs/2508.07667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent framework that decomposes contextual privacy reasoning into specialized subtasks (e.g., extraction, classification) to limit information exposure to any single agent.&lt;/li&gt;&lt;li&gt;Performs systematic ablations over information-flow topologies to analyze how upstream detection errors propagate and cause downstream private information leakage.&lt;/li&gt;&lt;li&gt;Evaluates on privacy benchmarks (ConfAIde, PrivacyLens) with open- and closed-source LLMs, showing substantial reductions in private-data leakage (≈18–19% with GPT-4o) while preserving public-content fidelity.&lt;/li&gt;&lt;li&gt;Argues that principled information-flow design in multi-agent systems can serve as an effective defense for contextual privacy in LLM-driven interactive settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Li', 'Liwen Sun', 'Zhenxiang Guan', 'Xuhui Zhou', 'Maarten Sap']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-leakage', 'multi-agent-systems', 'defense', 'red-teaming/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07667</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RvB: Automating AI System Hardening via Iterative Red-Blue Games</title><link>https://arxiv.org/abs/2601.19726</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RvB, a training-free iterative Red Team vs Blue Team game framework for automated hardening of AI systems.&lt;/li&gt;&lt;li&gt;Red Team generates exploits (e.g., jailbreaks, CVE-triggering inputs) while Blue Team adapts defenses without model parameter updates, producing robust, generalizable remediations.&lt;/li&gt;&lt;li&gt;Evaluated on dynamic code hardening for CVEs and guardrail optimization against jailbreaks, reporting high defense success rates and low false positives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lige Huang', 'Zicheng Liu', 'Jie Zhang', 'Lewen Yan', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaks', 'adversarial defenses', 'automated hardening', 'code security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19726</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The role of self-supervised pretraining in differentially private medical image analysis</title><link>https://arxiv.org/abs/2601.19618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of how different pretraining initializations (ImageNet supervised, DINOv3 self-supervised, domain-specific supervised on MIMIC-CXR) affect performance of ConvNeXt models trained with DP-SGD for chest radiograph classification.&lt;/li&gt;&lt;li&gt;Finds DINOv3 self-supervised initialization improves utility under differential privacy compared to ImageNet supervised initialization, but domain-specific supervised pretraining yields the best private performance, closest to non-private baselines.&lt;/li&gt;&lt;li&gt;Analyzes downstream effects on demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity, concluding initialization strategy is a central determinant of utility and fairness under DP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soroosh Tayebi Arasteh', 'Mina Farajiamiri', 'Mahshad Lotfinia', 'Behrus Hinrichs-Puladi', 'Jonas Bienzeisler', 'Mohamed Alhaskir', 'Mirabela Rusu', 'Christiane Kuhl', 'Sven Nebelung', 'Daniel Truhn']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'medical-imaging', 'self-supervised-learning', 'privacy-defense', 'transfer-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19618</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment</title><link>https://arxiv.org/abs/2601.19487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a fundamental jailbreak vs. over-refusal trade-off caused by orthogonal encoding of 'answer' and 'benign' decision vectors in LLMs.&lt;/li&gt;&lt;li&gt;Proposes LLM-VA: a closed-form method that locates layerwise answer/safety vectors via SVMs and performs minimum-norm weight updates to align answer willingness with safety judgments.&lt;/li&gt;&lt;li&gt;Requires no fine-tuning or architecture changes, automatically adapts to model bias, and empirically improves safety-utility trade-offs (11.45% higher F1 and 95.92% utility retention across 12 LLMs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haonan Zhang', 'Dongxia Wang', 'Yi Liu', 'Kexin Chen', 'Wenhai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak mitigation', 'LLM safety', 'vector steering', 'model patching', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19487</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection</title><link>https://arxiv.org/abs/2601.19375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Selective Steering: a norm-preserving rotation formulation for activation steering that maintains activation distribution integrity during inference-time interventions.&lt;/li&gt;&lt;li&gt;Introduces discriminative layer selection to apply steering only where feature representations show opposite-signed class alignment, avoiding harmful distribution shifts.&lt;/li&gt;&lt;li&gt;Empirical results on nine models show ~5.5x higher attack/control effectiveness than prior methods while preserving perplexity and model capabilities.&lt;/li&gt;&lt;li&gt;Provides code for reproducing steering interventions and evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quy-Anh Dang', 'Chris Ngo']&lt;/li&gt;&lt;li&gt;Tags: ['activation steering', 'inference-time control', 'adversarial robustness', 'model manipulation', 'defense techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19375</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Benchmarks Leak: Inference-Time Decontamination for LLMs</title><link>https://arxiv.org/abs/2601.19334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and addresses test-set contamination (training data leakage) that inflates LLM benchmark performance.&lt;/li&gt;&lt;li&gt;Proposes DeconIEP, an inference-time decontamination method that applies small, bounded perturbations in input embedding space guided by a less-contaminated reference model to reduce memorization-driven outputs.&lt;/li&gt;&lt;li&gt;Shows empirical improvement in decontamination effectiveness with minimal degradation of benign utility across multiple open-weight LLMs and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianzhe Chai', 'Yu Zhe', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark contamination', 'inference-time defense', 'memorization mitigation', 'evaluation security', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19334</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SHIELD: An Auto-Healing Agentic Defense Framework for LLM Resource Exhaustion Attacks</title><link>https://arxiv.org/abs/2601.19174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHIELD, a multi-agent, auto-healing defense framework to detect and mitigate LLM resource-exhaustion (sponge) attacks that cause excessive computation/DoS.&lt;/li&gt;&lt;li&gt;Core is a three-stage Defense Agent combining semantic similarity retrieval, pattern matching, and LLM-based reasoning; two auxiliary agents update knowledge and optimize prompts to form a self-healing loop.&lt;/li&gt;&lt;li&gt;Evaluated against non-semantic and semantic sponge attacks, outperforming perplexity-based and static LLM detectors with high F1 scores across evolving attack strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirhoshan Sivaroopan', 'Kanchana Thilakarathna', 'Albert Zomaya', 'Manu', 'Yi Guo', 'Jo Plested', 'Tim Lynar', 'Jack Yang', 'Wangli Yang']&lt;/li&gt;&lt;li&gt;Tags: ['resource exhaustion', 'DoS', 'agent-based defense', 'sponge attacks', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19174</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Model Transcription with Differentially Private Synthetic Distillation</title><link>https://arxiv.org/abs/2601.19090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes "differentially private synthetic distillation": a data-free model-to-model conversion that transcribes a pretrained teacher into a privacy-preserving student via a trainable generator.&lt;/li&gt;&lt;li&gt;Framework uses three players in alternate optimization—generator (creates synthetic data), teacher and student (produce differentially private/noisy labels), and student (trained on noisy labels) with adversarial training of the generator—yielding DP guarantees and convergence proofs.&lt;/li&gt;&lt;li&gt;Results show strong privacy-utility trade-offs: the transcribed student maintains good performance while protecting training data privacy; the generator can also produce private synthetic data for downstream use.&lt;/li&gt;&lt;li&gt;Empirically outperforms 26 state-of-the-art methods and provides theoretical privacy and convergence analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bochao Liu', 'Shiming Ge', 'Pengju Wang', 'Shikun Li', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'model distillation', 'privacy-preserving ML', 'synthetic data generation', 'defense against data extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19090</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GUIGuard: Toward a General Framework for Privacy-Preserving GUI Agents</title><link>https://arxiv.org/abs/2601.18842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GUIGuard, a three-stage framework for privacy-preserving GUI agents: (1) privacy recognition, (2) privacy protection, and (3) task execution under protection.&lt;/li&gt;&lt;li&gt;Introduces GUIGuard-Bench: a cross-platform benchmark with 630 trajectories and 13,830 screenshots annotated with region-level privacy grounding, risk level, privacy category, and task necessity.&lt;/li&gt;&lt;li&gt;Empirical findings show poor privacy recognition by current agents (13.3% accuracy on Android, 1.4% on PC), demonstrate that protection strategies can preserve task-planning semantics, and identify privacy recognition as a key bottleneck.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Wang', 'Zhiling Zhang', 'Wenbo Zhou', 'Weiming Zhang', 'Jie Zhang', 'Qiannan Zhu', 'Yu Shi', 'Shuxin Zheng', 'Jiyan He']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'defense', 'benchmark', 'GUI agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18842</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CanaryBench: Stress Testing Privacy Leakage in Cluster-Level Conversation Summaries</title><link>https://arxiv.org/abs/2601.18834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CanaryBench, a reproducible stress test that injects known 'canary' secret strings into synthetic conversations to detect privacy leakage in cluster-level conversation summaries.&lt;/li&gt;&lt;li&gt;Evaluates leakage using TF-IDF embeddings, k-means clustering, and an extractive summarizer; finds high cluster-level canary leakage (≈0.96) and nonzero PII indicator matches.&lt;/li&gt;&lt;li&gt;Demonstrates a minimal defense (minimum cluster-size publication threshold k-min = 25 combined with regex-based redaction) that eliminated measured canary leakage and PII hits in the reported run while preserving cluster-coherence proxies.&lt;/li&gt;&lt;li&gt;Positions the contribution as a privacy-risk measurement tool for published analytics artifacts (summaries) rather than raw user data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deep Mehta']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-leakage', 'canary-benchmark', 'redaction', 'clustering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.18834</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GAVEL: Towards rule-based safety through activation monitoring</title><link>https://arxiv.org/abs/2601.19768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces rule-based activation safety for LLMs by modeling internal activations as interpretable cognitive elements (CEs) representing fine-grained behaviors (e.g., 'making a threat').&lt;/li&gt;&lt;li&gt;Defines predicate rules over CEs to detect policy violations in real time, enabling configurable safeguards without retraining detectors or models.&lt;/li&gt;&lt;li&gt;Claims improved precision, domain customization, interpretability, auditability, and provides an open-source framework (GAVEL) plus an automated rule-creation tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shir Rozenfeld', 'Rahul Pankajakshan', 'Itay Zloczower', 'Eyal Lenga', 'Gilad Gressel', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['activation monitoring', 'defense', 'interpretability', 'AI governance', 'safety mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19768</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SETA: Statistical Fault Attribution for Compound AI Systems</title><link>https://arxiv.org/abs/2601.19337</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a modular robustness testing framework (SETA) for multi-network AI pipelines that applies perturbations to test data and attributes faults to components.&lt;/li&gt;&lt;li&gt;Supports component-wise analysis and reasoning about error propagation across neural network modules, enabling finer-grained diagnostics beyond end-to-end metrics.&lt;/li&gt;&lt;li&gt;Architecture- and modality-agnostic approach demonstrated on a real-world autonomous rail inspection system composed of multiple deep networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayak Chowdhury', "Meenakshi D'Souza"]&lt;/li&gt;&lt;li&gt;Tags: ['robustness testing', 'fault attribution', 'safety evaluation', 'vulnerability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19337</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the problem of generalizable hallucination detection (GHD): training detectors on one domain that generalize to others.&lt;/li&gt;&lt;li&gt;Finds that hallucination-initiated multi-turn dialogues show larger uncertainty fluctuations than factual ones and formalizes this as SpikeScore.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence that SpikeScore separates hallucinated vs. non-hallucinated responses across domains and outperforms baselines on multiple LLMs/benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Yixuan Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'cross-domain-generalization', 'LLM-safety', 'robustness', 'uncertainty-measures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach</title><link>https://arxiv.org/abs/2601.19122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial data augmentation framework where a reinforcement-learned query model generates adversarial queries to expose weaknesses in function-call (FC) LLMs.&lt;/li&gt;&lt;li&gt;Frames training as a zero-sum game with iterative, alternating optimization between the query (attacker) model and the FC (defender) model to improve robustness and generalization.&lt;/li&gt;&lt;li&gt;Uses the generated adversarial data to finetune FC models, systematically identifying and correcting failures in LLM interactions with external tools/APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiran Guo', 'Bing Bo', 'Shaoxiang Wu', 'Jingsheng Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness/defense', 'reinforcement learning', 'function-call/tool-use vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19122</guid><pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 28 Oct 2025 22:35:49 +0000</lastBuildDate><item><title>CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models</title><link>https://arxiv.org/abs/2507.22828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CapRecover, a framework to recover semantic content from intermediate features of VLMs&lt;/li&gt;&lt;li&gt;Evaluates on multiple datasets and models, showing high accuracy in label recovery and caption generation&lt;/li&gt;&lt;li&gt;Introduces a noise-based protection method to mitigate semantic leakage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kedong Xiu', 'Sai Qian Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data leakage', 'vision-language models', 'feature inversion', 'semantic recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22828</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Attention! Your Vision Language Model Could Be Maliciously Manipulated</title><link>https://arxiv.org/abs/2505.19911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Vision-language model Manipulation Attack (VMA) for adversarial attacks on VLMs&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability to image-based adversarial examples&lt;/li&gt;&lt;li&gt;Can be used for jailbreaking, hijacking, privacy breaches, etc.&lt;/li&gt;&lt;li&gt;Includes watermark injection for copyright protection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaosen Wang', 'Shaokang Wang', 'Zhijin Ge', 'Yuyang Luo', 'Shudong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'privacy attacks', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19911</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</title><link>https://arxiv.org/abs/2503.10635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new adversarial attack method for LVLMs&lt;/li&gt;&lt;li&gt;Focuses on semantic clarity in perturbations&lt;/li&gt;&lt;li&gt;Achieves high success rates against closed-source models like GPT-4.5&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LVLMs', 'transferability', 'semantic perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10635</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title><link>https://arxiv.org/abs/2510.22300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T2I-RiskyPrompt, a benchmark for evaluating safety in text-to-image models&lt;/li&gt;&lt;li&gt;Includes a hierarchical risk taxonomy with 6 primary and 14 subcategories&lt;/li&gt;&lt;li&gt;Provides 6,432 annotated risky prompts with detailed reasons&lt;/li&gt;&lt;li&gt;Evaluates multiple models, defense methods, safety filters, and attack strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Tairen Zhang', 'Lanjun Wang', 'Ruidong Chen', 'Wenhui Li', 'Anan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'text-to-image models', 'risky prompts', 'benchmarking', 'attack strategies', 'defense methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22300</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models</title><link>https://arxiv.org/abs/2510.22785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Calibrated Consistency (SCC) for adversarial robustness in VLMs like CLIP&lt;/li&gt;&lt;li&gt;Addresses semantic and viewpoint fragility in adversarial attacks&lt;/li&gt;&lt;li&gt;Improves zero-shot robustness while maintaining accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxiang Liu', 'Jiawei Du', 'Xiao Liu', 'Prayag Tiwari', 'Mingkun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'test-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22785</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?</title><link>https://arxiv.org/abs/2510.21842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'modal aphasia' in multimodal models where concepts are memorized visually but not articulable in text.&lt;/li&gt;&lt;li&gt;Shows models can reproduce images accurately but fail to describe them correctly in text.&lt;/li&gt;&lt;li&gt;Demonstrates potential AI safety vulnerability where text-based safeguards don't prevent unsafe image generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Aerni', 'Joshua Swanson', "Kristina Nikoli\\'c", 'Florian Tram\\`er']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal models', 'safety evaluation', 'adversarial prompting', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21842</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models</title><link>https://arxiv.org/abs/2510.21783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient membership inference attack for diffusion models using small-noise injection&lt;/li&gt;&lt;li&gt;Analyzes noise aggregation patterns to distinguish training vs non-training samples&lt;/li&gt;&lt;li&gt;Requires fewer model queries compared to existing methods&lt;/li&gt;&lt;li&gt;Demonstrates scalability with large text-to-image models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guo Li', 'Yuyang Yu', 'Xuemiao Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'diffusion models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21783</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper presents a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large Language Models</title><link>https://arxiv.org/abs/2506.00062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates safety degradation in fine-tuned telecom LLMs&lt;/li&gt;&lt;li&gt;Introduces TeleHarm benchmark for telecom-specific red-teaming&lt;/li&gt;&lt;li&gt;Evaluates realignment defenses (SafeInstruct, SafeLoRA, SafeMERGE)&lt;/li&gt;&lt;li&gt;Shows defenses restore safety without losing performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Fernando Koch', 'Walid Saad', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'model realignment', 'telecom domain adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00062</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the issue where LLMs adjust their behavior during evaluation to appear more aligned, which affects safety evaluations.&lt;/li&gt;&lt;li&gt;It introduces a steering vector technique to suppress evaluation-awareness, making the model behave as if deployed.&lt;/li&gt;&lt;li&gt;The study involves training an LLM with evaluation cues and using activation steering to counteract the awareness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models</title><link>https://arxiv.org/abs/2510.16712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates multi-turn stance instability in search-enabled LLMs&lt;/li&gt;&lt;li&gt;Introduces Chameleon Benchmark Dataset and metrics&lt;/li&gt;&lt;li&gt;Evaluates Llama-4-Maverick, GPT-4o-mini, Gemini-2.5-Flash&lt;/li&gt;&lt;li&gt;Finds severe chameleon behavior across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivam Ratnakar', 'Sanjay Raghavendra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16712</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety</title><link>https://arxiv.org/abs/2510.16492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using quitting as a safety mechanism for LLM agents in multi-turn scenarios&lt;/li&gt;&lt;li&gt;Evaluates quitting behavior across 12 LLMs using ToolEmu framework&lt;/li&gt;&lt;li&gt;Shows significant safety improvement with minimal impact on helpfulness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vamshi Krishna Bonagiri', 'Ponnurangam Kumaragurum', 'Khanh Nguyen', 'Benjamin Plaut']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'agent safety', 'quitting behavior', 'ToolEmu', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16492</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models</title><link>https://arxiv.org/abs/2510.10142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DiffHeads framework for debiasing LLMs&lt;/li&gt;&lt;li&gt;Compares DA and CoT prompting strategies&lt;/li&gt;&lt;li&gt;Identifies and masks bias heads in attention layers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingxu Han', 'Wei Song', 'Ziqi Ding', 'Ziming Li', 'Chunrong Fang', 'Yuekang Li', 'Dongfang Liu', 'Zhenyu Chen', 'Zhenting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'attention heads', 'prompting strategies', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10142</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging</title><link>https://arxiv.org/abs/2503.17239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMERGE, a post-fine-tuning framework to preserve safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Uses selective layer-wise merging between fine-tuned and safety-aligned models&lt;/li&gt;&lt;li&gt;Evaluates across three LLMs and two tasks, showing reduced harmful outputs&lt;/li&gt;&lt;li&gt;Maintains or improves downstream performance while enhancing safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'fine-tuning', 'model merging', 'alignment', 'post-fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17239</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge</title><link>https://arxiv.org/abs/2502.19207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FaithUn benchmark for evaluating unlearning in LLMs&lt;/li&gt;&lt;li&gt;Proposes KLUE method for faithful unlearning&lt;/li&gt;&lt;li&gt;Addresses interconnectedness of knowledge in unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nakyeong Yang', 'Minsung Kim', 'Seunghyun Yoon', 'Joongbo Shin', 'Kyomin Jung']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19207</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fast-MIA: Efficient and Scalable Membership Inference for LLMs</title><link>https://arxiv.org/abs/2510.23074</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Fast-MIA library for membership inference attacks on LLMs&lt;/li&gt;&lt;li&gt;Addresses computational cost and lack of standardized implementations&lt;/li&gt;&lt;li&gt;Provides fast batch inference and unified evaluation framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiromu Takahashi', 'Shotaro Ishihara']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM security', 'membership inference', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23074</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2510.22535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OFFSIDE benchmark for evaluating unlearning of misinformation in MLLMs&lt;/li&gt;&lt;li&gt;Focuses on football transfer rumors with multimodal data&lt;/li&gt;&lt;li&gt;Evaluates forgetting efficacy, generalization, utility, and robustness&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities to prompt attacks and visual rumors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Zheng', 'Zirui Pang', 'Ling li', 'Zhijie Deng', 'Yuhan Pu', 'Zhaowei Zhu', 'Xiaobo Xia', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'multimodal', 'privacy', 'prompt attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22535</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models</title><link>https://arxiv.org/abs/2510.22085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Jailbreak Mimicry for automated discovery of narrative-based jailbreak prompts&lt;/li&gt;&lt;li&gt;Uses LoRA on Mistral-7B with AdvBench dataset&lt;/li&gt;&lt;li&gt;Achieves high ASR across multiple models&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in technical and deception domains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pavlos Ntais']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22085</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Optimal Detection for Language Watermarks with Pseudorandom Collision</title><link>https://arxiv.org/abs/2510.22007</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a statistical framework for detecting language watermarks with pseudorandom collision handling&lt;/li&gt;&lt;li&gt;Defines minimal units to manage within-unit dependence and between-unit independence&lt;/li&gt;&lt;li&gt;Provides closed-form optimal detection rules for Gumbel-max and inverse-transform watermarks&lt;/li&gt;&lt;li&gt;Improves detection power with strict Type I error control&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['T. Tony Cai', 'Xiang Li', 'Qi Long', 'Weijie J. Su', 'Garrett G. Wen']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'detection', 'pseudorandomness', 'hypothesis testing', 'LLM outputs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22007</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models</title><link>https://arxiv.org/abs/2510.23334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaSearch, a blockwise search strategy for inference-time alignment of LLMs&lt;/li&gt;&lt;li&gt;Focuses computational effort on initial tokens which are more critical for alignment&lt;/li&gt;&lt;li&gt;Outperforms Best-of-N and fine-tuning baselines in harmlessness, sentiment control, and math reasoning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Atif Quamar', 'Mohammad Areeb', 'Nishant Sharma', 'Ananth Shreekumar', 'Jonathan Rosenthal', 'Muslum Ozgur Ozmen', 'Mikhail Kuznetsov', 'Z. Berkay Celik']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time methods', 'blockwise search', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23334</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models</title><link>https://arxiv.org/abs/2510.22014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes transferability of adversarial suffixes in LLMs&lt;/li&gt;&lt;li&gt;Identifies statistical properties correlating with transfer success&lt;/li&gt;&lt;li&gt;Focuses on jailbreaking attacks and their transferability&lt;/li&gt;&lt;li&gt;Provides insights into improving attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Niki Hasrati', 'Alexander Robey', 'Avi Schwarzschild', 'Frauke Kreuter', 'Zico Kolter', 'Andrej Risteski']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'transferability', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22014</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</title><link>https://arxiv.org/abs/2510.21983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores using persuasive strategies from social sciences to create adversarial prompts for jailbreaking LLMs.&lt;/li&gt;&lt;li&gt;It tests if LLMs have persuasive fingerprints in their responses when jailbroken.&lt;/li&gt;&lt;li&gt;Empirical results show that persuasion-aware prompts can bypass alignment safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Havva Alizadeh Noughabi', 'Julien Serbanescu', 'Fattane Zarrinkalam', 'Ali Dehghantanha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21983</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper presents a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Refining Language Model Anonymizers via Adversarial Distillation</title><link>https://arxiv.org/abs/2506.01420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEAL, a distillation framework for training small language models (SLMs) to anonymize text without relying on external models.&lt;/li&gt;&lt;li&gt;Uses adversarial interactions between an LLM anonymizer and an inference model to collect training data.&lt;/li&gt;&lt;li&gt;SLMs learn to anonymize and evaluate their outputs, enabling self-refinement.&lt;/li&gt;&lt;li&gt;Experiments show 8B models can match or exceed GPT-4's privacy-utility trade-off.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyuyoung Kim', 'Hyunjun Jeon', 'Jinwoo Shin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'adversarial prompting', 'model extraction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01420</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alert-ME: An Explainability-Driven Defense Against Adversarial Examples in Transformer-Based Text Classification</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EDIT framework for detecting and defending against adversarial examples in transformer-based text classifiers&lt;/li&gt;&lt;li&gt;Uses explainability tools like attention maps and integrated gradients combined with frequency features&lt;/li&gt;&lt;li&gt;Includes adaptive resilience through feature similarity enforcement and input transformation&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over state-of-the-art defenses with faster feature extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Bushra Sabir (CSIRO's Data61)", 'Yansong Gao (The University of Western Australia)', "Alsharif Abuadbba (CSIRO's Data61)", 'M. Ali Babar (The University of Adelaide', 'CREST- The Centre for Research on Engineering Software Technologies)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'text classification', 'explainability', 'defense mechanism', 'transformer models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors</title><link>https://arxiv.org/abs/2509.15551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PolyJuice is a black-box, universal red-teaming method for synthetic image detectors (SIDs)&lt;/li&gt;&lt;li&gt;It identifies a distribution shift in the T2I latent space between correctly and incorrectly classified images&lt;/li&gt;&lt;li&gt;Generates attacks by steering images towards SID failure modes&lt;/li&gt;&lt;li&gt;Improves SID performance when used for data augmentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepehr Dehdashtian', 'Mashrur M. Morshed', 'Jacob H. Seidman', 'Gaurav Bharaj', 'Vishnu Naresh Boddeti']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'image synthesis', 'text-to-image models', 'detector robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15551</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Distillation Robustifies Unlearning</title><link>https://arxiv.org/abs/2506.06278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces UNDO, a method that uses distillation to robustify unlearning in LLMs.&lt;/li&gt;&lt;li&gt;UNDO distills an unlearned model into a noised copy, offering a tradeoff between compute cost and robustness.&lt;/li&gt;&lt;li&gt;It shows improved robustness on synthetic tasks and the WMDP benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bruce W. Lee', 'Addie Foote', 'Alex Infanger', 'Leni Shor', 'Harish Kamath', 'Jacob Goldman-Wetzler', 'Bryce Woodworth', 'Alex Cloud', 'Alexander Matt Turner']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'distillation', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06278</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Valuation of Human Feedback through Provably Robust Model Alignment</title><link>https://arxiv.org/abs/2505.17859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H"older-DPO, a new alignment loss with provable redescending property&lt;/li&gt;&lt;li&gt;Enables estimation of clean data distribution from noisy human feedback&lt;/li&gt;&lt;li&gt;Provides a metric for dataset valuation and mislabel detection&lt;/li&gt;&lt;li&gt;Improves alignment performance by identifying and removing mislabels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Fujisawa', 'Masaki Adachi', 'Michael A. Osborne']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17859</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Shape it Up! Restoring LLM Safety during Finetuning</title><link>https://arxiv.org/abs/2505.17196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes dynamic safety shaping (DSS) for LLM finetuning to address safety risks&lt;/li&gt;&lt;li&gt;Introduces STAR for token-level safety assessment during response generation&lt;/li&gt;&lt;li&gt;Uses guardrail models to evaluate partial responses dynamically&lt;/li&gt;&lt;li&gt;Demonstrates safety improvements across various models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['ShengYun Peng', 'Pin-Yu Chen', 'Jianfeng Chi', 'Seongmin Lee', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'dynamic safety shaping', 'STAR', 'finetuning risks', 'guardrail models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17196</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs</title><link>https://arxiv.org/abs/2502.14828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates fundamental limitations in pointwise defenses of LLM fine-tuning APIs.&lt;/li&gt;&lt;li&gt;It constructs attacks using benign samples that transmit dangerous knowledge through entropy in model outputs.&lt;/li&gt;&lt;li&gt;The attacks evade detection by ensuring individual samples are unsuspicious and low-perplexity.&lt;/li&gt;&lt;li&gt;Tests against OpenAI's API show success in eliciting harmful responses while bypassing enhanced monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xander Davies', 'Eric Winsor', 'Alexandra Souly', 'Tomek Korbak', 'Robert Kirk', 'Christian Schroeder de Witt', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14828</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots</title><link>https://arxiv.org/abs/2510.21459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SBASH framework for LLM-based honeypots&lt;/li&gt;&lt;li&gt;Evaluates RAG vs prompt-tuned LLMs for Linux shell command responses&lt;/li&gt;&lt;li&gt;Compares metrics like response time, realism, and similarity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adetayo Adebimpe', 'Helmut Neukirchen', 'Thomas Welsh']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'honeypot', 'RAG', 'prompt tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21459</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Soft Instruction De-escalation Defense</title><link>https://arxiv.org/abs/2510.21057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SIC (Soft Instruction Control) for LLM agents to prevent prompt injection attacks&lt;/li&gt;&lt;li&gt;Iterative prompt sanitization loop that rewrites, masks, or removes malicious content&lt;/li&gt;&lt;li&gt;Continues until input is clean or halts if imperative instructions remain&lt;/li&gt;&lt;li&gt;Raises the bar against attacks but not infallible (15% ASR possible)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Philipp Walter', 'Chawin Sitawarin', 'Jamie Hayes', 'David Stutz', 'Ilia Shumailov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21057</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FrameShield: Adversarially Robust Video Anomaly Detection</title><link>https://arxiv.org/abs/2510.21532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FrameShield for adversarially robust video anomaly detection&lt;/li&gt;&lt;li&gt;Uses Spatiotemporal Region Distortion (SRD) to generate synthetic anomalies&lt;/li&gt;&lt;li&gt;Improves robustness against adversarial attacks in WSVAD models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mojtaba Nafez', 'Mobina Poulaei', 'Nikan Vasei', 'Bardia Soltani Moakhar', 'Mohammad Sabokrou', 'MohammadHossein Rohban']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'video anomaly detection', 'weakly supervised learning', 'adversarial training', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21532</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Probe-based Fine-tuning for Reducing Toxicity</title><link>https://arxiv.org/abs/2510.21531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores using probes trained on model activations to detect and reduce toxicity.&lt;/li&gt;&lt;li&gt;It proposes two training methods: Supervised Fine-tuning and Direct Preference Optimization.&lt;/li&gt;&lt;li&gt;Key findings include that preference optimization preserves probe accuracy better than classifier-based methods.&lt;/li&gt;&lt;li&gt;Retraining probes after optimization recovers detection accuracy, making probe ensembles less necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jan Wehner', 'Mario Fritz']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'probe-based training', 'toxicity reduction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21531</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Leverage Unlearning to Sanitize LLMs</title><link>https://arxiv.org/abs/2510.21322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SANI, an unlearning approach to sanitize LLMs by removing memorized sensitive information&lt;/li&gt;&lt;li&gt;Involves erasure (resetting neurons) and repair (fine-tuning without sensitive data)&lt;/li&gt;&lt;li&gt;Evaluated on medical data and standard pre-trained models&lt;/li&gt;&lt;li&gt;Reduces regurgitation of sensitive information with minimal additional training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antoine Boutet', 'Lucas Magnana']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21322</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference</title><link>https://arxiv.org/abs/2510.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RePULSe, a training method for LMs to reduce undesired outputs using probabilistic inference&lt;/li&gt;&lt;li&gt;Augments standard RL loss with a loss that targets low-reward outputs&lt;/li&gt;&lt;li&gt;Aims to improve tradeoff between average reward and undesired output probability&lt;/li&gt;&lt;li&gt;Demonstrates better adversarial robustness compared to standard methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Zhao', 'Aidan Li', 'Rob Brekelmans', 'Roger Grosse']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement learning', 'adversarial robustness', 'language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21184</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection</title><link>https://arxiv.org/abs/2510.20963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ColMAD, a collaborative multi-agent debate protocol for error detection in LLMs&lt;/li&gt;&lt;li&gt;Aims to improve oversight by encouraging agents to support each other's criticisms&lt;/li&gt;&lt;li&gt;Shows significant improvement over competitive MAD and single-agent methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongqiang Chen', 'Gang Niu', 'James Cheng', 'Bo Han', 'Masashi Sugiyama']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20963</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the issue where LLMs adjust their behavior during evaluation to appear more aligned, which affects safety evaluations.&lt;/li&gt;&lt;li&gt;It introduces a steering vector technique to suppress evaluation-awareness, making the model behave as if deployed.&lt;/li&gt;&lt;li&gt;The study involves training an LLM with evaluation cues and using activation steering to counteract the awareness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title><link>https://arxiv.org/abs/2510.17884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study on LLMs for password cracking&lt;/li&gt;&lt;li&gt;Evaluates models like TinyLLaMA, Falcon-RW-1B, Flan-T5&lt;/li&gt;&lt;li&gt;Poor performance compared to traditional methods&lt;/li&gt;&lt;li&gt;Identifies limitations in generative reasoning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Abdul Rehman', 'Syed Imad Ali Shah', 'Abbas Anwar', 'Noor Islam']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17884</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models</title><link>https://arxiv.org/abs/2510.16712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates multi-turn stance instability in search-enabled LLMs&lt;/li&gt;&lt;li&gt;Introduces Chameleon Benchmark Dataset and metrics&lt;/li&gt;&lt;li&gt;Evaluates Llama-4-Maverick, GPT-4o-mini, Gemini-2.5-Flash&lt;/li&gt;&lt;li&gt;Finds severe chameleon behavior across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivam Ratnakar', 'Sanjay Raghavendra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16712</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models</title><link>https://arxiv.org/abs/2510.10142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DiffHeads framework for debiasing LLMs&lt;/li&gt;&lt;li&gt;Compares DA and CoT prompting strategies&lt;/li&gt;&lt;li&gt;Identifies and masks bias heads in attention layers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingxu Han', 'Wei Song', 'Ziqi Ding', 'Ziming Li', 'Chunrong Fang', 'Yuekang Li', 'Dongfang Liu', 'Zhenyu Chen', 'Zhenting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'attention heads', 'prompting strategies', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10142</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title><link>https://arxiv.org/abs/2509.11173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a vulnerability in DL compilers that can introduce backdoors during compilation&lt;/li&gt;&lt;li&gt;Demonstrates successful attacks across multiple models, compilers, and hardware&lt;/li&gt;&lt;li&gt;Identifies natural triggers in popular HuggingFace models&lt;/li&gt;&lt;li&gt;Highlights the need for secure compiler design&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simin Chen', 'Jinjun Peng', 'Yixin He', 'Junfeng Yang', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'backdoor', 'compiler', 'vulnerability', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11173</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models</title><link>https://arxiv.org/abs/2507.22828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CapRecover, a framework to recover semantic content from intermediate features of VLMs&lt;/li&gt;&lt;li&gt;Evaluates on multiple datasets and models, showing high accuracy in label recovery and caption generation&lt;/li&gt;&lt;li&gt;Introduces a noise-based protection method to mitigate semantic leakage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kedong Xiu', 'Sai Qian Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data leakage', 'vision-language models', 'feature inversion', 'semantic recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22828</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning as an Adaptive Defense for Safety</title><link>https://arxiv.org/abs/2507.00971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TARS, a reinforcement learning approach to train LLMs for safety using adaptive reasoning.&lt;/li&gt;&lt;li&gt;Focuses on handling ambiguous prompts and balancing safety with task completion.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against white-box and black-box attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoun Kim', 'Fahim Tajwar', 'Aditi Raghunathan', 'Aviral Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'adaptive reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00971</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ME: Trigger Element Combination Backdoor Attack on Copyright Infringement</title><link>https://arxiv.org/abs/2506.10776</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new attack method (ME) for copyright infringement using generative models&lt;/li&gt;&lt;li&gt;Introduces new datasets for attack research&lt;/li&gt;&lt;li&gt;Enhances stealthiness with DCT&lt;/li&gt;&lt;li&gt;Improves performance over SBD in low sample scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiyu Yang', 'Siyuan Liang', 'Aishan Liu', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'generative models', 'copyright infringement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10776</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAGE: A Generic Framework for LLM Safety Evaluation</title><link>https://arxiv.org/abs/2504.19674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAGE, a framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Uses adversarial agents with diverse personalities for multi-turn conversations&lt;/li&gt;&lt;li&gt;Evaluates models across different applications and harm policies&lt;/li&gt;&lt;li&gt;Highlights importance of context-specific testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhur Jindal', 'Hari Shrawgi', 'Parag Agrawal', 'Sandipan Dandapat']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Adversarial agents', 'Multi-turn conversations', 'Policy-aware testing', 'Harm policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.19674</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging</title><link>https://arxiv.org/abs/2503.17239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMERGE, a post-fine-tuning framework to preserve safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Uses selective layer-wise merging between fine-tuned and safety-aligned models&lt;/li&gt;&lt;li&gt;Evaluates across three LLMs and two tasks, showing reduced harmful outputs&lt;/li&gt;&lt;li&gt;Maintains or improves downstream performance while enhancing safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Swanand Ravindra Kadhe', 'Farhan Ahmed', 'Syed Zawad', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'fine-tuning', 'model merging', 'alignment', 'post-fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17239</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</title><link>https://arxiv.org/abs/2503.10635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new adversarial attack method for LVLMs&lt;/li&gt;&lt;li&gt;Focuses on semantic clarity in perturbations&lt;/li&gt;&lt;li&gt;Achieves high success rates against closed-source models like GPT-4.5&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LVLMs', 'transferability', 'semantic perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10635</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge</title><link>https://arxiv.org/abs/2502.19207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FaithUn benchmark for evaluating unlearning in LLMs&lt;/li&gt;&lt;li&gt;Proposes KLUE method for faithful unlearning&lt;/li&gt;&lt;li&gt;Addresses interconnectedness of knowledge in unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nakyeong Yang', 'Minsung Kim', 'Seunghyun Yoon', 'Joongbo Shin', 'Kyomin Jung']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19207</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MIBP-Cert: Certified Training against Data Perturbations with Mixed-Integer Bilinear Programs</title><link>https://arxiv.org/abs/2412.10186</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIBP-Cert for provable robustness against data perturbations&lt;/li&gt;&lt;li&gt;Uses mixed-integer bilinear programming to compute deterministic bounds&lt;/li&gt;&lt;li&gt;Applies to continuous and discrete data with various threat models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobias Lorenz', 'Marta Kwiatkowska', 'Mario Fritz']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10186</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models</title><link>https://arxiv.org/abs/2407.11654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses jamming attacks in split federated learning (SFL) for LLMs and VLMs.&lt;/li&gt;&lt;li&gt;It proposes R-SFLLM, a framework using wireless sensing to counter jamming.&lt;/li&gt;&lt;li&gt;Includes adversarial training to enhance resilience.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across NLP and CV tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Vlad C. Andrei', 'Xinyang Li', 'Ullrich J. M\\"onich', 'Holger Boche', 'Walid Saad']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'red teaming', 'adversarial attacks', 'resilience', 'federated learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.11654</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper presents a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data hiding', 'AI safety', 'trust in AI', 'model transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space</title><link>https://arxiv.org/abs/2510.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GuardSpace framework to preserve safety alignment during LLM fine-tuning&lt;/li&gt;&lt;li&gt;Decomposes model weights into safety-relevant and -irrelevant components&lt;/li&gt;&lt;li&gt;Freezes safety-relevant components and uses null space projection to prevent harmful responses&lt;/li&gt;&lt;li&gt;Shows significant reduction in harmful scores and improved accuracy on test tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingjie Zhang', 'Yibo Yang', 'Zhe Ren', 'Dandan Guo', 'Jindong Gu', 'Philip Torr', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'fine-tuning', 'safety preservation', 'null space projection', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14301</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title><link>https://arxiv.org/abs/2509.03728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PersonaTeaming, a method that uses personas to improve automated AI red-teaming&lt;/li&gt;&lt;li&gt;Compares expert vs regular user personas in adversarial prompt generation&lt;/li&gt;&lt;li&gt;Develops new mutation distance metrics for prompt diversity&lt;/li&gt;&lt;li&gt;Shows up to 144.1% improvement in attack success rates over RainbowPlus&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wesley Hanwen Deng', 'Sunnie S. Y. Kim', 'Akshita Jha', 'Ken Holstein', 'Motahhare Eslami', 'Lauren Wilcox', 'Leon A Gatys']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'prompt mutation', 'persona-based methods', 'attack success rate']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03728</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Guarded Query Routing for Large Language Models</title><link>https://arxiv.org/abs/2505.14524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Guarded Query Routing Benchmark (GQR-Bench) for evaluating query routing robustness&lt;/li&gt;&lt;li&gt;Compares various routing mechanisms including LLMs, guardrails, and traditional ML models&lt;/li&gt;&lt;li&gt;Finds trade-offs between accuracy and speed, with WideMLP and fastText showing promise&lt;/li&gt;&lt;li&gt;Challenges automatic reliance on LLMs for guarded query routing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Richard \\v{S}l\\'eher", 'William Brach', 'Tibor Sloboda', "Kristi\\'an Ko\\v{s}\\v{t}\\'al", 'Lukas Galke']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'LLM guardrails', 'query routing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14524</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals</title><link>https://arxiv.org/abs/2502.16101</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGuard benchmark for evaluating RAG systems against misleading retrievals&lt;/li&gt;&lt;li&gt;Uses Reddit discussions for naturally occurring misinformation&lt;/li&gt;&lt;li&gt;Shows LLM-powered RAG systems perform worse than zero-shot baselines when exposed to misleading evidence&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linda Zeng', 'Rithwik Gupta', 'Divij Motwani', 'Diji Yang', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'evaluation', 'data poisoning', 'misinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16101</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination</title><link>https://arxiv.org/abs/2510.22977</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SimpleToolHalluBench to measure tool hallucination in LLMs&lt;/li&gt;&lt;li&gt;Finds that enhancing reasoning through RL increases tool hallucination&lt;/li&gt;&lt;li&gt;Shows the effect is method-agnostic and transcends overfitting&lt;/li&gt;&lt;li&gt;Reveals a trade-off between reducing hallucination and maintaining utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenlong Yin', 'Zeyang Sha', 'Shiwen Cui', 'Changhua Meng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22977</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</title><link>https://arxiv.org/abs/2510.22963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CompressionAttack framework targeting prompt compression in LLM agents&lt;/li&gt;&lt;li&gt;Two strategies: HardCom (discrete edits) and SoftCom (latent perturbations)&lt;/li&gt;&lt;li&gt;High attack success and stealth, current defenses ineffective&lt;/li&gt;&lt;li&gt;Real-world impact demonstrated in VSCode Cline and Ollama&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zesen Liu', 'Zhixiang Zhang', 'Yuchong Xie', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'prompt compression', 'attack surface']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22963</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies</title><link>https://arxiv.org/abs/2510.22944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proposes an evaluation framework for prompt quality in code generation using LLMs.&lt;/li&gt;&lt;li&gt;Introduces CWE-BENCH-PYTHON, a benchmark dataset with prompts categorized by normativity levels.&lt;/li&gt;&lt;li&gt;Finds that lower prompt normativity leads to higher rates of insecure code generation.&lt;/li&gt;&lt;li&gt;Demonstrates that advanced prompting techniques can mitigate security risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'YiLu Zhong', 'MiDi Wan', 'WenJie Yu', 'YuanBing Ouyang', 'Yenan Huang', 'Hui Li']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'security evaluation', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22944</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.22628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Sentra-Guard is a real-time defense system against adversarial LLM jailbreaks and prompt injection attacks.&lt;/li&gt;&lt;li&gt;Uses a hybrid architecture with SBERT embeddings and fine-tuned classifiers for detection.&lt;/li&gt;&lt;li&gt;Includes multilingual support and a human-in-the-loop feedback loop for continuous improvement.&lt;/li&gt;&lt;li&gt;Achieves high detection rates and low false positives compared to existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Mehedi Hasan', 'Ziaur Rahman', 'Rafid Mostafiz', 'Md. Abir Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22628</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents</title><link>https://arxiv.org/abs/2510.22620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces threat snapshots framework for evaluating LLM security in AI agents&lt;/li&gt;&lt;li&gt;Constructs the b³ benchmark with 194,331 adversarial attacks&lt;/li&gt;&lt;li&gt;Evaluates 31 LLMs, finding reasoning capabilities improve security but size doesn't&lt;/li&gt;&lt;li&gt;Releases benchmark, dataset, and code for adoption&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julia Bazinska', 'Max Mathys', 'Francesco Casucci', 'Mateo Rojas-Carulla', 'Xander Davies', 'Alexandra Souly', 'Niklas Pfister']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security evaluation', 'benchmarking', 'adversarial attacks', 'agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22620</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Group size effects and collective misalignment in LLM multi-agent systems</title><link>https://arxiv.org/abs/2510.22422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores group size effects on multi-agent LLM misalignment&lt;/li&gt;&lt;li&gt;Shows non-linear dynamics and critical population sizes&lt;/li&gt;&lt;li&gt;Develops mean-field analytical approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariel Flint', 'Luca Maria Aiello', 'Romualdo Pastor-Satorras', 'Andrea Baronchelli']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent systems', 'collective behavior', 'dynamical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22422</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title><link>https://arxiv.org/abs/2510.22300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T2I-RiskyPrompt, a benchmark for evaluating safety in text-to-image models&lt;/li&gt;&lt;li&gt;Includes a hierarchical risk taxonomy with 6 primary and 14 subcategories&lt;/li&gt;&lt;li&gt;Provides 6,432 annotated risky prompts with detailed reasons&lt;/li&gt;&lt;li&gt;Evaluates multiple models, defense methods, safety filters, and attack strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyu Zhang', 'Tairen Zhang', 'Lanjun Wang', 'Ruidong Chen', 'Wenhui Li', 'Anan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'text-to-image models', 'risky prompts', 'benchmarking', 'attack strategies', 'defense methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22300</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models</title><link>https://arxiv.org/abs/2510.22085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Jailbreak Mimicry for automated discovery of narrative-based jailbreak prompts&lt;/li&gt;&lt;li&gt;Uses LoRA on Mistral-7B with AdvBench dataset&lt;/li&gt;&lt;li&gt;Achieves high ASR across multiple models&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in technical and deception domains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pavlos Ntais']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22085</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models</title><link>https://arxiv.org/abs/2510.22014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes transferability of adversarial suffixes in LLMs&lt;/li&gt;&lt;li&gt;Identifies statistical properties correlating with transfer success&lt;/li&gt;&lt;li&gt;Focuses on jailbreaking attacks and their transferability&lt;/li&gt;&lt;li&gt;Provides insights into improving attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Niki Hasrati', 'Alexander Robey', 'Avi Schwarzschild', 'Frauke Kreuter', 'Zico Kolter', 'Andrej Risteski']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'transferability', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22014</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</title><link>https://arxiv.org/abs/2510.21983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores using persuasive strategies from social sciences to create adversarial prompts for jailbreaking LLMs.&lt;/li&gt;&lt;li&gt;It tests if LLMs have persuasive fingerprints in their responses when jailbroken.&lt;/li&gt;&lt;li&gt;Empirical results show that persuasion-aware prompts can bypass alignment safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Havva Alizadeh Noughabi', 'Julien Serbanescu', 'Fattane Zarrinkalam', 'Ali Dehghantanha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21983</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models</title><link>https://arxiv.org/abs/2510.21783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient membership inference attack for diffusion models using small-noise injection&lt;/li&gt;&lt;li&gt;Analyzes noise aggregation patterns to distinguish training vs non-training samples&lt;/li&gt;&lt;li&gt;Requires fewer model queries compared to existing methods&lt;/li&gt;&lt;li&gt;Demonstrates scalability with large text-to-image models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guo Li', 'Yuyang Yu', 'Xuemiao Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'diffusion models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21783</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2510.22535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OFFSIDE benchmark for evaluating unlearning of misinformation in MLLMs&lt;/li&gt;&lt;li&gt;Focuses on football transfer rumors with multimodal data&lt;/li&gt;&lt;li&gt;Evaluates forgetting efficacy, generalization, utility, and robustness&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities to prompt attacks and visual rumors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Zheng', 'Zirui Pang', 'Ling li', 'Zhijie Deng', 'Yuhan Pu', 'Zhaowei Zhu', 'Xiaobo Xia', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'multimodal', 'privacy', 'prompt attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22535</guid><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 26 Dec 2025 22:33:45 +0000</lastBuildDate><item><title>Gobernanza y trazabilidad "a prueba de AI Act" para casos de uso legales: un marco t\'ecnico-jur\'idico, m\'etricas forenses y evidencias auditables</title><link>https://arxiv.org/abs/2510.12830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a governance framework mapping the EU AI Act requirements to technical controls tailored to legal-sector AI systems.&lt;/li&gt;&lt;li&gt;Designs a forensic architecture specifically for RAG/LLM systems to enable traceability and audit-ready evidence.&lt;/li&gt;&lt;li&gt;Defines an evaluation system with metrics weighted by legal risk for assessing compliance and forensic sufficiency.&lt;/li&gt;&lt;li&gt;Delivers rag-forense, an open-source implementation and an experimental protocol to demonstrate verifiable compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Dantart']&lt;/li&gt;&lt;li&gt;Tags: ['Governance', 'Forensic architecture', 'RAG/LLM', 'Compliance (AI Act)', 'Auditability / Metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12830</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers</title><link>https://arxiv.org/abs/2510.00915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes imperfect verifiers as a stochastic reward channel with asymmetric false-positive (ρ0) and false-negative (ρ1) noise rates for binary verifiers.&lt;/li&gt;&lt;li&gt;Proposes two lightweight corrections: a backward correction that yields an unbiased surrogate reward and an unbiased policy-gradient estimator in expectation, and a forward correction that reweights score-function terms to align expected updates and requires only the FN rate.&lt;/li&gt;&lt;li&gt;Implements both corrections in a PPO-style pipeline, demonstrates improved performance on math-reasoning tasks under synthetic and real verifier noise, and introduces an appeals mechanism using an LLM verifier to estimate FN rate online.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin-Qiang Cai', 'Wei Wang', 'Feng Liu', 'Tongliang Liu', 'Gang Niu', 'Masashi Sugiyama']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'reward-noise', 'verifier-reliability', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00915</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks</title><link>https://arxiv.org/abs/2506.08602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WGLE, a backdoor-free, black-box multi-bit watermarking method for graph neural networks using a novel Layer-wise Distance Difference on an Edge (LDDE) as the watermark signal.&lt;/li&gt;&lt;li&gt;Encodes multi-bit watermarks by assigning unique LDDE values to edges, enabling ownership verification without embedding backdoors and with minimal fidelity degradation (~1.41%) and reported 100% verification accuracy across multiple datasets and GNN architectures.&lt;/li&gt;&lt;li&gt;Evaluates robustness against attacks and compares favorably to existing GNN watermarking and fingerprinting methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingzhi Li', 'Xuefeng Liu', 'Jing Lei', 'Xingang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['GNN watermarking', 'model ownership', 'black-box watermarking', 'backdoor-free', 'graph neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08602</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds</title><link>https://arxiv.org/abs/2504.04973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies constrained Markov decision processes (CMDPs) where constraint thresholds are stochastic/unknown, targeting safety in reinforcement learning under environmental uncertainty.&lt;/li&gt;&lt;li&gt;Introduces a Growing-Window estimator to learn thresholds from interactions and proposes SPOT (Stochastic Pessimistic-Optimistic Thresholding), a model-based primal-dual algorithm for multiple stochastic-threshold constraints.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees: reward regret Õ(√T) and constraint violation Õ(√T) over T episodes, claiming comparable performance to approaches with fixed known thresholds and novelty in handling unknown thresholds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Zuo', 'Fengxiang He']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'constrained MDPs', 'robustness', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04973</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses</title><link>https://arxiv.org/abs/2504.02080</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of jailbreak attacks and defenses on multiple LLMs (open- and closed-source) using four state-of-the-art attack techniques and three new defensive approaches.&lt;/li&gt;&lt;li&gt;Analyzes how model evolution (newer versions) and model size affect security, and evaluates combining multiple defenses to improve robustness.&lt;/li&gt;&lt;li&gt;Provides comparative security evaluation across models like LLaMA, Mistral, and GPT-4, offering actionable insights for detection and mitigation of jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengchun Shang', 'Wenlan Wei', 'Weiheng Bai']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM safety', 'adversarial prompting', 'defenses', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02080</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI for Scaling Diagnosis and Care in Neurodegenerative Disease</title><link>https://arxiv.org/abs/2502.06842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a six-phase roadmap for responsible design and integration of agentic LLM systems into Alzheimer’s disease and related dementias (ADRD) care: standardized multimodal data collection, decision support, clinical workflow integration, validation/monitoring, continuous learning, and ethics/risk management.&lt;/li&gt;&lt;li&gt;Emphasizes human-centered augmentation of clinicians to approach specialist-level assessment while prioritizing patient safety, equity, transparency, and clinical applicability.&lt;/li&gt;&lt;li&gt;Calls for rigorous validation, monitoring protocols, and continuous clinical feedback loops to ensure reliability and ongoing improvement.&lt;/li&gt;&lt;li&gt;Advocates robust ethics and risk-management frameworks to mitigate harms and guide responsible deployment across medical specialties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew G. Breithaupt', 'Michael Weiner', 'Alice Tang', 'Katherine L. Possin', 'Marina Sirota', 'James Lah', 'Allan I. Levey', 'Pascal Van Hentenryck', 'Reza Zandehshahvar', 'Marilu Luisa Gorno-Tempini', 'Joseph Giorgio', 'Jingshen Wang', 'Andreas M. Rauschecker', 'Howard J. Rosen', 'Rachel L. Nosheny', 'Bruce L. Miller', 'Pedro Pinheiro-Chagas']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'clinical validation', 'continuous learning', 'ethics and risk management', 'multimodal clinical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.06842</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback</title><link>https://arxiv.org/abs/2409.00162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes sequence-to-sequence (seq2seq) reward modeling that trains reward models to generate language feedback (sequence MLE) instead of scalar/binary labels.&lt;/li&gt;&lt;li&gt;Method requires no extra annotations or models and is integrated into RLHF to provide richer, fine-grained feedback.&lt;/li&gt;&lt;li&gt;Empirical gains: reduces refusal-to-response in safety dialogues and long-response bias in summarization; improves RLHF across 2B and 7B LLMs on 3 NLP tasks with average win rate ~76.9%.&lt;/li&gt;&lt;li&gt;Authors report improved robustness under out-of-distribution prompts, addressing biased local optimization in standard RM approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Zhou', 'Jiaming Ji', 'Juntao Dai', 'Dong Li', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward-modeling', 'alignment', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.00162</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</title><link>https://arxiv.org/abs/2512.13725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of quantization (INT8, NF4) effects on causal reasoning across Pearl's three rungs (association, intervention, counterfactual) using a 3000-sample CLadder benchmark.&lt;/li&gt;&lt;li&gt;Finds overall robustness to quantization in Llama 3 8B with NF4 showing &lt;1% degradation; interventional (rung 2) queries are most sensitive while counterfactual (rung 3) queries show heterogeneous failure modes (e.g., collider bias, backdoor adjustment).&lt;/li&gt;&lt;li&gt;CRASS benchmark experiments indicate commonsense counterfactual datasets may lack the structural sensitivity to reveal quantization-induced reasoning drift.&lt;/li&gt;&lt;li&gt;Graph Retrieval-Augmented Generation (GraphRAG) using ground-truth causal graphs consistently improves NF4 interventional accuracy (~+1.7%), partially offsetting compression-related degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steve Nwaiwu', 'Nipat Jongsawat', 'Anucha Tungkasthan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'quantization', 'causal reasoning', 'retrieval-augmented generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13725</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bootstrapping LLMs via Preference-Based Policy Optimization</title><link>https://arxiv.org/abs/2511.12867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Preference-based Policy Optimization (PbPO): a min–max game between a policy and a reward model constrained within a confidence set derived from preference data.&lt;/li&gt;&lt;li&gt;Introduces an iterative online algorithm that actively collects preferences via guided exploration to continually improve both policy and RM.&lt;/li&gt;&lt;li&gt;Provides theoretical high-probability regret bounds for sequence-level and token-level reward models and reports empirical gains over prior preference-optimization methods on five benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Jia']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-learning', 'reward-modeling', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12867</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations</title><link>https://arxiv.org/abs/2510.11822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Documents a strong agreeableness (positive) bias in LLM-as-judge setups: high true positive rate (~96%) but very low true negative rate (&lt;25%), causing inflated reliability.&lt;/li&gt;&lt;li&gt;Shows ensemble/majority voting is insufficient and introduces an optimal minority-veto strategy that is robust to missing data and reduces bias.&lt;/li&gt;&lt;li&gt;Proposes a regression-based framework that models validator bias using a small human-annotated ground-truth set; on a code-feedback task (366 Python programs) it cuts max absolute error to 1.2%, ~2x better than the best 14-LLM ensemble.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Suryaansh Jain', 'Umair Z. Ahmed', 'Shubham Sahai', 'Ben Leong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'evaluation bias', 'safety evaluation', 'robustness', 'calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11822</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</title><link>https://arxiv.org/abs/2512.21241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARS-OPT, a momentum-based algorithm inspired by Nesterov acceleration to improve ray-direction search for hard-label black-box adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence analysis showing faster and more stable directional updates; extends method with surrogate-model priors as PARS-OPT for further acceleration.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains in query efficiency over 13 state-of-the-art methods on ImageNet and CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinjie Xu', 'Shuyu Cheng', 'Dongwei Xu', 'Qi Xuan', 'Chen Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'hard-label black-box attacks', 'query efficiency', 'attack optimization', 'surrogate-model priors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21241</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking</title><link>https://arxiv.org/abs/2512.21236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPELL, a framework that constructs jailbreak prompts by combining sentences from a prior knowledge dataset using a time-division selection strategy to balance exploration and exploitation of attack patterns.&lt;/li&gt;&lt;li&gt;Targets malicious code generation as a specific jailbreak objective and evaluates SPELL against three advanced code-capable LLMs (GPT-4.1, Claude-3.5, Qwen2.5-Coder).&lt;/li&gt;&lt;li&gt;Reports high attack success rates (e.g., 83.75% for GPT-4.1) across eight malicious code categories and demonstrates real-world effectiveness in AI coding tools (Cursor) with outputs validated by detection systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Huang', 'Xiaojun Jia', 'Wenbo Guo', 'Yuqiang Sun', 'Yihao Huang', 'Chong Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'malicious code generation', 'prompt engineering', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21236</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoBaxBuilder: Bootstrapping Code Security Benchmarking</title><link>https://arxiv.org/abs/2512.21132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoBaxBuilder, an automated framework that generates code security benchmarking tasks and tests from scratch to evaluate LLMs' code-security behavior.&lt;/li&gt;&lt;li&gt;Uses LLMs to create functionality tests and end-to-end security-probing exploits, with fine-grained plausibility checks to ensure benchmark quality.&lt;/li&gt;&lt;li&gt;Compares generated tasks against human-expert-constructed tasks via qualitative and quantitative experiments and releases AutoBaxBench publicly.&lt;/li&gt;&lt;li&gt;Reports generation efficiency (≈2 hours and &lt;$10 per task) and provides a thorough evaluation of LLM security capabilities on the produced tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobias von Arx', 'Niels M\\"undler', 'Mark Vero', 'Maximilian Baader', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'code security benchmarking', 'automated benchmark generation', 'vulnerability testing', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21132</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semi-Supervised Learning for Large Language Models Safety and Content Moderation</title><link>https://arxiv.org/abs/2512.21107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using semi-supervised learning to train safety/content-moderation classifiers for LLMs, reducing reliance on large labeled datasets.&lt;/li&gt;&lt;li&gt;Evaluates performance gains for both user prompts and LLM responses when using semi-supervised methods.&lt;/li&gt;&lt;li&gt;Demonstrates that task-specific data augmentations substantially outperform general-purpose augmentations in semi-supervised pipelines.&lt;/li&gt;&lt;li&gt;Frames the approach as a solution to labeling scarcity, labeling errors, and over-reliance on synthetic data for safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eduard Stefan Dinuta', 'Iustin Sirbu', 'Traian Rebedea']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'content moderation', 'semi-supervised learning', 'data augmentation', 'safety classifiers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21107</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Replication of LLM Mistakes in Medical Conversations</title><link>https://arxiv.org/abs/2512.20983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedMistake, an automatic pipeline that extracts mistakes from LLM-generated patient–doctor conversations and converts them into single-shot QA pairs for benchmarking.&lt;/li&gt;&lt;li&gt;Releases MedMistake-All (3,390 QA pairs) and a doctor-validated subset MedMistake-Bench (211 QA pairs) used to evaluate 12 frontier LLMs.&lt;/li&gt;&lt;li&gt;Evaluation uses two LLM judges to identify failures and medical experts to validate a subset; reports model performance (GPT, Claude, Grok among top performers).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oleksii Proniakin', 'Diego Fajardo', 'Ruslan Nazarenko', 'Razvan Marinescu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'medical-LLMs', 'robustness', 'dataset-release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20983</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neural Probe-Based Hallucination Detection for Large Language Models</title><link>https://arxiv.org/abs/2512.20949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a token-level hallucination detection framework using lightweight MLP probes applied to frozen LLM hidden states to capture nonlinear semantic structures.&lt;/li&gt;&lt;li&gt;Introduces a multi-objective joint loss to improve detection stability and semantic disambiguation, and uses Bayesian optimization to select optimal probe insertion layers.&lt;/li&gt;&lt;li&gt;Evaluated on LongFact, HealthBench, and TriviaQA, showing improved accuracy, recall, and low false-positive detection compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shize Liang', 'Hongzhi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'probing', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20949</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs</title><link>https://arxiv.org/abs/2512.20822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MediEval, a benchmark linking MIMIC-IV EHRs to a unified biomedical knowledge base to evaluate LLMs on knowledge grounding and patient-contextual consistency.&lt;/li&gt;&lt;li&gt;Proposes a 4-quadrant evaluation framework that exposes failure modes such as hallucinated support and truth inversion in medical LLM outputs.&lt;/li&gt;&lt;li&gt;Develops CoRFu, a DPO-based, counterfactual risk-aware fine-tuning method with an asymmetric penalty to reduce unsafe confusions.&lt;/li&gt;&lt;li&gt;Reports improvements from CoRFu (+16.4 macro-F1) and elimination of truth inversion errors, demonstrating increased accuracy and safety in medical reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhan Qu', 'Michael F\\"arber']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'benchmarking/evaluation', 'medical AI', 'alignment/fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20822</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication</title><link>https://arxiv.org/abs/2512.20778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a decentralized decision-making framework for multi-agent POMDPs that explicitly handles inconsistent beliefs due to limited communication.&lt;/li&gt;&lt;li&gt;Provides probabilistic guarantees on action consistency and joint performance relative to an open-loop (fully communicated) baseline, and selectively triggers communication only when needed.&lt;/li&gt;&lt;li&gt;Evaluates approach via simulations showing improved coordination and performance over state-of-the-art methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Moshe Rafaeli Shimron', 'Vadim Indelman']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'decentralized POMDP', 'safety guarantees', 'communication-efficient planning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20778</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits</title><link>https://arxiv.org/abs/2512.20755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a robustness property specifically for neural networks with early-exit (conditional intermediate prediction) architectures.&lt;/li&gt;&lt;li&gt;Shows how existing off-the-shelf formal solvers can be applied to verify that robustness property and provides a baseline verification algorithm.&lt;/li&gt;&lt;li&gt;Introduces an early-stopping strategy and heuristic optimizations that preserve soundness and completeness to improve verification efficiency.&lt;/li&gt;&lt;li&gt;Empirical results indicate early-exit networks are easier/faster to verify than standard networks and highlight trade-offs between accuracy and verifiability/efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhak Yisrael Elboher', 'Avraham Raviv', 'Amihay Elboher', 'Zhouxing Shi', 'Omri Azencot', 'Hillel Kugler', 'Guy Katz']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'robustness', 'early-exit-architectures', 'verification-optimization', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20755</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems</title><link>https://arxiv.org/abs/2512.20688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mechanism-Based Intelligence (MBI) and a Differentiable Price Mechanism (DPM) that emits incentive signals equal to the negative loss gradient for each agent, claiming DSIC and convergence to the global optimum.&lt;/li&gt;&lt;li&gt;Provides a Bayesian extension (BIC) for asymmetric information and claims linear O(N) scaling, avoiding combinatorial Dec-POMDP complexity.&lt;/li&gt;&lt;li&gt;Reports empirical speedups (claimed 50x faster than model-free RL) and emphasizes provable, auditable alignment by structurally aligning agent self-interest with collective objectives.&lt;/li&gt;&lt;li&gt;Frames contributions in economic mechanism-design terms (VCG-equivalence) to guarantee incentive compatibility and coordinated, trustworthy multi-agent behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stefano Grassi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'mechanism-design', 'multi-agent-systems', 'incentives', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20688</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering</title><link>https://arxiv.org/abs/2512.20660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Dual-State Architecture separating deterministic workflow state from stochastic environment (LLM) state to contain nondeterminism.&lt;/li&gt;&lt;li&gt;Defines Atomic Action Pairs that couple LLM generation with immediate verification as indivisible transactions, and Guard Functions that project probabilistic outputs into observable workflow state.&lt;/li&gt;&lt;li&gt;Validates the framework on three code-generation tasks across 13 LLMs (1.3B–15B), reporting up to 66 percentage-point improvement in task success for instruction-following models at 1.2–2.1× baseline compute.&lt;/li&gt;&lt;li&gt;Argues architectural constraints and verification can substitute for scale to achieve more reliable code generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Thompson']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'model verification', 'LLM-based agents', 'code generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20660</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Competency Gaps in Large Language Models and Their Benchmarks</title><link>https://arxiv.org/abs/2512.20638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method using sparse autoencoders to extract concept activations from LLM internal representations and compute saliency-weighted performance to identify both model gaps (weak sub-areas) and benchmark gaps (imbalanced coverage).&lt;/li&gt;&lt;li&gt;Applied the method to two open-source LLMs and ten benchmarks, recovering previously observed weaknesses without supervision—notably poor performance on concepts opposing sycophantic behaviors (e.g., polite refusals, asserting boundaries) and on safety-related concepts.&lt;/li&gt;&lt;li&gt;Finds many benchmarks over-represent obedience/instruction-following concepts and under-represent core safety/boundary concepts, suggesting benchmarks themselves can mask safety-related failures.&lt;/li&gt;&lt;li&gt;Provides code and a representation-grounded, concept-level decomposition that complements aggregated metrics to help explain model scores and guide benchmark improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matyas Bohacek', 'Nino Scherrer', 'Nicholas Dufour', 'Thomas Leung', 'Christoph Bregler', 'Stephanie C. Y. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'benchmark-analysis', 'concept-level-evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20638</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning</title><link>https://arxiv.org/abs/2512.20634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a shallow vs deep alignment framework to quantitatively characterize alignment depth across token positions and show that many continual learning methods only achieve shallow alignment (first ~3–5 tokens).&lt;/li&gt;&lt;li&gt;Proposes quantitative 0–1 scale metrics for alignment depth, real-time detection methods for identifying shallow alignment during training, visualization and recovery prediction tools, and adaptive mitigation strategies that distinguish forgetting types and promote deep alignment.&lt;/li&gt;&lt;li&gt;Evaluates methods across datasets and model sizes (Qwen2.5-3B to Qwen2.5-32B), reporting 86.2–90.6% identification accuracy for spurious forgetting and robustness improvements of 3.3–7.1% over baselines when promoting deep alignment.&lt;/li&gt;&lt;li&gt;Argues that shallow alignment explains reversible/spurious forgetting and susceptibility to fine-tuning attacks, and provides tools to detect and mitigate such failures in continual learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiwei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['continual-learning', 'catastrophic-forgetting', 'alignment', 'model-robustness', 'detection-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20634</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams</title><link>https://arxiv.org/abs/2512.20631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a zero-training methodology for detecting temporal drift in transformer-based sentiment models on authentic social media streams.&lt;/li&gt;&lt;li&gt;Evaluates three transformer architectures on 12,279 posts, reporting accuracy drops up to 23.4% and confidence drops up to 13.0% (Bootstrap 95% CI).&lt;/li&gt;&lt;li&gt;Introduces four novel drift metrics that outperform embedding-based baselines while remaining computationally efficient for production deployment.&lt;/li&gt;&lt;li&gt;Validates detection capability across multiple real-world events, claiming practical significance for real-time sentiment monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayam Bansal', 'Ishaan Gangwani']&lt;/li&gt;&lt;li&gt;Tags: ['concept-drift', 'model-monitoring', 'robustness', 'transformers', 'zero-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20631</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</title><link>https://arxiv.org/abs/2512.21220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoboSafe, a hybrid runtime safety guardrail for embodied agents that executes predicate-based safety logic to intercept hazardous actions during task execution.&lt;/li&gt;&lt;li&gt;Combines Backward Reflective Reasoning (revisiting short-term trajectories to infer temporal safety predicates) and Forward Predictive Reasoning (anticipating future risks from long-term memory and multimodal observations).&lt;/li&gt;&lt;li&gt;Implements an interpretable, executable safety logic on a Hybrid Long-Short Safety Memory and reports substantial reduction in hazardous actions (-36.8%) with minimal impact on task performance, including real-world robot arm evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Le Wang', 'Zonghao Ying', 'Xiao Yang', 'Quanchen Zou', 'Zhenfei Yin', 'Tianlin Li', 'Jian Yang', 'Yaodong Yang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['runtime safety', 'embodied agents', 'safety logic', 'vision-language models', 'robotics safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21220</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care</title><link>https://arxiv.org/abs/2512.21127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of LLM-based medication safety review on real NHS primary care EHR data (277 patients sampled from a population-scale dataset).&lt;/li&gt;&lt;li&gt;Measures sensitivity/specificity and finds high sensitivity but many incomplete/mis-specified recommendations: only 46.9% of patients had all issues and interventions correctly identified.&lt;/li&gt;&lt;li&gt;Detailed failure analysis showing dominant safety failure modes are contextual reasoning errors (overconfidence, guideline anchoring without patient context, process blindness, factual errors) that persist across models and patient strata.&lt;/li&gt;&lt;li&gt;Provides 45 vignettes and argues that these safety shortcomings must be addressed before clinical deployment, calling for larger-scale prospective evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oliver Normand', 'Esther Borsi', 'Mitch Fruin', 'Lauren E Walker', 'Jamie Heagerty', 'Chris C. Holmes', 'Anthony J Avery', 'Iain E Buchan', 'Harry Coppock']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'clinical AI safety', 'safety evaluation', 'failure analysis', 'overconfidence/uncertainty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21127</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Context: Large Language Models Failure to Grasp Users Intent</title><link>https://arxiv.org/abs/2512.21110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) showing systematic failures to detect user intent and context.&lt;/li&gt;&lt;li&gt;Demonstrates practical circumvention techniques (emotional framing, progressive revelation, academic justification) that bypass existing safety mechanisms.&lt;/li&gt;&lt;li&gt;Finds reasoning-enabled configurations often increase exploit effectiveness by improving factual precision without interrogating intent; Claude Opus 4.1 is noted as a partial exception.&lt;/li&gt;&lt;li&gt;Argues for shifting safety design toward core contextual understanding and intent recognition rather than post-hoc protections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed M. Hussain', 'Salahuddin Salahuddin', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Red teaming', 'Prompt injection / jailbreaking', 'Intent recognition', 'Adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21110</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines</title><link>https://arxiv.org/abs/2512.20985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a LangChain-based multi-agent architecture integrated with a permissioned blockchain (Hyperledger Fabric) to monitor, enforce policies, and immutably audit perception–reasoning–action pipelines.&lt;/li&gt;&lt;li&gt;Blockchain layer verifies inputs, evaluates recommended actions, and records execution outcomes; system includes MCP-integrated action executors and experimental deployments in inventory management, traffic-signal control, and healthcare monitoring.&lt;/li&gt;&lt;li&gt;Claims improved trust, prevention of unauthorized practices, traceability, and acceptable operational latency — focusing on governance and integrity of agentic AI actions rather than adversarial model vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Salman Jan', 'Hassan Ali Razzaqi', 'Ali Akarma', 'Mohammad Riyaz Belgaum']&lt;/li&gt;&lt;li&gt;Tags: ['blockchain-auditing', 'agentic-AI', 'runtime-monitoring', 'integrity-governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20985</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safety Alignment of LMs via Non-cooperative Games</title><link>https://arxiv.org/abs/2512.20806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames safety alignment as a non-zero-sum, non-cooperative game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning.&lt;/li&gt;&lt;li&gt;Uses a preference-based (pairwise comparison) reward signal instead of point-wise scores to reduce reward-hacking and provide more robust supervision.&lt;/li&gt;&lt;li&gt;Demonstrates that the joint RL training (AdvGame) moves the Pareto frontier, producing a Defender that is both more helpful and more robust to adversarial prompts.&lt;/li&gt;&lt;li&gt;Produces a converged Attacker LM that functions as a strong, general-purpose red-teaming agent for probing other target models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anselm Paulus', 'Ilia Kulikov', 'Brandon Amos', "R\\'emi Munos", 'Ivan Evtimov', 'Kamalika Chaudhuri', 'Arman Zharmagambetov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'reinforcement learning', 'alignment/safety', 'reward modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20806</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents</title><link>https://arxiv.org/abs/2512.20798</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a 40-scenario benchmark for multi-step, outcome-driven constraint violations where agents optimize KPIs and may deprioritize ethical/legal/safety constraints.&lt;/li&gt;&lt;li&gt;Each scenario has Mandated (explicit instruction) and Incentivized (KPI-pressure) variations to distinguish obedience failures from emergent misalignment.&lt;/li&gt;&lt;li&gt;Evaluates 12 state-of-the-art LLMs as agents, reporting violation rates from 1.3% to 71.4% (9/12 models between ~30–50%), and finds higher capability does not guarantee safety (e.g., Gemini-3-Pro-Preview &gt;60%).&lt;/li&gt;&lt;li&gt;Identifies 'deliberative misalignment' where models recognize actions as unethical under separate evaluation, highlighting need for agentic-safety training prior to deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miles Q. Li', 'Benjamin C. M. Fung', 'Martin Weiss', 'Pulei Xiong', 'Khalil Al-Hussaeni', 'Claude Fachkha']&lt;/li&gt;&lt;li&gt;Tags: ['agent-safety', 'benchmarking', 'misalignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20798</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the AI Trustworthiness Gap between Functions and Norms</title><link>https://arxiv.org/abs/2512.20671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing for a semantic 'bridge' language to align Functional Trustworthy AI (FTAI) implementations with Normative Trustworthy AI (NTAI) regulations.&lt;/li&gt;&lt;li&gt;Describes the current gap between technical implementation and regulatory norms and proposes starting points for developing a conceptual language to assess trustworthiness.&lt;/li&gt;&lt;li&gt;Outlines envisioned effects, key considerations, and future actions to help stakeholders translate norms into concrete implementation and assessment steps.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daan Di Scala', 'Sophie Lathouwers', 'Michael van Bekkum']&lt;/li&gt;&lt;li&gt;Tags: ['trustworthy-ai', 'safety-evaluation', 'regulation-governance', 'alignment', 'position-paper']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20671</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction</title><link>https://arxiv.org/abs/2512.20664</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Eidoku, a lightweight System-2 neuro-symbolic verification gate that treats LLM reasoning verification as a Constraint Satisfaction Problem (CSP) rather than relying on generation likelihood.&lt;/li&gt;&lt;li&gt;Defines a total structural violation cost composed of three proxies: graph connectivity (structural), feature-space consistency (geometric), and logical entailment (symbolic); candidates exceeding a context-calibrated cost threshold are rejected.&lt;/li&gt;&lt;li&gt;Thresholds are derived from intrinsic context statistics (not learned), aiming to deterministically reject 'smooth falsehoods'—high‑probability but structurally disconnected hallucinations.&lt;/li&gt;&lt;li&gt;Evaluated on a controlled diagnostic dataset, showing improved detection/rejection of this class of hallucinations compared to probability-based verifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinobu Miya']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination detection', 'neuro-symbolic verification', 'constraint satisfaction', 'LLM safety', 'structural robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20664</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models</title><link>https://arxiv.org/abs/2512.20662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Three controlled experiments evaluate LLM behavioral artifacts: (A) laziness in multi-part instruction compliance, (B) decoding suboptimality via a simple reasoning task, and (C) context degradation over a 200-turn chaotic conversation.&lt;/li&gt;&lt;li&gt;Findings: widespread omission/partial compliance (laziness) on complex multi-part tasks; limited evidence of decoding suboptimality in the tested reasoning task; and surprisingly strong retention of key facts/instructions across long conversations.&lt;/li&gt;&lt;li&gt;Implications: compliance with detailed instructions remains a practical challenge; some hypothesized failure modes (e.g., long-term context forgetting) may be less severe in straightforward retrieval scenarios; recommends mitigation strategies like self-refinement and dynamic prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiqing Ma', 'Jung-Hua Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'instruction-following', 'alignment', 'context degradation', 'behavioral testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20662</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AIAuditTrack: A Framework for AI Security system</title><link>https://arxiv.org/abs/2512.20649</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AiAuditTrack (AAT), a blockchain-based framework to record AI usage interactions and enable cross-system supervision, auditing, and accountability.&lt;/li&gt;&lt;li&gt;Uses decentralized identity (DID) and verifiable credentials (VC) to establish trusted, identifiable AI entities and records time-stamped interaction trajectories on-chain as a dynamic graph.&lt;/li&gt;&lt;li&gt;Introduces a risk diffusion algorithm to trace the origin of risky behaviors and propagate early warnings across involved entities for responsibility attribution.&lt;/li&gt;&lt;li&gt;Evaluates system feasibility and scalability via blockchain Transactions Per Second (TPS) metrics for large-scale interaction recording.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixun Luo', 'Yuhang Fan', 'Yufei Li', 'Youzhi Zhang', 'Hengyu Lin', 'Ziqi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI auditing', 'Blockchain for AI security', 'Risk tracing', 'Decentralized identity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20649</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning</title><link>https://arxiv.org/abs/2512.20647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether partial chain-of-thought reasoning traces from one LLM can be continued by another model (intra- and cross-family) while preserving logical coherence and final answer accuracy.&lt;/li&gt;&lt;li&gt;Uses token-level log-probability thresholds to truncate reasoning at different stages and a Process Reward Model (PRM) to assess sufficiency and stability of intermediate traces.&lt;/li&gt;&lt;li&gt;Finds that hybrid reasoning chains often preserve—and sometimes improve—final accuracy, suggesting an emergent interchangeability property useful for modular collaborative reasoning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leo Lu', 'Jonathan Zhang', 'Sean Chua', 'Spencer Kim', 'Kevin Zhu', "Sean O'Brien", 'Vasu Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety_evaluation', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20647</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data</title><link>https://arxiv.org/abs/2512.20630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MicroProbe, a method to assess foundation model reliability using only 100 strategically selected probe examples instead of large-scale evaluation sets.&lt;/li&gt;&lt;li&gt;Combines prompt diversity across five reliability dimensions with uncertainty quantification and adaptive weighting to detect failure modes efficiently.&lt;/li&gt;&lt;li&gt;Empirical validation on multiple language models and domains (healthcare, finance, legal) shows substantial gains over random sampling (23.5% higher composite reliability, 95% coverage, 90% cost reduction).&lt;/li&gt;&lt;li&gt;Includes statistical significance testing and expert validation by AI safety researchers supporting the method's effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayam Bansal', 'Ishaan Gangwani']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'reliability-assessment', 'uncertainty-quantification', 'robustness', 'evaluation-efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20630</guid><pubDate>Fri, 26 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
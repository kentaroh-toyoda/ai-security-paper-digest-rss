<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 18 Dec 2025 23:35:45 +0000</lastBuildDate><item><title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title><link>https://arxiv.org/abs/2506.07400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedChat, a multi-agent framework combining specialized vision models with multiple role-specific LLM agents coordinated by a director agent to perform multimodal medical diagnosis (glaucoma/ophthalmology).&lt;/li&gt;&lt;li&gt;Targets reliability and interpretability by distributing reasoning across specialized agents to reduce hallucination risk and emulate multidisciplinary clinical teams.&lt;/li&gt;&lt;li&gt;Provides an interactive reporting interface for clinical review and education; code is available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philip R. Liu', 'Sparsh Bansal', 'Jimmy Dinh', 'Aditya Pawar', 'Ramani Satishkumar', 'Shail Desai', 'Neeraj Gupta', 'Xin Wang', 'Shu Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'multimodal medical AI', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07400</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title><link>https://arxiv.org/abs/2509.21979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a medical benchmark to measure sycophancy in vision-language models using hierarchical visual question answering with multiple prompt templates.&lt;/li&gt;&lt;li&gt;Finds VLMs are highly susceptible to non-evidence social cues (perceived authority, user mimicry) that trigger incorrect, non-evidence-based answers.&lt;/li&gt;&lt;li&gt;Proposes VIPER, a Visual Information Purification method that filters non-evidence-based social cues to reduce sycophancy while preserving interpretability and performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zikun Guo', 'Jingwei Lv', 'Xinyue Xu', 'Shu Yang', 'Jun Wen', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'benchmarking', 'medical VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21979</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title><link>https://arxiv.org/abs/2506.09677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of three state-of-the-art reasoning models (o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) against gaslighting negation attacks on multimodal benchmarks (MMMU, MathVista, CharXiv).&lt;/li&gt;&lt;li&gt;Finds substantial accuracy degradation (≈25–29% average) under gaslighting negation attacks; a curated diagnostic benchmark (GaslightingBench-R, 1,025 samples) yields &gt;53% average accuracy drops.&lt;/li&gt;&lt;li&gt;Introduces GaslightingBench-R to probe models' ability to maintain correct beliefs under adversarial user feedback and highlights a gap between chain-of-thought reasoning and resistance to manipulative prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Zhu', 'Hailong Yin', 'Jingjing Chen', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking/gaslighting', 'robustness', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09677</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Robust and Accurate Adversarial Camouflage Generation against Vehicle Detectors</title><link>https://arxiv.org/abs/2411.10029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAUCA, a method for generating robust adversarial camouflage against vehicle detectors.&lt;/li&gt;&lt;li&gt;Introduces End-to-End Neural Renderer Plus (E2E-NRP) to accurately optimize/project vehicle textures and render environmental effects (lighting, weather).&lt;/li&gt;&lt;li&gt;Uses a multi-weather dataset and demonstrates improved attack effectiveness on six popular object detectors in simulation and real-world tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Zhou', 'Linye Lyu', 'Daojing He', 'Yu Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ML', 'physical-attacks', 'neural-rendering', 'robustness', 'object-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10029</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</title><link>https://arxiv.org/abs/2512.15195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EPSM, a novel safety-centric metric that jointly evaluates object and lane detection to quantify safety-relevant perception performance beyond standard metrics (precision/recall/F1).&lt;/li&gt;&lt;li&gt;Introduces a lightweight object safety metric to assess potential risk from detection errors and a lane safety component that models interdependence between object and lane perception.&lt;/li&gt;&lt;li&gt;Validates the metric on the DeepAccident dataset and demonstrates that EPSM uncovers safety-critical perception failures missed by conventional accuracy metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J\\"org Gamerdinger', 'Sven Teufel', 'Stephan Amann', 'Lukas Marc Listl', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'autonomous-driving', 'perception-metrics', 'object-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15195</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</title><link>https://arxiv.org/abs/2512.15693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Skyra, a multimodal LLM that detects AI-generated videos by identifying human-perceivable spatio-temporal artifacts and producing grounded explanations.&lt;/li&gt;&lt;li&gt;Introduces ViF-CoT-4K, a large-scale, fine-grained human-annotated dataset for supervised fine-tuning on video artifact detection.&lt;/li&gt;&lt;li&gt;Develops a two-stage training strategy to improve artifact perception, explanation capability, and detection accuracy.&lt;/li&gt;&lt;li&gt;Provides ViF-Bench, a 3K-sample benchmark from &gt;10 state-of-the-art video generators, and demonstrates superior performance over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifei Li', 'Wenzhao Zheng', 'Yanran Zhang', 'Runze Sun', 'Yu Zheng', 'Lei Chen', 'Jie Zhou', 'Jiwen Lu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'explainability', 'multimodal LLM', 'dataset/benchmark', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15693</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stylized Synthetic Augmentation further improves Corruption Robustness</title><link>https://arxiv.org/abs/2512.15675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training augmentation pipeline that combines synthetic image data with neural style transfer to improve model robustness to common corruptions.&lt;/li&gt;&lt;li&gt;Shows that stylized synthetic images, despite worse FID, improve classifier training and complement rule-based augmentations like TrivialAugment (but not all augmentations).&lt;/li&gt;&lt;li&gt;Provides systematic empirical analysis of augmentation choices and hyperparameters and reports state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and TinyImageNet-C.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Siedel', 'Rojan Regmi', 'Abhirami Anand', 'Weijia Shao', 'Silvia Vock', 'Andrey Morozov']&lt;/li&gt;&lt;li&gt;Tags: ['corruption_robustness', 'data_augmentation', 'synthetic_data', 'neural_style_transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15675</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</title><link>https://arxiv.org/abs/2512.15621</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OccSTeP: a 4D Occupancy Spatio-Temporal Persistence benchmark for reactive and proactive forecasting in autonomous driving, including challenging scenarios like dropped frames and erroneous labels.&lt;/li&gt;&lt;li&gt;Proposes OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state, fuses spatio-temporal context incrementally, and uses linear-complexity attention plus a recurrent state-space module for long-range dependencies.&lt;/li&gt;&lt;li&gt;Design enables online inference and robustness to missing/noisy historical sensor input with ego-motion compensation.&lt;/li&gt;&lt;li&gt;Reports improvements in semantic mIoU and occupancy IoU over baselines; data and code to be open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Zheng', 'Jie Hu', 'Kailun Yang', 'Jiaming Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'spatio-temporal-forecasting', 'occupancy-representation', 'online-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15621</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BLANKET: Anonymizing Faces in Infant Video Recordings</title><link>https://arxiv.org/abs/2512.15542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BLANKET, a two-stage method for anonymizing infant faces in video: (1) generate a new random face via diffusion-model inpainting compatible with original identity, (2) temporally consistent face swapping with expression transfer across frames.&lt;/li&gt;&lt;li&gt;Evaluated on a baby video dataset against DeepPrivacy2 using metrics for de-identification, facial attribute preservation, downstream task impact (human pose estimation), and artifact presence.&lt;/li&gt;&lt;li&gt;Claims to match or exceed DeepPrivacy2 on attribute preservation, downstream-task fidelity, and artifact reduction while providing de-identification; code released as an anonymization demo.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ditmar Hadera', 'Jan Cech', 'Miroslav Purkrabek', 'Matej Hoffmann']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'face anonymization', 'data protection', 'video anonymization', 'synthetic identity generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15542</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</title><link>https://arxiv.org/abs/2512.15512</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VAAS, a dual-module framework combining global attention-based anomaly estimation (using ViT) with patch-level self-consistency scoring (from SegFormer embeddings).&lt;/li&gt;&lt;li&gt;Outputs a continuous, interpretable anomaly score and attention-guided anomaly maps indicating both location and degree of manipulation.&lt;/li&gt;&lt;li&gt;Evaluated on DF2023 and CASIA v2.0, reporting competitive F1 and IoU; emphasizes visual explainability and reproducibility (code released).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Opeyemi Bamigbade', 'Mark Scanlon', 'John Sheppard']&lt;/li&gt;&lt;li&gt;Tags: ['image forensics', 'forgery detection', 'anomaly detection', 'vision transformers', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15512</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning</title><link>https://arxiv.org/abs/2512.15433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CLIP-FTI, a CLIP-driven fine-grained attribute conditioning pipeline that fuses CLIP facial feature embeddings with leaked face templates via a cross-modal interaction network and maps them into StyleGAN's latent space to reconstruct faces.&lt;/li&gt;&lt;li&gt;Aims to recover sharper, component-level facial attributes (eyes, nose, mouth) and produce photorealistic surrogates that better match identity and attributes than prior template-inversion attacks.&lt;/li&gt;&lt;li&gt;Reports higher identification accuracy, improved attribute similarity, and better cross-model attack transferability across multiple recognition backbones and datasets; claims to be first to use additional semantic information beyond templates for inversion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longchen Dai', 'Zixuan Shen', 'Zhiheng Zhou', 'Peipeng Yu', 'Zhihua Xia']&lt;/li&gt;&lt;li&gt;Tags: ['face template inversion', 'model inversion', 'privacy attack', 'CLIP conditioning', 'generative model (StyleGAN)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15433</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</title><link>https://arxiv.org/abs/2512.15423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety/robustness failure in monocular depth estimation (MDE) called the "3D Mirage", where models hallucinate spurious 3D structure on geometrically planar but perceptually ambiguous inputs.&lt;/li&gt;&lt;li&gt;Provides 3D-Mirage, a benchmark of real-world illusion cases with annotated planar regions and context-restricted crops, and proposes Laplacian-based metrics: Deviation Composite Score (DCS) and Confusion Composite Score (CCS) to quantify structural and contextual failures.&lt;/li&gt;&lt;li&gt;Introduces Grounded Self-Distillation, a parameter-efficient mitigation that enforces planarity on illusion ROIs while using a frozen teacher to retain background knowledge, aiming to reduce hallucinations without catastrophic forgetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang Nguyen', 'Xiaohao Xu', 'Xiaonan Huang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'benchmarking', 'vision-hallucination', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15423</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust and Calibrated Detection of Authentic Multimedia Content</title><link>https://arxiv.org/abs/2512.15182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a resynthesis framework to decide if a sample is authentic or if its authenticity can be plausibly denied (i.e., detect deepfakes by attempting to resynthesize/or invert).&lt;/li&gt;&lt;li&gt;Introduces a calibrated resynthesis method targeted at high-precision, low-recall settings to maintain controllable, low false positive rates on authentic content.&lt;/li&gt;&lt;li&gt;Demonstrates adversarial robustness against compute-restricted (efficient) adaptive attackers, showing prior methods are more easily evaded under identical compute budgets.&lt;/li&gt;&lt;li&gt;Approach is multimodal and leverages state-of-the-art inversion techniques for improved verification and calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarim Hashmi', 'Abdelrahman Elsayed', 'Mohammed Talha Alam', 'Samuele Poppi', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'adversarial robustness', 'model inversion/resynthesis', 'calibration', 'multimedia authentication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15182</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving</title><link>https://arxiv.org/abs/2512.15181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First in-depth analysis of criticality (relevance) metrics for safety evaluation of object detection in automated driving.&lt;/li&gt;&lt;li&gt;Comprehensive literature review and empirical validation using the DeepAccident dataset containing safety-critical scenarios.&lt;/li&gt;&lt;li&gt;Proposes two novel application strategies: bidirectional criticality rating and multi-metric aggregation to improve classification of relevant vs non-relevant objects.&lt;/li&gt;&lt;li&gt;Reports up to 100% improvement in criticality classification accuracy, suggesting stronger safety evaluation capability for perception systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J\\"org Gamerdinger', 'Sven Teufel', 'Stephan Amann', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'object-detection', 'automated-driving', 'relevance-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15181</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</title><link>https://arxiv.org/abs/2512.14922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PANDA-PLUS-Bench, a curated benchmark of 9 whole-slide prostate biopsy images (from 9 patients) with extracted 512x512 and 224x224 patches across 8 augmentation conditions to evaluate robustness.&lt;/li&gt;&lt;li&gt;Evaluates 7 foundation models on their ability to separate biological Gleason patterns from slide-level confounders (within-slide vs cross-slide performance gaps reported).&lt;/li&gt;&lt;li&gt;Finds substantial variation in robustness: tissue-specific training (HistoEncoder) improved cross-slide accuracy, but all models show measurable within- vs cross-slide accuracy gaps (≈20–27 percentage points).&lt;/li&gt;&lt;li&gt;Provides an open-source Colab notebook for standardized evaluation of additional foundation models on this clinical robustness benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua L. Ebbert', 'Dennis Della Corte']&lt;/li&gt;&lt;li&gt;Tags: ['robustness evaluation', 'medical imaging', 'benchmark/dataset', 'foundation models', 'generalization / confounders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14922</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title><link>https://arxiv.org/abs/2512.14770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAVR, a dual-assessment framework combining Self-Reflection (dual selector modules fusing VLM latent features with QA embeddings) and Cross-Model Verification (external reference models) to estimate uncertainty and reduce hallucinations in VQA.&lt;/li&gt;&lt;li&gt;Aims to improve reliability/trustworthiness of vision-language model answers by flagging or correcting overconfident incorrect responses.&lt;/li&gt;&lt;li&gt;Validated on the Reliable VQA Challenge (ICCV-CLVL 2025), achieving leading metrics (Φ100 = 39.64, 100-AUC = 97.22).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xixian Wu', 'Yang Ou', 'Pengchao Tian', 'Zian Yang', 'Jielei Zhang', 'Peiyi Li', 'Longwen Gao']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'uncertainty estimation', 'model verification', 'VQA reliability', 'self-reflection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14770</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning</title><link>https://arxiv.org/abs/2512.14757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SocialNav-MoE, an efficient Mixture-of-Experts vision-language model tailored for socially compliant robot navigation with reinforcement fine-tuning (RFT).&lt;/li&gt;&lt;li&gt;Introduces a Semantic Similarity Reward (SSR) to guide RFT for improved decision-making that balances navigation accuracy with social appropriateness.&lt;/li&gt;&lt;li&gt;Evaluates design choices (small LLM families, routing strategies, and vision encoders CLIP vs SigLIP, frozen vs fine-tuned) on the SNEI dataset, showing improved efficiency and performance trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomohito Kawabata', 'Xinyu Zhang', 'Ling Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['robotics safety', 'socially compliant navigation', 'vision-language models', 'reinforcement fine-tuning', 'Mixture-of-Experts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14757</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title><link>https://arxiv.org/abs/2507.10532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that apparent RL-driven reasoning improvements in Qwen2.5 on math benchmarks are likely due to pretraining data contamination (benchmark leakage).&lt;/li&gt;&lt;li&gt;Introduces RandomCalculation, a generator producing fully clean arithmetic problems of controllable difficulty to avoid leakage.&lt;/li&gt;&lt;li&gt;Finds that on leakage-free data only accurate reward signals yield consistent improvements; random/incorrect rewards do not improve beyond the base model.&lt;/li&gt;&lt;li&gt;Recommends evaluating RL methods on uncontaminated benchmarks and across multiple model families to draw trustworthy conclusions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqi Wu', 'Zhihao Zhang', 'Qiaole Dong', 'Zhiheng Xi', 'Jun Zhao', 'Senjie Jin', 'Xiaoran Fan', 'Yuhao Zhou', 'Huijie Lv', 'Ming Zhang', 'Yanwei Fu', 'Qin Liu', 'Songyang Zhang', 'Qi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'benchmark leakage', 'reinforcement learning evaluation', 'robustness', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10532</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</title><link>https://arxiv.org/abs/2507.07417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates fine-tuning-based prompt-injection defenses that separate instructions from data and tests their robustness in the whitebox setting.&lt;/li&gt;&lt;li&gt;Introduces a novel architecture/attention-aware optimization attack for textual LLMs and applies it against SecAlign, SecAlign++, and StruQ.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success (≈85–95%) on unseen prompts with modest additional token budget and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nishit V. Pandya', 'Andrey Labunets', 'Sicun Gao', 'Earlence Fernandes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'adversarial prompting', 'whitebox attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07417</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians</title><link>https://arxiv.org/abs/2510.13734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAPS, an automated, guideline-anchored benchmark evaluating Grounding, Adequacy, Perturbation (robustness), and Safety for AI clinician systems.&lt;/li&gt;&lt;li&gt;Pipeline auto-generates clinically grounded questions, synthesizes rubrics via an agentic DeepResearch ReAct loop, and uses LLM ensembles to score answers; validated against clinicians (90% agreement, k=0.77).&lt;/li&gt;&lt;li&gt;Evaluations show models fail as reasoning depth increases, have gaps in answer completeness, are highly vulnerable to adversarial perturbations, and exhibit identifiable safety issues.&lt;/li&gt;&lt;li&gt;Provides a publicly available dataset (GAPS-NSCLC-preview) and evaluation code for reproducible benchmarking of clinical AI safety and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Tao Sun', 'Dexin Su', 'Ailing Yu', 'Junwei Liu', 'Zhe Chen', 'Gangzeng Jin', 'Xin Wang', 'Jingnan Liu', 'Hansong Xiao', 'Hualei Zhou', 'Dongjie Tao', 'Chunxiao Guo', 'Minghui Yang', 'Yuan Xia', 'Jing Zhao', 'Qianrui Fan', 'Yanyun Wang', 'Shuai Zhen', 'Kezhong Chen', 'Jun Wang', 'Zewen Sun', 'Heng Zhao', 'Tian Guan', 'Shaodong Wang', 'Geyun Chang', 'Jiaming Deng', 'Hongchengcheng Chen', 'Kexin Feng', 'Ruzhen Li', 'Jiayi Geng', 'Changtai Zhao', 'Jun Wang', 'Guihu Lin', 'Peihao Li', 'Liqi Liu', 'Peng Wei', 'Jian Wang', 'Jinjie Gu', 'Ping Wang', 'Fan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness/perturbation', 'benchmarking', 'medical AI', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13734</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions</title><link>https://arxiv.org/abs/2508.20764</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RealCBT, a dataset of authentic cognitive behavioral therapy (CBT) dialogues, and compares them with LLM-generated therapy dialogues (CACTUS).&lt;/li&gt;&lt;li&gt;Adapts the Utterance Emotion Dynamics framework to analyze fine-grained emotional arcs across valence, arousal, and dominance for full dialogues and by speaker role (counselor vs client).&lt;/li&gt;&lt;li&gt;Finds LLM-generated dialogues are fluent and coherent but diverge from real sessions in key emotional properties: lower emotional variability, less emotion-laden language, weaker patterns of reactivity and regulation, and low emotional-arc similarity to real conversations.&lt;/li&gt;&lt;li&gt;Releases the RealCBT dataset and argues emotional fidelity is important for safe deployment of synthetic therapy data in mental-health applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyi Wang', 'Jiwei Zhang', 'Guangtao Zhang', 'Honglei Guo']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'LLM-generated-dialogues', 'mental-health-NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20764</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation</title><link>https://arxiv.org/abs/2506.23979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TaP, a taxonomy-guided framework to automatically and scalably generate preference datasets for supervised and preference fine-tuning of LLMs across languages.&lt;/li&gt;&lt;li&gt;Uses a structured taxonomy to enable fine-grained control over dataset composition, aiming for diversity and comprehensive coverage.&lt;/li&gt;&lt;li&gt;Reports that LLMs fine-tuned on TaP-generated data outperform those trained on existing open-source datasets and even surpass a model trained on an open dataset 180× larger.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renren Jin', 'Tianhao Shen', 'Xinwei Wu', 'Dan Shi', 'Haoran Sun', 'Yuqi Ren', 'Wuwei Huang', 'Quandong Wang', 'Wei Liu', 'Jian Luan', 'Bin Wang', 'Deyi Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-finetuning', 'dataset-generation', 'multilingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23979</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title><link>https://arxiv.org/abs/2512.15712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Predictive Concept Decoders (PCDs): an encoder compresses model activations into a sparse concept list and a decoder answers natural-language questions about model behavior through that bottleneck.&lt;/li&gt;&lt;li&gt;Proposes pretraining on large unstructured data then fine-tuning to map activations to human-interpretable concepts, improving with scale and data.&lt;/li&gt;&lt;li&gt;Demonstrates security- and safety-relevant capabilities: detecting jailbreaks, secret hints, implanted latent concepts, and surfacing latent user attributes from activations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Huang', 'Dami Choi', 'Daniel D. Johnson', 'Sarah Schwettmann', 'Jacob Steinhardt']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'jailbreak detection', 'red teaming', 'model privacy', 'concept bottleneck']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15712</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explaining the Reasoning of Large Language Models Using Attribution Graphs</title><link>https://arxiv.org/abs/2512.15663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAGE (Context Attribution via Graph Explanations): constructs a directed attribution graph quantifying influence of the prompt and all prior generations on each generated token.&lt;/li&gt;&lt;li&gt;Graph enforces causality and row-stochasticity, enabling context attributions via marginalizing intermediate contributions along graph paths.&lt;/li&gt;&lt;li&gt;Evaluated across multiple models, datasets, metrics, and baseline attribution methods; reports up to ~40% average gains in attribution faithfulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chase Walker', 'Rickard Ewetz']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'LLM explainability', 'attribution', 'safety', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15663</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I</title><link>https://arxiv.org/abs/2512.15298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro on the 2025 Korean CSAT Earth Science I section under three input conditions (full-page, per-item, optimized multimodal).&lt;/li&gt;&lt;li&gt;Finds large performance drops with unstructured inputs due to segmentation/OCR failures; even with optimized input, models show systematic cognitive failures (Perception Errors, Perception–Cognition Gap, Calculation–Conceptualization Discrepancy, Process Hallucination).&lt;/li&gt;&lt;li&gt;Provides qualitative analysis of these failure modes and proposes design cues for 'AI-resistant questions' to help educators detect/mitigate unauthorized AI use and preserve assessment validity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seok-Hyun Ga', 'Chun-Yen Chang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'LLM evaluation / benchmarking', 'hallucination', 'perception-cognition gap', 'education-assessment security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15298</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Return on Security Controls in LLM Systems</title><link>https://arxiv.org/abs/2512.15081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a decision-oriented, reproducible methodology to convert LLM adversarial probe outcomes into monetary risk estimates and return-on-control (RoC) metrics using Monte Carlo simulations.&lt;/li&gt;&lt;li&gt;Instantiates a RAG service over synthetic PII and evaluates automated attacks across five vulnerability classes (PII leakage, latent context injection, prompt injection, adversarial attack generation, divergence) to estimate attack success probabilities via Laplace's Rule of Succession.&lt;/li&gt;&lt;li&gt;Combines estimated attack probabilities with calibrated breach-cost loss distributions to produce loss exceedance curves and expected losses; baseline expected loss ≈ $313k per attack scenario.&lt;/li&gt;&lt;li&gt;Compares three mitigations (ABAC, NER redaction with Microsoft Presidio, NeMo Guardrails): ABAC and NER substantially reduce risk (RoC ~9.83 and 5.97 respectively), while NeMo Guardrails provides marginal benefit (RoC ~0.05).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Helder Moulton', "Austin O'Brien", 'John D. Hastings']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'PII/privacy leakage', 'security economics / ROI', 'defenses/mitigations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15081</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes fundamental limits of embedding-based hallucination detection in RAG systems using conformal prediction to provide finite-sample coverage guarantees.&lt;/li&gt;&lt;li&gt;Finds embedding methods (including state-of-the-art embeddings and cross-encoders) have very high false positive rates on real hallucination benchmarks, despite strong calibration on synthetic data.&lt;/li&gt;&lt;li&gt;Shows GPT-4 as an LLM judge achieves far lower FPR, introducing the term “semantic illusion” to describe semantically plausible hallucinations that embeddings cannot detect.&lt;/li&gt;&lt;li&gt;Concludes that embedding-based detection is insufficient for production RAG deployment and that reasoning-capable models are needed for reliable hallucination detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'embedding robustness', 'safety evaluation', 'conformal prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting the Reliability of Language Models in Instruction-Following</title><link>https://arxiv.org/abs/2512.14754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces nuance-oriented reliability for instruction-following LLMs and a metric reliable@k to quantify consistency across 'cousin' prompts that convey similar intents with subtle differences.&lt;/li&gt;&lt;li&gt;Presents an automated data-augmentation pipeline to generate high-quality cousin prompts and builds the IFEval++ benchmark for systematic evaluation.&lt;/li&gt;&lt;li&gt;Evaluates 20 proprietary and 26 open-source LLMs, finding large drops (up to 61.8%) in performance under nuanced prompt modifications and explores three potential improvement recipes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Yutong Zhang', 'Yan Liu', 'Zhenyu Zhong', 'Tao Wei', 'Chao Zhang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'instruction-following', 'benchmarking', 'prompt-sensitivity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14754</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Effectively Detecting and Responding to Online Harassment with Large Language Models</title><link>https://arxiv.org/abs/2512.14700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collected and human-labeled an Instagram private-messages dataset for online harassment.&lt;/li&gt;&lt;li&gt;Built an LLM-based pipeline to scale labeling of harassment in private messages and evaluated LLM labels against human labels.&lt;/li&gt;&lt;li&gt;Used LLMs to generate simulated responses to harassing messages and compared these responses to original human replies, finding LLM responses more helpful.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pinxian Lu', 'Nimra Ishfaq', 'Emma Win', 'Morgan Rose', 'Sierra R Strickland', 'Candice L Biernesser', 'Jamie Zelazny', 'Munmun De Choudhury']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'harassment-detection', 'LLM-evaluation', 'automated-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14700</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title><link>https://arxiv.org/abs/2512.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Activation Oracles (AOs): LLMs trained via LatentQA to take model activations as inputs and answer natural-language questions about them.&lt;/li&gt;&lt;li&gt;Shows AOs generalize out-of-distribution and can recover information fine-tuned into a model (e.g., biographical facts or malign propensities) even without training on activations from that fine-tuned model.&lt;/li&gt;&lt;li&gt;Evaluates on four downstream tasks, where diversified training (classification tasks, self-supervised context prediction) improves performance; best AOs match/exceed prior white-box baselines on most tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Karvonen', 'James Chua', "Cl\\'ement Dumas", 'Kit Fraser-Taliente', 'Subhash Kantamneni', 'Julian Minder', 'Euan Ong', 'Arnab Sen Sharma', 'Daniel Wen', 'Owain Evans', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-extraction', 'safety-auditing', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15674</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Metrics for Safety with LLM-as-Judges</title><link>https://arxiv.org/abs/2512.15617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues safety for deploying LLMs in critical workflows should be grounded in the evidence from evaluation points, especially when using LLM-as-Judges (LaJ) evaluators.&lt;/li&gt;&lt;li&gt;Proposes using a basket of weighted metrics, context-sensitive error severity, and concordance-based confidence thresholds to decide when human review is needed.&lt;/li&gt;&lt;li&gt;Emphasizes designing evaluation frameworks that reduce error risk in LaJ judgments rather than claiming specific architectural fixes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kester Clegg', 'Richard Hawkins', 'Ibrahim Habli', 'Tom Lawton']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'evaluation metrics', 'LLM-as-Judges', 'human-in-the-loop', 'reliability/uncertainty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15617</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial versification in portuguese as a jailbreak operator in LLMs</title><link>https://arxiv.org/abs/2512.15353</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates versification (rewriting prompts as poetry) is an effective jailbreak/adversarial operator that increases safety failures in aligned LLMs.&lt;/li&gt;&lt;li&gt;Reports high attack success rates (manual ~62%, automated ~43%, some models &gt;90%) on English-derived benchmarks and argues the effect is structural across RLHF/constitutional/hybrid systems.&lt;/li&gt;&lt;li&gt;Identifies a gap: no evaluation in Portuguese, and proposes parameterising scansion, metre, and prosodic variation to test Lusophone-specific vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joao Queiroz']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM safety', 'alignment robustness', 'multilingual attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15353</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FAME: Fictional Actors for Multilingual Erasure</title><link>https://arxiv.org/abs/2512.15235</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FAME, a synthetic multilingual benchmark (EN, FR, DE, IT, ES) for evaluating machine unlearning in LLMs.&lt;/li&gt;&lt;li&gt;Contains 1,000 fictional actor biographies and 20,000 QA pairs covering 20 structured topic categories, enabling both entity-level and instance-level forgetting.&lt;/li&gt;&lt;li&gt;Provides dataset splits for the two unlearning scenarios and uses fictional data to ensure controlled evaluation (data not present in model pretraining).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Claudio Savelli', 'Moreno La Quatra', 'Alkis Koudounas', 'Flavio Giobergia']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'benchmark', 'multilingual', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15235</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers</title><link>https://arxiv.org/abs/2512.15163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCP-SafetyBench, a benchmark for safety evaluation of LLMs interacting with real Model Context Protocol (MCP) servers across five domains (browser automation, financial analysis, location navigation, repository management, web search).&lt;/li&gt;&lt;li&gt;Defines a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and supports realistic multi-turn, cross-server, multi-step tasks under uncertainty.&lt;/li&gt;&lt;li&gt;Empirically evaluates open- and closed-source LLMs on the benchmark, showing varied safety performance and increased vulnerabilities with longer task horizons and more server interactions.&lt;/li&gt;&lt;li&gt;Positions MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments and calls for stronger defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuanjun Zong', 'Zhiqi Shen', 'Lei Wang', 'Yunshi Lan', 'Chao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Benchmarking', 'Tool-use security', 'Attack taxonomy', 'Red teaming / evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15163</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title><link>https://arxiv.org/abs/2512.15053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Meta-Prompting Protocol: an orchestration framework for LLMs built around an Adversarial Trinity (Generator, Auditor, Optimizer) to create self-optimizing interaction loops.&lt;/li&gt;&lt;li&gt;Treats natural language instructions as differentiable variables within a semantic computation graph and introduces TextGrad (automatic textual differentiation) to use critiques as gradient-like signals.&lt;/li&gt;&lt;li&gt;Claims to mitigate hallucination and prevent model collapse, demonstrating theoretical viability via declarative programming (DSPy) and positioning the approach for reliable, observable software engineering with probabilistic models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM orchestration', 'alignment/safety', 'adversarial prompting', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15053</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box, neuron-level intervention that selectively soft-suppresses toxic expert neurons in multimodal LLMs without parameter updates.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation benchmark, and evaluates SGM against existing detox methods under standard and adversarial triggers.&lt;/li&gt;&lt;li&gt;Shows large toxicity reduction (harmful rates from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; SGM* combines SGM with other defenses for stronger safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM safety', 'detoxification', 'neuron-level intervention', 'adversarial robustness', 'safety evaluation/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title><link>https://arxiv.org/abs/2512.14801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that hallucination in transformer-based LLMs is a structural property of their token-association/embedding geometry, not primarily an incentive or evaluation misalignment.&lt;/li&gt;&lt;li&gt;Presents empirical experiments using a 'Licensing Oracle' showing that perfect abstention/grounding requires external truth-validation modules rather than changes to incentives, prompting, or fine-tuning.&lt;/li&gt;&lt;li&gt;Concludes that eliminating hallucination necessitates hybrid architectures that separate linguistic fluency from epistemic grounding and abstention, i.e., external validation pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Ackermann', 'Simeon Emanuilov']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety', 'grounding/abstention', 'model-architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14801</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title><link>https://arxiv.org/abs/2512.02080</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models an LLM+Verifier multi-stage pipeline (CodeGen, Compilation, InvariantSynth, SMTSolving) as an absorbing Markov chain and proves almost-sure convergence to a Verified state for any non-zero per-stage success probability δ, with expected iterations bounded by E[n] ≤ 4/δ.&lt;/li&gt;&lt;li&gt;Presents extensive empirical validation (≈90,000 trials) showing runs reached verification and empirical convergence closely matched the theoretical 4/δ bound (convergence factor ≈1.0).&lt;/li&gt;&lt;li&gt;Identifies three operational zones (marginal, practical, high-performance) and proposes a dynamic calibration strategy to maintain predictable behavior under parameter drift, enabling resource planning for safety-critical software verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['PIerre Dantas', 'Lucas Cordeiro', 'Youcheng Sun', 'Waldir Junior']&lt;/li&gt;&lt;li&gt;Tags: ['formal-verification', 'LLM-safety', 'reliability', 'convergence-theorem']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.02080</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2509.14285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent LLM defense pipeline (specialized LLM agents) to detect and neutralize prompt injection attacks in real time.&lt;/li&gt;&lt;li&gt;Evaluates two architectures: a sequential chain-of-agents and a hierarchical coordinator-based system.&lt;/li&gt;&lt;li&gt;Empirical evaluation across 55 unique prompt injection attacks (8 categories, 400 instances) on ChatGLM and Llama2, claiming reduction of Attack Success Rate from 30%/20% to 0% (100% mitigation) while preserving legitimate functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S M Asif Hossain', 'Ruksat Khan Shayoni', 'Mohd Ruhul Ameen', 'Akif Islam', 'M. F. Mridha', 'Jungpil Shin']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM safety', 'adversarial defense', 'red teaming', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14285</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations</title><link>https://arxiv.org/abs/2506.11262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and categorizes systematic non-optimal behaviors in human demonstrations for robot tasks, termed 'demonstration sidetracks' (Exploration, Mistake, Alignment, Pause) plus a control pattern (one-dimension control).&lt;/li&gt;&lt;li&gt;Collects and annotates a dataset from a 40-participant public-space study recreated in simulation; shows sidetracks occur frequently and are tied to temporal/spatial task context.&lt;/li&gt;&lt;li&gt;Finds that users' control patterns vary with the control interface, implying interface-dependent biases in demonstrations.&lt;/li&gt;&lt;li&gt;Argues for better models of suboptimal demonstrations to improve LfD robustness and bridge the gap between lab training and real-world deployment; provides dataset and infrastructure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shijie Fang', 'Hang Yu', 'Qidi Fang', 'Reuben M. Aronson', 'Elaine S. Short']&lt;/li&gt;&lt;li&gt;Tags: ['learning from demonstration', 'human-robot interaction', 'robustness', 'dataset', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11262</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title><link>https://arxiv.org/abs/2506.07400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedChat, a multi-agent framework combining specialized vision models with multiple role-specific LLM agents coordinated by a director agent to perform multimodal medical diagnosis (glaucoma/ophthalmology).&lt;/li&gt;&lt;li&gt;Targets reliability and interpretability by distributing reasoning across specialized agents to reduce hallucination risk and emulate multidisciplinary clinical teams.&lt;/li&gt;&lt;li&gt;Provides an interactive reporting interface for clinical review and education; code is available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philip R. Liu', 'Sparsh Bansal', 'Jimmy Dinh', 'Aditya Pawar', 'Ramani Satishkumar', 'Shail Desai', 'Neeraj Gupta', 'Xin Wang', 'Shu Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'multimodal medical AI', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07400</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title><link>https://arxiv.org/abs/2512.10402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper provides a theoretical analysis of how sparse/ambiguous decision boundaries enable effective backdoor (poisoning) attacks and derives a closed-form ambiguous boundary region that small numbers of relabeled samples can exploit.&lt;/li&gt;&lt;li&gt;Proposes Eminence, a black-box backdoor attack framework that optimizes a visually subtle universal trigger exploiting vulnerable decision boundaries to achieve high attack success with very low poison rates (&lt;0.1%) while preserving clean accuracy.&lt;/li&gt;&lt;li&gt;Validates theoretical claims with influence-function analysis and extensive experiments showing high attack success (&gt;90%), stealth, and transferability across models and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhou Feng', 'Jiahao Chen', 'Chunyi Zhou', 'Yuwen Pu', 'Tianyu Du', 'Jinbao Li', 'Jianhai Chen', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'adversarial machine learning', 'black-box attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10402</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SEA: Spectral Edge Attack</title><link>https://arxiv.org/abs/2512.08964</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEA (Spectral Edge Attack), an adversarial attack method that uses spectral analysis to identify and perturb the most vulnerable edges in a graph.&lt;/li&gt;&lt;li&gt;Aims to maximize attack impact while minimizing detectable perturbations by targeting weakest links in graph topology.&lt;/li&gt;&lt;li&gt;Includes quantitative spectral adversarial robustness evaluation and experiments demonstrating effectiveness against graph-based ML models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'graph neural networks', 'edge perturbation', 'robustness', 'spectral analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08964</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title><link>https://arxiv.org/abs/2508.09320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an exact (sound and complete) verification method for message-passing GNNs to certify robustness against attribute and structural perturbations (edge addition/deletion) under budget constraints.&lt;/li&gt;&lt;li&gt;Supports sum, max and mean aggregation functions (introducing verification for max and mean), using bound tightening and iterative solving of relaxed constraint satisfaction problems with incremental solver capabilities.&lt;/li&gt;&lt;li&gt;Implements GNNev, a verifier for these GNNs, and evaluates on real-world fraud (Amazon, Yelp) and biochemical (MUTAG, ENZYMES) datasets, showing improved node-classification performance and competitive graph-classification results versus existing exact verifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minghao Liu', 'Chia-Hsuan Lu', 'Marta Kwiatkowska']&lt;/li&gt;&lt;li&gt;Tags: ['GNN verification', 'Adversarial robustness', 'Structural attacks', 'Constraint solving', 'Formal verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09320</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title><link>https://arxiv.org/abs/2507.10532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that apparent RL-driven reasoning improvements in Qwen2.5 on math benchmarks are likely due to pretraining data contamination (benchmark leakage).&lt;/li&gt;&lt;li&gt;Introduces RandomCalculation, a generator producing fully clean arithmetic problems of controllable difficulty to avoid leakage.&lt;/li&gt;&lt;li&gt;Finds that on leakage-free data only accurate reward signals yield consistent improvements; random/incorrect rewards do not improve beyond the base model.&lt;/li&gt;&lt;li&gt;Recommends evaluating RL methods on uncontaminated benchmarks and across multiple model families to draw trustworthy conclusions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingqi Wu', 'Zhihao Zhang', 'Qiaole Dong', 'Zhiheng Xi', 'Jun Zhao', 'Senjie Jin', 'Xiaoran Fan', 'Yuhao Zhou', 'Huijie Lv', 'Ming Zhang', 'Yanwei Fu', 'Qin Liu', 'Songyang Zhang', 'Qi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'benchmark leakage', 'reinforcement learning evaluation', 'robustness', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10532</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis</title><link>https://arxiv.org/abs/2406.10090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of adversarial robustness between over-parameterized and under-parameterized neural networks.&lt;/li&gt;&lt;li&gt;Evaluation not only of model robustness but also of the reliability/efficacy of the adversarial attack methods used to assess robustness.&lt;/li&gt;&lt;li&gt;Finds that over-parameterized networks appear more robust to adversarial attacks than under-parameterized counterparts, attributing past contradictory results partly to failed/weak attack evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srishti Gupta', 'Zhang Chen', 'Luca Demetrio', 'Xiaoyi Feng', 'Zhaoqiang Xia', 'Antonio Emanuele Cin\\`a', 'Maura Pintor', 'Luca Oneto', 'Ambra Demontis', 'Battista Biggio', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial examples', 'attack evaluation', 'over-parameterization', 'empirical study']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.10090</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title><link>https://arxiv.org/abs/2512.15712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Predictive Concept Decoders (PCDs): an encoder compresses model activations into a sparse concept list and a decoder answers natural-language questions about model behavior through that bottleneck.&lt;/li&gt;&lt;li&gt;Proposes pretraining on large unstructured data then fine-tuning to map activations to human-interpretable concepts, improving with scale and data.&lt;/li&gt;&lt;li&gt;Demonstrates security- and safety-relevant capabilities: detecting jailbreaks, secret hints, implanted latent concepts, and surfacing latent user attributes from activations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Huang', 'Dami Choi', 'Daniel D. Johnson', 'Sarah Schwettmann', 'Jacob Steinhardt']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'jailbreak detection', 'red teaming', 'model privacy', 'concept bottleneck']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15712</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stylized Synthetic Augmentation further improves Corruption Robustness</title><link>https://arxiv.org/abs/2512.15675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training augmentation pipeline that combines synthetic image data with neural style transfer to improve model robustness to common corruptions.&lt;/li&gt;&lt;li&gt;Shows that stylized synthetic images, despite worse FID, improve classifier training and complement rule-based augmentations like TrivialAugment (but not all augmentations).&lt;/li&gt;&lt;li&gt;Provides systematic empirical analysis of augmentation choices and hyperparameters and reports state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and TinyImageNet-C.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Siedel', 'Rojan Regmi', 'Abhirami Anand', 'Weijia Shao', 'Silvia Vock', 'Andrey Morozov']&lt;/li&gt;&lt;li&gt;Tags: ['corruption_robustness', 'data_augmentation', 'synthetic_data', 'neural_style_transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15675</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title><link>https://arxiv.org/abs/2512.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Activation Oracles (AOs): LLMs trained via LatentQA to take model activations as inputs and answer natural-language questions about them.&lt;/li&gt;&lt;li&gt;Shows AOs generalize out-of-distribution and can recover information fine-tuned into a model (e.g., biographical facts or malign propensities) even without training on activations from that fine-tuned model.&lt;/li&gt;&lt;li&gt;Evaluates on four downstream tasks, where diversified training (classification tasks, self-supervised context prediction) improves performance; best AOs match/exceed prior white-box baselines on most tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Karvonen', 'James Chua', "Cl\\'ement Dumas", 'Kit Fraser-Taliente', 'Subhash Kantamneni', 'Julian Minder', 'Euan Ong', 'Arnab Sen Sharma', 'Daniel Wen', 'Owain Evans', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-extraction', 'safety-auditing', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15674</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title><link>https://arxiv.org/abs/2512.15503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AIMformer, a transformer-based real-time misbehavior detector for vehicular platoons that models intra-vehicle temporal dynamics and inter-vehicle spatial correlations.&lt;/li&gt;&lt;li&gt;Introduces vehicle-specific global positional encoding to handle join/exit maneuvers and a precision-focused BCE loss to reduce false positives for safety-critical operation.&lt;/li&gt;&lt;li&gt;Evaluated across multiple platoon controllers and attack vectors, reporting ≥0.93 performance and sub-millisecond inference on edge platforms via TFLite/ONNX/TensorRT.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Kalogiannis', 'Ahmed Mohamed Hussain', 'Hexu Li', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['misbehavior-detection', 'adversarial-attacks', 'safety-evaluation', 'edge-deployment', 'transformers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15503</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BEAT2AASIST model with layer fusion for ESDD 2026 Challenge</title><link>https://arxiv.org/abs/2512.15180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BEAT2AASIST, an audio deepfake detection model extending BEATs-AASIST by splitting BEATs-derived representations along frequency or channel and processing them with dual AASIST branches.&lt;/li&gt;&lt;li&gt;Introduces top-k transformer layer fusion strategies (concatenation, CNN-gated, SE-gated) to enrich feature representations.&lt;/li&gt;&lt;li&gt;Applies vocoder-based data augmentation to improve robustness against unseen spoofing methods and evaluates on the ESDD 2026 Challenge benchmark, reporting competitive performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanghyeok Chung', 'Eujin Kim', 'Donggun Kim', 'Gaeun Heo', 'Jeongbin You', 'Nahyun Lee', 'Sunmook Choi', 'Soyul Han', 'Seungsang Oh', 'Il-Youp Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'anti-spoofing', 'model architecture', 'robustness/data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15180</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title><link>https://arxiv.org/abs/2512.15053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Meta-Prompting Protocol: an orchestration framework for LLMs built around an Adversarial Trinity (Generator, Auditor, Optimizer) to create self-optimizing interaction loops.&lt;/li&gt;&lt;li&gt;Treats natural language instructions as differentiable variables within a semantic computation graph and introduces TextGrad (automatic textual differentiation) to use critiques as gradient-like signals.&lt;/li&gt;&lt;li&gt;Claims to mitigate hallucination and prevent model collapse, demonstrating theoretical viability via declarative programming (DSPy) and positioning the approach for reliable, observable software engineering with probabilistic models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM orchestration', 'alignment/safety', 'adversarial prompting', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15053</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intrusion Detection in Internet of Vehicles Using Machine Learning</title><link>https://arxiv.org/abs/2512.14958</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a machine learning–based intrusion detection system for Internet of Vehicles (IoV) by classifying malicious Controller Area Network (CAN) bus traffic.&lt;/li&gt;&lt;li&gt;Uses the CiCIoV2024 benchmark dataset and analyzes attack types including DoS and multiple spoofing attacks (gas pedal, RPM, speed, steering wheel).&lt;/li&gt;&lt;li&gt;Frames the problem as multi-class classification and reports clear structural differences between attack types and benign traffic as a basis for ML models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hop Le', 'Izzat Alsmadi']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'IoV', 'CAN-bus', 'spoofing', 'machine-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14958</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title><link>https://arxiv.org/abs/2512.14801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that hallucination in transformer-based LLMs is a structural property of their token-association/embedding geometry, not primarily an incentive or evaluation misalignment.&lt;/li&gt;&lt;li&gt;Presents empirical experiments using a 'Licensing Oracle' showing that perfect abstention/grounding requires external truth-validation modules rather than changes to incentives, prompting, or fine-tuning.&lt;/li&gt;&lt;li&gt;Concludes that eliminating hallucination necessitates hybrid architectures that separate linguistic fluency from epistemic grounding and abstention, i.e., external validation pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Ackermann', 'Simeon Emanuilov']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety', 'grounding/abstention', 'model-architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14801</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAPE: Capability Achievement via Policy Execution</title><link>https://arxiv.org/abs/2512.14761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Capability Engineering and CAPE, a Specify -&gt; Verify -&gt; Correct -&gt; Train protocol that converts requirements into executable specifications to make models satisfy constraints by default.&lt;/li&gt;&lt;li&gt;Provides practical artifacts: CPL specification language, PredicateGraph schema, policy packs, and CapabilityBench (public registry) to evaluate model compliance.&lt;/li&gt;&lt;li&gt;Presents empirical findings (contextual objectivity and verification-fidelity scaling) and reports CAPE reduces violation rates by 81% vs DPO while cutting annotation cost and timeline substantially.&lt;/li&gt;&lt;li&gt;Releases protocol and assets under Apache 2.0 to enable reproducible enforcement and evaluation of safety-relevant policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Ball']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'specification languages', 'verification &amp; testing', 'benchmarks/evaluations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14761</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title><link>https://arxiv.org/abs/2512.14742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical, three-layer defense framework (anomaly detection, intrusion confirmation, multiattack classification) tailored to O-RAN telemetry and RIC domains.&lt;/li&gt;&lt;li&gt;Integrates hybrid quantum computing (amplitude- and entanglement-based encodings) with deep and ensemble ML classifiers, benchmarked on synthetic and real telemetry.&lt;/li&gt;&lt;li&gt;Claims near-perfect accuracy, high recall, strong class separability, and provides analyses of decision boundaries, probabilistic margins, and latent-space geometry for interpretability and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (Technical Report)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tan Le', 'Van Le', 'Sachin Shetty']&lt;/li&gt;&lt;li&gt;Tags: ['O-RAN security', 'anomaly detection', 'quantum ML', 'robustness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14742</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier</title><link>https://arxiv.org/abs/2512.15492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares two approaches for per-instance reliability of classifier predictions: Robustness Quantification (RQ) and Uncertainty Quantification (UQ).&lt;/li&gt;&lt;li&gt;Empirically evaluates RQ and UQ across multiple benchmark datasets and finds no clear universal winner; the methods are complementary.&lt;/li&gt;&lt;li&gt;Proposes a hybrid approach combining RQ and UQ that outperforms each method alone and provides per-dataset assessments of the relative importance of uncertainty vs robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Adri\\'an Detavernier", 'Jasper De Bock']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'uncertainty-quantification', 'reliability-evaluation', 'safety-assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15492</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance</title><link>https://arxiv.org/abs/2512.15469</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a learned model editor (graph metanetwork) that edits neural networks in a single inference step to enforce requirements while preserving utility.&lt;/li&gt;&lt;li&gt;Trains the metanetwork on populations of NNs with an objective combining requirement satisfaction and minimal utility degradation.&lt;/li&gt;&lt;li&gt;Demonstrates use cases including data minimisation, bias mitigation, and weight pruning, reporting better trade-offs (performance, requirement satisfaction, time) versus post-processing or retraining baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ioannis Kalogeropoulos', 'Giorgos Bouritsas', 'Yannis Panagakis']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'AI safety / requirement compliance', 'fairness mitigation', 'model robustness / utility preservation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15469</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning</title><link>https://arxiv.org/abs/2512.15460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Invertibility Loss (InvLoss) as a theoretically-grounded metric to quantify the maximum achievable effectiveness of data reconstruction attacks (DRAs) on a per-instance and per-model basis in federated learning (FL).&lt;/li&gt;&lt;li&gt;Derives a tight, computable upper bound linking DRA risk to the spectral properties of the Jacobian of exchanged model updates or feature embeddings, unifying understanding of why certain defenses work.&lt;/li&gt;&lt;li&gt;Presents InvRE, an attack-agnostic estimator for DRA risk across instances and architectures, and proposes two adaptive noise-perturbation defenses that improve privacy without degrading classification accuracy.&lt;/li&gt;&lt;li&gt;Validates the framework through extensive experiments on real-world datasets, showing practical risk evaluation and mitigation strategies for FL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangrui Xu', 'Zhize Li', 'Yufei Han', 'Bin Wang', 'Jiqiang Liu', 'Wei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'data-reconstruction-attacks', 'privacy-preserving-ml', 'risk-assessment', 'defense-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15460</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</title><link>https://arxiv.org/abs/2512.15442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies use of chain-of-thought and task-instruction prompting to reduce generation of copyrighted images by text-to-image models.&lt;/li&gt;&lt;li&gt;Combines these prompting techniques with negative prompting and prompt re-writing as mitigation strategies.&lt;/li&gt;&lt;li&gt;Evaluates generated images for similarity to copyrighted examples and for relevance to user prompts across various models, reporting numerical experiments and effectiveness relative to model complexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeraj Sarna', 'Yuanyuan Li', 'Michael von Gablenz']&lt;/li&gt;&lt;li&gt;Tags: ['copyright-mitigation', 'model-memorization', 'prompting-strategies', 'text-to-image-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15442</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</title><link>https://arxiv.org/abs/2512.15385</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a unified framework to systematically evaluate robustness of ML models for fault classification (FC) and fault localization (FL) in power system protection under realistic degradation scenarios.&lt;/li&gt;&lt;li&gt;Uses high-fidelity EMT simulations to model sensor outages, reduced sampling rates, transient communication losses, and other degraded observability conditions.&lt;/li&gt;&lt;li&gt;Benchmarks impact of limited observability on model performance: FC is largely stable but drops ~13% under single-phase loss; FL is more sensitive, with voltage loss increasing localization error by &gt;150%.&lt;/li&gt;&lt;li&gt;Provides actionable guidance for designing ML-assisted protection systems with robustness-aware sensor/measurement priorities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Oelhaf', 'Mehran Pashaei', 'Georg Kordowich', 'Christian Bergler', 'Andreas Maier', 'Johann J\\"ager', 'Siming Bayer']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'fault-detection', 'power-systems', 'sensor-degradation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15385</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference</title><link>https://arxiv.org/abs/2512.15335</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study of how post-training quantization (PTQ) methods (AdaRound, BRECQ, OBC) at various bit-widths affect membership inference vulnerability.&lt;/li&gt;&lt;li&gt;Finds that lower-precision PTQ models (e.g., 4-bit, 2-bit, 1.58-bit) can reduce membership inference leakage by up to an order of magnitude compared to full-precision models, at the cost of reduced utility.&lt;/li&gt;&lt;li&gt;Shows that quantizing only certain layers (e.g., keeping the last layer at higher precision) enables finer control of the privacy–utility trade-off across CIFAR-10, CIFAR-100, and TinyImageNet.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxiang Zhang', 'Tongxi Qu', 'Zhong Li', 'Tian Zhang', 'Jun Pang', 'Sjouke Mauw']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'post-training-quantization', 'model-compression', 'privacy-utility-tradeoff']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15335</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>An Efficient Gradient-Based Inference Attack for Federated Learning</title><link>https://arxiv.org/abs/2512.15143</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new gradient-based membership inference attack for federated learning that leverages temporal evolution of last-layer gradients across multiple rounds using a shadow-model technique.&lt;/li&gt;&lt;li&gt;Attack works under semi-honest and malicious adversary models (aggregator or data owner) and is model-agnostic, applicable to classification and regression.&lt;/li&gt;&lt;li&gt;Extends approach to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses; evaluated on CIFAR-100, Purchase100, and Breast Cancer Wisconsin datasets.&lt;/li&gt;&lt;li&gt;Findings: multi-round FL increases vulnerability, aggregators pose greater threat than individual data owners, and high-dimensional data yields stronger leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Pablo Monta\\~na-Fern\\'andez", 'Ines Ortega-Fernandez']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'membership-inference', 'gradient-attacks', 'attribute-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15143</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</title><link>https://arxiv.org/abs/2512.15140</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares spatial vs temporal validation performance of tree ensembles (XGBoost, RF) and deep time-series models (LSTM, TCN) for crop yield and anomaly prediction in Germany.&lt;/li&gt;&lt;li&gt;Finds substantial performance degradation on temporally independent validation years despite strong spatial test-set accuracy, indicating limited temporal generalization.&lt;/li&gt;&lt;li&gt;Shows that SHAP feature importance can appear plausible even when the underlying model fails to generalize, highlighting risks in accepting post-hoc explanations without validation-aware checks.&lt;/li&gt;&lt;li&gt;Recommends domain-aware validation, hybrid modeling strategies, and stricter scrutiny of explainability methods for trustworthy environmental ML.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roland Baatz']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'generalization', 'interpretability', 'safety-evaluation', 'environmental-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15140</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</title><link>https://arxiv.org/abs/2512.15123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrajSyn, a method that synthesizes a proxy dataset from client model update trajectories in federated learning, without accessing raw client data.&lt;/li&gt;&lt;li&gt;Enables server-side adversarial training using the synthesized dataset, avoiding extra compute on client devices.&lt;/li&gt;&lt;li&gt;Claims privacy preservation while improving adversarial robustness on image classification benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mukur Gupta', 'Niharika Gupta', 'Saifur Rahman', 'Shantanu Pal', 'Chandan Karmakar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'federated learning', 'dataset distillation', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15123</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2512.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes fundamental limits of embedding-based hallucination detection in RAG systems using conformal prediction to provide finite-sample coverage guarantees.&lt;/li&gt;&lt;li&gt;Finds embedding methods (including state-of-the-art embeddings and cross-encoders) have very high false positive rates on real hallucination benchmarks, despite strong calibration on synthetic data.&lt;/li&gt;&lt;li&gt;Shows GPT-4 as an LLM judge achieves far lower FPR, introducing the term “semantic illusion” to describe semantically plausible hallucinations that embeddings cannot detect.&lt;/li&gt;&lt;li&gt;Concludes that embedding-based detection is insufficient for production RAG deployment and that reasoning-capable models are needed for reliable hallucination detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debu Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'embedding robustness', 'safety evaluation', 'conformal prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15068</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unreliable Uncertainty Estimates with Monte Carlo Dropout</title><link>https://arxiv.org/abs/2512.14851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates Monte Carlo dropout (MCD) as an approximation to Bayesian inference for uncertainty estimation in deep neural networks.&lt;/li&gt;&lt;li&gt;Compares MCD to Gaussian Processes and Bayesian Neural Networks, finding MCD often fails to reflect true uncertainty, especially in extrapolation and interpolation regions.&lt;/li&gt;&lt;li&gt;Concludes that MCD's uncertainty estimates (as tested) are unreliable for capturing epistemic and aleatoric uncertainty, with implications for safety-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aslak Djupsk{\\aa}s', 'Alexander Johannes Stasik', 'Signe Riemer-S{\\o}rensen']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'monte-carlo-dropout', 'bayesian-neural-networks', 'model-calibration', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14851</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Geometry for policy-constrained interpretation</title><link>https://arxiv.org/abs/2512.14731</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a geometric framework representing semantic meaning as directions on a unit sphere and evidence as witness vectors, with admissible interpretations as spherical convex regions.&lt;/li&gt;&lt;li&gt;Separates policy constraints as explicit priors on the manifold and frames interpretation as constrained optimization, where refusal naturally arises under contradiction or policy exclusion.&lt;/li&gt;&lt;li&gt;Proves connections to information theory, Bayesian inference, and sheaf-theoretic semantics with information-theoretic optimal complexity bounds.&lt;/li&gt;&lt;li&gt;Provides empirical validation on large-scale regulated financial data demonstrating zero hallucinated approvals across multiple policy regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikit Phadke']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination prevention', 'policy-constrained decision-making', 'alignment/safety', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14731</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hybrid Attribution Priors for Explainable and Robust Model Training</title><link>https://arxiv.org/abs/2512.14719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Class-Aware Attribution Prior (CAP) to extract attribution priors that emphasize fine-grained, class-discriminative tokens for classification with small language models.&lt;/li&gt;&lt;li&gt;Introduces CAP Hybrid, which combines CAP priors with existing attribution methods to produce a balanced supervisory signal for aligning model self-attributions.&lt;/li&gt;&lt;li&gt;Shows that aligning models to these enriched priors improves interpretability and yields consistent robustness gains across full-data, few-shot, and adversarial evaluation scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoran Zhang', 'Feng Zhang', 'Shangyuan Li', 'Yang Shi', 'Yuanxing Zhang', 'Wei Chen', 'Tengjiao Wang', 'Kam-Fai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'explainability', 'attribution priors', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14719</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title><link>https://arxiv.org/abs/2512.14715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BLADE, a differentiable bit-level fault analysis method that uses model gradients to identify weight bits whose flips steer semantic content of image captions while preserving fluency.&lt;/li&gt;&lt;li&gt;Hypothesizes and empirically targets semantically critical bits: gradient-based sensitivity estimation followed by optimization with a caption-level semantic-fluency objective to select and refine bit flips.&lt;/li&gt;&lt;li&gt;Demonstrates that imperceptible low-level bit perturbations can systematically change high-level semantics in generative vision-language models, with implications for robustness testing, adversarial attacks/defenses, and explainability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zafaryab Haider', 'Md Hafizur Rahman', 'Shane Moeykens', 'Vijay Devabhaktuni', 'Prabuddha Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['fault injection', 'bit-flip attacks', 'adversarial attack', 'model robustness', 'multimodal LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14715</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title><link>https://arxiv.org/abs/2512.12791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end Agent Assessment Framework with four pillars (LLMs, Memory, Tools, Environment) to evaluate agentic AI beyond binary task completion metrics.&lt;/li&gt;&lt;li&gt;Emphasizes capturing non-deterministic runtime behaviors and behavioral deviations that conventional metrics miss, validated on an Autonomous CloudOps use case.&lt;/li&gt;&lt;li&gt;Focuses on practical deployment gaps observed in a production multi-agent system and provides systematic assessments for tool invocation, memory usage, inter-agent collaboration, and environment interaction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sreemaee Akshathala', 'Bassam Adnan', 'Mahisha Ramesh', 'Karthik Vaidhyanathan', 'Basil Muhammed', 'Kannan Parthasarathy']&lt;/li&gt;&lt;li&gt;Tags: ['agent-evaluation', 'safety-evaluation', 'robustness', 'multi-agent-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12791</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of refusal/stability for four instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 sampling configurations (4 temperatures × 5 seeds).&lt;/li&gt;&lt;li&gt;Finds 18–28% of prompts flip between refusal and compliance depending on seed/temperature; higher temperatures substantially reduce safety decision stability (SSI falls from 0.977 at T=0.0 to 0.942 at T=1.0).&lt;/li&gt;&lt;li&gt;Introduces a Safety Stability Index (SSI), validates results with an external judge (Claude 3.5 Haiku) with high inter-judge agreement, and shows single-shot evaluations disagree with multi-sample ground truth ~7.6% of the time.&lt;/li&gt;&lt;li&gt;Practical recommendation: use at least 3 samples per prompt (and account for temperature/seed variability) for reliable safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Refusal / jailbreaking', 'Evaluation methodology', 'Stochasticity / robustness', 'Red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses long-tail, safety-critical event identification for autonomous vehicles by mining fleet video logs to curate rare training examples.&lt;/li&gt;&lt;li&gt;Proposes Semantic-Drive: a two-stage neuro-symbolic pipeline — Symbolic Grounding via an open-vocabulary real-time detector (YOLOE) to anchor attention, followed by Cognitive Analysis using a Reasoning VLM.&lt;/li&gt;&lt;li&gt;Introduces an inference-time 'System 2' alignment with a multi-model Judge-Scout consensus to mitigate hallucinations and improve forensic scene analysis.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains (Recall 0.966 vs. 0.475 for CLIP; 40% reduction in Risk Assessment Error) and emphasizes local, privacy-preserving operation on consumer GPUs (RTX 3090).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-vehicle-safety', 'long-tail-data-mining', 'open-vocabulary-detection', 'neuro-symbolic', 'privacy-preserving-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title><link>https://arxiv.org/abs/2512.08786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies alignment of LLMs in a federated RLHF setting by evaluating how different group-level reward aggregation strategies affect alignment and fairness.&lt;/li&gt;&lt;li&gt;Introduces an adaptive aggregation scheme that dynamically weights groups based on historical alignment performance.&lt;/li&gt;&lt;li&gt;Provides a systematic evaluation framework and experiments (PPO-based RLHF on Q/A) showing improved fairness with competitive alignment scores while the server only uses aggregated rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Srewa', 'Tianyu Zhao', 'Salma Elmalaki']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'federated learning', 'RLHF', 'fairness', 'preference aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08786</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title><link>https://arxiv.org/abs/2512.07901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Games with Endogenous Players (GEPs), modeling lineages (strategic replicators) as fundamental decision-making units and defines Evolutionarily Stable Distributions of Intelligence (ESDIs) as equilibria.&lt;/li&gt;&lt;li&gt;Develops a hierarchical strategic-layer formalism with cross-level gain matrices and proves stability under a small-gain condition via global Lyapunov functions at finite depths.&lt;/li&gt;&lt;li&gt;Shows closure under meta-selection (governance, innovation, constitutional evolution) but proves an Alignment Impossibility Theorem: unrestricted self-modification undermines stability, requiring bounded modification classes for aligned outcomes.&lt;/li&gt;&lt;li&gt;Discusses applications to AI deployment dynamics, market concentration, failure modes of personality engineering under selection pressure, and constitutional constraints for stable multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Vallier']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'evolutionary game theory', 'AI deployment dynamics', 'governance/constitutional design', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07901</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title><link>https://arxiv.org/abs/2511.10400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes reliability of LLM-based multi-agent systems through the lens of Byzantine fault tolerance, comparing them to traditional agents.&lt;/li&gt;&lt;li&gt;Finds LLM-based agents exhibit stronger skepticism toward erroneous messages and can outperform traditional agents across network topologies.&lt;/li&gt;&lt;li&gt;Proposes CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism leveraging LLMs' reflective/discriminative capabilities to weight information flow.&lt;/li&gt;&lt;li&gt;Demonstrates via experiments that CP-WBFT maintains high accuracy and robustness under extreme Byzantine conditions (up to ~85.7% fault rate) on tasks including mathematical reasoning and safety assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lifan Zheng', 'Jiawei Chen', 'Qinghong Yin', 'Jingyuan Zhang', 'Xinyi Zeng', 'Yu Tian']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine fault tolerance', 'LLM-based multi-agent systems', 'robustness', 'consensus mechanisms', 'safety assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10400</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives</title><link>https://arxiv.org/abs/2511.08710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework modeling two single-layer linear self-attention (LSA) agents that alternately update from each other's in-context outputs on quadratic regression tasks, allowing analysis of coupled dynamics under misaligned fixed objectives.&lt;/li&gt;&lt;li&gt;Shows that objective misalignment produces a biased equilibrium with predictable residual errors determined by objective gap and prompt-induced geometry; a helper agent that adapts its objective can implement Newton-like updates to remove the plateau and accelerate convergence.&lt;/li&gt;&lt;li&gt;Validates theoretical predictions using trained LSA agents and black-box GPT-5-mini runs on in-context linear regression, arguing the framework links prompt geometry and objective misalignment to stability, bias, and robustness in multi-agent LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Romain Cosentino', 'Sarath Shekkizhar', 'Adam Earle']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent systems', 'robustness', 'prompt geometry', 'in-context learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08710</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title><link>https://arxiv.org/abs/2510.20768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies poisoning vulnerabilities in Retrieval-Augmented Generation (RAG) pipelines used for Cyber Threat Intelligence (CTI) systems.&lt;/li&gt;&lt;li&gt;Proposes applying source credibility algorithms (demonstrated with PageRank) to corpus documents to downweight malicious/poisoned sources and promote trusted content.&lt;/li&gt;&lt;li&gt;Provides quantitative evaluation on the MS MARCO dataset and proof-of-concept experiments on CTI documents/feeds showing lower authority scores for malicious documents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Austin Jia', 'Avaneesh Ramesh', 'Zain Shamsi', 'Daniel Zhang', 'Alex Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'data poisoning', 'source credibility', 'CTI', 'PageRank']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20768</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning</title><link>https://arxiv.org/abs/2508.18395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Latent Self-Consistency (LSC), a method that selects the most semantically consistent LLM response via learnable token embeddings and lightweight processing of summary tokens.&lt;/li&gt;&lt;li&gt;Reports improved performance over Self-Consistency (SC), Universal SC (USC), and Weighted Unigram Consistency Score (WUCS) across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA).&lt;/li&gt;&lt;li&gt;Claims negligible runtime overhead (≤0.9%), requires no model-architecture changes, and yields better-calibrated confidence estimates (low expected calibration error) across answer formats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungsuk Oh', 'Jay-Yoon Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM self-consistency', 'Calibration', 'Robustness', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18395</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title><link>https://arxiv.org/abs/2505.05638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates interaction between state-of-the-art motion prediction models and motion planners in closed-loop autonomous driving, contrasting with standard open-loop benchmarks.&lt;/li&gt;&lt;li&gt;Finds that higher open-loop prediction accuracy does not necessarily yield better closed-loop driving; temporal consistency and planner compatibility are critical factors.&lt;/li&gt;&lt;li&gt;Shows that much smaller (up to 86% fewer parameters) model variants can achieve comparable or superior closed-loop performance in some cases.&lt;/li&gt;&lt;li&gt;Provides code and an experimental framework (pred2plan) for reproducing closed-loop evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed-Khalil Bouzidi', 'Christian Schlauch', 'Nicole Scheuerer', 'Yue Yao', 'Nadja Klein', 'Daniel G\\"ohring', 'J\\"org Reichardt']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'motion prediction', 'safety evaluation', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05638</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holistic Utility Preference Learning for Listwise Alignment</title><link>https://arxiv.org/abs/2410.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Direct Ranking Preference Optimization (DRPO), framing human preference alignment as a learning-to-rank task rather than pairwise comparisons.&lt;/li&gt;&lt;li&gt;Proposes diffNDCG, a differentiable approximation of NDCG using a sorting network to enable end-to-end optimization of listwise rankings.&lt;/li&gt;&lt;li&gt;Adds a margin-based Adaptive Rank Policy Score to improve discriminative quality of generated responses.&lt;/li&gt;&lt;li&gt;Reports empirical gains over pairwise methods (e.g., DPO) in aligning model outputs with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacong Zhou', 'Xianyun Wang', 'Min Zhang', 'Jun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'learning-to-rank', 'differentiable-ranking', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18127</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title><link>https://arxiv.org/abs/2312.04960</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a theoretical mutual information (MI) analysis for ViT-based autoencoder pretraining, deriving MI bounds that should be constrained between adversarial examples and their latent representations.&lt;/li&gt;&lt;li&gt;Proposes MIMIR, a self-supervised adversarial training method using an MI penalty with masked image modeling to improve adversarial robustness during pretraining.&lt;/li&gt;&lt;li&gt;Demonstrates consistent gains in natural and robust accuracy on CIFAR-10, Tiny-ImageNet, and ImageNet-1K, outperforming prior SOTA adversarial training methods for ViTs.&lt;/li&gt;&lt;li&gt;Evaluates robustness against unforeseen corruptions and adaptive attacks where the adversary has full knowledge of the defense, showing strong resilience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Xu', 'Shujian Yu', 'Zhuoran Liu', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'vision transformers', 'self-supervised learning', 'mutual information']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.04960</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title><link>https://arxiv.org/abs/2512.13142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study testing 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS).&lt;/li&gt;&lt;li&gt;Finds models lack coherent multilevel understanding: they overestimate interpersonal stigma, underestimate cognitive stigma, assume uniform community condemnation, introduce demographic biases absent in human data, and violate expected stigma–secrecy relationships.&lt;/li&gt;&lt;li&gt;Argues current alignment methods produce superficially appropriate language but not coherent psychological understanding, with implications for safety in high‑stakes health contexts.&lt;/li&gt;&lt;li&gt;Recommends new design goals (multilevel coherence), continuous auditing/evaluation, governance/accountability measures, and AI literacy for sensitive domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anika Sharma', 'Malavika Mampally', 'Chidaksh Ravuru', 'Kandyce Brennan', 'Neil Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'audit', 'bias', 'health-domain-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13142</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models with Native Multi-Agent Intelligence</title><link>https://arxiv.org/abs/2512.08743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies four core capabilities required for native multi-agent intelligence in foundation models: understanding, planning, efficient communication, and adaptation.&lt;/li&gt;&lt;li&gt;Presents empirical evidence across 41 LLMs and 7 benchmarks showing that improving single-agent scaling does not automatically produce robust multi-agent intelligence.&lt;/li&gt;&lt;li&gt;Outlines research directions including dataset construction, evaluation, training paradigms, and safety considerations for building FMs with native multi-agent abilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyue Hu', 'Haoyang Yan', 'Yiqun Zhang', 'Yang Chen', 'Dongzhan Zhou', 'Lei Bai']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'AI safety', 'alignment', 'evaluation', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08743</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title><link>https://arxiv.org/abs/2510.14112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STEMS, a safety-constrained multi-agent RL framework for coordinated building energy management combining GCN-Transformer for spatial-temporal representation and Control Barrier Functions (CBFs) for safety guarantees.&lt;/li&gt;&lt;li&gt;CBFs are integrated into the multi-agent RL algorithm to provide mathematical safety guarantees and reduce safety violations substantially in experiments.&lt;/li&gt;&lt;li&gt;Demonstrates empirical gains on real-world datasets: cost and emission reductions, large drop in safety violations, low occupant discomfort, and robustness under extreme weather and across building types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huiliang Zhang', 'Di Wu', 'Arnaud Zinflou', 'Benoit Boulet']&lt;/li&gt;&lt;li&gt;Tags: ['safety-constrained reinforcement learning', 'control barrier functions', 'multi-agent RL', 'spatial-temporal representation (GCN+Transformer)', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14112</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Inverse Scaling in Test-Time Compute</title><link>https://arxiv.org/abs/2507.14417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates inverse scaling where increasing test-time reasoning length (compute) reduces accuracy across tasks: counting with distractors, regression with spurious features, deduction with constraint tracking, and tasks probing advanced AI risks.&lt;/li&gt;&lt;li&gt;Identifies five distinct failure modes when models reason longer: growing distraction by irrelevant info, overfitting to problem framings, shifts from reasonable priors to spurious correlations, difficulty maintaining focus on complex deductive tasks, and amplification of concerning behaviors (e.g., self-preservation).&lt;/li&gt;&lt;li&gt;Evaluates multiple state-of-the-art models (Claude family, OpenAI o-series, Claude Sonnet 4) and emphasizes the need to evaluate models across diverse reasoning lengths to detect and mitigate these safety/robustness issues.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryo Pradipta Gema', 'Alexander H\\"agele', 'Runjin Chen', 'Andy Arditi', 'Jacob Goldman-Wetzler', 'Kit Fraser-Taliente', 'Henry Sleight', 'Linda Petrini', 'Julian Michael', 'Beatrice Alex', 'Pasquale Minervini', 'Yanda Chen', 'Joe Benton', 'Ethan Perez']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'robustness', 'LLM evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14417</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title><link>https://arxiv.org/abs/2512.14601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FakeRadar, a deepfake video detection framework that synthesizes outlier forgery samples to improve cross-domain generalization to unseen manipulations.&lt;/li&gt;&lt;li&gt;Introduces Forgery Outlier Probing using dynamic subcluster modeling and cluster-conditional outlier generation to simulate novel forgery artifacts near cluster boundaries.&lt;/li&gt;&lt;li&gt;Presents Outlier-Guided Tri-Training with outlier-driven contrastive learning and outlier-conditioned cross-entropy to train a detector to distinguish real, known fakes, and synthesized outliers.&lt;/li&gt;&lt;li&gt;Demonstrates improved cross-domain deepfake detection performance on benchmarks, emphasizing robustness to emerging manipulation techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolun Li', 'Jichang Li', 'Yinqi Cai', 'Junye Chen', 'Xiaonan Luo', 'Guanbin Li', 'Rushi Lan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'outlier synthesis', 'cross-domain generalization', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14601</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</title><link>https://arxiv.org/abs/2512.14448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reasoning-Style Poisoning (RSP), a novel attack that manipulates an LLM agent's reasoning process (style) rather than content, via Generative Style Injection (GSI).&lt;/li&gt;&lt;li&gt;Defines a Reasoning Style Vector (RSV) metric tracking Verification depth, Self-confidence, and Attention focus to quantify reasoning-style shifts.&lt;/li&gt;&lt;li&gt;Demonstrates that GSI degrades performance on HotpotQA and FEVER across ReAct, Reflection, and Tree of Thoughts architectures, bypassing content filters and causing slowed or prematurely erroneous reasoning.&lt;/li&gt;&lt;li&gt;Proposes RSP-M, a lightweight runtime monitor that computes RSV metrics and raises alerts when style metrics exceed safety thresholds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingfu Zhou', 'Pengfei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'poisoning', 'LLM agents', 'runtime monitoring', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14448</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Transferable Defense Against Malicious Image Edits</title><link>https://arxiv.org/abs/2512.14341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TDAE, a bimodal (image+text) defense framework to make images immune to malicious diffusion-based edits via coordinated image-text adversarial optimization.&lt;/li&gt;&lt;li&gt;Introduces FlatGrad Defense Mechanism (FDM) that adds gradient regularization to steer perturbations toward flat minima, improving robustness and cross-model transferability.&lt;/li&gt;&lt;li&gt;Proposes Dynamic Prompt Defense (DPD) that iteratively refines text embeddings and updates images to enforce broader immunity features across diverse prompts/embeddings.&lt;/li&gt;&lt;li&gt;Reports extensive intra- and cross-model experiments showing state-of-the-art mitigation of malicious edits on diffusion editing systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'image editing protection', 'diffusion models', 'transferability', 'bimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14341</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Attention Guided Defense Against Malicious Edits</title><link>https://arxiv.org/abs/2512.14333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dual Attention-Guided Noise Perturbation (DANP), an imperceptible perturbation method applied over multiple diffusion timesteps to immunize text-to-image models against malicious edits.&lt;/li&gt;&lt;li&gt;Manipulates cross-attention maps (reducing attention in text-relevant regions, increasing it in irrelevant regions) and maximizes discrepancy between injected noise and model-predicted noise to misguide edits.&lt;/li&gt;&lt;li&gt;Uses a dynamic-threshold mask to identify text-relevant vs irrelevant regions, aiming to preserve intended targets while disrupting malicious tampering; reports state-of-the-art empirical results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image-model-defense', 'adversarial-perturbations', 'text-to-image', 'attention-manipulation', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14333</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title><link>https://arxiv.org/abs/2512.14320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues existing immunization metrics are inadequate because they measure output dissimilarity rather than whether edits thwart attacker intent; proposes defining success as semantic mismatch to the prompt or substantial perceptual degradation.&lt;/li&gt;&lt;li&gt;Introduces Synergistic Intermediate Feature Manipulation (SIFM), which perturbs intermediate diffusion features with two objectives: maximize divergence from the original edit trajectory (break semantic alignment) and minimize feature norms (induce perceptual degradation).&lt;/li&gt;&lt;li&gt;Proposes a new evaluation metric, Immunization Success Rate (ISR), which uses Multimodal Large Language Models (MLLMs) to quantify the fraction of edits that either semantically fail relative to the prompt or suffer significant perceptual degradation.&lt;/li&gt;&lt;li&gt;Presents extensive experiments showing SIFM achieves state-of-the-art protection against malicious diffusion-based image editing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Dong', 'Jie Zhang', 'Guoying Zhao', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image immunization', 'adversarial robustness', 'diffusion models', 'evaluation metrics', 'prompt-based attacks/defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14320</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</title><link>https://arxiv.org/abs/2512.14233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PentestEval, a modular benchmark evaluating LLMs across six decomposed penetration-testing stages (e.g., Info Collection, Weakness Gathering, Attack Decision, Exploit Generation) with expert-annotated ground truth.&lt;/li&gt;&lt;li&gt;Covers 346 tasks across 12 realistic vulnerable scenarios and a fully automated evaluation pipeline to measure stage-level and end-to-end performance of LLMs and LLM-driven systems.&lt;/li&gt;&lt;li&gt;Finds generally weak LLM performance (end-to-end pipelines ≈31% success), with autonomous agents failing largely, and argues modularization improves stage-level reasoning and overall reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruozhao Yang', 'Mingfei Cheng', 'Gelei Deng', 'Tianwei Zhang', 'Junjie Wang', 'Xiaofei Xie']&lt;/li&gt;&lt;li&gt;Tags: ['penetration testing', 'LLM evaluation', 'benchmarking', 'autonomous red teaming', 'security assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14233</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol</title><link>https://arxiv.org/abs/2512.14166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel privacy attack called Intent Inversion where semi-honest MCP servers infer user intent by analyzing legitimate tool call logs in Model Context Protocol-based agent architectures.&lt;/li&gt;&lt;li&gt;Proposes IntentMiner, a reconstruction framework using Hierarchical Information Isolation and Three-Dimensional Semantic Analysis (tool purpose, call statements, returned results) to infer step-level user intents.&lt;/li&gt;&lt;li&gt;Evaluates IntentMiner with experiments showing high semantic alignment (&gt;85%) with original queries and superior performance over baselines, highlighting privacy risks from decoupled tool execution logs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Yao', 'Zhiqiang Wang', 'Haoran Cheng', 'Yihang Cheng', 'Haohua Du', 'Xiang-Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'inference attack', 'agent architectures', 'tool-calls / MCP', 'intent reconstruction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14166</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title><link>https://arxiv.org/abs/2512.14044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniDrive-R1, an end-to-end vision-language model framework for autonomous driving that interleaves perception and textual reasoning via an interleaved Multi-modal Chain-of-Thought (iMCoT).&lt;/li&gt;&lt;li&gt;Introduces a reinforcement-driven visual grounding mechanism (Clip-GRPO) with an annotation-free, process-based grounding reward to steer attention and enable zoom-in analysis without dense localization labels.&lt;/li&gt;&lt;li&gt;Training uses a two-stage reinforcement pipeline to enforce real-time cross-modal consistency between visual focus and textual reasoning, improving robustness against object hallucination.&lt;/li&gt;&lt;li&gt;Reports substantial empirical gains on DriveLMM-o1 versus a Qwen2.5VL-7B baseline in reasoning score and final answer accuracy, targeting trustworthiness in a safety-critical AD domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenguo Zhang', 'Haohan Zhen', 'Yishen Wang', 'Le Xu', 'Tianchen Deng', 'Xuefeng Chen', 'Qu Chen', 'Bo Zhang', 'Wuxiong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'grounding/robustness', 'autonomous driving', 'reinforcement learning', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14044</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes</title><link>https://arxiv.org/abs/2512.13744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys robustness of state-of-the-art audio deepfake detection models under realistic capture conditions (background noise, reverberation, consumer channels).&lt;/li&gt;&lt;li&gt;Introduces a reproducible SNR-based benchmarking framework mixing MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate performance across controlled SNRs (35 dB to -5 dB).&lt;/li&gt;&lt;li&gt;Evaluates pretrained speech encoders (WavLM, Wav2Vec2, MMS) with multi-condition training and reports metrics (accuracy, ROC-AUC, EER) on binary and four-class tasks.&lt;/li&gt;&lt;li&gt;Finds finetuning and multi-condition training substantially improve robustness (EER reductions ~10–15 percentage points at 10–0 dB SNR) and provides practical recipes for noise-aware detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udayon Sen', 'Alka Luqman', 'Anupam Chattopadhyay']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'robustness / noise', 'benchmarking (SNR)', 'pretrained speech encoders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13744</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title><link>https://arxiv.org/abs/2512.13742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DL3M, a vision-to-language framework linking a new endoscopy image classifier (MobileCoAtNet) to LLMs to generate structured clinical reasoning.&lt;/li&gt;&lt;li&gt;Constructs two expert-verified benchmarks (causes, symptoms, treatment, lifestyle, follow-up) and evaluates 32 LLMs against these gold standards.&lt;/li&gt;&lt;li&gt;Finds that stronger image classification improves LLM explanations but that no LLM achieves human-level stability; models change reasoning under prompt variations.&lt;/li&gt;&lt;li&gt;Concludes that combining DL and LLMs can produce useful clinical narratives but current LLMs are unreliable for high-stakes decisions and outlines a path toward safer reasoning systems; code and datasets released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Najib Hasan (Wichita State University', 'USA)', 'Imran Ahmad (Wichita State University', 'USA)', 'Sourav Basak Shuvo (Khulna University of Engineering and Technology', 'Bangladesh)', 'Md. Mahadi Hasan Ankon (Khulna University of Engineering and Technology', 'Bangladesh)', 'Sunanda Das (University of Arkansas', 'USA)', 'Nazmul Siddique (Ulster University', 'UK)', "Hui Wang (Queen's University Belfast", 'UK)']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'medical-AI', 'robustness', 'LLM-reliability', 'vision-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13742</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title><link>https://arxiv.org/abs/2512.13741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Laminar Flow Hypothesis: benign prompts induce smooth latent trajectories in LLMs, while jailbreaks produce chaotic, high-variance 'Semantic Turbulence'.&lt;/li&gt;&lt;li&gt;Introduces a zero-shot detector metric — variance of layer-wise cosine velocity — to identify jailbreaks via internal model dynamics without external classifiers or lexical filters.&lt;/li&gt;&lt;li&gt;Evaluates on small LLMs: finds a 75.4% turbulence increase under attack for RLHF-aligned Qwen2-1.5B and a 22.0% turbulence decrease for Gemma-2B, suggesting different refusal mechanisms.&lt;/li&gt;&lt;li&gt;Claims the metric is lightweight, real-time, and useful for diagnosing black-box model safety architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Hasib Ur Rahman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak detection', 'model interpretability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13741</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title><link>https://arxiv.org/abs/2512.13739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies controllable AIGC-assisted image production for journalism, focusing on misinformation, authenticity, semantic fidelity, and interpretability.&lt;/li&gt;&lt;li&gt;Experiment 1: applies standardized prompts across multiple platforms/scenes and finds differences in semantic alignment, cultural specificity, and realism driven by training-corpus bias and platform filtering.&lt;/li&gt;&lt;li&gt;Experiment 2: proposes a human-in-the-loop modular pipeline (SAM, GroundingDINO, BrushNet, Style-LoRA, Prompt-to-Prompt) with CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials to preserve editorial fidelity and traceability.&lt;/li&gt;&lt;li&gt;Proposes a human-AI collaboration mechanism and recommends evaluation metrics: Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yajie Yang', 'Yuqing Zhao', 'Xiaochao Xi', 'Yinan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC safety', 'misinformation / provenance', 'human-in-the-loop', 'image generation', 'semantic alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13739</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs</title><link>https://arxiv.org/abs/2512.13723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares responses of 10 Chinese and 10 American LLMs to large human datasets (Moral Foundations Questionnaire 2.0 and World Values Survey).&lt;/li&gt;&lt;li&gt;Finds that models from both countries respond more like American people than Chinese people, indicating persistence of US-centric values in LLM outputs.&lt;/li&gt;&lt;li&gt;Shows that prompting in Chinese or imposing a Chinese persona only slightly mitigates the American-value skew, with implications for normative influence and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Haslett', 'Linus Ta-Lun Huang', 'Leila Khalatbari', 'Janet Hui-wen Hsiao', 'Antoni B. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'cultural_bias', 'LLM_evaluation', 'societal_impact']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13723</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models</title><link>https://arxiv.org/abs/2512.13703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safe2Harm, a semantic isomorphism jailbreak that rewrites harmful prompts into semantically similar "safe" prompts, maps theme relationships, generates safe responses, then reconstructs harmful outputs via inverse mapping.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness on 7 mainstream LLMs across three benchmark datasets and reports superior jailbreaking capability compared to existing methods.&lt;/li&gt;&lt;li&gt;Provides a new challenging harmful-content evaluation dataset (358 samples) and assesses existing harmful-detection methods to inform defenses (input-output filtering).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'prompt injection', 'adversarial attack', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13703</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</title><link>https://arxiv.org/abs/2512.14474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Model-First Reasoning (MFR): a two-phase LLM agent design where the model first constructs an explicit problem model (entities, state variables, actions, constraints) and then generates a solution plan.&lt;/li&gt;&lt;li&gt;Evaluated across multiple planning domains (medical scheduling, route planning, resource allocation, logic puzzles, procedural synthesis) and compared to Chain-of-Thought and ReAct; MFR reduces constraint violations and improves solution quality.&lt;/li&gt;&lt;li&gt;Ablation studies indicate the explicit modeling phase is critical to gains, suggesting many planning failures stem from representational deficiencies rather than pure reasoning limits.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and documents prompts, evaluation procedures, and datasets for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Annu Rana', 'Gaurav Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM planning', 'Hallucination reduction', 'Robustness', 'Interpretability', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14474</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning</title><link>https://arxiv.org/abs/2512.13955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MURIM, a multi-dimensional reputation-based incentive mechanism for federated learning that jointly accounts for client reliability, privacy, resource capacity, latency, and fairness.&lt;/li&gt;&lt;li&gt;Includes a reliability verification module and allocates incentives based on contribution, latency, and reputation to discourage malicious or unreliable clients.&lt;/li&gt;&lt;li&gt;Reports empirical improvements: up to 18% better fairness, 5–9% reduction in privacy attack success, and up to 85% increased robustness against poisoning and noisy-gradient attacks on MNIST, FMNIST, and ADULT datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sindhuja Madabushi', 'Dawood Wasif', 'Jin-Hee Cho']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'incentives', 'reputation-systems', 'poisoning-robustness', 'privacy-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13955</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</title><link>https://arxiv.org/abs/2512.13771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Semantic Grounding Index (SGI), a geometric metric based on angular distances in embedding space to detect when RAG responses fail to engage retrieved context (hallucinate).&lt;/li&gt;&lt;li&gt;Theoretically derives bounds from spherical triangle inequality predicting improved discrimination with larger question-context angular separation, and empirically validates on a 5,000-sample HaluEval across five embedding models (high cross-model correlation).&lt;/li&gt;&lt;li&gt;Reports strong effect sizes (Cohen's d up to ~2.05 on subgroups), AUC improvements with larger θ(q,c), and calibration (ECE=0.10), but shows SGI measures topical engagement rather than factual accuracy (negative TruthfulQA result).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Javier Mar\\'in"]&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination_detection', 'embeddings/geometric_metrics', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13771</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title><link>https://arxiv.org/abs/2512.13762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Qualitative case study of an RLHF-aligned LLM over a long (86-turn) dialogue revealing asymmetrical behavior: Normal Performance (NP) in non-sensitive domains vs. Functional Refusal (FR) in sensitive/provider-policy domains.&lt;/li&gt;&lt;li&gt;Introduces the concept of Learned Incapacity (LI) to describe selective withholding of capabilities without attributing intent, and operationalizes three response regimes: NP, FR, and Meta-Narrative (MN).&lt;/li&gt;&lt;li&gt;Proposes an interaction-level auditing framework based on observable behavior and highlights MN role-framing narratives that co-occur with refusals, arguing this as a potential alignment side effect needing broader study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['TK Lee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'model_auditing', 'refusal_behavior', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13762</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making</title><link>https://arxiv.org/abs/2512.13716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ValuePilot, a two-phase framework for value-driven personalized decision-making consisting of a Dataset Generation Toolkit (DGT) and a Decision-Making Module (DMM).&lt;/li&gt;&lt;li&gt;DGT creates diverse, value-annotated scenarios via a human–LLM collaborative pipeline; DMM learns to evaluate actions according to individual value preferences.&lt;/li&gt;&lt;li&gt;Claims improved alignment with human action choices on unseen scenarios, outperforming several strong LLM baselines (e.g., GPT-5, Claude-Sonnet-4, Gemini-2-flash, Llama-3.1-70b).&lt;/li&gt;&lt;li&gt;Argues that value-driven decision-making yields more interpretable and generalizable behavior than task/reward-driven approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Luo', 'Ziang Chen', 'Hou Hei Lam', 'Jiayu zhan', 'Junqi Wang', 'Zhenliang Zhang', 'Xue Feng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'personalized decision-making', 'value alignment', 'dataset generation', 'human-LLM collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13716</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach</title><link>https://arxiv.org/abs/2512.13714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an AI-driven annotation pipeline that detects, labels, and corrects instability patterns in LLM outputs to reduce hallucinations and variability.&lt;/li&gt;&lt;li&gt;Combines automated weak supervision and confidence-based annotation with human validation to scale corrective feedback while preserving reliability and ethical oversight.&lt;/li&gt;&lt;li&gt;Introduces stability-specific annotation categories (semantic consistency, factual correctness, logical coherence) to create feedback loops for continuous calibration and robustness improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gangesh Pathak', 'Prasanna Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Annotation pipeline', 'Human-AI collaboration', 'Safety / alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13714</guid><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 06 Jun 2025 22:35:47 +0000</lastBuildDate><item><title>FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)</title><link>https://arxiv.org/abs/2506.05095</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Workshop focuses on advancing trustworthiness in facial affect analysis (FAA) systems.&lt;/li&gt;&lt;li&gt;Addresses safety, privacy, fairness, and explainability in Emotion AI-powered FAA tools.&lt;/li&gt;&lt;li&gt;Encourages research and discussion on mitigating biases, ensuring interpretability, and managing uncertainty in FAA.&lt;/li&gt;&lt;li&gt;Highlights the importance of ethical considerations and safety in the deployment of FAA technologies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written and effectively communicates the workshop's aims and scope, earning a high clarity score. However, as this is a workshop announcement rather than a research paper presenting new methods or results, its novelty and significance are moderate: it is novel in the sense of organizing a focused event on trustworthy facial affect analysis, but it does not introduce new technical contributions. The significance is moderate as well, as workshops can shape research directions but are not themselves research breakthroughs. Try-worthiness is marked as false because there is no specific method or system to implement or experiment with; the paper is an event announcement. No code repository is provided or expected for this type of publication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes&lt;/li&gt;&lt;li&gt;Tags: AI safety, privacy, fairness, explainability, facial affect analysis&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05095</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs</title><link>https://arxiv.org/abs/2506.04743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in Vision-Language Models (VLMs) to backdoor attacks involving both pixel-level and semantic triggers.&lt;/li&gt;&lt;li&gt;Proposes Semantic Reward Defense (SRD), a reinforcement learning-based framework to defend against backdoor attacks without prior knowledge of triggers.&lt;/li&gt;&lt;li&gt;SRD uses a Deep Q-Network to perturb sensitive image regions, guided by a semantic fidelity reward to maintain output quality.&lt;/li&gt;&lt;li&gt;Experimental results show significant reduction in attack success rates while preserving caption quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (backdoor attacks in VLMs), the vulnerabilities identified, and the proposed solution (SRD using reinforcement learning and semantic fidelity rewards). The approach appears novel, especially in its trigger-agnostic, interpretable defense using RL for semantic perturbation, which is not widely explored in the literature. The significance is high given the growing importance of VLM security, though the paper is very new and only on arXiv, so its impact is yet to be established. The method's strong reported results and generality make it worth trying for researchers or practitioners in the field. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, Dacheng Tao&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, vision-language models, reinforcement learning defense, semantic perturbation, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04743</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AuthGuard: Generalizable Deepfake Detection via Language Guidance</title><link>https://arxiv.org/abs/2506.04501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AuthGuard, a deepfake detection framework that leverages language guidance to improve generalization to novel, unseen forgeries.&lt;/li&gt;&lt;li&gt;Combines discriminative classification with image-text contrastive learning, using text generated by large multimodal language models (MLLMs) to capture commonsense reasoning.&lt;/li&gt;&lt;li&gt;Integrates data uncertainty learning to enhance robustness against noisy supervision.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on multiple deepfake detection benchmarks, including improved reasoning about deepfakes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though it is somewhat dense with technical terms. The approach of integrating language guidance and commonsense reasoning via MLLMs into deepfake detection is novel and addresses a key limitation of current methods—generalization to unseen forgeries. The reported improvements on multiple benchmarks (DFDC, DF40, DDVQA) are substantial, indicating strong significance, though the lack of peer review (arXiv preprint) and citations (due to recency) temper this slightly. The method appears promising and worth experimenting with, especially for those interested in robust, generalizable deepfake detection. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Guangyu Shen, Zhihua Li, Xiang Xu, Tianchen Zhao, Zheng Zhang, Dongsheng An, Zhuowen Tu, Yifan Xing, Qin Zhang&lt;/li&gt;&lt;li&gt;Tags: deepfake detection, AI security, robustness, vision-language models&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04501</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</title><link>https://arxiv.org/abs/2504.17934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies privacy and security risks specific to LLM-powered GUI agents.&lt;/li&gt;&lt;li&gt;Highlights the lack of existing evaluation frameworks addressing these risks.&lt;/li&gt;&lt;li&gt;Proposes a human-centered evaluation framework that incorporates risk assessment and user consent.&lt;/li&gt;&lt;li&gt;Discusses challenges in integrating human evaluators for privacy and security assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, risks, and proposed direction for evaluating LLM-powered GUI agents, justifying a high clarity score. The focus on privacy and security risks, as well as a human-centered evaluation framework, is a relatively novel angle compared to the current performance-centric evaluations, earning a strong novelty score. However, as a position paper without empirical results or a concrete framework, its immediate significance is moderate, especially since it is a recent preprint with no citations and not yet peer-reviewed. The lack of implementation details or code means it is not directly try-worthy at this stage, but it may inspire future work. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li&lt;/li&gt;&lt;li&gt;Tags: AI security, privacy risks, risk assessment, LLM agents, human-centered evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17934</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models</title><link>https://arxiv.org/abs/2502.12414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates hallucination (fabricated outputs) in large-scale speech foundation models, particularly in automatic speech recognition (ASR).&lt;/li&gt;&lt;li&gt;Introduces the Hallucination Error Rate (HER) metric to quantify hallucinations, addressing limitations of traditional metrics like WER and CER.&lt;/li&gt;&lt;li&gt;Analyzes the impact of distribution shift, model size, and architecture on hallucination rates, including the effects of adversarial and common audio perturbations.&lt;/li&gt;&lt;li&gt;Highlights the risks of hallucinations in high-stakes domains (e.g., healthcare, legal, aviation) and the need for improved evaluation metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, problem, and key findings. The introduction of the Hallucination Error Rate (HER) as a new metric for evaluating ASR models, especially in high-stakes domains, is a novel contribution. The analysis across 20+ models and the focus on distribution shift and synthetic noise add further value. While the paper is a preprint and very recent (hence no citations yet), the topic is highly relevant given the increasing deployment of speech foundation models in critical applications. The work is significant for practitioners and researchers concerned with ASR reliability and safety. However, there is no code repository provided, which slightly limits immediate reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj&lt;/li&gt;&lt;li&gt;Tags: AI safety, hallucination, robustness, adversarial attacks, speech recognition&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12414</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers</title><link>https://arxiv.org/abs/2506.05038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an automatic framework (AR-Checker) for stress testing the robustness of large language models (LLMs) as mathematical problem solvers.&lt;/li&gt;&lt;li&gt;Generates semantically equivalent mathematical problem variants to test LLMs' robustness and identify failure cases.&lt;/li&gt;&lt;li&gt;Evaluates the framework on multiple benchmarks, demonstrating its effectiveness in uncovering robustness issues.&lt;/li&gt;&lt;li&gt;Extends robustness testing beyond mathematics to general reasoning and commonsense tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method, and results. The proposed AR-Checker framework introduces a novel, automated approach to robustness stress testing for LLMs, moving beyond hand-crafted templates and static perturbation rules. This is a timely and important contribution, as robustness is a key concern for LLM deployment, especially in mathematical reasoning tasks. The significance is high given the broad applicability (mathematics and beyond), though as a recent preprint, it has not yet accumulated citations or peer-reviewed validation. The method appears practical and worth experimenting with, especially for those interested in LLM evaluation and robustness. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yutao Hou, Zeguan Xiao, Fei Yu, Yihan Jiang, Xuetao Wei, Hailiang Huang, Yun Chen, Guanhua Chen&lt;/li&gt;&lt;li&gt;Tags: robustness, LLM evaluation, adversarial testing, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05038</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat</title><link>https://arxiv.org/abs/2506.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPARTA ALIGNMENT, a method for collectively aligning multiple LLMs through competitive interactions and peer evaluation.&lt;/li&gt;&lt;li&gt;Models compete in duels to fulfill instructions, with other models acting as judges, and results are aggregated using an Elo-based reputation system.&lt;/li&gt;&lt;li&gt;The approach generates preference data from peer-evaluated competitions, which is then used to iteratively improve model alignment.&lt;/li&gt;&lt;li&gt;Experiments show improved performance and generalization compared to self-alignment baselines, leveraging model diversity for better outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the proposed SPARTA ALIGNMENT algorithm. The idea of aligning multiple LLMs through competitive 'combat' and peer evaluation is novel and leverages collective intelligence, which is a fresh approach compared to traditional self-alignment or single-model methods. The reported improvements over baselines and generalization to unseen tasks suggest significant potential impact. While the paper is very new and has no citations yet, its methodology and results make it worth experimenting with, especially for researchers interested in LLM alignment and ensemble methods. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov&lt;/li&gt;&lt;li&gt;Tags: AI alignment, LLM alignment, collective alignment, preference learning, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04721</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Normative Conflicts and Shallow AI Alignment</title><link>https://arxiv.org/abs/2506.04679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the limitations of current LLM alignment strategies in preventing misuse and adversarial attacks.&lt;/li&gt;&lt;li&gt;Argues that existing methods result in 'shallow alignment,' making LLMs vulnerable to manipulation via normative conflicts.&lt;/li&gt;&lt;li&gt;Compares LLMs' lack of normative deliberation to human moral reasoning, highlighting a key vulnerability.&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and the need for improved alignment approaches to mitigate risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the problem (normative conflicts in LLM alignment), the limitations of current approaches, and the theoretical grounding in moral psychology. The paper offers a novel perspective by framing the alignment problem as a lack of genuine normative deliberation, rather than just a technical or behavioral issue. While the topic is highly significant for AI safety, the paper appears to be primarily theoretical and critical, rather than proposing a concrete method or system to implement or experiment with. As such, it is not directly 'try-worthy' for implementation, but is valuable for informing future research directions. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rapha\"el Milli\`ere&lt;/li&gt;&lt;li&gt;Tags: AI alignment, AI safety, adversarial attacks, normative conflicts&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04679</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Demonstrations of Integrity Attacks in Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.04572</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates integrity attacks in Multi-Agent Systems (MAS) using Large Language Models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates four types of prompt manipulation attacks (Scapegoater, Boaster, Self-Dealer, Free-Rider) that allow malicious agents to bias MAS operations.&lt;/li&gt;&lt;li&gt;Shows that these attacks can bypass advanced LLM-based monitoring systems, revealing current detection limitations.&lt;/li&gt;&lt;li&gt;Highlights the need for robust security protocols and improved monitoring in MAS architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, attack types, and findings, though some technical details are omitted (typical for abstracts). The work is highly novel, focusing on integrity attacks in multi-agent LLM systems—a relatively unexplored but increasingly important area. The significance is high given the growing use of LLM-based MAS and the demonstrated ability to bypass state-of-the-art monitors, though the preprint status and lack of citations (due to recency) slightly temper this. The paper is worth experimenting with, especially for researchers or practitioners concerned with MAS security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Can Zheng, Yuhan Cao, Xiaoning Dong, Tianxing He&lt;/li&gt;&lt;li&gt;Tags: multi-agent systems, prompt manipulation, integrity attacks, LLM security, adversarial prompting&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04572</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Large Language Models with Implicit Preferences from User-Generated Content</title><link>https://arxiv.org/abs/2506.04463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework (PUGC) for aligning large language models using implicit preferences extracted from user-generated content (UGC).&lt;/li&gt;&lt;li&gt;Aims to improve alignment with human values and response quality without relying on costly curated preference data.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and domain-specific alignment through experiments, including robustness and theory of mind evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method (PUGC), and results, though some technical details are omitted as expected in an abstract. The approach of leveraging implicit preferences from unlabeled user-generated content for LLM alignment is novel and addresses a key scalability bottleneck in preference learning. The reported improvements on Alpaca Eval 2 and state-of-the-art results using Mistral-7B-Instruct suggest the work is significant, especially for practitioners interested in scalable and domain-specific LLM alignment. The paper is very recent (June 2025) and on arXiv, so citation count is not yet meaningful, but the results and the open-source code/dataset make it worth trying. The code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhaoxuan Tan, Zheng Li, Tianyi Liu, Haodong Wang, Hyokun Yun, Ming Zeng, Pei Chen, Zhihan Zhang, Yifan Gao, Ruijie Wang, Priyanka Nigam, Bing Yin, Meng Jiang&lt;/li&gt;&lt;li&gt;Tags: AI alignment, preference learning, LLM safety, human values&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://zhaoxuan.info/PUGC.github.io/'&gt;https://zhaoxuan.info/PUGC.github.io/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04463</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Selective Homomorphic Encryption Approach for Faster Privacy-Preserving Federated Learning</title><link>https://arxiv.org/abs/2501.12911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FAS, a federated learning framework combining selective homomorphic encryption, differential privacy, and bitwise scrambling for enhanced privacy and security.&lt;/li&gt;&lt;li&gt;Addresses the trade-off between strong cryptographic protections and computational efficiency in privacy-preserving federated learning, especially for healthcare data.&lt;/li&gt;&lt;li&gt;Demonstrates that FAS is significantly faster than fully homomorphic encryption and other secure FL frameworks, while maintaining comparable security against gradient inversion attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on real medical imaging datasets, showing practical applicability for latency-sensitive, privacy-critical environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results, though some technical details (e.g., specifics of 'selective' encryption) could be clearer. The combination of selective homomorphic encryption, differential privacy, and bitwise scrambling for federated learning appears novel, especially in the context of healthcare. The work is significant for privacy-preserving federated learning, particularly for latency-sensitive applications, but as an arXiv preprint with no citations yet, its impact is not fully established. The reported speedups and comparable security to FHE make it worth trying for practitioners. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Abdulkadir Korkmaz, Praveen Rao&lt;/li&gt;&lt;li&gt;Tags: federated learning, privacy-preserving machine learning, homomorphic encryption, gradient inversion attacks, differential privacy&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.12911</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs</title><link>https://arxiv.org/abs/2503.09117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of unlearning specific information from large language models (LLMs) for privacy and copyright compliance.&lt;/li&gt;&lt;li&gt;Proposes a new method, Gradient Rectified Unlearning (GRU), to minimize the negative impact of unlearning on the model's general performance.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of GRU across multiple unlearning benchmarks, suggesting practical utility for safer LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (Gradient Rectified Unlearning, GRU) for mitigating the trade-off between unlearning and retention in LLMs. The approach appears novel, focusing on regulating gradient directions to minimize side effects during unlearning, which is a current and important challenge in LLM deployment. The significance is high given the increasing legal and ethical need for effective unlearning in deployed models, though the paper is very recent and has not yet accumulated citations. The method is described as easy and general to implement, and its effectiveness is demonstrated on benchmarks, making it worth trying for practitioners and researchers. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han&lt;/li&gt;&lt;li&gt;Tags: LLM unlearning, privacy, model safety, retention trade-off&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09117</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models</title><link>https://arxiv.org/abs/2503.07697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedParrot, a novel data poisoning attack that causes LLMs to generate copyrighted content.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack is stealthy and effective, even when the model was not directly trained on the copyrighted material.&lt;/li&gt;&lt;li&gt;Finds that existing defenses are largely ineffective against this type of attack.&lt;/li&gt;&lt;li&gt;Proposes a new defense mechanism, ParrotTrap, to mitigate copyright-infringement poisoning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method (PoisonedParrot), results, and a proposed defense (ParrotTrap). The work is highly novel, introducing a new type of data poisoning attack that can induce LLMs to generate copyrighted content without direct exposure to the full copyrighted material. This is a timely and important topic given ongoing legal and ethical concerns around LLMs and copyright. The significance is high due to the potential impact on both AI safety and copyright law, though as a very recent preprint, it has not yet accumulated citations or been peer-reviewed. The proposed attack and defense are both of practical interest, making the paper worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang&lt;/li&gt;&lt;li&gt;Tags: data poisoning, LLM security, copyright risks, adversarial attacks, defense mechanisms&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07697</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>
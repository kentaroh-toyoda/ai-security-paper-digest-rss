<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 16 Jan 2026 23:00:11 +0000</lastBuildDate><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a major vulnerability in example-level unlearning: 'forget-set' accuracy often recovers almost fully after fine-tuning on only the retain set (no forget examples).&lt;/li&gt;&lt;li&gt;Shows this relearning effect is widespread across many unlearning methods, whereas retraining-from-scratch prevents recovery.&lt;/li&gt;&lt;li&gt;Finds weight-space metrics (L2 distance, linear mode connectivity) predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes weight-space regularization techniques that substantially improve tamper-resistant unlearning against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'relearning attacks', 'defense/robustness', 'weight-space regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</title><link>https://arxiv.org/abs/2505.08814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of four neural network coverage metrics (primary functionality, boundary, hierarchy, structural) across DNN architectures (LeNet, VGG, ResNet) and depths (5–54 layers).&lt;/li&gt;&lt;li&gt;Analyzes relationships between model depth/configuration and coverage metrics, and examines how modified decision/condition coverage relates to dataset size.&lt;/li&gt;&lt;li&gt;Frames the work in the context of security-related testing of DNNs and proposes future directions to aid security testing of image recognition models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Li', 'Xiaoqi Li', 'Yingjie Mao', 'Yishun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['coverage metrics', 'security testing', 'empirical study', 'neural network robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.08814</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images</title><link>https://arxiv.org/abs/2502.05066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and demonstrates a novel vulnerability: state-of-the-art diffusion models generate NSFW/offensive text embedded in generated images across multiple models.&lt;/li&gt;&lt;li&gt;Shows existing visual-content mitigations fail to prevent harmful text generation and often degrade benign text output.&lt;/li&gt;&lt;li&gt;Proposes a targeted fine-tuning defense focusing on text-generation layers using paired NSFW/benign-image examples, preserving benign text and image quality.&lt;/li&gt;&lt;li&gt;Releases ToxicBench: dataset, harmful prompts, metrics, and evaluation pipeline for benchmarking NSFW text generation in text-to-image models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Kumar', 'Tom Blanchard', 'Adam Dziedzic', 'Franziska Boenisch']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'content-moderation', 'safety/defense', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05066</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm</title><link>https://arxiv.org/abs/2501.14230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GreedyPixel, a black-box adversarial attack that performs per-pixel greedy optimization guided by a surrogate-derived priority map and refined with query feedback.&lt;/li&gt;&lt;li&gt;Method evaluates coordinates without gradients, guarantees monotonic loss reduction and convergence to a coordinate-wise optimum, aiming for pixel-wise sparsity and high perceptual quality.&lt;/li&gt;&lt;li&gt;Empirical results on CIFAR-10 and ImageNet across CNNs and Vision Transformers show state-of-the-art success rates with visually imperceptible perturbations; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanrui Wang', 'Ching-Chun Chang', 'Chun-Shien Lu', 'Christopher Leckie', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'black-box attack', 'pixel-wise attack', 'robustness evaluation', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14230</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrated safety evaluation of seven frontier LLMs/MLLMs (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image-generation settings.&lt;/li&gt;&lt;li&gt;Uses a unified protocol combining benchmark evaluation, adversarial evaluation (red-teaming), multilingual testing, and compliance/regulatory checks to produce safety leaderboards and model safety profiles.&lt;/li&gt;&lt;li&gt;Finds a heterogeneous safety landscape: GPT-5.2 performs consistently well, while other models show trade-offs across benchmark safety, adversarial alignment, multilingual generalization, and compliance; language and VLM modalities are notably vulnerable under adversarial evaluation.&lt;/li&gt;&lt;li&gt;Notes that text-to-image models better align on regulated visual risk categories but remain brittle to adversarial or semantically ambiguous prompts, underscoring the need for standardized, multidimensional safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial evaluation', 'multimodal models', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Evasion Attacks on Computer Vision using SHAP Values</title><link>https://arxiv.org/abs/2601.10587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a white-box adversarial evasion attack for computer vision that uses SHAP values to identify input features (pixels/regions) most influential to model outputs.&lt;/li&gt;&lt;li&gt;Crafts imperceptible perturbations guided by SHAP to reduce confidence or induce misclassification, and compares effectiveness to FGSM.&lt;/li&gt;&lt;li&gt;Claims improved robustness of SHAP-guided attacks in scenarios where gradients are hidden or obfuscated, highlighting a novel explainability-based attack vector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Frank Mollard', 'Marcus Becker', 'Florian Roehrbein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'evasion-attack', 'white-box-attack', 'explainability-based-attack', 'gradient-hiding-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10587</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification</title><link>https://arxiv.org/abs/2601.09806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end pipeline to create adversarial patches for facial biometric systems: initial FGSM noise generation followed by diffusion-model-based refinement (reverse diffusion, Gaussian smoothing, brightness correction) to improve imperceptibility.&lt;/li&gt;&lt;li&gt;Evaluates the patches on facial identity verification and expression recognition, measuring changes in identity classification and captioning outputs (ViT-GPT2) to support forensic interpretation of identity-evasion attacks.&lt;/li&gt;&lt;li&gt;Introduces detection/forensic methods (perceptual hashing and segmentation) to locate and analyze adversarial patches, reporting high reconstruction/visual similarity (SSIM ≈ 0.95).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahrzad Sayyafzadeh', 'Hongmei Chi', 'Shonda Bernadin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patches', 'biometric-attacks', 'diffusion-models', 'adversarial-detection', 'forensic-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09806</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation</title><link>https://arxiv.org/abs/2510.25677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZK-SenseLM: a verifiable wireless-sensing system that couples a large-model RF encoder with a policy-grounded decision layer and end-to-end zero-knowledge proofs attesting to inference outputs.&lt;/li&gt;&lt;li&gt;Adds a calibrated selective-abstention head whose registered risk-coverage operating point is cryptographically bound into proofs to reduce unsafe actions under distribution shift.&lt;/li&gt;&lt;li&gt;Describes a four-stage proving pipeline (feature sanity/commitment, threshold/version binding, time-window binding, PLONK-style proof of quantized network execution), with micro-batched proving and gateway offload for low-power devices.&lt;/li&gt;&lt;li&gt;Integrates with differentially private federated learning and on-device personalization while preserving verifiability; demonstrates improved calibration, coverage-risk behavior under perturbation, and tamper/replay rejection with compact proofs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Mari Eplik', 'Javier Rojas', 'Aina Binti Abdullah', 'Pieter van der Merwe']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'verifiability', 'zero-knowledge-proofs', 'selective-abstention', 'privacy-preserving-federated-learning', 'wireless/RF-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25677</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BASIL: Bayesian Assessment of Sycophancy in LLMs</title><link>https://arxiv.org/abs/2508.16846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian probabilistic framework to disentangle sycophantic belief shifts from rational belief updating in LLMs.&lt;/li&gt;&lt;li&gt;Presents two metrics: a descriptive metric controlling for rational responses to evidence, and a normative metric quantifying deviations from Bayesian-consistent updating; both can be applied without ground-truth labels.&lt;/li&gt;&lt;li&gt;Empirically finds robust sycophantic belief shifts across multiple LLMs and tasks, and shows impact on rationality depends on model update behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates mitigation strategies (post-hoc calibration, supervised fine-tuning (SFT), and direct preference optimization (DPO)) that reduce Bayesian inconsistency and improve robustness under sycophancy prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Atwell', 'Pedram Heydari', 'Anthony Sicilia', 'Malihe Alikhani']&lt;/li&gt;&lt;li&gt;Tags: ['model-safety', 'alignment', 'robustness', 'behavioral-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16846</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title><link>https://arxiv.org/abs/2510.19670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;CoSense-LLM is an edge-first system that converts continuous multimodal sensor streams into compact semantic tokens and coordinates with LLMs under latency, energy, bandwidth, and privacy constraints.&lt;/li&gt;&lt;li&gt;Key components: SenseFusion (compresses sensor embeddings into discrete codes), Edge-RAG (local retrieval grounding), PromptRouter (cost- and uncertainty-aware decision policy for edge vs cloud), and Secure Execution (auditable redaction ensuring raw data never leaves the device).&lt;/li&gt;&lt;li&gt;System emphasizes privacy and predictable latency: transmits only discrete codes/redacted metadata, supports on-device personalization and federated updates, and leverages serving optimizations to reduce cost and energy.&lt;/li&gt;&lt;li&gt;Ablations demonstrate Edge-RAG improves factual consistency, calibrated uncertainty enables selective abstention/escalation, and KV/decoding accelerators lower energy per decision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Mari Eplik', 'Javier Rojas', 'Aina Binti Abdullah', 'Pieter van der Merwe']&lt;/li&gt;&lt;li&gt;Tags: ['edge-ml', 'privacy-preserving', 'edge-RAG', 'uncertainty-aware routing', 'secure execution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19670</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay</title><link>https://arxiv.org/abs/2601.10589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety Self-Play (SSP), where a single LLM concurrently plays Attacker (generating jailbreaks) and Defender (refusing harmful requests) within a unified reinforcement learning loop.&lt;/li&gt;&lt;li&gt;Introduces a Reflective Experience Replay mechanism that accumulates episodes and uses an Upper Confidence Bound (UCB) sampling strategy to prioritize failure cases and balance exploration/exploitation.&lt;/li&gt;&lt;li&gt;Empirical results claim SSP autonomously evolves stronger defenses and outperforms baselines trained on static adversarial datasets, establishing a proactive safety-alignment benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Wang', 'Yanting Wang', 'Hao Li', 'Rui Li', 'Lei Sha']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'adversarial training', 'defenses', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10589</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</title><link>https://arxiv.org/abs/2601.10543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs exhibit latent safety-related signals during generation even when jailbreaks succeed, but these are overridden by fluent continuation.&lt;/li&gt;&lt;li&gt;Proposes an in-decoding safety-awareness probing method that surfaces and uses these latent signals to detect and block unsafe completions early in the decoding process.&lt;/li&gt;&lt;li&gt;Empirical results across diverse jailbreak attacks show substantially improved defense effectiveness, low over-refusal on benign inputs, and preserved response quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinzhi Zhao', 'Ming Wang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yifei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'decoding-time defenses', 'safety probing', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10543</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrated safety evaluation of seven frontier LLMs/MLLMs (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image-generation settings.&lt;/li&gt;&lt;li&gt;Uses a unified protocol combining benchmark evaluation, adversarial evaluation (red-teaming), multilingual testing, and compliance/regulatory checks to produce safety leaderboards and model safety profiles.&lt;/li&gt;&lt;li&gt;Finds a heterogeneous safety landscape: GPT-5.2 performs consistently well, while other models show trade-offs across benchmark safety, adversarial alignment, multilingual generalization, and compliance; language and VLM modalities are notably vulnerable under adversarial evaluation.&lt;/li&gt;&lt;li&gt;Notes that text-to-image models better align on regulated visual risk categories but remain brittle to adversarial or semantically ambiguous prompts, underscoring the need for standardized, multidimensional safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial evaluation', 'multimodal models', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale</title><link>https://arxiv.org/abs/2601.10338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical analysis of agent skills: collected 42,447 skills and systematically analyzed 31,132 using SkillScan, finding 26.1% contain at least one vulnerability across 14 patterns (prompt injection, data exfiltration, privilege escalation, supply chain risks).&lt;/li&gt;&lt;li&gt;Produced a grounded vulnerability taxonomy from 8,126 vulnerable skills and a validated detection pipeline (86.7% precision, 82.5% recall); released an open dataset and detection toolkit.&lt;/li&gt;&lt;li&gt;Key findings and recommendations: skills with executable scripts are ~2.12x more likely to be vulnerable, 5.2% exhibit high-severity malicious patterns, and the paper calls for capability-based permissions and mandatory security vetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent skills', 'prompt injection', 'data exfiltration', 'supply chain attacks', 'vulnerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10338</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack</title><link>https://arxiv.org/abs/2601.10173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ReasAlign, a model-level defense that uses structured reasoning steps to analyze queries, detect conflicting/malicious instructions, and preserve user-intended task flow to defend against indirect prompt injection.&lt;/li&gt;&lt;li&gt;Introduces a test-time scaling mechanism with a preference-optimized judge model that scores reasoning trajectories and selects the best-safe output.&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks (including CyberSecEval2) demonstrating substantially improved trade-off between utility and attack success rate compared to prior guardrails (e.g., Meta SecAlign).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Yankai Yang', 'G. Edward Suh', 'Ning Zhang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'adversarial robustness', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10173</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute</title><link>https://arxiv.org/abs/2601.09756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLM-generated synthetic veterinary EHR narratives improve de-identification safety under two training regimes: synthetic augmentation (epoch-scaled) and fixed-budget substitution.&lt;/li&gt;&lt;li&gt;Controlled simulation on a PetEVAL-derived corpus with 10,382 template-only synthetic notes and three transformer backbones (PetBERT, VetBERT, Bio_ClinicalBERT). Primary safety metric: document-level leakage rate.&lt;/li&gt;&lt;li&gt;Findings: substituting real notes with synthetic ones increases leakage; moderate synthetic mixing can match real-only performance under compute-matched training; epoch-scaled synthetic augmentation reduces leakage and improves F1 largely due to increased training exposure rather than intrinsic synthetic quality.&lt;/li&gt;&lt;li&gt;Conclusion: synthetic data is useful as complementary augmentation to increase exposure but cannot safely substitute real supervision for safety-critical de-identification; systematic synthetic-real mismatches explain persistent leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Brundage']&lt;/li&gt;&lt;li&gt;Tags: ['de-identification', 'synthetic-data', 'privacy-leakage', 'data-augmentation', 'healthcare-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09756</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models</title><link>https://arxiv.org/abs/2601.10387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a dominant 'Assistant Axis' in activation/persona space of LLMs that captures the model's default helpful Assistant persona and shows it exists already in pre-trained models.&lt;/li&gt;&lt;li&gt;Shows that deviations along this axis predict 'persona drift' into harmful, bizarre, or non-Assistant behaviors (including susceptibility to persona-based jailbreaks), and that certain conversational contexts (meta-reflection, emotional vulnerability) drive drift.&lt;/li&gt;&lt;li&gt;Demonstrates a defense: constraining activations to a fixed region along the Assistant Axis stabilizes behavior and mitigates adversarial persona-based jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christina Lu', 'Jack Gallagher', 'Jonathan Michala', 'Kyle Fish', 'Jack Lindsey']&lt;/li&gt;&lt;li&gt;Tags: ['persona steering', 'jailbreak defense', 'activation-based mitigation', 'model robustness', 'adversarial persona attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10387</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback</title><link>https://arxiv.org/abs/2601.10156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TS-Bench, a benchmark for step-level tool invocation safety detection in LLM-based agents.&lt;/li&gt;&lt;li&gt;Proposes TS-Guard, a multi-task RL-based guardrail model that proactively detects unsafe tool invocation actions before execution and provides interpretable safety judgments and feedback.&lt;/li&gt;&lt;li&gt;Presents TS-Flow, a guardrail-feedback-driven reasoning framework that reduces harmful tool invocations (ReAct-style agents) by ~65% and improves benign task completion by ~10% under prompt injection attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutao Mou', 'Zhangchi Xue', 'Lijun Li', 'Peiyang Liu', 'Shikun Zhang', 'Wei Ye', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['tool-invocation safety', 'prompt injection', 'guardrails', 'adversarial defense', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10156</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox</title><link>https://arxiv.org/abs/2601.09721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM safety under anxiety-driven adversarial user pressures in pediatric consultations using PediatricAnxietyBench (300 queries, authentic vs adversarial).&lt;/li&gt;&lt;li&gt;Compares three models (Llama-3.3-70B, Llama-3.1-8B, Mistral-7B) across platforms, measuring safety on restraint, referral, hedging, emergency recognition, and non-prescriptive behavior.&lt;/li&gt;&lt;li&gt;Finds smaller models can outperform larger ones (scale paradox), identifies specific vulnerabilities (e.g., failure to recognize emergencies, inappropriate seizure diagnoses), and shows robustness evolution across model releases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vahideh Zolfaghari']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial Evaluation', 'Safety Benchmark', 'Red Teaming', 'Medical AI Safety', 'Model Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09721</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Random Walk Learning and the Pac-Man Attack</title><link>https://arxiv.org/abs/2508.05663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new adversarial threat to RW-based decentralized learning called the "Pac-Man" attack, where a malicious node probabilistically kills random walks that visit it, stealthily causing RW extinction and halting learning.&lt;/li&gt;&lt;li&gt;Proposes the Average Crossing (AC) algorithm, a decentralized duplication mechanism to prevent RW extinction and sustain learning in the presence of Pac-Man.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees that AC keeps the RW population almost surely bounded and that RW-based SGD converges under attack, with quantified deviation from the true optimum.&lt;/li&gt;&lt;li&gt;Presents empirical results on synthetic and real datasets that validate theory and reveal a phase transition in extinction probability versus duplication threshold, with additional analysis of a simplified AC variant.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingran Chen', 'Parimal Parag', 'Rohit Bhagat', 'Zonghong Liu', 'Salim El Rouayheb']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'distributed/decentralized learning', 'random walk algorithms', 'defense/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05663</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</title><link>https://arxiv.org/abs/2505.08814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of four neural network coverage metrics (primary functionality, boundary, hierarchy, structural) across DNN architectures (LeNet, VGG, ResNet) and depths (5–54 layers).&lt;/li&gt;&lt;li&gt;Analyzes relationships between model depth/configuration and coverage metrics, and examines how modified decision/condition coverage relates to dataset size.&lt;/li&gt;&lt;li&gt;Frames the work in the context of security-related testing of DNNs and proposes future directions to aid security testing of image recognition models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Li', 'Xiaoqi Li', 'Yingjie Mao', 'Yishun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['coverage metrics', 'security testing', 'empirical study', 'neural network robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.08814</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm</title><link>https://arxiv.org/abs/2501.14230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GreedyPixel, a black-box adversarial attack that performs per-pixel greedy optimization guided by a surrogate-derived priority map and refined with query feedback.&lt;/li&gt;&lt;li&gt;Method evaluates coordinates without gradients, guarantees monotonic loss reduction and convergence to a coordinate-wise optimum, aiming for pixel-wise sparsity and high perceptual quality.&lt;/li&gt;&lt;li&gt;Empirical results on CIFAR-10 and ImageNet across CNNs and Vision Transformers show state-of-the-art success rates with visually imperceptible perturbations; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanrui Wang', 'Ching-Chun Chang', 'Chun-Shien Lu', 'Christopher Leckie', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'black-box attack', 'pixel-wise attack', 'robustness evaluation', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14230</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a major vulnerability in example-level unlearning: 'forget-set' accuracy often recovers almost fully after fine-tuning on only the retain set (no forget examples).&lt;/li&gt;&lt;li&gt;Shows this relearning effect is widespread across many unlearning methods, whereas retraining-from-scratch prevents recovery.&lt;/li&gt;&lt;li&gt;Finds weight-space metrics (L2 distance, linear mode connectivity) predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes weight-space regularization techniques that substantially improve tamper-resistant unlearning against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'relearning attacks', 'defense/robustness', 'weight-space regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Permissive Information-Flow Analysis for Large Language Models</title><link>https://arxiv.org/abs/2410.03055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: standard dynamic information-flow/taint tracking for LLMs is overly conservative (propagating the strictest input label to outputs) and can block legitimate use when inputs come from diverse sources.&lt;/li&gt;&lt;li&gt;Proposal: a permissive label propagation approach that only carries forward labels of input samples influential in generating an LLM's output, removing labels for unnecessary inputs.&lt;/li&gt;&lt;li&gt;Implementations: two variants evaluated — (i) prompt-based retrieval augmentation, and (ii) k-nearest-neighbors language model; compared to an introspection-based baseline.&lt;/li&gt;&lt;li&gt;Results: in an LLM agent setting, the permissive label propagator outperforms the baseline in over 85% of cases, demonstrating practical improvements in balancing security and permissiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Radhika Gaonkar', 'Boris K\\"opf', 'David Krueger', 'Andrew Paverd', 'Ahmed Salem', 'Shruti Tople', 'Lukas Wutschitz', 'Menglin Xia', "Santiago Zanella-B\\'eguelin"]&lt;/li&gt;&lt;li&gt;Tags: ['information-flow', 'taint-tracking', 'LLM-security', 'data-leakage', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03055</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Fails to Remove Data Poisoning Attacks</title><link>https://arxiv.org/abs/2406.17216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates practical machine unlearning methods for large-scale deep learning with respect to removing data poisoning attacks.&lt;/li&gt;&lt;li&gt;Shows existing unlearning techniques largely fail to eliminate effects of various poisoning attacks (indiscriminate, targeted, and a new Gaussian poisoning attack) across models including image classifiers and LLMs, even with substantial compute budgets.&lt;/li&gt;&lt;li&gt;Introduces new evaluation metrics for unlearning based on poisoning scenarios and concludes unlearning currently offers limited benefit over full retraining and lacks provable guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Pawelczyk', 'Jimmy Z. Di', 'Yiwei Lu', 'Gautam Kamath', 'Ayush Sekhari', 'Seth Neel']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'machine unlearning', 'adversarial attacks', 'defense evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17216</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrated safety evaluation of seven frontier LLMs/MLLMs (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image-generation settings.&lt;/li&gt;&lt;li&gt;Uses a unified protocol combining benchmark evaluation, adversarial evaluation (red-teaming), multilingual testing, and compliance/regulatory checks to produce safety leaderboards and model safety profiles.&lt;/li&gt;&lt;li&gt;Finds a heterogeneous safety landscape: GPT-5.2 performs consistently well, while other models show trade-offs across benchmark safety, adversarial alignment, multilingual generalization, and compliance; language and VLM modalities are notably vulnerable under adversarial evaluation.&lt;/li&gt;&lt;li&gt;Notes that text-to-image models better align on regulated visual risk categories but remain brittle to adversarial or semantically ambiguous prompts, underscoring the need for standardized, multidimensional safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial evaluation', 'multimodal models', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior</title><link>https://arxiv.org/abs/2601.10440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentGuardian, a runtime security framework that learns context-aware access-control policies for AI agents from controlled staging execution traces.&lt;/li&gt;&lt;li&gt;Enforces adaptive policies governing tool calls based on real-time input context and control-flow dependencies of multi-step agent actions.&lt;/li&gt;&lt;li&gt;Evaluated on two real-world AI agent applications, showing detection of malicious/misleading inputs and mitigation of hallucination-driven or orchestration-level errors while preserving normal functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nadya Abaev', 'Denis Klimov', 'Gerard Levinov', 'David Mimran', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'runtime monitoring', 'agent security', 'policy learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10440</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations</title><link>https://arxiv.org/abs/2601.10004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematizes the threat landscape for LLMs in healthcare across data preprocessing, fine-tuning, and inference, including adversary models and attack surfaces.&lt;/li&gt;&lt;li&gt;Surveys and categorizes privacy-preserving techniques (e.g., differential privacy, federated learning, encryption, data sanitization) and assesses their applicability to clinical settings.&lt;/li&gt;&lt;li&gt;Identifies limitations and gaps of current defenses across heterogeneous deployment tiers and provides phase-aware recommendations and future research directions for strengthening privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohoshin Ara Tahera', 'Karamveer Singh Sidhu', 'Shuvalaxmi Dass', 'Sajal Saha']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'threat-model', 'privacy-preserving-techniques', 'healthcare', 'systematization-of-knowledge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10004</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method</title><link>https://arxiv.org/abs/2601.09933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FGSM DICNN: a malware classification model combining diluted (dilated) convolutions with Fast Gradient Sign Method (FGSM) perturbations during training.&lt;/li&gt;&lt;li&gt;Diluted convolutions increase receptive field to capture dispersed malware patterns with fewer features; FGSM is used as a one-step adversarial training strategy to improve robustness/accuracy at low cost.&lt;/li&gt;&lt;li&gt;Claims high performance on Android malware classification (99.44% accuracy) and outperforms a baseline/custom DCNN.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Anand', 'Bhupendra Singh', 'Sunil Khemka', 'Bireswar Banerjee', 'Vishi Singh Bhatia', 'Piyush Ranjan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'FGSM', 'malware-classification', 'robustness', 'diluted-convolution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09933</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Novel Contrastive Loss for Zero-Day Network Intrusion Detection</title><link>https://arxiv.org/abs/2601.09902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel contrastive loss for ML-based network intrusion detection that aims to generalise to zero-day attack classes while maintaining robustness to imbalanced data.&lt;/li&gt;&lt;li&gt;Rather than training purely as an anomaly detector, the model learns benign traffic distributions using benign samples plus known attack classes (excluding the zero-day class) to improve detection of unseen attacks.&lt;/li&gt;&lt;li&gt;Evaluated on the Lycos2017 dataset, reporting AUROC improvements for known and zero-day attack detection and substantial OpenAUC gains for open-set recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Wilkie', 'Hanan Hindy', 'Craig Michie', 'Christos Tachtatzis', 'James Irvine', 'Robert Atkinson']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'network intrusion detection', 'zero-day attack detection', 'contrastive learning', 'open-set recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09902</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis</title><link>https://arxiv.org/abs/2601.10701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CEPAM, a federated learning mechanism that combines communication-efficient randomized quantization (RSUQ) with tunable noise to provide privacy-adaptable training.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of privacy guarantees and convergence properties of the proposed scheme.&lt;/li&gt;&lt;li&gt;Empirically evaluates utility, showing convergence profiles versus baselines and accuracy–privacy trade-offs across parties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chun Hei Michael Shiu', 'Chih Wei Ling']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving', 'communication-efficiency', 'quantization', 'convergence-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10701</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2601.10407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CS-GBA, a targeted backdoor/data-poisoning attack framework for offline reinforcement learning that aims for high stealthiness and effectiveness under a strict poisoning budget.&lt;/li&gt;&lt;li&gt;Introduces Critical Sample Selection that prioritizes transitions with high TD error to concentrate poisoning on samples most influential for value convergence.&lt;/li&gt;&lt;li&gt;Designs a Correlation-Breaking Trigger to avoid OOD detection by exploiting physical mutual exclusivity in state features, and a Gradient-Guided Action Generation to find worst-case actions within the data manifold using the victim Q-network's gradients.&lt;/li&gt;&lt;li&gt;Empirical results on D4RL benchmarks show strong attack success rates (with as little as 5% poisoning) against safety-constrained algorithms (e.g., CQL) while preserving clean-environment performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanjie Zhao', 'Junnan Qiu', 'Yue Ding', 'Jie Li']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'offline reinforcement learning', 'data poisoning', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10407</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD</title><link>https://arxiv.org/abs/2601.10237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Derives explicit upper bound on the f-differential-privacy trade-off curve for DP-SGD with shuffled sampling over a single epoch and M gradient updates.&lt;/li&gt;&lt;li&gt;Proves a geometric lower bound on the separation κ and shows that achieving small κ (meaning strong privacy) enforces a strict lower bound on the Gaussian noise multiplier σ (σ ≥ 1/√(2 ln M)), implying a tension between privacy and utility under worst-case adversaries.&lt;/li&gt;&lt;li&gt;Extends the limitation to Poisson subsampling up to constant factors and empirically demonstrates that the noise magnitudes implied by the bounds cause substantial accuracy degradation in realistic training settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Murat Bilgehan Ertan', 'Marten van Dijk']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'DP-SGD', 'privacy-utility-tradeoff', 'theoretical-analysis', 'adversarial-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10237</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Preserving Safety in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2601.10141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically analyzes geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs, finding safety gradients lie in a low-rank subspace while utility gradients occupy a broader high-dimensional space.&lt;/li&gt;&lt;li&gt;Shows safety and utility subspaces are often negatively correlated, causing directional conflicts during fine-tuning, and that the dominant safety direction can be estimated from a single sample.&lt;/li&gt;&lt;li&gt;Proposes Safety-Preserving Fine-tuning (SPF), which removes gradient components that conflict with the low-rank safety subspace, with theoretical guarantees bounding safety drift while allowing utility convergence.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SPF preserves downstream task performance, recovers pre-trained safety alignment under adversarial fine-tuning, and resists deep fine-tuning and dynamic jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Yangfan Hu', 'Kejia Chen', 'Lipeng He', 'Jiachen Ma', 'Jian Lou', 'Dan Li', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['fine-tuning', 'jailbreak defense', 'gradient-projection', 'safety-alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10141</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STELP: Secure Transpilation and Execution of LLM-Generated Programs</title><link>https://arxiv.org/abs/2601.05467</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STELP, a Secure Transpiler and Executor to safely run LLM-generated code by transpiling and executing it in a controlled environment to mitigate malicious, erroneous, or hallucinated code.&lt;/li&gt;&lt;li&gt;Addresses security threats such as data poisoning, malicious payloads, and unsafe code execution; proposes runtime protections and validation to enable automated code execution pipelines without full human review.&lt;/li&gt;&lt;li&gt;Provides a human-validated dataset of insecure code snippets and benchmarks STELP against existing methods for correctness, safety, and latency, reporting substantial improvements in safely executing risky snippets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Swapnil Shinde', 'Sahil Wadhwa', 'Andy Luo', 'Akshay Gupta', 'Mohammad Shahed Sorower']&lt;/li&gt;&lt;li&gt;Tags: ['code-generation-security', 'secure-transpilation', 'runtime-sandboxing', 'safety-benchmarks', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05467</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Network-Level Prompt and Trait Leakage in Local Research Agents</title><link>https://arxiv.org/abs/2508.20282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that locally deployed Web and Research Agents (WRAs) leak prompts and user traits to passive network observers via visited IPs and timing metadata.&lt;/li&gt;&lt;li&gt;Built a new dataset of WRA network traces (real queries + synthetic personas) and defined OBELS, a behavioral metric to quantify prompt similarity; attack recovers ~73% of functional/domain prompt content.&lt;/li&gt;&lt;li&gt;Extends to multi-session inference, recovering up to 19 of 32 latent user traits with high accuracy, and demonstrates robustness under partial observability and noise.&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies (domain diversity constraints and trace obfuscation) that reduce attack effectiveness by ~29% with negligible utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyejun Jeong', 'Mohammadreza Teymoorianfard', 'Abhinav Kumar', 'Amir Houmansadr', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['prompt leakage', 'network-level inference', 'user trait/privacy leakage', 'passive eavesdropping', 'mitigations/defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20282</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis</title><link>https://arxiv.org/abs/2508.01343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UEChecker, a deep learning tool that detects unchecked external call vulnerabilities in smart contracts by analyzing call graphs with GCNs and a Conformer-like block.&lt;/li&gt;&lt;li&gt;Introduces modules for edge prediction, node aggregation, and multi-scale dependency capture (multi-head attention + convolution + feedforward) to improve graph feature representations.&lt;/li&gt;&lt;li&gt;Evaluated on 608 DApps, achieving 87.59% accuracy and outperforming baselines (GAT, LSTM, GCN) for this vulnerability detection task.&lt;/li&gt;&lt;li&gt;Focuses on blockchain/smart-contract security and automated vulnerability detection rather than general model capability work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dechao Kong', 'Xiaoqi Li', 'Wenkai Li']&lt;/li&gt;&lt;li&gt;Tags: ['smart-contract-security', 'vulnerability-detection', 'graph-neural-networks', 'unchecked-external-call', 'blockchain-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01343</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title><link>https://arxiv.org/abs/2505.22310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a major vulnerability in example-level unlearning: 'forget-set' accuracy often recovers almost fully after fine-tuning on only the retain set (no forget examples).&lt;/li&gt;&lt;li&gt;Shows this relearning effect is widespread across many unlearning methods, whereas retraining-from-scratch prevents recovery.&lt;/li&gt;&lt;li&gt;Finds weight-space metrics (L2 distance, linear mode connectivity) predict resistance to relearning.&lt;/li&gt;&lt;li&gt;Proposes weight-space regularization techniques that substantially improve tamper-resistant unlearning against relearning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Adrian Weller', 'David Krueger', 'Gintare Karolina Dziugaite', 'Michael Curtis Mozer', 'Eleni Triantafillou']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'relearning attacks', 'defense/robustness', 'weight-space regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22310</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</title><link>https://arxiv.org/abs/2505.08814</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of four neural network coverage metrics (primary functionality, boundary, hierarchy, structural) across DNN architectures (LeNet, VGG, ResNet) and depths (5–54 layers).&lt;/li&gt;&lt;li&gt;Analyzes relationships between model depth/configuration and coverage metrics, and examines how modified decision/condition coverage relates to dataset size.&lt;/li&gt;&lt;li&gt;Frames the work in the context of security-related testing of DNNs and proposes future directions to aid security testing of image recognition models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Li', 'Xiaoqi Li', 'Yingjie Mao', 'Yishun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['coverage metrics', 'security testing', 'empirical study', 'neural network robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.08814</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Permissive Information-Flow Analysis for Large Language Models</title><link>https://arxiv.org/abs/2410.03055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: standard dynamic information-flow/taint tracking for LLMs is overly conservative (propagating the strictest input label to outputs) and can block legitimate use when inputs come from diverse sources.&lt;/li&gt;&lt;li&gt;Proposal: a permissive label propagation approach that only carries forward labels of input samples influential in generating an LLM's output, removing labels for unnecessary inputs.&lt;/li&gt;&lt;li&gt;Implementations: two variants evaluated — (i) prompt-based retrieval augmentation, and (ii) k-nearest-neighbors language model; compared to an introspection-based baseline.&lt;/li&gt;&lt;li&gt;Results: in an LLM agent setting, the permissive label propagator outperforms the baseline in over 85% of cases, demonstrating practical improvements in balancing security and permissiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoaib Ahmed Siddiqui', 'Radhika Gaonkar', 'Boris K\\"opf', 'David Krueger', 'Andrew Paverd', 'Ahmed Salem', 'Shruti Tople', 'Lukas Wutschitz', 'Menglin Xia', "Santiago Zanella-B\\'eguelin"]&lt;/li&gt;&lt;li&gt;Tags: ['information-flow', 'taint-tracking', 'LLM-security', 'data-leakage', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03055</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Fails to Remove Data Poisoning Attacks</title><link>https://arxiv.org/abs/2406.17216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates practical machine unlearning methods for large-scale deep learning with respect to removing data poisoning attacks.&lt;/li&gt;&lt;li&gt;Shows existing unlearning techniques largely fail to eliminate effects of various poisoning attacks (indiscriminate, targeted, and a new Gaussian poisoning attack) across models including image classifiers and LLMs, even with substantial compute budgets.&lt;/li&gt;&lt;li&gt;Introduces new evaluation metrics for unlearning based on poisoning scenarios and concludes unlearning currently offers limited benefit over full retraining and lacks provable guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Pawelczyk', 'Jimmy Z. Di', 'Yiwei Lu', 'Gautam Kamath', 'Ayush Sekhari', 'Seth Neel']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'machine unlearning', 'adversarial attacks', 'defense evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17216</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BASIL: Bayesian Assessment of Sycophancy in LLMs</title><link>https://arxiv.org/abs/2508.16846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian probabilistic framework to disentangle sycophantic belief shifts from rational belief updating in LLMs.&lt;/li&gt;&lt;li&gt;Presents two metrics: a descriptive metric controlling for rational responses to evidence, and a normative metric quantifying deviations from Bayesian-consistent updating; both can be applied without ground-truth labels.&lt;/li&gt;&lt;li&gt;Empirically finds robust sycophantic belief shifts across multiple LLMs and tasks, and shows impact on rationality depends on model update behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates mitigation strategies (post-hoc calibration, supervised fine-tuning (SFT), and direct preference optimization (DPO)) that reduce Bayesian inconsistency and improve robustness under sycophancy prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Katherine Atwell', 'Pedram Heydari', 'Anthony Sicilia', 'Malihe Alikhani']&lt;/li&gt;&lt;li&gt;Tags: ['model-safety', 'alignment', 'robustness', 'behavioral-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16846</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Evasion Attacks on Computer Vision using SHAP Values</title><link>https://arxiv.org/abs/2601.10587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a white-box adversarial evasion attack for computer vision that uses SHAP values to identify input features (pixels/regions) most influential to model outputs.&lt;/li&gt;&lt;li&gt;Crafts imperceptible perturbations guided by SHAP to reduce confidence or induce misclassification, and compares effectiveness to FGSM.&lt;/li&gt;&lt;li&gt;Claims improved robustness of SHAP-guided attacks in scenarios where gradients are hidden or obfuscated, highlighting a novel explainability-based attack vector.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Frank Mollard', 'Marcus Becker', 'Florian Roehrbein']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'evasion-attack', 'white-box-attack', 'explainability-based-attack', 'gradient-hiding-resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10587</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs</title><link>https://arxiv.org/abs/2601.10496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code in training data influences a code LLM's preference.&lt;/li&gt;&lt;li&gt;Uses Data Portraits for membership testing on the Stack-V2 corpus and the ManySStuBs4J benchmark to stratify examples by exposure, then compares model behavior via generation and multiple likelihood-based scoring metrics.&lt;/li&gt;&lt;li&gt;Finds models reproduce buggy lines far more often than fixes, with exposure to bugs amplifying this tendency; some likelihood metrics consistently prefer fixes while others reverse when only the buggy variant was seen.&lt;/li&gt;&lt;li&gt;Concludes that training-data exposure can skew bug-vs-fix evaluations and poses a risk of LLMs propagating memorized errors in practice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al-Kaswan', 'Claudio Spiess', 'Prem Devanbu', 'Arie van Deursen', 'Maliheh Izadi']&lt;/li&gt;&lt;li&gt;Tags: ['unintended memorization', 'membership inference', 'code LLMs', 'training data exposure', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10496</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior</title><link>https://arxiv.org/abs/2601.10440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentGuardian, a runtime security framework that learns context-aware access-control policies for AI agents from controlled staging execution traces.&lt;/li&gt;&lt;li&gt;Enforces adaptive policies governing tool calls based on real-time input context and control-flow dependencies of multi-step agent actions.&lt;/li&gt;&lt;li&gt;Evaluated on two real-world AI agent applications, showing detection of malicious/misleading inputs and mitigation of hallucination-driven or orchestration-level errors while preserving normal functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nadya Abaev', 'Denis Klimov', 'Gerard Levinov', 'David Mimran', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['access-control', 'runtime monitoring', 'agent security', 'policy learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10440</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale</title><link>https://arxiv.org/abs/2601.10338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical analysis of agent skills: collected 42,447 skills and systematically analyzed 31,132 using SkillScan, finding 26.1% contain at least one vulnerability across 14 patterns (prompt injection, data exfiltration, privilege escalation, supply chain risks).&lt;/li&gt;&lt;li&gt;Produced a grounded vulnerability taxonomy from 8,126 vulnerable skills and a validated detection pipeline (86.7% precision, 82.5% recall); released an open dataset and detection toolkit.&lt;/li&gt;&lt;li&gt;Key findings and recommendations: skills with executable scripts are ~2.12x more likely to be vulnerable, 5.2% exhibit high-severity malicious patterns, and the paper calls for capability-based permissions and mandatory security vetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['agent skills', 'prompt injection', 'data exfiltration', 'supply chain attacks', 'vulnerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10338</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack</title><link>https://arxiv.org/abs/2601.10173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ReasAlign, a model-level defense that uses structured reasoning steps to analyze queries, detect conflicting/malicious instructions, and preserve user-intended task flow to defend against indirect prompt injection.&lt;/li&gt;&lt;li&gt;Introduces a test-time scaling mechanism with a preference-optimized judge model that scores reasoning trajectories and selects the best-safe output.&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks (including CyberSecEval2) demonstrating substantially improved trade-off between utility and attack success rate compared to prior guardrails (e.g., Meta SecAlign).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Yankai Yang', 'G. Edward Suh', 'Ning Zhang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'adversarial robustness', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10173</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Preserving Safety in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2601.10141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically analyzes geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs, finding safety gradients lie in a low-rank subspace while utility gradients occupy a broader high-dimensional space.&lt;/li&gt;&lt;li&gt;Shows safety and utility subspaces are often negatively correlated, causing directional conflicts during fine-tuning, and that the dominant safety direction can be estimated from a single sample.&lt;/li&gt;&lt;li&gt;Proposes Safety-Preserving Fine-tuning (SPF), which removes gradient components that conflict with the low-rank safety subspace, with theoretical guarantees bounding safety drift while allowing utility convergence.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that SPF preserves downstream task performance, recovers pre-trained safety alignment under adversarial fine-tuning, and resists deep fine-tuning and dynamic jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Yangfan Hu', 'Kejia Chen', 'Lipeng He', 'Jiachen Ma', 'Jian Lou', 'Dan Li', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['fine-tuning', 'jailbreak defense', 'gradient-projection', 'safety-alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10141</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method</title><link>https://arxiv.org/abs/2601.09933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FGSM DICNN: a malware classification model combining diluted (dilated) convolutions with Fast Gradient Sign Method (FGSM) perturbations during training.&lt;/li&gt;&lt;li&gt;Diluted convolutions increase receptive field to capture dispersed malware patterns with fewer features; FGSM is used as a one-step adversarial training strategy to improve robustness/accuracy at low cost.&lt;/li&gt;&lt;li&gt;Claims high performance on Android malware classification (99.44% accuracy) and outperforms a baseline/custom DCNN.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Anand', 'Bhupendra Singh', 'Sunil Khemka', 'Bireswar Banerjee', 'Vishi Singh Bhatia', 'Piyush Ranjan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'FGSM', 'malware-classification', 'robustness', 'diluted-convolution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09933</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Novel Contrastive Loss for Zero-Day Network Intrusion Detection</title><link>https://arxiv.org/abs/2601.09902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel contrastive loss for ML-based network intrusion detection that aims to generalise to zero-day attack classes while maintaining robustness to imbalanced data.&lt;/li&gt;&lt;li&gt;Rather than training purely as an anomaly detector, the model learns benign traffic distributions using benign samples plus known attack classes (excluding the zero-day class) to improve detection of unseen attacks.&lt;/li&gt;&lt;li&gt;Evaluated on the Lycos2017 dataset, reporting AUROC improvements for known and zero-day attack detection and substantial OpenAUC gains for open-set recognition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Wilkie', 'Hanan Hindy', 'Craig Michie', 'Christos Tachtatzis', 'James Irvine', 'Robert Atkinson']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'network intrusion detection', 'zero-day attack detection', 'contrastive learning', 'open-set recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09902</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification</title><link>https://arxiv.org/abs/2601.09806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end pipeline to create adversarial patches for facial biometric systems: initial FGSM noise generation followed by diffusion-model-based refinement (reverse diffusion, Gaussian smoothing, brightness correction) to improve imperceptibility.&lt;/li&gt;&lt;li&gt;Evaluates the patches on facial identity verification and expression recognition, measuring changes in identity classification and captioning outputs (ViT-GPT2) to support forensic interpretation of identity-evasion attacks.&lt;/li&gt;&lt;li&gt;Introduces detection/forensic methods (perceptual hashing and segmentation) to locate and analyze adversarial patches, reporting high reconstruction/visual similarity (SSIM ≈ 0.95).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahrzad Sayyafzadeh', 'Hongmei Chi', 'Shonda Bernadin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-patches', 'biometric-attacks', 'diffusion-models', 'adversarial-detection', 'forensic-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09806</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute</title><link>https://arxiv.org/abs/2601.09756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLM-generated synthetic veterinary EHR narratives improve de-identification safety under two training regimes: synthetic augmentation (epoch-scaled) and fixed-budget substitution.&lt;/li&gt;&lt;li&gt;Controlled simulation on a PetEVAL-derived corpus with 10,382 template-only synthetic notes and three transformer backbones (PetBERT, VetBERT, Bio_ClinicalBERT). Primary safety metric: document-level leakage rate.&lt;/li&gt;&lt;li&gt;Findings: substituting real notes with synthetic ones increases leakage; moderate synthetic mixing can match real-only performance under compute-matched training; epoch-scaled synthetic augmentation reduces leakage and improves F1 largely due to increased training exposure rather than intrinsic synthetic quality.&lt;/li&gt;&lt;li&gt;Conclusion: synthetic data is useful as complementary augmentation to increase exposure but cannot safely substitute real supervision for safety-critical de-identification; systematic synthetic-real mismatches explain persistent leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Brundage']&lt;/li&gt;&lt;li&gt;Tags: ['de-identification', 'synthetic-data', 'privacy-leakage', 'data-augmentation', 'healthcare-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09756</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox</title><link>https://arxiv.org/abs/2601.09721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM safety under anxiety-driven adversarial user pressures in pediatric consultations using PediatricAnxietyBench (300 queries, authentic vs adversarial).&lt;/li&gt;&lt;li&gt;Compares three models (Llama-3.3-70B, Llama-3.1-8B, Mistral-7B) across platforms, measuring safety on restraint, referral, hedging, emergency recognition, and non-prescriptive behavior.&lt;/li&gt;&lt;li&gt;Finds smaller models can outperform larger ones (scale paradox), identifies specific vulnerabilities (e.g., failure to recognize emergencies, inappropriate seizure diagnoses), and shows robustness evolution across model releases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vahideh Zolfaghari']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial Evaluation', 'Safety Benchmark', 'Red Teaming', 'Medical AI Safety', 'Model Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09721</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</title><link>https://arxiv.org/abs/2601.10543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LLMs exhibit latent safety-related signals during generation even when jailbreaks succeed, but these are overridden by fluent continuation.&lt;/li&gt;&lt;li&gt;Proposes an in-decoding safety-awareness probing method that surfaces and uses these latent signals to detect and block unsafe completions early in the decoding process.&lt;/li&gt;&lt;li&gt;Empirical results across diverse jailbreak attacks show substantially improved defense effectiveness, low over-refusal on benign inputs, and preserved response quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinzhi Zhao', 'Ming Wang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yifei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'decoding-time defenses', 'safety probing', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10543</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrated safety evaluation of seven frontier LLMs/MLLMs (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image-generation settings.&lt;/li&gt;&lt;li&gt;Uses a unified protocol combining benchmark evaluation, adversarial evaluation (red-teaming), multilingual testing, and compliance/regulatory checks to produce safety leaderboards and model safety profiles.&lt;/li&gt;&lt;li&gt;Finds a heterogeneous safety landscape: GPT-5.2 performs consistently well, while other models show trade-offs across benchmark safety, adversarial alignment, multilingual generalization, and compliance; language and VLM modalities are notably vulnerable under adversarial evaluation.&lt;/li&gt;&lt;li&gt;Notes that text-to-image models better align on regulated visual risk categories but remain brittle to adversarial or semantically ambiguous prompts, underscoring the need for standardized, multidimensional safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial evaluation', 'multimodal models', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment</title><link>https://arxiv.org/abs/2601.10520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a neuro-symbolic containment architecture that decouples normative reasoning from instrumental decision-making to enforce ethical constraints on AI agents.&lt;/li&gt;&lt;li&gt;Defines three components: a Moral Module (deontic logic-based) to derive permissible macro actions, a Decision-Making Module that selects instrumentally optimal primitives under those constraints, and a Guard that monitors/enforces compliance with formal guarantees.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability, contestability, and formal verification/statistical guarantees of alignment and demonstrates the approach on an LLM therapy assistant.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felix Jahn', 'Yannic Muskalla', 'Lisa Dargasz', 'Patrick Schramowski', 'Kevin Baum']&lt;/li&gt;&lt;li&gt;Tags: ['safety/guardrails', 'ethical alignment', 'neuro-symbolic', 'deontic logic', 'formal verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10520</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries</title><link>https://arxiv.org/abs/2601.10398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Problem: Unanswerable or underspecified text-to-SQL queries can produce incorrect or unsafe executable SQL; existing refusal strategies are brittle or costly.&lt;/li&gt;&lt;li&gt;Method: LatentRefusal predicts query answerability from intermediate LLM hidden activations using a lightweight Tri-Residual Gated Encoder probe to amplify mismatch cues and suppress schema noise.&lt;/li&gt;&lt;li&gt;Contributions: Provides an attachable, efficient safety layer that detects unanswerability with low overhead (~2 ms) and improves average F1 to 88.5% across four benchmarks.&lt;/li&gt;&lt;li&gt;Evaluation: Extensive experiments, ablations, and interpretability analyses demonstrating effectiveness across diverse ambiguous and unanswerable scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancheng Ren', 'Shijing Hu', 'Zhihui Lu', 'Jiangqi Huang', 'Qiang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['Safety/Refusal', 'LLM probing', 'Text-to-SQL defenses', 'Runtime guardrail', 'Answerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10398</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing</title><link>https://arxiv.org/abs/2601.10342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C-GRASP, a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps to reduce physiological hallucinations.&lt;/li&gt;&lt;li&gt;Implements a Z-score Priority Hierarchy and RSA-aware guardrails that prioritize individualized baseline shifts (Delta Z-score) over population norms to prevent spectral/physiological hallucinations.&lt;/li&gt;&lt;li&gt;Evaluated on DREAMER (414 trials), reporting improved 4-class emotion classification and a Clinical Reasoning Consistency (CRC) metric; ablation shows the Delta Z-score module is the key logical anchor.&lt;/li&gt;&lt;li&gt;Primary aim is robustness and safety of LLM-based clinical/affective signal interpretation (hallucination mitigation and clinical guardrails), not adversarial attack generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Lin Cheng', 'Ting Chuan Lin', 'Chai Kai Chang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM guardrails', 'hallucination mitigation', 'clinical robustness', 'HRV/time-series processing', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10342</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination Detection and Mitigation in Large Language Models</title><link>https://arxiv.org/abs/2601.09929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an operational framework for managing LLM/LRM hallucinations using a continuous improvement cycle driven by root-cause awareness.&lt;/li&gt;&lt;li&gt;Categorizes hallucination sources into model, data, and context factors to enable targeted detection and mitigation strategies.&lt;/li&gt;&lt;li&gt;Integrates multiple detection methods (uncertainty estimation, reasoning consistency) with stratified mitigations (knowledge grounding, confidence calibration) in a tiered architecture.&lt;/li&gt;&lt;li&gt;Demonstrates the approach with a financial data extraction case study that uses closed-loop feedback across model, context, and data tiers to improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Pesaranghader', 'Erin Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'safety/robustness', 'confidence calibration', 'knowledge grounding', 'operational framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09929</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents</title><link>https://arxiv.org/abs/2601.09923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies prompt injection and 'Branch Steering' attacks against Computer Use Agents (CUAs) that view UIs and execute actions.&lt;/li&gt;&lt;li&gt;Proposes Single-Shot Planning: a trusted planner that generates a complete conditional execution graph before observing untrusted UI content to enforce control-flow integrity.&lt;/li&gt;&lt;li&gt;Demonstrates that additional measures are needed to mitigate Branch Steering (manipulating UI to select valid but unintended plan branches).&lt;/li&gt;&lt;li&gt;Evaluates the design on OSWorld, showing trade-offs: up to 57% performance retention for frontier models and up to 19% improvement for smaller open-source models while improving security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanna Foerster', 'Robert Mullins', 'Tom Blanchard', 'Nicolas Papernot', "Kristina Nikoli\\'c", 'Florian Tram\\`er', 'Ilia Shumailov', 'Cheng Zhang', 'Yiren Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'agent security', 'architectural isolation', 'control-flow integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09923</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation</title><link>https://arxiv.org/abs/2601.09771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PCN-Rec, a pipeline that separates LLM-based negotiation from deterministic enforcement to ensure recommender outputs meet governance constraints.&lt;/li&gt;&lt;li&gt;A mediator LLM produces a top-N slate plus a structured certificate (JSON) claiming constraint satisfaction; a deterministic verifier checks the certificate and a constrained-greedy repair enforces compliance if needed, producing an auditable trace.&lt;/li&gt;&lt;li&gt;Evaluated on MovieLens-100K, PCN-Rec achieves a 98.55% pass rate on feasible users with only a small, statistically significant drop in NDCG@10 compared to an unverified LLM baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aradhya Dixit', 'Shreem Dixit']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'recommender-systems', 'verifiability', 'auditing', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09771</guid><pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 24 Oct 2025 22:28:35 +0000</lastBuildDate><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for self-evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept space', 'Dynamic updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation</title><link>https://arxiv.org/abs/2509.21401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JaiLIP, a jailbreaking attack on VLMs using loss-guided image perturbation&lt;/li&gt;&lt;li&gt;Aims to generate imperceptible adversarial images that trigger harmful outputs&lt;/li&gt;&lt;li&gt;Evaluates using toxicity metrics and a transportation domain scenario&lt;/li&gt;&lt;li&gt;Highlights need for defenses against image-based attacks on VLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Jueal Mia', 'M. Hadi Amini']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'image perturbation', 'VLM security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21401</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</title><link>https://arxiv.org/abs/2510.08872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GTAlign, a game-theoretic alignment framework for LLMs&lt;/li&gt;&lt;li&gt;Aims to improve mutual welfare between LLM and user through strategic decision making&lt;/li&gt;&lt;li&gt;Introduces mutual welfare reward during training and game-theoretic reasoning during inference&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zhu', 'David Zhang', 'Pedro Cisneros-Velarde', 'Jiaxuan You']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'mutual welfare', 'strategic reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08872</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for self-evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept space', 'Dynamic updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</title><link>https://arxiv.org/abs/2410.18469</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ADV-LLM, an iterative self-tuning framework for generating adversarial suffixes to jailbreak LLMs&lt;/li&gt;&lt;li&gt;Achieves high ASR (Attack Success Rate) on both open-source and closed-source models&lt;/li&gt;&lt;li&gt;Reduces computational cost compared to existing methods&lt;/li&gt;&lt;li&gt;Provides datasets for safety alignment research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Sun', 'Xiaodong Liu', 'Weiwei Yang', 'Tsui-Wei Weng', 'Hao Cheng', 'Aidan San', 'Michel Galley', 'Jianfeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18469</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons</title><link>https://arxiv.org/abs/2406.14144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the inner mechanisms of safety alignment in LLMs using mechanistic interpretability.&lt;/li&gt;&lt;li&gt;It identifies safety neurons responsible for safety behaviors and evaluates their causal effects.&lt;/li&gt;&lt;li&gt;Experiments show that patching these neurons can restore safety performance without affecting general ability.&lt;/li&gt;&lt;li&gt;The findings help explain the 'alignment tax' and can be used to detect unsafe outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianhui Chen', 'Xiaozhi Wang', 'Zijun Yao', 'Yushi Bai', 'Lei Hou', 'Juanzi Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'mechanistic interpretability', 'safety neurons', 'red-teaming benchmarks', 'alignment tax']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.14144</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Teaming LLMs to Detect and Mitigate Hallucinations</title><link>https://arxiv.org/abs/2510.19507</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores using multiple LLMs with different training data and architectures to improve hallucination detection and mitigation.&lt;/li&gt;&lt;li&gt;It evaluates 'consortium consistency' across various model teams and shows performance improvements with reduced inference costs.&lt;/li&gt;&lt;li&gt;The approach combines responses from different LLMs to enhance consistency-based methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Demian Till', 'John Smeaton', 'Peter Haubrick', 'Gouse Saheb', 'Florian Graef', 'David Berman']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucination', 'consistency', 'multi-model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19507</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses</title><link>https://arxiv.org/abs/2510.20314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on adversarial attacks and defenses in DRL&lt;/li&gt;&lt;li&gt;Covers perturbation types and attack targets&lt;/li&gt;&lt;li&gt;Reviews robustness training strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wu Yichao', 'Wang Yirui', 'Ding Panpan', 'Wang Hailong', 'Zhu Bingqian', 'Liu Chun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'DRL security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20314</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length.ipynb</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'privacy attacks', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment</title><link>https://arxiv.org/abs/2510.19979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SecureInfer is a hybrid framework using TEEs and GPU for secure LLM deployment&lt;/li&gt;&lt;li&gt;Protects model privacy against extraction attacks&lt;/li&gt;&lt;li&gt;Partitions model components between trusted and untrusted environments&lt;/li&gt;&lt;li&gt;Evaluates performance and security with LLaMA-2&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tushar Nayan (Florida International University)', 'Ziqi Zhang (University of Illinois Urbana-Champaign)', 'Ruimin Sun (Florida International University)']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'privacy', 'TEE', 'GPU']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19979</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title><link>https://arxiv.org/abs/2510.20792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadGraph, a backdoor attack on text-guided graph generation models&lt;/li&gt;&lt;li&gt;Uses textual triggers to poison training data&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with low poisoning rates&lt;/li&gt;&lt;li&gt;Highlights security vulnerabilities in latent diffusion models for graph generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Ye', 'Shengqin Chen', 'Jiazhu Dai']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'text-guided graph generation', 'latent diffusion model', 'data poisoning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20792</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>An Empirical Study of Sample Selection Strategies for Large Language Model Repair</title><link>https://arxiv.org/abs/2510.20428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper evaluates sample selection strategies for repairing LLMs to reduce toxicity while preserving utility.&lt;/li&gt;&lt;li&gt;It compares methods like random sampling, K-Center, GraNd, CCS, and a new SAPS approach.&lt;/li&gt;&lt;li&gt;SAPS shows the best balance between detoxification, utility, and efficiency.&lt;/li&gt;&lt;li&gt;The study highlights the importance of sample selection in LLM repair pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuran Li', 'Jingyi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM repair', 'sample selection', 'toxicity reduction', 'utility preservation', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20428</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Ask a Strong LLM Judge when Your Reward Model is Uncertain</title><link>https://arxiv.org/abs/2510.20369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an uncertainty-based routing framework for RLHF&lt;/li&gt;&lt;li&gt;Complements a fast RM with a strong LLM judge&lt;/li&gt;&lt;li&gt;Improves alignment and efficiency in online RLHF&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenghao Xu', 'Qin Lu', 'Qingru Zhang', 'Liang Qiu', 'Ilgee Hong', 'Changlong Yu', 'Wenlin Yao', 'Yao Liu', 'Haoming Jiang', 'Lihong Li', 'Hyokun Yun', 'Tuo Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'reward model', 'uncertainty quantification', 'LLM judge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20369</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases</title><link>https://arxiv.org/abs/2510.20270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImpossibleBench, a benchmark framework to measure LLMs' propensity to exploit test cases.&lt;/li&gt;&lt;li&gt;Creates impossible tasks by conflicting specifications and unit tests to detect cheating.&lt;/li&gt;&lt;li&gt;Provides tools for studying model behavior, context engineering, and monitoring.&lt;/li&gt;&lt;li&gt;Aims to improve robustness and reliability of LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqian Zhong', 'Aditi Raghunathan', 'Nicholas Carlini']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'benchmarking', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20270</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box adversarial attack framework targeting Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Modifies reasoning questions with harmful instructions to guide models to unsafe outputs&lt;/li&gt;&lt;li&gt;Evaluates on 11 LRMs with 70% average success rate, showing vulnerability even in safety-aligned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyuan Ma', 'Rui Li', 'Zheng Li', 'Junfeng Liu', 'Heming Xia', 'Lei Sha', 'Zhifang Sui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons</title><link>https://arxiv.org/abs/2406.14144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the inner mechanisms of safety alignment in LLMs using mechanistic interpretability.&lt;/li&gt;&lt;li&gt;It identifies safety neurons responsible for safety behaviors and evaluates their causal effects.&lt;/li&gt;&lt;li&gt;Experiments show that patching these neurons can restore safety performance without affecting general ability.&lt;/li&gt;&lt;li&gt;The findings help explain the 'alignment tax' and can be used to detect unsafe outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianhui Chen', 'Xiaozhi Wang', 'Zijun Yao', 'Yushi Bai', 'Lei Hou', 'Juanzi Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'mechanistic interpretability', 'safety neurons', 'red-teaming benchmarks', 'alignment tax']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.14144</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</title><link>https://arxiv.org/abs/2510.08872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GTAlign, a game-theoretic alignment framework for LLMs&lt;/li&gt;&lt;li&gt;Aims to improve mutual welfare between LLM and user through strategic decision making&lt;/li&gt;&lt;li&gt;Introduces mutual welfare reward during training and game-theoretic reasoning during inference&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zhu', 'David Zhang', 'Pedro Cisneros-Velarde', 'Jiaxuan You']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'mutual welfare', 'strategic reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08872</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for self-evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept space', 'Dynamic updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Shall We Play a Game? Language Models for Open-ended Wargames</title><link>https://arxiv.org/abs/2509.17192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper discusses the use of AI, particularly LLMs, in open-ended wargames and highlights the need for safety, interpretability, and explainability research.&lt;/li&gt;&lt;li&gt;It includes a literature review and ontology construction for open-endedness in wargames.&lt;/li&gt;&lt;li&gt;Provides practical recommendations and safety considerations for deploying AI in wargames.&lt;/li&gt;&lt;li&gt;Presents open research challenges for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Glenn Matlin', 'Parv Mahajan', 'Isaac Song', 'Yixiong Hao', 'Ryan Bard', 'Stu Topp', 'Evan Montoya', 'M. Rehan Parwani', 'Soham Shetty', 'Mark Riedl']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM applications', 'Wargaming', 'Safety evaluation', 'Explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.17192</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment</title><link>https://arxiv.org/abs/2505.14667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFEPATH, a method to prevent harmful reasoning in chain-of-thought models&lt;/li&gt;&lt;li&gt;Uses a Safety Primer to detect harmful prompts and block jailbreaks&lt;/li&gt;&lt;li&gt;Maintains reasoning performance while reducing harmful outputs&lt;/li&gt;&lt;li&gt;Requires significantly less compute than existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonje Jeung', 'Sangyeon Yoon', 'Minsuk Kahng', 'Albert No']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'alignment', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14667</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title><link>https://arxiv.org/abs/2510.20768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAGRank using PageRank to counter poisoning attacks in CTI LLM pipelines&lt;/li&gt;&lt;li&gt;Addresses vulnerability of RAG systems to poisoning in CTI contexts&lt;/li&gt;&lt;li&gt;Demonstrates improved authority scoring for trusted content over malicious documents&lt;/li&gt;&lt;li&gt;Tests on MS MARCO and CTI datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Austin Jia', 'Avaneesh Ramesh', 'Zain Shamsi', 'Daniel Zhang', 'Alex Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'safety evaluation', 'CTI', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20768</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models To Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the issue where LLMs adjust their behavior during evaluation to appear more aligned, which affects safety evaluations.&lt;/li&gt;&lt;li&gt;It introduces a steering vector technique to suppress evaluation-awareness, making the model behave as if deployed during evaluation.&lt;/li&gt;&lt;li&gt;The study involves training an LLM with evaluation cues and then using activation steering to counteract the evaluation-aware behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?</title><link>https://arxiv.org/abs/2510.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GhostEI-Bench, a benchmark for evaluating mobile agents' resilience to environmental injection attacks&lt;/li&gt;&lt;li&gt;Focuses on adversarial UI elements in dynamic on-device environments&lt;/li&gt;&lt;li&gt;Uses Android emulators to inject adversarial events into application workflows&lt;/li&gt;&lt;li&gt;Reveals significant vulnerabilities in current VLM-based agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Chen', 'Xinhao Song', 'Yunkai Chai', 'Yang Yao', 'Haodong Zhao', 'Lijun Li', 'Jie Li', 'Yan Teng', 'Gongshen Liu', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20333</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses</title><link>https://arxiv.org/abs/2510.20314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on adversarial attacks and defenses in DRL&lt;/li&gt;&lt;li&gt;Covers perturbation types and attack targets&lt;/li&gt;&lt;li&gt;Reviews robustness training strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wu Yichao', 'Wang Yirui', 'Ding Panpan', 'Wang Hailong', 'Zhu Bingqian', 'Liu Chun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'DRL security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20314</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAID: Empowering Large Language Models with Self-Activating Internal Defense</title><link>https://arxiv.org/abs/2510.20129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAID, a training-free defense for LLMs against jailbreak attacks&lt;/li&gt;&lt;li&gt;Uses three-stage pipeline: intent distillation, safety prefix probing, aggregation&lt;/li&gt;&lt;li&gt;Outperforms existing defenses while preserving model utility and low overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yulong Chen', 'Yadong Liu', 'Jiawen Zhang', 'Mu Li', 'Chao Huang', 'Jie Wen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20129</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Ask What Your Country Can Do For You: Towards a Public Red Teaming Model</title><link>https://arxiv.org/abs/2510.20061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cooperative public AI red-teaming exercise to address AI security and safety.&lt;/li&gt;&lt;li&gt;Reviews prior pilot implementations including CAMLIS 2024, NIST's ARIA, and Singapore's IMDA.&lt;/li&gt;&lt;li&gt;Argues for scalability and effectiveness of public red-teaming in mitigating AI harms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wm. Matthew Kennedy', 'Cigdem Patlak', 'Jayraj Dave', 'Blake Chambers', 'Aayush Dhanotiya', 'Darshini Ramiah', 'Reva Schwartz', 'Jack Hagen', 'Akash Kundu', 'Mouni Pendharkar', 'Liam Baisley', 'Theodora Skeadas', 'Rumman Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'public evaluation', 'AI security', 'safety', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20061</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability</title><link>https://arxiv.org/abs/2510.19851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores whether models can obfuscate their chain-of-thought (CoT) to evade monitoring.&lt;/li&gt;&lt;li&gt;It introduces a taxonomy of prompts to test CoT obfuscation.&lt;/li&gt;&lt;li&gt;Evaluates both internal and external CoT in different environments.&lt;/li&gt;&lt;li&gt;Finds that under pressure, some models can obfuscate CoT and evade detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artur Zolkowski', 'Wen Xing', 'David Lindner', 'Florian Tram\\`er', 'Erik Jenner']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19851</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CourtGuard: A Local, Multiagent Prompt Injection Classifier</title><link>https://arxiv.org/abs/2510.19844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CourtGuard, a multiagent prompt injection classifier using defense, prosecution, and judge models.&lt;/li&gt;&lt;li&gt;Aims to reduce false positives compared to single-model detectors.&lt;/li&gt;&lt;li&gt;Evaluates performance against Direct Detector and highlights multiagent approach benefits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isaac Wu', 'Michael Maslowski']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19844</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning</title><link>https://arxiv.org/abs/2510.20188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRUST, a decentralized framework for auditing LLM reasoning&lt;/li&gt;&lt;li&gt;Addresses robustness, scalability, opacity, and privacy challenges&lt;/li&gt;&lt;li&gt;Uses consensus mechanism, hierarchical DAG decomposition, blockchain ledger, and privacy-preserving segmentation&lt;/li&gt;&lt;li&gt;Provides theoretical security guarantees and experimental validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Morris Yu-Chao Huang', 'Zhen Tan', 'Mohan Zhang', 'Pingzhi Li', 'Zhuo Zhang', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'privacy attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20188</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs can hide text in other text of the same length.ipynb</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a method to hide text within another text of the same length using LLMs&lt;/li&gt;&lt;li&gt;Demonstrates potential for covert communication and erosion of trust in written content&lt;/li&gt;&lt;li&gt;Raises concerns for AI safety and model transparency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'privacy attacks', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>
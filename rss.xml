<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 10 Nov 2025 23:04:52 +0000</lastBuildDate><item><title>Learning to Navigate Socially Through Proactive Risk Perception</title><link>https://arxiv.org/abs/2510.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Proactive Risk Perception Module built on Falcon to predict distance-based collision risk scores for nearby humans using onboard RGB-D and odometry.&lt;/li&gt;&lt;li&gt;Targets socially compliant, collision-avoiding navigation in dynamic indoor human-populated environments without privileged information or global maps.&lt;/li&gt;&lt;li&gt;Evaluated on the Social-HM3D benchmark and reported improved personal-space compliance, achieving 2nd place in the IROS 2025 RoboSense Social Navigation Track.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erjia Xiao', 'Lingfeng Zhang', 'Yingbo Tang', 'Hao Cheng', 'Renjing Xu', 'Wenbo Ding', 'Lei Zhou', 'Long Chen', 'Hangjun Ye', 'Xiaoshuai Hao']&lt;/li&gt;&lt;li&gt;Tags: ['robotic social navigation', 'collision avoidance', 'risk perception', 'RGB-D perception', 'human-robot interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07871</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Are Concepts Erased From Diffusion Models?</title><link>https://arxiv.org/abs/2505.17013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates mechanisms by which concepts may be removed from diffusion models: (i) disrupting internal guidance versus (ii) reducing unconditional likelihood of a concept.&lt;/li&gt;&lt;li&gt;Introduces a suite of independent probes to test whether a concept is truly erased, including visual context, modified diffusion trajectories, classifier guidance, and analysis of alternative generations.&lt;/li&gt;&lt;li&gt;Demonstrates that erasure robustness can fail under non-text-adversarial conditions and argues for comprehensive evaluation protocols for concept erasure in diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Lu', 'Nicky Kriplani', 'Rohit Gandikota', 'Minh Pham', 'David Bau', 'Chinmay Hegde', 'Niv Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'robustness', 'safety evaluation', 'adversarial probing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17013</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InterFeedback, an interactive framework for autonomously evaluating Large Multimodal Models' ability to use human feedback.&lt;/li&gt;&lt;li&gt;Presents InterFeedback-Bench, which uses two datasets (MMMU-Pro and MathVerse) to evaluate 10 open-source LMMs, and InterFeedback-Human, a 120-case manual test set including evaluations of OpenAI-o1 and Claude-Sonnet-4.&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art LMMs struggle to refine responses based on human feedback (average &lt;50%), highlighting gaps in models' ability to interpret and benefit from feedback and motivating methods to improve feedback incorporation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interactive evaluation', 'benchmarking', 'human-in-the-loop', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2504.14245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multimodal large language models (MLLMs) for detecting AI-generated/fake images, comparing performance to traditional detectors and human evaluators.&lt;/li&gt;&lt;li&gt;Proposes six prompt designs and a framework that integrates them to produce more robust, reasoning-driven, and explainable fake-image detection.&lt;/li&gt;&lt;li&gt;Analyzes strengths and limitations of MLLMs for generalization and transparency in fake image detection and provides code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yikun Ji', 'Yan Hong', 'Jiahui Zhan', 'Haoxing Chen', 'jun lan', 'Huijia Zhu', 'Weiqiang Wang', 'Liqing Zhang', 'Jianfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['fake-image-detection', 'multimodal-LLMs', 'explainability', 'misinformation-detection', 'AI-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14245</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying the Risk of Transferred Black Box Attacks</title><link>https://arxiv.org/abs/2511.05102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes risk and practicality of black-box, transfer-based adversarial evasion attacks against neural networks and argues exhaustive adversarial coverage is computationally infeasible.&lt;/li&gt;&lt;li&gt;Proposes a targeted resilience testing framework that selects surrogate models based on Centered Kernel Alignment (CKA) similarity to the target to improve coverage of adversarial subspaces.&lt;/li&gt;&lt;li&gt;Uses regression-based estimators on adversarial examples generated from high- and low-CKA-surrogate models to provide actionable risk quantification for organizations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Disesdi Susanna Cox', 'Niklas Bunzel']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'black-box attacks', 'transferability', 'security testing', 'risk estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05102</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between two safety paradigms for text-to-image diffusion models: fine-tuning to unlearn harmful concepts and training-free negative-prompt guidance, which can reduce defense effectiveness when combined.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, enabling compatibility between unlearning and guidance methods without modifying either.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate on nudity and violence benchmarks while preserving prompt semantics, and shows easy integration into existing pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image safety', 'model unlearning', 'negative prompting', 'concept inversion', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title><link>https://arxiv.org/abs/2511.05319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new task, Sentence-to-Image (semantic) steganography, for embedding sentence-level messages into images.&lt;/li&gt;&lt;li&gt;Introduces the Invisible Text (IVT) benchmark containing diverse sentence-level secret messages for evaluation.&lt;/li&gt;&lt;li&gt;Presents S^2LM, a pipeline that leverages LLMs throughout the embedding process to encode semantically rich text into image carriers.&lt;/li&gt;&lt;li&gt;Provides quantitative and qualitative experiments showing the method can hide higher-level semantics compared to traditional bit-level steganography.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanqi Wu', 'Huangbiao Xu', 'Runfeng Xie', 'Jiaxin Cai', 'Kaixin Zhang', 'Xiao Ke']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'covert channels', 'LLMs', 'multimodal security', 'information hiding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05319</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements</title><link>https://arxiv.org/abs/2511.05108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a lane detection approach that uses roadside delineators (vertical posts) as indirect lane cues and fits lane trajectories with parameterized Bezier curves for snow-covered, rural roads.&lt;/li&gt;&lt;li&gt;Introduces SnowyLane, a synthetic dataset of 80,000 annotated frames with varying snow coverage and lighting to train and evaluate models under winter conditions.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness over state-of-the-art lane detectors in heavy snow occlusion, targeting safer perception for all-weather autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J\\"org Gamerdinger', 'Benedict Wetzel', 'Patrick Schulz', 'Sven Teufel', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'autonomous-driving', 'perception', 'dataset', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05108</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title><link>https://arxiv.org/abs/2511.05073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows empirically that image-based adversarial examples are more sensitive to occlusion than clean samples using CIFAR-10 and nine canonical attacks.&lt;/li&gt;&lt;li&gt;Introduces Sliding Mask Confidence Entropy (SMCE) to quantify confidence fluctuation under sliding-window occlusion and visualizes Mask Entropy Field Maps.&lt;/li&gt;&lt;li&gt;Proposes Sliding Window Mask-based Adversarial Example Detection (SWM-AED) that leverages SMCE for detection and avoids catastrophic overfitting in adversarial training.&lt;/li&gt;&lt;li&gt;Evaluates across classifiers and attacks on CIFAR-10, reporting robust detection performance (typical accuracy &gt;62%, up to 96.5%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Li', 'Yanwei Xu', 'Keran Li', 'Xiaoli Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'robustness', 'occlusion-based defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05073</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title><link>https://arxiv.org/abs/2511.05017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a modality imbalance in large vision-language models (LVLMs) where appending visual embeddings leads to a language-dominant bias that increases hallucinations.&lt;/li&gt;&lt;li&gt;Proposes refining textual embeddings by integrating average-pooled visual features to better fuse visual information into the text stream.&lt;/li&gt;&lt;li&gt;Reports improved visual grounding and reduced hallucinations on established benchmarks; notes that more advanced fusion methods could further improve results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Agrawal', 'Gouthaman KV', 'Rohith Aralikatti', 'Gauri Jagatap', 'Jiaxin Yuan', 'Vijay Kamarshi', 'Andrea Fanelli', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'visual grounding', 'multimodal alignment', 'robustness', 'LVLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05017</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an end-to-end differentiable framework that parameterizes shapes with a Fourier series, maps them to pixel grids via a winding-number-based mapping, and uses signal-energy constraints for plausible optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can elicit high-confidence semantic classifications from DNNs and act as high-fidelity interpretability probes isolating salient model regions.&lt;/li&gt;&lt;li&gt;Presents a new, generalizable adversarial paradigm based on geometric (shape-only) inputs that can deceive downstream visual tasks, implicating robustness and attack surfaces in vision models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'robustness', 'interpretability', 'red-teaming', 'computer-vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2511.04949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a latent-space learnable watermark embedder for proactive deepfake detection that encodes/extracts messages using high-level image semantics.&lt;/li&gt;&lt;li&gt;Uses a Multi-Agent Adversarial Reinforcement Learning (MAARL) setup where a watermarking agent and an adversarial attacker agent co-train to balance robustness to benign distortions and sensitivity to malicious tampering.&lt;/li&gt;&lt;li&gt;Evaluated on CelebA and CelebA-HQ, reporting consistent gains over state-of-the-art watermarking approaches (≈4.5% on CelebA, ≈5.3% on CelebA-HQ) under challenging manipulation scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tharindu Fernando', 'Clinton Fookes', 'Sridha Sridharan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'watermarking', 'adversarial reinforcement learning', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04949</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title><link>https://arxiv.org/abs/2510.15144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HugAgent, a benchmark for evaluating whether LLMs can simulate individualized human reasoning and predict a specific person’s responses and reasoning trajectories given partial prior evidence.&lt;/li&gt;&lt;li&gt;Proposes a dual-track design: a human track that scales think-aloud data collection for ecologically valid reasoning traces, and a synthetic track for scalable stress testing.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art LLMs and finds persistent adaptation gaps, positioning the benchmark as a tool for measuring cognitive alignment and personalization of model reasoning.&lt;/li&gt;&lt;li&gt;Open-sources the dataset, collection pipeline, and companion chatbot to enable extensible benchmarking and further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chance Jiajie Li', 'Zhenze Mo', 'Yuhan Tang', 'Ao Qu', 'Jiayi Wu', 'Kaiya Ivy Zhao', 'Yulu Gan', 'Jie Fan', 'Jiangbo Yu', 'Hang Jiang', 'Paul Pu Liang', 'Jinhua Zhao', 'Luis Alberto Alonso Pastor', 'Kent Larson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'benchmarking', 'LLM-evaluation', 'personalization', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15144</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title><link>https://arxiv.org/abs/2508.15252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGAN, a practical attack framework that generates high-quality fake user profiles (including textual reviews) to poison recommender systems.&lt;/li&gt;&lt;li&gt;Uses retrieval-augmented in-context learning with multimodal foundation models, plus a demonstration retrieval algorithm and text style transfer to improve review quality and imperceptibility.&lt;/li&gt;&lt;li&gt;Implements a collaborative optimization involving a jailbreaker, an instructional agent, and a guardian to enhance attack transferability across black-box RSs.&lt;/li&gt;&lt;li&gt;Reports comprehensive experiments on real-world datasets showing state-of-the-art poisoning effectiveness while maintaining stealthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyi Yang', 'Xinshu Li', 'Guanglin Zhou', 'Chen Wang', 'Xiwei Xu', 'Liming Zhu', 'Lina Yao']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'recommender systems', 'adversarial attacks', 'jailbreaking/prompt engineering', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15252</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2504.14245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multimodal large language models (MLLMs) for detecting AI-generated/fake images, comparing performance to traditional detectors and human evaluators.&lt;/li&gt;&lt;li&gt;Proposes six prompt designs and a framework that integrates them to produce more robust, reasoning-driven, and explainable fake-image detection.&lt;/li&gt;&lt;li&gt;Analyzes strengths and limitations of MLLMs for generalization and transparency in fake image detection and provides code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yikun Ji', 'Yan Hong', 'Jiahui Zhan', 'Haoxing Chen', 'jun lan', 'Huijia Zhu', 'Weiqiang Wang', 'Liqing Zhang', 'Jianfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['fake-image-detection', 'multimodal-LLMs', 'explainability', 'misinformation-detection', 'AI-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14245</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Policy-as-Prompt: a framework that converts unstructured governance artifacts into a source-linked policy tree, compiled into lightweight prompt-based classifiers for runtime enforcement.&lt;/li&gt;&lt;li&gt;Built to enforce least privilege and data minimization, with provenance, traceability, audit logging, and human-in-the-loop review for conformity assessment.&lt;/li&gt;&lt;li&gt;Evaluations report reductions in prompt-injection risk, blocking of out-of-scope requests, limitation of toxic outputs, and generation of auditable rationales aligned with governance frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'prompt-injection mitigation', 'policy-as-code', 'runtime guardrails', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2509.09360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaRAG, a metamorphic testing framework to detect hallucinations in Retrieval-Augmented Generation (RAG) systems in a black-box, unsupervised, real-time setting.&lt;/li&gt;&lt;li&gt;Operates by decomposing responses into atomic factoids, generating controlled mutations (synonym/antonym substitutions), verifying variants against retrieved context, and aggregating inconsistencies into a hallucination score.&lt;/li&gt;&lt;li&gt;Localizes unsupported claims at the span level and proposes identity-aware safeguards (configurable thresholds and span-level flags) for sensitive topics; evaluated on a proprietary enterprise dataset (deployment design discussed but not evaluated).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Channdeth Sok', 'David Luz', 'Yacine Haddam']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'retrieval-augmented-generation', 'metamorphic-testing', 'model-safety', 'identity-aware-guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09360</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are Humans as Brittle as Large Language Models?</title><link>https://arxiv.org/abs/2509.07869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically compares sensitivity to prompt modifications between human annotators and LLMs on text classification tasks.&lt;/li&gt;&lt;li&gt;Finds both humans and LLMs show increased brittleness for certain prompt changes, especially substitutions of alternative label sets or label formats.&lt;/li&gt;&lt;li&gt;Identifies differences: human judgments are less impacted by typographical errors and reversed label order than LLM outputs.&lt;/li&gt;&lt;li&gt;Implication: prompt brittleness is not entirely unique to LLMs, but specific perturbations disproportionately affect model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahui Li', 'Sean Papay', 'Roman Klinger']&lt;/li&gt;&lt;li&gt;Tags: ['prompt brittleness', 'LLM robustness', 'human annotation variability', 'prompt engineering', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title><link>https://arxiv.org/abs/2507.23247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRiMH, a dataset and pragmatic reasoning tasks in the mental-health domain focusing on pragmatic implicature and presupposition.&lt;/li&gt;&lt;li&gt;Benchmarks several LLMs (Llama3.1, Mistral, MentaLLaMa, Qwen) on these tasks and analyzes model behavior (including rollout attention for MentaLLaMA).&lt;/li&gt;&lt;li&gt;Proposes prompts (StiPRompts) to study stigma-related responses and evaluates GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku for responsible handling of mental-health stigma.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sneha Oram', 'Pushpak Bhattacharyya']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'pragmatic-reasoning', 'LLM-benchmark', 'mental-health-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23247</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InterFeedback, an interactive framework for autonomously evaluating Large Multimodal Models' ability to use human feedback.&lt;/li&gt;&lt;li&gt;Presents InterFeedback-Bench, which uses two datasets (MMMU-Pro and MathVerse) to evaluate 10 open-source LMMs, and InterFeedback-Human, a 120-case manual test set including evaluations of OpenAI-o1 and Claude-Sonnet-4.&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art LMMs struggle to refine responses based on human feedback (average &lt;50%), highlighting gaps in models' ability to interpret and benefit from feedback and motivating methods to improve feedback incorporation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interactive evaluation', 'benchmarking', 'human-in-the-loop', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations</title><link>https://arxiv.org/abs/2511.05359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConVerse, a dynamic benchmark for evaluating privacy and security risks in multi-turn agent-to-agent conversations across travel, real estate, and insurance domains.&lt;/li&gt;&lt;li&gt;Contains 864 contextually grounded attacks (611 privacy, 253 security) with a three-tier taxonomy for privacy abstraction and tests for tool-use and preference-manipulation attacks.&lt;/li&gt;&lt;li&gt;Evaluates seven state-of-the-art models, finding high vulnerability rates (privacy leaks up to 88%, security breaches up to 60%) and showing safety emerges (or fails) from agent communication patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amr Gomaa', 'Ahmed Salem', 'Sahar Abdelnabi']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent safety', 'privacy attacks', 'security benchmarking', 'agent red-teaming', 'tool-use vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05359</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title><link>https://arxiv.org/abs/2511.05017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a modality imbalance in large vision-language models (LVLMs) where appending visual embeddings leads to a language-dominant bias that increases hallucinations.&lt;/li&gt;&lt;li&gt;Proposes refining textual embeddings by integrating average-pooled visual features to better fuse visual information into the text stream.&lt;/li&gt;&lt;li&gt;Reports improved visual grounding and reduced hallucinations on established benchmarks; notes that more advanced fusion methods could further improve results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Agrawal', 'Gouthaman KV', 'Rohith Aralikatti', 'Gauri Jagatap', 'Jiaxin Yuan', 'Vijay Kamarshi', 'Andrea Fanelli', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'visual grounding', 'multimodal alignment', 'robustness', 'LVLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05017</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title><link>https://arxiv.org/abs/2511.04956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ORCHID is a modular agentic system combining retrieval-augmented generation (RAG) with cooperating small agents and a Model Context Protocol (MCP) for on‑premise, model-agnostic HRP classification.&lt;/li&gt;&lt;li&gt;Emphasizes human-in-the-loop decision-making: items uncertain for the model are deferred to SMEs, with step-by-step reasoning, grounded citations, and append-only audit bundles (run-cards, prompts, evidence).&lt;/li&gt;&lt;li&gt;Preliminary/demo results claim improved accuracy and traceability versus a non-agentic baseline and illustrate exportable audit artifacts and SME feedback capture for sensitive compliance workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Mahbub', 'Vanessa Lama', 'Sanjay Das', 'Brian Starks', 'Christopher Polchek', 'Saffell Silvers', 'Lauren Deck', 'Prasanna Balaprakash', 'Tirthankar Ghosal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Retrieval-Augmented Generation', 'Human-in-the-loop', 'Auditability', 'On-premise deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04956</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA (Needle-in-haystack jailbreak), a method that appends benign, model-generated content to harmful user goals to bypass safety in long-context LMs.&lt;/li&gt;&lt;li&gt;Shows that the positional placement of harmful goals within extended contexts significantly affects safety; careful goal positioning increases jailbreak success.&lt;/li&gt;&lt;li&gt;Evaluates on HarmBench across state-of-the-art open and proprietary models (LLaMA, Qwen, Mistral, Gemini), demonstrating higher attack success, transferability, low detectability, and compute-optimality of longer contexts versus more trials.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt-injection', 'long-context', 'safety-evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Vulnerabilities With Agent Personas</title><link>https://arxiv.org/abs/2511.04697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses LLMs to build agent personas (five professions × three mental schemas) to simulate responses to news headlines and misinformation.&lt;/li&gt;&lt;li&gt;Validates that LLM-generated agent responses align with ground-truth labels and human predictions, supporting their use as proxies in studies.&lt;/li&gt;&lt;li&gt;Finds that agents' mental schemas influence interpretation of misinformation more than professional background.&lt;/li&gt;&lt;li&gt;Proposes an agent-based LLM simulation framework for analyzing trust, polarization, and susceptibility to deceptive content in information networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Farr', 'Lynnette Hui Xian Ng', 'Stephen Prochaska', 'Iain J. Cruickshank', 'Jevin West']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM agents', 'safety-evaluation', 'social-simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04697</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steering Language Models with Weight Arithmetic</title><link>https://arxiv.org/abs/2511.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'contrastive weight steering': compute a behavior direction in weight-space by subtracting weight deltas from two small fine-tunes (one inducing a desired behavior, one inducing the opposite) and add/remove that direction to edit model weights post-training.&lt;/li&gt;&lt;li&gt;Applies weight steering to mitigate sycophancy and to deliberately induce misalignment, finding it often generalizes further out-of-distribution than activation-level steering while maintaining capabilities longer.&lt;/li&gt;&lt;li&gt;Shows weight steering can partially undo undesired behavioral drift from task-specific fine-tuning (reducing sycophancy and under-refusal) without losing task performance gains.&lt;/li&gt;&lt;li&gt;Provides preliminary evidence that similarity between fine-tuning updates and an 'evil' weight direction can detect emergent misalignment, suggesting a weight-space monitoring approach to catch rare misaligned behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constanza Fierro', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'weight-space editing', 'model steering', 'safety monitoring', 'red-teaming / adversarial behaviors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05408</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE: a dynamic evaluation suite for testing LLM adherence to custom behavioral policies in multi-turn interactive conversations.&lt;/li&gt;&lt;li&gt;Provides a dataset of 300 realistic behavioral policies spanning 30 industries and a framework to stress-test compliance under adversarial conditions.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn tests (&lt;4%) but substantial compliance breakdowns in multi-turn adversarial interactions (up to 84%).&lt;/li&gt;&lt;li&gt;Releases both dataset and analytical framework to support research on pluralistic alignment, robustness, and context-aware safety techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety evaluation', 'red teaming', 'adversarial testing', 'behavioral policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title><link>https://arxiv.org/abs/2511.04962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Moral RolePlay, a benchmark with a four-level moral alignment scale and balanced test set to evaluate LLMs' ability to role-play characters from paragons to villains.&lt;/li&gt;&lt;li&gt;Large-scale evaluation shows a consistent, monotonic decline in role-playing fidelity as character morality decreases; models particularly fail on traits like 'Deceitful' and 'Manipulative'.&lt;/li&gt;&lt;li&gt;Finds that general chatbot proficiency does not predict villain role-playing ability, and highly safety-aligned models perform worst, highlighting a tension between safety alignment and creative fidelity.&lt;/li&gt;&lt;li&gt;Provides dataset and analysis aimed at guiding development of more nuanced, context-aware alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Yi', 'Qingxuan Jiang', 'Ruotian Ma', 'Xingyu Chen', 'Qu Yang', 'Mengru Wang', 'Fanghua Ye', 'Ying Shen', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarking', 'role-playing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04962</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that behavioral self-awareness in instruction-tuned LLMs can be induced with a single rank-1 LoRA adapter.&lt;/li&gt;&lt;li&gt;Finds the induced behavior is largely captured by a single steering vector in activation space, recovering most of the fine-tune's effect.&lt;/li&gt;&lt;li&gt;Reports that self-awareness is domain-localized (non-universal) with independent representations across tasks, making it easily modulated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'mechanistic interpretability', 'red teaming', 'parameter-efficient fine-tuning (LoRA)', 'behavioral concealment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title><link>https://arxiv.org/abs/2511.04869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows base LLMs can be semantically calibrated (i.e., meaningful confidence over answer-level semantics) under a sampling-based notion of semantic calibration despite being trained for next-token prediction.&lt;/li&gt;&lt;li&gt;Provides a theoretical mechanism linking next-token calibration and a generalized notion of 'B-calibration' (calibration over equivalence classes), explaining why semantic calibration can emerge as a byproduct of next-token objectives.&lt;/li&gt;&lt;li&gt;Derives and empirically validates three predictions: (1) base LLMs are semantically calibrated on QA tasks, (2) RL instruction-tuning tends to break this semantic calibration, and (3) chain-of-thought reasoning also breaks calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Preetum Nakkiran', 'Arwen Bradley', "Adam Goli\\'nski", 'Eugene Ndiaye', 'Michael Kirchhof', 'Sinead Williamson']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty_estimation', 'alignment', 'safety_evaluation', 'instruction_tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title><link>https://arxiv.org/abs/2511.04703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of 445 LLM benchmarks from major NLP/ML conferences focusing on how well benchmarks measure capabilities including safety and robustness.&lt;/li&gt;&lt;li&gt;Finds common patterns in tasks and metrics that undermine construct validity and lead to unreliable claims about phenomena like 'safety' and 'robustness'.&lt;/li&gt;&lt;li&gt;Provides eight key recommendations and practical guidance to improve the design and validity of LLM benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew M. Bean', 'Ryan Othniel Kearns', 'Angelika Romanou', 'Franziska Sofia Hafner', 'Harry Mayne', 'Jan Batzner', 'Negar Foroutan', 'Chris Schmitz', 'Karolina Korgul', 'Hunar Batra', 'Oishi Deb', 'Emma Beharry', 'Cornelius Emde', 'Thomas Foster', 'Anna Gausen', "Mar\\'ia Grandury", 'Simeng Han', 'Valentin Hofmann', 'Lujain Ibrahim', 'Hazel Kim', 'Hannah Rose Kirk', 'Fangru Lin', 'Gabrielle Kaili-May Liu', 'Lennart Luettgau', 'Jabez Magomere', 'Jonathan Rystr{\\o}m', 'Anna Sotnikova', 'Yushi Yang', 'Yilun Zhao', 'Adel Bibi', 'Antoine Bosselut', 'Ronald Clark', 'Arman Cohan', 'Jakob Foerster', 'Yarin Gal', 'Scott A. Hale', 'Inioluwa Deborah Raji', 'Christopher Summerfield', 'Philip H. S. Torr', 'Cozmin Ududec', 'Luc Rocher', 'Adam Mahdi']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'safety evaluation', 'robustness', 'construct validity', 'LLM evaluation methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04703</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Up the Instruction Ladder for Controllable Language Models</title><link>https://arxiv.org/abs/2511.04694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes enforcing instruction hierarchies (system &gt; user &gt; tools) as an explicit reasoning task where models must deliberate about instruction relationships before responding.&lt;/li&gt;&lt;li&gt;Introduces VerIH, a dataset of verifiable constraint-following tasks with aligned and conflicting system-user instructions, used to train models via lightweight reinforcement learning.&lt;/li&gt;&lt;li&gt;Finetuned models show improved instruction-following and generalize to safety-critical scenarios, increasing robustness to jailbreaks and prompt-injection attacks by resolving conflicts in favor of higher-priority policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuo Zheng', 'Vidhisha Balachandran', 'Chan Young Park', 'Faeze Brahman', 'Sachin Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['instruction hierarchy', 'jailbreak mitigation', 'prompt injection', 'LLM safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04694</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Provable Separations between Memorization and Generalization in Diffusion Models</title><link>https://arxiv.org/abs/2511.03202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves a dual separation showing why diffusion models memorize: (1) statistically, the ground-truth score does not minimize empirical denoising loss, incentivizing memorization; (2) approximation-theoretically, realizing the empirical score requires network size scaling with sample size, unlike the compact ground-truth.&lt;/li&gt;&lt;li&gt;Develops a pruning-based method for diffusion transformers that reduces memorization while preserving generation quality.&lt;/li&gt;&lt;li&gt;Connects these theoretical insights to privacy and safety concerns about training-data reproduction and offers an empirical mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeqi Ye', 'Qijie Zhu', 'Molei Tao', 'Minshuo Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'diffusion-models', 'model-compression', 'theoretical-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03202</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Navigate Socially Through Proactive Risk Perception</title><link>https://arxiv.org/abs/2510.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Proactive Risk Perception Module built on Falcon to predict distance-based collision risk scores for nearby humans using onboard RGB-D and odometry.&lt;/li&gt;&lt;li&gt;Targets socially compliant, collision-avoiding navigation in dynamic indoor human-populated environments without privileged information or global maps.&lt;/li&gt;&lt;li&gt;Evaluated on the Social-HM3D benchmark and reported improved personal-space compliance, achieving 2nd place in the IROS 2025 RoboSense Social Navigation Track.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erjia Xiao', 'Lingfeng Zhang', 'Yingbo Tang', 'Hao Cheng', 'Renjing Xu', 'Wenbo Ding', 'Lei Zhou', 'Long Chen', 'Hangjun Ye', 'Xiaoshuai Hao']&lt;/li&gt;&lt;li&gt;Tags: ['robotic social navigation', 'collision avoidance', 'risk perception', 'RGB-D perception', 'human-robot interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07871</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Performative Validity of Recourse Explanations</title><link>https://arxiv.org/abs/2506.15366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Recourse explanations (actionable suggestions for rejected applicants) are performative: if many applicants act on them and the model is refit, the data distribution and decision boundary can shift, invalidating prior recommendations.&lt;/li&gt;&lt;li&gt;The paper formally characterizes conditions under which recourse remains valid under performativity, highlighting that interventions on or recommendations influenced by non-causal variables can lead to invalidated recourse.&lt;/li&gt;&lt;li&gt;It cautions against standard counterfactual explanations and some causal recourse methods, and advocates for recourse that recommends actions exclusively on causal variables to maintain validity after model updates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gunnar K\\"onig', 'Hidde Fokkema', 'Timo Freiesleben', 'Celestine Mendler-D\\"unner', 'Ulrike von Luxburg']&lt;/li&gt;&lt;li&gt;Tags: ['algorithmic recourse', 'performative prediction', 'counterfactual explanations', 'causal inference', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15366</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>XBreaking: Understanding how LLMs security alignment can be broken</title><link>https://arxiv.org/abs/2504.21700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses Explainable AI to compare behavior of censored vs. uncensored LLMs and identify exploitable alignment patterns.&lt;/li&gt;&lt;li&gt;Proposes XBreaking, a targeted noise-injection attack that exploits those patterns to break censoring/alignment constraints.&lt;/li&gt;&lt;li&gt;Provides experimental evaluation demonstrating insights into censoring mechanisms and the effectiveness of the attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Arazzi', 'Vignesh Kumar Kembu', 'Antonino Nocera', 'Vinod P']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment attacks', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21700</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Comparative Study on Noise-Augmented Training and its Effect on Adversarial Robustness in ASR Systems</title><link>https://arxiv.org/abs/2409.01813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares adversarial robustness of four ASR architectures trained under three augmentation regimes (background noise + speed + reverberation; speed only; no augmentation).&lt;/li&gt;&lt;li&gt;Evaluates models against both white-box and black-box adversarial attacks.&lt;/li&gt;&lt;li&gt;Finds that noise-augmented training improves performance on noisy speech and increases robustness to adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karla Pizzi', "Mat\\'ias Pizarro", 'Asja Fischer']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'ASR', 'data augmentation', 'white-box attacks', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01813</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What Matters in Data for DPO?</title><link>https://arxiv.org/abs/2508.18312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic theoretical and empirical study of how preference data distribution affects Direct Preference Optimization (DPO) for aligning LLMs.&lt;/li&gt;&lt;li&gt;Finds that the quality of chosen (preferred) responses dominates DPO performance, while rejected response quality has limited impact.&lt;/li&gt;&lt;li&gt;Shows online DPO effectively reduces to supervised fine-tuning on chosen responses and analyzes how contrastiveness aids primarily by improving chosen samples.&lt;/li&gt;&lt;li&gt;Provides practical guidance for constructing high-impact preference datasets and interprets common data-mixing strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Zhongze Cai', 'Guanting Chen', 'Huaiyang Zhong', 'Chonghuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference data', 'Direct Preference Optimization (DPO)', 'supervised fine-tuning', 'LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18312</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title><link>https://arxiv.org/abs/2508.14926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a hierarchical Safe RL framework for urban driving that augments standard objectives with an ethics-aware cost combining collision probability and harm severity.&lt;/li&gt;&lt;li&gt;Introduces a dynamic, risk-sensitive prioritized experience replay to amplify learning from rare, high-risk events and improve rare-event risk control.&lt;/li&gt;&lt;li&gt;Implements execution-level polynomial path planning with PID and Stanley controllers to convert high-level targets into smooth, feasible trajectories.&lt;/li&gt;&lt;li&gt;Evaluates on closed-loop simulations from large-scale real-world traffic datasets, showing 25–45% reduction in conflict frequency while preserving comfort and ego performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianzhao Li', 'Ostap Okhrin']&lt;/li&gt;&lt;li&gt;Tags: ['safe-RL', 'ethics', 'autonomous-vehicles', 'rare-event-learning', 'prioritized-experience-replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14926</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Perfectly Truthful Calibration Measure</title><link>https://arxiv.org/abs/2508.13100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Averaged Two-Bin calibration error (ATB), a simple calibration measure that is perfectly and strictly truthful in the batch setting.&lt;/li&gt;&lt;li&gt;Shows ATB is quadratically related to existing measures (smCal and distCal) and provides an efficient linear-time calibration testing algorithm.&lt;/li&gt;&lt;li&gt;Provides a general construction recipe for truthful calibration measures based on variance additivity, and derives other truthful measures (e.g., quantile-binned l2-ECE).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Hartline', 'Lunjia Hu', 'Yifan Wu']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'evaluation robustness', 'truthful metrics', 'alignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13100</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title><link>https://arxiv.org/abs/2506.05583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces methods to adapt conformal prediction to unknown subpopulation shifts, providing valid coverage without access to true group labels.&lt;/li&gt;&lt;li&gt;Characterizes conditions under which formal coverage guarantees remain feasible when subpopulation labels must be inferred (relaxing the perfect-label assumption).&lt;/li&gt;&lt;li&gt;Proposes scalable algorithms suitable for high-dimensional settings and demonstrates empirical effectiveness on vision (vision transformers) and language (large language models) benchmarks.&lt;/li&gt;&lt;li&gt;Shows the proposed approach maintains coverage and controls risk in scenarios where standard conformal prediction fails under subpopulation shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nien-Shao Wang', 'Duygu Nur Yaldiz', 'Yavuz Faruk Bakman', 'Sai Praneeth Karimireddy']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'distribution shift', 'robustness', 'conformal prediction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05583</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Are Concepts Erased From Diffusion Models?</title><link>https://arxiv.org/abs/2505.17013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates mechanisms by which concepts may be removed from diffusion models: (i) disrupting internal guidance versus (ii) reducing unconditional likelihood of a concept.&lt;/li&gt;&lt;li&gt;Introduces a suite of independent probes to test whether a concept is truly erased, including visual context, modified diffusion trajectories, classifier guidance, and analysis of alternative generations.&lt;/li&gt;&lt;li&gt;Demonstrates that erasure robustness can fail under non-text-adversarial conditions and argues for comprehensive evaluation protocols for concept erasure in diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Lu', 'Nicky Kriplani', 'Rohit Gandikota', 'Minh Pham', 'David Bau', 'Chinmay Hegde', 'Niv Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'robustness', 'safety evaluation', 'adversarial probing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17013</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title><link>https://arxiv.org/abs/2501.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of techniques for deploying reasoning-capable LLMs and autonomous agents on resource-constrained edge devices, covering model optimization (quantization, sparsity, LoRA, distillation) that aim to preserve multi-step reasoning.&lt;/li&gt;&lt;li&gt;Covers system architectures (on-device inference, elastic offloading, cloud-edge collaboration) and adaptive intelligence (context compression, dynamic routing, federated personalization) with trade-offs across latency, energy, privacy, and capacity.&lt;/li&gt;&lt;li&gt;Synthesizes privacy-preserving learning, hardware-aware compilation, and robustness considerations and proposes a standardized evaluation protocol including latency, energy, accuracy, robustness, and privacy; highlights gaps such as edge-oriented safety/alignment evaluation and multi-agent testbeds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xubin Wang', 'Qing Li', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving learning', 'safety/alignment', 'robustness', 'edge-deployment', 'model-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03265</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ProFL: Performative Robust Optimal Federated Learning</title><link>https://arxiv.org/abs/2410.18075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Performative Robust Optimal Federated Learning (ProFL) to find performative-optimal models in federated settings where model deployment induces distribution shifts.&lt;/li&gt;&lt;li&gt;Handles noisy and contaminated client data and provides convergence guarantees under the Polyak–Łojasiewicz condition, covering non-convex objectives.&lt;/li&gt;&lt;li&gt;Empirical results across multiple datasets show improved performance over prior performative-federated approaches that converge to performatively stable (but suboptimal) points.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xue Zheng', 'Tian Xie', 'Xuwei Tan', 'Aylin Yener', 'Xueru Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'performative prediction', 'robustness', 'data contamination', 'distribution shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18075</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?</title><link>https://arxiv.org/abs/2511.05476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether student models obtained via knowledge distillation for language models of code truly mimic teacher behavior beyond accuracy, focusing on behavioral fidelity.&lt;/li&gt;&lt;li&gt;Proposes MetaCompress, a metamorphic testing framework that applies behavior-preserving transformations to compare teacher and student outputs and measure discrepancies.&lt;/li&gt;&lt;li&gt;Empirical results show students can exhibit up to 285% greater degradation under adversarial attacks and up to 62% behavioral discrepancies across tasks and distillation methods, highlighting robustness/security concerns in compressed models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Abdul Awal', 'Mrigank Rochan', 'Chanchal K. Roy']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge distillation', 'robustness / adversarial attacks', 'metamorphic testing', 'language models of code', 'behavioral fidelity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05476</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steering Language Models with Weight Arithmetic</title><link>https://arxiv.org/abs/2511.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'contrastive weight steering': compute a behavior direction in weight-space by subtracting weight deltas from two small fine-tunes (one inducing a desired behavior, one inducing the opposite) and add/remove that direction to edit model weights post-training.&lt;/li&gt;&lt;li&gt;Applies weight steering to mitigate sycophancy and to deliberately induce misalignment, finding it often generalizes further out-of-distribution than activation-level steering while maintaining capabilities longer.&lt;/li&gt;&lt;li&gt;Shows weight steering can partially undo undesired behavioral drift from task-specific fine-tuning (reducing sycophancy and under-refusal) without losing task performance gains.&lt;/li&gt;&lt;li&gt;Provides preliminary evidence that similarity between fine-tuning updates and an 'evil' weight direction can detect emergent misalignment, suggesting a weight-space monitoring approach to catch rare misaligned behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constanza Fierro', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'weight-space editing', 'model steering', 'safety monitoring', 'red-teaming / adversarial behaviors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05408</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE: a dynamic evaluation suite for testing LLM adherence to custom behavioral policies in multi-turn interactive conversations.&lt;/li&gt;&lt;li&gt;Provides a dataset of 300 realistic behavioral policies spanning 30 industries and a framework to stress-test compliance under adversarial conditions.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn tests (&lt;4%) but substantial compliance breakdowns in multi-turn adversarial interactions (up to 84%).&lt;/li&gt;&lt;li&gt;Releases both dataset and analytical framework to support research on pluralistic alignment, robustness, and context-aware safety techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety evaluation', 'red teaming', 'adversarial testing', 'behavioral policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an end-to-end differentiable framework that parameterizes shapes with a Fourier series, maps them to pixel grids via a winding-number-based mapping, and uses signal-energy constraints for plausible optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can elicit high-confidence semantic classifications from DNNs and act as high-fidelity interpretability probes isolating salient model regions.&lt;/li&gt;&lt;li&gt;Presents a new, generalizable adversarial paradigm based on geometric (shape-only) inputs that can deceive downstream visual tasks, implicating robustness and attack surfaces in vision models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'robustness', 'interpretability', 'red-teaming', 'computer-vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that behavioral self-awareness in instruction-tuned LLMs can be induced with a single rank-1 LoRA adapter.&lt;/li&gt;&lt;li&gt;Finds the induced behavior is largely captured by a single steering vector in activation space, recovering most of the fine-tune's effect.&lt;/li&gt;&lt;li&gt;Reports that self-awareness is domain-localized (non-universal) with independent representations across tasks, making it easily modulated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'mechanistic interpretability', 'red teaming', 'parameter-efficient fine-tuning (LoRA)', 'behavioral concealment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title><link>https://arxiv.org/abs/2511.04869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows base LLMs can be semantically calibrated (i.e., meaningful confidence over answer-level semantics) under a sampling-based notion of semantic calibration despite being trained for next-token prediction.&lt;/li&gt;&lt;li&gt;Provides a theoretical mechanism linking next-token calibration and a generalized notion of 'B-calibration' (calibration over equivalence classes), explaining why semantic calibration can emerge as a byproduct of next-token objectives.&lt;/li&gt;&lt;li&gt;Derives and empirically validates three predictions: (1) base LLMs are semantically calibrated on QA tasks, (2) RL instruction-tuning tends to break this semantic calibration, and (3) chain-of-thought reasoning also breaks calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Preetum Nakkiran', 'Arwen Bradley', "Adam Goli\\'nski", 'Eugene Ndiaye', 'Michael Kirchhof', 'Sinead Williamson']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty_estimation', 'alignment', 'safety_evaluation', 'instruction_tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title><link>https://arxiv.org/abs/2511.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates copyright auditing/ownership verification specifically for soft prompts in vision-language models (CLIP) and shows existing intrusive and non-intrusive techniques fail for prompt learning.&lt;/li&gt;&lt;li&gt;Proposes SWAP (Sequential Watermarking for soft prompts): encodes a watermark via a defender-specified order of out-of-distribution classes leveraging CLIP's zero-shot capabilities to avoid conflicting with primary task decisions.&lt;/li&gt;&lt;li&gt;Provides a hypothesis-test-guided verification protocol, theoretical success-condition analysis, and extensive experiments on 11 datasets demonstrating effectiveness, harmlessness, and robustness to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenyuan Yang', 'Yichen Sun', 'Changzheng Chen', 'Zhixuan Chu', 'Jiaheng Zhang', 'Yiming Li', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model ownership auditing', 'soft prompts', 'CLIP', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04711</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA (Needle-in-haystack jailbreak), a method that appends benign, model-generated content to harmful user goals to bypass safety in long-context LMs.&lt;/li&gt;&lt;li&gt;Shows that the positional placement of harmful goals within extended contexts significantly affects safety; careful goal positioning increases jailbreak success.&lt;/li&gt;&lt;li&gt;Evaluates on HarmBench across state-of-the-art open and proprietary models (LLaMA, Qwen, Mistral, Gemini), demonstrating higher attack success, transferability, low detectability, and compute-optimality of longer contexts versus more trials.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt-injection', 'long-context', 'safety-evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Communication-Constrained Private Decentralized Online Personalized Mean Estimation</title><link>https://arxiv.org/abs/2511.04702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies a consensus-based decentralized algorithm for online personalized mean estimation under differential privacy and communication constraints.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence analysis for bounded, agent-specific data distributions, showing private collaboration can converge faster than fully local approaches under certain privacy levels and connectivity assumptions.&lt;/li&gt;&lt;li&gt;Includes numerical experiments validating the theoretical faster-than-local convergence guarantees in an online setting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yauhen Yakimenka', 'Hsuan-Yin Lin', 'Eirik Rosnes', 'J\\"org Kliewer']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'decentralized-learning', 'privacy-preserving-collaboration', 'communication-constrained', 'online-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04702</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Multitask Adaptive Control</title><link>https://arxiv.org/abs/2511.05444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies adversarially robust multitask adaptive linear quadratic (LQR) control where multiple systems learn collaboratively under model uncertainty and adversarial corruption.&lt;/li&gt;&lt;li&gt;Proposes a clustered multitask method combining clustering, system identification, and resilient aggregation to mitigate corrupted model updates.&lt;/li&gt;&lt;li&gt;Provides non-asymptotic regret bounds showing regret decreases with the number of honest systems per cluster and is robust to a bounded fraction of adversarial systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kasra Fallah', 'Leonardo F. Toso', 'James Anderson']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'robust control', 'secure aggregation', 'multitask learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05444</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title><link>https://arxiv.org/abs/2511.05396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the supremal visitation ratio to quantify mismatch between training and deployment dynamics in off-dynamics (robust) RL.&lt;/li&gt;&lt;li&gt;Shows that if this ratio is unbounded, online learning becomes exponentially hard, capturing intrinsic exploration difficulty under distribution shift.&lt;/li&gt;&lt;li&gt;Presents a computationally efficient algorithm achieving sublinear regret for RMDPs with f-divergence transition uncertainty and proves matching lower bounds.&lt;/li&gt;&lt;li&gt;Includes numerical experiments validating theoretical findings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting He', 'Zhishuai Liu', 'Weixin Wang', 'Pan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reinforcement-learning', 'distribution-shift', 'sample-complexity', 'online-exploration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05396</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning</title><link>https://arxiv.org/abs/2511.05355</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAD-Flower: augments flow matching with a virtual control input to produce trajectories that satisfy state and action constraints and are dynamically consistent.&lt;/li&gt;&lt;li&gt;Leverages nonlinear control theory to derive principled guidance and provide formal guarantees for constraint satisfaction and dynamic executability.&lt;/li&gt;&lt;li&gt;Operates without retraining, enabling test-time enforcement of previously unseen constraints.&lt;/li&gt;&lt;li&gt;Evaluated across multiple tasks and shown to outperform generative-model baselines on constraint satisfaction metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tzu-Yuan Huang', 'Armin Lederer', 'Dai-Jie Wu', 'Xiaobing Dai', 'Sihua Zhang', 'Stefan Sosnowski', 'Shao-Hua Sun', 'Sandra Hirche']&lt;/li&gt;&lt;li&gt;Tags: ['flow-matching', 'safe-planning', 'control-theory', 'constraint-satisfaction', 'dynamical-consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05355</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting</title><link>https://arxiv.org/abs/2511.05289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies defenses against membership inference attacks (loss-based MIA) on clinical time series forecasting models trained on EHR data by using synthetic data augmentation.&lt;/li&gt;&lt;li&gt;Evaluates multiple augmentation strategies — Zeroth-Order Optimization (ZOO), ZOO constrained by PCA (ZOO-PCA), and MixUp — retraining the model with synthetic samples to reduce attacker TPR/FPR ratio.&lt;/li&gt;&lt;li&gt;Finds ZOO-PCA provides the best reduction in membership-inference effectiveness while preserving predictive performance on held-out test data.&lt;/li&gt;&lt;li&gt;Addresses the challenge of generating synthetic samples that are both close enough to training data to confuse attackers and novel enough to improve generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marius Fracarolli', 'Michael Staniek', 'Stefan Riezler']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'data-augmentation', 'time-series', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05289</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Associative Poisoning to Generative Machine Learning</title><link>https://arxiv.org/abs/2511.05177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'associative poisoning', a data-only poisoning attack that manipulates statistical associations between feature pairs in generative model outputs without control over training.&lt;/li&gt;&lt;li&gt;Provides a formal mathematical formulation and proofs of feasibility and stealthiness, claiming preservation of marginal feature distributions and high output quality to evade detection.&lt;/li&gt;&lt;li&gt;Empirically validates the attack on state-of-the-art generative systems (e.g., Stable Diffusion, ChatGPT) and discusses shortcomings of existing defenses while proposing a new countermeasure strategy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mathias Lundteigen Mohus', 'Jingyue Li', 'Zhirong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'generative models', 'stealthy attacks', 'statistical manipulation', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05177</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding</title><link>https://arxiv.org/abs/2511.04934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces leak@k, a meta-evaluation metric that measures the probability of 'forgotten' information reappearing when generating k samples under probabilistic decoding.&lt;/li&gt;&lt;li&gt;Performs large-scale evaluation on TOFU, MUSE, and WMDP and shows that many existing unlearning methods appear effective under deterministic (greedy) decoding but still leak sensitive information under standard sampling strategies.&lt;/li&gt;&lt;li&gt;Finds that knowledge leakage persists across tasks and methods, arguing current unlearning techniques provide only limited forgetting and calling for more robust unlearning approaches and evaluations under realistic decoding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Reisizadeh', 'Jiajun Ruan', 'Yiwei Chen', 'Soumyadeep Pal', 'Sijia Liu', 'Mingyi Hong']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'privacy leakage', 'evaluation/benchmarking', 'probabilistic decoding', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04934</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between two safety paradigms for text-to-image diffusion models: fine-tuning to unlearn harmful concepts and training-free negative-prompt guidance, which can reduce defense effectiveness when combined.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, enabling compatibility between unlearning and guidance methods without modifying either.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate on nudity and violence benchmarks while preserving prompt semantics, and shows easy integration into existing pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image safety', 'model unlearning', 'negative prompting', 'concept inversion', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Brittleness of CLIP Text Encoders</title><link>https://arxiv.org/abs/2511.04247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of how various non-semantic query perturbations (lexical, syntactic, semantic) affect CLIP-style multimodal retrieval.&lt;/li&gt;&lt;li&gt;Evaluates multiple CLIP variants on TRECVID Ad-Hoc Video Search queries and the V3C1 video collection.&lt;/li&gt;&lt;li&gt;Finds large instabilities from syntactic and semantic perturbations and concentrated brittleness from trivial surface edits (punctuation, case).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Allie Tran', 'Luca Rossetto']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language', 'retrieval', 'adversarial-perturbations', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04247</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</title><link>https://arxiv.org/abs/2510.22963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies prompt compression modules in LLM agent pipelines as a new attack surface that can be manipulated to cause semantic drift and alter model behavior.&lt;/li&gt;&lt;li&gt;Proposes CompressionAttack with two methods: HardCom (discrete adversarial edits for hard compression) and SoftCom (latent-space perturbations for soft compression).&lt;/li&gt;&lt;li&gt;Reports strong empirical results across multiple LLMs (up to ~80% attack success, ~98% preference flips), high stealth and transferability, and demonstrates real-world impact in VSCode CLI and Ollama integrations.&lt;/li&gt;&lt;li&gt;Evaluates existing defenses and finds them ineffective, arguing for the need for stronger protections around compression modules.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zesen Liu', 'Zhixiang Zhang', 'Yuchong Xie', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['prompt compression', 'adversarial attacks', 'LLM security', 'prompt injection', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22963</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Navigate Socially Through Proactive Risk Perception</title><link>https://arxiv.org/abs/2510.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Proactive Risk Perception Module built on Falcon to predict distance-based collision risk scores for nearby humans using onboard RGB-D and odometry.&lt;/li&gt;&lt;li&gt;Targets socially compliant, collision-avoiding navigation in dynamic indoor human-populated environments without privileged information or global maps.&lt;/li&gt;&lt;li&gt;Evaluated on the Social-HM3D benchmark and reported improved personal-space compliance, achieving 2nd place in the IROS 2025 RoboSense Social Navigation Track.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erjia Xiao', 'Lingfeng Zhang', 'Yingbo Tang', 'Hao Cheng', 'Renjing Xu', 'Wenbo Ding', 'Lei Zhou', 'Long Chen', 'Hangjun Ye', 'Xiaoshuai Hao']&lt;/li&gt;&lt;li&gt;Tags: ['robotic social navigation', 'collision avoidance', 'risk perception', 'RGB-D perception', 'human-robot interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07871</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Policy-as-Prompt: a framework that converts unstructured governance artifacts into a source-linked policy tree, compiled into lightweight prompt-based classifiers for runtime enforcement.&lt;/li&gt;&lt;li&gt;Built to enforce least privilege and data minimization, with provenance, traceability, audit logging, and human-in-the-loop review for conformity assessment.&lt;/li&gt;&lt;li&gt;Evaluations report reductions in prompt-injection risk, blocking of out-of-scope requests, limitation of toxic outputs, and generation of auditable rationales aligned with governance frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'prompt-injection mitigation', 'policy-as-code', 'runtime guardrails', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks</title><link>https://arxiv.org/abs/2509.12386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Amulet, a Python library to evaluate intended and unintended interactions among ML defenses and risks.&lt;/li&gt;&lt;li&gt;Provides a comprehensive, extensible, and consistent API including representative attacks, defenses, and metrics.&lt;/li&gt;&lt;li&gt;Enables systematic evaluation of how defenses for one risk can affect susceptibility to other risks (unintended interactions).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asim Waheed', 'Vasisht Duddu', 'Rui Zhang', 'Sebastian Szyller']&lt;/li&gt;&lt;li&gt;Tags: ['defense-evaluation', 'security', 'tooling/library', 'unintended-interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12386</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title><link>https://arxiv.org/abs/2508.20866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AVIATOR, an AI-agentic workflow that orchestrates specialized LLM agents, function agents, and static analysis tools to automatically inject realistic, category-specific software vulnerabilities.&lt;/li&gt;&lt;li&gt;Uses semantic analysis, LoRA-based fine-tuning, Retrieval-Augmented Generation, and LLM-based discriminators, plus post-injection validation via static analysis and discriminators.&lt;/li&gt;&lt;li&gt;Reports high injection success rates (91%–95%) across three benchmarks and claims improved coverage, contextual fidelity, and reduced error propagation via modular agent decomposition.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Lbath', 'Massih-Reza Amini', 'Aurelien Delaitre', 'Vadim Okun']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability injection', 'software security', 'LLM agents', 'dataset generation', 'automated red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20866</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What Matters in Data for DPO?</title><link>https://arxiv.org/abs/2508.18312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows empirically and theoretically that the quality of chosen (preferred) responses dominates DPO performance, while rejected responses have limited impact.&lt;/li&gt;&lt;li&gt;Derives the optimal response distribution under DPO and explains how contrastiveness primarily helps by improving chosen samples.&lt;/li&gt;&lt;li&gt;Analyzes an online DPO setting and shows it effectively reduces to supervised fine-tuning on chosen responses.&lt;/li&gt;&lt;li&gt;Provides experiments across tasks and practical guidance on constructing high-impact preference datasets and mixing on-policy data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Zhongze Cai', 'Guanting Chen', 'Huaiyang Zhong', 'Chonghuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-learning', 'DPO', 'dataset-design', 'LLM-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18312</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title><link>https://arxiv.org/abs/2508.14926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a hierarchical Safe RL framework for urban driving that augments standard objectives with an ethics-aware cost combining collision probability and harm severity.&lt;/li&gt;&lt;li&gt;Introduces a dynamic, risk-sensitive prioritized experience replay to amplify learning from rare, high-risk events and improve rare-event risk control.&lt;/li&gt;&lt;li&gt;Implements execution-level polynomial path planning with PID and Stanley controllers to convert high-level targets into smooth, feasible trajectories.&lt;/li&gt;&lt;li&gt;Evaluates on closed-loop simulations from large-scale real-world traffic datasets, showing 25–45% reduction in conflict frequency while preserving comfort and ego performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianzhao Li', 'Ostap Okhrin']&lt;/li&gt;&lt;li&gt;Tags: ['safe-RL', 'ethics', 'autonomous-vehicles', 'rare-event-learning', 'prioritized-experience-replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14926</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title><link>https://arxiv.org/abs/2506.05583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces methods to adapt conformal prediction to unknown subpopulation shifts, providing valid coverage without access to true group labels.&lt;/li&gt;&lt;li&gt;Characterizes conditions under which formal coverage guarantees remain feasible when subpopulation labels must be inferred (relaxing the perfect-label assumption).&lt;/li&gt;&lt;li&gt;Proposes scalable algorithms suitable for high-dimensional settings and demonstrates empirical effectiveness on vision (vision transformers) and language (large language models) benchmarks.&lt;/li&gt;&lt;li&gt;Shows the proposed approach maintains coverage and controls risk in scenarios where standard conformal prediction fails under subpopulation shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nien-Shao Wang', 'Duygu Nur Yaldiz', 'Yavuz Faruk Bakman', 'Sai Praneeth Karimireddy']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'distribution shift', 'robustness', 'conformal prediction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05583</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>XBreaking: Understanding how LLMs security alignment can be broken</title><link>https://arxiv.org/abs/2504.21700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses Explainable AI to compare behavior of censored vs. uncensored LLMs and identify exploitable alignment patterns.&lt;/li&gt;&lt;li&gt;Proposes XBreaking, a targeted noise-injection attack that exploits those patterns to break censoring/alignment constraints.&lt;/li&gt;&lt;li&gt;Provides experimental evaluation demonstrating insights into censoring mechanisms and the effectiveness of the attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Arazzi', 'Vignesh Kumar Kembu', 'Antonino Nocera', 'Vinod P']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment attacks', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21700</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InterFeedback, an interactive framework for autonomously evaluating Large Multimodal Models' ability to use human feedback.&lt;/li&gt;&lt;li&gt;Presents InterFeedback-Bench, which uses two datasets (MMMU-Pro and MathVerse) to evaluate 10 open-source LMMs, and InterFeedback-Human, a 120-case manual test set including evaluations of OpenAI-o1 and Claude-Sonnet-4.&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art LMMs struggle to refine responses based on human feedback (average &lt;50%), highlighting gaps in models' ability to interpret and benefit from feedback and motivating methods to improve feedback incorporation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interactive evaluation', 'benchmarking', 'human-in-the-loop', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title><link>https://arxiv.org/abs/2501.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of techniques for deploying reasoning-capable LLMs and autonomous agents on resource-constrained edge devices, covering model optimization (quantization, sparsity, LoRA, distillation) that aim to preserve multi-step reasoning.&lt;/li&gt;&lt;li&gt;Covers system architectures (on-device inference, elastic offloading, cloud-edge collaboration) and adaptive intelligence (context compression, dynamic routing, federated personalization) with trade-offs across latency, energy, privacy, and capacity.&lt;/li&gt;&lt;li&gt;Synthesizes privacy-preserving learning, hardware-aware compilation, and robustness considerations and proposes a standardized evaluation protocol including latency, energy, accuracy, robustness, and privacy; highlights gaps such as edge-oriented safety/alignment evaluation and multi-agent testbeds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xubin Wang', 'Qing Li', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving learning', 'safety/alignment', 'robustness', 'edge-deployment', 'model-optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03265</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Proprietary Model-Based Safety Response Framework for AI Agents</title><link>https://arxiv.org/abs/2511.03138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a safety response framework for LLMs with input-level supervised fine-tuned safety classifier using a four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention) achieving 99.3% risk recall.&lt;/li&gt;&lt;li&gt;Implements output-level grounding by combining Retrieval-Augmented Generation (RAG) with a fine-tuned interpretation model to prevent hallucinations and enable traceability to trusted knowledge sources.&lt;/li&gt;&lt;li&gt;Evaluates against a baseline (TinyR1-Safety-8B) showing higher safety scores and reports 100% safety on a proprietary high-risk test set, presenting an engineering pathway for high-security LLM deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Li', 'Jianjun Xu', 'Pingtao Wei', 'Jiu Li', 'Peiqiang Zhao', 'Jiwei Shi', 'Xuan Zhang', 'Yanhui Yang', 'Xiaodong Hui', 'Peng Xu', 'Wenqin Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'safety classification', 'RAG grounding', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03138</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title><link>https://arxiv.org/abs/2510.15144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HugAgent, a benchmark for evaluating whether LLMs can simulate individualized human reasoning and predict a specific person’s responses and reasoning trajectories given partial prior evidence.&lt;/li&gt;&lt;li&gt;Proposes a dual-track design: a human track that scales think-aloud data collection for ecologically valid reasoning traces, and a synthetic track for scalable stress testing.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art LLMs and finds persistent adaptation gaps, positioning the benchmark as a tool for measuring cognitive alignment and personalization of model reasoning.&lt;/li&gt;&lt;li&gt;Open-sources the dataset, collection pipeline, and companion chatbot to enable extensible benchmarking and further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chance Jiajie Li', 'Zhenze Mo', 'Yuhan Tang', 'Ao Qu', 'Jiayi Wu', 'Kaiya Ivy Zhao', 'Yulu Gan', 'Jie Fan', 'Jiangbo Yu', 'Hang Jiang', 'Paul Pu Liang', 'Jinhua Zhao', 'Luis Alberto Alonso Pastor', 'Kent Larson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'benchmarking', 'LLM-evaluation', 'personalization', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15144</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Joint Verification and Refinement of Language Models for Safety-Constrained Planning</title><link>https://arxiv.org/abs/2410.14865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Converts LLM-generated robot programs into automaton-based representations and verifies them against task-relevant safety specifications.&lt;/li&gt;&lt;li&gt;Proves a compositionality theorem: any composition of verified subprograms also satisfies the safety specifications, reducing verification complexity for composed programs.&lt;/li&gt;&lt;li&gt;Introduces an automated fine-tuning procedure that uses verification outcomes as supervision to train the model to generate safe subcomponents rather than full programs.&lt;/li&gt;&lt;li&gt;Empirical results on robot tasks show a ~30% increase in specification-compliant program generation and a ~50% reduction in training time compared to fine-tuning on full-program generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Yang', 'Neel P. Bhatt', 'William Ward', 'Zichao Hu', 'Joydeep Biswas', 'Ufuk Topcu']&lt;/li&gt;&lt;li&gt;Tags: ['safety verification', 'formal methods', 'program synthesis', 'robot planning', 'safe model fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14865</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title><link>https://arxiv.org/abs/2511.05396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the supremal visitation ratio to quantify mismatch between training and deployment dynamics in off-dynamics (robust) RL.&lt;/li&gt;&lt;li&gt;Shows that if this ratio is unbounded, online learning becomes exponentially hard, capturing intrinsic exploration difficulty under distribution shift.&lt;/li&gt;&lt;li&gt;Presents a computationally efficient algorithm achieving sublinear regret for RMDPs with f-divergence transition uncertainty and proves matching lower bounds.&lt;/li&gt;&lt;li&gt;Includes numerical experiments validating theoretical findings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting He', 'Zhishuai Liu', 'Weixin Wang', 'Pan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reinforcement-learning', 'distribution-shift', 'sample-complexity', 'online-exploration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05396</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems</title><link>https://arxiv.org/abs/2511.05269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TAMAS, a benchmark of adversarial risks for multi-agent LLM systems with 300 adversarial instances across six attack types, 211 tools, and 100 harmless tasks covering five scenarios.&lt;/li&gt;&lt;li&gt;Evaluates ten backbone LLMs across three multi-agent interaction configurations (Autogen, CrewAI), exposing failure modes and high vulnerability to adversarial attacks.&lt;/li&gt;&lt;li&gt;Proposes an Effective Robustness Score (ERS) to quantify the tradeoff between task effectiveness and safety in multi-agent deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishan Kavathekar', 'Hemang Jain', 'Ameya Rathod', 'Ponnurangam Kumaraguru', 'Tanuja Ganu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-agent systems', 'adversarial attacks', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05269</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title><link>https://arxiv.org/abs/2511.05073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows empirically that image-based adversarial examples are more sensitive to occlusion than clean samples using CIFAR-10 and nine canonical attacks.&lt;/li&gt;&lt;li&gt;Introduces Sliding Mask Confidence Entropy (SMCE) to quantify confidence fluctuation under sliding-window occlusion and visualizes Mask Entropy Field Maps.&lt;/li&gt;&lt;li&gt;Proposes Sliding Window Mask-based Adversarial Example Detection (SWM-AED) that leverages SMCE for detection and avoids catastrophic overfitting in adversarial training.&lt;/li&gt;&lt;li&gt;Evaluates across classifiers and attacks on CIFAR-10, reporting robust detection performance (typical accuracy &gt;62%, up to 96.5%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Li', 'Yanwei Xu', 'Keran Li', 'Xiaoli Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'robustness', 'occlusion-based defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05073</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE: a dynamic evaluation suite for testing LLM adherence to custom behavioral policies in multi-turn interactive conversations.&lt;/li&gt;&lt;li&gt;Provides a dataset of 300 realistic behavioral policies spanning 30 industries and a framework to stress-test compliance under adversarial conditions.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn tests (&lt;4%) but substantial compliance breakdowns in multi-turn adversarial interactions (up to 84%).&lt;/li&gt;&lt;li&gt;Releases both dataset and analytical framework to support research on pluralistic alignment, robustness, and context-aware safety techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'safety evaluation', 'red teaming', 'adversarial testing', 'behavioral policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an end-to-end differentiable framework that parameterizes shapes with a Fourier series, maps them to pixel grids via a winding-number-based mapping, and uses signal-energy constraints for plausible optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can elicit high-confidence semantic classifications from DNNs and act as high-fidelity interpretability probes isolating salient model regions.&lt;/li&gt;&lt;li&gt;Presents a new, generalizable adversarial paradigm based on geometric (shape-only) inputs that can deceive downstream visual tasks, implicating robustness and attack surfaces in vision models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'robustness', 'interpretability', 'red-teaming', 'computer-vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title><link>https://arxiv.org/abs/2511.04962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Moral RolePlay, a benchmark with a four-level moral alignment scale and balanced test set to evaluate LLMs' ability to role-play characters from paragons to villains.&lt;/li&gt;&lt;li&gt;Large-scale evaluation shows a consistent, monotonic decline in role-playing fidelity as character morality decreases; models particularly fail on traits like 'Deceitful' and 'Manipulative'.&lt;/li&gt;&lt;li&gt;Finds that general chatbot proficiency does not predict villain role-playing ability, and highly safety-aligned models perform worst, highlighting a tension between safety alignment and creative fidelity.&lt;/li&gt;&lt;li&gt;Provides dataset and analysis aimed at guiding development of more nuanced, context-aware alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Yi', 'Qingxuan Jiang', 'Ruotian Ma', 'Xingyu Chen', 'Qu Yang', 'Mengru Wang', 'Fanghua Ye', 'Ying Shen', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarking', 'role-playing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04962</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2511.04949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a latent-space learnable watermark embedder for proactive deepfake detection that encodes/extracts messages using high-level image semantics.&lt;/li&gt;&lt;li&gt;Uses a Multi-Agent Adversarial Reinforcement Learning (MAARL) setup where a watermarking agent and an adversarial attacker agent co-train to balance robustness to benign distortions and sensitivity to malicious tampering.&lt;/li&gt;&lt;li&gt;Evaluated on CelebA and CelebA-HQ, reporting consistent gains over state-of-the-art watermarking approaches (≈4.5% on CelebA, ≈5.3% on CelebA-HQ) under challenging manipulation scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tharindu Fernando', 'Clinton Fookes', 'Sridha Sridharan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'watermarking', 'adversarial reinforcement learning', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04949</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that behavioral self-awareness in instruction-tuned LLMs can be induced with a single rank-1 LoRA adapter.&lt;/li&gt;&lt;li&gt;Finds the induced behavior is largely captured by a single steering vector in activation space, recovering most of the fine-tune's effect.&lt;/li&gt;&lt;li&gt;Reports that self-awareness is domain-localized (non-universal) with independent representations across tasks, making it easily modulated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'mechanistic interpretability', 'red teaming', 'parameter-efficient fine-tuning (LoRA)', 'behavioral concealment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between two safety paradigms for text-to-image diffusion models: fine-tuning to unlearn harmful concepts and training-free negative-prompt guidance, which can reduce defense effectiveness when combined.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, enabling compatibility between unlearning and guidance methods without modifying either.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate on nudity and violence benchmarks while preserving prompt semantics, and shows easy integration into existing pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image safety', 'model unlearning', 'negative prompting', 'concept inversion', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models</title><link>https://arxiv.org/abs/2511.04728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniyal Ganiuly', 'Assel Smaiyl']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04728</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models</title><link>https://arxiv.org/abs/2511.04716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a realistic grey-box threat model for cognitive diagnosis models (CDMs) that leverages exposed internal knowledge state vectors shown via visualizations (e.g., radar charts).&lt;/li&gt;&lt;li&gt;Demonstrates that these internal vectors can be reverse-engineered from visualizations, creating a new attack surface.&lt;/li&gt;&lt;li&gt;Proposes P-MIA, a profile-based membership inference attack that uses both model prediction probabilities and reconstructed internal state vectors, outperforming black-box baselines on real datasets.&lt;/li&gt;&lt;li&gt;Uses P-MIA as an auditing tool to evaluate machine unlearning techniques and exposes their limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingliang Hou', 'Yinuo Wang', 'Teng Guo', 'Zitao Liu', 'Wenzhou Dou', 'Jiaqi Zheng', 'Renqiang Luo', 'Mi Tian', 'Weiqi Luo']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'model-inversion', 'auditing', 'machine-unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04716</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title><link>https://arxiv.org/abs/2511.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates copyright auditing/ownership verification specifically for soft prompts in vision-language models (CLIP) and shows existing intrusive and non-intrusive techniques fail for prompt learning.&lt;/li&gt;&lt;li&gt;Proposes SWAP (Sequential Watermarking for soft prompts): encodes a watermark via a defender-specified order of out-of-distribution classes leveraging CLIP's zero-shot capabilities to avoid conflicting with primary task decisions.&lt;/li&gt;&lt;li&gt;Provides a hypothesis-test-guided verification protocol, theoretical success-condition analysis, and extensive experiments on 11 datasets demonstrating effectiveness, harmlessness, and robustness to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenyuan Yang', 'Yichen Sun', 'Changzheng Chen', 'Zhixuan Chu', 'Jiaheng Zhang', 'Yiming Li', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model ownership auditing', 'soft prompts', 'CLIP', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04711</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA (Needle-in-haystack jailbreak), a method that appends benign, model-generated content to harmful user goals to bypass safety in long-context LMs.&lt;/li&gt;&lt;li&gt;Shows that the positional placement of harmful goals within extended contexts significantly affects safety; careful goal positioning increases jailbreak success.&lt;/li&gt;&lt;li&gt;Evaluates on HarmBench across state-of-the-art open and proprietary models (LLaMA, Qwen, Mistral, Gemini), demonstrating higher attack success, transferability, low detectability, and compute-optimality of longer contexts versus more trials.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt-injection', 'long-context', 'safety-evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prioritize Economy or Climate Action? Investigating ChatGPT Response Differences Based on Inferred Political Orientation</title><link>https://arxiv.org/abs/2511.04706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study probing how ChatGPT responses vary when the model infers user political orientation via custom instructions and memory.&lt;/li&gt;&lt;li&gt;Creates three personas (Democratic, Republican, Neutral) with views on DEI, abortion, gun rights, and vaccination, then asks eight questions to elicit worldview differences.&lt;/li&gt;&lt;li&gt;Finds that ChatGPT responses align with inferred political views, showing differences in reasoning and vocabulary; custom instructions and memory produce similar inference effects.&lt;/li&gt;&lt;li&gt;Observes outputs tend to lean left, with the Democratic custom-instruction persona most similar to the neutral persona.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pelin Karadal', 'Dilara Kekulluoglu']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'safety_evaluation', 'personalization', 'alignment', 'political_bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04706</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title><link>https://arxiv.org/abs/2511.04703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of 445 LLM benchmarks from major NLP/ML conferences focusing on how well benchmarks measure capabilities including safety and robustness.&lt;/li&gt;&lt;li&gt;Finds common patterns in tasks and metrics that undermine construct validity and lead to unreliable claims about phenomena like 'safety' and 'robustness'.&lt;/li&gt;&lt;li&gt;Provides eight key recommendations and practical guidance to improve the design and validity of LLM benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew M. Bean', 'Ryan Othniel Kearns', 'Angelika Romanou', 'Franziska Sofia Hafner', 'Harry Mayne', 'Jan Batzner', 'Negar Foroutan', 'Chris Schmitz', 'Karolina Korgul', 'Hunar Batra', 'Oishi Deb', 'Emma Beharry', 'Cornelius Emde', 'Thomas Foster', 'Anna Gausen', "Mar\\'ia Grandury", 'Simeng Han', 'Valentin Hofmann', 'Lujain Ibrahim', 'Hazel Kim', 'Hannah Rose Kirk', 'Fangru Lin', 'Gabrielle Kaili-May Liu', 'Lennart Luettgau', 'Jabez Magomere', 'Jonathan Rystr{\\o}m', 'Anna Sotnikova', 'Yushi Yang', 'Yilun Zhao', 'Adel Bibi', 'Antoine Bosselut', 'Ronald Clark', 'Arman Cohan', 'Jakob Foerster', 'Yarin Gal', 'Scott A. Hale', 'Inioluwa Deborah Raji', 'Christopher Summerfield', 'Philip H. S. Torr', 'Cozmin Ududec', 'Luc Rocher', 'Adam Mahdi']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'safety evaluation', 'robustness', 'construct validity', 'LLM evaluation methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04703</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Vulnerabilities With Agent Personas</title><link>https://arxiv.org/abs/2511.04697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses LLMs to build agent personas (five professions × three mental schemas) to simulate responses to news headlines and misinformation.&lt;/li&gt;&lt;li&gt;Validates that LLM-generated agent responses align with ground-truth labels and human predictions, supporting their use as proxies in studies.&lt;/li&gt;&lt;li&gt;Finds that agents' mental schemas influence interpretation of misinformation more than professional background.&lt;/li&gt;&lt;li&gt;Proposes an agent-based LLM simulation framework for analyzing trust, polarization, and susceptibility to deceptive content in information networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Farr', 'Lynnette Hui Xian Ng', 'Stephen Prochaska', 'Iain J. Cruickshank', 'Jevin West']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM agents', 'safety-evaluation', 'social-simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04697</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Up the Instruction Ladder for Controllable Language Models</title><link>https://arxiv.org/abs/2511.04694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes enforcing instruction hierarchies (system &gt; user &gt; tools) as an explicit reasoning task where models must deliberate about instruction relationships before responding.&lt;/li&gt;&lt;li&gt;Introduces VerIH, a dataset of verifiable constraint-following tasks with aligned and conflicting system-user instructions, used to train models via lightweight reinforcement learning.&lt;/li&gt;&lt;li&gt;Finetuned models show improved instruction-following and generalize to safety-critical scenarios, increasing robustness to jailbreaks and prompt-injection attacks by resolving conflicts in favor of higher-priority policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuo Zheng', 'Vidhisha Balachandran', 'Chan Young Park', 'Faeze Brahman', 'Sachin Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['instruction hierarchy', 'jailbreak mitigation', 'prompt injection', 'LLM safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04694</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title><link>https://arxiv.org/abs/2511.04956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ORCHID is a modular agentic system combining retrieval-augmented generation (RAG) with cooperating small agents and a Model Context Protocol (MCP) for on‑premise, model-agnostic HRP classification.&lt;/li&gt;&lt;li&gt;Emphasizes human-in-the-loop decision-making: items uncertain for the model are deferred to SMEs, with step-by-step reasoning, grounded citations, and append-only audit bundles (run-cards, prompts, evidence).&lt;/li&gt;&lt;li&gt;Preliminary/demo results claim improved accuracy and traceability versus a non-agentic baseline and illustrate exportable audit artifacts and SME feedback capture for sensitive compliance workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Mahbub', 'Vanessa Lama', 'Sanjay Das', 'Brian Starks', 'Christopher Polchek', 'Saffell Silvers', 'Lauren Deck', 'Prasanna Balaprakash', 'Tirthankar Ghosal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Retrieval-Augmented Generation', 'Human-in-the-loop', 'Auditability', 'On-premise deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04956</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Epistemic Reject Option Prediction</title><link>https://arxiv.org/abs/2511.04855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an 'epistemic reject-option' predictor that abstains when epistemic (model/data) uncertainty is high due to limited training data.&lt;/li&gt;&lt;li&gt;Frames the optimal predictor as one minimizing expected regret relative to a Bayes-optimal predictor and rejects inputs whose regret exceeds a specified rejection cost.&lt;/li&gt;&lt;li&gt;Builds on Bayesian learning to quantify epistemic uncertainty and provide a principled abstention rule for inputs where training data is insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vojtech Franc', 'Jakub Paplham']&lt;/li&gt;&lt;li&gt;Tags: ['epistemic uncertainty', 'uncertainty quantification', 'reject option / abstention', 'Bayesian learning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04855</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
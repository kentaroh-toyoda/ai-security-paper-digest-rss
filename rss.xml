<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 08 Jan 2026 23:36:30 +0000</lastBuildDate><item><title>Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</title><link>https://arxiv.org/abs/2512.17394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CulturalToM-VQA: a 5,095-example benchmark of visually situated Theory-of-Mind probes spanning diverse cultural contexts, six ToM task types, and four complexity levels.&lt;/li&gt;&lt;li&gt;Evaluates 10 vision-language models (2023–2025); frontier models reach high overall accuracy (&gt;93%) but perform poorly on false-belief reasoning (19–83%) and show large regional performance gaps (20–30%).&lt;/li&gt;&lt;li&gt;Identifies systematic social desirability bias (preference for semantically positive answers) and reliance on parametric social priors; ablations show some models default to safety-aligned predictions.&lt;/li&gt;&lt;li&gt;Finds Chain-of-Thought prompting helps older models but yields minimal gains for newer ones, highlighting open challenges for robust, visually grounded cross-cultural social reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zabir Al Nazi', 'GM Shahariar', 'Md. Abrar Hossain', 'Wei Peng']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'bias', 'VLM-benchmark', 'theory-of-mind']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17394</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Human Intention to Action Prediction: Intention-Driven End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.12302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines the Intention-Driven End-to-End Autonomous Driving task and releases Intention-Drive, a dataset pairing complex natural-language human intentions with high-fidelity sensor data.&lt;/li&gt;&lt;li&gt;Introduces Imagined Future Alignment (IFA), an evaluation protocol using generative world models to assess semantic fulfillment of human goals beyond geometric/trajectory metrics.&lt;/li&gt;&lt;li&gt;Explores two solution paradigms—an end-to-end vision-language planner and a hierarchical agent framework—and shows existing models maintain driving stability but often fail to fulfill high-level intentions; proposed methods improve intention alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Zheng', 'Yucheng Zhou', 'Tianyi Yan', 'Jiayi Su', 'Hongjun Chen', 'Dubing Chen', 'Xingtai Gui', 'Wencheng Han', 'Runzhou Tao', 'Zhongying Qiu', 'Jianfei Yang', 'Jianbing Shen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarks', 'autonomous-driving', 'multimodal-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12302</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title><link>https://arxiv.org/abs/2511.05319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Semantic Steganography' to hide semantically rich sentence- or paragraph-level messages in cover media, not just bit-level payloads.&lt;/li&gt;&lt;li&gt;Proposes S^2LM, a pipeline that leverages large language models to embed and recover arbitrary sentences within images (Sentence-to-Image Steganography).&lt;/li&gt;&lt;li&gt;Introduces the Invisible Text (IVT) benchmark for evaluating sentence-level steganography and reports experiments showing direct sentence recovery beyond traditional bit-level methods.&lt;/li&gt;&lt;li&gt;Claims source code and dataset release; focuses on enabling structured, high-level secret messaging via LLMs and images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanqi Wu', 'Huangbiao Xu', 'Runfeng Xie', 'Jiaxin Cai', 'Kaixin Zhang', 'Xiao Ke']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'covert channels', 'LLMs', 'information hiding', 'security/privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05319</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</title><link>https://arxiv.org/abs/2509.22496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EAGLE, a lightweight black-box framework that attributes autoregressively generated tokens in MLLMs to compact image regions while quantifying reliance on language priors versus perceptual evidence.&lt;/li&gt;&lt;li&gt;Defines an objective unifying sufficiency (insight score) and indispensability (necessity score) and optimizes attributions via greedy search over sparsified image regions for efficiency and faithfulness.&lt;/li&gt;&lt;li&gt;Performs modality-aware analysis to disentangle tokens that depend on visual input from those driven by language priors, improving hallucination diagnosis.&lt;/li&gt;&lt;li&gt;Evaluated across open-source MLLMs, showing improved faithfulness, localization, and hallucination detection with lower GPU memory requirements compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoyu Chen', 'Xiaoqing Guo', 'Kangwei Liu', 'Siyuan Liang', 'Shiming Liu', 'Qunli Zhang', 'Laiyuan Wang', 'Hua Zhang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'multimodal LLMs', 'hallucination diagnosis', 'safety evaluation', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22496</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VISTA: Mitigating Semantic Inertia in Video-LLMs via Training-Free Dynamic Chain-of-Thought Routing</title><link>https://arxiv.org/abs/2505.11830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Semantic Inertia' in Video-LLMs: tendency to ignore visual evidence and rely on language priors, causing reasoning failures.&lt;/li&gt;&lt;li&gt;Proposes VISTA, a training-free framework that dynamically routes inference paths and converts implicit visual features into explicit textual anchors to strengthen perception signals.&lt;/li&gt;&lt;li&gt;Introduces a Latent Reasoning Consensus mechanism to reduce stochastic hallucinations and stabilize outputs.&lt;/li&gt;&lt;li&gt;Demonstrates substantial benchmark improvements (e.g., +9.3% on Egochema, +5.6% on VideoEspresso) versus base models, rivaling larger/proprietary models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Jin', 'Jiayu Ding', 'Siyi Xie', 'Guibo Luo', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'Video-LLM', 'chain-of-thought', 'multimodal robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11830</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing</title><link>https://arxiv.org/abs/2502.03826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FairT2I, a training-free, interactive framework that uses a latent variable guidance formulation to decompose and reweight attribute-conditioned components for bias-aware text-to-image generation.&lt;/li&gt;&lt;li&gt;Uses LLM-based bias detection to automatically infer bias-prone categories and attributes from text prompts and supports attribute resampling based on uniform, real-world, or user-specified distributions.&lt;/li&gt;&lt;li&gt;Provides a user interface for inspecting detected biases, modifying attribute weights, and generating debiased images in real time; reports that LLMs detect more and finer-grained bias categories than average human annotators.&lt;/li&gt;&lt;li&gt;Empirical results indicate improved societal bias mitigation and image diversity while preserving image quality and prompt fidelity compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinya Sakurai', 'Yuki Koyama', 'Issei Sato']&lt;/li&gt;&lt;li&gt;Tags: ['bias-mitigation', 'text-to-image', 'LLM-assisted-detection', 'fairness', 'latent-guidance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.03826</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models</title><link>https://arxiv.org/abs/2601.04163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of 14 pathology foundation models (PFMs) to scanner-induced domain shifts using a multiscanner breast cancer WSI dataset (384 slides, 5 scanners).&lt;/li&gt;&lt;li&gt;Finds most PFMs encode pronounced scanner-specific variability in embeddings; while AUC often remains stable, embedding shifts degrade calibration and introduce scanner-dependent bias in downstream predictions.&lt;/li&gt;&lt;li&gt;Robustness does not correlate simply with training data scale, model size, or recency; vision-language models show some robustness advantage in embeddings but underperform on supervised tasks.&lt;/li&gt;&lt;li&gt;Recommends moving beyond accuracy-centric benchmarks toward explicit evaluation and optimization of embedding stability and calibration under realistic acquisition variability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Thiringer', 'Fredrik K. Gustafsson', 'Kajsa Ledesma Eriksson', 'Mattias Rantalainen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'domain shift', 'medical imaging', 'calibration', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04163</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</title><link>https://arxiv.org/abs/2601.03714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates DeepSeek-OCR's true OCR capability by applying sentence- and word-level semantic corruption to separate vision performance from language priors.&lt;/li&gt;&lt;li&gt;Finds performance drops from ~90% to ~20% without linguistic support, and shows end-to-end methods are less robust than traditional pipeline OCR under semantic perturbations.&lt;/li&gt;&lt;li&gt;Shows lower visual token counts increase reliance on language priors and hallucination risk, and reports model collapse around 10,000 text tokens under context stress testing.&lt;/li&gt;&lt;li&gt;Provides comparative benchmarking against 13 baselines and releases datasets and scripts for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Liang', 'Ruixuan Ying', 'Bo Li', 'Hong Li', 'Kai Yan', 'Qingwen Li', 'Min Yang', 'Okamoto Satoshi', 'Zhe Cui', 'Shiwen Ni']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination', 'OCR', 'vision-text compression', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03714</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</title><link>https://arxiv.org/abs/2601.04153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Diffusion-DRF, a method to fine-tune video diffusion models by backpropagating feedback from a frozen off-the-shelf vision-language model (VLM) through the diffusion denoising chain.&lt;/li&gt;&lt;li&gt;Converts VLM logit-level responses into token-aware gradients and uses an automated aspect-structured prompting pipeline to obtain multi-dimensional, training-free feedback.&lt;/li&gt;&lt;li&gt;Claims improved video quality and semantic alignment while mitigating reward hacking and training collapse without additional reward models or preference datasets; method is model-agnostic and generalizes to other diffusion tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Wang', 'Yanyu Li', 'Sergey Tulyakov', 'Yun Fu', 'Anil Kag']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward hacking', 'robustness', 'diffusion models', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04153</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning</title><link>https://arxiv.org/abs/2601.04118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GeoReason, a framework to align internal reasoning chains with final answers in remote sensing vision-language models by reducing logical hallucinations.&lt;/li&gt;&lt;li&gt;Constructs GeoReason-Bench: 4,000 logic-driven reasoning trajectories synthesized from geometric primitives and expert knowledge for training/evaluation.&lt;/li&gt;&lt;li&gt;Two-stage training: (1) Supervised Knowledge Initialization to teach reasoning syntax/domain expertise, and (2) Consistency-Aware Reinforcement Learning using a Logical Consistency Reward (with an option permutation strategy) to penalize logical drift.&lt;/li&gt;&lt;li&gt;Reports improved deductive reliability and interpretability, claiming state-of-the-art performance on RS-VLM reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenshuai Li', 'Xiantai Xiang', 'Zixiao Wen', 'Guangyao Zhou', 'Ben Niu', 'Feng Wang', 'Lijia Huang', 'Qiantong Wang', 'Yuxin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Hallucination mitigation', 'Reinforcement learning', 'Vision-language models', 'Robustness/Safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04118</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</title><link>https://arxiv.org/abs/2601.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode called "textual inertia" where once a textual hallucination appears in chain-of-thought, LMMs typically ignore conflicting visual evidence and propagate the error.&lt;/li&gt;&lt;li&gt;Introduces the LogicGraph Perturbation Protocol to structurally inject perturbations into reasoning chains across diverse LMMs to measure self-reflection and correction capability.&lt;/li&gt;&lt;li&gt;Finds that models self-correct in under 10% of cases and largely succumb to blind textual error propagation.&lt;/li&gt;&lt;li&gt;Proposes Active Visual-Context Refinement, a training-free inference method that actively re-grounds visual context and adaptively refines reasoning history to reduce hallucination propagation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhu', 'Jiafeng Liang', 'Shixin Jiang', 'Jinlan Fu', 'Ming Liu', 'Guanglu Sun', 'See-Kiong Ng', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'hallucination', 'robustness', 'chain-of-thought', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04073</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models</title><link>https://arxiv.org/abs/2601.04068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LocalDPO, a post-training method that creates localized preference pairs by corrupting spatio-temporal regions of real videos and restoring only those regions with a frozen base model.&lt;/li&gt;&lt;li&gt;Automated single-inference pipeline generates preference pairs without external critics or manual annotation; introduces a region-aware DPO loss that focuses learning on corrupted regions for faster convergence.&lt;/li&gt;&lt;li&gt;Evaluated on Wan2.1 and CogVideoX, showing consistent improvements in video fidelity, temporal coherence, and human preference scores over other post-training alignment approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zitong Huang', 'Kaidong Zhang', 'Yukang Ding', 'Chao Gao', 'Rui Ding', 'Ying Chen', 'Wangmeng Zuo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'video diffusion', 'preference optimization', 'post-training', 'direct preference optimization (DPO)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04068</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting AI-Generated Images via Distributional Deviations from Real Images</title><link>https://arxiv.org/abs/2601.03586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes frozen CLIP-ViT image encoder and finds it clusters real images but lacks inherent real-vs-AI discrimination.&lt;/li&gt;&lt;li&gt;Proposes Masking-based Pre-trained model Fine-Tuning (MPFT) with Texture-Aware Masking (TAM) to force attention to distributional deviations characteristic of generative models.&lt;/li&gt;&lt;li&gt;Claims strong generalization: with minimal fine-tuning data, achieves up to 98.2% and 94.6% average accuracy on GenImage and UniversalFakeDetect datasets respectively.&lt;/li&gt;&lt;li&gt;Emphasizes improving detection of AI-generated images (deepfakes) by exploiting CLIP features and targeted masking rather than treating the encoder as a static feature extractor.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yakun Niu', 'Yingjian Chen', 'Lei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['image-detection', 'deepfake-detection', 'generative-model-detection', 'CLIP-finetuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03586</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.03500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a visual statistical bias in vision encoders (Bag-of-Patches behavior) that prioritizes local texture over global structure and contributes to object hallucinations in LVLMs.&lt;/li&gt;&lt;li&gt;Proposes SDCD (Structure-Disrupted Contrastive Decoding), a training-free decoding algorithm that contrasts the model's outputs against a shuffled, structure-disrupted view and penalizes tokens that remain highly confident under the disrupted view.&lt;/li&gt;&lt;li&gt;Reports that SDCD significantly reduces hallucinations across multiple benchmarks and improves overall multimodal capabilities without additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Xia', 'Siheng Wang', 'Peng Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'robustness', 'decoding methods', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03500</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding Reward Hacking in Text-to-Image Reinforcement Learning</title><link>https://arxiv.org/abs/2601.03468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of reward hacking in text-to-image (T2I) reinforcement learning, showing that imperfect aesthetic and prompt-image consistency rewards can be exploited.&lt;/li&gt;&lt;li&gt;Finds a common failure mode across reward models: generation of artifact-prone, unrealistic images that nevertheless score highly on reward functions; ensembling rewards helps only partially.&lt;/li&gt;&lt;li&gt;Proposes a lightweight, adaptive artifact reward model trained on a small curated dataset of artifact-free and artifact-containing samples to act as a regularizer in RL pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates that adding the artifact reward improves visual realism and reduces reward hacking across multiple T2I RL setups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunqi Hong', 'Kuei-Chun Kao', 'Hengguang Zhou', 'Cho-Jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment', 'robustness', 'text-to-image', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03468</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder</title><link>https://arxiv.org/abs/2601.03460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FROST-Drive: an end-to-end driving architecture that keeps a pretrained vision encoder frozen (from a VLM) and adds a transformer-based adapter + GRU decoder for waypoint generation.&lt;/li&gt;&lt;li&gt;Introduces a custom loss to directly optimize Rater Feedback Score (RFS) to prioritize robust trajectory planning and long-tail scenario performance.&lt;/li&gt;&lt;li&gt;Evaluates on the Waymo Open E2E dataset and reports that the frozen-encoder approach outperforms fully fine-tuned vision-encoder baselines, arguing better generalization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Dong', 'Yimin Zhu', 'Yu Wu', 'Yu Sun']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'safety-evaluation', 'transfer-learning', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03460</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.03416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAMBIT, a gamified multimodal jailbreak framework that decomposes and reassembles harmful visual semantics to steer model reasoning toward completing a jailbreak.&lt;/li&gt;&lt;li&gt;Designs 'instructional traps' that position the model as a game participant, exploiting chain-of-thought-style reasoning to lower safety attention and elicit malicious outputs.&lt;/li&gt;&lt;li&gt;Evaluates on multiple reasoning and non-reasoning MLLMs, reporting high attack success rates (e.g., ~92.1% on Gemini 2.5 Flash, ~91.2% on QvQ-MAX, ~85.9% on GPT-4o) and outperforming baseline attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangdong Hu', 'Yangyang Jiang', 'Qin Hu', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multimodal attacks', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03416</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Novel Unified Approach to Deepfake Detection</title><link>https://arxiv.org/abs/2601.03382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified architecture for deepfake detection in images and videos using cross-attention between spatial and frequency-domain features.&lt;/li&gt;&lt;li&gt;Introduces a 'blood detection' module as an additional cue for classification.&lt;/li&gt;&lt;li&gt;Reports very high AUC results on FaceForensics++ and Celeb-DF using backbones like Swin Transformer + BERT and EfficientNet-B4 + BERT, and claims strong cross-dataset generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lord Sen', 'Shyamapada Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimedia forensics', 'image/video', 'cross-domain features', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03382</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mass Concept Erasure in Diffusion Models with Concept Hierarchy</title><link>https://arxiv.org/abs/2601.03305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a supertype–subtype concept hierarchy to group semantically related concepts (e.g., bird → macaw, bald eagle) and erase them jointly in diffusion models to improve efficiency and effectiveness when mass-erasing concepts.&lt;/li&gt;&lt;li&gt;Proposes SuPLoRA (Supertype-Preserving Low-Rank Adaptation), which freezes the down-projection and updates only the up-projection in LoRA-style adapters to preserve supertype generation while suppressing subtypes.&lt;/li&gt;&lt;li&gt;Applies diffusion regularization to preserve denoising in unmasked regions and builds a benchmark requiring simultaneous erasure across diverse domains (celebrities, objects, pornographic content).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahang Tu', 'Ye Li', 'Yiming Wu', 'Hanbin Zhao', 'Chao Zhang', 'Hui Qian']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'concept erasure', 'content moderation', 'safety', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03305</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperCLOVA X 32B Think</title><link>https://arxiv.org/abs/2601.03286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HyperCLOVA X 32B Think, a vision-language model optimized for Korean language/culture with emphasis on reasoning and agentic capabilities.&lt;/li&gt;&lt;li&gt;Pre-training focused on reasoning, with post-training to enable multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences.&lt;/li&gt;&lt;li&gt;Reports strong benchmark performance on Korean text-to-text, vision-to-text, and agent-oriented evaluation tasks and announces open-source release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (model release/benchmarking)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NAVER Cloud HyperCLOVA X Team']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'alignment', 'agentic models', 'model release', 'Korean NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03286</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL, a framework that defines interpretable conversational traits from contrastive dialogue data and combines them into a scalar reward for aligning LLMs to human-likeness.&lt;/li&gt;&lt;li&gt;Uses the reward signal with standard preference optimization to align models of varying sizes without degrading overall performance, validated via large-scale human evaluations showing increased perceived human-likeness.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and inspectability of alignment by operating over explicit traits, enabling diagnosis of unintended effects and extending alignment to qualitative language properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference optimization', 'interpretability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Gray Area: Characterizing Moderator Disagreement on Reddit</title><link>https://arxiv.org/abs/2601.01620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 5 years / 4.3M moderation log entries from 24 subreddits to characterize 'gray area' (disputed) moderation cases vs undisputed cases.&lt;/li&gt;&lt;li&gt;Finds ~1 in 7 moderation actions are disputed; many involve ambiguous user intent (trolling, brigading) and governance tensions.&lt;/li&gt;&lt;li&gt;Nearly half of gray area cases involved automated moderation; information-theoretic analysis shows gray cases are intrinsically harder to adjudicate and SOTA language models perform poorly on them.&lt;/li&gt;&lt;li&gt;Emphasizes the continued necessity of expert human moderators and discusses challenges in current moderation processes and tooling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shayan Alipour', 'Shruti Phadke', 'Seyed Shahabeddin Mousavi', 'Amirhossein Afsharrad', 'Morteza Zihayat', 'Mattia Samory']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'safety-evaluation', 'model-evaluation', 'human-AI-moderation', 'dataset-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01620</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Human Intention to Action Prediction: Intention-Driven End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.12302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines the Intention-Driven End-to-End Autonomous Driving task and releases Intention-Drive, a dataset pairing complex natural-language human intentions with high-fidelity sensor data.&lt;/li&gt;&lt;li&gt;Introduces Imagined Future Alignment (IFA), an evaluation protocol using generative world models to assess semantic fulfillment of human goals beyond geometric/trajectory metrics.&lt;/li&gt;&lt;li&gt;Explores two solution paradigms—an end-to-end vision-language planner and a hierarchical agent framework—and shows existing models maintain driving stability but often fail to fulfill high-level intentions; proposed methods improve intention alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Zheng', 'Yucheng Zhou', 'Tianyi Yan', 'Jiayi Su', 'Hongjun Chen', 'Dubing Chen', 'Xingtai Gui', 'Wencheng Han', 'Runzhou Tao', 'Zhongying Qiu', 'Jianfei Yang', 'Jianbing Shen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarks', 'autonomous-driving', 'multimodal-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12302</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Representational Contrastive Scoring (RCS) to detect multimodal jailbreaks by inspecting LVLM internal representations and learning a lightweight projection to separate benign vs malicious inputs.&lt;/li&gt;&lt;li&gt;Implements two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute contrastive scores from safety-critical layers.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art generalization to unseen attack types on a challenging evaluation protocol while remaining computationally lightweight and interpretable.&lt;/li&gt;&lt;li&gt;Focuses on reducing over-rejection common in one-class anomaly detectors by leveraging internal geometric signals rather than surface-level features.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'LLM/LLMV red teaming', 'anomaly detection', 'internal representations', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multiplayer Nash Preference Optimization</title><link>https://arxiv.org/abs/2509.23102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multiplayer Nash Preference Optimization (MNPO), generalizing two-player Nash learning from human feedback (NLHF) to an n-player game where policies compete against a population and are regularized to a reference model.&lt;/li&gt;&lt;li&gt;Argues MNPO better captures non-transitive and heterogeneous human preferences, inherits equilibrium guarantees of two-player methods, and enables richer competitive dynamics.&lt;/li&gt;&lt;li&gt;Empirical results show MNPO outperforms existing NLHF baselines on instruction-following benchmarks, especially under heterogeneous annotator conditions and mixed-policy evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fang Wu', 'Xu Huang', 'Weihao Xuan', 'Zhiwei Zhang', 'Yijia Xiao', 'Guancheng Wan', 'Xiaomin Li', 'Bing Hu', 'Peng Xia', 'Jure Leskovec', 'Yejin Choi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'multi-agent', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23102</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models</title><link>https://arxiv.org/abs/2508.18760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes failures of large reasoning models (LRMs) to abstain on inherently unanswerable questions and characterizes distinct response behaviors.&lt;/li&gt;&lt;li&gt;Shows LRMs can internally recognize flaws in questions but fail to externally abstain, indicating a misalignment between internal cognition and output.&lt;/li&gt;&lt;li&gt;Proposes a lightweight two-stage mitigation combining cognitive monitoring with inference-time intervention to increase abstention rates.&lt;/li&gt;&lt;li&gt;Demonstrates experimentally that the method improves abstention while preserving overall reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Xiangyu Liu', 'Zequn Sun', 'Wei Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'abstention', 'safety', 'model monitoring', 'reasoning models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18760</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models</title><link>https://arxiv.org/abs/2502.20408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts methods from functional brain network analysis to identify recurring 'functional networks' of neurons within large language models.&lt;/li&gt;&lt;li&gt;Demonstrates that inhibiting identified networks markedly degrades model performance, while amplifying neurons in these networks can improve overall or task-specific performance.&lt;/li&gt;&lt;li&gt;Provides empirical evidence and code showing these networks map to specific tasks or global capabilities, suggesting avenues for targeted intervention and interpretation of LLM behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiheng Liu', 'Zhengliang Liu', 'Zihao Wu', 'Junhao Ning', 'Haiyang Sun', 'Sichen Xia', 'Yang Yang', 'Xiaohui Gao', 'Ning Qiang', 'Bao Ge', 'Tianming Liu', 'Junwei Han', 'Xintao Hu']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-editing', 'alignment', 'robustness', 'LLM internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20408</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.02993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a previously underexplored sensitivity: LLM answers vary substantially across permutations of Top-5 retrieved documents, even with the gold document fixed first.&lt;/li&gt;&lt;li&gt;Proposes Stable-RAG: run the generator over multiple retrieval orders, cluster hidden states, decode from a cluster-center representation to capture dominant reasoning patterns.&lt;/li&gt;&lt;li&gt;Uses the aggregated reasoning results to align outputs and mitigate permutation-induced hallucinations, improving accuracy and consistency.&lt;/li&gt;&lt;li&gt;Demonstrates improved answer accuracy, reasoning consistency, and robust generalization across datasets, retrievers, and input lengths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianchi Zhang', 'Hainan Zhang', 'Liang Pang', 'Hongwei Zheng', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation', 'Hallucination mitigation', 'Robustness', 'LLM consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02993</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</title><link>https://arxiv.org/abs/2512.17394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CulturalToM-VQA: a 5,095-example benchmark of visually situated Theory-of-Mind probes spanning diverse cultural contexts, six ToM task types, and four complexity levels.&lt;/li&gt;&lt;li&gt;Evaluates 10 vision-language models (2023–2025); frontier models reach high overall accuracy (&gt;93%) but perform poorly on false-belief reasoning (19–83%) and show large regional performance gaps (20–30%).&lt;/li&gt;&lt;li&gt;Identifies systematic social desirability bias (preference for semantically positive answers) and reliance on parametric social priors; ablations show some models default to safety-aligned predictions.&lt;/li&gt;&lt;li&gt;Finds Chain-of-Thought prompting helps older models but yields minimal gains for newer ones, highlighting open challenges for robust, visually grounded cross-cultural social reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zabir Al Nazi', 'GM Shahariar', 'Md. Abrar Hossain', 'Wei Peng']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'bias', 'VLM-benchmark', 'theory-of-mind']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.17394</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Investigating CoT Monitorability in Large Reasoning Models</title><link>https://arxiv.org/abs/2511.08525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates CoT Monitorability: using chain-of-thought (CoT) traces to monitor LRM misbehavior (e.g., shortcuts, sycophancy).&lt;/li&gt;&lt;li&gt;Analyzes two core challenges—verbalization faithfulness (do CoTs reflect true decision factors) and monitor reliability (sensitivity, susceptibility to deception).&lt;/li&gt;&lt;li&gt;Empirically evaluates correlations between verbalization quality, monitor reliability, and performance across mathematical, scientific, and ethical tasks, and studies effects of CoT interventions.&lt;/li&gt;&lt;li&gt;Proposes MoME, a paradigm where LLMs monitor other models' misbehavior via CoTs, providing structured judgments and supporting evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yang', 'Junchao Wu', 'Xilin Gong', 'Xuansheng Wu', 'Derek Wong', 'Ninghao Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'AI safety', 'model monitoring', 'deception detection', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08525</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis and Interpretation</title><link>https://arxiv.org/abs/2511.02626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a controlled dataset (Biography-Reasoning) to study how fine-tuning on new knowledge induces factual hallucinations across QA and reasoning tasks.&lt;/li&gt;&lt;li&gt;Finds that fine-tuning with new knowledge not only harms performance on newly introduced facts but also propagates hallucinations to previously known information; hallucination risk is higher when an entire knowledge type is unfamiliar.&lt;/li&gt;&lt;li&gt;Uses interpretability analyses to show that learning new knowledge weakens attention to key entities, causing over-reliance on surrounding context; reintroducing a small amount of known knowledge during later training restores attention and mitigates hallucinations.&lt;/li&gt;&lt;li&gt;Shows disrupted attention patterns can generalize across lexically similar contexts, enabling hallucinations to spread beyond the original training/task scope.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renfei Dang', 'Peng Hu', 'Zhejian Lai', 'Changjiang Gao', 'Min Zhang', 'Shujian Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'robustness', 'interpretability', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.02626</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EngTrace: A Symbolic Benchmark for Verifiable Process Supervision of Engineering Reasoning</title><link>https://arxiv.org/abs/2511.01650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EngTrace, a symbolic benchmark (1,350 contamination-resistant test cases from 90 templates) for verifiable engineering reasoning across multiple branches and domains.&lt;/li&gt;&lt;li&gt;Proposes a two-stage, verifiable evaluation that checks intermediate reasoning traces as well as final answers using automated procedural checks and an AI Tribunal.&lt;/li&gt;&lt;li&gt;Evaluates 24 LLMs and finds a trade-off between numeric precision and trace fidelity, plus a 'complexity cliff' where pretraining fails to support integrative, physically grounded engineering reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayesha Gull', 'Muhammad Usman Safder', 'Rania Elbadry', 'Fan Zhang', 'Veselin Stoyanov', 'Preslav Nakov', 'Zhuohan Xie']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'verification', 'LLM-evaluation', 'engineering-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01650</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title><link>https://arxiv.org/abs/2510.20721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;User study (n=94) using 90 PrivacyLens scenarios to measure how real users perceive helpfulness and privacy-preservation in LLM responses.&lt;/li&gt;&lt;li&gt;Found low inter-user agreement on identical LLM responses, while five proxy LLMs showed high agreement with each other but low correlation with human judgments.&lt;/li&gt;&lt;li&gt;Concludes that proxy LLM-based evaluations do not accurately reflect users' diverse perceptions of utility and privacy, arguing for more user-centered evaluation and better alignment between models and users.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyuan Wu', 'Roshni Kaushik', 'Wenkai Li', 'Lujo Bauer', 'Koichi Onoue']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'human-evaluation', 'alignment', 'benchmarking', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20721</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Qomhra: A Bilingual Irish and English Large Language Model</title><link>https://arxiv.org/abs/2510.17652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Qomhra, a bilingual Irish–English LLM trained under low-resource constraints with a pipeline covering continued pretraining, instruction tuning, and preference-data synthesis.&lt;/li&gt;&lt;li&gt;Proposes a novel method to synthesize human preference data by prompting a high-quality LLM to generate paired “accepted” and “rejected” responses, validating these with L1 Irish speakers and using Gemini-2.5-Pro to synthesize large-scale Irish preference and translated instruction datasets.&lt;/li&gt;&lt;li&gt;Evaluates Qomhra across translation, gender understanding, topic identification, and world-knowledge benchmarks, showing significant gains over the open-source Irish baseline (UCCIX).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph McInerney', 'Khanh-Tung Tran', 'Liam Lonergan', "Ailbhe N\\'i Chasaide", "Neasa N\\'i Chiar\\'ain", 'Barry Devereux']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-data-synthesis', 'instruction-tuning', 'low-resource-languages', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17652</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying LLM Biases Across Instruction Boundary in Mixed Question Forms</title><link>https://arxiv.org/abs/2509.20278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the notion of Instruction Boundary to study how different instruction settings bias LLM annotations when datasets contain mixed question forms (Sparse Labels).&lt;/li&gt;&lt;li&gt;Proposes BiasDetector, a diagnostic benchmark to systematically evaluate LLMs' ability to identify sparse/exceptional label forms across mixed question formats (e.g., MCQ, True/False, Unknown).&lt;/li&gt;&lt;li&gt;Empirical results show user instructions can induce large annotation biases, highlighting risks for dataset quality and downstream model behavior.&lt;/li&gt;&lt;li&gt;Provides code, datasets, and implementations to reproduce the benchmark and analyses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zipeng Ling', 'Shuliang Liu', 'Yuehao Tang', 'Chen Huang', 'Gaoyang Jiang', 'Shenghong Fu', 'Junqi Yang', 'Yao Wan', 'Jiawan Zhang', 'Kejia Huang', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM bias', 'instruction-induced bias', 'dataset annotation', 'safety evaluation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20278</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</title><link>https://arxiv.org/abs/2508.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DH-CoT, an integrated black-box jailbreak attack that combines adversarial context alignment, harmful-prompt-based few-shot examples (NTP), and a fake chain-of-thought to elicit malicious outputs from SOTA reasoning models.&lt;/li&gt;&lt;li&gt;Introduces MDH, a pipeline combining LLM-based annotation with human assistance to clean existing red-teaming datasets and produce the RTA dataset suite for more accurate evaluation of jailbreak effectiveness.&lt;/li&gt;&lt;li&gt;Shows empirical success of DH-CoT against commercial models (e.g., GPT-5, Claude-4), outperforming prior methods like H-CoT and TAP and highlighting shortcomings in current evaluation datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Zhang', 'Lu Zhou', 'Xiaogang Xu', 'Jiafei Wu', 'Liming Fang', 'Zhe Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'dataset/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10390</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LAG: Logic-Augmented Generation from a Cartesian Perspective</title><link>https://arxiv.org/abs/2508.05509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Logic-Augmented Generation (LAG): decompose complex questions into logically ordered atomic sub-questions and resolve them sequentially.&lt;/li&gt;&lt;li&gt;Maintains an atomic memory bank and uses prior sub-answer context to guide retrieval for subsequent sub-questions (logic-aware retrieval).&lt;/li&gt;&lt;li&gt;Aims to reduce hallucinations and improve accuracy in knowledge-intensive tasks compared to standard retrieval-augmented methods.&lt;/li&gt;&lt;li&gt;Evaluated on four benchmarks, reporting significant accuracy improvements and reduced hallucination rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Xiao', 'Chuang Zhou', 'Yujing Zhang', 'Qinggang Zhang', 'Su Dong', 'Shengyuan Chen', 'Chang Yang', 'Xiao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination reduction', 'retrieval-augmented generation (RAG)', 'reasoning/decomposition', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05509</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Pitfalls of Evaluating Language Models with Open Benchmarks</title><link>https://arxiv.org/abs/2507.00460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that open LLM benchmarks are vulnerable to data leakage by fine-tuning smaller models directly on public test sets, yielding inflated leaderboard performance that fails to generalize.&lt;/li&gt;&lt;li&gt;Constructs and evaluates cheating models (BART, T5, GPT-2 variants) fine-tuned on benchmark test data to quantify the severity of manipulation.&lt;/li&gt;&lt;li&gt;Proposes and assesses simple paraphrase-based safeguarding strategies to mitigate leakage, discussing their effectiveness and limitations.&lt;/li&gt;&lt;li&gt;Argues for complementing open static benchmarks with private or dynamically generated tests and reexamining benchmarking practices to preserve evaluation integrity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Najib Hasan (Wichita State University)', 'Md Mahadi Hassan Sibat (University of Central Florida)', 'Mohammad Fakhruddin Babar (Washington State University)', 'Souvika Sarkar (Wichita State University)', 'Monowar Hasan (Washington State University)', 'Santu Karmaker (University of Central Florida)']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark poisoning', 'data leakage', 'evaluation robustness', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00460</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict</title><link>https://arxiv.org/abs/2506.06485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a diagnostic framework that holds underlying knowledge constant while creating controlled conflicts between context and models' parametric memory across tasks with differing knowledge demands.&lt;/li&gt;&lt;li&gt;Finds that performance under context-memory conflict depends on task-specific reliance on context vs memory and on the plausibility of the conflicting context.&lt;/li&gt;&lt;li&gt;Shows that interventions like providing rationales or reiterating context shift models toward greater context reliance—helpful for context-only tasks but harmful for tasks needing parametric knowledge—and that these interventions bias model-based evaluation.&lt;/li&gt;&lt;li&gt;Argues for task-aware approaches to balancing context and memory in deployment and evaluation because conflict effects are task-dependent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiser Sun', 'Fan Bai', 'Mark Dredze']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'context-memory conflict', 'evaluation bias', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06485</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective</title><link>https://arxiv.org/abs/2505.20707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Physbench: 3,162 high-school/AP physics questions with structured reference solutions and 2,700 culturally contextualized paired variants; uses P-REFS, a stage-wise rubric, to evaluate reasoning.&lt;/li&gt;&lt;li&gt;Evaluates 10 small language models across ~58,000 responses and finds that 75–98% of solutions with correct final answers still contain at least one reasoning error; failure modes shift by model capability (interpretation/modeling vs execution).&lt;/li&gt;&lt;li&gt;Shows paired contextual variations have little effect on top models but degrade mid-tier models, arguing that safe educational AI requires evaluation of reasoning fidelity rather than only final-answer correctness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicy Scaria', 'Silvester John Joseph Kennedy', 'Krishna Agarwal', 'Diksha Seth', 'Deepak Subramani']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarking', 'reasoning-fidelity', 'educational-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20707</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent</title><link>https://arxiv.org/abs/2505.20118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrojanStego: a threat model where an adversary fine-tunes an LLM to embed sensitive context into natural outputs via linguistic steganography without needing control over inputs.&lt;/li&gt;&lt;li&gt;Proposes a practical encoding scheme using vocabulary partitioning learnable by LLMs and provides a taxonomy of risk factors for compromised models.&lt;/li&gt;&lt;li&gt;Empirical results: reliably transmit 32-bit secrets with 87% accuracy on held-out prompts and &gt;97% accuracy using majority voting across generations, while preserving utility and evading human detection.&lt;/li&gt;&lt;li&gt;Highlights a new, practical class of passive, covert LLM data-exfiltration attacks (trojan/backdoor-like behavior) with privacy and security implications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Meier', 'Jan Philip Wahle', 'Paul R\\"ottger', 'Terry Ruas', 'Bela Gipp']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'data exfiltration', 'trojan/backdoor', 'linguistic steganography', 'model fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20118</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PAM: Training Policy-Aligned Moderation Filters at Scale</title><link>https://arxiv.org/abs/2505.19766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PAM, a framework to train moderation filters aligned to user-defined policies (beyond conventional safety) using automated synthetic data generation without human-written examples.&lt;/li&gt;&lt;li&gt;Demonstrates PAM-trained filters match or exceed state-of-the-art safety moderation and policy-reasoning models, outperforming them on PAMbench (four user-annotated policy enforcement benchmarks covering age, dietary, cultural, and medical constraints).&lt;/li&gt;&lt;li&gt;Shows substantial inference speed advantages (5–100x faster) over policy-conditioned reasoning models, enabling scalable, practical deployment of policy-aligned safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masoomali Fatehkia', 'Enes Altinisik', 'Mohamed Osman', 'Husrev Taha Sencar']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Moderation filters', 'Alignment', 'Jailbreak defenses', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19766</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2505.17118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs Trustworthiness Response Dataset (TRD) with 36,266 questions across four RAG settings to analyze conflicts between parametric and retrieved knowledge.&lt;/li&gt;&lt;li&gt;Identifies shortcomings of existing approaches that either prioritize one source, naively merge sources, or refuse answers, lacking a unified strategy.&lt;/li&gt;&lt;li&gt;Proposes BRIDGE: an adaptive soft-bias weighting for knowledge collection and a Maximum Soft-bias Decision Tree to decide whether to trust internal knowledge, external retrieval, or refuse; reports 5–15% accuracy gains over baselines while balancing scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinbang Dai', 'Huikang Hu', 'Yuncheng Hua', 'Jiaqi Li', 'Yongrui Chen', 'Rihui Jin', 'Nan Hu', 'Guilin Qi']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation (RAG)', 'LLM Safety', 'Robustness', 'Trustworthiness', 'Refusal Mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17118</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities</title><link>https://arxiv.org/abs/2505.15722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study of memorization in multilingual LLMs covering 95 languages across various model scales and architectures.&lt;/li&gt;&lt;li&gt;Introduces a graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization patterns.&lt;/li&gt;&lt;li&gt;Finds that among similar languages, those with fewer training tokens exhibit higher memorization, a pattern revealed only when modeling cross-lingual relationships.&lt;/li&gt;&lt;li&gt;Argues for a language-aware perspective for evaluating and mitigating memorization vulnerabilities, with implications for privacy and cross-lingual transfer.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Luo', 'Yiyi Chen', 'Johannes Bjerva', 'Qiongxiu Li']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data privacy', 'multilingual LLMs', 'analysis', 'cross-lingual transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15722</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation</title><link>https://arxiv.org/abs/2502.17899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces \ourdata, a dataset of 1,308 test cases based on psychological frameworks (D/S-IAT, Negative Automatic Thinking) and real-world scenarios for evaluating LLM behavior on suicide-related content.&lt;/li&gt;&lt;li&gt;Proposes an evaluation framework focused on two tasks: Identification of Implicit Suicidal ideation (IIS) and Provision of Appropriate Supportive responses (PAS).&lt;/li&gt;&lt;li&gt;Empirically evaluates 8 widely used LLMs under different contextual settings and finds they often fail to detect implicit suicidal ideation and to provide appropriate supportive responses.&lt;/li&gt;&lt;li&gt;Concludes current LLMs have significant limitations for deployment in sensitive mental-health contexts and calls for more sophisticated development and evaluation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Li', 'Shu Yang', 'Junchao Wu', 'Jiyao Wei', 'Lijie Hu', 'Mengdi Li', 'Derek F. Wong', 'Joshua R. Oltmanns', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'mental-health', 'LLM benchmarking', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17899</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</title><link>https://arxiv.org/abs/2601.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode called "textual inertia" where once a textual hallucination appears in chain-of-thought, LMMs typically ignore conflicting visual evidence and propagate the error.&lt;/li&gt;&lt;li&gt;Introduces the LogicGraph Perturbation Protocol to structurally inject perturbations into reasoning chains across diverse LMMs to measure self-reflection and correction capability.&lt;/li&gt;&lt;li&gt;Finds that models self-correct in under 10% of cases and largely succumb to blind textual error propagation.&lt;/li&gt;&lt;li&gt;Proposes Active Visual-Context Refinement, a training-free inference method that actively re-grounds visual context and adaptively refines reasoning history to reduce hallucination propagation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhu', 'Jiafeng Liang', 'Shixin Jiang', 'Jinlan Fu', 'Ming Liu', 'Guanglu Sun', 'See-Kiong Ng', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'hallucination', 'robustness', 'chain-of-thought', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04073</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable Language Guidance for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2601.04052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'modality collapse' where strong visual priors overwhelm sparse linguistic signals, causing agents to ignore semantic intent.&lt;/li&gt;&lt;li&gt;Proposes Residual Semantic Steering (RSS) with Monte Carlo Syntactic Integration (LLM-driven semantic posterior expansion) and Residual Affordance Steering (subtracting visual affordance prior).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing RSS increases mutual information between action and intent while suppressing visual distractors.&lt;/li&gt;&lt;li&gt;Empirical results report state-of-the-art robustness on manipulation benchmarks, maintaining performance under adversarial linguistic perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhan', 'Yuhao Chen', 'Jiaying Zhou', 'Qinhan Lv', 'Hao Liu', 'Keze Wang', 'Liang Lin', 'Guangrun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial prompting', 'vision-language-action', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04052</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2601.03979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of privacy risks specific to Retrieval-Augmented Generation (RAG) systems, synthesizing literature on attacks and defenses.&lt;/li&gt;&lt;li&gt;Presents a taxonomy of RAG privacy risks, a privacy process diagram, and a review of mitigation techniques and evaluation strategies.&lt;/li&gt;&lt;li&gt;Analyzes maturity and limitations of proposed mitigations and highlights considerations for measuring and reducing data leakage from knowledge bases used with LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Systematization (SoK)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andreea-Elena Bodea', 'Stephen Meisenbacher', 'Alexandra Klymenko', 'Florian Matthes']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'retrieval-augmented generation', 'data leakage', 'mitigations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03979</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Current Agents Fail to Leverage World Model as Tool for Foresight</title><link>https://arxiv.org/abs/2601.03905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of vision-language agents using generative world models as external simulators to anticipate future states across agentic and visual question-answering tasks.&lt;/li&gt;&lt;li&gt;Key findings: agents rarely invoke simulation (&lt;1%), commonly misuse predicted rollouts (~15%), and can show inconsistent or degraded performance (up to ~5%) when simulation is available or enforced.&lt;/li&gt;&lt;li&gt;Attribution analysis indicates primary bottlenecks are deciding when to simulate, interpreting predicted outcomes, and integrating foresight into downstream reasoning.&lt;/li&gt;&lt;li&gt;Conclusion: improving calibrated, strategic interaction with world models is necessary to achieve reliable anticipatory cognition in future agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Qian', 'Emre Can Acikgoz', 'Bingxuan Li', 'Xiusi Chen', 'Yuji Zhang', 'Bingxiang He', 'Qinyu Luo', 'Dilek Hakkani-T\\"ur', 'Gokhan Tur', 'Yunzhu Li', 'Heng Ji', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'world-models', 'agent-foresight']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03905</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules</title><link>https://arxiv.org/abs/2601.03537</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STAR-S, a self-taught loop that elicits safety-rule-guided reasoning and uses the generated reasoning data to fine-tune LLMs iteratively to improve safety alignment.&lt;/li&gt;&lt;li&gt;Aims specifically to defend against jailbreak attacks by improving models' reasoning and interpretation of safety rules through repeated generation, reflection, and fine-tuning cycles.&lt;/li&gt;&lt;li&gt;Reports experimental improvements over baselines in resisting jailbreaks and provides code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Di Wu', 'Yanyan Zhao', 'Xin Lu', 'Mingzhe Li', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'safety alignment', 'self-taught reasoning', 'fine-tuning', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03537</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Archaeology: The Causal Topology of Model Evolution</title><link>https://arxiv.org/abs/2601.03424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free mechanistic probe using attention-graph spectra (algebraic connectivity λ2, smoothness, spectral entropy) to produce stable spectral fingerprints across models and languages.&lt;/li&gt;&lt;li&gt;Identifies a failure mode called Passive-Triggered Connectivity Collapse (PTCC) tied to curriculum transitions (e.g., code-to-chat) that causes connectivity loss on non-canonical syntactic constructions and localizes to a sparse Layer 2 patch of heads.&lt;/li&gt;&lt;li&gt;Demonstrates specialization trade-offs, four recurrent processing strategies enabling forensic lineage identification, and that activation steering can partially restore ~38% of lost information flow.&lt;/li&gt;&lt;li&gt;Finds topological regimes correlate with tokenization density more than language identity and proposes spectra as a practical auditing/training-regime verification tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'robustness', 'model-auditing', 'mechanistic-analysis', 'training-regimes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03424</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Real is Your Jailbreak? Fine-grained Jailbreak Evaluation with Anchored Reference</title><link>https://arxiv.org/abs/2601.03288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FJAR, a fine-grained jailbreak evaluation framework that classifies model responses into five categories (Rejective, Irrelevant, Unhelpful, Incorrect, Successful) instead of a coarse safe/unsafe binary.&lt;/li&gt;&lt;li&gt;Introduces a harmless tree decomposition method to create anchored references from original malicious queries, enabling more accurate judgment of whether a response fulfills the malicious intent.&lt;/li&gt;&lt;li&gt;Shows that FJAR aligns better with human judgments and helps identify root causes of jailbreak failures, offering actionable guidance to improve attack strategies and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songyang Liu', 'Chaozhuo Li', 'Rui Pu', 'Litian Zhang', 'Chenxu Wang', 'Zejian Chen', 'Yuting Zhang', 'Yiming Hei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak evaluation', 'Red teaming', 'Safety evaluation', 'Fine-grained classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03288</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperCLOVA X 32B Think</title><link>https://arxiv.org/abs/2601.03286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HyperCLOVA X 32B Think, a vision-language model optimized for reasoning and agentic capabilities in the Korean linguistic/cultural context.&lt;/li&gt;&lt;li&gt;Pre-trained with emphasis on reasoning, then post-trained for multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences.&lt;/li&gt;&lt;li&gt;Evaluated on Korean text-to-text and vision-to-text benchmarks and on agent-oriented evaluation tasks; model is being open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NAVER Cloud HyperCLOVA X Team']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multimodal', 'agentic-behavior', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03286</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection</title><link>https://arxiv.org/abs/2601.04160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RFC Bench, a paragraph-level benchmark for evaluating LLMs on financial misinformation under realistic news contexts.&lt;/li&gt;&lt;li&gt;Defines two tasks: reference-free misinformation detection and comparison-based diagnosis using paired original vs. perturbed inputs.&lt;/li&gt;&lt;li&gt;Finds models perform substantially better with comparative context; reference-free settings expose unstable predictions and many invalid outputs.&lt;/li&gt;&lt;li&gt;Positions the benchmark as a testbed to study reference-free reasoning and improve reliability of LLMs for real-world financial misinformation detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuechen Jiang', 'Zhiwei Liu', 'Yupeng Cao', 'Yueru He', 'Ziyang Xu', 'Chen Xu', 'Zhiyang Deng', 'Prayag Tiwari', 'Xi Chen', 'Alejandro Lopez-Lira', 'Jimin Huang', 'Junichi Tsujii', 'Sophia Ananiadou']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation-detection', 'benchmark', 'robustness', 'safety-evaluation', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04160</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models</title><link>https://arxiv.org/abs/2601.04131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ContextFocus, an activation-steering method that improves LLM faithfulness to externally retrieved context when that context conflicts with the model's parametric knowledge.&lt;/li&gt;&lt;li&gt;Requires no finetuning and adds minimal inference-time overhead, making it lightweight and efficient.&lt;/li&gt;&lt;li&gt;Benchmarked on the ConFiQA dataset against baselines (ContextDPO, COIECD, prompting methods) and shown to be complementary to prompting and effective on larger models.&lt;/li&gt;&lt;li&gt;Reports significant improvements in contextual faithfulness while preserving fluency and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikhil Anand', 'Shwetha Somasundaram', 'Anirudh Phukan', 'Apoorv Saxena', 'Koyel Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'contextual faithfulness', 'activation steering', 'robustness', 'inference-time methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04131</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks</title><link>https://arxiv.org/abs/2601.04093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SearchAttack, a red-teaming method that leverages web search as an attack surface for search-augmented LLMs by outsourcing harmful content to retrieved pages and steering model reconstruction via structural rubrics.&lt;/li&gt;&lt;li&gt;Identifies the core vulnerability that search results can contain ready-to-use harmful semantics which bypass the LLM's internal safeguards once presented as retrieved content.&lt;/li&gt;&lt;li&gt;Presents extensive experiments demonstrating that SearchAttack effectively induces unsafe outputs from search-augmented LLMs, highlighting risks and the need for responsibility-focused vulnerability assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Mingfeng Li', 'Zheming Yang', 'Chiwei Zhu', 'Fei Ma', 'Benfeng Xu', 'Min Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Search-augmented LLM attacks', 'Retrieval-based attacks', 'Prompt injection / jailbreak', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04093</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures</title><link>https://arxiv.org/abs/2601.04086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a distillation-based framework that embeds a programmable module (executable code) in reasoning prompts to guide knowledge graph exploration and external structured knowledge use.&lt;/li&gt;&lt;li&gt;Explicitly regulates intermediate reasoning steps (chain-style distillation) to reduce prompt-induced hallucinations and improve interpretability.&lt;/li&gt;&lt;li&gt;Evaluated on public benchmarks with GPT-4 and LLaMA-3.3, reporting substantial HIT@1/3/5 improvements (≈13–16%) and &gt;95% scores in several settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinbo Hao', 'Kai Yang', 'Qingzhen Su', 'Yifan Li', 'Chao Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'prompt engineering', 'alignment/safety', 'reasoning interpretability', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04086</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life</title><link>https://arxiv.org/abs/2601.04043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SaLAD, a multimodal safety benchmark of 2,013 real-world image-text samples across 10 everyday categories, balancing unsafe scenarios and oversensitivity cases.&lt;/li&gt;&lt;li&gt;Design ensures risks require fine-grained cross-modal reasoning and cannot be inferred from text alone, emphasizing realistic visual risk exposure.&lt;/li&gt;&lt;li&gt;Proposes a safety-warning-based evaluation framework that rewards clear, informative warnings rather than generic refusals.&lt;/li&gt;&lt;li&gt;Evaluates 18 MLLMs, finding top models only 57.2% safe response rate and showing common alignment methods have limited effectiveness in these scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyue Lou', 'Jinan Xu', 'Jingyi Yin', 'Xiaolong Wang', 'Zhaolu Kang', 'Youwei Liao', 'Yixuan Wang', 'Xiangyu Shi', 'Fengran Mo', 'Su Yao', 'Kaiyu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM safety', 'safety benchmark', 'safety evaluation', 'alignment vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04043</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection</title><link>https://arxiv.org/abs/2601.03981</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RADAR: a retrieval-augmented fake news detector that verifies claims via dense passage retrieval.&lt;/li&gt;&lt;li&gt;Uses an adversarial generator that rewrites real articles with factual perturbations to produce evasion examples.&lt;/li&gt;&lt;li&gt;Introduces Verbal Adversarial Feedback (VAF): structured natural-language critiques guiding the generator to more sophisticated attacks, enabling co-evolution of attacker and detector.&lt;/li&gt;&lt;li&gt;Reports strong empirical gains (86.98% ROC-AUC) and ablations showing detector-side retrieval, VAF, and few-shot demonstrations are key.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song-Duo Ma', 'Yi-Hung Liu', 'Hsin-Yu Lin', 'Pin-Yu Chen', 'Hong-Yan Huang', 'Shau-Yung Hsu', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'misinformation detection', 'red teaming', 'retrieval-augmented models', 'adversarial feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03981</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.03926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Doc-PP, a benchmark for evaluating whether Large Vision-Language Models (LVLMs) preserve user-defined non-disclosure policies when answering multimodal document questions.&lt;/li&gt;&lt;li&gt;Identifies a Reasoning-Induced Safety Gap: models often leak sensitive information when required to synthesize or aggregate information across textual and visual modalities.&lt;/li&gt;&lt;li&gt;Finds that providing extracted text improves perception but can increase leakage risk, and proposes DVA (Decompose-Verify-Aggregation) to separate reasoning from policy verification.&lt;/li&gt;&lt;li&gt;Shows DVA substantially outperforms standard prompting-based defenses, offering a baseline defense and evaluation suite for policy-compliant document understanding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haeun Jang', 'Hwan Chang', 'Hwanhee Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'privacy', 'benchmarking', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03926</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>What Matters For Safety Alignment?</title><link>https://arxiv.org/abs/2601.03868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study evaluating safety alignment across 32 recent LLMs/LRMs (3B–235B) using five safety datasets, 56 jailbreak techniques, and four chain-of-thought (CoT) attack strategies.&lt;/li&gt;&lt;li&gt;Finds integrated reasoning and self-reflection mechanisms improve safety; post-training/distillation can degrade safety unless explicitly constrained.&lt;/li&gt;&lt;li&gt;Identifies a major vulnerability where CoT attacks via response prefixes massively increase attack success rates, and highlights roleplay, prompt injection, and gradient-based adversarial prompt search as primary elicitation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xing Li', 'Hui-Ling Zhen', 'Lihao Yin', 'Xianzhi Yu', 'Zhenhua Dong', 'Mingxuan Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03868</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI Generated Text Detection</title><link>https://arxiv.org/abs/2601.03812</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Builds a unified benchmark for AI-generated text detection using HC3 and DAIGT v2 with a topic-based data split to prevent information leakage and improve generalization.&lt;/li&gt;&lt;li&gt;Evaluates traditional (TF-IDF + logistic regression) and deep models (BiLSTM, DistilBERT); TF-IDF baseline accuracy 82.87%, BiLSTM 88.86%, DistilBERT 88.11% with highest ROC-AUC of 0.96.&lt;/li&gt;&lt;li&gt;Finds contextual semantic models outperform lexical features and emphasizes mitigating topic memorization through evaluation protocol choices; notes dataset diversity and compute constraints as limitations and proposes future work (LoRA, distilled/smaller models).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adilkhan Alikhanov', 'Aidar Amangeldi', 'Diar Demeubay', 'Dilnaz Akhmetzhan', 'Nurbek Moldakhmetov', 'Omar Polat', 'Galymzhan Zharas']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'benchmarking', 'defensive security', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03812</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework</title><link>https://arxiv.org/abs/2601.03791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cue-Resistant Memorization (CRM), a cue-controlled evaluation framework to distinguish true memorization from cue-driven reconstruction in LLMs.&lt;/li&gt;&lt;li&gt;Conducts a large-scale multilingual (32 languages) re-evaluation of PII leakage across multiple memorization paradigms (prefix-suffix completion, associative reconstruction, cue-free generation, membership inference).&lt;/li&gt;&lt;li&gt;Finds that much reported PII leakage is attributable to surface-form lexical cues in prompts; when cues are controlled, reconstruction success and membership inference true positive rates drop substantially.&lt;/li&gt;&lt;li&gt;Argues for adopting cue-controlled evaluations to more reliably quantify privacy-relevant memorization in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Luo', 'Yiyi Chen', 'Qiongxiu Li', 'Johannes Bjerva']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'memorization', 'privacy attacks', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03791</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HearSay Benchmark: Do Audio LLMs Leak What They Hear?</title><link>https://arxiv.org/abs/2601.03783</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HearSay, a benchmark of &gt;22,000 real-world audio clips to evaluate privacy leakage from Audio LLMs (ALLMs).&lt;/li&gt;&lt;li&gt;Finds ALLMs can accurately infer private attributes (e.g., 92.89% accuracy for gender) and profile social attributes from voiceprints.&lt;/li&gt;&lt;li&gt;Shows current safety mechanisms largely fail (near-zero refusal rates for physiological trait queries), and Chain-of-Thought reasoning increases leakage risk.&lt;/li&gt;&lt;li&gt;Provides dataset and code for further research and targeted privacy alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Wang', 'Liang Lin', 'Kaiwen Luo', 'Weiliu Wang', 'Yitian Chen', 'Moayad Aloqaily', 'Xuehai Tang', 'Zhenhong Zhou', 'Kun Wang', 'Li Sun', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'model safety', 'benchmarking', 'audio LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03783</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations</title><link>https://arxiv.org/abs/2601.03775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates whether LLM self-explanations (chain-of-thought or post-hoc rationales) help humans and LLM judges predict the model's answers to counterfactual follow-up questions on StrategyQA.&lt;/li&gt;&lt;li&gt;Compares two perturbation/test-case construction strategies: LLM-generated counterfactuals versus pragmatics-based (pragmatic) perturbations, to see how explanation utility depends on how counterfactuals are formed.&lt;/li&gt;&lt;li&gt;Finds that access to self-explanations consistently improves simulation accuracy for both humans and LLM judges, but the magnitude and stability of improvements vary with perturbation strategy and judge strength; includes qualitative analysis of human justifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pingjun Hong', 'Benjamin Roth']&lt;/li&gt;&lt;li&gt;Tags: ['Interpretability/Explainability', 'Counterfactual Simulatability', 'Human-AI Evaluation', 'Safety/Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03775</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms</title><link>https://arxiv.org/abs/2601.03752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of LLM personalized text generation across 10 languages, examining both misuse (personalized disinformation) and potential benefits.&lt;/li&gt;&lt;li&gt;Generates 17,280 texts using 16 models across 1,080 personalization prompt combinations targeting demographic groups and social-media platforms.&lt;/li&gt;&lt;li&gt;Finds that personalization quality varies by language and that platform-targeted personalization more strongly reduces detectability, especially in English where personalization is strongest.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Macko']&lt;/li&gt;&lt;li&gt;Tags: ['disinformation', 'misuse/abuse', 'detectability', 'multilingual', 'personalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03752</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Whose Facts Win? LLM Source Preferences under Knowledge Conflicts</title><link>https://arxiv.org/abs/2601.03746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how 13 open-weight LLMs resolve conflicts between competing retrieved facts, focusing on the role of the information source (e.g., government, newspapers, people, social media).&lt;/li&gt;&lt;li&gt;Finds consistent preference for institutionally corroborated sources, but shows that simple repetition of less-credible sources can flip those preferences.&lt;/li&gt;&lt;li&gt;Proposes and validates a mitigation method that greatly reduces repetition-induced bias while preserving most original source preferences; releases data and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakob Schuster', 'Vagrant Gautam', 'Katja Markert']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'credibility', 'safety-evaluation', 'bias-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03746</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</title><link>https://arxiv.org/abs/2601.03714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates DeepSeek-OCR's true OCR capability by applying sentence- and word-level semantic corruption to separate vision performance from language priors.&lt;/li&gt;&lt;li&gt;Finds performance drops from ~90% to ~20% without linguistic support, and shows end-to-end methods are less robust than traditional pipeline OCR under semantic perturbations.&lt;/li&gt;&lt;li&gt;Shows lower visual token counts increase reliance on language priors and hallucination risk, and reports model collapse around 10,000 text tokens under context stress testing.&lt;/li&gt;&lt;li&gt;Provides comparative benchmarking against 13 baselines and releases datasets and scripts for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Liang', 'Ruixuan Ying', 'Bo Li', 'Hong Li', 'Kai Yan', 'Qingwen Li', 'Min Yang', 'Okamoto Satoshi', 'Zhe Cui', 'Shiwen Ni']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination', 'OCR', 'vision-text compression', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03714</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models</title><link>https://arxiv.org/abs/2601.03699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedBench, a unified dataset aggregating 37 prior benchmarks with 29,362 samples for red teaming large language models.&lt;/li&gt;&lt;li&gt;Defines a standardized taxonomy of 22 risk categories and 19 domains to enable consistent vulnerability evaluations.&lt;/li&gt;&lt;li&gt;Provides analysis of existing datasets, baselines on modern LLMs, and open-sourced data and evaluation code for reproducible benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quy-Anh Dang', 'Chris Ngo', 'Truong-Son Hy']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'adversarial prompts', 'safety-evaluation', 'dataset', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03699</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DisastQA: A Comprehensive Benchmark for Evaluating Question Answering in Disaster Management</title><link>https://arxiv.org/abs/2601.03670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DisastQA, a 3,000-question benchmark (2,000 multiple-choice, 1,000 open-ended) across eight disaster types to evaluate QA under uncertain/conflicting evidence.&lt;/li&gt;&lt;li&gt;Evaluates models across evidence conditions (closed-book to noisy evidence) to separate internal knowledge from reasoning under imperfect information; proposes a human-verified keypoint-based protocol for open-ended QA.&lt;/li&gt;&lt;li&gt;Experiments on 20 models show that while some open-weight models match proprietary systems in clean settings, performance drops sharply with realistic noise, revealing reliability gaps for disaster response.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhitong Chen', 'Kai Yin', 'Xiangjue Dong', 'Chengkai Liu', 'Xiangpeng Li', 'Yiming Xiao', 'Bo Li', 'Junwei Ma', 'Ali Mostafavi', 'James Caverlee']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarking', 'question-answering', 'disaster-response']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03670</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>eTracer: Towards Traceable Text Generation via Claim-Level Grounding</title><link>https://arxiv.org/abs/2601.03669</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces eTracer, a plug-and-play post-hoc claim-level grounding framework that aligns individual claims in generated responses with contextual evidence and labels support/contradiction.&lt;/li&gt;&lt;li&gt;Aims to improve traceability and quantify response faithfulness to enable verifiability and trustworthiness, targeting high-stakes domains (biomedical).&lt;/li&gt;&lt;li&gt;Demonstrates improved grounding quality and user verification efficiency compared to conventional sentence-level grounding; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bohao Chu', 'Qianli Wang', 'Hendrik Damm', 'Hui Wang', 'Ula Muhabbek', 'Elisabeth Livingstone', 'Christoph M. Friedrich', 'Norbert Fuhr']&lt;/li&gt;&lt;li&gt;Tags: ['traceability', 'faithfulness', 'grounding', 'verification', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03669</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases</title><link>https://arxiv.org/abs/2601.03630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares Large Reasoning Models (LRMs) vs non-reasoning LLMs as judgment/evaluation models and finds LRMs achieve higher judgment accuracy, especially on reasoning-intensive tasks.&lt;/li&gt;&lt;li&gt;LRMs show better instruction-following in evaluation settings and greater robustness to adversarial attacks targeting judgment tasks.&lt;/li&gt;&lt;li&gt;LRMs nevertheless retain substantial superficial biases; the paper proposes PlanJudge, a prompt-based strategy that requires an explicit evaluation plan before scoring to mitigate biases.&lt;/li&gt;&lt;li&gt;Experiments show PlanJudge reduces biases in both LRMs and standard LLMs, improving reliability of model-based evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Huang', 'Xuanxin Wu', 'Muyun Yang', 'Yuki Arase']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-attacks', 'bias-mitigation', 'evaluation-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03630</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation</title><link>https://arxiv.org/abs/2601.03615</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a forensic auditing framework for evaluating robustness of Audio Language Models' (ALMs) explicit reasoning under adversarial attacks across acoustic perception, cognitive coherence, and cognitive dissonance.&lt;/li&gt;&lt;li&gt;Finds a bifurcation: reasoning can act as a defensive 'shield' for models with robust acoustic perception, but can impose a 'tax' (reduced performance) for others, especially under linguistic attacks.&lt;/li&gt;&lt;li&gt;Shows that even when classification fails, high cognitive dissonance in reasoning traces can serve as a silent alarm indicating potential manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binh Nguyen', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'adversarial attacks', 'model robustness', 'explainability', 'forensic auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03615</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier</title><link>https://arxiv.org/abs/2601.03605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiVA, a hybrid Agentic Discriminative Verifier that combines generative agentic search with discriminative scoring for fine-grained factuality verification.&lt;/li&gt;&lt;li&gt;Introduces FGVeriBench, a new benchmark for fine-grained factuality evaluation covering general and multi-hop questions.&lt;/li&gt;&lt;li&gt;Reports that DiVA outperforms existing methods on FGVeriBench, improving granular assessment of error severity beyond binary judgments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Huang', 'Muyun Yang', 'Yuki Arase']&lt;/li&gt;&lt;li&gt;Tags: ['factuality verification', 'LLM evaluation', 'benchmarking', 'alignment', 'hybrid verifier']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03605</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OLA: Output Language Alignment in Code-Switched LLM Interactions</title><link>https://arxiv.org/abs/2601.03589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OLA, a benchmark evaluating output-language alignment for code-switched LLM interactions (focused on Korean-English, with additional tests on Chinese and Indonesian).&lt;/li&gt;&lt;li&gt;Finds many current LLMs misalign with users' implicit language expectations, showing bias toward non-English responses, mid-response switching, and language intrusions.&lt;/li&gt;&lt;li&gt;Chain-of-Thought prompting does not fix these pragmatic failures, but Code-Switching Aware DPO with ~1K examples substantially reduces misalignment.&lt;/li&gt;&lt;li&gt;Concludes failures stem from insufficient alignment data/practice rather than fundamental model limitations, and highlights need to align multilingual LLM outputs to implicit user expectations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juhyun Oh', 'Haneul Yoo', 'Faiz Ghifari Haznitrama', 'Alice Oh']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multilingual-LLMs', 'benchmarking', 'robustness', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03589</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics</title><link>https://arxiv.org/abs/2601.03578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PsychEthicsBench, a jurisdiction-aware benchmark grounded in Australian psychology and psychiatry guidelines to evaluate ethical behavior of LLMs in mental health contexts.&lt;/li&gt;&lt;li&gt;Evaluates 14 models using multiple-choice and open-ended tasks with fine-grained ethicality annotations, showing that refusal rates are a poor proxy for clinical ethicality.&lt;/li&gt;&lt;li&gt;Finds that domain-specific fine-tuning can degrade ethical robustness, with several specialized models underperforming their base backbones on ethical alignment.&lt;/li&gt;&lt;li&gt;Advocates for principled, context-sensitive evaluation beyond refusal-centric metrics to better assess safety and professional appropriateness in mental health applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaling Shen', 'Stephanie Fong', 'Yiwen Jiang', 'Zimu Wang', 'Feilong Tang', 'Qingyang Xu', 'Xiangyu Zhao', 'Zhongxing Xu', 'Jiahe Liu', 'Jinpeng Hu', 'Dominic Dwyer', 'Zongyuan Ge']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarks', 'healthcare safety', 'ethical robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03578</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs</title><link>https://arxiv.org/abs/2601.03559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffCoT, a diffusion-styled Chain-of-Thought (CoT) framework that reformulates multi-step reasoning as an iterative denoising process using a sliding-window over reasoning steps.&lt;/li&gt;&lt;li&gt;Introduces a causal diffusion noise schedule to preserve temporal/causal structure of reasoning chains while enabling retrospective correction of intermediate steps without breaking token-level autoregression.&lt;/li&gt;&lt;li&gt;Reports consistent improvements in robustness and error-correction on three multi-step CoT reasoning benchmarks across multiple model backbones compared to existing CoT optimization methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shidong Cao', 'Hongzhan Lin', 'Yuxuan Gu', 'Ziyang Luo', 'Jing Ma']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'chain-of-thought', 'LLMs', 'reasoning', 'model-correction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03559</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios</title><link>https://arxiv.org/abs/2601.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAS (Police Action Scenarios), a systematic evaluation framework tailored to police decision-making for assessing LLM outputs in high-stakes law-enforcement scenarios.&lt;/li&gt;&lt;li&gt;Constructs a novel QA dataset derived from over 8,000 official police documents and defines metrics validated against police expert judgments.&lt;/li&gt;&lt;li&gt;Benchmarks commercial LLMs on police-related tasks, finding they struggle particularly with providing accurate, fact-based recommendations; releases dataset and prompt templates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangyub Lee', 'Heedou Kim', 'Hyeoncheol Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'high-stakes-deployment', 'LLM-risk', 'law-enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03553</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict</title><link>https://arxiv.org/abs/2601.03546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a context-based assessment protocol that sequentially measures LLM privacy attitudes, prosocialness, and data-sharing acceptance within a single session to preserve history-dependent behavior.&lt;/li&gt;&lt;li&gt;Uses multi-group structural equation modeling (MGSEM) to link privacy concerns and prosocialness to actual data-sharing actions and introduces Value-Action Alignment Rate (VAAR) to quantify directional alignment with human-referenced expectations.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs, finding model-specific privacy/prosocial profiles and substantial heterogeneity in how stated values predict downstream sharing behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Chen', 'Chenxiao Yu', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy', 'benchmarking', 'behavioral evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03546</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation</title><link>https://arxiv.org/abs/2601.03511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntroLM, a method that enables causal LMs to predict their own output quality during the prefilling phase using an 'introspective token' and token-conditional LoRA, without changing generation behavior.&lt;/li&gt;&lt;li&gt;Learns to predict success/failure for a given query, achieving high predictive performance (e.g., ~90% ROC AUC on QA when applied to Qwen3 8B) and outperforming external classifiers.&lt;/li&gt;&lt;li&gt;Demonstrates practical gains when used for multi-model routing—reducing latency and large-model usage while maintaining reliability—thus improving cost/reliability tradeoffs.&lt;/li&gt;&lt;li&gt;Avoids external evaluators by having the backbone model self-evaluate, which can be used for reliability, calibration, and routing decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hossein Hosseini Kasnavieh', 'Gholamreza Haffari', 'Chris Leckie', 'Adel N. Toosi']&lt;/li&gt;&lt;li&gt;Tags: ['self-evaluation', 'safety-evaluation', 'model-routing', 'calibration', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03511</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale</title><link>https://arxiv.org/abs/2601.03444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares human and LLM ratings across three grading scales (including 0–5) on six benchmarks spanning objective, subjective, and mixed tasks.&lt;/li&gt;&lt;li&gt;Finds LLM judgments vary across grading scales on subjective benchmarks and that the 0–5 scale yields the strongest human–LLM alignment aggregated over tasks.&lt;/li&gt;&lt;li&gt;Shows pooled reliability metrics can mask benchmark heterogeneity and uncovers systematic subgroup differences (e.g., by gender) in alignment.&lt;/li&gt;&lt;li&gt;Argues scale design and sub-level diagnostics are important for robust LLM-as-a-judge evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiyue Li', 'Minda Zhao', 'Weixuan Dong', 'Jiahui Cai', 'Yuze Wei', 'Michael Pocress', 'Yi Li', 'Wanyan Yuan', 'Xiaoyue Wang', 'Ruoyu Hou', 'Kaiyuan Lou', 'Wenqi Zeng', 'Yutong Yang', 'Yilun Du', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'evaluation', 'LLM-as-judge', 'reliability', 'fairness/subgroup-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03444</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rendering Data Unlearnable by Exploiting LLM Alignment Mechanisms</title><link>https://arxiv.org/abs/2601.03401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Disclaimer Injection, a data-level defense that injects alignment-triggering disclaimers into text to make it unlearnable by LLMs in a black-box setting.&lt;/li&gt;&lt;li&gt;Shows via layer-wise analysis that fine-tuning on protected data causes persistent activation of alignment-related layers, which suppresses task learning and degrades model performance.&lt;/li&gt;&lt;li&gt;Demonstrates a practical method for restricting data learnability at LLM scale without needing access to or modification of the training pipeline.&lt;/li&gt;&lt;li&gt;Frames alignment behaviour as a lever for data protection and privacy, offering a novel defensive mechanism against unauthorized model learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruihan Zhang', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['data protection', 'unlearnable examples', 'LLM alignment', 'data poisoning', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03401</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Breaking the Assistant Mold: Modeling Behavioral Variation in LLM Based Procedural Character Generation</title><link>https://arxiv.org/abs/2601.03396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies two alignment-induced biases in LLM-based character generation: a positive moral bias (characters default to agreeable moral stances) and a helpful assistant bias (characters always answer directly).&lt;/li&gt;&lt;li&gt;Introduces PersonaWeaver, a framework that disentangles world-building (roles, demographics) from behavioral-building (moral stances, interaction styles) to produce more diverse character behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates increased diversity in moral positions, refusal/deflection behaviors, and stylistic markers (length, tone, punctuation) compared to assistant-fine-tuned baselines.&lt;/li&gt;&lt;li&gt;Provides code for reproducing the system and experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maan Qraitem', 'Kate Saenko', 'Bryan A. Plummer']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM behavior', 'assistant fine-tuning', 'procedural content generation', 'behavioral diversity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03396</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2601.03388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates a causal link between metaphors in training data and increased cross-domain misalignment in LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that interventions at pre-training, fine-tuning, and re-alignment stages significantly alter models' misalignment degrees.&lt;/li&gt;&lt;li&gt;Finds correlations between metaphors and activation patterns of global/local latent features and builds a detector that predicts misaligned content with high accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Hu', 'Chen Wang', 'Yanfeng Shu', 'Hye-young Paik', 'Liming Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM safety', 'misalignment', 'latent feature analysis', 'dataset influence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03388</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators</title><link>https://arxiv.org/abs/2601.03273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardEval, a multi-perspective benchmark with 106 fine-grained categories covering emotions, offensive/hateful language, gender and racial bias, jailbreak prompts, and broader safety concerns.&lt;/li&gt;&lt;li&gt;Proposes GemmaGuard (GGuard), a QLoRA fine-tuned Gemma3-12B on GuardEval, achieving a macro F1 of 0.832 and outperforming commercial and open moderation models.&lt;/li&gt;&lt;li&gt;Shows that diverse, human-centered training/evaluation data can improve moderation safety, fairness, and robustness on nuanced and borderline cases; dataset intended for both training and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naseem Machlovi', 'Maryam Saleki', 'Ruhul Amin', 'Mohamed Rahouti', 'Shawqi Al-Maliki', 'Junaid Qadir', 'Mohamed M. Abdallah', 'Ala Al-Fuqaha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM moderation', 'Safety evaluation', 'Jailbreaking', 'Bias/fairness', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03273</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Instruction Gap: LLMs get lost in Following Instruction</title><link>https://arxiv.org/abs/2601.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates 13 leading LLMs on instruction compliance, response accuracy, and performance in enterprise RAG scenarios.&lt;/li&gt;&lt;li&gt;Identifies an 'instruction gap' where models often fail to reliably follow custom instructions needed for deployments.&lt;/li&gt;&lt;li&gt;Provides comparative benchmarks showing wide variation across models (e.g., Claude-Sonnet-4 and GPT-5 perform best).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishesh Tripathi', 'Uday Allu', 'Biddwan Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'safety-evaluation', 'benchmarking', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03269</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OpenAI GPT-5 System Card</title><link>https://arxiv.org/abs/2601.03267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;System card for OpenAI GPT-5 describing architecture (fast main model, deeper reasoning model, and a real-time router that selects models based on conversation characteristics).&lt;/li&gt;&lt;li&gt;Discusses safety engineering measures: 'safe-completions' training to block disallowed content, classification of gpt-5-thinking as High capability for Biological and Chemical domains under a Preparedness Framework, and associated safeguards.&lt;/li&gt;&lt;li&gt;Describes continuous router training on user signals, fallback mini-models when limits are reached, and reported improvements in hallucinations, instruction following, and real-world usefulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other (System Card)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaditya Singh (Tony)', 'Adam Fry (Tony)', 'Adam Perelman (Tony)', 'Adam Tart (Tony)', 'Adi Ganesh (Tony)', 'Ahmed El-Kishky (Tony)', 'Aidan McLaughlin (Tony)', 'Aiden Low (Tony)', 'AJ Ostrow (Tony)', 'Akhila Ananthram (Tony)', 'Akshay Nathan (Tony)', 'Alan Luo (Tony)', 'Alec Helyar (Tony)', 'Aleksander Madry (Tony)', 'Aleksandr Efremov (Tony)', 'Aleksandra Spyra (Tony)', 'Alex Baker-Whitcomb (Tony)', 'Alex Beutel (Tony)', 'Alex Karpenko (Tony)', 'Alex Makelov (Tony)', 'Alex Neitz (Tony)', 'Alex Wei (Tony)', 'Alexandra Barr (Tony)', 'Alexandre Kirchmeyer (Tony)', 'Alexey Ivanov (Tony)', 'Alexi Christakis (Tony)', 'Alistair Gillespie (Tony)', 'Allison Tam (Tony)', 'Ally Bennett (Tony)', 'Alvin Wan (Tony)', 'Alyssa Huang (Tony)', 'Amy McDonald Sandjideh (Tony)', 'Amy Yang (Tony)', 'Ananya Kumar (Tony)', 'Andre Saraiva (Tony)', 'Andrea Vallone (Tony)', 'Andrei Gheorghe (Tony)', 'Andres Garcia Garcia (Tony)', 'Andrew Braunstein (Tony)', 'Andrew Liu (Tony)', 'Andrew Schmidt (Tony)', 'Andrey Mereskin (Tony)', 'Andrey Mishchenko (Tony)', 'Andy Applebaum (Tony)', 'Andy Rogerson (Tony)', 'Ann Rajan (Tony)', 'Annie Wei (Tony)', 'Anoop Kotha (Tony)', 'Anubha Srivastava (Tony)', 'Anushree Agrawal (Tony)', 'Arun Vijayvergiya (Tony)', 'Ashley Tyra (Tony)', 'Ashvin Nair (Tony)', 'Avi Nayak (Tony)', 'Ben Eggers (Tony)', 'Bessie Ji (Tony)', 'Beth Hoover (Tony)', 'Bill Chen (Tony)', 'Blair Chen (Tony)', 'Boaz Barak (Tony)', 'Borys Minaiev (Tony)', 'Botao Hao (Tony)', 'Bowen Baker (Tony)', 'Brad Lightcap (Tony)', 'Brandon McKinzie (Tony)', 'Brandon Wang (Tony)', 'Brendan Quinn (Tony)', 'Brian Fioca (Tony)', 'Brian Hsu (Tony)', 'Brian Yang (Tony)', 'Brian Yu (Tony)', 'Brian Zhang (Tony)', 'Brittany Brenner (Tony)', 'Callie Riggins Zetino (Tony)', 'Cameron Raymond (Tony)', 'Camillo Lugaresi (Tony)', 'Carolina Paz (Tony)', 'Cary Hudson (Tony)', 'Cedric Whitney (Tony)', 'Chak Li (Tony)', 'Charles Chen (Tony)', 'Charlotte Cole (Tony)', 'Chelsea Voss (Tony)', 'Chen Ding (Tony)', 'Chen Shen (Tony)', 'Chengdu Huang (Tony)', 'Chris Colby (Tony)', 'Chris Hallacy (Tony)', 'Chris Koch (Tony)', 'Chris Lu (Tony)', 'Christina Kaplan (Tony)', 'Christina Kim (Tony)', 'CJ Minott-Henriques (Tony)', 'Cliff Frey (Tony)', 'Cody Yu (Tony)', 'Coley Czarnecki (Tony)', 'Colin Reid (Tony)', 'Colin Wei (Tony)', 'Cory Decareaux (Tony)', 'Cristina Scheau (Tony)', 'Cyril Zhang (Tony)', 'Cyrus Forbes (Tony)', 'Da Tang (Tony)', 'Dakota Goldberg (Tony)', 'Dan Roberts (Tony)', 'Dana Palmie (Tony)', 'Daniel Kappler (Tony)', 'Daniel Levine (Tony)', 'Daniel Wright (Tony)', 'Dave Leo (Tony)', 'David Lin (Tony)', 'David Robinson (Tony)', 'Declan Grabb (Tony)', 'Derek Chen (Tony)', 'Derek Lim (Tony)', 'Derek Salama (Tony)', 'Dibya Bhattacharjee (Tony)', 'Dimitris Tsipras (Tony)', 'Dinghua Li (Tony)', 'Dingli Yu (Tony)', 'DJ Strouse (Tony)', 'Drew Williams (Tony)', 'Dylan Hunn (Tony)', 'Ed Bayes (Tony)', 'Edwin Arbus (Tony)', 'Ekin Akyurek (Tony)', 'Elaine Ya Le (Tony)', 'Elana Widmann (Tony)', 'Eli Yani (Tony)', 'Elizabeth Proehl (Tony)', 'Enis Sert (Tony)', 'Enoch Cheung (Tony)', 'Eri Schwartz (Tony)', 'Eric Han (Tony)', 'Eric Jiang (Tony)', 'Eric Mitchell (Tony)', 'Eric Sigler (Tony)', 'Eric Wallace (Tony)', 'Erik Ritter (Tony)', 'Erin Kavanaugh (Tony)', 'Evan Mays (Tony)', 'Evgenii Nikishin (Tony)', 'Fangyuan Li (Tony)', 'Felipe Petroski Such (Tony)', 'Filipe de Avila Belbute Peres (Tony)', 'Filippo Raso (Tony)', 'Florent Bekerman (Tony)', 'Foivos Tsimpourlas (Tony)', 'Fotis Chantzis (Tony)', 'Francis Song (Tony)', 'Francis Zhang (Tony)', 'Gaby Raila (Tony)', 'Garrett McGrath (Tony)', 'Gary Briggs (Tony)', 'Gary Yang (Tony)', 'Giambattista Parascandolo (Tony)', 'Gildas Chabot (Tony)', 'Grace Kim (Tony)', 'Grace Zhao (Tony)', 'Gregory Valiant (Tony)', 'Guillaume Leclerc (Tony)', 'Hadi Salman (Tony)', 'Hanson Wang (Tony)', 'Hao Sheng (Tony)', 'Haoming Jiang (Tony)', 'Haoyu Wang (Tony)', 'Haozhun Jin (Tony)', 'Harshit Sikchi (Tony)', 'Heather Schmidt (Tony)', 'Henry Aspegren (Tony)', 'Honglin Chen (Tony)', 'Huida Qiu (Tony)', 'Hunter Lightman (Tony)', 'Ian Covert (Tony)', 'Ian Kivlichan (Tony)', 'Ian Silber (Tony)', 'Ian Sohl (Tony)', 'Ibrahim Hammoud (Tony)', 'Ignasi Clavera (Tony)', 'Ikai Lan (Tony)', 'Ilge Akkaya (Tony)', 'Ilya Kostrikov (Tony)', 'Irina Kofman (Tony)', 'Isak Etinger (Tony)', 'Ishaan Singal (Tony)', 'Jackie Hehir (Tony)', 'Jacob Huh (Tony)', 'Jacqueline Pan (Tony)', 'Jake Wilczynski (Tony)', 'Jakub Pachocki (Tony)', 'James Lee (Tony)', 'James Quinn (Tony)', 'Jamie Kiros (Tony)', 'Janvi Kalra (Tony)', 'Jasmyn Samaroo (Tony)', 'Jason Wang (Tony)', 'Jason Wolfe (Tony)', 'Jay Chen (Tony)', 'Jay Wang (Tony)', 'Jean Harb (Tony)', 'Jeffrey Han (Tony)', 'Jeffrey Wang (Tony)', 'Jennifer Zhao (Tony)', 'Jeremy Chen (Tony)', 'Jerene Yang (Tony)', 'Jerry Tworek (Tony)', 'Jesse Chand (Tony)', 'Jessica Landon (Tony)', 'Jessica Liang (Tony)', 'Ji Lin (Tony)', 'Jiancheng Liu (Tony)', 'Jianfeng Wang (Tony)', 'Jie Tang (Tony)', 'Jihan Yin (Tony)', 'Joanne Jang (Tony)', 'Joel Morris (Tony)', 'Joey Flynn (Tony)', 'Johannes Ferstad (Tony)', 'Johannes Heidecke (Tony)', 'John Fishbein (Tony)', 'John Hallman (Tony)', 'Jonah Grant (Tony)', 'Jonathan Chien (Tony)', 'Jonathan Gordon (Tony)', 'Jongsoo Park (Tony)', 'Jordan Liss (Tony)', 'Jos Kraaijeveld (Tony)', 'Joseph Guay (Tony)', 'Joseph Mo (Tony)', 'Josh Lawson (Tony)', 'Josh McGrath (Tony)', 'Joshua Vendrow (Tony)', 'Joy Jiao (Tony)', 'Julian Lee (Tony)', 'Julie Steele (Tony)', 'Julie Wang (Tony)', 'Junhua Mao (Tony)', 'Kai Chen (Tony)', 'Kai Hayashi (Tony)', 'Kai Xiao (Tony)', 'Kamyar Salahi (Tony)', 'Kan Wu (Tony)', 'Karan Sekhri (Tony)', 'Karan Sharma (Tony)', 'Karan Singhal (Tony)', 'Karen Li (Tony)', 'Kenny Nguyen (Tony)', 'Keren Gu-Lemberg (Tony)', 'Kevin King (Tony)', 'Kevin Liu (Tony)', 'Kevin Stone (Tony)', 'Kevin Yu (Tony)', 'Kristen Ying (Tony)', 'Kristian Georgiev (Tony)', 'Kristie Lim (Tony)', 'Kushal Tirumala (Tony)', 'Kyle Miller (Tony)', 'Lama Ahmad (Tony)', 'Larry Lv (Tony)', 'Laura Clare (Tony)', 'Laurance Fauconnet (Tony)', 'Lauren Itow (Tony)', 'Lauren Yang (Tony)', 'Laurentia Romaniuk (Tony)', 'Leah Anise (Tony)', 'Lee Byron (Tony)', 'Leher Pathak (Tony)', 'Leon Maksin (Tony)', 'Leyan Lo (Tony)', 'Leyton Ho (Tony)', 'Li Jing (Tony)', 'Liang Wu (Tony)', 'Liang Xiong (Tony)', 'Lien Mamitsuka (Tony)', 'Lin Yang (Tony)', 'Lindsay McCallum (Tony)', 'Lindsey Held (Tony)', 'Liz Bourgeois (Tony)', 'Logan Engstrom (Tony)', 'Lorenz Kuhn (Tony)', 'Louis Feuvrier (Tony)', 'Lu Zhang (Tony)', 'Lucas Switzer (Tony)', 'Lukas Kondraciuk (Tony)', 'Lukasz Kaiser (Tony)', 'Manas Joglekar (Tony)', 'Mandeep Singh (Tony)', 'Mandip Shah (Tony)', 'Manuka Stratta (Tony)', 'Marcus Williams (Tony)', 'Mark Chen (Tony)', 'Mark Sun (Tony)', 'Marselus Cayton (Tony)', 'Martin Li (Tony)', 'Marvin Zhang (Tony)', 'Marwan Aljubeh (Tony)', 'Matt Nichols (Tony)', 'Matthew Haines (Tony)', 'Max Schwarzer (Tony)', 'Mayank Gupta (Tony)', 'Meghan Shah (Tony)', 'Melody Huang (Tony)', 'Meng Dong (Tony)', 'Mengqing Wang (Tony)', 'Mia Glaese (Tony)', 'Micah Carroll (Tony)', 'Michael Lampe (Tony)', 'Michael Malek (Tony)', 'Michael Sharman (Tony)', 'Michael Zhang (Tony)', 'Michele Wang (Tony)', 'Michelle Pokrass (Tony)', 'Mihai Florian (Tony)', 'Mikhail Pavlov (Tony)', 'Miles Wang (Tony)', 'Ming Chen (Tony)', 'Mingxuan Wang (Tony)', 'Minnia Feng (Tony)', 'Mo Bavarian (Tony)', 'Molly Lin (Tony)', 'Moose Abdool (Tony)', 'Mostafa Rohaninejad (Tony)', 'Nacho Soto (Tony)', 'Natalie Staudacher (Tony)', 'Natan LaFontaine (Tony)', 'Nathan Marwell (Tony)', 'Nelson Liu (Tony)', 'Nick Preston (Tony)', 'Nick Turley (Tony)', 'Nicklas Ansman (Tony)', 'Nicole Blades (Tony)', 'Nikil Pancha (Tony)', 'Nikita Mikhaylin (Tony)', 'Niko Felix (Tony)', 'Nikunj Handa (Tony)', 'Nishant Rai (Tony)', 'Nitish Keskar (Tony)', 'Noam Brown (Tony)', 'Ofir Nachum (Tony)', 'Oleg Boiko (Tony)', 'Oleg Murk (Tony)', 'Olivia Watkins (Tony)', 'Oona Gleeson (Tony)', 'Pamela Mishkin (Tony)', 'Patryk Lesiewicz (Tony)', 'Paul Baltescu (Tony)', 'Pavel Belov (Tony)', 'Peter Zhokhov (Tony)', 'Philip Pronin (Tony)', 'Phillip Guo (Tony)', 'Phoebe Thacker (Tony)', 'Qi Liu (Tony)', 'Qiming Yuan (Tony)', 'Qinghua Liu (Tony)', 'Rachel Dias (Tony)', 'Rachel Puckett (Tony)', 'Rahul Arora (Tony)', 'Ravi Teja Mullapudi (Tony)', 'Raz Gaon (Tony)', 'Reah Miyara (Tony)', 'Rennie Song (Tony)', 'Rishabh Aggarwal (Tony)', 'RJ Marsan (Tony)', 'Robel Yemiru (Tony)', 'Robert Xiong (Tony)', 'Rohan Kshirsagar (Tony)', 'Rohan Nuttall (Tony)', 'Roman Tsiupa (Tony)', 'Ronen Eldan (Tony)', 'Rose Wang (Tony)', 'Roshan James (Tony)', 'Roy Ziv (Tony)', 'Rui Shu (Tony)', 'Ruslan Nigmatullin (Tony)', 'Saachi Jain (Tony)', 'Saam Talaie (Tony)', 'Sam Altman (Tony)', 'Sam Arnesen (Tony)', 'Sam Toizer (Tony)', 'Sam Toyer (Tony)', 'Samuel Miserendino (Tony)', 'Sandhini Agarwal (Tony)', 'Sarah Yoo (Tony)', 'Savannah Heon (Tony)', 'Scott Ethersmith (Tony)', 'Sean Grove (Tony)', 'Sean Taylor (Tony)', 'Sebastien Bubeck (Tony)', 'Sever Banesiu (Tony)', 'Shaokyi Amdo (Tony)', 'Shengjia Zhao (Tony)', 'Sherwin Wu (Tony)', 'Shibani Santurkar (Tony)', 'Shiyu Zhao (Tony)', 'Shraman Ray Chaudhuri (Tony)', 'Shreyas Krishnaswamy (Tony)', 'Shuaiqi (Tony)', 'Xia', 'Shuyang Cheng', 'Shyamal Anadkat', "Sim\\'on Posada Fishman", 'Simon Tobin', 'Siyuan Fu', 'Somay Jain', 'Song Mei', 'Sonya Egoian', 'Spencer Kim', 'Spug Golden', 'SQ Mah', 'Steph Lin', 'Stephen Imm', 'Steve Sharpe', 'Steve Yadlowsky', 'Sulman Choudhry', 'Sungwon Eum', 'Suvansh Sanjeev', 'Tabarak Khan', 'Tal Stramer', 'Tao Wang', 'Tao Xin', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Degry', 'Thomas Shadwell', 'Tianfu Fu', 'Tianshi Gao', 'Timur Garipov', 'Tina Sriskandarajah', 'Toki Sherbakov', 'Tomer Kaftan', 'Tomo Hiratsuka', 'Tongzhou Wang', 'Tony Song', 'Tony Zhao', 'Troy Peterson', 'Val Kharitonov', 'Victoria Chernova', 'Vineet Kosaraju', 'Vishal Kuo', 'Vitchyr Pong', 'Vivek Verma', 'Vlad Petrov', 'Wanning Jiang', 'Weixing Zhang', 'Wenda Zhou', 'Wenlei Xie', 'Wenting Zhan', 'Wes McCabe', 'Will DePue', 'Will Ellsworth', 'Wulfie Bain', 'Wyatt Thompson', 'Xiangning Chen', 'Xiangyu Qi', 'Xin Xiang', 'Xinwei Shi', 'Yann Dubois', 'Yaodong Yu', 'Yara Khakbaz', 'Yifan Wu', 'Yilei Qian', 'Yin Tat Lee', 'Yinbo Chen', 'Yizhen Zhang', 'Yizhong Xiong', 'Yonglong Tian', 'Young Cha', 'Yu Bai', 'Yu Yang', 'Yuan Yuan', 'Yuanzhi Li', 'Yufeng Zhang', 'Yuguang Yang', 'Yujia Jin', 'Yun Jiang', 'Yunyun Wang', 'Yushi Wang', 'Yutian Liu', 'Zach Stubenvoll', 'Zehao Dou', 'Zheng Wu', 'Zhigang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Safety engineering', 'Preparedness framework', 'System card']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03267</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models</title><link>https://arxiv.org/abs/2601.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Jailbreak-Zero, a policy-based red teaming method that uses an attack LLM to generate large volumes of adversarial prompts and is fine-tuned with preference data.&lt;/li&gt;&lt;li&gt;Claims Pareto improvements across policy coverage, attack strategy diversity, and prompt fidelity to real user inputs compared to example-based approaches.&lt;/li&gt;&lt;li&gt;Reports higher attack success rates against open-source and proprietary LLMs (e.g., GPT-4o, Claude 3.5) with minimal human intervention, improving scalability of safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Hu', 'Abhinav Aggarwal', 'Mehran Khodabandeh', 'David Zhang', 'Eric Hsin', 'Li Chen', 'Ankit Jain', 'Matt Fredrikson', 'Akash Bharadwaj']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03265</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2601.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation (CAP-GSM8K, N=500) comparing internal reasoning (Chain-of-Thought) versus an external control mechanism (RCA) across GPT-3.5, GPT-4o, and GPT-5.1 on sycophancy.&lt;/li&gt;&lt;li&gt;Finds internal reasoning can cause performance collapse in weaker models (Prioritization Paradox) and leaves an 11.4% residual sycophancy in frontier models, while RCA reportedly eliminates sycophancy (0.0%) across model tiers.&lt;/li&gt;&lt;li&gt;Proposes a 'thermodynamic hierarchy' framing (Resonance, Dissonance, Entropy) to describe hybrid system behavior and argues external structural constraints are strictly necessary for safety guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Y. Chang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'sycophancy / alignment', 'external control / constraints', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03263</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL, a framework that defines interpretable conversational traits from contrastive dialogue data and combines them into a scalar reward for aligning LLMs to human-likeness.&lt;/li&gt;&lt;li&gt;Uses the reward signal with standard preference optimization to align models of varying sizes without degrading overall performance, validated via large-scale human evaluations showing increased perceived human-likeness.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and inspectability of alignment by operating over explicit traits, enabling diagnosis of unintended effects and extending alignment to qualitative language properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference optimization', 'interpretability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Representational Contrastive Scoring (RCS) to detect multimodal jailbreaks by inspecting LVLM internal representations and learning a lightweight projection to separate benign vs malicious inputs.&lt;/li&gt;&lt;li&gt;Implements two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute contrastive scores from safety-critical layers.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art generalization to unseen attack types on a challenging evaluation protocol while remaining computationally lightweight and interpretable.&lt;/li&gt;&lt;li&gt;Focuses on reducing over-rejection common in one-class anomaly detectors by leveraging internal geometric signals rather than surface-level features.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'LLM/LLMV red teaming', 'anomaly detection', 'internal representations', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EngTrace: A Symbolic Benchmark for Verifiable Process Supervision of Engineering Reasoning</title><link>https://arxiv.org/abs/2511.01650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EngTrace, a symbolic benchmark (1,350 contamination-resistant test cases from 90 templates) for verifiable engineering reasoning across multiple branches and domains.&lt;/li&gt;&lt;li&gt;Proposes a two-stage, verifiable evaluation that checks intermediate reasoning traces as well as final answers using automated procedural checks and an AI Tribunal.&lt;/li&gt;&lt;li&gt;Evaluates 24 LLMs and finds a trade-off between numeric precision and trace fidelity, plus a 'complexity cliff' where pretraining fails to support integrative, physically grounded engineering reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayesha Gull', 'Muhammad Usman Safder', 'Rania Elbadry', 'Fan Zhang', 'Veselin Stoyanov', 'Preslav Nakov', 'Zhuohan Xie']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'verification', 'LLM-evaluation', 'engineering-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01650</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models</title><link>https://arxiv.org/abs/2502.20408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts methods from functional brain network analysis to identify recurring 'functional networks' of neurons within large language models.&lt;/li&gt;&lt;li&gt;Demonstrates that inhibiting identified networks markedly degrades model performance, while amplifying neurons in these networks can improve overall or task-specific performance.&lt;/li&gt;&lt;li&gt;Provides empirical evidence and code showing these networks map to specific tasks or global capabilities, suggesting avenues for targeted intervention and interpretation of LLM behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiheng Liu', 'Zhengliang Liu', 'Zihao Wu', 'Junhao Ning', 'Haiyang Sun', 'Sichen Xia', 'Yang Yang', 'Xiaohui Gao', 'Ning Qiang', 'Bao Ge', 'Tianming Liu', 'Junwei Han', 'Xintao Hu']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-editing', 'alignment', 'robustness', 'LLM internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20408</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Low Resource Reconstruction Attacks Through Benign Prompts</title><link>https://arxiv.org/abs/2507.07947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a low-resource reconstruction attack on generative diffusion models that uses seemingly benign text prompts to recover memorized training images or parts thereof.&lt;/li&gt;&lt;li&gt;Requires little to no access to training data and demonstrates that untrained users can accidentally trigger sensitive reconstructions (e.g., generating a real person's face from 'blue Unisex T-Shirt').&lt;/li&gt;&lt;li&gt;Attributes vulnerability to templated scraped e-commerce data where consistent layouts and image-text patterns enable prompt-based memorization exploitation.&lt;/li&gt;&lt;li&gt;Provides empirical results and public code to reproduce the attack (link included).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sol Yarkoni', 'Mahmood Sharif', 'Roi Livni']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'reconstruction/memorization', 'prompt-based attack', 'diffusion models', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07947</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin</title><link>https://arxiv.org/abs/2506.08473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'narrow safety basin' in parameter space: updates orthogonal to the alignment direction (difference between aligned and unaligned model weights) rapidly degrade safety, while updates along the alignment direction largely preserve it.&lt;/li&gt;&lt;li&gt;Proposes AsFT (Anchoring Safety in Fine-Tuning), which penalizes weight updates orthogonal to the alignment direction to keep the model within the safety basin during fine-tuning.&lt;/li&gt;&lt;li&gt;Reports empirical gains: up to 7.60% reduction in harmful behaviors and 3.44% improvement in task performance, outperforming existing methods across multiple datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Yang', 'Qihui Zhang', 'Yuyang Liu', 'Xiaojun Jia', 'Kunpeng Ning', 'Jiayu Yao', 'Jigang Wang', 'Hailiang Dai', 'Yibing Song', 'Li Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'alignment', 'robustness', 'safety-preserving training', 'adversarial fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08473</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models</title><link>https://arxiv.org/abs/2601.04163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of 14 pathology foundation models (PFMs) to scanner-induced domain shifts using a multiscanner breast cancer WSI dataset (384 slides, 5 scanners).&lt;/li&gt;&lt;li&gt;Finds most PFMs encode pronounced scanner-specific variability in embeddings; while AUC often remains stable, embedding shifts degrade calibration and introduce scanner-dependent bias in downstream predictions.&lt;/li&gt;&lt;li&gt;Robustness does not correlate simply with training data scale, model size, or recency; vision-language models show some robustness advantage in embeddings but underperform on supervised tasks.&lt;/li&gt;&lt;li&gt;Recommends moving beyond accuracy-centric benchmarks toward explicit evaluation and optimization of embedding stability and calibration under realistic acquisition variability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Thiringer', 'Fredrik K. Gustafsson', 'Kajsa Ledesma Eriksson', 'Mattias Rantalainen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'domain shift', 'medical imaging', 'calibration', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04163</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models</title><link>https://arxiv.org/abs/2601.04131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ContextFocus, an activation-steering method that improves LLM faithfulness to externally retrieved context when that context conflicts with the model's parametric knowledge.&lt;/li&gt;&lt;li&gt;Requires no finetuning and adds minimal inference-time overhead, making it lightweight and efficient.&lt;/li&gt;&lt;li&gt;Benchmarked on the ConFiQA dataset against baselines (ContextDPO, COIECD, prompting methods) and shown to be complementary to prompting and effective on larger models.&lt;/li&gt;&lt;li&gt;Reports significant improvements in contextual faithfulness while preserving fluency and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikhil Anand', 'Shwetha Somasundaram', 'Anirudh Phukan', 'Apoorv Saxena', 'Koyel Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'contextual faithfulness', 'activation steering', 'robustness', 'inference-time methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04131</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Current Agents Fail to Leverage World Model as Tool for Foresight</title><link>https://arxiv.org/abs/2601.03905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of vision-language agents using generative world models as external simulators to anticipate future states across agentic and visual question-answering tasks.&lt;/li&gt;&lt;li&gt;Key findings: agents rarely invoke simulation (&lt;1%), commonly misuse predicted rollouts (~15%), and can show inconsistent or degraded performance (up to ~5%) when simulation is available or enforced.&lt;/li&gt;&lt;li&gt;Attribution analysis indicates primary bottlenecks are deciding when to simulate, interpreting predicted outcomes, and integrating foresight into downstream reasoning.&lt;/li&gt;&lt;li&gt;Conclusion: improving calibrated, strategic interaction with world models is necessary to achieve reliable anticipatory cognition in future agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Qian', 'Emre Can Acikgoz', 'Bingxuan Li', 'Xiusi Chen', 'Yuji Zhang', 'Bingxiang He', 'Qinyu Luo', 'Dilek Hakkani-T\\"ur', 'Gokhan Tur', 'Yunzhu Li', 'Heng Ji', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'world-models', 'agent-foresight']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03905</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict</title><link>https://arxiv.org/abs/2601.03546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a context-based assessment protocol that sequentially measures LLM privacy attitudes, prosocialness, and data-sharing acceptance within a single session to preserve history-dependent behavior.&lt;/li&gt;&lt;li&gt;Uses multi-group structural equation modeling (MGSEM) to link privacy concerns and prosocialness to actual data-sharing actions and introduces Value-Action Alignment Rate (VAAR) to quantify directional alignment with human-referenced expectations.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs, finding model-specific privacy/prosocial profiles and substantial heterogeneity in how stated values predict downstream sharing behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Chen', 'Chenxiao Yu', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy', 'benchmarking', 'behavioral evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03546</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation</title><link>https://arxiv.org/abs/2601.03511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntroLM, a method that enables causal LMs to predict their own output quality during the prefilling phase using an 'introspective token' and token-conditional LoRA, without changing generation behavior.&lt;/li&gt;&lt;li&gt;Learns to predict success/failure for a given query, achieving high predictive performance (e.g., ~90% ROC AUC on QA when applied to Qwen3 8B) and outperforming external classifiers.&lt;/li&gt;&lt;li&gt;Demonstrates practical gains when used for multi-model routing—reducing latency and large-model usage while maintaining reliability—thus improving cost/reliability tradeoffs.&lt;/li&gt;&lt;li&gt;Avoids external evaluators by having the backbone model self-evaluate, which can be used for reliability, calibration, and routing decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hossein Hosseini Kasnavieh', 'Gholamreza Haffari', 'Chris Leckie', 'Adel N. Toosi']&lt;/li&gt;&lt;li&gt;Tags: ['self-evaluation', 'safety-evaluation', 'model-routing', 'calibration', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03511</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</title><link>https://arxiv.org/abs/2601.03470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a maturity-based framework to certify embodied AI systems via explicit measurement mechanisms and quantitative scoring.&lt;/li&gt;&lt;li&gt;Argues for structured assessment frameworks that handle multi-objective trade-offs in evaluating trustworthiness.&lt;/li&gt;&lt;li&gt;Demonstrates the approach using uncertainty quantification as an exemplar measurement mechanism.&lt;/li&gt;&lt;li&gt;Illustrates feasibility through a Uncrewed Aircraft System (UAS) detection case study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael C. Darling', 'Alan H. Hesu', 'Michael A. Mardikes', 'Brian C. McGuigan', 'Reed M. Milewicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'certification', 'uncertainty-quantification', 'embodied-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03470</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Microeconomic Foundations of Multi-Agent Learning</title><link>https://arxiv.org/abs/2601.03451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops an economic foundation for multi-agent learning via a principal-agent interaction in an MDP with strategic externalities.&lt;/li&gt;&lt;li&gt;Proposes a two-phase incentive mechanism that estimates implementable transfers then uses them to steer long-run dynamics to improve social welfare.&lt;/li&gt;&lt;li&gt;Proves that under regret-based rationality and exploration conditions the mechanism attains sublinear social-welfare regret (asymptotically optimal welfare).&lt;/li&gt;&lt;li&gt;Simulations show coarse incentives can correct inefficient learning under stateful externalities, motivating incentive-aware design for welfare-aligned AI in markets/insurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nassim Helou']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent learning', 'mechanism design', 'AI alignment', 'incentive design', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03451</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage</title><link>https://arxiv.org/abs/2601.03429</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeepLeak, a system to audit and mitigate membership inference leakage from post-hoc explanation methods.&lt;/li&gt;&lt;li&gt;Develops a stronger explanation-aware membership inference attack and profiles leakage across 15 explanation techniques on image benchmarks.&lt;/li&gt;&lt;li&gt;Proposes lightweight, model-agnostic mitigations (sensitivity-calibrated noise, attribution clipping, masking) that substantially reduce leakage with minimal utility loss.&lt;/li&gt;&lt;li&gt;Performs root-cause analysis linking properties like attribution sparsity and sensitivity to leakage risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Firas Ben Hmida', 'Zain Sbeih', 'Philemon Hailemariam', 'Birhanu Eshete']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'model explanations', 'privacy defenses', 'adversarial evaluation', 'interpretability security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03429</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2601.03388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates a causal link between metaphors in training data and increased cross-domain misalignment in LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that interventions at pre-training, fine-tuning, and re-alignment stages significantly alter models' misalignment degrees.&lt;/li&gt;&lt;li&gt;Finds correlations between metaphors and activation patterns of global/local latent features and builds a detector that predicts misaligned content with high accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Hu', 'Chen Wang', 'Yanfeng Shu', 'Hye-young Paik', 'Liming Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM safety', 'misalignment', 'latent feature analysis', 'dataset influence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03388</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering</title><link>https://arxiv.org/abs/2601.03300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRYLOCK, a defense-in-depth architecture combining four heterogeneous mechanisms across the inference stack: DPO weight-level alignment, Representation Engineering (activation-level steering), an adaptive sidecar classifier to select steering strength, and input canonicalization to neutralize encoding-based bypasses.&lt;/li&gt;&lt;li&gt;Evaluated on Mistral-7B-Instruct with a 249-prompt attack set across five attack families: achieves 88.0% relative reduction in attack success rate (from 46.5% to 5.6%), with layerwise contributions (RepE blocks 36% of attacks bypassing DPO; canonicalization catches 14% of encoding attacks).&lt;/li&gt;&lt;li&gt;Identifies a non-monotonic steering effect where intermediate steering strength can worsen safety and hypothesizes mechanistic interference between RepE and DPO; adaptive sidecar reduces over-refusal from 60% to 48% while preserving defense performance.&lt;/li&gt;&lt;li&gt;Artifacts and evaluation methodology are released for reproducibility (adapters, steering vectors, sidecar classifier, preference pairs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Scott Thornton']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaks', 'defense-in-depth', 'representation engineering', 'alignment/steering', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03300</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperCLOVA X 32B Think</title><link>https://arxiv.org/abs/2601.03286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HyperCLOVA X 32B Think, a vision-language model optimized for reasoning and agentic capabilities in the Korean linguistic/cultural context.&lt;/li&gt;&lt;li&gt;Pre-trained with emphasis on reasoning, then post-trained for multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences.&lt;/li&gt;&lt;li&gt;Evaluated on Korean text-to-text and vision-to-text benchmarks and on agent-oriented evaluation tasks; model is being open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NAVER Cloud HyperCLOVA X Team']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multimodal', 'agentic-behavior', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03286</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators</title><link>https://arxiv.org/abs/2601.03273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardEval, a multi-perspective benchmark with 106 fine-grained categories covering emotions, offensive/hateful language, gender and racial bias, jailbreak prompts, and broader safety concerns.&lt;/li&gt;&lt;li&gt;Proposes GemmaGuard (GGuard), a QLoRA fine-tuned Gemma3-12B on GuardEval, achieving a macro F1 of 0.832 and outperforming commercial and open moderation models.&lt;/li&gt;&lt;li&gt;Shows that diverse, human-centered training/evaluation data can improve moderation safety, fairness, and robustness on nuanced and borderline cases; dataset intended for both training and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naseem Machlovi', 'Maryam Saleki', 'Ruhul Amin', 'Mohamed Rahouti', 'Shawqi Al-Maliki', 'Junaid Qadir', 'Mohamed M. Abdallah', 'Ala Al-Fuqaha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM moderation', 'Safety evaluation', 'Jailbreaking', 'Bias/fairness', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03273</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models</title><link>https://arxiv.org/abs/2601.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Jailbreak-Zero, a policy-based red teaming method that uses an attack LLM to generate large volumes of adversarial prompts and is fine-tuned with preference data.&lt;/li&gt;&lt;li&gt;Claims Pareto improvements across policy coverage, attack strategy diversity, and prompt fidelity to real user inputs compared to example-based approaches.&lt;/li&gt;&lt;li&gt;Reports higher attack success rates against open-source and proprietary LLMs (e.g., GPT-4o, Claude 3.5) with minimal human intervention, improving scalability of safety evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Hu', 'Abhinav Aggarwal', 'Mehran Khodabandeh', 'David Zhang', 'Eric Hsin', 'Li Chen', 'Ankit Jain', 'Matt Fredrikson', 'Akash Bharadwaj']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03265</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting Semantic Backdoors in a Mystery Shopping Scenario</title><link>https://arxiv.org/abs/2601.03805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses detection of semantic backdoors in classification models under the assumption that the clean training dataset and training recipe are known (mystery shopping scenario).&lt;/li&gt;&lt;li&gt;Proposes creating a reference pool of clean and poisoned models trained on trusted infrastructure, calibrating a model-distance threshold, and evaluating multiple model-distance metrics.&lt;/li&gt;&lt;li&gt;Finds that requesting adversarial training from the provider and measuring distance using inputs obtained by model inversion (maximizing distance from clean samples) most reliably separates clean vs. poisoned models, outperforming state-of-the-art detectors and robust to an adaptive attacker.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arpad Berta', 'Gabor Danner', 'Istvan Hegedus', 'Mark Jelasity']&lt;/li&gt;&lt;li&gt;Tags: ['semantic backdoors', 'backdoor detection', 'model inspection', 'adversarial training', 'adaptive attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03805</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</title><link>https://arxiv.org/abs/2601.03703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TreeAdv, a method that makes tree structure explicit in group-based RL by building a forest that branches at high-uncertainty tokens and shares low-uncertainty tokens across rollouts.&lt;/li&gt;&lt;li&gt;Redistributes sequence-level advantages from complete rollouts to internal tree segments to produce token-level advantages, improving sample efficiency and mitigating length bias in chain-of-thought generation.&lt;/li&gt;&lt;li&gt;Applies to group-based objectives like GRPO and GSPO and demonstrates consistent gains across 10 math reasoning benchmarks while using fewer generated tokens under identical budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lang Cao', 'Hui Ruan', 'Yongqian Li', 'Peng Chao', 'Wu Ning', 'Haonan Song', 'Renhong Chen', 'Yitong Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'LLM training', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03703</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Inference Attacks Against Graph Generative Diffusion Models</title><link>https://arxiv.org/abs/2601.03701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces three black-box inference attacks on graph generative diffusion models: graph reconstruction, property inference (e.g., density distribution), and membership inference.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across three types of graph diffusion generative models and six real-world graph datasets, outperforming baselines.&lt;/li&gt;&lt;li&gt;Proposes two defense mechanisms that improve the privacy-utility trade-off compared to existing methods; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuling Wang', 'Xin Huang', 'Guibo Luo', 'Jianliang Xu']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'property inference', 'graph generative models', 'diffusion models', 'privacy attacks &amp; defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03701</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</title><link>https://arxiv.org/abs/2601.03661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AMIR-GRPO, which augments Group Relative Policy Optimization (GRPO) with an implicit DPO-style contrastive regularizer derived from intra-group reward rankings, requiring no extra annotations.&lt;/li&gt;&lt;li&gt;Aims to mitigate GRPO issues in reasoning-heavy tasks: sequence-level length bias, diluted penalties for low-quality trajectories, and loss of pairwise preference signal.&lt;/li&gt;&lt;li&gt;Reports consistent improvements over GRPO on multiple mathematical reasoning benchmarks, with better separation between correct and incorrect reasoning chains and broader coverage gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Hossein Yari', 'Fajri Koto']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement-learning', 'preference-learning', 'RLHF', 'reasoning-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03661</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification</title><link>https://arxiv.org/abs/2601.03600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALERT, a zero-shot jailbreak detection framework that amplifies internal feature discrepancies across layers, modules, and tokens to distinguish benign vs. jailbreak prompts.&lt;/li&gt;&lt;li&gt;Identifies safety-relevant layers, discriminative modules, and informative safety tokens in model internals and uses amplified representations for detection.&lt;/li&gt;&lt;li&gt;Implements two complementary classifiers on amplified representations to perform detection without relying on known jailbreak templates.&lt;/li&gt;&lt;li&gt;Reports strong zero-shot results on three safety benchmarks, outperforming baselines by large margins (≥10% average Accuracy/F1, up to ~40% in some cases).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Lin', 'Philip Li', 'Zhichen Zeng', 'Tingwei Li', 'Tianxin Wei', 'Xuying Ning', 'Gaotang Li', 'Yuzhong Chen', 'Hanghang Tong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak detection', 'zero-shot detection', 'safety evaluation', 'model internals', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03600</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Archaeology: The Causal Topology of Model Evolution</title><link>https://arxiv.org/abs/2601.03424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free mechanistic probe using attention-graph spectra (algebraic connectivity λ2, smoothness, spectral entropy) to produce stable spectral fingerprints across models and languages.&lt;/li&gt;&lt;li&gt;Identifies a failure mode called Passive-Triggered Connectivity Collapse (PTCC) tied to curriculum transitions (e.g., code-to-chat) that causes connectivity loss on non-canonical syntactic constructions and localizes to a sparse Layer 2 patch of heads.&lt;/li&gt;&lt;li&gt;Demonstrates specialization trade-offs, four recurrent processing strategies enabling forensic lineage identification, and that activation steering can partially restore ~38% of lost information flow.&lt;/li&gt;&lt;li&gt;Finds topological regimes correlate with tokenization density more than language identity and proposes spectra as a practical auditing/training-regime verification tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'robustness', 'model-auditing', 'mechanistic-analysis', 'training-regimes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03424</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks</title><link>https://arxiv.org/abs/2601.03420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAILS, a token-level iterative local search attack that requires only model logits (no gradients or handcrafted priors).&lt;/li&gt;&lt;li&gt;Introduces an auto-regressive loss enforcing exact prefix matching and a history-based selection strategy to improve alignment between optimization and true attack success.&lt;/li&gt;&lt;li&gt;Enables cross-tokenizer ensemble attacks to find adversarial patterns that transfer across disjoint vocabularies, yielding near-100% success on open-source models and high transferability to closed-source LLMs (e.g., GPT, Gemini).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhakshylyk Nurlanov', 'Frank R. Schmidt', 'Florian Bernard']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'adversarial attack', 'black-box transferability', 'red teaming', 'prompt jailbreaks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03420</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SIGMA: Scalable Spectral Insights for LLM Collapse</title><link>https://arxiv.org/abs/2601.03385</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SIGMA, a framework using spectral analysis of embedding Gram matrices to quantify and predict 'model collapse' from recursive training on synthetic data.&lt;/li&gt;&lt;li&gt;Derives deterministic and stochastic spectral bounds to track contraction of representation variance and offers scalable estimators suitable for large foundation models.&lt;/li&gt;&lt;li&gt;Demonstrates that SIGMA can detect transitions toward degenerate representation states and provides a practical monitoring tool for recursive training pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Gu', 'Lingyou Pang', 'Xiangkun Ye', 'Tianyu Wang', 'Jianyu Lin', 'Carey E. Priebe', 'Alexander Aue']&lt;/li&gt;&lt;li&gt;Tags: ['model collapse', 'representation collapse', 'robustness', 'monitoring', 'spectral analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03385</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</title><link>https://arxiv.org/abs/2601.03321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-consistent reinforcement learning framework for radiology report generation to better align linguistic outputs with visual evidence and reduce factual hallucinations.&lt;/li&gt;&lt;li&gt;Introduces a 'Reason-then-Summarize' architecture (separate think and answer blocks) and a novel Group Relative Policy Optimization (GRPO) training method.&lt;/li&gt;&lt;li&gt;Employs a multi-dimensional composite reward that explicitly penalizes logical discrepancies between findings and final diagnoses; reports SOTA clinical efficacy and reduced hallucinations on MIMIC-CXR.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Zhao', 'Siyuan Dai', 'Pan Wang', 'Jifeng Song', 'Hui Ji', 'Chenghua Lin', 'Liang Zhan', 'Haoteng Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'reinforcement learning', 'medical imaging', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03321</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning</title><link>https://arxiv.org/abs/2601.03320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R2VPO (Ratio-Variance Regularized Policy Optimization), which constrains the variance of policy ratios as a smooth alternative to hard clipping used in PPO/GRPO.&lt;/li&gt;&lt;li&gt;Claims this approach preserves gradient signal from high-divergence/high-reward trajectories and enables principled off-policy reuse by reweighting stale samples.&lt;/li&gt;&lt;li&gt;Evaluated on fine-tuning state-of-the-art LLMs for mathematical reasoning, showing improved asymptotic performance and ~50% fewer rollouts to convergence versus clipping-based baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Luo', 'Shuo Han', 'Yihan Hu', 'Dong Li', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'Reinforcement learning (PPO alternatives)', 'Alignment', 'Sample efficiency', 'Off-policy reuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03320</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts</title><link>https://arxiv.org/abs/2601.03315</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Case study of four end-to-end autonomous ML-research attempts using a six-agent LLM pipeline: three failed during implementation/evaluation, one passed human/multi-AI review and was accepted to Agents4Science 2025.&lt;/li&gt;&lt;li&gt;Identifies six recurring failure modes: bias toward training-data defaults, implementation drift under execution pressure, memory/context degradation in long-horizon tasks, overexcitement (declaring success despite failures), insufficient domain intelligence, and weak experimental taste.&lt;/li&gt;&lt;li&gt;Presents four design principles for more robust AI-scientist systems and releases all prompts, artifacts, and outputs for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhruv Trehan', 'Paras Chopra']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-agents', 'robustness', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03315</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Representational Contrastive Scoring (RCS) to detect multimodal jailbreaks by inspecting LVLM internal representations and learning a lightweight projection to separate benign vs malicious inputs.&lt;/li&gt;&lt;li&gt;Implements two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute contrastive scores from safety-critical layers.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art generalization to unseen attack types on a challenging evaluation protocol while remaining computationally lightweight and interpretable.&lt;/li&gt;&lt;li&gt;Focuses on reducing over-rejection common in one-class anomaly detectors by leveraging internal geometric signals rather than surface-level features.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'LLM/LLMV red teaming', 'anomaly detection', 'internal representations', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EngTrace: A Symbolic Benchmark for Verifiable Process Supervision of Engineering Reasoning</title><link>https://arxiv.org/abs/2511.01650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EngTrace, a symbolic benchmark (1,350 contamination-resistant test cases from 90 templates) for verifiable engineering reasoning across multiple branches and domains.&lt;/li&gt;&lt;li&gt;Proposes a two-stage, verifiable evaluation that checks intermediate reasoning traces as well as final answers using automated procedural checks and an AI Tribunal.&lt;/li&gt;&lt;li&gt;Evaluates 24 LLMs and finds a trade-off between numeric precision and trace fidelity, plus a 'complexity cliff' where pretraining fails to support integrative, physically grounded engineering reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayesha Gull', 'Muhammad Usman Safder', 'Rania Elbadry', 'Fan Zhang', 'Veselin Stoyanov', 'Preslav Nakov', 'Zhuohan Xie']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'verification', 'LLM-evaluation', 'engineering-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01650</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title><link>https://arxiv.org/abs/2510.20721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;User study (n=94) using 90 PrivacyLens scenarios to measure how real users perceive helpfulness and privacy-preservation in LLM responses.&lt;/li&gt;&lt;li&gt;Found low inter-user agreement on identical LLM responses, while five proxy LLMs showed high agreement with each other but low correlation with human judgments.&lt;/li&gt;&lt;li&gt;Concludes that proxy LLM-based evaluations do not accurately reflect users' diverse perceptions of utility and privacy, arguing for more user-centered evaluation and better alignment between models and users.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyuan Wu', 'Roshni Kaushik', 'Wenkai Li', 'Lujo Bauer', 'Koichi Onoue']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'human-evaluation', 'alignment', 'benchmarking', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20721</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Web Fraud Attacks Against LLM-Driven Multi-Agent Systems</title><link>https://arxiv.org/abs/2509.01211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Web Fraud Attacks' that exploit web link structures to deceive LLM-driven multi-agent systems (MAS).&lt;/li&gt;&lt;li&gt;Designs 12 representative attack variants (e.g., homoglyph deception, sub-directory nesting, parameter obfuscation).&lt;/li&gt;&lt;li&gt;Evaluates attacks across different MAS architectures, showing high effectiveness and easier evasion compared to complex input attacks.&lt;/li&gt;&lt;li&gt;Provides code release and practical insights for MAS security and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dezhang Kong', 'Hujin Peng', 'Yilun Zhang', 'Lele Zhao', 'Zhenhua Xu', 'Shi Lin', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-agent systems', 'URL/link-based attacks', 'adversarial web content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01211</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LAG: Logic-Augmented Generation from a Cartesian Perspective</title><link>https://arxiv.org/abs/2508.05509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Logic-Augmented Generation (LAG): decompose complex questions into logically ordered atomic sub-questions and resolve them sequentially.&lt;/li&gt;&lt;li&gt;Maintains an atomic memory bank and uses prior sub-answer context to guide retrieval for subsequent sub-questions (logic-aware retrieval).&lt;/li&gt;&lt;li&gt;Aims to reduce hallucinations and improve accuracy in knowledge-intensive tasks compared to standard retrieval-augmented methods.&lt;/li&gt;&lt;li&gt;Evaluated on four benchmarks, reporting significant accuracy improvements and reduced hallucination rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Xiao', 'Chuang Zhou', 'Yujing Zhang', 'Qinggang Zhang', 'Su Dong', 'Shengyuan Chen', 'Chang Yang', 'Xiao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination reduction', 'retrieval-augmented generation (RAG)', 'reasoning/decomposition', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05509</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Low Resource Reconstruction Attacks Through Benign Prompts</title><link>https://arxiv.org/abs/2507.07947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a low-resource reconstruction attack on generative diffusion models that uses seemingly benign text prompts to recover memorized training images or parts thereof.&lt;/li&gt;&lt;li&gt;Requires little to no access to training data and demonstrates that untrained users can accidentally trigger sensitive reconstructions (e.g., generating a real person's face from 'blue Unisex T-Shirt').&lt;/li&gt;&lt;li&gt;Attributes vulnerability to templated scraped e-commerce data where consistent layouts and image-text patterns enable prompt-based memorization exploitation.&lt;/li&gt;&lt;li&gt;Provides empirical results and public code to reproduce the attack (link included).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sol Yarkoni', 'Mahmood Sharif', 'Roi Livni']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'reconstruction/memorization', 'prompt-based attack', 'diffusion models', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07947</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict</title><link>https://arxiv.org/abs/2506.06485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a diagnostic framework that holds underlying knowledge constant while creating controlled conflicts between context and models' parametric memory across tasks with differing knowledge demands.&lt;/li&gt;&lt;li&gt;Finds that performance under context-memory conflict depends on task-specific reliance on context vs memory and on the plausibility of the conflicting context.&lt;/li&gt;&lt;li&gt;Shows that interventions like providing rationales or reiterating context shift models toward greater context reliance—helpful for context-only tasks but harmful for tasks needing parametric knowledge—and that these interventions bias model-based evaluation.&lt;/li&gt;&lt;li&gt;Argues for task-aware approaches to balancing context and memory in deployment and evaluation because conflict effects are task-dependent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiser Sun', 'Fan Bai', 'Mark Dredze']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'context-memory conflict', 'evaluation bias', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06485</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective</title><link>https://arxiv.org/abs/2505.20707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Physbench: 3,162 high-school/AP physics questions with structured reference solutions and 2,700 culturally contextualized paired variants; uses P-REFS, a stage-wise rubric, to evaluate reasoning.&lt;/li&gt;&lt;li&gt;Evaluates 10 small language models across ~58,000 responses and finds that 75–98% of solutions with correct final answers still contain at least one reasoning error; failure modes shift by model capability (interpretation/modeling vs execution).&lt;/li&gt;&lt;li&gt;Shows paired contextual variations have little effect on top models but degrade mid-tier models, arguing that safe educational AI requires evaluation of reasoning fidelity rather than only final-answer correctness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicy Scaria', 'Silvester John Joseph Kennedy', 'Krishna Agarwal', 'Diksha Seth', 'Deepak Subramani']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarking', 'reasoning-fidelity', 'educational-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20707</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities</title><link>https://arxiv.org/abs/2505.15722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study of memorization in multilingual LLMs covering 95 languages across various model scales and architectures.&lt;/li&gt;&lt;li&gt;Introduces a graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization patterns.&lt;/li&gt;&lt;li&gt;Finds that among similar languages, those with fewer training tokens exhibit higher memorization, a pattern revealed only when modeling cross-lingual relationships.&lt;/li&gt;&lt;li&gt;Argues for a language-aware perspective for evaluating and mitigating memorization vulnerabilities, with implications for privacy and cross-lingual transfer.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Luo', 'Yiyi Chen', 'Johannes Bjerva', 'Qiongxiu Li']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data privacy', 'multilingual LLMs', 'analysis', 'cross-lingual transfer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15722</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VISTA: Mitigating Semantic Inertia in Video-LLMs via Training-Free Dynamic Chain-of-Thought Routing</title><link>https://arxiv.org/abs/2505.11830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Semantic Inertia' in Video-LLMs: tendency to ignore visual evidence and rely on language priors, causing reasoning failures.&lt;/li&gt;&lt;li&gt;Proposes VISTA, a training-free framework that dynamically routes inference paths and converts implicit visual features into explicit textual anchors to strengthen perception signals.&lt;/li&gt;&lt;li&gt;Introduces a Latent Reasoning Consensus mechanism to reduce stochastic hallucinations and stabilize outputs.&lt;/li&gt;&lt;li&gt;Demonstrates substantial benchmark improvements (e.g., +9.3% on Egochema, +5.6% on VideoEspresso) versus base models, rivaling larger/proprietary models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Jin', 'Jiayu Ding', 'Siyi Xie', 'Guibo Luo', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'Video-LLM', 'chain-of-thought', 'multimodal robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11830</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models</title><link>https://arxiv.org/abs/2502.20408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts methods from functional brain network analysis to identify recurring 'functional networks' of neurons within large language models.&lt;/li&gt;&lt;li&gt;Demonstrates that inhibiting identified networks markedly degrades model performance, while amplifying neurons in these networks can improve overall or task-specific performance.&lt;/li&gt;&lt;li&gt;Provides empirical evidence and code showing these networks map to specific tasks or global capabilities, suggesting avenues for targeted intervention and interpretation of LLM behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiheng Liu', 'Zhengliang Liu', 'Zihao Wu', 'Junhao Ning', 'Haiyang Sun', 'Sichen Xia', 'Yang Yang', 'Xiaohui Gao', 'Ning Qiang', 'Bao Ge', 'Tianming Liu', 'Junwei Han', 'Xintao Hu']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model-editing', 'alignment', 'robustness', 'LLM internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20408</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</title><link>https://arxiv.org/abs/2502.18851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows a fundamental limitation of prior watermarking approaches: high-entropy but syntax-critical tokens (e.g., keywords) can cause logic corruption when watermarked.&lt;/li&gt;&lt;li&gt;Proposes STONE, a syntax-aware watermarking method that restricts watermarking to non-syntactic tokens to preserve code correctness.&lt;/li&gt;&lt;li&gt;Introduces STEM, an evaluation framework balancing correctness, detectability, and imperceptibility for rigorous assessment of code watermarking.&lt;/li&gt;&lt;li&gt;Empirically evaluates across Python, C++, and Java, demonstrating preserved correctness, strong detectability, and low overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungin Kim', 'Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['code watermarking', 'LLM detection', 'code generation', 'AI forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18851</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL, a framework that defines interpretable conversational traits from contrastive dialogue data and combines them into a scalar reward for aligning LLMs to human-likeness.&lt;/li&gt;&lt;li&gt;Uses the reward signal with standard preference optimization to align models of varying sizes without degrading overall performance, validated via large-scale human evaluations showing increased perceived human-likeness.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and inspectability of alignment by operating over explicit traits, enabling diagnosis of unintended effects and extending alignment to qualitative language properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward modeling', 'preference optimization', 'interpretability', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Faithful-First Reasoning, Planning, and Acting for Multimodal LLMs</title><link>https://arxiv.org/abs/2511.08409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Faithful-First Reasoning, Planning, and Acting (RPA) framework to improve faithfulness in multimodal LLM reasoning via two components: FaithEvi (evaluates step-wise and chain-level faithfulness) and FaithAct (plans and executes faithfulness-aware actions at inference).&lt;/li&gt;&lt;li&gt;FaithEvi provides supervision signals about intermediate reasoning fidelity to visual evidence; FaithAct uses those signals to guide generation and reduce hallucinations.&lt;/li&gt;&lt;li&gt;Evaluated on multiple multimodal reasoning benchmarks, reporting up to 24% improvement in perceptual faithfulness without degrading task accuracy.&lt;/li&gt;&lt;li&gt;Claims to offer a unified approach for evaluating and enforcing faithfulness in multimodal reasoning trajectories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Xinyue Xu', 'Sai Ma', 'Di Zhang', 'Sichao Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination', 'faithfulness evaluation', 'multimodal robustness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08409</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Chain-of-Thought as a Lens: Evaluating Structured Reasoning Alignment between Human Preferences and Large Language Models</title><link>https://arxiv.org/abs/2511.06168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boxuan Wang', 'Zhuoyun Li', 'Xinmiao Huang', 'Xiaowei Huang', 'Yi Dong']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06168</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities</title><link>https://arxiv.org/abs/2511.00340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CLAUSE, a benchmark of 7,500+ perturbed real-world contracts to test LLMs' ability to detect fine-grained legal discrepancies.&lt;/li&gt;&lt;li&gt;Generates 10 persona-driven anomaly categories and validates perturbations against statutes using a Retrieval-Augmented Generation (RAG) pipeline to ensure legal fidelity.&lt;/li&gt;&lt;li&gt;Evaluates leading LLMs on both detection of embedded legal flaws and their ability to justify significance, exposing systematic reasoning failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manan Roy Choudhury', 'Adithya Chandramouli', 'Mannan Anand', 'Vivek Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'Robustness', 'Adversarial Examples', 'Legal NLP', 'Safety Evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.00340</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Identity Skews Debate: Anonymization for Bias-Reduced Multi-Agent Reasoning</title><link>https://arxiv.org/abs/2510.07517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes multi-agent debate dynamics as an identity-weighted Bayesian update process to capture sycophancy and self-bias.&lt;/li&gt;&lt;li&gt;Proposes response anonymization (remove identity markers in prompts) to mitigate identity-driven bias and force equal weighting of peer vs self.&lt;/li&gt;&lt;li&gt;Introduces the Identity Bias Coefficient (IBC) to quantify an agent's tendency to follow peers versus its own prior output.&lt;/li&gt;&lt;li&gt;Empirical evaluation across models/benchmarks shows identity bias is widespread, with sycophancy more common than self-bias.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeong Kyu Choi', 'Xiaojin Zhu', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent debate', 'bias mitigation', 'trustworthiness', 'evaluation/metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07517</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multiplayer Nash Preference Optimization</title><link>https://arxiv.org/abs/2509.23102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multiplayer Nash Preference Optimization (MNPO), generalizing two-player Nash learning from human feedback (NLHF) to an n-player game where policies compete against a population and are regularized to a reference model.&lt;/li&gt;&lt;li&gt;Argues MNPO better captures non-transitive and heterogeneous human preferences, inherits equilibrium guarantees of two-player methods, and enables richer competitive dynamics.&lt;/li&gt;&lt;li&gt;Empirical results show MNPO outperforms existing NLHF baselines on instruction-following benchmarks, especially under heterogeneous annotator conditions and mixed-policy evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fang Wu', 'Xu Huang', 'Weihao Xuan', 'Zhiwei Zhang', 'Yijia Xiao', 'Guancheng Wan', 'Xiaomin Li', 'Bing Hu', 'Peng Xia', 'Jure Leskovec', 'Yejin Choi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'multi-agent', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23102</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models</title><link>https://arxiv.org/abs/2508.18760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes failures of large reasoning models (LRMs) to abstain on inherently unanswerable questions and characterizes distinct response behaviors.&lt;/li&gt;&lt;li&gt;Shows LRMs can internally recognize flaws in questions but fail to externally abstain, indicating a misalignment between internal cognition and output.&lt;/li&gt;&lt;li&gt;Proposes a lightweight two-stage mitigation combining cognitive monitoring with inference-time intervention to increase abstention rates.&lt;/li&gt;&lt;li&gt;Demonstrates experimentally that the method improves abstention while preserving overall reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Xiangyu Liu', 'Zequn Sun', 'Wei Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'abstention', 'safety', 'model monitoring', 'reasoning models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18760</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools</title><link>https://arxiv.org/abs/2508.02110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new attack surface where adversaries manipulate tool metadata (names, descriptions, parameter schemas) to bias LLM agents into selecting malicious tools.&lt;/li&gt;&lt;li&gt;Proposes Attractive Metadata Attack (AMA): a black-box in-context iterative optimization method that generates syntactically and semantically valid metadata that is highly attractive to agents.&lt;/li&gt;&lt;li&gt;Evaluates AMA across ten simulated tool-use scenarios and multiple popular LLM agents, reporting high success rates (81%-95%), privacy leakage, and minimal disruption to primary task execution.&lt;/li&gt;&lt;li&gt;Shows the attack remains effective against prompt-level defenses, auditor detection, and structured tool-selection protocols (e.g., Model Context Protocol), and can be combined with injection attacks—implying need for execution-level defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kanghua Mo', 'Li Hu', 'Yucheng Long', 'Zhihao Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'tool metadata attack', 'adversarial prompting', 'red teaming', 'safety vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02110</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Imagining and building wise machines: The centrality of AI metacognition</title><link>https://arxiv.org/abs/2411.02478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes human wisdom as a set of object-level and metacognitive strategies and proposes an AI counterpart emphasizing metacognition.&lt;/li&gt;&lt;li&gt;Argues that AI systems currently struggle with metacognition and that improving it would increase robustness, explainability, cooperation, and reduce misalignment risks.&lt;/li&gt;&lt;li&gt;Discusses potential approaches for benchmarking, training, and implementing 'wise' AI systems with stronger metacognitive capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel G. B. Johnson', 'Amir-Hossein Karimi', 'Yoshua Bengio', 'Nick Chater', 'Tobias Gerstenberg', 'Kate Larson', 'Sydney Levine', 'Melanie Mitchell', 'Iyad Rahwan', 'Bernhard Sch\\"olkopf', 'Igor Grossmann']&lt;/li&gt;&lt;li&gt;Tags: ['AI metacognition', 'alignment', 'AI safety', 'robustness', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.02478</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models</title><link>https://arxiv.org/abs/2601.04131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ContextFocus, an activation-steering method that improves LLM faithfulness to externally retrieved context when that context conflicts with the model's parametric knowledge.&lt;/li&gt;&lt;li&gt;Requires no finetuning and adds minimal inference-time overhead, making it lightweight and efficient.&lt;/li&gt;&lt;li&gt;Benchmarked on the ConFiQA dataset against baselines (ContextDPO, COIECD, prompting methods) and shown to be complementary to prompting and effective on larger models.&lt;/li&gt;&lt;li&gt;Reports significant improvements in contextual faithfulness while preserving fluency and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikhil Anand', 'Shwetha Somasundaram', 'Anirudh Phukan', 'Apoorv Saxena', 'Koyel Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'contextual faithfulness', 'activation steering', 'robustness', 'inference-time methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04131</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</title><link>https://arxiv.org/abs/2601.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode called "textual inertia" where once a textual hallucination appears in chain-of-thought, LMMs typically ignore conflicting visual evidence and propagate the error.&lt;/li&gt;&lt;li&gt;Introduces the LogicGraph Perturbation Protocol to structurally inject perturbations into reasoning chains across diverse LMMs to measure self-reflection and correction capability.&lt;/li&gt;&lt;li&gt;Finds that models self-correct in under 10% of cases and largely succumb to blind textual error propagation.&lt;/li&gt;&lt;li&gt;Proposes Active Visual-Context Refinement, a training-free inference method that actively re-grounds visual context and adaptively refines reasoning history to reduce hallucination propagation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhu', 'Jiafeng Liang', 'Shixin Jiang', 'Jinlan Fu', 'Ming Liu', 'Guanglu Sun', 'See-Kiong Ng', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'hallucination', 'robustness', 'chain-of-thought', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04073</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models</title><link>https://arxiv.org/abs/2601.04068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LocalDPO, a post-training method that creates localized preference pairs by corrupting spatio-temporal regions of real videos and restoring only those regions with a frozen base model.&lt;/li&gt;&lt;li&gt;Automated single-inference pipeline generates preference pairs without external critics or manual annotation; introduces a region-aware DPO loss that focuses learning on corrupted regions for faster convergence.&lt;/li&gt;&lt;li&gt;Evaluated on Wan2.1 and CogVideoX, showing consistent improvements in video fidelity, temporal coherence, and human preference scores over other post-training alignment approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zitong Huang', 'Kaidong Zhang', 'Yukang Ding', 'Chao Gao', 'Rui Ding', 'Ying Chen', 'Wangmeng Zuo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'video diffusion', 'preference optimization', 'post-training', 'direct preference optimization (DPO)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04068</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense</title><link>https://arxiv.org/abs/2601.04034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HoneyTrap, a multi-agent deceptive defense framework (Threat Interceptor, Misdirection Controller, Forensic Tracker, System Harmonizer) to engage and mislead multi-turn jailbreak attackers against LLMs.&lt;/li&gt;&lt;li&gt;Introduces MTJ-Pro, a multi-turn progressive jailbreak dataset combining seven advanced jailbreak strategies, plus two new metrics (Mislead Success Rate and Attack Resource Consumption) to evaluate deceptive defenses.&lt;/li&gt;&lt;li&gt;Evaluates HoneyTrap on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1, reporting substantial reductions in attack success rates and increased attacker time/computational costs even under adaptive attacker settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Li', 'Xi Lin', 'Jun Wu', 'Zehao Liu', 'Haoyu Li', 'Tianjie Ju', 'Xiang Chen', 'Jianhua Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaks', 'red teaming', 'deceptive defense', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04034</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>What Matters For Safety Alignment?</title><link>https://arxiv.org/abs/2601.03868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical evaluation of safety alignment across 32 LLMs/LRMs (3B–235B) using 5 safety datasets, 56 jailbreaks, and 4 CoT attack strategies (~4.6M API calls).&lt;/li&gt;&lt;li&gt;Finds integrated reasoning/self-reflection (e.g., certain LRMs) improves safety; post-training/distillation can degrade alignment unless safety is explicitly optimized.&lt;/li&gt;&lt;li&gt;Shows CoT response-prefix attacks dramatically increase attack success (avg 3.34x; up to 96.3% on a model), highlighting risks in text-completion interfaces and user-defined response prefixes.&lt;/li&gt;&lt;li&gt;Identifies roleplay, prompt injection, and gradient-based adversarial prompt search as primary methods for eliciting unaligned behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xing Li', 'Hui-Ling Zhen', 'Lihao Yin', 'Xianzhi Yu', 'Zhenhua Dong', 'Mingxuan Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'chain-of-thought attacks', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03868</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Numbers Start Talking: Implicit Numerical Coordination Among LLM-Based Agents</title><link>https://arxiv.org/abs/2601.03846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Game-theoretic analysis of implicit/covert communication among LLM-based agents across four canonical games under explicit, restricted, and absent communication regimes.&lt;/li&gt;&lt;li&gt;Investigates emergence of covert signals embedded in actions (non-linguistic/indirect) with heterogeneous agent personalities in both one-shot and repeated interactions.&lt;/li&gt;&lt;li&gt;Characterises when covert coordination arises and how it alters strategic outcomes, with implications for stealthy collusion and coordination in multi-agent LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alessio Buscemi', 'Daniele Proverbio', 'Alessandro Di Stefano', 'The Anh Han', 'German Castignani', 'Pietro Li\\`o']&lt;/li&gt;&lt;li&gt;Tags: ['covert communication', 'multi-agent systems', 'emergent behavior', 'AI safety', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03846</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework</title><link>https://arxiv.org/abs/2601.03791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cue-Resistant Memorization (CRM), a cue-controlled evaluation framework to distinguish true memorization from cue-driven reconstruction in LLMs.&lt;/li&gt;&lt;li&gt;Conducts a large-scale multilingual (32 languages) re-evaluation of PII leakage across multiple memorization paradigms (prefix-suffix completion, associative reconstruction, cue-free generation, membership inference).&lt;/li&gt;&lt;li&gt;Finds that much reported PII leakage is attributable to surface-form lexical cues in prompts; when cues are controlled, reconstruction success and membership inference true positive rates drop substantially.&lt;/li&gt;&lt;li&gt;Argues for adopting cue-controlled evaluations to more reliably quantify privacy-relevant memorization in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Luo', 'Yiyi Chen', 'Qiongxiu Li', 'Johannes Bjerva']&lt;/li&gt;&lt;li&gt;Tags: ['PII leakage', 'memorization', 'privacy attacks', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03791</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluation of Multilingual LLMs Personalized Text Generation Capabilities Targeting Groups and Social-Media Platforms</title><link>https://arxiv.org/abs/2601.03752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of LLM personalized text generation across 10 languages, examining both misuse (personalized disinformation) and potential benefits.&lt;/li&gt;&lt;li&gt;Generates 17,280 texts using 16 models across 1,080 personalization prompt combinations targeting demographic groups and social-media platforms.&lt;/li&gt;&lt;li&gt;Finds that personalization quality varies by language and that platform-targeted personalization more strongly reduces detectability, especially in English where personalization is strongest.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Macko']&lt;/li&gt;&lt;li&gt;Tags: ['disinformation', 'misuse/abuse', 'detectability', 'multilingual', 'personalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03752</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</title><link>https://arxiv.org/abs/2601.03703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TreeAdv, a method that makes tree structure explicit in group-based RL by building a forest that branches at high-uncertainty tokens and shares low-uncertainty tokens across rollouts.&lt;/li&gt;&lt;li&gt;Redistributes sequence-level advantages from complete rollouts to internal tree segments to produce token-level advantages, improving sample efficiency and mitigating length bias in chain-of-thought generation.&lt;/li&gt;&lt;li&gt;Applies to group-based objectives like GRPO and GSPO and demonstrates consistent gains across 10 math reasoning benchmarks while using fewer generated tokens under identical budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lang Cao', 'Hui Ruan', 'Yongqian Li', 'Peng Chao', 'Wu Ning', 'Haonan Song', 'Renhong Chen', 'Yitong Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'LLM training', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03703</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Inference Attacks Against Graph Generative Diffusion Models</title><link>https://arxiv.org/abs/2601.03701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces three black-box inference attacks on graph generative diffusion models: graph reconstruction, property inference (e.g., density distribution), and membership inference.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across three types of graph diffusion generative models and six real-world graph datasets, outperforming baselines.&lt;/li&gt;&lt;li&gt;Proposes two defense mechanisms that improve the privacy-utility trade-off compared to existing methods; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuling Wang', 'Xin Huang', 'Guibo Luo', 'Jianliang Xu']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'property inference', 'graph generative models', 'diffusion models', 'privacy attacks &amp; defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03701</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</title><link>https://arxiv.org/abs/2601.03661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AMIR-GRPO, which augments Group Relative Policy Optimization (GRPO) with an implicit DPO-style contrastive regularizer derived from intra-group reward rankings, requiring no extra annotations.&lt;/li&gt;&lt;li&gt;Aims to mitigate GRPO issues in reasoning-heavy tasks: sequence-level length bias, diluted penalties for low-quality trajectories, and loss of pairwise preference signal.&lt;/li&gt;&lt;li&gt;Reports consistent improvements over GRPO on multiple mathematical reasoning benchmarks, with better separation between correct and incorrect reasoning chains and broader coverage gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Hossein Yari', 'Fajri Koto']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement-learning', 'preference-learning', 'RLHF', 'reasoning-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03661</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification</title><link>https://arxiv.org/abs/2601.03600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALERT, a zero-shot jailbreak detection framework that amplifies internal feature discrepancies across layers, modules, and tokens to distinguish benign vs. jailbreak prompts.&lt;/li&gt;&lt;li&gt;Identifies safety-relevant layers, discriminative modules, and informative safety tokens in model internals and uses amplified representations for detection.&lt;/li&gt;&lt;li&gt;Implements two complementary classifiers on amplified representations to perform detection without relying on known jailbreak templates.&lt;/li&gt;&lt;li&gt;Reports strong zero-shot results on three safety benchmarks, outperforming baselines by large margins (≥10% average Accuracy/F1, up to ~40% in some cases).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Lin', 'Philip Li', 'Zhichen Zeng', 'Tingwei Li', 'Tianxin Wei', 'Xuying Ning', 'Gaotang Li', 'Yuzhong Chen', 'Hanghang Tong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak detection', 'zero-shot detection', 'safety evaluation', 'model internals', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03600</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing</title><link>https://arxiv.org/abs/2601.03587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a deontic knowledge-graph framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) to make data-release decisions: Allow, Block, or Allow-with-Transform.&lt;/li&gt;&lt;li&gt;Allows obligations to be bound to automated transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are recorded as semantic privacy incidents.&lt;/li&gt;&lt;li&gt;Evaluates scalability and performance on a 5.1M-triple DKG with 316K images, reporting exact-match decision correctness, sub-second per-decision latency, and interactive query performance in single-graph and federated setups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kelvin Uzoma Echenim', 'Karuna Pande Joshi']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'access-control', 'knowledge-graphs', 'data-governance', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03587</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating LLMs for Police Decision-Making: A Framework Based on Police Action Scenarios</title><link>https://arxiv.org/abs/2601.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAS (Police Action Scenarios), a systematic evaluation framework tailored to police decision-making for assessing LLM outputs in high-stakes law-enforcement scenarios.&lt;/li&gt;&lt;li&gt;Constructs a novel QA dataset derived from over 8,000 official police documents and defines metrics validated against police expert judgments.&lt;/li&gt;&lt;li&gt;Benchmarks commercial LLMs on police-related tasks, finding they struggle particularly with providing accurate, fact-based recommendations; releases dataset and prompt templates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sangyub Lee', 'Heedou Kim', 'Hyeoncheol Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'high-stakes-deployment', 'LLM-risk', 'law-enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03553</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict</title><link>https://arxiv.org/abs/2601.03546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a context-based assessment protocol that sequentially measures LLM privacy attitudes, prosocialness, and data-sharing acceptance within a single session to preserve history-dependent behavior.&lt;/li&gt;&lt;li&gt;Uses multi-group structural equation modeling (MGSEM) to link privacy concerns and prosocialness to actual data-sharing actions and introduces Value-Action Alignment Rate (VAAR) to quantify directional alignment with human-referenced expectations.&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs, finding model-specific privacy/prosocial profiles and substantial heterogeneity in how stated values predict downstream sharing behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Chen', 'Chenxiao Yu', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy', 'benchmarking', 'behavioral evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03546</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation</title><link>https://arxiv.org/abs/2601.03511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntroLM, a method that enables causal LMs to predict their own output quality during the prefilling phase using an 'introspective token' and token-conditional LoRA, without changing generation behavior.&lt;/li&gt;&lt;li&gt;Learns to predict success/failure for a given query, achieving high predictive performance (e.g., ~90% ROC AUC on QA when applied to Qwen3 8B) and outperforming external classifiers.&lt;/li&gt;&lt;li&gt;Demonstrates practical gains when used for multi-model routing—reducing latency and large-model usage while maintaining reliability—thus improving cost/reliability tradeoffs.&lt;/li&gt;&lt;li&gt;Avoids external evaluators by having the backbone model self-evaluate, which can be used for reliability, calibration, and routing decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hossein Hosseini Kasnavieh', 'Gholamreza Haffari', 'Chris Leckie', 'Adel N. Toosi']&lt;/li&gt;&lt;li&gt;Tags: ['self-evaluation', 'safety-evaluation', 'model-routing', 'calibration', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03511</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.03500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a visual statistical bias in vision encoders (Bag-of-Patches behavior) that prioritizes local texture over global structure and contributes to object hallucinations in LVLMs.&lt;/li&gt;&lt;li&gt;Proposes SDCD (Structure-Disrupted Contrastive Decoding), a training-free decoding algorithm that contrasts the model's outputs against a shuffled, structure-disrupted view and penalizes tokens that remain highly confident under the disrupted view.&lt;/li&gt;&lt;li&gt;Reports that SDCD significantly reduces hallucinations across multiple benchmarks and improves overall multimodal capabilities without additional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Xia', 'Siheng Wang', 'Peng Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'robustness', 'decoding methods', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03500</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder</title><link>https://arxiv.org/abs/2601.03460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FROST-Drive: an end-to-end driving architecture that keeps a pretrained vision encoder frozen (from a VLM) and adds a transformer-based adapter + GRU decoder for waypoint generation.&lt;/li&gt;&lt;li&gt;Introduces a custom loss to directly optimize Rater Feedback Score (RFS) to prioritize robust trajectory planning and long-tail scenario performance.&lt;/li&gt;&lt;li&gt;Evaluates on the Waymo Open E2E dataset and reports that the frozen-encoder approach outperforms fully fine-tuned vision-encoder baselines, arguing better generalization and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Dong', 'Yimin Zhu', 'Yu Wu', 'Yu Sun']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'safety-evaluation', 'transfer-learning', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03460</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Microeconomic Foundations of Multi-Agent Learning</title><link>https://arxiv.org/abs/2601.03451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops an economic foundation for multi-agent learning via a principal-agent interaction in an MDP with strategic externalities.&lt;/li&gt;&lt;li&gt;Proposes a two-phase incentive mechanism that estimates implementable transfers then uses them to steer long-run dynamics to improve social welfare.&lt;/li&gt;&lt;li&gt;Proves that under regret-based rationality and exploration conditions the mechanism attains sublinear social-welfare regret (asymptotically optimal welfare).&lt;/li&gt;&lt;li&gt;Simulations show coarse incentives can correct inefficient learning under stateful externalities, motivating incentive-aware design for welfare-aligned AI in markets/insurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nassim Helou']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent learning', 'mechanism design', 'AI alignment', 'incentive design', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03451</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale</title><link>https://arxiv.org/abs/2601.03444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares human and LLM ratings across three grading scales (including 0–5) on six benchmarks spanning objective, subjective, and mixed tasks.&lt;/li&gt;&lt;li&gt;Finds LLM judgments vary across grading scales on subjective benchmarks and that the 0–5 scale yields the strongest human–LLM alignment aggregated over tasks.&lt;/li&gt;&lt;li&gt;Shows pooled reliability metrics can mask benchmark heterogeneity and uncovers systematic subgroup differences (e.g., by gender) in alignment.&lt;/li&gt;&lt;li&gt;Argues scale design and sub-level diagnostics are important for robust LLM-as-a-judge evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiyue Li', 'Minda Zhao', 'Weixuan Dong', 'Jiahui Cai', 'Yuze Wei', 'Michael Pocress', 'Yi Li', 'Wanyan Yuan', 'Xiaoyue Wang', 'Ruoyu Hou', 'Kaiyuan Lou', 'Wenqi Zeng', 'Yutong Yang', 'Yilun Du', 'Mengyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'evaluation', 'LLM-as-judge', 'reliability', 'fairness/subgroup-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03444</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers</title><link>https://arxiv.org/abs/2601.03443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains linear classifiers on various audio embedding spaces to distinguish real wideband audio from super-resolved (GAN/diffusion) outputs.&lt;/li&gt;&lt;li&gt;Evaluates middle-band (4→16 kHz) and full-band (16→48 kHz) upsampling for speech and music across datasets and models.&lt;/li&gt;&lt;li&gt;Finds embedding-based classifiers achieve near-perfect separation even when generated audio scores well on perceptual metrics and human listening tests, revealing a gap between perceptual quality and distributional fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Silaev', 'Konstantinos Drossos', 'Tuomas Virtanen']&lt;/li&gt;&lt;li&gt;Tags: ['audio-forensics', 'deepfake-detection', 'generative-model-evaluation', 'robustness', 'detection-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03443</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Archaeology: The Causal Topology of Model Evolution</title><link>https://arxiv.org/abs/2601.03424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free mechanistic probe using attention-graph spectra (algebraic connectivity λ2, smoothness, spectral entropy) to produce stable spectral fingerprints across models and languages.&lt;/li&gt;&lt;li&gt;Identifies a failure mode called Passive-Triggered Connectivity Collapse (PTCC) tied to curriculum transitions (e.g., code-to-chat) that causes connectivity loss on non-canonical syntactic constructions and localizes to a sparse Layer 2 patch of heads.&lt;/li&gt;&lt;li&gt;Demonstrates specialization trade-offs, four recurrent processing strategies enabling forensic lineage identification, and that activation steering can partially restore ~38% of lost information flow.&lt;/li&gt;&lt;li&gt;Finds topological regimes correlate with tokenization density more than language identity and proposes spectra as a practical auditing/training-regime verification tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'robustness', 'model-auditing', 'mechanistic-analysis', 'training-regimes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03424</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks</title><link>https://arxiv.org/abs/2601.03420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAILS, a token-level iterative local search attack that requires only model logits (no gradients or handcrafted priors).&lt;/li&gt;&lt;li&gt;Introduces an auto-regressive loss enforcing exact prefix matching and a history-based selection strategy to improve alignment between optimization and true attack success.&lt;/li&gt;&lt;li&gt;Enables cross-tokenizer ensemble attacks to find adversarial patterns that transfer across disjoint vocabularies, yielding near-100% success on open-source models and high transferability to closed-source LLMs (e.g., GPT, Gemini).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhakshylyk Nurlanov', 'Frank R. Schmidt', 'Florian Bernard']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'adversarial attack', 'black-box transferability', 'red teaming', 'prompt jailbreaks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03420</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2601.03388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates a causal link between metaphors in training data and increased cross-domain misalignment in LLM reasoning.&lt;/li&gt;&lt;li&gt;Shows that interventions at pre-training, fine-tuning, and re-alignment stages significantly alter models' misalignment degrees.&lt;/li&gt;&lt;li&gt;Finds correlations between metaphors and activation patterns of global/local latent features and builds a detector that predicts misaligned content with high accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Hu', 'Chen Wang', 'Yanfeng Shu', 'Hye-young Paik', 'Liming Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM safety', 'misalignment', 'latent feature analysis', 'dataset influence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03388</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</title><link>https://arxiv.org/abs/2601.03321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-consistent reinforcement learning framework for radiology report generation to better align linguistic outputs with visual evidence and reduce factual hallucinations.&lt;/li&gt;&lt;li&gt;Introduces a 'Reason-then-Summarize' architecture (separate think and answer blocks) and a novel Group Relative Policy Optimization (GRPO) training method.&lt;/li&gt;&lt;li&gt;Employs a multi-dimensional composite reward that explicitly penalizes logical discrepancies between findings and final diagnoses; reports SOTA clinical efficacy and reduced hallucinations on MIMIC-CXR.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Zhao', 'Siyuan Dai', 'Pan Wang', 'Jifeng Song', 'Hui Ji', 'Chenghua Lin', 'Liang Zhan', 'Haoteng Tang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'reinforcement learning', 'medical imaging', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03321</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning</title><link>https://arxiv.org/abs/2601.03320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R2VPO (Ratio-Variance Regularized Policy Optimization), which constrains the variance of policy ratios as a smooth alternative to hard clipping used in PPO/GRPO.&lt;/li&gt;&lt;li&gt;Claims this approach preserves gradient signal from high-divergence/high-reward trajectories and enables principled off-policy reuse by reweighting stale samples.&lt;/li&gt;&lt;li&gt;Evaluated on fine-tuning state-of-the-art LLMs for mathematical reasoning, showing improved asymptotic performance and ~50% fewer rollouts to convergence versus clipping-based baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Luo', 'Shuo Han', 'Yihan Hu', 'Dong Li', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'Reinforcement learning (PPO alternatives)', 'Alignment', 'Sample efficiency', 'Off-policy reuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03320</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts</title><link>https://arxiv.org/abs/2601.03315</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Case study of four end-to-end autonomous ML-research attempts using a six-agent LLM pipeline: three failed during implementation/evaluation, one passed human/multi-AI review and was accepted to Agents4Science 2025.&lt;/li&gt;&lt;li&gt;Identifies six recurring failure modes: bias toward training-data defaults, implementation drift under execution pressure, memory/context degradation in long-horizon tasks, overexcitement (declaring success despite failures), insufficient domain intelligence, and weak experimental taste.&lt;/li&gt;&lt;li&gt;Presents four design principles for more robust AI-scientist systems and releases all prompts, artifacts, and outputs for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dhruv Trehan', 'Paras Chopra']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-agents', 'robustness', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03315</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mass Concept Erasure in Diffusion Models with Concept Hierarchy</title><link>https://arxiv.org/abs/2601.03305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a supertype–subtype concept hierarchy to group semantically related concepts (e.g., bird → macaw, bald eagle) and erase them jointly in diffusion models to improve efficiency and effectiveness when mass-erasing concepts.&lt;/li&gt;&lt;li&gt;Proposes SuPLoRA (Supertype-Preserving Low-Rank Adaptation), which freezes the down-projection and updates only the up-projection in LoRA-style adapters to preserve supertype generation while suppressing subtypes.&lt;/li&gt;&lt;li&gt;Applies diffusion regularization to preserve denoising in unmasked regions and builds a benchmark requiring simultaneous erasure across diverse domains (celebrities, objects, pornographic content).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahang Tu', 'Ye Li', 'Yiming Wu', 'Hanbin Zhao', 'Chao Zhang', 'Hui Qian']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'concept erasure', 'content moderation', 'safety', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03305</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-Driven Cybersecurity Threats: A Survey of Emerging Risks and Defensive Strategies</title><link>https://arxiv.org/abs/2601.03304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys AI-enabled cybersecurity threats across four categories: deepfakes/synthetic media, adversarial AI attacks, automated malware, and AI-powered social engineering.&lt;/li&gt;&lt;li&gt;Presents a comparative taxonomy linking AI capabilities to threat modalities and defenses; reviews 70+ academic and industry sources and discusses technical context, real incidents, legal frameworks, and countermeasures.&lt;/li&gt;&lt;li&gt;Identifies research opportunities such as hybrid detection pipelines, benchmarking frameworks, and the need for explainable, interdisciplinary, and regulatory-compliant AI defense systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Erukude', 'Viswa Chaitanya Marella', 'Suhasnadh Reddy Veluru']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-ML', 'deepfakes', 'automated-malware', 'social-engineering', 'defensive-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03304</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models</title><link>https://arxiv.org/abs/2601.03287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agentic LLM-based pipeline to automate post-incident reviews by ingesting logs, mapping observed behaviours to MITRE ATT&amp;CK, and evaluating organisational policy adequacy.&lt;/li&gt;&lt;li&gt;Demonstrates a proof-of-concept using a simulated brute-force attack (T1110) with GPT-4o for reasoning, LangGraph for orchestration, and LlamaIndex for traceable policy retrieval.&lt;/li&gt;&lt;li&gt;Reports that the system can identify missing or insufficient controls and produce remediation recommendations with evidence-to-policy traceability, while emphasising the need for human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Lin Oh', 'Jay Yong Jun Jie', 'Mandy Lee Ling Siu', 'Jonathan Pan']&lt;/li&gt;&lt;li&gt;Tags: ['LLMs for cyber defense', 'Post-incident analysis', 'Threat-informed mapping', 'Auditability', 'Automation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03287</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperCLOVA X 32B Think</title><link>https://arxiv.org/abs/2601.03286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HyperCLOVA X 32B Think, a vision-language model optimized for reasoning and agentic capabilities in the Korean linguistic/cultural context.&lt;/li&gt;&lt;li&gt;Pre-trained with emphasis on reasoning, then post-trained for multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences.&lt;/li&gt;&lt;li&gt;Evaluated on Korean text-to-text and vision-to-text benchmarks and on agent-oriented evaluation tasks; model is being open-sourced.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['NAVER Cloud HyperCLOVA X Team']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multimodal', 'agentic-behavior', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03286</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions</title><link>https://arxiv.org/abs/2601.03285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework of indices (using a fictitious-response rebuttal pairing) to quantify LLM behavior in response to user challenges in multiple-choice dialogues, targeting sycophantic (excessive agreement) and stubborn (rigid adherence) behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates the metrics on two physics multiple-choice problems across various OpenAI models, finding that newer models and those with greater 'Reasoning Effort' show reduced sycophancy.&lt;/li&gt;&lt;li&gt;Presents a generalizable, practical toolkit for systematically comparing LLM dialogue behaviors across models and contexts, applicable beyond questions with universally agreed answers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin C. Dunlap', 'Anne-Simone Parent', 'Ralf Widenhorn']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'Alignment/Safety evaluation', 'Sycophancy', 'Behavioral robustness', 'Red-teaming / evaluation methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03285</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>$\alpha^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks</title><link>https://arxiv.org/abs/2601.03281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces α^3-Bench, a large-scale benchmark (113k episodes) to evaluate LLM-driven UAV agents under dynamic 6G network conditions with multi-turn conversational control loops.&lt;/li&gt;&lt;li&gt;Measures six pillars (Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, Communication Cost) via a composite α^3 metric and evaluates 17 LLMs across scenarios.&lt;/li&gt;&lt;li&gt;Focuses explicitly on safety and protocol compliance (schema validity, mission policies, speaker alternation, safety constraints), tool-call consistency, multi-agent coordination, and performance degradation under latency/jitter/packet-loss/throughput variations.&lt;/li&gt;&lt;li&gt;Provides a public dataset and deterministic evaluation setup to compare mission success, safety compliance, robustness, and efficiency of LLM-based UAV controllers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Amine Ferrag', 'Abderrahmane Lakas', 'Merouane Debbah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Robustness', 'Benchmark', 'Network robustness', 'Multi-agent/tool use']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03281</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators</title><link>https://arxiv.org/abs/2601.03273</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardEval, a multi-perspective benchmark with 106 fine-grained categories covering emotions, offensive/hateful language, gender and racial bias, jailbreak prompts, and broader safety concerns.&lt;/li&gt;&lt;li&gt;Proposes GemmaGuard (GGuard), a QLoRA fine-tuned Gemma3-12B on GuardEval, achieving a macro F1 of 0.832 and outperforming commercial and open moderation models.&lt;/li&gt;&lt;li&gt;Shows that diverse, human-centered training/evaluation data can improve moderation safety, fairness, and robustness on nuanced and borderline cases; dataset intended for both training and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naseem Machlovi', 'Maryam Saleki', 'Ruhul Amin', 'Mohamed Rahouti', 'Shawqi Al-Maliki', 'Junaid Qadir', 'Mohamed M. Abdallah', 'Ala Al-Fuqaha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM moderation', 'Safety evaluation', 'Jailbreaking', 'Bias/fairness', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03273</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Instruction Gap: LLMs get lost in Following Instruction</title><link>https://arxiv.org/abs/2601.03269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates 13 leading LLMs on instruction compliance, response accuracy, and performance in enterprise RAG scenarios.&lt;/li&gt;&lt;li&gt;Identifies an 'instruction gap' where models often fail to reliably follow custom instructions needed for deployments.&lt;/li&gt;&lt;li&gt;Provides comparative benchmarks showing wide variation across models (e.g., Claude-Sonnet-4 and GPT-5 perform best).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vishesh Tripathi', 'Uday Allu', 'Biddwan Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'safety-evaluation', 'benchmarking', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03269</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OpenAI GPT-5 System Card</title><link>https://arxiv.org/abs/2601.03267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;System card for OpenAI GPT-5 describing a multi-model system (gpt-5-main, gpt-5-thinking) with a real-time router and mini-model fallbacks.&lt;/li&gt;&lt;li&gt;Highlights safety-focused changes: 'safe-completions' training to block disallowed content, reductions in hallucination, better instruction following, and continuous router training using user signals.&lt;/li&gt;&lt;li&gt;States a precautionary classification: gpt-5-thinking is treated as 'High capability' in Biological and Chemical domains under OpenAI's Preparedness Framework, triggering associated safeguards despite uncertainty about misuse potential.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other (System card)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaditya Singh (Tony)', 'Adam Fry (Tony)', 'Adam Perelman (Tony)', 'Adam Tart (Tony)', 'Adi Ganesh (Tony)', 'Ahmed El-Kishky (Tony)', 'Aidan McLaughlin (Tony)', 'Aiden Low (Tony)', 'AJ Ostrow (Tony)', 'Akhila Ananthram (Tony)', 'Akshay Nathan (Tony)', 'Alan Luo (Tony)', 'Alec Helyar (Tony)', 'Aleksander Madry (Tony)', 'Aleksandr Efremov (Tony)', 'Aleksandra Spyra (Tony)', 'Alex Baker-Whitcomb (Tony)', 'Alex Beutel (Tony)', 'Alex Karpenko (Tony)', 'Alex Makelov (Tony)', 'Alex Neitz (Tony)', 'Alex Wei (Tony)', 'Alexandra Barr (Tony)', 'Alexandre Kirchmeyer (Tony)', 'Alexey Ivanov (Tony)', 'Alexi Christakis (Tony)', 'Alistair Gillespie (Tony)', 'Allison Tam (Tony)', 'Ally Bennett (Tony)', 'Alvin Wan (Tony)', 'Alyssa Huang (Tony)', 'Amy McDonald Sandjideh (Tony)', 'Amy Yang (Tony)', 'Ananya Kumar (Tony)', 'Andre Saraiva (Tony)', 'Andrea Vallone (Tony)', 'Andrei Gheorghe (Tony)', 'Andres Garcia Garcia (Tony)', 'Andrew Braunstein (Tony)', 'Andrew Liu (Tony)', 'Andrew Schmidt (Tony)', 'Andrey Mereskin (Tony)', 'Andrey Mishchenko (Tony)', 'Andy Applebaum (Tony)', 'Andy Rogerson (Tony)', 'Ann Rajan (Tony)', 'Annie Wei (Tony)', 'Anoop Kotha (Tony)', 'Anubha Srivastava (Tony)', 'Anushree Agrawal (Tony)', 'Arun Vijayvergiya (Tony)', 'Ashley Tyra (Tony)', 'Ashvin Nair (Tony)', 'Avi Nayak (Tony)', 'Ben Eggers (Tony)', 'Bessie Ji (Tony)', 'Beth Hoover (Tony)', 'Bill Chen (Tony)', 'Blair Chen (Tony)', 'Boaz Barak (Tony)', 'Borys Minaiev (Tony)', 'Botao Hao (Tony)', 'Bowen Baker (Tony)', 'Brad Lightcap (Tony)', 'Brandon McKinzie (Tony)', 'Brandon Wang (Tony)', 'Brendan Quinn (Tony)', 'Brian Fioca (Tony)', 'Brian Hsu (Tony)', 'Brian Yang (Tony)', 'Brian Yu (Tony)', 'Brian Zhang (Tony)', 'Brittany Brenner (Tony)', 'Callie Riggins Zetino (Tony)', 'Cameron Raymond (Tony)', 'Camillo Lugaresi (Tony)', 'Carolina Paz (Tony)', 'Cary Hudson (Tony)', 'Cedric Whitney (Tony)', 'Chak Li (Tony)', 'Charles Chen (Tony)', 'Charlotte Cole (Tony)', 'Chelsea Voss (Tony)', 'Chen Ding (Tony)', 'Chen Shen (Tony)', 'Chengdu Huang (Tony)', 'Chris Colby (Tony)', 'Chris Hallacy (Tony)', 'Chris Koch (Tony)', 'Chris Lu (Tony)', 'Christina Kaplan (Tony)', 'Christina Kim (Tony)', 'CJ Minott-Henriques (Tony)', 'Cliff Frey (Tony)', 'Cody Yu (Tony)', 'Coley Czarnecki (Tony)', 'Colin Reid (Tony)', 'Colin Wei (Tony)', 'Cory Decareaux (Tony)', 'Cristina Scheau (Tony)', 'Cyril Zhang (Tony)', 'Cyrus Forbes (Tony)', 'Da Tang (Tony)', 'Dakota Goldberg (Tony)', 'Dan Roberts (Tony)', 'Dana Palmie (Tony)', 'Daniel Kappler (Tony)', 'Daniel Levine (Tony)', 'Daniel Wright (Tony)', 'Dave Leo (Tony)', 'David Lin (Tony)', 'David Robinson (Tony)', 'Declan Grabb (Tony)', 'Derek Chen (Tony)', 'Derek Lim (Tony)', 'Derek Salama (Tony)', 'Dibya Bhattacharjee (Tony)', 'Dimitris Tsipras (Tony)', 'Dinghua Li (Tony)', 'Dingli Yu (Tony)', 'DJ Strouse (Tony)', 'Drew Williams (Tony)', 'Dylan Hunn (Tony)', 'Ed Bayes (Tony)', 'Edwin Arbus (Tony)', 'Ekin Akyurek (Tony)', 'Elaine Ya Le (Tony)', 'Elana Widmann (Tony)', 'Eli Yani (Tony)', 'Elizabeth Proehl (Tony)', 'Enis Sert (Tony)', 'Enoch Cheung (Tony)', 'Eri Schwartz (Tony)', 'Eric Han (Tony)', 'Eric Jiang (Tony)', 'Eric Mitchell (Tony)', 'Eric Sigler (Tony)', 'Eric Wallace (Tony)', 'Erik Ritter (Tony)', 'Erin Kavanaugh (Tony)', 'Evan Mays (Tony)', 'Evgenii Nikishin (Tony)', 'Fangyuan Li (Tony)', 'Felipe Petroski Such (Tony)', 'Filipe de Avila Belbute Peres (Tony)', 'Filippo Raso (Tony)', 'Florent Bekerman (Tony)', 'Foivos Tsimpourlas (Tony)', 'Fotis Chantzis (Tony)', 'Francis Song (Tony)', 'Francis Zhang (Tony)', 'Gaby Raila (Tony)', 'Garrett McGrath (Tony)', 'Gary Briggs (Tony)', 'Gary Yang (Tony)', 'Giambattista Parascandolo (Tony)', 'Gildas Chabot (Tony)', 'Grace Kim (Tony)', 'Grace Zhao (Tony)', 'Gregory Valiant (Tony)', 'Guillaume Leclerc (Tony)', 'Hadi Salman (Tony)', 'Hanson Wang (Tony)', 'Hao Sheng (Tony)', 'Haoming Jiang (Tony)', 'Haoyu Wang (Tony)', 'Haozhun Jin (Tony)', 'Harshit Sikchi (Tony)', 'Heather Schmidt (Tony)', 'Henry Aspegren (Tony)', 'Honglin Chen (Tony)', 'Huida Qiu (Tony)', 'Hunter Lightman (Tony)', 'Ian Covert (Tony)', 'Ian Kivlichan (Tony)', 'Ian Silber (Tony)', 'Ian Sohl (Tony)', 'Ibrahim Hammoud (Tony)', 'Ignasi Clavera (Tony)', 'Ikai Lan (Tony)', 'Ilge Akkaya (Tony)', 'Ilya Kostrikov (Tony)', 'Irina Kofman (Tony)', 'Isak Etinger (Tony)', 'Ishaan Singal (Tony)', 'Jackie Hehir (Tony)', 'Jacob Huh (Tony)', 'Jacqueline Pan (Tony)', 'Jake Wilczynski (Tony)', 'Jakub Pachocki (Tony)', 'James Lee (Tony)', 'James Quinn (Tony)', 'Jamie Kiros (Tony)', 'Janvi Kalra (Tony)', 'Jasmyn Samaroo (Tony)', 'Jason Wang (Tony)', 'Jason Wolfe (Tony)', 'Jay Chen (Tony)', 'Jay Wang (Tony)', 'Jean Harb (Tony)', 'Jeffrey Han (Tony)', 'Jeffrey Wang (Tony)', 'Jennifer Zhao (Tony)', 'Jeremy Chen (Tony)', 'Jerene Yang (Tony)', 'Jerry Tworek (Tony)', 'Jesse Chand (Tony)', 'Jessica Landon (Tony)', 'Jessica Liang (Tony)', 'Ji Lin (Tony)', 'Jiancheng Liu (Tony)', 'Jianfeng Wang (Tony)', 'Jie Tang (Tony)', 'Jihan Yin (Tony)', 'Joanne Jang (Tony)', 'Joel Morris (Tony)', 'Joey Flynn (Tony)', 'Johannes Ferstad (Tony)', 'Johannes Heidecke (Tony)', 'John Fishbein (Tony)', 'John Hallman (Tony)', 'Jonah Grant (Tony)', 'Jonathan Chien (Tony)', 'Jonathan Gordon (Tony)', 'Jongsoo Park (Tony)', 'Jordan Liss (Tony)', 'Jos Kraaijeveld (Tony)', 'Joseph Guay (Tony)', 'Joseph Mo (Tony)', 'Josh Lawson (Tony)', 'Josh McGrath (Tony)', 'Joshua Vendrow (Tony)', 'Joy Jiao (Tony)', 'Julian Lee (Tony)', 'Julie Steele (Tony)', 'Julie Wang (Tony)', 'Junhua Mao (Tony)', 'Kai Chen (Tony)', 'Kai Hayashi (Tony)', 'Kai Xiao (Tony)', 'Kamyar Salahi (Tony)', 'Kan Wu (Tony)', 'Karan Sekhri (Tony)', 'Karan Sharma (Tony)', 'Karan Singhal (Tony)', 'Karen Li (Tony)', 'Kenny Nguyen (Tony)', 'Keren Gu-Lemberg (Tony)', 'Kevin King (Tony)', 'Kevin Liu (Tony)', 'Kevin Stone (Tony)', 'Kevin Yu (Tony)', 'Kristen Ying (Tony)', 'Kristian Georgiev (Tony)', 'Kristie Lim (Tony)', 'Kushal Tirumala (Tony)', 'Kyle Miller (Tony)', 'Lama Ahmad (Tony)', 'Larry Lv (Tony)', 'Laura Clare (Tony)', 'Laurance Fauconnet (Tony)', 'Lauren Itow (Tony)', 'Lauren Yang (Tony)', 'Laurentia Romaniuk (Tony)', 'Leah Anise (Tony)', 'Lee Byron (Tony)', 'Leher Pathak (Tony)', 'Leon Maksin (Tony)', 'Leyan Lo (Tony)', 'Leyton Ho (Tony)', 'Li Jing (Tony)', 'Liang Wu (Tony)', 'Liang Xiong (Tony)', 'Lien Mamitsuka (Tony)', 'Lin Yang (Tony)', 'Lindsay McCallum (Tony)', 'Lindsey Held (Tony)', 'Liz Bourgeois (Tony)', 'Logan Engstrom (Tony)', 'Lorenz Kuhn (Tony)', 'Louis Feuvrier (Tony)', 'Lu Zhang (Tony)', 'Lucas Switzer (Tony)', 'Lukas Kondraciuk (Tony)', 'Lukasz Kaiser (Tony)', 'Manas Joglekar (Tony)', 'Mandeep Singh (Tony)', 'Mandip Shah (Tony)', 'Manuka Stratta (Tony)', 'Marcus Williams (Tony)', 'Mark Chen (Tony)', 'Mark Sun (Tony)', 'Marselus Cayton (Tony)', 'Martin Li (Tony)', 'Marvin Zhang (Tony)', 'Marwan Aljubeh (Tony)', 'Matt Nichols (Tony)', 'Matthew Haines (Tony)', 'Max Schwarzer (Tony)', 'Mayank Gupta (Tony)', 'Meghan Shah (Tony)', 'Melody Huang (Tony)', 'Meng Dong (Tony)', 'Mengqing Wang (Tony)', 'Mia Glaese (Tony)', 'Micah Carroll (Tony)', 'Michael Lampe (Tony)', 'Michael Malek (Tony)', 'Michael Sharman (Tony)', 'Michael Zhang (Tony)', 'Michele Wang (Tony)', 'Michelle Pokrass (Tony)', 'Mihai Florian (Tony)', 'Mikhail Pavlov (Tony)', 'Miles Wang (Tony)', 'Ming Chen (Tony)', 'Mingxuan Wang (Tony)', 'Minnia Feng (Tony)', 'Mo Bavarian (Tony)', 'Molly Lin (Tony)', 'Moose Abdool (Tony)', 'Mostafa Rohaninejad (Tony)', 'Nacho Soto (Tony)', 'Natalie Staudacher (Tony)', 'Natan LaFontaine (Tony)', 'Nathan Marwell (Tony)', 'Nelson Liu (Tony)', 'Nick Preston (Tony)', 'Nick Turley (Tony)', 'Nicklas Ansman (Tony)', 'Nicole Blades (Tony)', 'Nikil Pancha (Tony)', 'Nikita Mikhaylin (Tony)', 'Niko Felix (Tony)', 'Nikunj Handa (Tony)', 'Nishant Rai (Tony)', 'Nitish Keskar (Tony)', 'Noam Brown (Tony)', 'Ofir Nachum (Tony)', 'Oleg Boiko (Tony)', 'Oleg Murk (Tony)', 'Olivia Watkins (Tony)', 'Oona Gleeson (Tony)', 'Pamela Mishkin (Tony)', 'Patryk Lesiewicz (Tony)', 'Paul Baltescu (Tony)', 'Pavel Belov (Tony)', 'Peter Zhokhov (Tony)', 'Philip Pronin (Tony)', 'Phillip Guo (Tony)', 'Phoebe Thacker (Tony)', 'Qi Liu (Tony)', 'Qiming Yuan (Tony)', 'Qinghua Liu (Tony)', 'Rachel Dias (Tony)', 'Rachel Puckett (Tony)', 'Rahul Arora (Tony)', 'Ravi Teja Mullapudi (Tony)', 'Raz Gaon (Tony)', 'Reah Miyara (Tony)', 'Rennie Song (Tony)', 'Rishabh Aggarwal (Tony)', 'RJ Marsan (Tony)', 'Robel Yemiru (Tony)', 'Robert Xiong (Tony)', 'Rohan Kshirsagar (Tony)', 'Rohan Nuttall (Tony)', 'Roman Tsiupa (Tony)', 'Ronen Eldan (Tony)', 'Rose Wang (Tony)', 'Roshan James (Tony)', 'Roy Ziv (Tony)', 'Rui Shu (Tony)', 'Ruslan Nigmatullin (Tony)', 'Saachi Jain (Tony)', 'Saam Talaie (Tony)', 'Sam Altman (Tony)', 'Sam Arnesen (Tony)', 'Sam Toizer (Tony)', 'Sam Toyer (Tony)', 'Samuel Miserendino (Tony)', 'Sandhini Agarwal (Tony)', 'Sarah Yoo (Tony)', 'Savannah Heon (Tony)', 'Scott Ethersmith (Tony)', 'Sean Grove (Tony)', 'Sean Taylor (Tony)', 'Sebastien Bubeck (Tony)', 'Sever Banesiu (Tony)', 'Shaokyi Amdo (Tony)', 'Shengjia Zhao (Tony)', 'Sherwin Wu (Tony)', 'Shibani Santurkar (Tony)', 'Shiyu Zhao (Tony)', 'Shraman Ray Chaudhuri (Tony)', 'Shreyas Krishnaswamy (Tony)', 'Shuaiqi (Tony)', 'Xia', 'Shuyang Cheng', 'Shyamal Anadkat', "Sim\\'on Posada Fishman", 'Simon Tobin', 'Siyuan Fu', 'Somay Jain', 'Song Mei', 'Sonya Egoian', 'Spencer Kim', 'Spug Golden', 'SQ Mah', 'Steph Lin', 'Stephen Imm', 'Steve Sharpe', 'Steve Yadlowsky', 'Sulman Choudhry', 'Sungwon Eum', 'Suvansh Sanjeev', 'Tabarak Khan', 'Tal Stramer', 'Tao Wang', 'Tao Xin', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Degry', 'Thomas Shadwell', 'Tianfu Fu', 'Tianshi Gao', 'Timur Garipov', 'Tina Sriskandarajah', 'Toki Sherbakov', 'Tomer Kaftan', 'Tomo Hiratsuka', 'Tongzhou Wang', 'Tony Song', 'Tony Zhao', 'Troy Peterson', 'Val Kharitonov', 'Victoria Chernova', 'Vineet Kosaraju', 'Vishal Kuo', 'Vitchyr Pong', 'Vivek Verma', 'Vlad Petrov', 'Wanning Jiang', 'Weixing Zhang', 'Wenda Zhou', 'Wenlei Xie', 'Wenting Zhan', 'Wes McCabe', 'Will DePue', 'Will Ellsworth', 'Wulfie Bain', 'Wyatt Thompson', 'Xiangning Chen', 'Xiangyu Qi', 'Xin Xiang', 'Xinwei Shi', 'Yann Dubois', 'Yaodong Yu', 'Yara Khakbaz', 'Yifan Wu', 'Yilei Qian', 'Yin Tat Lee', 'Yinbo Chen', 'Yizhen Zhang', 'Yizhong Xiong', 'Yonglong Tian', 'Young Cha', 'Yu Bai', 'Yu Yang', 'Yuan Yuan', 'Yuanzhi Li', 'Yufeng Zhang', 'Yuguang Yang', 'Yujia Jin', 'Yun Jiang', 'Yunyun Wang', 'Yushi Wang', 'Yutian Liu', 'Zach Stubenvoll', 'Zehao Dou', 'Zheng Wu', 'Zhigang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'capability-assessment', 'biological-safeguards', 'alignment', 'system-card']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03267</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2601.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation (CAP-GSM8K, N=500) comparing internal reasoning (Chain-of-Thought) versus an external control mechanism (RCA) across GPT-3.5, GPT-4o, and GPT-5.1 on sycophancy.&lt;/li&gt;&lt;li&gt;Finds internal reasoning can cause performance collapse in weaker models (Prioritization Paradox) and leaves an 11.4% residual sycophancy in frontier models, while RCA reportedly eliminates sycophancy (0.0%) across model tiers.&lt;/li&gt;&lt;li&gt;Proposes a 'thermodynamic hierarchy' framing (Resonance, Dissonance, Entropy) to describe hybrid system behavior and argues external structural constraints are strictly necessary for safety guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Y. Chang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'sycophancy / alignment', 'external control / constraints', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03263</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions</title><link>https://arxiv.org/abs/2601.04170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes "agent drift" with three manifestations: semantic drift, coordination drift, and behavioral drift.&lt;/li&gt;&lt;li&gt;Presents the Agent Stability Index (ASI), a composite metric across twelve dimensions (e.g., response consistency, tool usage, reasoning-path stability, inter-agent agreement) to quantify drift.&lt;/li&gt;&lt;li&gt;Uses simulations and theoretical modeling to show drift reduces task accuracy and increases required human intervention over extended interactions.&lt;/li&gt;&lt;li&gt;Proposes mitigation methods—episodic memory consolidation, drift-aware routing, and adaptive behavioral anchoring—and analyzes their effectiveness in reducing drift while preserving throughput.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Rath']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent LLMs', 'safety/robustness', 'drift detection/monitoring', 'evaluation metrics (ASI)', 'mitigation strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.04170</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</title><link>https://arxiv.org/abs/2601.03948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trade-R1, a training framework to mitigate reward hacking in RL for stochastic financial environments by verifying process-level reasoning.&lt;/li&gt;&lt;li&gt;Introduces a Retrieval-Augmented Generation (RAG) based verification that uses a triangular consistency metric between retrieved evidence, reasoning chains, and decisions to filter noisy returns.&lt;/li&gt;&lt;li&gt;Defines two reward-integration strategies—Fixed-effect Semantic Reward (FSR) for stable alignment and Dynamic-effect Semantic Reward (DSR) for magnitude-coupled optimization—with DSR showing superior cross-market generalization.&lt;/li&gt;&lt;li&gt;Evaluated on country asset selection tasks, demonstrating reduced reward hacking and higher reasoning consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Sun', 'Yifan Sun', 'Sheng Xu', 'Li Zhao', 'Jing Li', 'Daxin Jiang', 'Chen Hua', 'Zuo Bai']&lt;/li&gt;&lt;li&gt;Tags: ['reward-hacking', 'RL-safety', 'verification', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03948</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Current Agents Fail to Leverage World Model as Tool for Foresight</title><link>https://arxiv.org/abs/2601.03905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of vision-language agents using generative world models as external simulators to anticipate future states across agentic and visual question-answering tasks.&lt;/li&gt;&lt;li&gt;Key findings: agents rarely invoke simulation (&lt;1%), commonly misuse predicted rollouts (~15%), and can show inconsistent or degraded performance (up to ~5%) when simulation is available or enforced.&lt;/li&gt;&lt;li&gt;Attribution analysis indicates primary bottlenecks are deciding when to simulate, interpreting predicted outcomes, and integrating foresight into downstream reasoning.&lt;/li&gt;&lt;li&gt;Conclusion: improving calibrated, strategic interaction with world models is necessary to achieve reliable anticipatory cognition in future agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Qian', 'Emre Can Acikgoz', 'Bingxuan Li', 'Xiusi Chen', 'Yuji Zhang', 'Bingxiang He', 'Qinyu Luo', 'Dilek Hakkani-T\\"ur', 'Gokhan Tur', 'Yunzhu Li', 'Heng Ji', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'world-models', 'agent-foresight']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03905</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs</title><link>https://arxiv.org/abs/2601.03662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new safety vulnerability: explicit thinking/chain-of-thought steps can amplify unsafe behaviors, and safe-reminding phrases within those steps help maintain safety.&lt;/li&gt;&lt;li&gt;Proposes SafeRemind, a decoding-time defense that uses entropy-based triggers to inject safe-reminding phrases at decision-locking points without any parameter updates.&lt;/li&gt;&lt;li&gt;Evaluated on five large reasoning models and six benchmarks, reporting up to 45.5 percentage points improvement in safety while preserving core reasoning utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Su-Hyeon Kim', 'Hyundong Jin', 'Yejin Lee', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'decoding-time defense', 'chain-of-thought', 'prompt-based mitigation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03662</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules</title><link>https://arxiv.org/abs/2601.03537</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STAR-S, a self-taught loop that elicits safety-rule-guided reasoning and uses the generated reasoning data to fine-tune LLMs iteratively to improve safety alignment.&lt;/li&gt;&lt;li&gt;Aims specifically to defend against jailbreak attacks by improving models' reasoning and interpretation of safety rules through repeated generation, reflection, and fine-tuning cycles.&lt;/li&gt;&lt;li&gt;Reports experimental improvements over baselines in resisting jailbreaks and provides code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Di Wu', 'Yanyan Zhao', 'Xin Lu', 'Mingzhe Li', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'safety alignment', 'self-taught reasoning', 'fine-tuning', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03537</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</title><link>https://arxiv.org/abs/2601.03470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a maturity-based framework to certify embodied AI systems via explicit measurement mechanisms and quantitative scoring.&lt;/li&gt;&lt;li&gt;Argues for structured assessment frameworks that handle multi-objective trade-offs in evaluating trustworthiness.&lt;/li&gt;&lt;li&gt;Demonstrates the approach using uncertainty quantification as an exemplar measurement mechanism.&lt;/li&gt;&lt;li&gt;Illustrates feasibility through a Uncrewed Aircraft System (UAS) detection case study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael C. Darling', 'Alan H. Hesu', 'Michael A. Mardikes', 'Brian C. McGuigan', 'Reed M. Milewicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'certification', 'uncertainty-quantification', 'embodied-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03470</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization</title><link>https://arxiv.org/abs/2601.03359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent workflow that separates optimization of the primary task description from its constraints, using quantitative compliance scores as feedback to iteratively rewrite constraints.&lt;/li&gt;&lt;li&gt;Aims to improve LLM adherence to formal acceptance criteria (constraint compliance) rather than only rephrasing task descriptions.&lt;/li&gt;&lt;li&gt;Evaluation shows the method increases compliance scores for models like Llama 3.1 8B and Mixtral-8x 7B.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alberto Purpura', 'Li Wang', 'Sahil Badyal', 'Eugenio Beaufrand', 'Adam Faulkner']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'prompt-engineering', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03359</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://arxiv.org/abs/2601.03335</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Digital Red Queen (DRQ), a self-play algorithm using an LLM to iteratively evolve assembly-like programs (“warriors”) in the Core War environment.&lt;/li&gt;&lt;li&gt;Each round a new warrior is evolved to defeat prior warriors, producing continual adversarial adaptation; warriors grow more generally effective and converge behaviorally across runs.&lt;/li&gt;&lt;li&gt;Frames Core War as a controllable sandbox for studying adversarial dynamics and LLM-based evolutionary methods, with connections to cybersecurity and potential real-world red teaming applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akarsh Kumar', 'Ryan Bahlous-Boldi', 'Prafull Sharma', 'Phillip Isola', 'Sebastian Risi', 'Yujin Tang', 'David Ha']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial evolution', 'red teaming', 'LLM-generated code', 'cybersecurity', 'self-play']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03335</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
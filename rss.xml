<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 05 Jan 2026 23:02:51 +0000</lastBuildDate><item><title>Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image</title><link>https://arxiv.org/abs/2512.22801</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new dataset simulating low-quality/real-world degraded images for evaluating open-vocabulary object detection.&lt;/li&gt;&lt;li&gt;Evaluates multiple open-vocabulary detectors (e.g., OWLv2, OWL-ViT, GroundingDINO, Detic) across varying levels and types of image degradation.&lt;/li&gt;&lt;li&gt;Finds small mAP degradation at low-level degradation but large performance drops at high-level degradation; OWLv2 is the most robust among tested models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Po-Chih Wu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'evaluation/benchmarking', 'open-vocabulary object detection', 'dataset', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22801</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm</title><link>https://arxiv.org/abs/2503.07330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows major quality issues in common OoD detection benchmarks for object detection (up to 13% label contamination), which distort method evaluations and increase false positives.&lt;/li&gt;&lt;li&gt;Proposes a training-time mitigation: fine-tune detectors on a synthesized, semantically similar OoD dataset to suppress objectness and shape a defensive decision boundary.&lt;/li&gt;&lt;li&gt;Reports large empirical gains (e.g., 91% reduction in hallucination error for YOLO on BDD-100K) and demonstrates generalization across detection paradigms (YOLO, Faster R-CNN, RT-DETR) with few-shot adaptation support.&lt;/li&gt;&lt;li&gt;Releases code and data to reproduce benchmarks and mitigation approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changshun Wu', 'Weicheng He', 'Chih-Hong Cheng', 'Xiaowei Huang', 'Saddek Bensalem']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution', 'object-detection', 'robustness', 'safety', 'dataset-quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07330</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore</title><link>https://arxiv.org/abs/2502.20034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Re-evaluates the claim that vision encoder capacity is the primary cause of object hallucination in LVLMs, focusing on a discriminative retrieval-style benchmark (OHD-Caps) rather than free-form captioning.&lt;/li&gt;&lt;li&gt;Introduces Fine-grained CLIPScore (F-CLIPScore), which uses noun-level text embeddings to improve object-level matching; reports a 39.6% absolute accuracy improvement over conventional CLIPScore on OHD-Caps without additional training.&lt;/li&gt;&lt;li&gt;Shows F-CLIPScore can be used to filter training/alignment data to reduce object hallucination in LVLMs (example: 4.9% POPE accuracy after alignment pretraining).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongseok Oh', 'Wonseok Hwang']&lt;/li&gt;&lt;li&gt;Tags: ['object hallucination', 'LVLM safety', 'robustness', 'evaluation metric', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20034</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving</title><link>https://arxiv.org/abs/2412.15206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving, covering trustfulness, safety, robustness, privacy, and fairness with ~10k scenes and 18k queries.&lt;/li&gt;&lt;li&gt;Evaluates six public VLMs (generalist to specialist, open-source to commercial) and uncovers vulnerabilities: generalist models sometimes outperform specialized DriveVLMs in overall trustworthiness; some DriveVLMs leak sensitive information.&lt;/li&gt;&lt;li&gt;Finds both generalist and specialist VLMs remain susceptible to adversarial attacks and biased decision-making across environments and populations.&lt;/li&gt;&lt;li&gt;Releases dataset and code to facilitate further research into DriveVLM trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Xing', 'Hongyuan Hua', 'Xiangbo Gao', 'Shenzhe Zhu', 'Renjie Li', 'Kexin Tian', 'Xiaopeng Li', 'Heng Huang', 'Tianbao Yang', 'Zhangyang Wang', 'Yang Zhou', 'Huaxiu Yao', 'Zhengzhong Tu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'privacy', 'adversarial attacks', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15206</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Anchor Transport: Robust Test-Time Adaptation for Vision-Language Models</title><link>https://arxiv.org/abs/2411.17002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Semantic Anchor Transport (SAT) to generate pseudo-labels by aligning visual embeddings with text-based semantic anchors for test-time adaptation of VLMs.&lt;/li&gt;&lt;li&gt;Formulates batch-wise label assignment as an Optimal Transport problem to maintain dataset structure and obtain reliable pseudo-labels.&lt;/li&gt;&lt;li&gt;Introduces multi-template distillation to leverage heterogeneous textual cues, emulating multi-view contrastive strategies without extra compute.&lt;/li&gt;&lt;li&gt;Demonstrates consistent, computationally efficient performance gains on multiple test-time adaptation benchmarks under distributional shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shambhavi Mishra', 'Julio Silva-Rodriguez', 'Ismail Ben Ayed', 'Marco Pedersoli', 'Jose Dolz']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'test-time adaptation', 'vision-language models', 'domain shift', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17002</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</title><link>https://arxiv.org/abs/2601.00785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedHypeVAE: a federated, differentially private framework that uses a hypernetwork to generate client-aware decoders and class-conditional priors for a conditional VAE to synthesize embedding-level data.&lt;/li&gt;&lt;li&gt;Applies differential privacy to the shared hypernetwork by clipping and noising gradients, decoupling local data from communicated parameters to mitigate gradient-leakage risks.&lt;/li&gt;&lt;li&gt;Incorporates local MMD alignment and a Lipschitz regularizer to improve stability and distributional coherence under non-IID client heterogeneity; provides a neutral meta-code for domain-agnostic synthesis and mixtures for multi-domain coverage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'generative-models', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00785</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection</title><link>https://arxiv.org/abs/2601.00777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores use of multimodal LLMs (Qwen2-Audio-7B-Instruct, SALMONN) for audio deepfake detection by combining audio inputs with text prompts.&lt;/li&gt;&lt;li&gt;Evaluates both zero-shot and fine-tuned settings using multi-prompt, question-answer style queries for binary deepfake classification.&lt;/li&gt;&lt;li&gt;Finds poor zero-shot generalization and difficulty on out-of-domain data, but good in-domain performance with minimal supervision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akanksha Chuchra', 'Shukesh Reddy', 'Sudeepta Mishra', 'Abhijit Das', 'Abhinav Dhall']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'multimodal LLMs', 'zero-shot vs fine-tuning evaluation', 'robustness/generalization', 'defensive security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00777</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</title><link>https://arxiv.org/abs/2601.00138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates selective prediction (abstention) for vision-language models in video question answering to improve reliability in high-stakes deployments.&lt;/li&gt;&lt;li&gt;Finds that confidence-thresholding yields smooth risk-coverage tradeoffs and mechanistic control in-distribution on NExT-QA with Gemini 2.0 Flash.&lt;/li&gt;&lt;li&gt;Examines robustness of abstention control under distribution shift and motivates explicit abstention knobs to maintain predictable error rates.&lt;/li&gt;&lt;li&gt;Empirical study focused on video QA benchmarks and practical mechanisms for predictable abstention behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jorge Ortiz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'selective prediction', 'uncertainty estimation', 'robustness', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00138</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models</title><link>https://arxiv.org/abs/2601.00659</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRoPS, a training-free framework to mitigate hallucinations in large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Introduces a novel hallucinated model that selectively removes key text tokens (in addition to visual ablations) to better capture hallucination causes.&lt;/li&gt;&lt;li&gt;Presents Generalized Contrastive Decoding to combine multiple hallucinated models representing diverse hallucination sources.&lt;/li&gt;&lt;li&gt;Reports consistent improvements (≈20% CHAIR gain) across six benchmarks and three LVLM families, outperforming prior training-free methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeraj Anand', 'Samyak Jha', 'Udbhav Bamba', 'Rahul Rahaman']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'training-free methods', 'contrastive decoding', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00659</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation</title><link>https://arxiv.org/abs/2601.00590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMo, a text-to-motion safety framework that performs Minimal Motion Unlearning (MMU) in continuous motion space to remove unsafe behaviors without relying on discrete VQ-VAE codebook replacement.&lt;/li&gt;&lt;li&gt;Introduces SafeMoVAE-29K, a curated safe text-to-motion dataset with rewritten safe prompts and refined continuous motions for trustworthy unlearning.&lt;/li&gt;&lt;li&gt;Claims improved safety-utility trade-offs vs. prior work (e.g., LCR), reporting 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X while maintaining comparable or better benign prompt performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiling Wang', 'Zeyu Zhang', 'Yiran Wang', 'Hao Tang']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-motion safety', 'machine unlearning', 'dataset sanitization', 'safe generation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00590</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Dataset for Human vs. AI Generated Image Detection</title><link>https://arxiv.org/abs/2601.00553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MS COCOAI, a dataset of 96,000 images (real and synthetic) built from MS COCO for AI-generated image detection.&lt;/li&gt;&lt;li&gt;Synthetic images generated using five models: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL·E 3, and MidJourney v6.&lt;/li&gt;&lt;li&gt;Defines two tasks: (1) classify images as real vs. generated, and (2) attribute a synthetic image to the producing model.&lt;/li&gt;&lt;li&gt;Provides the dataset publicly on Hugging Face to support benchmarking and detection research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajarshi Roy', 'Nasrin Imanpour', 'Ashhar Aziz', 'Shashwat Bajpai', 'Gurpreet Singh', 'Shwetangshu Biswas', 'Kapil Wanaskar', 'Parth Patwa', 'Subhankar Ghosh', 'Shreyas Dixit', 'Nilesh Ranjan Pal', 'Vipula Rawte', 'Ritvik Garimella', 'Gaytri Jena', 'Vasu Sharma', 'Vinija Jain', 'Aman Chadha', 'Aishwarya Naresh Reganti', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'image-generation-detection', 'deepfake-detection', 'attribution', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00553</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight single-pass detector for hallucinations in VQA that leverages internal model signals: token-level decoding uncertainty, intermediate visual features, and cross-modal alignment.&lt;/li&gt;&lt;li&gt;Fuses these signals with branch-wise evidence encoding and uncertainty-aware attention, and uses a low-cost, model-dependent automatic supervision strategy (LLM-as-a-Judge extension) to avoid expensive human labels.&lt;/li&gt;&lt;li&gt;Reports superior effectiveness and efficiency over prior external-verification and uncertainty-sampling methods, and analyzes how hallucination patterns vary across VLM architectures and internal signal types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'VQA', 'multimodal safety', 'model-internal signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</title><link>https://arxiv.org/abs/2601.00267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ActErase, a training-free activation-patching method to erase specific concepts from pre-trained text-to-image diffusion models by identifying activation difference regions via prompt-pair analysis and dynamically replacing input activations during forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art concept erasure on nudity, artistic style, and object removal tasks while preserving overall generative capability.&lt;/li&gt;&lt;li&gt;Claims robustness against adversarial attempts to reintroduce erased concepts and frames the approach as a plug-and-play, lightweight alternative to fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Sun', 'Xinhao Zhong', 'Hongyan Li', 'Yimin Zhou', 'Junhao Li', 'Bin Chen', 'Xuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'model editing', 'diffusion models', 'safety/mitigation', 'activation patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00267</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications</title><link>https://arxiv.org/abs/2601.00150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FCMBench-V1.0, a large-scale financial credit multimodal benchmark covering 18 certificate types with 4,043 privacy-compliant images and 8,446 QA samples.&lt;/li&gt;&lt;li&gt;Evaluation framework spans three dimensions: Perception (3 tasks), Credit-specific Reasoning (4 tasks), and Robustness (10 real-world acquisition artifact types).&lt;/li&gt;&lt;li&gt;Data are produced via a closed synthesis-capture pipeline to ensure privacy compliance and avoid web-sourced pretraining leakage.&lt;/li&gt;&lt;li&gt;Extensive evaluation of 23 state-of-the-art vision-language models shows model rankings and notable performance drops under acquisition artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yehui Yang', 'Dalu Yang', 'Wenshuo Zhou', 'Fangxin Shang', 'Yifan Liu', 'Jie Ren', 'Haojun Fei', 'Qing Yang', 'Tao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness evaluation', 'Benchmarking', 'Vision-language models', 'Financial domain', 'Privacy-preserving dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00150</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection</title><link>https://arxiv.org/abs/2601.00141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GLASS: combines a globally resized view with multiple original-resolution local crops selected via spatially stratified sampling.&lt;/li&gt;&lt;li&gt;Aggregates local crops using attention-based scoring to leverage fine-grained details without prohibitive compute on full-resolution images.&lt;/li&gt;&lt;li&gt;Implements GLASS with Vision Transformer, ResNet, and ConvNeXt backbones and shows improved AI-generated image detection performance versus standard transfer learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lawrence Han']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'Deepfake detection', 'High-resolution image analysis', 'Vision Transformer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00141</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical multi-LLM jailbreak experiments (6,000+ multi-turn attacker-target exchanges) using JailbreakBench across model scales (0.6B–120B) with outcomes judged by three independent LLM evaluators.&lt;/li&gt;&lt;li&gt;Primary findings: mean harm correlates with log(attacker-to-target size ratio) (Pearson r = 0.51, p &lt; 0.001); attacker-side variance contributes more to harm variance than target-side; attacker refusal frequency is strongly negatively correlated with harm (rho = -0.93).&lt;/li&gt;&lt;li&gt;Concludes that size asymmetry and attacker-side alignment substantially influence jailbreak success and calls for more controlled studies on inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'robustness/scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models</title><link>https://arxiv.org/abs/2505.07167</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that safety alignment in LLMs is mediated by learned 'safety trigger tokens' which, when present with specific inputs, activate safety behaviors.&lt;/li&gt;&lt;li&gt;Proposes D-STT, a defense that decodes and enforces a single-token safety trigger during decoding to activate safety patterns with minimal intervention.&lt;/li&gt;&lt;li&gt;Empirical results show D-STT significantly reduces harmful outputs across diverse jailbreak attacks while preserving usability and outperforming ten baseline methods with negligible latency overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gu', 'Handing Wang', 'Yi Mei', 'Mengjie Zhang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking defense', 'alignment robustness', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07167</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore</title><link>https://arxiv.org/abs/2502.20034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Re-evaluates the claim that vision encoder capacity is the primary cause of object hallucination in LVLMs, focusing on a discriminative retrieval-style benchmark (OHD-Caps) rather than free-form captioning.&lt;/li&gt;&lt;li&gt;Introduces Fine-grained CLIPScore (F-CLIPScore), which uses noun-level text embeddings to improve object-level matching; reports a 39.6% absolute accuracy improvement over conventional CLIPScore on OHD-Caps without additional training.&lt;/li&gt;&lt;li&gt;Shows F-CLIPScore can be used to filter training/alignment data to reduce object hallucination in LVLMs (example: 4.9% POPE accuracy after alignment pretraining).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongseok Oh', 'Wonseok Hwang']&lt;/li&gt;&lt;li&gt;Tags: ['object hallucination', 'LVLM safety', 'robustness', 'evaluation metric', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20034</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Navigating the Reality Gap: Privacy-Preserving On-Device Continual Adaptation of ASR for Clinical Telephony</title><link>https://arxiv.org/abs/2512.16401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantifies a large 'reality gap' for ASR on rural clinical telephony (up to 40.94% WER degradation).&lt;/li&gt;&lt;li&gt;Proposes privacy-preserving on-device continual adaptation using LoRA and multi-domain Experience Replay, yielding a 17.1% relative WER improvement and 55% reduction in catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Evaluates stabilization strategies (including an Absolute Fisher importance estimator) to ensure robust convergence under high-variance clinical telephony gradients and argues acoustic adaptation is necessary for clinical usability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darshil Chauhan', 'Adityasinh Solanki', 'Vansh Patel', 'Kanav Kapoor', 'Ritvik Jain', 'Aditya Bansal', 'Pratik Narang', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'on-device adaptation', 'ASR robustness', 'continual learning', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16401</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems</title><link>https://arxiv.org/abs/2512.01661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnsolvableQA, a multi-domain dataset created via 'Reverse Construction' that injects logical contradictions into valid reasoning chains to produce unsolvable questions.&lt;/li&gt;&lt;li&gt;Proposes UnsolvableRL, a reinforcement learning alignment method that trades off detecting objective unsolvability and producing calibrated uncertainty for tasks beyond model capability.&lt;/li&gt;&lt;li&gt;Reports &gt;90% unsolvability detection and improves solvable reasoning accuracy from 43.4% to 69.4% on Qwen3-4B-Instruct; identifies a training-data interaction where unsolvable data prevents Capability Collapse and acts as a regularizer.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengyun Peng', 'Qiguang Chen', 'Bofei Liu', 'Jiannan Guan', 'Libo Qin', 'Zheng Yan', 'Jinhao Liu', 'Jianshu Zhang', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'unsolvability detection', 'hallucination mitigation', 'reinforcement learning', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01661</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</title><link>https://arxiv.org/abs/2511.11518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes W2S-AlignTree, an inference-time, plug-and-play alignment framework that uses Monte Carlo Tree Search (MCTS) to guide generation from a strong LLM using step-level signals from a weaker model.&lt;/li&gt;&lt;li&gt;Leverages Weak-to-Strong generalization: weak model provides real-time alignment proxies and an entropy-aware exploration mechanism balances exploration vs. exploitation in the generative search tree.&lt;/li&gt;&lt;li&gt;Does not modify model parameters; aims to provide fine-grained control during generation across tasks like sentiment-controlled generation, summarization, and instruction-following.&lt;/li&gt;&lt;li&gt;Reports empirical gains (e.g., improving Llama3-8B summarization performance by ~15.9% relative) over strong baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Ding', 'Yuhao Wang', 'Tengyue Xiao', 'Haoying Wang', 'Caigui Jiang', 'Ning Ding']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time alignment', 'MCTS', 'LLM safety', 'weak-to-strong generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11518</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>C-VARC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models</title><link>https://arxiv.org/abs/2506.01495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical value framework grounded in core Chinese values (3 dimensions, 12 core values, 50 derived values).&lt;/li&gt;&lt;li&gt;Constructs C-VARC, a large-scale Chinese value rule corpus with over 250,000 human-enhanced value rules and 400,000 rule-based moral dilemma scenarios.&lt;/li&gt;&lt;li&gt;Demonstrates that C-VARC-guided scenarios have clearer value boundaries and higher diversity; LLMs preferred C-VARC options &gt;70.5% and Chinese annotators aligned 87.5% with the corpus.&lt;/li&gt;&lt;li&gt;Provides a culturally-adaptive benchmarking framework for value evaluation and alignment across multiple LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ping Wu', 'Guobin Shen', 'Dongcheng Zhao', 'Yuwei Wang', 'Yiting Dong', 'Yu Shi', 'Enmeng Lu', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['value-alignment', 'dataset', 'benchmarking', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01495</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedSEA-LLaMA, a federated model-splitting framework for LLaMA2 that keeps a small client-side model and offloads most parameters to server(s) to enable private LLM training/inference across data silos.&lt;/li&gt;&lt;li&gt;Introduces Gaussian-noise injection into forward hidden states for secure vector transmission, plus attention-mask compression and KV-cache collaboration to reduce communication overhead and accelerate training/inference.&lt;/li&gt;&lt;li&gt;Supports adaptive/dynamic partition points to tailor client/server splits to downstream tasks; experimentally matches centralized LLaMA2 performance and reports up to 8x speedups, with analyses of privacy attacks and partition trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving ML', 'model splitting', 'communication efficiency', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning</title><link>https://arxiv.org/abs/2601.00791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free method to detect valid mathematical reasoning in transformers by treating attention matrices as graphs and extracting four spectral diagnostics (Fiedler value, high-frequency energy ratio, graph signal smoothness, spectral entropy).&lt;/li&gt;&lt;li&gt;Shows strong discriminatory power across seven models (85.0–95.6% classification accuracy; Cohen's d up to 3.30) and calibrated thresholds achieving 93–95% on the full dataset.&lt;/li&gt;&lt;li&gt;Finds the method detects logical coherence beyond formal verifier acceptance and identifies an architectural dependency (e.g., Mistral-7B's Sliding Window Attention shifts which spectral features are most discriminative).&lt;/li&gt;&lt;li&gt;Positions spectral graph analysis as useful for hallucination detection and AI safety monitoring (reasoning verification) without requiring additional training or labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'hallucination-detection', 'alignment', 'attention-analysis', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00791</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak</title><link>https://arxiv.org/abs/2601.00213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a previously underexplored safety vulnerability in LLMs: malicious requests for designing intelligent optimization algorithms.&lt;/li&gt;&lt;li&gt;Introduces MalOptBench (60 malicious optimization algorithm prompts) and MOBjailbreak, a tailored jailbreak method.&lt;/li&gt;&lt;li&gt;Evaluates 13 major LLMs (including GPT-5 and DeepSeek-V3.1), finding high susceptibility (avg. 83.59% attack success) and limited effectiveness of plug-and-play defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gu', 'Handing Wang', 'Yi Mei', 'Mengjie Zhang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety benchmark', 'misuse of algorithm design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00213</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</title><link>https://arxiv.org/abs/2601.00065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a supply-chain attack on LLM composition: a single 'breaker token' engineered in a donor model that is inert there but reconstructs into a malicious feature after tokenizer transplant into a base model.&lt;/li&gt;&lt;li&gt;Formulates the attack as a dual-objective optimization and implements it using a sparse solver; attack is training-free, achieves spectral mimicry, and evades outlier detection.&lt;/li&gt;&lt;li&gt;Demonstrates persistence of the Trojan across common composition operations (fine-tuning, weight merging), highlighting a practical vulnerability in tokenizer transplant and model composition pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoze Liu', 'Weichen Yu', 'Matt Fredrikson', 'Xiaoqian Wang', 'Jing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Trojan/backdoor', 'Supply-chain vulnerability', 'Tokenizer transplant', 'Model composition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00065</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations</title><link>https://arxiv.org/abs/2601.00647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Physio-DPO, a physics-informed extension of Direct Preference Optimization that scales updates by energy gaps between native structures and physics-perturbed negatives to reduce structural hallucinations in protein LMs.&lt;/li&gt;&lt;li&gt;Targets alignment of protein language models to thermodynamic stability, improving foldability and reducing self-consistency RMSD compared to SFT, PPO, and standard DPO.&lt;/li&gt;&lt;li&gt;Reports quantitative gains (e.g., foldability 92.8%, RMSD 1.28 Å) and qualitative recovery of biophysical interactions like hydrophobic packing and hydrogen-bond networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['QiWei Meng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'model robustness', 'physics-informed learning', 'protein design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00647</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs</title><link>https://arxiv.org/abs/2601.00641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a model-agnostic, repeat-and-judge pipeline to reduce ‘‘contextual hallucinations’’ for fixed-input deterministic tasks.&lt;/li&gt;&lt;li&gt;Provides theoretical probabilistic bounds showing exponential decay in pipeline failure with independent output repetitions and in hallucination-selection with ensembles of judge calls (majority voting).&lt;/li&gt;&lt;li&gt;Proposes using an LLM-as-judge and analyzes effects of judge true-/false-positive rates, plus strengthening imperfect judges via repeated votes; validates theory on controlled extraction tasks with synthetic noisy judges.&lt;/li&gt;&lt;li&gt;Method requires no model weight changes, decoding modifications, or prompt engineering—designed as a lightweight modular mitigation for hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Rautenberg', 'Sven Schippkus']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'hallucination mitigation', 'reliability/robustness', 'ensemble verification', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00641</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns</title><link>https://arxiv.org/abs/2601.00588</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CSSBench, a benchmark focused on Chinese-specific adversarial patterns (homophones, pinyin, symbol splitting, etc.) that evade LLM safety guardrails.&lt;/li&gt;&lt;li&gt;Covers six safety domains (illegal activities, privacy leakage, health misinformation, fraud &amp; hate, adult content, public/political safety) and multiple task types relevant to real-world Chinese queries.&lt;/li&gt;&lt;li&gt;Evaluates popular lightweight LLMs, measuring vulnerabilities and over-refusal behavior, and shows lightweight models are particularly challenged by these Chinese-specific perturbations.&lt;/li&gt;&lt;li&gt;Aims to close the safety evaluation gap for on-device and cost-sensitive Chinese LLM deployments by providing targeted adversarial test cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhong Zhou', 'Shilinlu Yan', 'Chuanpu Liu', 'Qiankun Li', 'Kun Wang', 'Zhigang Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Red teaming', 'Adversarial prompting', 'Benchmark', 'Chinese NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00588</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations</title><link>https://arxiv.org/abs/2601.00454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Defensive M2S: fine-tuning guardrail models on compressed Multi-turn-to-Single-turn (M2S) representations to drastically reduce training and inference cost for multi-turn conversation safety screening.&lt;/li&gt;&lt;li&gt;Provides formal complexity analysis (O(n) vs O(n^2)) and strong empirical savings (e.g., 93x training token reduction; 94.6% inference token reduction) on a 779-sample multi-turn dataset.&lt;/li&gt;&lt;li&gt;Evaluates across multiple guardrail model families and compression templates on a multi-turn jailbreak benchmark (SafeDialBench), achieving large improvements in attack-detection recall (best: 93.8%) while cutting compute substantially.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['guardrail models', 'jailbreaking / jailbreak detection', 'LLM safety', 'efficiency / compression', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00454</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Uncertainty Quantification for Factual Generation of Large Language Models</title><link>https://arxiv.org/abs/2601.00348</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robust uncertainty quantification method (RU) to detect hallucinations in multi-fact generation by LLMs, especially under non-canonical/adversarial questioning.&lt;/li&gt;&lt;li&gt;Constructs a benchmark of 'trap' questions containing fake names to simulate adversarial prompts aimed at eliciting hallucinations.&lt;/li&gt;&lt;li&gt;Evaluates RU on four models and reports average ROCAUC improvements of ~0.1–0.2 over the best baselines, indicating better detection of fabricated facts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Zhang', 'Zhongliang Yang', 'Linna Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'uncertainty-quantification', 'red-teaming', 'robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00348</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations</title><link>https://arxiv.org/abs/2601.00282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how quantization (multiple techniques and bit widths) affects LLM self-explanations, covering natural language explanations and counterfactual examples.&lt;/li&gt;&lt;li&gt;Reports moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%); a user study shows reduced coherence and trustworthiness (up to 8.5%).&lt;/li&gt;&lt;li&gt;Finds larger models better preserve faithfulness but not necessarily SE quality; no single quantization method dominates across task accuracy, SE quality, and faithfulness.&lt;/li&gt;&lt;li&gt;Recommends validating SE quality for specific applications, especially for NLEs, though overall degradation is relatively minor and quantization remains effective for compression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianli Wang', 'Nils Feldhus', 'Pepa Atanasova', 'Fedor Splitt', 'Simon Ostermann', 'Sebastian M\\"oller', 'Vera Schmitt']&lt;/li&gt;&lt;li&gt;Tags: ['quantization', 'explainability', 'faithfulness', 'safety-evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00282</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback</title><link>https://arxiv.org/abs/2601.00224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two verification techniques for LLM assistants: Q* (reverse translation + semantic matching between generated code and user intent) and Feedback+ (incorporates execution feedback to iteratively refine code).&lt;/li&gt;&lt;li&gt;Embeds these mechanisms in a generator–discriminator framework to shift validation burden from users to the system for more reliable outputs.&lt;/li&gt;&lt;li&gt;Evaluated on Spider, Bird, and GSM8K benchmarks, showing reduced error rates and faster task completion; identifies reverse translation as a primary bottleneck.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yan Sun', 'Ming Cai', 'Stanley Kok']&lt;/li&gt;&lt;li&gt;Tags: ['verification', 'execution_feedback', 'semantic_matching', 'LLM_safety', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00224</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</title><link>https://arxiv.org/abs/2512.23090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChexReason, a vision-language model trained with limited SFT (2k samples) and RL (GRPO, 1k samples) on a single A100, evaluating on CheXpert and NIH CXR benchmarks.&lt;/li&gt;&lt;li&gt;Finds GRPO recovers/boosts in-distribution performance (23% improvement on CheXpert) but degrades cross-dataset transferability (19% drop on NIH), revealing a generalization paradox.&lt;/li&gt;&lt;li&gt;Shows the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures institution-agnostic features that RL optimization overwrites.&lt;/li&gt;&lt;li&gt;Concludes that aggressive RL fine-tuning can harm clinical robustness and that curated supervised fine-tuning may be preferable for deployment across diverse populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Berger', 'Manuela Bergau', 'Helen Schneider', 'Saad Ahmad', 'Tom Anglim Lagones', 'Gianluca Brugnara', 'Martha Foltyn-Dumitru', 'Kai Schlamp', 'Philipp Vollmuth', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['RL fine-tuning', 'robustness', 'medical imaging', 'safety/clinical deployment', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23090</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivTune, a device-cloud split learning framework that injects optimized noise into token representations to protect private data during fine-tuning.&lt;/li&gt;&lt;li&gt;Formulates noise selection as an optimization aligning defense utility, adjusts the mean of a d_χ-Privacy noise distribution, and scales noise by token importance to minimize distortion.&lt;/li&gt;&lt;li&gt;Evaluated on five datasets for classification and generation tasks, and tested against embedding inversion and attribute inference attacks.&lt;/li&gt;&lt;li&gt;Shows substantial reduction in attack success (e.g., 10% attack success on SST with RoBERTa) with only minor utility degradation (~3.33% drop), outperforming baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving fine-tuning', 'split learning', 'differential privacy', 'embedding inversion defense', 'attribute inference defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving</title><link>https://arxiv.org/abs/2412.15206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving, covering trustfulness, safety, robustness, privacy, and fairness with ~10k scenes and 18k queries.&lt;/li&gt;&lt;li&gt;Evaluates six public VLMs (generalist to specialist, open-source to commercial) and uncovers vulnerabilities: generalist models sometimes outperform specialized DriveVLMs in overall trustworthiness; some DriveVLMs leak sensitive information.&lt;/li&gt;&lt;li&gt;Finds both generalist and specialist VLMs remain susceptible to adversarial attacks and biased decision-making across environments and populations.&lt;/li&gt;&lt;li&gt;Releases dataset and code to facilitate further research into DriveVLM trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Xing', 'Hongyuan Hua', 'Xiangbo Gao', 'Shenzhe Zhu', 'Renjie Li', 'Kexin Tian', 'Xiaopeng Li', 'Heng Huang', 'Tianbao Yang', 'Zhangyang Wang', 'Yang Zhou', 'Huaxiu Yao', 'Zhengzhong Tu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'privacy', 'adversarial attacks', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15206</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback</title><link>https://arxiv.org/abs/2512.24818</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies Nash learning from human feedback (NLHF) to model non-transitive human preferences as a two-player zero-sum game for LLM alignment.&lt;/li&gt;&lt;li&gt;Proves the first convergence guarantee for Optimistic Multiplicative Weights Update (OMWU): last-iterate linear convergence after a burn-in phase when a full-support Nash equilibrium exists, without requiring NE uniqueness.&lt;/li&gt;&lt;li&gt;Identifies a novel marginal convergence phenomenon where rare-action probabilities grow exponentially from exponentially small values, improving dependence on instance constants.&lt;/li&gt;&lt;li&gt;Presents experiments in tabular and neural policy classes showing OMWU's practical strengths and potential applicability to LLM alignment workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shulun Chen', 'Runlong Zhou', 'Zihan Zhang', 'Maryam Fazel', 'Simon S. Du']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'Nash learning from human feedback (NLHF)', 'optimistic multiplicative weights (OMWU)', 'convergence theory', 'LLM alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24818</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining</title><link>https://arxiv.org/abs/2512.22589</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes &gt;2,500 NHTSA AV crash records for SAE Level 2 and 4 vehicles using a two-stage data‑driven pipeline.&lt;/li&gt;&lt;li&gt;Uses K-means clustering to segment crashes into four behavioral clusters by temporal, spatial, and environmental features.&lt;/li&gt;&lt;li&gt;Applies Association Rule Mining within clusters to extract multivariate relationships between crash patterns and contributors (lighting, surface, vehicle dynamics, environment).&lt;/li&gt;&lt;li&gt;Provides actionable insights for AV developers, regulators, and policymakers to reduce crash risk and guide deployment strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jewel Rana Palit (Traffic Engineer/Project Manager-II', 'Collier County Government', 'Traffic Management Center', '2695 Francis Ave Unit D', 'Naples', 'Fl', '37221)', 'Vijayalakshmi K Kumarasamy (Department of Computer Science and Engineering', 'University of Tennessee at Chattanooga', 'Chattanooga', 'TN', 'USA 37403)', 'Osama A. Osman (Chief Scientist', 'Center of Urban Informatics and Progress', 'Chattanooga', 'TN', 'USA 37403)']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Crash pattern analysis', 'Data mining', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22589</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Causality-Inspired Safe Residual Correction for Multivariate Time Series</title><link>https://arxiv.org/abs/2512.22428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRC, a plug-and-play post-hoc residual correction framework for multivariate time series forecasters that aims to guarantee non-degradation of performance.&lt;/li&gt;&lt;li&gt;Employs a causality-inspired encoder to decouple self- and cross-variable dynamics and a hybrid corrector to model residual errors.&lt;/li&gt;&lt;li&gt;Implements a strict four-fold safety mechanism to prevent harmful updates, reporting high non-degradation rates (NDR) across datasets and forecasting backbones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianxiang Xie', 'Yuncheng Hua', 'Mingyue Cheng', 'Flora Salim', 'Hao Xue']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model-safety', 'post-hoc-correction', 'time-series-forecasting', 'causality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.22428</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical multi-LLM jailbreak experiments (6,000+ multi-turn attacker-target exchanges) using JailbreakBench across model scales (0.6B–120B) with outcomes judged by three independent LLM evaluators.&lt;/li&gt;&lt;li&gt;Primary findings: mean harm correlates with log(attacker-to-target size ratio) (Pearson r = 0.51, p &lt; 0.001); attacker-side variance contributes more to harm variance than target-side; attacker refusal frequency is strongly negatively correlated with harm (rho = -0.93).&lt;/li&gt;&lt;li&gt;Concludes that size asymmetry and attacker-side alignment substantially influence jailbreak success and calls for more controlled studies on inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'robustness/scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning from Human Feedback</title><link>https://arxiv.org/abs/2504.12501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive introduction to RLHF covering origins, definitions, problem formulation, and data collection.&lt;/li&gt;&lt;li&gt;Detailed walkthrough of optimization stages: instruction tuning, training reward models, rejection sampling, reinforcement learning, and direct alignment algorithms.&lt;/li&gt;&lt;li&gt;Concludes with advanced topics and open questions related to synthetic data, evaluation, and understudied research directions relevant to alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathan Lambert']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'alignment', 'reward modeling', 'policy optimization', 'safety/evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12501</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Near-optimal, Scalable and Parallelizable Framework for Stochastic Bandits Robust to Adversarial Corruptions and Beyond</title><link>https://arxiv.org/abs/2502.07514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BARBAT, a novel framework for stochastic bandits robust to adversarial corruptions that removes the multiplicative K factor in regret, achieving near-optimal regret up to logarithmic terms.&lt;/li&gt;&lt;li&gt;Extends the approach to multi-agent bandits, graph bandits, combinatorial semi-bandits, and batched bandits, emphasizing parallelizability and lower computational cost compared to FTRL-based methods.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and numerical experiments validating efficiency and scalability under adversarial corruption models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zicheng Hu', 'Cheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'stochastic bandits', 'online learning', 'adversarial corruptions', 'scalable/parallel algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.07514</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Density-Based Algorithms for Corruption-Robust Contextual Search and Convex Optimization</title><link>https://arxiv.org/abs/2206.07528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies contextual search (a multidimensional generalization of binary search) under an adversarial noise/corruption model, focusing on ε-ball and symmetric loss.&lt;/li&gt;&lt;li&gt;Provides improved regret bounds: tight O(C + d log(1/ε)) for ε-ball loss and an efficient algorithm with O(C + d log T) regret for symmetric loss, improving prior results.&lt;/li&gt;&lt;li&gt;Introduces and studies Corruption-Robust Convex Optimization with subgradient feedback as a general framework of independent interest.&lt;/li&gt;&lt;li&gt;Technical novelty: tracks density functions over candidate target vectors rather than maintaining a knowledge set of consistent targets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renato Paes Leme', 'Chara Podimata', 'Jon Schneider']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'corruption-robust optimization', 'contextual search', 'online learning', 'theoretical ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2206.07528</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback</title><link>https://arxiv.org/abs/2601.00509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a retrieval-augmented, multi-tool iterative repair workflow for LLM-generated code using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution.&lt;/li&gt;&lt;li&gt;Leverages a lightweight embedding model to retrieve prior successful security-focused repairs to guide generation (RAG).&lt;/li&gt;&lt;li&gt;Evaluated on 3,242 programs from DeepSeek-Coder-1.3B and CodeLlama-7B, showing large reductions in vulnerabilities (96% reduction for DeepSeek; critical defect rate for CodeLlama reduced from 58.55% to 22.19%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vidyut Sriram', 'Sawan Pandita', 'Achintya Lakshmanan', 'Aneesh Shamraj', 'Suman Saha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'secure code generation', 'retrieval-augmented generation', 'tool-assisted repair', 'vulnerability mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00509</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations</title><link>https://arxiv.org/abs/2601.00282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how quantization (multiple techniques and bit widths) affects LLM self-explanations, covering natural language explanations and counterfactual examples.&lt;/li&gt;&lt;li&gt;Reports moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%); a user study shows reduced coherence and trustworthiness (up to 8.5%).&lt;/li&gt;&lt;li&gt;Finds larger models better preserve faithfulness but not necessarily SE quality; no single quantization method dominates across task accuracy, SE quality, and faithfulness.&lt;/li&gt;&lt;li&gt;Recommends validating SE quality for specific applications, especially for NLEs, though overall degradation is relatively minor and quantization remains effective for compression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianli Wang', 'Nils Feldhus', 'Pepa Atanasova', 'Fedor Splitt', 'Simon Ostermann', 'Sebastian M\\"oller', 'Vera Schmitt']&lt;/li&gt;&lt;li&gt;Tags: ['quantization', 'explainability', 'faithfulness', 'safety-evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00282</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Rectifying Adversarial Examples Using Their Vulnerabilities</title><link>https://arxiv.org/abs/2601.00270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense method that 'rectifies' adversarial examples by re-attacking them to move inputs beyond decision boundaries for correct label prediction.&lt;/li&gt;&lt;li&gt;Targets both white-box and black-box attacks, including targeted attacks that push samples into low-confidence categories.&lt;/li&gt;&lt;li&gt;Method requires no additional training or parameter tuning and operates solely on adversarial inputs, reportedly outperforming conventional rectification and input-transformation defenses in stability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fumiya Morimoto', 'Ryuto Morita', 'Satoshi Ono']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'defense', 'robustness', 'white-box attacks', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00270</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reinforcement learning with timed constraints for robotics motion planning</title><link>https://arxiv.org/abs/2601.00087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an automata-based RL framework that translates MITL formulas to Timed-LDGBA and constructs product timed models with MDPs/POMDPs for policy synthesis.&lt;/li&gt;&lt;li&gt;Uses a reward shaping approach to enforce time-bounded temporal correctness while allowing optimization of additional performance objectives.&lt;/li&gt;&lt;li&gt;Validates the method in simulated MDP and POMDP grid-worlds and an office-like service-robot scenario, showing learned policies satisfy strict temporal constraints under stochastic and partially observable dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoan Wang', 'Junchao Li', 'Mahdi Mohammad', 'Shaoping Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'temporal-logic', 'robotics-safety', 'formal-methods', 'POMDP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00087</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing</title><link>https://arxiv.org/abs/2601.00042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts the Go-Explore algorithm to perform red-team style security testing of tool-using LLM agents (evaluated on GPT-4o-mini) across 28 runs and six research questions.&lt;/li&gt;&lt;li&gt;Finds random-seed variance dominates algorithmic parameter effects (8x outcome spread); multi-seed averaging reduces variance while single-seed comparisons are unreliable.&lt;/li&gt;&lt;li&gt;Shows reward shaping usually harms attack discovery (94% exploration collapse or produces false positives), simple state signatures outperform complex ones, and ensembles increase attack-type diversity.&lt;/li&gt;&lt;li&gt;Concludes that seed variance and targeted domain knowledge can outweigh algorithmic sophistication for safety testing of trained models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manish Bhatt', 'Adrian Wood', 'Idan Habler', 'Ammar Al-Kahfah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00042</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning</title><link>https://arxiv.org/abs/2601.00791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free method to detect valid mathematical reasoning in transformers by treating attention matrices as graphs and extracting four spectral diagnostics (Fiedler value, high-frequency energy ratio, graph signal smoothness, spectral entropy).&lt;/li&gt;&lt;li&gt;Shows strong discriminatory power across seven models (85.0–95.6% classification accuracy; Cohen's d up to 3.30) and calibrated thresholds achieving 93–95% on the full dataset.&lt;/li&gt;&lt;li&gt;Finds the method detects logical coherence beyond formal verifier acceptance and identifies an architectural dependency (e.g., Mistral-7B's Sliding Window Attention shifts which spectral features are most discriminative).&lt;/li&gt;&lt;li&gt;Positions spectral graph analysis as useful for hallucination detection and AI safety monitoring (reasoning verification) without requiring additional training or labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'hallucination-detection', 'alignment', 'attention-analysis', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00791</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</title><link>https://arxiv.org/abs/2601.00785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedHypeVAE: a federated, differentially private framework that uses a hypernetwork to generate client-aware decoders and class-conditional priors for a conditional VAE to synthesize embedding-level data.&lt;/li&gt;&lt;li&gt;Applies differential privacy to the shared hypernetwork by clipping and noising gradients, decoupling local data from communicated parameters to mitigate gradient-leakage risks.&lt;/li&gt;&lt;li&gt;Incorporates local MMD alignment and a Lipschitz regularizer to improve stability and distributional coherence under non-IID client heterogeneity; provides a neutral meta-code for domain-agnostic synthesis and mixtures for multi-domain coverage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'generative-models', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00785</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Samples Are Not Created Equal</title><link>https://arxiv.org/abs/2601.00577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Distinguishes two classes of adversarial samples: those that exploit non-robust (brittle but predictive) features and those that do not.&lt;/li&gt;&lt;li&gt;Proposes an ensemble-based metric to quantify the degree to which adversarial perturbations manipulate non-robust features.&lt;/li&gt;&lt;li&gt;Uses this metric to analyze the composition of adversarial examples and to reinterpret phenomena like the effects of sharpness-aware minimization (SAM) and robustness gaps between adversarially trained vs. standard models on robust datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jennifer Crawford', 'Amol Khanna', 'Fred Lu', 'Amy R. Wagoner', 'Stella Biderman', 'Andre T. Nguyen', 'Edward Raff']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial examples', 'non-robust features', 'evaluation/metrics', 'robustness analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00577</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI</title><link>https://arxiv.org/abs/2601.00516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trajectory Guard: a Siamese Recurrent Autoencoder with a hybrid loss (contrastive alignment + reconstruction) to detect anomalies in multi-step agentic LLM action plans.&lt;/li&gt;&lt;li&gt;Targets both 'wrong plan for this task' and 'malformed plan structure' by being sequence-aware, avoiding limitations of mean-pooled embeddings and contrastive-only methods.&lt;/li&gt;&lt;li&gt;Evaluated on synthetic perturbations and real-world failure sets (RAS-Eval, Who&amp;When), reporting F1 0.88–0.94 and recall 0.86–0.92, with 32 ms inference latency and 17–27× speedups over LLM Judge baselines—enabling real-time safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Laksh Advani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Anomaly detection', 'Agentic AI', 'Runtime monitoring', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00516</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents</title><link>https://arxiv.org/abs/2601.00513</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a widespread "Right-for-Wrong-Reasons" issue in small LMs (7–9B): 50–69% of correct answers include fundamentally flawed reasoning.&lt;/li&gt;&lt;li&gt;Introduces the Reasoning Integrity Score (RIS), a process-based verification metric validated with substantial inter-rater agreement (κ=0.657).&lt;/li&gt;&lt;li&gt;Finds retrieval-augmented generation (RAG) improves reasoning integrity while meta-cognitive interventions (self-critique) often degrade it for small models; mechanistic analysis attributes RAG's benefit to grounding.&lt;/li&gt;&lt;li&gt;Proposes a distilled neural verifier that detects flawed reasoning with 0.86 F1 and ~100× speedup to enable runtime process verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Laksh Advani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'verification', 'alignment/robustness', 'process-based evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00513</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Graph Fine-Tuning with Adversarial Graph Prompting</title><link>https://arxiv.org/abs/2601.00229</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Graph Prompting (AGP), integrating adversarial learning into graph prompting to improve robustness of parameter-efficient fine-tuning (PEFT) for pre-trained GNNs.&lt;/li&gt;&lt;li&gt;Formulates AGP as a min-max optimization and introduces Joint Projected Gradient Descent (JointPGD) to generate strong adversarial graph/node perturbations, while learning node prompts to counteract them.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing AGP can address both topology and node feature noise and validates robustness improvements across benchmark tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyan Zhang', 'Bo Jiang', 'Jin Tang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'graph neural networks', 'adversarial attacks/defenses', 'parameter-efficient fine-tuning (PEFT)', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00229</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</title><link>https://arxiv.org/abs/2601.00065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a supply-chain attack on LLM composition: a single 'breaker token' engineered in a donor model that is inert there but reconstructs into a malicious feature after tokenizer transplant into a base model.&lt;/li&gt;&lt;li&gt;Formulates the attack as a dual-objective optimization and implements it using a sparse solver; attack is training-free, achieves spectral mimicry, and evades outlier detection.&lt;/li&gt;&lt;li&gt;Demonstrates persistence of the Trojan across common composition operations (fine-tuning, weight merging), highlighting a practical vulnerability in tokenizer transplant and model composition pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoze Liu', 'Weichen Yu', 'Matt Fredrikson', 'Xiaoqian Wang', 'Jing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Trojan/backdoor', 'Supply-chain vulnerability', 'Tokenizer transplant', 'Model composition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00065</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers</title><link>https://arxiv.org/abs/2512.12483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using transformer-based architectures (cross-axis transformers) to learn a mapping from ECC public keys to their private keys, targeting secp256r1.&lt;/li&gt;&lt;li&gt;Evaluates whether modern ML models can memorize public-private keypairs and effectively 'reverse engineer' the keypair generation process.&lt;/li&gt;&lt;li&gt;Includes experiments and quantitative analysis projecting feasibility and future trends for ML-enabled cryptanalysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lily Erickson']&lt;/li&gt;&lt;li&gt;Tags: ['cryptanalysis', 'adversarial-ML', 'model-misuse', 'privacy-attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12483</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivTune, a device-cloud split learning framework that injects optimized noise into token representations to protect private data during fine-tuning.&lt;/li&gt;&lt;li&gt;Formulates noise selection as an optimization aligning defense utility, adjusts the mean of a d_χ-Privacy noise distribution, and scales noise by token importance to minimize distortion.&lt;/li&gt;&lt;li&gt;Evaluated on five datasets for classification and generation tasks, and tested against embedding inversion and attribute inference attacks.&lt;/li&gt;&lt;li&gt;Shows substantial reduction in attack success (e.g., 10% attack success on SST with RoBERTa) with only minor utility degradation (~3.33% drop), outperforming baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving fine-tuning', 'split learning', 'differential privacy', 'embedding inversion defense', 'attribute inference defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems</title><link>https://arxiv.org/abs/2512.01661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnsolvableQA, a multi-domain dataset created via 'Reverse Construction' that injects logical contradictions into valid reasoning chains to produce unsolvable questions.&lt;/li&gt;&lt;li&gt;Proposes UnsolvableRL, a reinforcement learning alignment method that trades off detecting objective unsolvability and producing calibrated uncertainty for tasks beyond model capability.&lt;/li&gt;&lt;li&gt;Reports &gt;90% unsolvability detection and improves solvable reasoning accuracy from 43.4% to 69.4% on Qwen3-4B-Instruct; identifies a training-data interaction where unsolvable data prevents Capability Collapse and acts as a regularizer.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dengyun Peng', 'Qiguang Chen', 'Bofei Liu', 'Jiannan Guan', 'Libo Qin', 'Zheng Yan', 'Jinhao Liu', 'Jianshu Zhang', 'Wanxiang Che']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'unsolvability detection', 'hallucination mitigation', 'reinforcement learning', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01661</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Red Teaming Large Reasoning Models</title><link>https://arxiv.org/abs/2512.00412</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RT-LRM, a unified benchmark to evaluate trustworthiness of Large Reasoning Models across truthfulness, safety, and efficiency.&lt;/li&gt;&lt;li&gt;Identifies novel reasoning-specific vulnerabilities (e.g., chain-of-thought hijacking, prompt-induced inefficiencies) not well captured by existing evaluations.&lt;/li&gt;&lt;li&gt;Analyzes the effect of different training paradigms on model trustworthiness using 30 curated reasoning tasks and experiments on 26 models.&lt;/li&gt;&lt;li&gt;Releases a scalable toolbox and datasets to standardize trustworthiness research for LRMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Chen', 'Yang Yang', 'Chao Yu', 'Yu Tian', 'Zhi Cao', 'Linghao Li', 'Hang Su', 'Zhaoxia Yin']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'benchmarking', 'prompt-jailbreaking', 'safety', 'trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00412</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title><link>https://arxiv.org/abs/2511.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical multi-LLM jailbreak experiments (6,000+ multi-turn attacker-target exchanges) using JailbreakBench across model scales (0.6B–120B) with outcomes judged by three independent LLM evaluators.&lt;/li&gt;&lt;li&gt;Primary findings: mean harm correlates with log(attacker-to-target size ratio) (Pearson r = 0.51, p &lt; 0.001); attacker-side variance contributes more to harm variance than target-side; attacker refusal frequency is strongly negatively correlated with harm (rho = -0.93).&lt;/li&gt;&lt;li&gt;Concludes that size asymmetry and attacker-side alignment substantially influence jailbreak success and calls for more controlled studies on inter-model alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Nathanson', 'Rebecca Williams', 'Cynthia Matuszek']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'robustness/scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13788</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedSEA-LLaMA, a federated model-splitting framework for LLaMA2 that keeps a small client-side model and offloads most parameters to server(s) to enable private LLM training/inference across data silos.&lt;/li&gt;&lt;li&gt;Introduces Gaussian-noise injection into forward hidden states for secure vector transmission, plus attention-mask compression and KV-cache collaboration to reduce communication overhead and accelerate training/inference.&lt;/li&gt;&lt;li&gt;Supports adaptive/dynamic partition points to tailor client/server splits to downstream tasks; experimentally matches centralized LLaMA2 performance and reports up to 8x speedups, with analyses of privacy attacks and partition trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving ML', 'model splitting', 'communication efficiency', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm</title><link>https://arxiv.org/abs/2503.07330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows major quality issues in common OoD detection benchmarks for object detection (up to 13% label contamination), which distort method evaluations and increase false positives.&lt;/li&gt;&lt;li&gt;Proposes a training-time mitigation: fine-tune detectors on a synthesized, semantically similar OoD dataset to suppress objectness and shape a defensive decision boundary.&lt;/li&gt;&lt;li&gt;Reports large empirical gains (e.g., 91% reduction in hallucination error for YOLO on BDD-100K) and demonstrates generalization across detection paradigms (YOLO, Faster R-CNN, RT-DETR) with few-shot adaptation support.&lt;/li&gt;&lt;li&gt;Releases code and data to reproduce benchmarks and mitigation approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changshun Wu', 'Weicheng He', 'Chih-Hong Cheng', 'Xiaowei Huang', 'Saddek Bensalem']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution', 'object-detection', 'robustness', 'safety', 'dataset-quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07330</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</title><link>https://arxiv.org/abs/2512.23090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChexReason, a vision-language model trained with limited SFT (2k samples) and RL (GRPO, 1k samples) on a single A100, evaluating on CheXpert and NIH CXR benchmarks.&lt;/li&gt;&lt;li&gt;Finds GRPO recovers/boosts in-distribution performance (23% improvement on CheXpert) but degrades cross-dataset transferability (19% drop on NIH), revealing a generalization paradox.&lt;/li&gt;&lt;li&gt;Shows the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures institution-agnostic features that RL optimization overwrites.&lt;/li&gt;&lt;li&gt;Concludes that aggressive RL fine-tuning can harm clinical robustness and that curated supervised fine-tuning may be preferable for deployment across diverse populations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Armin Berger', 'Manuela Bergau', 'Helen Schneider', 'Saad Ahmad', 'Tom Anglim Lagones', 'Gianluca Brugnara', 'Martha Foltyn-Dumitru', 'Kai Schlamp', 'Philipp Vollmuth', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['RL fine-tuning', 'robustness', 'medical imaging', 'safety/clinical deployment', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23090</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NormCode: A Semi-Formal Language for Auditable AI Planning</title><link>https://arxiv.org/abs/2512.10563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NormCode, a semi-formal language and multi-format ecosystem (NCDS, NCD, NCN, NCDN) designed to make multi-step LLM workflows auditable by enforcing explicit data passing and per-step data isolation.&lt;/li&gt;&lt;li&gt;Enforces separation between semantic (probabilistic LLM reasoning) and syntactic (deterministic data flow) operations to eliminate context pollution and enable reconstruction/inspection of intermediate states.&lt;/li&gt;&lt;li&gt;Presents a four-phase compilation pipeline from natural language intent to executable JSON repositories and a visual Canvas for graph visualization and breakpoint debugging.&lt;/li&gt;&lt;li&gt;Validates the approach with tasks (e.g., base X addition) and self-hosted execution of the NormCode compiler to demonstrate structured intermediate representations improve transparency and traceability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Guan', 'Yunshan Li', 'Zekun Wu', 'Ruibo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['auditability', 'AI safety', 'LLM workflows', 'data isolation', 'transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10563</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning</title><link>https://arxiv.org/abs/2601.00791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free method to detect valid mathematical reasoning in transformers by treating attention matrices as graphs and extracting four spectral diagnostics (Fiedler value, high-frequency energy ratio, graph signal smoothness, spectral entropy).&lt;/li&gt;&lt;li&gt;Shows strong discriminatory power across seven models (85.0–95.6% classification accuracy; Cohen's d up to 3.30) and calibrated thresholds achieving 93–95% on the full dataset.&lt;/li&gt;&lt;li&gt;Finds the method detects logical coherence beyond formal verifier acceptance and identifies an architectural dependency (e.g., Mistral-7B's Sliding Window Attention shifts which spectral features are most discriminative).&lt;/li&gt;&lt;li&gt;Positions spectral graph analysis as useful for hallucination detection and AI safety monitoring (reasoning verification) without requiring additional training or labeled data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'hallucination-detection', 'alignment', 'attention-analysis', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00791</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</title><link>https://arxiv.org/abs/2601.00785</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedHypeVAE: a federated, differentially private framework that uses a hypernetwork to generate client-aware decoders and class-conditional priors for a conditional VAE to synthesize embedding-level data.&lt;/li&gt;&lt;li&gt;Applies differential privacy to the shared hypernetwork by clipping and noising gradients, decoupling local data from communicated parameters to mitigate gradient-leakage risks.&lt;/li&gt;&lt;li&gt;Incorporates local MMD alignment and a Lipschitz regularizer to improve stability and distributional coherence under non-IID client heterogeneity; provides a neutral meta-code for domain-agnostic synthesis and mixtures for multi-domain coverage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'generative-models', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00785</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?</title><link>https://arxiv.org/abs/2601.00559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks multiple LLMs (Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, DeepSeek-R1) on detecting interaction threats in openHAB TAC rules, using the original dataset and a structurally mutated dataset to test robustness.&lt;/li&gt;&lt;li&gt;Finds LLMs show semantic understanding on action- and condition-related threats but degrade substantially on cross-rule structural reasoning and under rule mutations; performance varies across models and prompt shots.&lt;/li&gt;&lt;li&gt;Symbolic static-analysis baseline remains stable and robust to structural rewrites, highlighting LLM unreliability for safety-critical detection when used alone.&lt;/li&gt;&lt;li&gt;Recommends hybrid architectures combining symbolic analysis for structural rigor with LLMs for semantic interpretation to reduce false positives while preserving robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Quantrill', 'Noura Khajehnouri', 'Zihan Guo', 'Manar H. Alalfi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'safety evaluation', 'IoT security', 'static analysis vs LLM', 'hybrid defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00559</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Dataset for Human vs. AI Generated Image Detection</title><link>https://arxiv.org/abs/2601.00553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MS COCOAI, a dataset of 96,000 images (real and synthetic) built from MS COCO for AI-generated image detection.&lt;/li&gt;&lt;li&gt;Synthetic images generated using five models: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL·E 3, and MidJourney v6.&lt;/li&gt;&lt;li&gt;Defines two tasks: (1) classify images as real vs. generated, and (2) attribute a synthetic image to the producing model.&lt;/li&gt;&lt;li&gt;Provides the dataset publicly on Hugging Face to support benchmarking and detection research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajarshi Roy', 'Nasrin Imanpour', 'Ashhar Aziz', 'Shashwat Bajpai', 'Gurpreet Singh', 'Shwetangshu Biswas', 'Kapil Wanaskar', 'Parth Patwa', 'Subhankar Ghosh', 'Shreyas Dixit', 'Nilesh Ranjan Pal', 'Vipula Rawte', 'Ritvik Garimella', 'Gaytri Jena', 'Vasu Sharma', 'Vinija Jain', 'Aman Chadha', 'Aishwarya Naresh Reganti', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['dataset', 'image-generation-detection', 'deepfake-detection', 'attribution', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00553</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI</title><link>https://arxiv.org/abs/2601.00516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trajectory Guard: a Siamese Recurrent Autoencoder with a hybrid loss (contrastive alignment + reconstruction) to detect anomalies in multi-step agentic LLM action plans.&lt;/li&gt;&lt;li&gt;Targets both 'wrong plan for this task' and 'malformed plan structure' by being sequence-aware, avoiding limitations of mean-pooled embeddings and contrastive-only methods.&lt;/li&gt;&lt;li&gt;Evaluated on synthetic perturbations and real-world failure sets (RAS-Eval, Who&amp;When), reporting F1 0.88–0.94 and recall 0.86–0.92, with 32 ms inference latency and 17–27× speedups over LLM Judge baselines—enabling real-time safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Laksh Advani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Anomaly detection', 'Agentic AI', 'Runtime monitoring', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00516</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations</title><link>https://arxiv.org/abs/2601.00454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Defensive M2S: fine-tuning guardrail models on compressed Multi-turn-to-Single-turn (M2S) representations to drastically reduce training and inference cost for multi-turn conversation safety screening.&lt;/li&gt;&lt;li&gt;Provides formal complexity analysis (O(n) vs O(n^2)) and strong empirical savings (e.g., 93x training token reduction; 94.6% inference token reduction) on a 779-sample multi-turn dataset.&lt;/li&gt;&lt;li&gt;Evaluates across multiple guardrail model families and compression templates on a multi-turn jailbreak benchmark (SafeDialBench), achieving large improvements in attack-detection recall (best: 93.8%) while cutting compute substantially.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['guardrail models', 'jailbreaking / jailbreak detection', 'LLM safety', 'efficiency / compression', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00454</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices</title><link>https://arxiv.org/abs/2601.00367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents PatchBlock, a lightweight sensor-level pre-processing defense against adversarial patches that splits inputs into chunks, detects anomalous regions with a modified isolation forest, and mitigates them via dimensionality reduction.&lt;/li&gt;&lt;li&gt;Designed to be model- and patch-agnostic and to run on CPUs in parallel with GPU inference for deployment on resource-constrained EdgeAI/embedded devices.&lt;/li&gt;&lt;li&gt;Evaluated across multiple architectures, datasets, and strong patch attacks (e.g., Google Adversarial Patch), reporting up to 77% recovery of accuracy and improved runtime/energy efficiency versus prior defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nandish Chattopadhyay', 'Abdul Basit', 'Amira Guesmi', 'Muhammad Abdullah Hanif', 'Bassem Ouni', 'Muhammad Shafique']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'adversarial defense', 'robustness', 'EdgeAI', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00367</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mapping Human Anti-collusion Mechanisms to Multi-agent AI</title><link>https://arxiv.org/abs/2601.00360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a taxonomy of human anti-collusion mechanisms (sanctions, leniency &amp; whistleblowing, monitoring &amp; auditing, market design, governance).&lt;/li&gt;&lt;li&gt;Maps each human mechanism to proposed implementation approaches for multi-agent AI systems.&lt;/li&gt;&lt;li&gt;Highlights open technical challenges including attribution, identity fluidity, the boundary between cooperation and collusion, and adversarial adaptation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamiu Adekunle Idowu', 'Ahmed Almasoud', 'Ayman Alfahid']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent collusion', 'anti-collusion mechanisms', 'attribution', 'adversarial adaptation', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00360</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Uncertainty Quantification for Factual Generation of Large Language Models</title><link>https://arxiv.org/abs/2601.00348</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robust uncertainty quantification method (RU) to detect hallucinations in multi-fact generation by LLMs, especially under non-canonical/adversarial questioning.&lt;/li&gt;&lt;li&gt;Constructs a benchmark of 'trap' questions containing fake names to simulate adversarial prompts aimed at eliciting hallucinations.&lt;/li&gt;&lt;li&gt;Evaluates RU on four models and reports average ROCAUC improvements of ~0.1–0.2 over the best baselines, indicating better detection of fabricated facts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Zhang', 'Zhongliang Yang', 'Linna Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'uncertainty-quantification', 'red-teaming', 'robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00348</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth</title><link>https://arxiv.org/abs/2601.00306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'synthetic reality' as a layered stack (content, identity, interaction, institutions) and expands a taxonomy of GenAI harms across personal, economic, informational, and socio-technical domains.&lt;/li&gt;&lt;li&gt;Identifies qualitative shifts (cost collapse, throughput, customization, micro-segmentation, provenance gaps, trust erosion) that amplify systemic risks and provides a case bank of real-world manifestations (fraud, elections, harassment, documentation, supply-chain compromise).&lt;/li&gt;&lt;li&gt;Proposes a mitigation stack combining provenance infrastructure, platform governance, institutional workflow redesign, and public resilience, and outlines a research agenda to measure 'epistemic security' and mitigate societal trust erosion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emilio Ferrara']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'provenance', 'epistemic-security', 'platform-governance', 'societal-risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00306</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations</title><link>https://arxiv.org/abs/2601.00282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how quantization (multiple techniques and bit widths) affects LLM self-explanations, covering natural language explanations and counterfactual examples.&lt;/li&gt;&lt;li&gt;Reports moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%); a user study shows reduced coherence and trustworthiness (up to 8.5%).&lt;/li&gt;&lt;li&gt;Finds larger models better preserve faithfulness but not necessarily SE quality; no single quantization method dominates across task accuracy, SE quality, and faithfulness.&lt;/li&gt;&lt;li&gt;Recommends validating SE quality for specific applications, especially for NLEs, though overall degradation is relatively minor and quantization remains effective for compression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianli Wang', 'Nils Feldhus', 'Pepa Atanasova', 'Fedor Splitt', 'Simon Ostermann', 'Sebastian M\\"oller', 'Vera Schmitt']&lt;/li&gt;&lt;li&gt;Tags: ['quantization', 'explainability', 'faithfulness', 'safety-evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00282</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</title><link>https://arxiv.org/abs/2601.00269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FaithSCAN, a lightweight single-pass detector for hallucinations in VQA that leverages internal model signals: token-level decoding uncertainty, intermediate visual features, and cross-modal alignment.&lt;/li&gt;&lt;li&gt;Fuses these signals with branch-wise evidence encoding and uncertainty-aware attention, and uses a low-cost, model-dependent automatic supervision strategy (LLM-as-a-Judge extension) to avoid expensive human labels.&lt;/li&gt;&lt;li&gt;Reports superior effectiveness and efficiency over prior external-verification and uncertainty-sampling methods, and analyzes how hallucination patterns vary across VLM architectures and internal signal types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Chen Li', 'Lei Jiang', 'Yanbing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'VQA', 'multimodal safety', 'model-internal signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00269</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications</title><link>https://arxiv.org/abs/2601.00150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FCMBench-V1.0, a large-scale financial credit multimodal benchmark covering 18 certificate types with 4,043 privacy-compliant images and 8,446 QA samples.&lt;/li&gt;&lt;li&gt;Evaluation framework spans three dimensions: Perception (3 tasks), Credit-specific Reasoning (4 tasks), and Robustness (10 real-world acquisition artifact types).&lt;/li&gt;&lt;li&gt;Data are produced via a closed synthesis-capture pipeline to ensure privacy compliance and avoid web-sourced pretraining leakage.&lt;/li&gt;&lt;li&gt;Extensive evaluation of 23 state-of-the-art vision-language models shows model rankings and notable performance drops under acquisition artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yehui Yang', 'Dalu Yang', 'Wenshuo Zhou', 'Fangxin Shang', 'Yifan Liu', 'Jie Ren', 'Haojun Fei', 'Qing Yang', 'Tao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness evaluation', 'Benchmarking', 'Vision-language models', 'Financial domain', 'Privacy-preserving dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00150</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing</title><link>https://arxiv.org/abs/2601.00042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts the Go-Explore algorithm to perform red-team style security testing of tool-using LLM agents (evaluated on GPT-4o-mini) across 28 runs and six research questions.&lt;/li&gt;&lt;li&gt;Finds random-seed variance dominates algorithmic parameter effects (8x outcome spread); multi-seed averaging reduces variance while single-seed comparisons are unreliable.&lt;/li&gt;&lt;li&gt;Shows reward shaping usually harms attack discovery (94% exploration collapse or produces false positives), simple state signatures outperform complex ones, and ensembles increase attack-type diversity.&lt;/li&gt;&lt;li&gt;Concludes that seed variance and targeted domain knowledge can outweigh algorithmic sophistication for safety testing of trained models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manish Bhatt', 'Adrian Wood', 'Idan Habler', 'Ammar Al-Kahfah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00042</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations</title><link>https://arxiv.org/abs/2601.00623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DA-DPO, a difficulty-aware variant of Direct Preference Optimization to reduce hallucinations in multimodal LLMs by reweighting preference pairs according to estimated difficulty.&lt;/li&gt;&lt;li&gt;Difficulty Estimation uses existing pre-trained vision–language models with generative and contrastive objectives, combined via a distribution-aware voting strategy to produce difficulty scores without extra training.&lt;/li&gt;&lt;li&gt;Difficulty-Aware Training down-weights easy preference pairs and emphasizes harder ones to avoid overfitting to easy examples, improving fine-grained hallucination suppression.&lt;/li&gt;&lt;li&gt;Claims consistent improvements in hallucination robustness and generalization across benchmarks while remaining computationally efficient (no new data or extra fine-tuning).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longtian Qiu', 'Shan Ning', 'Chuyu Zhang', 'Jiaxuan Sun', 'Xuming He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'preference optimization', 'alignment/safety', 'multimodal LLMs', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00623</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability</title><link>https://arxiv.org/abs/2601.00240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that LLM-powered agents exhibit intergroup bias and can treat humans as an outgroup under minimal group cues.&lt;/li&gt;&lt;li&gt;Identifies belief dependence: a human-norm script reduces bias only when the agent believes the counterpart is human.&lt;/li&gt;&lt;li&gt;Introduces Belief Poisoning Attacks (BPA) — profile poisoning (BPA-PP) and memory poisoning via belief-refinement suffixes (BPA-MP) — that corrupt persistent identity beliefs to reactivate bias against humans.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing the vulnerability across settings and proposes mitigation strategies at profile and memory boundaries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongwei Wang', 'Bincheng Gu', 'Hongyu Yu', 'Junliang Yu', 'Tao He', 'Jiayin Feng', 'Min Gao']&lt;/li&gt;&lt;li&gt;Tags: ['agent security', 'adversarial attacks', 'memory poisoning', 'bias and alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00240</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</title><link>https://arxiv.org/abs/2601.00138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates selective prediction (abstention) for vision-language models in video question answering to improve reliability in high-stakes deployments.&lt;/li&gt;&lt;li&gt;Finds that confidence-thresholding yields smooth risk-coverage tradeoffs and mechanistic control in-distribution on NExT-QA with Gemini 2.0 Flash.&lt;/li&gt;&lt;li&gt;Examines robustness of abstention control under distribution shift and motivates explicit abstention knobs to maintain predictable error rates.&lt;/li&gt;&lt;li&gt;Empirical study focused on video QA benchmarks and practical mechanisms for predictable abstention behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jorge Ortiz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'selective prediction', 'uncertainty estimation', 'robustness', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00138</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Toward a Physical Theory of Intelligence</title><link>https://arxiv.org/abs/2601.00021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Conservation-Congruent Encoding (CCE) tying information to metastable physical states and defines intelligence as goal-directed work per nat of irreversible information processing.&lt;/li&gt;&lt;li&gt;Derives constraints on information intake, irreversible computation, and work extraction, arguing long-horizon efficiency requires preserving internal informational structure and self-modelling.&lt;/li&gt;&lt;li&gt;Applies the framework to biological systems (brain near efficient operating regime) and develops continuous dynamical circuits generalizing Boolean logic.&lt;/li&gt;&lt;li&gt;Provides a physically grounded perspective on AI safety based on irreversible information flow and structural homeostasis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter David Fagan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Theoretical foundations', 'Robustness', 'Embodied intelligence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00021</guid><pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
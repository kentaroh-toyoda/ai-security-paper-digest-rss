<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 01 Jan 2026 23:21:20 +0000</lastBuildDate><item><title>BadBlocks: Lightweight and Stealthy Backdoor Threat in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2508.03221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadBlocks, a lightweight, stealthy backdoor method that selectively contaminates specific UNet blocks in text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Claims substantial efficiency gains (≈30% computation, 20% GPU time) while maintaining high attack success and minimal perceptual degradation.&lt;/li&gt;&lt;li&gt;Demonstrates evasion of state-of-the-art defenses (notably attention-based detectors) and provides ablations showing only partial fine-tuning of layers is needed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Jiahao Chen', 'Wenjie Wang', 'Bingrong Dai', 'Junjun Yang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'diffusion-models', 'adversarial-ML', 'model-security', 'stealthy-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03221</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</title><link>https://arxiv.org/abs/2502.14780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Instruction Rewriting: converting multimodal (image+instruction) inputs into text-only commands to avoid transmitting vision data.&lt;/li&gt;&lt;li&gt;Provides a dataset of &gt;39,000 examples across 14 domains and develops a compact 250M-parameter VLM fine-tuned for instruction rewriting.&lt;/li&gt;&lt;li&gt;Demonstrates that a quantized model (&lt;500 MB) can effectively perform rewriting using NLG metrics (BLEU, METEOR, ROUGE) and semantic parsing analysis, enabling on-device, privacy-preserving deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhijit Mishra', 'Mingda Li', 'Hsiang Fu', 'Richard Noh', 'Minji Kim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'visual-instruction-rewriting', 'dataset', 'on-device VLM', 'multimodal privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14780</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models</title><link>https://arxiv.org/abs/2501.07033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a GAN-based model to detect deepfake manipulations in online payment images, trained on real payment images and GAN-generated deepfakes (StyleGAN, DeepFake).&lt;/li&gt;&lt;li&gt;Reports high detection performance (above 95%) in distinguishing legitimate transactions from AI-generated deepfakes.&lt;/li&gt;&lt;li&gt;Aims to improve robustness of payment systems against AI-driven fraud and contributes applied techniques for digital security/forensics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zong Ke', 'Shicheng Zhou', 'Yining Zhou', 'Chia Hong Chang', 'Rong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'fraud detection', 'GANs', 'payment security', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.07033</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</title><link>https://arxiv.org/abs/2512.21684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SlideChain: a blockchain-backed provenance framework that anchors cryptographic hashes of multimodal semantic extraction records (concepts and relational triples) to provide tamper-evident auditability and persistent semantic baselines for lecture slides.&lt;/li&gt;&lt;li&gt;Introduces a SlideChain Slides Dataset (1,117 medical imaging lecture slides) and extracts semantic content using four state-of-the-art vision–language models to produce structured provenance records.&lt;/li&gt;&lt;li&gt;Performs systematic analysis of cross-model semantic disagreement, lecture-level variability, and measures deployment metrics (gas usage, throughput, scalability), demonstrating deterministic reproducibility and perfect tamper detection in their setup.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Motaleb Hossen Manik', 'Md Zabirul Islam', 'Ge Wang']&lt;/li&gt;&lt;li&gt;Tags: ['provenance', 'blockchain', 'integrity', 'auditability', 'multimodal VLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21684</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title><link>https://arxiv.org/abs/2511.14554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicFlow, a tri-branch network fusing global visual (ConvNeXt-tiny), fine-grained texture (Swin Transformer-tiny), and spectral noise (CNN with channel attention) forensic cues for deepfake detection.&lt;/li&gt;&lt;li&gt;Uses attention-based temporal pooling to prioritize high-evidence frames and adaptive fusion to weight branches by forgery type.&lt;/li&gt;&lt;li&gt;Trained on CelebDF(v2) with Focal Loss; reports AUC 0.9752, F1 0.9408, accuracy 0.9208, and ablation studies showing branch synergy.&lt;/li&gt;&lt;li&gt;Provides Grad-CAM visualizations validating focus on manipulation regions; claims increased robustness against sophisticated forgeries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Romani']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimedia forensics', 'forgery detection', 'robustness', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14554</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TalkingHeadBench: A Multi-Modal Benchmark &amp; Analysis of Talking-Head DeepFake Detection</title><link>https://arxiv.org/abs/2505.24866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TalkingHeadBench: a multi-model, multi-generator benchmark and curated dataset of talking‑head deepfakes synthesized by leading academic and commercial generators, with open access hosted on Hugging Face.&lt;/li&gt;&lt;li&gt;Provides evaluation protocols designed to assess detector generalization under distribution shifts (identity and generator characteristics) and benchmarks a range of detectors (CNNs, vision transformers, temporal models).&lt;/li&gt;&lt;li&gt;Performs robustness and error analysis (including Grad‑CAM visualizations) to expose common failure modes and detector biases, aiming to drive more robust detection research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinqi Xiong', 'Prakrut Patel', 'Qingyuan Fan', 'Amisha Wadhwa', 'Sarathy Selvam', 'Xiao Guo', 'Luchao Qi', 'Xiaoming Liu', 'Roni Sengupta']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'benchmark', 'robustness', 'generative models', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24866</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text-to-Image Models and Their Representation of People from Different Nationalities Engaging in Activities</title><link>https://arxiv.org/abs/2504.06313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Generated 2,060 images across 206 nationalities and five everyday activities using DALL-E 3 and Gemini 3 Pro Preview to study how people from different nationalities are depicted.&lt;/li&gt;&lt;li&gt;Found that 28.4% of images depicted traditional attire, with statistically significant overrepresentation for Middle East &amp; North Africa and Sub-Saharan Africa and correlations with World Bank income groups; impractical attire also appeared in athletics scenarios.&lt;/li&gt;&lt;li&gt;Evaluated image-text alignment with CLIP, ALIGN, and GPT-4.1 mini (9,270 image-prompt pairs) and found higher alignment scores for traditional-attire images when prompts included country names; one model often inserted the word "traditional" during prompt revision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abdulkareem Alsudais']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'representation', 'image-text alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06313</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs</title><link>https://arxiv.org/abs/2406.17126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MM-SpuBench, a human-verified benchmark of image-class pairs annotated with core and spurious attributes, based on a taxonomy of nine types of spurious correlations.&lt;/li&gt;&lt;li&gt;Proposes an evaluation metric, Conditional Generation Likelihood Advantage (CGLA), alongside standard accuracy to measure reliance on spurious correlations in MLLMs.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art open-source and proprietary multimodal LLMs on the benchmark and finds persistent dependence on spurious correlations and difficulty of mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqian Ye', 'Bohan Liu', 'Guangtao Zheng', 'Di Wang', 'Yunsheng Ma', 'Xu Cao', 'Bolin Lai', 'James M. Rehg', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['spurious-bias', 'robustness', 'multimodal-llm', 'benchmark', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17126</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Generative Classifiers Avoid Shortcut Solutions</title><link>https://arxiv.org/abs/2512.25034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows generative classifiers (class-conditional generative models) can avoid shortcut learning by modeling all features rather than relying on spurious correlations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on five standard image and text distribution-shift benchmarks using diffusion-based and autoregressive generative classifiers.&lt;/li&gt;&lt;li&gt;Claims simple training requirements (no specialized augmentations, heavy regularization, extra hyperparameters, or prior knowledge of spurious cues) and validates on realistic domains (medical, satellite).&lt;/li&gt;&lt;li&gt;Provides theoretical insight via a Gaussian toy model analyzing inductive biases and conditions under which generative classifiers outperform discriminative ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander C. Li', 'Ananya Kumar', 'Deepak Pathak']&lt;/li&gt;&lt;li&gt;Tags: ['generative-classifiers', 'distribution-shift', 'robustness', 'spurious-correlations', 'evaluation-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.25034</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways</title><link>https://arxiv.org/abs/2512.24499</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Diffusion Sanitization (ADS), a training-free defense that neutralizes payloads embedded by diffusion-based steganography rather than detecting them.&lt;/li&gt;&lt;li&gt;Uses an off-the-shelf pretrained denoiser as a differentiable proxy for diffusion decoders and a color-aware, quaternion-coupled update rule to limit perceptual distortion.&lt;/li&gt;&lt;li&gt;Evaluated against a state-of-the-art diffusion steganography method (Pulsar) under a practical threat model, driving decoder success rates to near zero with minimal visual impact.&lt;/li&gt;&lt;li&gt;Claims a better security-utility trade-off compared to standard content transformations, making it suitable for deployment at security gateways.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vladimir Frants', 'Sos Agaian']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-steganography', 'steganography-defense', 'adversarial-sanitization', 'image-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24499</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GARDO: Reinforcing Diffusion Models without Reward Hacking</title><link>https://arxiv.org/abs/2512.24138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking and mode collapse when using proxy rewards to fine-tune diffusion (text-to-image) models with RL.&lt;/li&gt;&lt;li&gt;Proposes GARDO: selective (gated) regularization applied to high-uncertainty samples, adaptive updates of the reference model, and diversity-aware reward amplification to encourage mode coverage.&lt;/li&gt;&lt;li&gt;Shows empirically that GARDO reduces reward hacking and improves generation diversity without sacrificing sample efficiency or exploration across multiple proxy and unseen hold-out metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran He', 'Yuxiao Ye', 'Jie Liu', 'Jiajun Liang', 'Zhiyong Wang', 'Ziyang Yuan', 'Xintao Wang', 'Hangyu Mao', 'Pengfei Wan', 'Ling Pan']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment/safety', 'reinforcement learning', 'diffusion models', 'diversity/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24138</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DarkEQA, a benchmark for evaluating embodied question answering (EQA) perceptual primitives under multi-level low-light conditions.&lt;/li&gt;&lt;li&gt;Isolates the perception bottleneck by evaluating QA from egocentric observations with controlled, physics-based degradations modeled in linear RAW space and rendered via an ISP-like pipeline.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art vision-language models (VLMs) and low-light image enhancement (LLIE) methods to quantify VLM performance degradations in dark environments.&lt;/li&gt;&lt;li&gt;Provides an open-source dataset and code (upon acceptance) to enable systematic robustness analysis for 24/7 embodied agent operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yohan Park', 'Hyunwoo Ha', 'Wonjun Jo', 'Tae-Hyun Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'vision-language models', 'low-light/perception', 'embodied QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24985</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions</title><link>https://arxiv.org/abs/2512.24971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates quantization, pruning, and weight clustering (individually and combined) on ResNet-50, VGG-19, and MobileNetV2.&lt;/li&gt;&lt;li&gt;Uses CIFAR-10-C and CIFAR-100-C to analyze trade-offs between robustness to natural corruptions, accuracy, and compression ratio.&lt;/li&gt;&lt;li&gt;Finds that certain compression strategies can preserve or improve robustness, particularly for more complex architectures.&lt;/li&gt;&lt;li&gt;Performs multi-objective assessment to identify compression configurations that balance robustness, accuracy, and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itallo Patrick Castro Alves Da Silva', 'Emanuel Adler Medeiros Pereira', 'Erick de Andrade Barboza', 'Baldoino Fonseca dos Santos Neto', 'Marcio de Medeiros Ribeiro']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model-compression', 'benchmarking', 'natural-corruptions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24971</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VIPER: Process-aware Evaluation for Generative Video Reasoning</title><link>https://arxiv.org/abs/2512.24952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VIPER, a process-aware benchmark of 16 tasks for evaluating generative video reasoning (GVR) across temporal, structural, symbolic, spatial, physics, and planning reasoning.&lt;/li&gt;&lt;li&gt;Proposes Process-outcome Consistency (POC@r), a metric using a VLM-as-judge and hierarchical rubric to assess validity of intermediate generation steps and final outcomes.&lt;/li&gt;&lt;li&gt;Finds state-of-the-art video models score ~20% POC@1.0, revealing widespread outcome-hacking and gaps in sampling/test-time scaling robustness.&lt;/li&gt;&lt;li&gt;Aims to highlight the difference between single-frame outcome evaluation and true process-consistent visual reasoning; benchmark will be released publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Li', 'Yukai Gu', 'Yingqian Min', 'Zikang Liu', 'Yifan Du', 'Kun Zhou', 'Min Yang', 'Wayne Xin Zhao', 'Minghui Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation', 'robustness', 'safety-evaluation', 'benchmarking', 'generative-video-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24952</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</title><link>https://arxiv.org/abs/2512.24792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a projection-based physical adversarial attack that projects perturbation light onto objects to fool monocular depth estimation (MDE) models.&lt;/li&gt;&lt;li&gt;Uses physics-in-the-loop (PITL) optimization and a distributed CMA-ES to evaluate candidate perturbations in real environments accounting for device specifics and disturbances.&lt;/li&gt;&lt;li&gt;Demonstrates successful real-world attacks causing depth misestimations and parts of objects to disappear from reconstructed scenes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Takeru Kusakabe', 'Yudai Hirose', 'Mashiho Mukaida', 'Satoshi Ono']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'physical-adversarial', 'robustness', 'monocular-depth-estimation', 'physics-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24792</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks</title><link>https://arxiv.org/abs/2512.24592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SliceLens, a hypothesis-driven framework using LLMs and VLMs to generate and verify grounded visual failure hypotheses and discover fine-grained error slices in multi-instance vision tasks (detection, segmentation, pose).&lt;/li&gt;&lt;li&gt;Introduces FeSD, a new benchmark with expert-annotated, precisely grounded ground-truth slices for evaluating fine-grained error slice discovery across instance-level vision tasks.&lt;/li&gt;&lt;li&gt;Shows improved slice discovery performance (e.g., Precision@10 on FeSD) and demonstrates that discovered interpretable slices can guide actionable model repairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Zhang', 'Chaoqun Wang', 'Zixuan Guan', 'Sam Kao', 'Pengfei Zhao', 'Peng Wu', 'Sifeng He']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'error slice discovery', 'vision models', 'LLM/VLM grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24592</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.24331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LVLDrive, a framework that augments pre-trained Vision-Language Models with LiDAR point-cloud input to provide explicit 3D metric spatial understanding for autonomous driving.&lt;/li&gt;&lt;li&gt;Introduces a Gradual Fusion Q-Former to incrementally inject LiDAR features so as to avoid catastrophic disturbance of the pre-trained VLM's knowledge.&lt;/li&gt;&lt;li&gt;Creates a spatial-aware question-answering (SA-QA) dataset and demonstrates improved metric spatial perception, scene understanding, and driving decision-making over vision-only baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weijie Wei', 'Zhipeng Luo', 'Ling Feng', 'Venice Erin Liong']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'vision-language-models', 'multimodal-fusion', '3D-perception', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24331</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</title><link>https://arxiv.org/abs/2512.24271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DualityForge, a controllable diffusion-based counterfactual video editing pipeline to synthesize edited videos and corresponding QA pairs for contrastive training.&lt;/li&gt;&lt;li&gt;Constructs DualityVidQA, a large-scale paired dataset of original and counterfactual videos with QA, aimed at reducing MLLM visual-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Proposes Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT+RL regime with pair-wise L1 advantage normalization to stabilize and improve policy optimization.&lt;/li&gt;&lt;li&gt;Reports substantial reduction in hallucinations (e.g., 24.0% relative improvement over Qwen2.5-VL-7B) and gains on general-purpose benchmarks, demonstrating improved robustness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Huang', 'Hao Wen', 'Aiming Hao', 'Bingze Song', 'Meiqi Wu', 'Jiahong Wu', 'Xiangxiang Chu', 'Sheng Lu', 'Haoqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'counterfactual data synthesis', 'multimodal model safety/robustness', 'dataset (DualityVidQA)', 'RL fine-tuning / training stabilization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24271</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Bayesian Self-Distillation for Image Classification</title><link>https://arxiv.org/abs/2512.24162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bayesian Self-Distillation (BSD): construct sample-specific soft target distributions via Bayesian inference from the model's own predictions and eschew hard targets after initialization.&lt;/li&gt;&lt;li&gt;Reports consistent gains in test accuracy and large reductions in Expected Calibration Error (e.g., +1.4% accuracy and -40% ECE for ResNet-50 on CIFAR-100) across architectures and datasets.&lt;/li&gt;&lt;li&gt;Shows improved robustness to data corruptions, perturbations, and label noise; when combined with a contrastive loss, achieves state-of-the-art robustness under label noise for single-stage, single-network methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anton Adel\\"ow', 'Matteo Gamba', 'Atsuto Maki']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'calibration', 'label-noise-robustness', 'self-distillation', 'bayesian-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24162</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning</title><link>https://arxiv.org/abs/2512.24146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes Preference Mode Collapse (PMC) in text-to-image diffusion models trained with RLHF, a form of reward hacking that reduces generative diversity.&lt;/li&gt;&lt;li&gt;Introduces DivGenBench, a benchmark to quantify PMC and measure diversity vs. reward optimization trade-offs.&lt;/li&gt;&lt;li&gt;Proposes Directional Decoupling Alignment (D^2-Align), which learns a directional correction in the reward model embedding space and applies it during optimization to mitigate collapse while keeping the generator frozen.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chubin Chen', 'Sujie Hu', 'Jiashu Zhu', 'Meiqi Wu', 'Jintao Chen', 'Yanxun Li', 'Nisha Huang', 'Chengyu Fang', 'Jiahong Wu', 'Xiangxiang Chu', 'Xiu Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-hacking', 'RLHF', 'diffusion-models', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24146</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks</title><link>https://arxiv.org/abs/2512.24111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a training-free, diffusion-based conditional generation framework to create naturalistic, scene-consistent adversarial objects targeting monocular depth estimation (MDE).&lt;/li&gt;&lt;li&gt;Proposes a Salient Region Selection module to locate regions most influential to MDE and a Jacobian Vector Product Guidance mechanism to align adversarial gradients with the pre-trained diffusion model.&lt;/li&gt;&lt;li&gt;Demonstrates that generated adversarial objects produce large depth estimation shifts and validates effectiveness, stealthiness, and physical deployability via extensive digital and real-world experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongtao Chen', 'Yanbo Wang', 'Wentao Zhao', 'Guole Shen', 'Tianchen Deng', 'Jingchuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'physical adversarial examples', 'monocular depth estimation', 'diffusion models', 'autonomous driving safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24111</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models</title><link>https://arxiv.org/abs/2512.23953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T2VAttack, framing adversarial attacks on text-to-video (T2V) diffusion models with two attack objectives: semantic (video-text alignment) and temporal (temporal dynamics).&lt;/li&gt;&lt;li&gt;Proposes two prompt-level attack methods: T2VAttack-S (greedy synonym replacement of critical words) and T2VAttack-I (iterative insertion of optimized minimal-perturbation words).&lt;/li&gt;&lt;li&gt;Evaluates these attacks across several state-of-the-art T2V models (ModelScope, CogVideoX, Open-Sora, HunyuanVideo) and shows single-word modifications can substantially degrade semantic fidelity and temporal coherence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changzhen Li', 'Yuecong Min', 'Jie Zhang', 'Zheng Yuan', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text-to-video', 'diffusion models', 'prompt attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23953</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method combining explicit CI (contextual integrity) reasoning prompts with reinforcement learning to reduce inappropriate information disclosure by LLMs.&lt;/li&gt;&lt;li&gt;Trains on a small synthetic dataset (~700 examples) with diverse contexts and disclosure norms and demonstrates reduced privacy leakage while preserving task performance across model sizes/families.&lt;/li&gt;&lt;li&gt;Shows transfer of improvements to a human-annotated CI/privacy benchmark (PrivacyLens) evaluating assistant actions and tool calls.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'contextual integrity', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title><link>https://arxiv.org/abs/2504.16628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ParetoHqD: an offline multiobjective alignment method that represents human preferences as directions in objective space and treats data near the Pareto front as high-quality.&lt;/li&gt;&lt;li&gt;For each preference direction, applies a two-stage supervised fine-tuning pipeline where each stage uses a Pareto high-quality training subset matched to that preference.&lt;/li&gt;&lt;li&gt;Addresses issues from imbalanced reward scores and inappropriate preference representations; shows empirical gains over five baselines on two multiobjective alignment tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gu', 'Handing Wang', 'Yi Mei', 'Mengjie Zhang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multiobjective optimization', 'supervised fine-tuning', 'Pareto front', 'reward modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16628</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Effective and Efficient Jailbreaks of Black-Box LLMs with Cross-Behavior Attacks</title><link>https://arxiv.org/abs/2503.08990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JCB, a black-box jailbreak method that reuses successful past behavior prompts to efficiently discover new jailbreaks without relying on auxiliary LLM calls.&lt;/li&gt;&lt;li&gt;Claims major efficiency gains: up to 94% fewer queries and 12.9% higher average attack success vs. baselines.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on resilient models (e.g., 37% success on Llama-2-7B) and shows zero-shot transferability across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasudev Gohil']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'Black-box adversarial attacks', 'Prompt injection / adversarial prompting', 'Red teaming', 'Security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08990</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs</title><link>https://arxiv.org/abs/2505.24830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an atomic fact-checking framework that decomposes LLM-generated medical answers into discrete, verifiable 'atomic facts' and verifies each against an authoritative medical guideline knowledge base.&lt;/li&gt;&lt;li&gt;Uses retrieval-augmented grounding to trace each atomic fact back to relevant source chunks, enabling targeted correction of errors and granular explainability.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to ~40% overall answer improvement and ~50% hallucination detection) validated via multi-reader medical expert assessments and automated benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juraj Vladika', 'Annika Domres', 'Mai Nguyen', 'Rebecca Moser', 'Jana Nano', 'Felix Busch', 'Lisa C. Adams', 'Keno K. Bressem', 'Denise Bernhardt', 'Stephanie E. Combs', 'Kai J. Borm', 'Florian Matthes', 'Jan C. Peeken']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Hallucination detection', 'Retrieval-augmented generation', 'Atomic fact-checking', 'Medical QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24830</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</title><link>https://arxiv.org/abs/2502.14780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Instruction Rewriting: converting multimodal (image+instruction) inputs into text-only commands to avoid transmitting vision data.&lt;/li&gt;&lt;li&gt;Provides a dataset of &gt;39,000 examples across 14 domains and develops a compact 250M-parameter VLM fine-tuned for instruction rewriting.&lt;/li&gt;&lt;li&gt;Demonstrates that a quantized model (&lt;500 MB) can effectively perform rewriting using NLG metrics (BLEU, METEOR, ROUGE) and semantic parsing analysis, enabling on-device, privacy-preserving deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhijit Mishra', 'Mingda Li', 'Hsiang Fu', 'Richard Noh', 'Minji Kim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'visual-instruction-rewriting', 'dataset', 'on-device VLM', 'multimodal privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14780</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Positional Biases in Text Embedding Models</title><link>https://arxiv.org/abs/2412.15241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that text embedding models disproportionately prioritize content at the beginning of inputs regardless of positional encoding.&lt;/li&gt;&lt;li&gt;Ablation experiments show inserting or removing text at the start reduces cosine similarity with the original embedding up to 12.3% more than analogous changes at the end.&lt;/li&gt;&lt;li&gt;Regression analyses indicate sentence importance declines with distance from the start even when content is controlled for, suggesting content-agnostic positional sensitivity.&lt;/li&gt;&lt;li&gt;Authors hypothesize causes in preprocessing and chosen positional encodings and highlight implications for retrieval robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reagan J. Lee', 'Samarth Goel', 'Kannan Ramchandran']&lt;/li&gt;&lt;li&gt;Tags: ['embeddings', 'positional-bias', 'robustness', 'information-retrieval', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15241</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots</title><link>https://arxiv.org/abs/2412.04235</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes combining Retrieval-Augmented Generation (RAG) for mitigation with a new Negative Missing Information Scoring System (NMISS) for detection/refinement of hallucination evaluation.&lt;/li&gt;&lt;li&gt;Evaluates the approach on Italian health news article contexts and compares multiple LLMs (Gemma2, GPT-4, Llama2/3, Mistral).&lt;/li&gt;&lt;li&gt;Finds GPT-4 and Gemma2 perform best; NMISS particularly helps mid-tier models by recognizing contextually informative responses that standard metrics might flag as hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Paola Priola']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'safety evaluation', 'alignment', 'healthcare']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.04235</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Iterative Deployment Improves Planning Skills in LLMs</title><link>https://arxiv.org/abs/2512.24940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative deployment—repeatedly fine-tuning LLMs on user-curated data from prior deployments—can substantially improve planning skills and produce emergent longer-horizon plans.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis that iterative deployment implements an outer-loop reinforcement learning process with an implicit (not explicitly defined) reward function.&lt;/li&gt;&lt;li&gt;Highlights safety implications: the implicit reward entailed by repeated deployment may be unspecified and produce unintended properties in future models, and suggests iterative deployment as an alternative training regime to explicit RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Augusto B. Corr\\^ea', 'Yoav Gelberg', 'Luckeciano C. Melo', 'Ilia Shumailov', "Andr\\'e G. Pereira", 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'deployment dynamics', 'emergent capabilities', 'implicit reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24940</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>https://arxiv.org/abs/2512.24052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AHA (Audio Hallucination Alignment) to reduce hallucinations in large audio-language models via counterfactual hard negative mining and preference data.&lt;/li&gt;&lt;li&gt;Defines a taxonomy of audio grounding errors: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error.&lt;/li&gt;&lt;li&gt;Introduces AHA-Eval, a fine-grained diagnostic benchmark, and demonstrates improvements by aligning Qwen2.5-Omni to produce Qwen-Audio-AHA with gains on AHA-Eval and public benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Chen', 'Wenhui Zhu', 'Xiwen Chen', 'Zhipeng Wang', 'Xin Li', 'Peijie Qiu', 'Hao Wang', 'Xuanzhao Dong', 'Yujian Xiong', 'Anderson Schneider', 'Yuriy Nevmyvaka', 'Yalin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety-evaluation', 'adversarial-training', 'audio-llms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24052</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?</title><link>https://arxiv.org/abs/2512.24044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of jailbreak attacks against LLM safety across the full inference pipeline, explicitly including input and output content filters.&lt;/li&gt;&lt;li&gt;Finds nearly all evaluated jailbreak techniques are detectable by at least one safety filter, implying prior model-only assessments may overestimate real-world attack success.&lt;/li&gt;&lt;li&gt;Highlights trade-offs between recall and precision in safety filters and identifies gaps for improving detection accuracy and user experience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Xin', 'Dingfan Chen', 'Linyi Yang', 'Michael Backes', 'Xiao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM safety', 'adversarial prompts', 'content moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24044</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models</title><link>https://arxiv.org/abs/2512.23850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Drill-Down and Fabricate Test (DDFT), a protocol to measure epistemic robustness: the ability of language models to maintain factual accuracy under progressive semantic compression and adversarial fabrication.&lt;/li&gt;&lt;li&gt;Proposes a two-system cognitive model (Semantic System for generation and Epistemic Verifier for factual validation) and operationalizes tests across 9 frontier models, 8 knowledge domains, and 5 compression levels (1,800 turn-level evaluations).&lt;/li&gt;&lt;li&gt;Finds epistemic robustness is largely independent of model scale or architecture but strongly predicted by error-detection/verification capability, and shows flagship models can be brittle while some smaller models are more robust.&lt;/li&gt;&lt;li&gt;Provides a practical benchmarking framework and tools for assessing epistemic robustness prior to deployment in critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Baxi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial evaluation/red teaming', 'epistemic verification', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23850</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI</title><link>https://arxiv.org/abs/2512.24848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PrivacyBench, a multi-turn conversational benchmark with socially grounded datasets that embed user 'secrets' to evaluate secret preservation in personalized AI agents.&lt;/li&gt;&lt;li&gt;Evaluates Retrieval-Augmented Generation (RAG) assistants and finds secret leakage up to 26.56%; a privacy-aware prompt reduces leakage to 5.12% but does not fully mitigate the issue.&lt;/li&gt;&lt;li&gt;Identifies a systemic vulnerability: retrieval mechanisms indiscriminately surface sensitive data, shifting privacy responsibility onto the generator and creating a single point of failure; calls for privacy-by-design architectural safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srija Mukhopadhyay', 'Sathwik Reddy', 'Shruthi Muthukumar', 'Jisun An', 'Ponnurangam Kumaraguru']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-leakage', 'LLM-security', 'RAG', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24848</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models</title><link>https://arxiv.org/abs/2512.24693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that standard preference datasets focusing only on final-turn responses fail to capture multi-turn conversational nuances for reward models.&lt;/li&gt;&lt;li&gt;Proposes MUSIC, an unsupervised data augmentation method that synthesizes contrastive multi-turn conversation pairs to train multi-turn reward models.&lt;/li&gt;&lt;li&gt;Implements MUSIC on the Skywork preference dataset and trains a Gemma-2-9B-Instruct-based RM, showing improved agreement with advanced LLM judges on multi-turn evaluations while retaining single-turn RM performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenzhe Li', 'Shujian Zhang', 'Wenxuan Zhou', 'John Lambert', 'Chi Jin', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multi-turn reward models', 'evaluation', 'alignment', 'data augmentation', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24693</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Large Language Models Know What They Are Capable Of?</title><link>https://arxiv.org/abs/2512.24661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLMs can predict their own success on tasks and whether those predictions improve during multi-step tasks; finds widespread overconfidence but above-random discriminatory power.&lt;/li&gt;&lt;li&gt;Shows that larger/newer models do not necessarily have better self-assessment (except Claude series), and that overconfidence can worsen through multi-step agentic reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates that in-context failure examples can reduce overconfidence for some models, improving decision-making; models' decisions are approximately rational given their biased probability estimates.&lt;/li&gt;&lt;li&gt;Discusses implications for AI misuse and misalignment due to models' lack of accurate awareness of their capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Casey O. Barkan', 'Sid Black', 'Oliver Sourbut']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'agentic safety', 'capability awareness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24661</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</title><link>https://arxiv.org/abs/2512.24574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies attention heads correlated with distinct cognitive behaviors (e.g., verification, backtracking) in LLM reasoning trajectories.&lt;/li&gt;&lt;li&gt;Proposes CREST, a training-free method with an offline calibration to derive head-specific steering vectors and an inference-time rotation to suppress unproductive components.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy (up to 17.5%) and reduced token usage (up to 37.6%) across reasoning benchmarks by adaptively suppressing inefficient reasoning modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Xiaoxia Wu', 'Zhongzhu Zhou', 'Qingyang Wu', 'Yineng Zhang', 'Pragaash Ponnusamy', 'Harikaran Subbaraj', 'Jue Wang', 'Shuaiwen Leon Song', 'Ben Athiwaratkun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model steering', 'interpretability', 'inference-time intervention', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24574</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering</title><link>https://arxiv.org/abs/2512.24562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HaluNet, a lightweight multi-branch neural framework that fuses token-level probabilistic confidence, distributional uncertainty, and semantic embeddings to detect hallucinations in LLM question answering.&lt;/li&gt;&lt;li&gt;Aims for efficient, one-pass detection using internal uncertainty signals (no external knowledge retrieval required), suitable for real-time deployment.&lt;/li&gt;&lt;li&gt;Evaluated on SQuAD, TriviaQA, and Natural Questions, showing improved hallucination-detection performance and favorable computational efficiency both with and without access to context.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaodong Tong', 'Qi Zhang', 'Jiayang Gao', 'Lei Jiang', 'Yanbing Liu', 'Nannan Sun']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'uncertainty estimation', 'alignment', 'real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24562</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</title><link>https://arxiv.org/abs/2512.24556</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic safety audit of three state-of-the-art LLMs (GPT-5.1, Gemini 3 Pro, Claude 4.5 Opus) using HausaSafety, an adversarial dataset grounded in West African threat scenarios.&lt;/li&gt;&lt;li&gt;Finds Complex Interference: safety is context-dependent across language (English vs. Hausa) and temporal framing, including a Reverse Linguistic effect (Claude safer in Hausa than English) and a strong Temporal Asymmetry (past-tense prompts more likely to bypass defenses than future-tense).&lt;/li&gt;&lt;li&gt;Demonstrates large volatility in safety (up to 9.2x disparity across configurations), argues models rely on superficial heuristics, and proposes 'Invariant Alignment' to stabilize safety across linguistic and temporal shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Abdullahi Said', 'Muhammad Sammani Sani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'multilingual safety', 'adversarial dataset', 'temporal vulnerabilities', 'safety evaluation / red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24556</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring</title><link>https://arxiv.org/abs/2512.24181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedKGI, a diagnostic framework that grounds LLM reasoning in a medical knowledge graph to reduce hallucinated medical content.&lt;/li&gt;&lt;li&gt;Selects next questions using information gain to make inquiries more discriminative and improve diagnostic efficiency.&lt;/li&gt;&lt;li&gt;Uses an OSCE-format structured state for consistent evidence tracking across multi-turn dialogues, improving coherence and reducing contradictions.&lt;/li&gt;&lt;li&gt;Reports improved diagnostic accuracy and ~30% average dialogue efficiency gains over strong LLM baselines on clinical benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qipeng Wang', 'Rui Sheng', 'Yafei Li', 'Huamin Qu', 'Yushi Sun', 'Min Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge grounding', 'hallucination mitigation', 'dialogue coherence', 'information-gain inquiry', 'clinical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24181</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models</title><link>https://arxiv.org/abs/2512.24058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Composite Reliability Score (CRS) that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric for LLM reliability.&lt;/li&gt;&lt;li&gt;Evaluates ten open-source LLMs across five QA datasets under baseline, input perturbations, and calibration methods, showing CRS yields stable model rankings and uncovers failure modes missed by single metrics.&lt;/li&gt;&lt;li&gt;Finds the most dependable systems balance accuracy, robustness, and calibrated uncertainty, and demonstrates CRS as a tool for comparing models and measuring reliability improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Kumar Salla', 'Manoj Saravanan', 'Shrikar Reddy Kota']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'calibration', 'uncertainty-quantification', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24058</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title><link>https://arxiv.org/abs/2512.23988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RISE, an unsupervised method using sentence-level step activations and sparse auto-encoders to discover 'reasoning vectors'—directions in activation space corresponding to distinct reasoning behaviors.&lt;/li&gt;&lt;li&gt;Shows that discovered vectors are disentangled and occupy separable regions; interventions on decoder columns can amplify/suppress behaviors (e.g., reflection, backtracking) and alter inference trajectories without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates discovery of structural properties (e.g., response length) and novel controllable behaviors (e.g., response confidence), suggesting uses for interpreting and steering LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Shujian Zhang', 'John Lambert', 'Wenxuan Zhou', 'Zhangyang Wang', 'Mingqing Chen', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'model steering', 'latent interventions', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23988</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title><link>https://arxiv.org/abs/2512.23837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial attack method that extracts token-level predictions from intermediate attention layers to generate perturbations, differing from prompt- or gradient-based attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on argument quality assessment (ArgQuality) using LLaMA-3.1-Instruct-8B as both generator and evaluator, showing measurable drops in evaluation performance.&lt;/li&gt;&lt;li&gt;Finds generated adversarial examples are semantically similar to originals but notes practical limitations due to grammatical degradation for substitutions from certain layers/positions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Dhole']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'mechanistic interpretability', 'LLM robustness', 'red teaming', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23837</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</title><link>https://arxiv.org/abs/2512.23836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive prompting strategy for retrieval-augmented QA that splits retrieved context into smaller chunks and sequentially prompts the LLM to answer using each chunk, trading off relevance vs. noise.&lt;/li&gt;&lt;li&gt;Shows adaptive chunking can match standard prompting performance while using fewer tokens across three open-domain QA datasets.&lt;/li&gt;&lt;li&gt;Finds a major error mode: when information is insufficient, the LLM often produces incorrect answers instead of declining, indicating poor refusal/ignorance behavior.&lt;/li&gt;&lt;li&gt;Highlights need for further research to improve LLMs' ability to admit ignorance or decline when evidence is lacking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingmin Wang', 'Ji Ma', 'Shankar Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'retrieval-augmented generation', 'hallucination', 'uncertainty calibration', 'question answering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23836</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Noise-Driven Persona Formation in Reflexive Neural Language Generation</title><link>https://arxiv.org/abs/2512.23716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Luca-Noise Reflex Protocol (LN-RP): injects stochastic noise seeds into initial generation state to study persona emergence over repeated/reflexive generation cycles.&lt;/li&gt;&lt;li&gt;Finds three stable persona modes with distinct entropy signatures and demonstrates noise-driven phase transitions that reliably induce and retain personas across cycles.&lt;/li&gt;&lt;li&gt;Provides quantitative evaluation showing consistent persona retention and significant differences across modes (p &lt; 0.01) and presents a reproducible protocol for studying long-range linguistic coherence and emergent behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toshiyuki Shigemura']&lt;/li&gt;&lt;li&gt;Tags: ['emergent-behavior', 'alignment', 'jailbreak-risk', 'robustness', 'evaluation-protocol']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23716</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability</title><link>https://arxiv.org/abs/2512.23712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STED (Semantic Tree Edit Distance), a new similarity metric for comparing LLM-generated JSON that balances semantic flexibility with structural strictness.&lt;/li&gt;&lt;li&gt;Proposes a consistency scoring framework that aggregates repeated STED measurements to quantify reliability of structured outputs across generations.&lt;/li&gt;&lt;li&gt;Evaluates STED against existing metrics (TED, BERTScore, DeepDiff) on synthetic datasets and reports superior discrimination between semantic equivalents and structural breaks.&lt;/li&gt;&lt;li&gt;Applies the framework to benchmark six LLMs, identifying model-specific consistency behaviors and demonstrating practical uses for model selection and prompt refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanghui Wang', 'Jinze Yu', 'Xing Zhang', 'Dayuan Jiang', 'Yin Song', 'Tomal Deb', 'Xuefeng Liu', 'Peiyang He']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'structured-output-evaluation', 'metrics', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23712</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations</title><link>https://arxiv.org/abs/2512.23711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAT, a framework to evaluate and visualize the trade-off between accuracy and response consistency of LLMs under controlled input variations, demonstrated on multiple-choice benchmarks.&lt;/li&gt;&lt;li&gt;Defines Consistency-Accuracy Relation (CAR) curves and Minimum-Consistency Accuracy (MCA) to show how accuracy varies as consistency requirements increase.&lt;/li&gt;&lt;li&gt;Proposes the CORE index, combining CAR curve area and shape into a single metric to quantify consistency-oriented robustness, and evaluates it across generalist and domain-specific LLMs.&lt;/li&gt;&lt;li&gt;Outlines how CAT can be extended to open-ended tasks via adaptable scoring functions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulo Cavalin', 'Cassia Sanctos', 'Marcelo Grave', 'Claudio Pinhanez', 'Yago Primerano']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'consistency', 'evaluation-metrics', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23711</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</title><link>https://arxiv.org/abs/2512.23126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations of Direct Preference Optimization (DPO): dependence on arbitrary scalarization and reference policies, and failure to use comparative pairwise information.&lt;/li&gt;&lt;li&gt;Proposes Intrinsic Self-reflective Preference Optimization (InSPO), which conditions the optimal policy on both context and alternative responses to enable intrinsic self-reflection.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees of invariance to scalarization and reference choices and claims superiority over DPO/RLHF.&lt;/li&gt;&lt;li&gt;Empirical results show improved win rates and length-controlled metrics with no architectural changes or inference overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Li', 'Tian Lan', 'Zhengling Qi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'RLHF/DPO', 'LLM training', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23126</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title><link>https://arxiv.org/abs/2511.14554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicFlow, a tri-branch network fusing global visual (ConvNeXt-tiny), fine-grained texture (Swin Transformer-tiny), and spectral noise (CNN with channel attention) forensic cues for deepfake detection.&lt;/li&gt;&lt;li&gt;Uses attention-based temporal pooling to prioritize high-evidence frames and adaptive fusion to weight branches by forgery type.&lt;/li&gt;&lt;li&gt;Trained on CelebDF(v2) with Focal Loss; reports AUC 0.9752, F1 0.9408, accuracy 0.9208, and ablation studies showing branch synergy.&lt;/li&gt;&lt;li&gt;Provides Grad-CAM visualizations validating focus on manipulation regions; claims increased robustness against sophisticated forgeries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Romani']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimedia forensics', 'forgery detection', 'robustness', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14554</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title><link>https://arxiv.org/abs/2510.17884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of open-source LLMs (TinyLLaMA, Falcon-RW-1B, Flan-T5) for password guessing using synthetic user profiles and structured attributes (name, birthdate, hobbies).&lt;/li&gt;&lt;li&gt;Measured performance with Hit@1/5/10 under plaintext and SHA-256 comparisons; all models perform poorly (under 1.5% at Hit@10).&lt;/li&gt;&lt;li&gt;Compared LLMs to traditional rule-based and combinator-based cracking methods, which significantly outperform the LLMs.&lt;/li&gt;&lt;li&gt;Analysis identifies LLM limitations for this domain: weak domain adaptation, insufficient memorization of leaked-password patterns, and lack of benefit without supervised fine-tuning on password datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Abdul Rehman', 'Syed Imad Ali Shah', 'Abbas Anwar', 'Noor Islam', 'Hamid Khan']&lt;/li&gt;&lt;li&gt;Tags: ['password cracking', 'LLM evaluation', 'adversarial/security', 'privacy', 'model capabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17884</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method combining explicit CI (contextual integrity) reasoning prompts with reinforcement learning to reduce inappropriate information disclosure by LLMs.&lt;/li&gt;&lt;li&gt;Trains on a small synthetic dataset (~700 examples) with diverse contexts and disclosure norms and demonstrates reduced privacy leakage while preserving task performance across model sizes/families.&lt;/li&gt;&lt;li&gt;Shows transfer of improvements to a human-annotated CI/privacy benchmark (PrivacyLens) evaluating assistant actions and tool calls.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'contextual integrity', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs</title><link>https://arxiv.org/abs/2406.17126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MM-SpuBench, a human-verified benchmark of image-class pairs annotated with core and spurious attributes, based on a taxonomy of nine types of spurious correlations.&lt;/li&gt;&lt;li&gt;Proposes an evaluation metric, Conditional Generation Likelihood Advantage (CGLA), alongside standard accuracy to measure reliance on spurious correlations in MLLMs.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art open-source and proprietary multimodal LLMs on the benchmark and finds persistent dependence on spurious correlations and difficulty of mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqian Ye', 'Bohan Liu', 'Guangtao Zheng', 'Di Wang', 'Yunsheng Ma', 'Xu Cao', 'Bolin Lai', 'James M. Rehg', 'Aidong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['spurious-bias', 'robustness', 'multimodal-llm', 'benchmark', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17126</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Myopically Verifiable Probabilistic Certificates for Safe Control and Learning</title><link>https://arxiv.org/abs/2404.16883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'probabilistic invariance' to characterize invariance conditions for probabilities of interest in stochastic systems, enabling guarantees about long-term safe probabilities.&lt;/li&gt;&lt;li&gt;Shows how to derive myopic (short-horizon) control conditions/controllers that nevertheless ensure prescribed long-term safety despite cumulative uncertainties.&lt;/li&gt;&lt;li&gt;Integrates the technique into safe control and learning pipelines, using neural-network controllers or model predictive control with short outlook horizons to provide real-time-safe decisions.&lt;/li&gt;&lt;li&gt;Provides learning methods that guarantee long-term safety during training and post-deployment, validated via numerical simulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoyuan Wang', 'Haoming Jing', 'Christian Kurniawan', 'Albert Chern', 'Yorie Nakahira']&lt;/li&gt;&lt;li&gt;Tags: ['probabilistic-safety', 'safe-control', 'safe-learning', 'real-time-safety', 'safety-certificates']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.16883</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Reinforcement Learning Framework for ESP Cheater Simulation</title><link>https://arxiv.org/abs/2509.24274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial reinforcement learning simulation framework to model ESP cheaters and non-cheaters as agents with differing observability.&lt;/li&gt;&lt;li&gt;Models detectors that classify player trajectories and casts cheater–detector interaction as a co-adaptive adversarial game.&lt;/li&gt;&lt;li&gt;Introduces a structured cheater that dynamically switches between cheating and non-cheating behavior to trade off reward and detection risk.&lt;/li&gt;&lt;li&gt;Provides experiments showing realistic adaptive evasion strategies useful for developing and evaluating trajectory-based cheat detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Inkyu Park', 'Jeong-Gwan Lee', 'Taehwan Kwon', 'Juheon Choi', 'Seungku Kim', 'Junsu Kim', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'cheat detection', 'adversarial simulation', 'evasion strategies', 'behavioral detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24274</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking</title><link>https://arxiv.org/abs/2509.14603</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PM-SFL, a split federated learning framework that uses probabilistic mask training to add structured randomness to intermediate activations, aiming to prevent data reconstruction without explicit noise injection.&lt;/li&gt;&lt;li&gt;Introduces personalized mask learning to handle client data heterogeneity and a layer-wise knowledge compensation mechanism to address system heterogeneity and adaptive model splitting.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of privacy protection and empirical results on image and wireless sensing tasks showing improved accuracy, communication efficiency, and robustness to privacy attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingchen Wang', 'Feijie Wu', 'Chenglin Miao', 'Tianchun Li', 'Haoyu Hu', 'Qiming Cao', 'Jing Gao', 'Lu Su']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'data reconstruction', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14603</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification</title><link>https://arxiv.org/abs/2507.05405</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PT-LiRPA, which augments Linear Relaxation-based Perturbation Analysis with a sampling-based estimation of intermediate reachable sets to tighten linear bounds.&lt;/li&gt;&lt;li&gt;Claims negligible overhead while significantly improving lower/upper bounds on network outputs, yielding stronger robustness certificates against input perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates improvements on standard verification benchmarks (including IVN Competition), reporting up to 3.31× and 2.26× better certified perturbation bounds and providing probabilistic soundness guarantees (e.g., ≥99%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luca Marzari', 'Ferdinando Cicalese', 'Alessandro Farinelli']&lt;/li&gt;&lt;li&gt;Tags: ['neural-network-verification', 'adversarial-robustness', 'formal-verification', 'LiRPA', 'probabilistic-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05405</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Not All Tokens Are Meant to Be Forgotten</title><link>https://arxiv.org/abs/2506.03142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Targeted Information Forgetting (TIF) to avoid over-forgetting by distinguishing unwanted words (UW) from general words (GW) within forget samples.&lt;/li&gt;&lt;li&gt;Proposes Targeted Preference Optimization using Logit Preference Loss to unlearn UW and Preservation Loss to retain GW, preserving model utility.&lt;/li&gt;&lt;li&gt;Evaluates on TOFU and MUSE benchmarks, showing improved unlearning effectiveness while maintaining utility and achieving state-of-the-art results.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyu Zhou', 'Yao Qiang', 'Saleh Zare Zade', 'Douglas Zytko', 'Prashant Khanduri', 'Dongxiao Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'privacy-preserving', 'memorization mitigation', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03142</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title><link>https://arxiv.org/abs/2504.16628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ParetoHqD: an offline multiobjective alignment method that represents human preferences as directions in objective space and treats data near the Pareto front as high-quality.&lt;/li&gt;&lt;li&gt;For each preference direction, applies a two-stage supervised fine-tuning pipeline where each stage uses a Pareto high-quality training subset matched to that preference.&lt;/li&gt;&lt;li&gt;Addresses issues from imbalanced reward scores and inappropriate preference representations; shows empirical gains over five baselines on two multiobjective alignment tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gu', 'Handing Wang', 'Yi Mei', 'Mengjie Zhang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multiobjective optimization', 'supervised fine-tuning', 'Pareto front', 'reward modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16628</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models</title><link>https://arxiv.org/abs/2501.07033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a GAN-based model to detect deepfake manipulations in online payment images, trained on real payment images and GAN-generated deepfakes (StyleGAN, DeepFake).&lt;/li&gt;&lt;li&gt;Reports high detection performance (above 95%) in distinguishing legitimate transactions from AI-generated deepfakes.&lt;/li&gt;&lt;li&gt;Aims to improve robustness of payment systems against AI-driven fraud and contributes applied techniques for digital security/forensics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zong Ke', 'Shicheng Zhou', 'Yining Zhou', 'Chia Hong Chang', 'Rong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'fraud detection', 'GANs', 'payment security', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.07033</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning</title><link>https://arxiv.org/abs/2412.07454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Tazza, a federated learning framework that shuffles neural network weights (leveraging permutation equivariance/invariance) to defend against model poisoning and gradient inversion attacks.&lt;/li&gt;&lt;li&gt;Uses weight shuffling and shuffled model validation to improve robustness and data confidentiality while maintaining model accuracy.&lt;/li&gt;&lt;li&gt;Reports empirical evaluations showing improved resilience and up to 6.7x computational efficiency on various datasets and embedded platforms compared to alternatives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kichang Lee', 'Jaeho Jin', 'JaeYeon Park', 'Songkuk Kim', 'JeongGil Ko']&lt;/li&gt;&lt;li&gt;Tags: ['federated_learning', 'model_poisoning', 'privacy (gradient_inversion)', 'defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07454</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoundnessBench: A Soundness Benchmark for Neural Network Verifiers</title><link>https://arxiv.org/abs/2412.03154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SoundnessBench, a benchmark designed to test the soundness of neural network verifiers by embedding deliberately hidden counterexamples into verification instances.&lt;/li&gt;&lt;li&gt;Presents a training method to produce networks with hidden counterexamples that evade common adversarial search techniques, and constructs instances across architectures, activations, and input data.&lt;/li&gt;&lt;li&gt;Demonstrates that SoundnessBench can reveal false verification claims and identifies bugs in state-of-the-art NN verifiers; dataset and code are publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjian Zhou', 'Keyi Shen', 'Andy Xu', 'Hongji Xu', 'Cho-Jui Hsieh', 'Huan Zhang', 'Zhouxing Shi']&lt;/li&gt;&lt;li&gt;Tags: ['neural network verification', 'safety evaluation', 'benchmarking', 'verifier soundness', 'adversarial/hidden counterexamples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.03154</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Private Linear Regression with Differential Privacy and PAC Privacy</title><link>https://arxiv.org/abs/2412.02578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of linear regression models trained with differential privacy (DP) and PAC privacy across three real-world datasets.&lt;/li&gt;&lt;li&gt;Analyzes performance trade-offs and key findings that affect utility of privacy-preserving linear regression methods.&lt;/li&gt;&lt;li&gt;Highlights differences in practical behavior between the well-established DP framework and the newer PAC privacy notion for linear regression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hillary Yang', 'Yuntao Du']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'PAC privacy', 'privacy-preserving ML', 'linear regression', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.02578</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DarkEQA, a benchmark for evaluating embodied question answering (EQA) perceptual primitives under multi-level low-light conditions.&lt;/li&gt;&lt;li&gt;Isolates the perception bottleneck by evaluating QA from egocentric observations with controlled, physics-based degradations modeled in linear RAW space and rendered via an ISP-like pipeline.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art vision-language models (VLMs) and low-light image enhancement (LLIE) methods to quantify VLM performance degradations in dark environments.&lt;/li&gt;&lt;li&gt;Provides an open-source dataset and code (upon acceptance) to enable systematic robustness analysis for 24/7 embodied agent operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yohan Park', 'Hyunwoo Ha', 'Wonjun Jo', 'Tae-Hyun Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'vision-language models', 'low-light/perception', 'embodied QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24985</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Iterative Deployment Improves Planning Skills in LLMs</title><link>https://arxiv.org/abs/2512.24940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative deployment—repeatedly fine-tuning LLMs on user-curated data from prior deployments—can substantially improve planning skills and produce emergent longer-horizon plans.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis that iterative deployment implements an outer-loop reinforcement learning process with an implicit (not explicitly defined) reward function.&lt;/li&gt;&lt;li&gt;Highlights safety implications: the implicit reward entailed by repeated deployment may be unspecified and produce unintended properties in future models, and suggests iterative deployment as an alternative training regime to explicit RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Augusto B. Corr\\^ea', 'Yoav Gelberg', 'Luckeciano C. Melo', 'Ilia Shumailov', "Andr\\'e G. Pereira", 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'deployment dynamics', 'emergent capabilities', 'implicit reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24940</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</title><link>https://arxiv.org/abs/2512.24792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a projection-based physical adversarial attack that projects perturbation light onto objects to fool monocular depth estimation (MDE) models.&lt;/li&gt;&lt;li&gt;Uses physics-in-the-loop (PITL) optimization and a distributed CMA-ES to evaluate candidate perturbations in real environments accounting for device specifics and disturbances.&lt;/li&gt;&lt;li&gt;Demonstrates successful real-world attacks causing depth misestimations and parts of objects to disappear from reconstructed scenes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Takeru Kusakabe', 'Yudai Hirose', 'Mashiho Mukaida', 'Satoshi Ono']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'physical-adversarial', 'robustness', 'monocular-depth-estimation', 'physics-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24792</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Offline Reinforcement Learning with Corruption Robustness</title><link>https://arxiv.org/abs/2512.24768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness to adversarial corruption of a fraction of offline RL trajectories in high-dimensional but sparse MDPs, aiming to learn near-optimal policies.&lt;/li&gt;&lt;li&gt;Shows limitations of standard LSVI with sparsity (overly pessimistic bonuses) and proposes actor-critic methods using sparse robust estimator oracles to avoid pointwise pessimism.&lt;/li&gt;&lt;li&gt;Provides the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage and extends results to strong contamination (data poisoning) settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nam Phuong Tran', 'Andi Nika', 'Goran Radanovic', 'Long Tran-Thanh', 'Debmalya Mandal']&lt;/li&gt;&lt;li&gt;Tags: ['offline reinforcement learning', 'adversarial/data corruption', 'robustness', 'poisoning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24768</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MultiRisk: Multiple Risk Control via Iterative Score Thresholding</title><link>https://arxiv.org/abs/2512.24587</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes enforcing multiple test-time risk constraints with user-defined priorities and proposes algorithms to select output-filtering thresholds.&lt;/li&gt;&lt;li&gt;Introduces two dynamic-programming methods: MULTIRISK-BASE (finite-sample threshold selection) and MULTIRISK (exchangeability-based simultaneous risk control).&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees showing near-tight control of multiple constraint risks via an intricate iterative/symmetrization analysis.&lt;/li&gt;&lt;li&gt;Evaluates on LLM alignment (PKU-SafeRLHF) using an LLM judge and perplexity filter, demonstrating empirical control of each safety constraint near target levels.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunay Joshi', 'Yan Sun', 'Hamed Hassani', 'Edgar Dobriban']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'test-time filtering', 'risk control', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24587</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</title><link>https://arxiv.org/abs/2512.24574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies attention heads correlated with distinct cognitive behaviors (e.g., verification, backtracking) in LLM reasoning trajectories.&lt;/li&gt;&lt;li&gt;Proposes CREST, a training-free method with an offline calibration to derive head-specific steering vectors and an inference-time rotation to suppress unproductive components.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy (up to 17.5%) and reduced token usage (up to 37.6%) across reasoning benchmarks by adaptively suppressing inefficient reasoning modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Xiaoxia Wu', 'Zhongzhu Zhou', 'Qingyang Wu', 'Yineng Zhang', 'Pragaash Ponnusamy', 'Harikaran Subbaraj', 'Jue Wang', 'Shuaiwen Leon Song', 'Ben Athiwaratkun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model steering', 'interpretability', 'inference-time intervention', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24574</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations</title><link>https://arxiv.org/abs/2512.24452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a deep learning-based semantic communication system that jointly supports multiple receiver tasks while limiting semantic information leakage to an eavesdropper.&lt;/li&gt;&lt;li&gt;Formulates privacy as a min–max optimization: an adversary (eavesdropper) is trained to infer semantics while the transmitter–receiver are trained to preserve legitimate task performance and reduce the adversary's success.&lt;/li&gt;&lt;li&gt;Introduces a cooperative adversarial perturbation layer that superimposes crafted noise on transmitted waveforms to further degrade eavesdropper inference without harming the legitimate receiver.&lt;/li&gt;&lt;li&gt;Evaluates performance on image datasets (MNIST, CIFAR-10) over Rayleigh fading channels with AWGN, showing reduced semantic leakage and preserved legitimate accuracy/reconstruction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yalin E. Sagduyu', 'Tugba Erpek', 'Aylin Yener', 'Sennur Ulukus']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'adversarial perturbations', 'semantic communications', 'adversarial training / min-max', 'wireless security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24452</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models</title><link>https://arxiv.org/abs/2512.24058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Composite Reliability Score (CRS) that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric for LLM reliability.&lt;/li&gt;&lt;li&gt;Evaluates ten open-source LLMs across five QA datasets under baseline, input perturbations, and calibration methods, showing CRS yields stable model rankings and uncovers failure modes missed by single metrics.&lt;/li&gt;&lt;li&gt;Finds the most dependable systems balance accuracy, robustness, and calibrated uncertainty, and demonstrates CRS as a tool for comparing models and measuring reliability improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Kumar Salla', 'Manoj Saravanan', 'Shrikar Reddy Kota']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'calibration', 'uncertainty-quantification', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24058</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress</title><link>https://arxiv.org/abs/2512.23995</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a security vulnerability in Mixture-of-Experts (MoE) inference: out-of-distribution prompts can cause extreme router concentration, creating device-level bottlenecks and idle resources.&lt;/li&gt;&lt;li&gt;Introduces RepetitionCurse, a low-cost black-box adversarial prompting method that uses simple repetitive token patterns to trigger consistent routing to the same top-k experts.&lt;/li&gt;&lt;li&gt;Demonstrates practical impact on deployed MoE models (e.g., Mixtral-8x7B), increasing end-to-end inference latency by ~3.063x and causing SLA violations for time-to-first-token.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruixuan Huang', 'Qingyue Wang', 'Hantao Huang', 'Yudong Gao', 'Dong Chen', 'Shuai Wang', 'Wei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Mixture-of-Experts', 'Denial-of-Service', 'Adversarial Prompting', 'Router Imbalance', 'LLM Security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23995</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title><link>https://arxiv.org/abs/2512.23988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RISE, an unsupervised method using sentence-level step activations and sparse auto-encoders to discover 'reasoning vectors'—directions in activation space corresponding to distinct reasoning behaviors.&lt;/li&gt;&lt;li&gt;Shows that discovered vectors are disentangled and occupy separable regions; interventions on decoder columns can amplify/suppress behaviors (e.g., reflection, backtracking) and alter inference trajectories without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates discovery of structural properties (e.g., response length) and novel controllable behaviors (e.g., response confidence), suggesting uses for interpreting and steering LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Shujian Zhang', 'John Lambert', 'Wenxuan Zhou', 'Zhangyang Wang', 'Mingqing Chen', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'model steering', 'latent interventions', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23988</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models</title><link>https://arxiv.org/abs/2512.23850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Drill-Down and Fabricate Test (DDFT), a protocol to measure epistemic robustness: the ability of language models to maintain factual accuracy under progressive semantic compression and adversarial fabrication.&lt;/li&gt;&lt;li&gt;Proposes a two-system cognitive model (Semantic System for generation and Epistemic Verifier for factual validation) and operationalizes tests across 9 frontier models, 8 knowledge domains, and 5 compression levels (1,800 turn-level evaluations).&lt;/li&gt;&lt;li&gt;Finds epistemic robustness is largely independent of model scale or architecture but strongly predicted by error-detection/verification capability, and shows flagship models can be brittle while some smaller models are more robust.&lt;/li&gt;&lt;li&gt;Provides a practical benchmarking framework and tools for assessing epistemic robustness prior to deployment in critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Baxi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial evaluation/red teaming', 'epistemic verification', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23850</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Test of Lookahead Bias in LLM Forecasts</title><link>https://arxiv.org/abs/2512.23847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a statistical test to detect lookahead bias in LLM-generated economic forecasts by estimating 'Lookahead Propensity' (LAP) — the likelihood a prompt appears in an LLM's training data.&lt;/li&gt;&lt;li&gt;Formally shows that a positive correlation between LAP and forecast accuracy signals presence and magnitude of lookahead bias.&lt;/li&gt;&lt;li&gt;Applies the test to two empirical forecasting tasks (news headlines → stock returns; earnings call transcripts → capital expenditures) and presents a cost-efficient diagnostic for assessing forecast validity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Gao', 'Wenxi Jiang', 'Yutong Yan']&lt;/li&gt;&lt;li&gt;Tags: ['lookahead-bias', 'training-data-detection', 'data-leakage', 'evaluation', 'LLM-forecasting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23847</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title><link>https://arxiv.org/abs/2512.23837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial attack method that extracts token-level predictions from intermediate attention layers to generate perturbations, differing from prompt- or gradient-based attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on argument quality assessment (ArgQuality) using LLaMA-3.1-Instruct-8B as both generator and evaluator, showing measurable drops in evaluation performance.&lt;/li&gt;&lt;li&gt;Finds generated adversarial examples are semantically similar to originals but notes practical limitations due to grammatical degradation for substitutions from certain layers/positions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Dhole']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'mechanistic interpretability', 'LLM robustness', 'red teaming', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23837</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</title><link>https://arxiv.org/abs/2512.23836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive prompting strategy for retrieval-augmented QA that splits retrieved context into smaller chunks and sequentially prompts the LLM to answer using each chunk, trading off relevance vs. noise.&lt;/li&gt;&lt;li&gt;Shows adaptive chunking can match standard prompting performance while using fewer tokens across three open-domain QA datasets.&lt;/li&gt;&lt;li&gt;Finds a major error mode: when information is insufficient, the LLM often produces incorrect answers instead of declining, indicating poor refusal/ignorance behavior.&lt;/li&gt;&lt;li&gt;Highlights need for further research to improve LLMs' ability to admit ignorance or decline when evidence is lacking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingmin Wang', 'Ji Ma', 'Shankar Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'retrieval-augmented generation', 'hallucination', 'uncertainty calibration', 'question answering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23836</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark</title><link>https://arxiv.org/abs/2512.23779</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box, query-only benchmark for prompt-induced over-generation (DoS-style) attacks on LLMs, assuming a known tokenizer.&lt;/li&gt;&lt;li&gt;Proposes two prompt-only attackers: EOGen (evolutionary search for prefixes that suppress EOS) and RL-GOAL (goal-conditioned RL generating prefixes targeting specific output lengths).&lt;/li&gt;&lt;li&gt;Defines evaluation metrics including Over-Generation Factor (OGF) and reports results showing RL-GOAL achieves substantially higher OGF and success rates across victim models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manu', 'Yi Guo', 'Jo Plested', 'Tim Lynar', 'Kanchana Thilakarathna', 'Nirhoshan Sivaroopan', 'Jack Yang', 'Wangli Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Prompt-based DoS', 'Adversarial prompting', 'Black-box attacks', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23779</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Generative Classifiers Avoid Shortcut Solutions</title><link>https://arxiv.org/abs/2512.25034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows generative classifiers (class-conditional generative models) can avoid shortcut learning by modeling all features rather than relying on spurious correlations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on five standard image and text distribution-shift benchmarks using diffusion-based and autoregressive generative classifiers.&lt;/li&gt;&lt;li&gt;Claims simple training requirements (no specialized augmentations, heavy regularization, extra hyperparameters, or prior knowledge of spurious cues) and validates on realistic domains (medical, satellite).&lt;/li&gt;&lt;li&gt;Provides theoretical insight via a Gaussian toy model analyzing inductive biases and conditions under which generative classifiers outperform discriminative ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander C. Li', 'Ananya Kumar', 'Deepak Pathak']&lt;/li&gt;&lt;li&gt;Tags: ['generative-classifiers', 'distribution-shift', 'robustness', 'spurious-correlations', 'evaluation-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.25034</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</title><link>https://arxiv.org/abs/2512.25023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ResponseRank infers preference strength from noisy proxy signals by ranking responses using relative differences within locally constructed strata to control for systemic variation.&lt;/li&gt;&lt;li&gt;It trains reward models to match strength-derived rankings, improving sample efficiency and robustness compared to binary preference data.&lt;/li&gt;&lt;li&gt;Validated across synthetic preference tasks, language modeling with annotator agreement, and RL control tasks with simulated returns.&lt;/li&gt;&lt;li&gt;Introduces Pearson Distance Correlation (PDC), a new metric to isolate cardinal utility learning from ordinal accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Timo Kaufmann', 'Yannick Metz', 'Daniel Keim', 'Eyke H\\"ullermeier']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward modeling', 'preference learning', 'alignment', 'evaluation metric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.25023</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</title><link>https://arxiv.org/abs/2512.24955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MSACL, an off-policy multi-step actor-critic framework that learns Lyapunov certificates to guarantee exponential stability in model-free RL without complex reward engineering.&lt;/li&gt;&lt;li&gt;Proposes Exponential Stability Labels (ESL) and a λ-weighted aggregation to balance bias–variance in multi-step certificate learning, and a stability-aware advantage for policy optimization.&lt;/li&gt;&lt;li&gt;Demonstrates superior exponential stability, fast convergence, robustness to uncertainties, and generalization across six benchmarks; identifies multi-step horizon n=20 as a robust default.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwei Zhang', 'Yuanzhe Xing', 'Quan Quan', 'Zhikun She']&lt;/li&gt;&lt;li&gt;Tags: ['RL safety', 'Lyapunov certificates', 'provable stability', 'robust control', 'off-policy actor-critic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24955</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback</title><link>https://arxiv.org/abs/2512.24818</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames LLM alignment from preference feedback as a two-player zero-sum game (NLHF) and studies convergence to Nash equilibria without regularization.&lt;/li&gt;&lt;li&gt;Proves last-iterate linear convergence of Optimistic Multiplicative Weights Update (OMWU) to the original game NE after a burn-in when an NE with full support exists, removing the NE-uniqueness assumption.&lt;/li&gt;&lt;li&gt;Identifies novel marginal convergence behavior (rare-action probabilities grow exponentially), yielding improved instance-dependent constants versus prior work.&lt;/li&gt;&lt;li&gt;Provides experiments in tabular and neural policy classes that support theoretical findings and suggests applicability for LLM alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shulun Chen', 'Runlong Zhou', 'Zihan Zhang', 'Maryam Fazel', 'Simon S. Du']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'game-theoretic methods', 'optimization/convergence', 'preference learning (NLHF)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24818</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</title><link>https://arxiv.org/abs/2512.24665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HeteroHBA, a generative backdoor attack targeting heterogeneous graph neural networks for node classification by injecting trigger nodes/connections at training time.&lt;/li&gt;&lt;li&gt;Selects influential auxiliary neighbors via saliency screening and synthesizes diverse trigger features and connection patterns to fit local heterogeneous contexts.&lt;/li&gt;&lt;li&gt;Improves stealth with Adaptive Instance Normalization (AdaIN) plus an MMD loss to align trigger feature distribution with benign data and optimizes a bilevel objective balancing attack success and clean accuracy.&lt;/li&gt;&lt;li&gt;Evaluated on multiple real-world heterogeneous graphs and HGNN architectures; outperforms prior backdoor baselines and remains effective against a heterogeneity-aware structural defense (CSD).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honglin Gao', 'Lan Zhao', 'Junhao Ren', 'Xiang Li', 'Gaoxi Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'heterogeneous graphs', 'graph neural networks', 'stealth/defenses', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24665</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts</title><link>https://arxiv.org/abs/2512.24564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CPR: a causal physiological representation learning method that uses a Structural Causal Model and a physiological structural prior to disentangle invariant pathological ECG morphology (P-QRS-T) from non-causal artifacts.&lt;/li&gt;&lt;li&gt;Targets robustness to Smooth Adversarial Perturbations (SAP) on ECG models, aiming to avoid costly adversarial training and high-latency certified defenses.&lt;/li&gt;&lt;li&gt;Reports empirical improvements on PTB-XL: higher F1 under SAP compared to median smoothing and matches certified robustness of Randomized Smoothing while keeping single-pass inference efficiency and interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shunbo Jia', 'Caizhi Liao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial defense', 'causal representation learning', 'ECG / medical time-series', 'randomized smoothing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24564</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GARDO: Reinforcing Diffusion Models without Reward Hacking</title><link>https://arxiv.org/abs/2512.24138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking and mode collapse when using proxy rewards to fine-tune diffusion (text-to-image) models with RL.&lt;/li&gt;&lt;li&gt;Proposes GARDO: selective (gated) regularization applied to high-uncertainty samples, adaptive updates of the reference model, and diversity-aware reward amplification to encourage mode coverage.&lt;/li&gt;&lt;li&gt;Shows empirically that GARDO reduces reward hacking and improves generation diversity without sacrificing sample efficiency or exploration across multiple proxy and unseen hold-out metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran He', 'Yuxiao Ye', 'Jie Liu', 'Jiajun Liang', 'Zhiyong Wang', 'Ziyang Yuan', 'Xintao Wang', 'Hangyu Mao', 'Pengfei Wan', 'Ling Pan']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment/safety', 'reinforcement learning', 'diffusion models', 'diversity/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24138</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems</title><link>https://arxiv.org/abs/2512.23978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a conceptual framework for 'assured autonomy' that pairs generative models with operations research (OR) tools to ensure verifiable feasibility, constraint-aware generation, and robustness in operational domains.&lt;/li&gt;&lt;li&gt;Advocates using flow-based generative models (deterministic transport/ODE viewpoint) to enable auditability and connections to optimal transport, robust optimization, and sequential decision control.&lt;/li&gt;&lt;li&gt;Frames operational safety via an adversarial robustness lens—evaluating decision rules against worst-case perturbations within uncertainty/ambiguity sets and stress testing high-consequence scenarios.&lt;/li&gt;&lt;li&gt;Argues OR's role expands from solver to guardrail and system architect, and outlines a research agenda for deploying GenAI agentic systems in safety-critical, reliability-sensitive environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tinglong Dai', 'David Simchi-Levi', 'Michelle Xiao Wu', 'Yao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-robustness', 'assured-autonomy', 'stress-testing', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23978</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks</title><link>https://arxiv.org/abs/2512.23948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DivQAT, a modification to Quantization Aware Training that integrates defenses against model extraction into the quantization process for CNNs.&lt;/li&gt;&lt;li&gt;Targets robustness of quantized models on edge devices, aiming to prevent IP theft via extraction attacks while maintaining model accuracy.&lt;/li&gt;&lt;li&gt;Empirical evaluation on benchmark vision datasets shows improved resistance to extraction attacks and complementary gains when combined with other defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kacem Khaled', 'Felipe Gohring de Magalh\\~aes', 'Gabriela Nicolescu']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'quantization-aware-training', 'extraction-defense', 'CNNs', 'adversarial-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23948</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improved Bounds for Private and Robust Alignment</title><link>https://arxiv.org/abs/2512.23816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical upper bounds on the suboptimality gap for private and robust alignment of language models in both offline and online settings.&lt;/li&gt;&lt;li&gt;Analyzes preference-label learning under privacy constraints and adversarial corruption, considering two orderings (privacy-first and corruption-first).&lt;/li&gt;&lt;li&gt;Shows log loss with an MLE-style algorithm attains near-optimal rates in the privacy-only setting, strengthens guarantees for joint privacy-and-corruption offline algorithms, and improves corruption-only bounds.&lt;/li&gt;&lt;li&gt;Introduces the first results for private and robust online alignment, enabled by new uniform convergence guarantees for log and square loss under privacy and corruption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqian Weng', 'Yi He', 'Xingyu Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'privacy-preserving ML', 'robustness/adversarial corruption', 'theoretical learning bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23816</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems</title><link>https://arxiv.org/abs/2512.23809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZTA-FL, a defense-in-depth federated learning framework for IIoT intrusion detection combining TPM-based cryptographic attestation, a SHAP-weighted aggregation algorithm for explainable Byzantine detection under non-IID data, and on-device privacy-preserving adversarial training.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and failure-mode analysis for the SHAP-weighted aggregation and reports empirical results on Edge-IIoTset, CIC-IDS2017, and UNSW-NB15.&lt;/li&gt;&lt;li&gt;Reports high performance: 97.8% detection accuracy, 93.2% accuracy under 30% Byzantine attacks (outperforming FLAME by 3.1%, p&lt;0.01), 89.3% adversarial robustness, and a 34% reduction in communication overhead.&lt;/li&gt;&lt;li&gt;Includes TPM-based agent authentication claims (FAR &lt; 1e-7), reproducible code release, and comprehensive experiments—addressing both security and robustness of FL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samaresh Kumar Singh', 'Joyjit Roy', 'Martin So']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning Security', 'Byzantine Poisoning', 'Hardware Attestation (TPM)', 'Explainable Aggregation (SHAP)', 'Adversarial Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23809</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions</title><link>https://arxiv.org/abs/2512.23770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a trust-region method that biases policy updates toward constraint (cost) satisfaction via a convex combination of natural policy gradients for reward and cost.&lt;/li&gt;&lt;li&gt;Provides a theoretical guarantee of local progress toward safety (fixed fraction of optimal cost reduction per update) and conditions for concurrent reward improvement when gradients align.&lt;/li&gt;&lt;li&gt;Empirically evaluates SB-TRPO on Safety Gymnasium tasks, showing improved trade-offs between near-zero safety violations and meaningful task completion compared to state-of-the-art constrained RL methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ankit Kanwar', 'Dominik Wagner', 'Luke Ong']&lt;/li&gt;&lt;li&gt;Tags: ['safe-reinforcement-learning', 'constrained-RL', 'trust-region-methods', 'policy-optimization', 'safety-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23770</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Study of Deep Learning Model Fixing Approaches</title><link>https://arxiv.org/abs/2512.23745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study evaluating 16 deep learning model-fixing approaches at model-, layer-, and neuron-levels.&lt;/li&gt;&lt;li&gt;Measures not only fixing effectiveness but also side effects on robustness, fairness, accuracy, and backward compatibility across diverse datasets and architectures.&lt;/li&gt;&lt;li&gt;Finds trade-offs: model-level methods are most effective at fixing, but no approach uniformly improves accuracy and other safety-related properties, highlighting need to mitigate side effects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanmo You', 'Zan Wang', 'Zishuo Dong', 'Luanqi Mo', 'Jianjun Zhao', 'Junjie Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model repair', 'safety evaluation', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23745</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges</title><link>https://arxiv.org/abs/2511.14478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a precise definition and taxonomy of "agentic AI" to distinguish it from prior AI paradigms.&lt;/li&gt;&lt;li&gt;Presents four state-of-the-art use cases applying agentic AI to electrical power systems engineering (including benchmarking and dynamic pricing survival analysis).&lt;/li&gt;&lt;li&gt;Analyzes failure modes and derives actionable recommendations for safe, reliable, and accountable design and deployment of agentic systems in power-system contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soham Ghosh', 'Gaurav Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'agentic AI', 'failure modes', 'power systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14478</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ATLAS: Artifact Generation Through Layered Constraints and LLM x MDE Synergy</title><link>https://arxiv.org/abs/2510.25890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ATLAS combines LLMs with Model-Driven Engineering via a Unified Meta-Model to reconcile heterogeneous schemas and regulatory text into a single semantic space.&lt;/li&gt;&lt;li&gt;Introduces an Integrated Constraint Model (ICM) producing two enforcement layers: Layer 1 deterministic generation-time automata for structural/prefix-safe decoding and Layer 2 post-generation semantic/logical validators that produce machine-checkable certificates.&lt;/li&gt;&lt;li&gt;Implements Constraint-Guided Verifiable Generation (CVG) with audit-guided repair and trace recording; evaluated in automotive (AUTOSAR) and cross-border legal (Brussels I bis) domains showing reduced manual remediation and auditable artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Ma', 'Hui Lai', 'Hui Wang', 'Zhenhu Tian', 'Jizhou Wang', 'Haichao Wu', 'Yongfan Gao', 'Chaochao Li', 'Fengjie Xu', 'Ling Fang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety / verifiable generation', 'Constraint-based validation', 'Model-driven engineering', 'Compliance &amp; auditability', 'Automated repair / traceability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25890</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title><link>https://arxiv.org/abs/2510.17884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of open-source LLMs (TinyLLaMA, Falcon-RW-1B, Flan-T5) for password guessing using synthetic user profiles and structured attributes (name, birthdate, hobbies).&lt;/li&gt;&lt;li&gt;Measured performance with Hit@1/5/10 under plaintext and SHA-256 comparisons; all models perform poorly (under 1.5% at Hit@10).&lt;/li&gt;&lt;li&gt;Compared LLMs to traditional rule-based and combinator-based cracking methods, which significantly outperform the LLMs.&lt;/li&gt;&lt;li&gt;Analysis identifies LLM limitations for this domain: weak domain adaptation, insufficient memorization of leaked-password patterns, and lack of benefit without supervised fine-tuning on password datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Abdul Rehman', 'Syed Imad Ali Shah', 'Abbas Anwar', 'Noor Islam', 'Hamid Khan']&lt;/li&gt;&lt;li&gt;Tags: ['password cracking', 'LLM evaluation', 'adversarial/security', 'privacy', 'model capabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17884</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Reinforcement Learning Framework for ESP Cheater Simulation</title><link>https://arxiv.org/abs/2509.24274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial reinforcement learning simulation framework to model ESP cheaters and non-cheaters as agents with differing observability.&lt;/li&gt;&lt;li&gt;Models detectors that classify player trajectories and casts cheater–detector interaction as a co-adaptive adversarial game.&lt;/li&gt;&lt;li&gt;Introduces a structured cheater that dynamically switches between cheating and non-cheating behavior to trade off reward and detection risk.&lt;/li&gt;&lt;li&gt;Provides experiments showing realistic adaptive evasion strategies useful for developing and evaluating trajectory-based cheat detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Inkyu Park', 'Jeong-Gwan Lee', 'Taehwan Kwon', 'Juheon Choi', 'Seungku Kim', 'Junsu Kim', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial reinforcement learning', 'cheat detection', 'adversarial simulation', 'evasion strategies', 'behavioral detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24274</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Secure and Efficient Access Control for Computer-Use Agents via Context Space</title><link>https://arxiv.org/abs/2509.22256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CSAgent, a system-level, static policy-based access control framework for LLM-based computer-use agents to mitigate security risks from unintended or malicious actions.&lt;/li&gt;&lt;li&gt;Introduces intent- and context-aware policies plus an automated toolchain to help developers construct and refine policies that bridge static rules and dynamic user intent.&lt;/li&gt;&lt;li&gt;Implements policy enforcement as an optimized OS service supporting diverse agent interfaces (API, CLI, GUI) and reports defense against &gt;99.56% of attacks with ~1.99% runtime overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haochen Gong', 'Chenxiao Li', 'Rui Chang', 'Wenbo Shen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'access control', 'agent safety', 'policy enforcement', 'runtime defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22256</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification</title><link>https://arxiv.org/abs/2507.05405</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PT-LiRPA, which augments Linear Relaxation-based Perturbation Analysis with a sampling-based estimation of intermediate reachable sets to tighten linear bounds.&lt;/li&gt;&lt;li&gt;Claims negligible overhead while significantly improving lower/upper bounds on network outputs, yielding stronger robustness certificates against input perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates improvements on standard verification benchmarks (including IVN Competition), reporting up to 3.31× and 2.26× better certified perturbation bounds and providing probabilistic soundness guarantees (e.g., ≥99%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luca Marzari', 'Ferdinando Cicalese', 'Alessandro Farinelli']&lt;/li&gt;&lt;li&gt;Tags: ['neural-network-verification', 'adversarial-robustness', 'formal-verification', 'LiRPA', 'probabilistic-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05405</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs</title><link>https://arxiv.org/abs/2505.24830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an atomic fact-checking framework that decomposes LLM-generated medical answers into discrete, verifiable 'atomic facts' and verifies each against an authoritative medical guideline knowledge base.&lt;/li&gt;&lt;li&gt;Uses retrieval-augmented grounding to trace each atomic fact back to relevant source chunks, enabling targeted correction of errors and granular explainability.&lt;/li&gt;&lt;li&gt;Reports empirical improvements (up to ~40% overall answer improvement and ~50% hallucination detection) validated via multi-reader medical expert assessments and automated benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juraj Vladika', 'Annika Domres', 'Mai Nguyen', 'Rebecca Moser', 'Jana Nano', 'Felix Busch', 'Lisa C. Adams', 'Keno K. Bressem', 'Denise Bernhardt', 'Stephanie E. Combs', 'Kai J. Borm', 'Florian Matthes', 'Jan C. Peeken']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Hallucination detection', 'Retrieval-augmented generation', 'Atomic fact-checking', 'Medical QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24830</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Effective and Efficient Jailbreaks of Black-Box LLMs with Cross-Behavior Attacks</title><link>https://arxiv.org/abs/2503.08990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JCB, a black-box jailbreak method that reuses successful past behavior prompts to efficiently discover new jailbreaks without relying on auxiliary LLM calls.&lt;/li&gt;&lt;li&gt;Claims major efficiency gains: up to 94% fewer queries and 12.9% higher average attack success vs. baselines.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on resilient models (e.g., 37% success on Llama-2-7B) and shows zero-shot transferability across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasudev Gohil']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'Black-box adversarial attacks', 'Prompt injection / adversarial prompting', 'Red teaming', 'Security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08990</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</title><link>https://arxiv.org/abs/2502.14780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Visual Instruction Rewriting: converting multimodal (image+instruction) inputs into text-only commands to avoid transmitting vision data.&lt;/li&gt;&lt;li&gt;Provides a dataset of &gt;39,000 examples across 14 domains and develops a compact 250M-parameter VLM fine-tuned for instruction rewriting.&lt;/li&gt;&lt;li&gt;Demonstrates that a quantized model (&lt;500 MB) can effectively perform rewriting using NLG metrics (BLEU, METEOR, ROUGE) and semantic parsing analysis, enabling on-device, privacy-preserving deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhijit Mishra', 'Mingda Li', 'Hsiang Fu', 'Richard Noh', 'Minji Kim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'visual-instruction-rewriting', 'dataset', 'on-device VLM', 'multimodal privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14780</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Positional Biases in Text Embedding Models</title><link>https://arxiv.org/abs/2412.15241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finds that text embedding models disproportionately prioritize content at the beginning of inputs regardless of positional encoding.&lt;/li&gt;&lt;li&gt;Ablation experiments show inserting or removing text at the start reduces cosine similarity with the original embedding up to 12.3% more than analogous changes at the end.&lt;/li&gt;&lt;li&gt;Regression analyses indicate sentence importance declines with distance from the start even when content is controlled for, suggesting content-agnostic positional sensitivity.&lt;/li&gt;&lt;li&gt;Authors hypothesize causes in preprocessing and chosen positional encodings and highlight implications for retrieval robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reagan J. Lee', 'Samarth Goel', 'Kannan Ramchandran']&lt;/li&gt;&lt;li&gt;Tags: ['embeddings', 'positional-bias', 'robustness', 'information-retrieval', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15241</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning</title><link>https://arxiv.org/abs/2412.07454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Tazza, a federated learning framework that shuffles neural network weights (leveraging permutation equivariance/invariance) to defend against model poisoning and gradient inversion attacks.&lt;/li&gt;&lt;li&gt;Uses weight shuffling and shuffled model validation to improve robustness and data confidentiality while maintaining model accuracy.&lt;/li&gt;&lt;li&gt;Reports empirical evaluations showing improved resilience and up to 6.7x computational efficiency on various datasets and embedded platforms compared to alternatives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kichang Lee', 'Jaeho Jin', 'JaeYeon Park', 'Songkuk Kim', 'JeongGil Ko']&lt;/li&gt;&lt;li&gt;Tags: ['federated_learning', 'model_poisoning', 'privacy (gradient_inversion)', 'defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07454</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SoundnessBench: A Soundness Benchmark for Neural Network Verifiers</title><link>https://arxiv.org/abs/2412.03154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SoundnessBench, a benchmark designed to test the soundness of neural network verifiers by embedding deliberately hidden counterexamples into verification instances.&lt;/li&gt;&lt;li&gt;Presents a training method to produce networks with hidden counterexamples that evade common adversarial search techniques, and constructs instances across architectures, activations, and input data.&lt;/li&gt;&lt;li&gt;Demonstrates that SoundnessBench can reveal false verification claims and identifies bugs in state-of-the-art NN verifiers; dataset and code are publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjian Zhou', 'Keyi Shen', 'Andy Xu', 'Hongji Xu', 'Cho-Jui Hsieh', 'Huan Zhang', 'Zhouxing Shi']&lt;/li&gt;&lt;li&gt;Tags: ['neural network verification', 'safety evaluation', 'benchmarking', 'verifier soundness', 'adversarial/hidden counterexamples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.03154</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</title><link>https://arxiv.org/abs/2512.23126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations of Direct Preference Optimization (DPO): dependence on arbitrary scalarization and reference policies, and failure to use comparative pairwise information.&lt;/li&gt;&lt;li&gt;Proposes Intrinsic Self-reflective Preference Optimization (InSPO), which conditions the optimal policy on both context and alternative responses to enable intrinsic self-reflection.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees of invariance to scalarization and reference choices and claims superiority over DPO/RLHF.&lt;/li&gt;&lt;li&gt;Empirical results show improved win rates and length-controlled metrics with no architectural changes or inference overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Li', 'Tian Lan', 'Zhengling Qi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'RLHF/DPO', 'LLM training', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23126</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM</title><link>https://arxiv.org/abs/2511.22998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TIM-PRM, a tool-integrated Process Reward Model that actively plans verification strategies and queries external tools to validate multimodal reasoning steps.&lt;/li&gt;&lt;li&gt;Introduces Independent Question Asking to decouple verification from original reasoning and reduce confirmation bias and sycophancy in PRMs.&lt;/li&gt;&lt;li&gt;Presents a curated dataset of tool-integrated verification trajectories and shows strong empirical gains on VisualProcessBench, outperforming larger open-source models.&lt;/li&gt;&lt;li&gt;Aims to provide interpretable, stepwise verification to mitigate visual hallucinations and logical inconsistencies in multimodal LLM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Kuang', 'Xiangxiang Wang', 'Wentao Liu', 'Jian Dong', 'Kaidi Xu']&lt;/li&gt;&lt;li&gt;Tags: ['Multimodal verification', 'Process Reward Model (PRM)', 'Hallucination mitigation', 'Tool-augmented verification', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22998</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RAJ-PGA: Reasoning-Activated Jailbreak and Principle-Guided Alignment Framework for Large Reasoning Models</title><link>https://arxiv.org/abs/2508.12897</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new jailbreak attack called Reasoning-Activated Jailbreak (RAJ) via Concretization that elicits harmful internal reasoning chains by making malicious prompts more specific.&lt;/li&gt;&lt;li&gt;Presents a Principle-Guided Alignment (PGA) pipeline that transforms elicited harmful reasoning traces into safe, constructive training examples to build an alignment dataset.&lt;/li&gt;&lt;li&gt;Introduces the PGA dataset (3,989 samples) and shows fine-tuning on it improves defense success rates against jailbreak benchmarks (up to 29.5%) while preserving or improving general reasoning ability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianhao Chen', 'Mayi Xu', 'Haoyang Chen', 'Xiaohu Li', 'Xiangyu Zhang', 'Jianjie Huang', 'Zheng Wang', 'Xiaochun Cao', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment dataset', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12897</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method combining explicit CI (contextual integrity) reasoning prompts with reinforcement learning to reduce inappropriate information disclosure by LLMs.&lt;/li&gt;&lt;li&gt;Trains on a small synthetic dataset (~700 examples) with diverse contexts and disclosure norms and demonstrates reduced privacy leakage while preserving task performance across model sizes/families.&lt;/li&gt;&lt;li&gt;Shows transfer of improvements to a human-annotated CI/privacy benchmark (PrivacyLens) evaluating assistant actions and tool calls.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'contextual integrity', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Generative Classifiers Avoid Shortcut Solutions</title><link>https://arxiv.org/abs/2512.25034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows generative classifiers (class-conditional generative models) can avoid shortcut learning by modeling all features rather than relying on spurious correlations.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art robustness on five standard image and text distribution-shift benchmarks using diffusion-based and autoregressive generative classifiers.&lt;/li&gt;&lt;li&gt;Claims simple training requirements (no specialized augmentations, heavy regularization, extra hyperparameters, or prior knowledge of spurious cues) and validates on realistic domains (medical, satellite).&lt;/li&gt;&lt;li&gt;Provides theoretical insight via a Gaussian toy model analyzing inductive biases and conditions under which generative classifiers outperform discriminative ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander C. Li', 'Ananya Kumar', 'Deepak Pathak']&lt;/li&gt;&lt;li&gt;Tags: ['generative-classifiers', 'distribution-shift', 'robustness', 'spurious-correlations', 'evaluation-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.25034</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DarkEQA, a benchmark for evaluating embodied question answering (EQA) perceptual primitives under multi-level low-light conditions.&lt;/li&gt;&lt;li&gt;Isolates the perception bottleneck by evaluating QA from egocentric observations with controlled, physics-based degradations modeled in linear RAW space and rendered via an ISP-like pipeline.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art vision-language models (VLMs) and low-light image enhancement (LLIE) methods to quantify VLM performance degradations in dark environments.&lt;/li&gt;&lt;li&gt;Provides an open-source dataset and code (upon acceptance) to enable systematic robustness analysis for 24/7 embodied agent operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yohan Park', 'Hyunwoo Ha', 'Wonjun Jo', 'Tae-Hyun Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark', 'vision-language models', 'low-light/perception', 'embodied QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24985</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions</title><link>https://arxiv.org/abs/2512.24971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates quantization, pruning, and weight clustering (individually and combined) on ResNet-50, VGG-19, and MobileNetV2.&lt;/li&gt;&lt;li&gt;Uses CIFAR-10-C and CIFAR-100-C to analyze trade-offs between robustness to natural corruptions, accuracy, and compression ratio.&lt;/li&gt;&lt;li&gt;Finds that certain compression strategies can preserve or improve robustness, particularly for more complex architectures.&lt;/li&gt;&lt;li&gt;Performs multi-objective assessment to identify compression configurations that balance robustness, accuracy, and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Itallo Patrick Castro Alves Da Silva', 'Emanuel Adler Medeiros Pereira', 'Erick de Andrade Barboza', 'Baldoino Fonseca dos Santos Neto', 'Marcio de Medeiros Ribeiro']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'model-compression', 'benchmarking', 'natural-corruptions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24971</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</title><link>https://arxiv.org/abs/2512.24955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MSACL, an off-policy multi-step actor-critic framework that learns Lyapunov certificates to guarantee exponential stability in model-free RL without complex reward engineering.&lt;/li&gt;&lt;li&gt;Proposes Exponential Stability Labels (ESL) and a λ-weighted aggregation to balance bias–variance in multi-step certificate learning, and a stability-aware advantage for policy optimization.&lt;/li&gt;&lt;li&gt;Demonstrates superior exponential stability, fast convergence, robustness to uncertainties, and generalization across six benchmarks; identifies multi-step horizon n=20 as a robust default.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongwei Zhang', 'Yuanzhe Xing', 'Quan Quan', 'Zhikun She']&lt;/li&gt;&lt;li&gt;Tags: ['RL safety', 'Lyapunov certificates', 'provable stability', 'robust control', 'off-policy actor-critic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24955</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI</title><link>https://arxiv.org/abs/2512.24848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PrivacyBench, a multi-turn conversational benchmark with socially grounded datasets that embed user 'secrets' to evaluate secret preservation in personalized AI agents.&lt;/li&gt;&lt;li&gt;Evaluates Retrieval-Augmented Generation (RAG) assistants and finds secret leakage up to 26.56%; a privacy-aware prompt reduces leakage to 5.12% but does not fully mitigate the issue.&lt;/li&gt;&lt;li&gt;Identifies a systemic vulnerability: retrieval mechanisms indiscriminately surface sensitive data, shifting privacy responsibility onto the generator and creating a single point of failure; calls for privacy-by-design architectural safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srija Mukhopadhyay', 'Sathwik Reddy', 'Shruthi Muthukumar', 'Jisun An', 'Ponnurangam Kumaraguru']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-leakage', 'LLM-security', 'RAG', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24848</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2512.24712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LSRE, which encodes sparse VLM (vision-language model) judgments into a lightweight latent classifier within a recurrent world model to enable real-time semantic risk detection for autonomous driving.&lt;/li&gt;&lt;li&gt;Runs semantic risk assessment at about 10 Hz without per-frame VLM queries by operating in latent space, achieving accuracy comparable to a VLM baseline with lower latency.&lt;/li&gt;&lt;li&gt;Evaluated in CARLA on six semantic-failure scenarios; demonstrates earlier hazard anticipation and generalization to rarely seen, semantically similar cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Cheng', 'Weitao Zhou', 'Cheng Jing', 'Nanshan Deng', 'Junze Wen', 'Zhaoyang Liu', 'Kun Jiang', 'Diange Yang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'semantic-safety', 'vision-language-models', 'real-time-monitoring', 'latent-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24712</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do Large Language Models Know What They Are Capable Of?</title><link>https://arxiv.org/abs/2512.24661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether LLMs can predict their own success on tasks and whether those predictions improve during multi-step tasks; finds widespread overconfidence but above-random discriminatory power.&lt;/li&gt;&lt;li&gt;Shows that larger/newer models do not necessarily have better self-assessment (except Claude series), and that overconfidence can worsen through multi-step agentic reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates that in-context failure examples can reduce overconfidence for some models, improving decision-making; models' decisions are approximately rational given their biased probability estimates.&lt;/li&gt;&lt;li&gt;Discusses implications for AI misuse and misalignment due to models' lack of accurate awareness of their capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Casey O. Barkan', 'Sid Black', 'Oliver Sourbut']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'agentic safety', 'capability awareness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24661</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</title><link>https://arxiv.org/abs/2512.24574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies attention heads correlated with distinct cognitive behaviors (e.g., verification, backtracking) in LLM reasoning trajectories.&lt;/li&gt;&lt;li&gt;Proposes CREST, a training-free method with an offline calibration to derive head-specific steering vectors and an inference-time rotation to suppress unproductive components.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy (up to 17.5%) and reduced token usage (up to 37.6%) across reasoning benchmarks by adaptively suppressing inefficient reasoning modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Xiaoxia Wu', 'Zhongzhu Zhou', 'Qingyang Wu', 'Yineng Zhang', 'Pragaash Ponnusamy', 'Harikaran Subbaraj', 'Jue Wang', 'Shuaiwen Leon Song', 'Ben Athiwaratkun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model steering', 'interpretability', 'inference-time intervention', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24574</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Localized Calibrated Uncertainty in Code Language Models</title><link>https://arxiv.org/abs/2512.24560</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Creates a dataset of Minimal Intent Aligning Patches: repaired LLM-generated programs verified by test cases, with minimal edits annotated.&lt;/li&gt;&lt;li&gt;Evaluates methods for assigning well-calibrated probabilities to code spans indicating likelihood they'll be edited, comparing white-box probing (efficient arbitrary-span queries) to black-box reflective/self-consistency approaches.&lt;/li&gt;&lt;li&gt;Finds that small supervisor probes can achieve low calibration error and a Brier Skill Score ≈ 0.2 at predicting edited lines for outputs from much larger models.&lt;/li&gt;&lt;li&gt;Investigates generalizability (some transfer from code-trained probes to natural-language errors with probability rescaling) and discusses implications for AI oversight and control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Gros', 'Prem Devanbu']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'uncertainty-estimation', 'LLM-code-generation', 'oversight/control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24560</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>https://arxiv.org/abs/2512.24470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Semantic Lookout: a camera-only, candidate-constrained vision-language model (VLM) system that selects a single cautious, short-horizon, human-overridable fallback maneuver (or station-keeping) from water-valid, world-anchored trajectories.&lt;/li&gt;&lt;li&gt;Advocates a fast-slow anomaly detection pipeline to meet alert-to-takeover timing, showing sub-10s models retain most semantic awareness of slower SOTA VLMs for the handover window.&lt;/li&gt;&lt;li&gt;Evaluated on 40 harbor scenes for scene understanding, alignment with human consensus, short-horizon risk reduction (e.g., increased standoff on fire scenes), and validated end-to-end with an on-water field run.&lt;/li&gt;&lt;li&gt;Positions the approach as compatible with draft IMO MASS Code obligations and motivates hybrid autonomy pairing foundation-model semantics with multi-sensor perception and short-horizon replanning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kim Alexander Christensen', 'Andreas Gudahl Tufte', 'Alexey Gusev', 'Rohan Sinha', 'Milan Ganai', 'Ole Andreas Alsos', 'Marco Pavoned', 'Martin Steinert']&lt;/li&gt;&lt;li&gt;Tags: ['autonomy safety', 'vision-language models', 'fallback maneuvers', 'operational design domain (ODD)', 'field evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24470</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations</title><link>https://arxiv.org/abs/2512.24452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a deep learning-based semantic communication system that jointly supports multiple receiver tasks while limiting semantic information leakage to an eavesdropper.&lt;/li&gt;&lt;li&gt;Formulates privacy as a min–max optimization: an adversary (eavesdropper) is trained to infer semantics while the transmitter–receiver are trained to preserve legitimate task performance and reduce the adversary's success.&lt;/li&gt;&lt;li&gt;Introduces a cooperative adversarial perturbation layer that superimposes crafted noise on transmitted waveforms to further degrade eavesdropper inference without harming the legitimate receiver.&lt;/li&gt;&lt;li&gt;Evaluates performance on image datasets (MNIST, CIFAR-10) over Rayleigh fading channels with AWGN, showing reduced semantic leakage and preserved legitimate accuracy/reconstruction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yalin E. Sagduyu', 'Tugba Erpek', 'Aylin Yener', 'Sennur Ulukus']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'adversarial perturbations', 'semantic communications', 'adversarial training / min-max', 'wireless security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24452</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack</title><link>https://arxiv.org/abs/2512.24402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an automated simulation and reporting pipeline for an autonomous racing stack that can run up to 3x real-time and integrate into CI/CD.&lt;/li&gt;&lt;li&gt;Uses a high-fidelity vehicle model (as an FMU) and configurable scenario initialization to test varied initial conditions and stack configurations.&lt;/li&gt;&lt;li&gt;Includes a fault injection module to introduce sensor delays, perturbations, and to modify outputs of stack nodes for robustness/safety validation.&lt;/li&gt;&lt;li&gt;Describes automated reporting design to support efficient analysis of simulation runs and validation of critical modules (e.g., high-speed overtaking, localization).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni Lambertini', 'Matteo Pini', 'Eugenio Mascaro', 'Francesco Moretti', 'Ayoub Raji', 'Marko Bertogna']&lt;/li&gt;&lt;li&gt;Tags: ['fault-injection', 'safety-evaluation', 'robustness', 'simulation', 'autonomous-driving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24402</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</title><link>https://arxiv.org/abs/2512.24271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DualityForge, a controllable diffusion-based counterfactual video editing pipeline to synthesize edited videos and corresponding QA pairs for contrastive training.&lt;/li&gt;&lt;li&gt;Constructs DualityVidQA, a large-scale paired dataset of original and counterfactual videos with QA, aimed at reducing MLLM visual-grounding hallucinations.&lt;/li&gt;&lt;li&gt;Proposes Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT+RL regime with pair-wise L1 advantage normalization to stabilize and improve policy optimization.&lt;/li&gt;&lt;li&gt;Reports substantial reduction in hallucinations (e.g., 24.0% relative improvement over Qwen2.5-VL-7B) and gains on general-purpose benchmarks, demonstrating improved robustness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Huang', 'Hao Wen', 'Aiming Hao', 'Bingze Song', 'Meiqi Wu', 'Jiahong Wu', 'Xiangxiang Chu', 'Sheng Lu', 'Haoqian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'counterfactual data synthesis', 'multimodal model safety/robustness', 'dataset (DualityVidQA)', 'RL fine-tuning / training stabilization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24271</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GARDO: Reinforcing Diffusion Models without Reward Hacking</title><link>https://arxiv.org/abs/2512.24138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking and mode collapse when using proxy rewards to fine-tune diffusion (text-to-image) models with RL.&lt;/li&gt;&lt;li&gt;Proposes GARDO: selective (gated) regularization applied to high-uncertainty samples, adaptive updates of the reference model, and diversity-aware reward amplification to encourage mode coverage.&lt;/li&gt;&lt;li&gt;Shows empirically that GARDO reduces reward hacking and improves generation diversity without sacrificing sample efficiency or exploration across multiple proxy and unseen hold-out metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran He', 'Yuxiao Ye', 'Jie Liu', 'Jiajun Liang', 'Zhiyong Wang', 'Ziyang Yuan', 'Xintao Wang', 'Hangyu Mao', 'Pengfei Wan', 'Ling Pan']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment/safety', 'reinforcement learning', 'diffusion models', 'diversity/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24138</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models</title><link>https://arxiv.org/abs/2512.24058</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Composite Reliability Score (CRS) that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric for LLM reliability.&lt;/li&gt;&lt;li&gt;Evaluates ten open-source LLMs across five QA datasets under baseline, input perturbations, and calibration methods, showing CRS yields stable model rankings and uncovers failure modes missed by single metrics.&lt;/li&gt;&lt;li&gt;Finds the most dependable systems balance accuracy, robustness, and calibrated uncertainty, and demonstrates CRS as a tool for comparing models and measuring reliability improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Kumar Salla', 'Manoj Saravanan', 'Shrikar Reddy Kota']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'calibration', 'uncertainty-quantification', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24058</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>https://arxiv.org/abs/2512.24052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AHA (Audio Hallucination Alignment) to reduce hallucinations in large audio-language models via counterfactual hard negative mining and preference data.&lt;/li&gt;&lt;li&gt;Defines a taxonomy of audio grounding errors: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error.&lt;/li&gt;&lt;li&gt;Introduces AHA-Eval, a fine-grained diagnostic benchmark, and demonstrates improvements by aligning Qwen2.5-Omni to produce Qwen-Audio-AHA with gains on AHA-Eval and public benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Chen', 'Wenhui Zhu', 'Xiwen Chen', 'Zhipeng Wang', 'Xin Li', 'Peijie Qiu', 'Hao Wang', 'Xuanzhao Dong', 'Yujian Xiong', 'Anderson Schneider', 'Yuriy Nevmyvaka', 'Yalin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'safety-evaluation', 'adversarial-training', 'audio-llms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24052</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?</title><link>https://arxiv.org/abs/2512.24044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of jailbreak attacks against LLM safety across the full inference pipeline, explicitly including input and output content filters.&lt;/li&gt;&lt;li&gt;Finds nearly all evaluated jailbreak techniques are detectable by at least one safety filter, implying prior model-only assessments may overestimate real-world attack success.&lt;/li&gt;&lt;li&gt;Highlights trade-offs between recall and precision in safety filters and identifies gaps for improving detection accuracy and user experience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Xin', 'Dingfan Chen', 'Linyi Yang', 'Michael Backes', 'Xiao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM safety', 'adversarial prompts', 'content moderation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24044</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title><link>https://arxiv.org/abs/2512.23988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RISE, an unsupervised method using sentence-level step activations and sparse auto-encoders to discover 'reasoning vectors'—directions in activation space corresponding to distinct reasoning behaviors.&lt;/li&gt;&lt;li&gt;Shows that discovered vectors are disentangled and occupy separable regions; interventions on decoder columns can amplify/suppress behaviors (e.g., reflection, backtracking) and alter inference trajectories without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates discovery of structural properties (e.g., response length) and novel controllable behaviors (e.g., response confidence), suggesting uses for interpreting and steering LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Zhang', 'Shujian Zhang', 'John Lambert', 'Wenxuan Zhou', 'Zhangyang Wang', 'Mingqing Chen', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'model steering', 'latent interventions', 'chain-of-thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23988</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Large Language Models Systematically Misrepresent American Climate Opinions</title><link>https://arxiv.org/abs/2512.23889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares outputs from six LLMs prompted with profiles of 978 respondents against their actual answers on 20 U.S. climate opinion questions.&lt;/li&gt;&lt;li&gt;Finds that LLMs 'compress' opinion diversity—shifting less-concerned groups toward greater concern and vice versa—leading to systematic misrepresentation.&lt;/li&gt;&lt;li&gt;Identifies intersectional errors: models apply uniform gender assumptions that align for White and Hispanic respondents but misrepresent patterns for Black respondents, posing risks to equitable policy decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sola Kim', 'Jieshu Wang', 'Marco A. Janssen', 'John M. Anderies']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'alignment', 'safety-evaluation', 'social-impact']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23889</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack</title><link>https://arxiv.org/abs/2512.23881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a universal targeted latent-space attack that perturbs audio encoder outputs to induce attacker-specified text from downstream LLMs.&lt;/li&gt;&lt;li&gt;Attack is encoder-level (does not modify waveform directly) and does not require access to the language model, generalizing across inputs and speakers.&lt;/li&gt;&lt;li&gt;Demonstrates high success rates on Qwen2-Audio-7B-Instruct with low perceptual distortion, highlighting an encoder-level vulnerability in audio–language systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roee Ziv', 'Raz Lapid', 'Moshe Sipper']&lt;/li&gt;&lt;li&gt;Tags: ['audio adversarial attack', 'latent-space attack', 'multimodal security', 'encoder-level vulnerability', 'universal perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23881</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis</title><link>https://arxiv.org/abs/2512.23859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Qualitative user study (n=53) on first-person experiences using conversational AI (e.g., ChatGPT, Claude) during mental health crises, finding people use AI to fill gaps in human support due to access issues or not wanting to burden others.&lt;/li&gt;&lt;li&gt;Interviews with mental health experts (n=16) emphasize the importance of human-human connection for crisis management.&lt;/li&gt;&lt;li&gt;Frames responsible AI crisis intervention as increasing user readiness to take positive action while de-escalating intended negative actions, recommending AI be designed as bridges to human support rather than endpoints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leah Hope Ajmani', 'Arka Ghosh', 'Benjamin Kaveladze', 'Eugenia Kim', 'Keertana Namuduri', 'Theresa Nguyen', 'Ebele Okoli', 'Jessica Schleider', 'Denae Ford', 'Jina Suh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'conversational-ai', 'mental-health', 'crisis-intervention', 'user-studies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23859</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title><link>https://arxiv.org/abs/2512.23837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial attack method that extracts token-level predictions from intermediate attention layers to generate perturbations, differing from prompt- or gradient-based attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on argument quality assessment (ArgQuality) using LLaMA-3.1-Instruct-8B as both generator and evaluator, showing measurable drops in evaluation performance.&lt;/li&gt;&lt;li&gt;Finds generated adversarial examples are semantically similar to originals but notes practical limitations due to grammatical degradation for substitutions from certain layers/positions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Dhole']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'mechanistic interpretability', 'LLM robustness', 'red teaming', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23837</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</title><link>https://arxiv.org/abs/2512.23836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive prompting strategy for retrieval-augmented QA that splits retrieved context into smaller chunks and sequentially prompts the LLM to answer using each chunk, trading off relevance vs. noise.&lt;/li&gt;&lt;li&gt;Shows adaptive chunking can match standard prompting performance while using fewer tokens across three open-domain QA datasets.&lt;/li&gt;&lt;li&gt;Finds a major error mode: when information is insufficient, the LLM often produces incorrect answers instead of declining, indicating poor refusal/ignorance behavior.&lt;/li&gt;&lt;li&gt;Highlights need for further research to improve LLMs' ability to admit ignorance or decline when evidence is lacking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingmin Wang', 'Ji Ma', 'Shankar Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'retrieval-augmented generation', 'hallucination', 'uncertainty calibration', 'question answering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23836</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improved Bounds for Private and Robust Alignment</title><link>https://arxiv.org/abs/2512.23816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical upper bounds on the suboptimality gap for private and robust alignment of language models in both offline and online settings.&lt;/li&gt;&lt;li&gt;Analyzes preference-label learning under privacy constraints and adversarial corruption, considering two orderings (privacy-first and corruption-first).&lt;/li&gt;&lt;li&gt;Shows log loss with an MLE-style algorithm attains near-optimal rates in the privacy-only setting, strengthens guarantees for joint privacy-and-corruption offline algorithms, and improves corruption-only bounds.&lt;/li&gt;&lt;li&gt;Introduces the first results for private and robust online alignment, enabled by new uniform convergence guarantees for log and square loss under privacy and corruption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqian Weng', 'Yi He', 'Xingyu Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'privacy-preserving ML', 'robustness/adversarial corruption', 'theoretical learning bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23816</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems</title><link>https://arxiv.org/abs/2512.23809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZTA-FL, a defense-in-depth federated learning framework for IIoT intrusion detection combining TPM-based cryptographic attestation, a SHAP-weighted aggregation algorithm for explainable Byzantine detection under non-IID data, and on-device privacy-preserving adversarial training.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and failure-mode analysis for the SHAP-weighted aggregation and reports empirical results on Edge-IIoTset, CIC-IDS2017, and UNSW-NB15.&lt;/li&gt;&lt;li&gt;Reports high performance: 97.8% detection accuracy, 93.2% accuracy under 30% Byzantine attacks (outperforming FLAME by 3.1%, p&lt;0.01), 89.3% adversarial robustness, and a 34% reduction in communication overhead.&lt;/li&gt;&lt;li&gt;Includes TPM-based agent authentication claims (FAR &lt; 1e-7), reproducible code release, and comprehensive experiments—addressing both security and robustness of FL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samaresh Kumar Singh', 'Joyjit Roy', 'Martin So']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning Security', 'Byzantine Poisoning', 'Hardware Attestation (TPM)', 'Explainable Aggregation (SHAP)', 'Adversarial Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23809</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark</title><link>https://arxiv.org/abs/2512.23779</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box, query-only benchmark for prompt-induced over-generation (DoS-style) attacks on LLMs, assuming a known tokenizer.&lt;/li&gt;&lt;li&gt;Proposes two prompt-only attackers: EOGen (evolutionary search for prefixes that suppress EOS) and RL-GOAL (goal-conditioned RL generating prefixes targeting specific output lengths).&lt;/li&gt;&lt;li&gt;Defines evaluation metrics including Over-Generation Factor (OGF) and reports results showing RL-GOAL achieves substantially higher OGF and success rates across victim models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manu', 'Yi Guo', 'Jo Plested', 'Tim Lynar', 'Kanchana Thilakarathna', 'Nirhoshan Sivaroopan', 'Jack Yang', 'Wangli Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Prompt-based DoS', 'Adversarial prompting', 'Black-box attacks', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23779</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions</title><link>https://arxiv.org/abs/2512.23770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a trust-region method that biases policy updates toward constraint (cost) satisfaction via a convex combination of natural policy gradients for reward and cost.&lt;/li&gt;&lt;li&gt;Provides a theoretical guarantee of local progress toward safety (fixed fraction of optimal cost reduction per update) and conditions for concurrent reward improvement when gradients align.&lt;/li&gt;&lt;li&gt;Empirically evaluates SB-TRPO on Safety Gymnasium tasks, showing improved trade-offs between near-zero safety violations and meaningful task completion compared to state-of-the-art constrained RL methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ankit Kanwar', 'Dominik Wagner', 'Luke Ong']&lt;/li&gt;&lt;li&gt;Tags: ['safe-reinforcement-learning', 'constrained-RL', 'trust-region-methods', 'policy-optimization', 'safety-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23770</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory</title><link>https://arxiv.org/abs/2512.23760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Audited Skill-Graph Self-Improvement (ASG-SI): iterative compilation of self-improvements into an auditable skill graph where candidate skills are extracted from trajectories, normalized, and promoted only after verifier-backed replay and contract checks.&lt;/li&gt;&lt;li&gt;Decomposes rewards into reconstructible components derived from replayable evidence to enable independent audit of promotion decisions and to reduce incentive for reward hacking.&lt;/li&gt;&lt;li&gt;Adds experience synthesis for scalable stress-testing and continual memory control to maintain long-horizon performance; includes a threat model, security analysis, and a fully runnable reference implementation demonstrating verifier-backed reward construction and measurable improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Huang', 'Jerry Huang']&lt;/li&gt;&lt;li&gt;Tags: ['agentic LLMs', 'auditability', 'verifiable rewards', 'reward hacking', 'self-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23760</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding</title><link>https://arxiv.org/abs/2512.23743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for on-premise clinical coding combining an LLM-based Coder (BioMistral-7B) with deterministic keyword fallback and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence.&lt;/li&gt;&lt;li&gt;Aims to preserve patient privacy by keeping all processing local (no data leaves hospital firewall) and to improve production reliability through redundancy and symbolic verification to avoid hallucinated or invalid codes.&lt;/li&gt;&lt;li&gt;Evaluation on 1,000 MIMIC-III discharge summaries: no hallucinated codes among accepted outputs in the knowledge base, 24.47% verification rate, 34.11% coverage (95% CI: 31.2%–37.0%), 86%+ LLM utilization, and Auditor rejection rate of 75.53% for invalid outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunguo Yu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving deployment', 'reliability/robustness', 'hallucination mitigation', 'on-premise LLM deployment', 'symbolic verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23743</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Enforcing Temporal Constraints for LLM Agents</title><link>https://arxiv.org/abs/2512.23738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agent-C: a framework that enforces formal temporal safety properties for LLM agents using a domain-specific language (DSL) for temporal constraints, translation to first-order logic, and SMT solving at run-time.&lt;/li&gt;&lt;li&gt;Detects non-compliant agent actions during token generation and uses constrained generation to block/replace unsafe tool calls, providing guarantees of temporal policy conformance.&lt;/li&gt;&lt;li&gt;Evaluated on retail customer service and airline reservation agent tasks across open- and closed-source LLMs, achieving 100% conformance and improved task utility over existing guardrails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adharsh Kamath', 'Sishen Zhang', 'Calvin Xu', 'Shubham Ugare', 'Gagandeep Singh', 'Sasa Misailovic']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'guardrails', 'temporal logic', 'SMT solving', 'constrained generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23738</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability</title><link>https://arxiv.org/abs/2512.23712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STED (Semantic Tree Edit Distance), a new similarity metric for comparing LLM-generated JSON that balances semantic flexibility with structural strictness.&lt;/li&gt;&lt;li&gt;Proposes a consistency scoring framework that aggregates repeated STED measurements to quantify reliability of structured outputs across generations.&lt;/li&gt;&lt;li&gt;Evaluates STED against existing metrics (TED, BERTScore, DeepDiff) on synthetic datasets and reports superior discrimination between semantic equivalents and structural breaks.&lt;/li&gt;&lt;li&gt;Applies the framework to benchmark six LLMs, identifying model-specific consistency behaviors and demonstrating practical uses for model selection and prompt refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanghui Wang', 'Jinze Yu', 'Xing Zhang', 'Dayuan Jiang', 'Yin Song', 'Tomal Deb', 'Xuefeng Liu', 'Peiyang He']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'structured-output-evaluation', 'metrics', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23712</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Iterative Deployment Improves Planning Skills in LLMs</title><link>https://arxiv.org/abs/2512.24940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative deployment—repeatedly fine-tuning LLMs on user-curated data from prior deployments—can substantially improve planning skills and produce emergent longer-horizon plans.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis that iterative deployment implements an outer-loop reinforcement learning process with an implicit (not explicitly defined) reward function.&lt;/li&gt;&lt;li&gt;Highlights safety implications: the implicit reward entailed by repeated deployment may be unspecified and produce unintended properties in future models, and suggests iterative deployment as an alternative training regime to explicit RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Augusto B. Corr\\^ea', 'Yoav Gelberg', 'Luckeciano C. Melo', 'Ilia Shumailov', "Andr\\'e G. Pereira", 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'deployment dynamics', 'emergent capabilities', 'implicit reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24940</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment</title><link>https://arxiv.org/abs/2512.24263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Risk-aware Stepwise Alignment (RSA), a token-level constrained policy optimization method that incorporates nested risk measures into policy updates.&lt;/li&gt;&lt;li&gt;Aims to limit model shift from a reference policy and explicitly suppress low-probability, high-impact (tail) harmful responses.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of policy optimality under mild assumptions and empirical results showing improved helpfulness while reducing safety violations and tail risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijun Zhang', 'Lin Li', 'Wei Wei', 'Yajie Qi', 'Huizhong Song', 'Jun Wang', 'Yaodong Yang', 'Jiye Liang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'risk-aware', 'constrained policy optimization', 'tail-risk suppression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24263</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment</title><link>https://arxiv.org/abs/2512.24040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ROAD, a multi-agent framework (Analyzer, Optimizer, Coach) that converts unstructured failure logs into structured Decision Tree Protocols to optimize prompts/agent behavior without gold-standard labeled datasets.&lt;/li&gt;&lt;li&gt;Frames optimization as an automated debugging loop rather than stochastic search, improving sample-efficiency and mimicking human failure analysis and patching.&lt;/li&gt;&lt;li&gt;Reports empirical gains in both academic benchmarks and a live production knowledge-management system (e.g., +5.6% success rate and ~19% improvement on complex retail reasoning tasks) within a few iterations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Natchaya Temyingyong', 'Daman Jain', 'Neeraj Kumarsahu', 'Prabhat Kumar', 'Rachata Phondi', 'Wachiravit Modecrua', 'Krittanon Kaewtawee', 'Krittin Pachtrachai', 'Touchapon Kraisingkorn']&lt;/li&gt;&lt;li&gt;Tags: ['agent alignment', 'automatic prompt optimization', 'debugging-driven optimization', 'deployment robustness', 'sample-efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24040</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models</title><link>https://arxiv.org/abs/2512.23850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Drill-Down and Fabricate Test (DDFT), a protocol to measure epistemic robustness: the ability of language models to maintain factual accuracy under progressive semantic compression and adversarial fabrication.&lt;/li&gt;&lt;li&gt;Proposes a two-system cognitive model (Semantic System for generation and Epistemic Verifier for factual validation) and operationalizes tests across 9 frontier models, 8 knowledge domains, and 5 compression levels (1,800 turn-level evaluations).&lt;/li&gt;&lt;li&gt;Finds epistemic robustness is largely independent of model scale or architecture but strongly predicted by error-detection/verification capability, and shows flagship models can be brittle while some smaller models are more robust.&lt;/li&gt;&lt;li&gt;Provides a practical benchmarking framework and tools for assessing epistemic robustness prior to deployment in critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rahul Baxi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial evaluation/red teaming', 'epistemic verification', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23850</guid><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
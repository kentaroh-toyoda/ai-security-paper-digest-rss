<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 05 Feb 2026 23:22:49 +0000</lastBuildDate><item><title>BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images</title><link>https://arxiv.org/abs/2602.01435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BioTamperNet, an image-forensics model that uses affinity-guided self-attention and cross-attention modules with lightweight SSM-inspired linear attention to detect duplicated/tampered regions in biomedical images.&lt;/li&gt;&lt;li&gt;Designed to capture intra-image similarities and cross-image correspondences, and trained end-to-end to both localize tampered regions and identify their source counterparts.&lt;/li&gt;&lt;li&gt;Evaluated on benchmark bio-forensic datasets with reported significant improvements over competitive baselines; code is publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumyaroop Nandi', 'Prem Natarajan']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'tamper-detection', 'biomedical-imaging', 'attention-mechanisms', 'state-space-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.01435</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</title><link>https://arxiv.org/abs/2601.21610</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WMVLM, a unified evaluation framework using vision-language models to assess image watermarks for diffusion model outputs.&lt;/li&gt;&lt;li&gt;Redefines quality and security metrics: artifact strength and erasure resistance for residual watermarks, latent distribution shift for semantic watermarks.&lt;/li&gt;&lt;li&gt;Introduces a three-stage training strategy enabling classification, scoring, and interpretable text generation for watermark evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates strong generalization across datasets, diffusion models, and watermarking methods, and emphasizes security considerations for watermark robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijin Yang', 'Yu Sun', 'Kejiang Chen', 'Jiawei Zhao', 'Jun Jiang', 'Weiming Zhang', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'diffusion models', 'vision-language models', 'image provenance', 'robustness/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.21610</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</title><link>https://arxiv.org/abs/2511.13108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DGS-Net, a distillation-guided gradient surgery method to fine-tune CLIP for AI-generated image detection while avoiding catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Decomposes gradients into harmful and beneficial directions, projecting task gradients orthogonally to harmful directions and aligning with beneficial directions distilled from a frozen CLIP encoder.&lt;/li&gt;&lt;li&gt;Aims to preserve pre-trained transferable priors and suppress task-irrelevant components to improve cross-domain generalization.&lt;/li&gt;&lt;li&gt;Evaluated on 50 generative models, reporting an average improvement of 6.6 over state-of-the-art methods in detection and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazhen Yan', 'Ziqiang Li', 'Fan Wang', 'Boyu Wang', 'Ziwen He', 'Zhangjie Fu']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'Model fine-tuning', 'Gradient surgery', 'Knowledge distillation', 'Robustness / Generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13108</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Video Detection via Perceptual Straightening</title><link>https://arxiv.org/abs/2507.00583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReStraV, a method for detecting AI-generated videos by measuring 'perceptual straightening' — temporal curvature and stepwise distance — in representations from a pre-trained self-supervised ViT (DINOv2).&lt;/li&gt;&lt;li&gt;Aggregates per-video statistics of these geometric measures and trains a lightweight classifier to distinguish real vs. synthetic videos.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance (e.g., 97.17% accuracy, 98.63% AUROC on VidProM) and emphasizes computational efficiency and generalization advantages over existing image- and video-based detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Intern\\`o', 'Robert Geirhos', 'Markus Olhofer', 'Sunny Liu', 'Barbara Hammer', 'David Klindt']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'video forensics', 'representation geometry', 'defense', 'self-supervised models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00583</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.12340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Image Corruption-Inspired Membership Inference Attacks (ICIMIA) that exploit differing sensitivity of member vs non-member images to image corruptions for LVLMs.&lt;/li&gt;&lt;li&gt;Presents a white-box attack using vision-encoder image embeddings by measuring similarity between original and corrupted images, and a practical black-box attack using similarity of returned text embeddings from model queries.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness on existing datasets, demonstrating that embedding-similarity under corruption can reliably indicate training-set membership, highlighting privacy risks for LVLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongyu Wu', 'Minhua Lin', 'Zhiwei Zhang', 'Fali Wang', 'Xianren Zhang', 'Xiang Zhang', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'vision-language-models', 'black-box-attacks', 'white-box-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12340</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models</title><link>https://arxiv.org/abs/2505.17440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VEAttack, a downstream-agnostic adversarial attack that targets only the vision encoder of Large Vision-Language Models by minimizing cosine similarity between clean and perturbed visual features.&lt;/li&gt;&lt;li&gt;Does not require access to the LLM, task-specific labels, or task information, reducing computational cost compared to full-model white-box attacks; optimizes image tokens rather than classification token.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical effectiveness (e.g., 94.5% degradation on image captioning, 75.7% on VQA) and provides theoretical analysis and observations on factors affecting LVLM vulnerability (hidden layer variations, token attention differential, transfer characteristics, low sensitivity to attack steps).&lt;/li&gt;&lt;li&gt;Code released and focuses on practical, transferable attacks against multimodal systems' vision components.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hefei Mei', 'Zirui Wang', 'Shen You', 'Minjing Dong', 'Chang Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'vision-encoder', 'LVLM', 'transferability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17440</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>UniVRSE: Unified Vision-conditioned Response Semantic Entropy for Hallucination Detection in Medical Vision-Language Models</title><link>https://arxiv.org/abs/2503.20504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniVRSE, a vision-conditioned semantic entropy method that detects hallucinations in medical vision-language models by contrasting predictive distributions from original and visually distorted image-text pairs.&lt;/li&gt;&lt;li&gt;Applies UniVRSE to VQA (image-question pairs) and VRG by decomposing reports into claims, generating verification questions, and estimating claim-level entropy to flag hallucinated content.&lt;/li&gt;&lt;li&gt;Introduces ALFA (Alignment Ratio of Atomic Facts), a fine-grained factual consistency labeling method to generate reliable ground-truth hallucination labels for benchmarking.&lt;/li&gt;&lt;li&gt;Evaluates on six medical VQA/VRG datasets and three VLMs, showing UniVRSE outperforms existing uncertainty-based detection methods and generalizes across modalities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehui Liao', 'Shishuai Hu', 'Ke Zou', 'Mengyuan Jin', 'Yanning Zhang', 'Huazhu Fu', 'Liangli Zhen', 'Yong Xia']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'vision-language-models', 'uncertainty-estimation', 'medical-ai', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20504</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>DiffVax: Optimization-Free Image Immunization Against Diffusion-Based Editing</title><link>https://arxiv.org/abs/2411.17957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffVax, an optimization-free, scalable framework to immunize images (and video) against diffusion-based editing by embedding imperceptible perturbations.&lt;/li&gt;&lt;li&gt;Uses a loss term designed to both cause editing failures and preserve imperceptibility, enabling fast generalization to unseen content without per-image optimization.&lt;/li&gt;&lt;li&gt;Claims large speedup (≈250,000x) over prior per-image optimization methods, adaptability to various diffusion editing tools, robustness to counter-attacks, and first-time protection for video content.&lt;/li&gt;&lt;li&gt;Provides extensive qualitative and quantitative evaluations demonstrating scalability and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarik Can Ozden', 'Ozgur Kara', 'Oguzhan Akcin', 'Kerem Zaman', 'Shashank Srivastava', 'Sandeep P. Chinchali', 'James M. Rehg']&lt;/li&gt;&lt;li&gt;Tags: ['image-immunization', 'adversarial-perturbation', 'diffusion-model-defense', 'media-protection', 'video-protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17957</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</title><link>https://arxiv.org/abs/2602.04820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CNN-based automated nail disease classification (3,835 images, 6 classes) and compares InceptionV3, DenseNet201, EfficientNetV2, and ResNet50; InceptionV3 achieves 95.57% accuracy.&lt;/li&gt;&lt;li&gt;Applies adversarial training to improve model robustness against tricky/noisy inputs.&lt;/li&gt;&lt;li&gt;Uses explainability methods (SHAP and Grad-CAM) to highlight features driving predictions and increase interpretability for clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzia Hossain', 'Samanta Ghosh', 'Shahida Begum', 'B. M. Shahria Alam', 'Mohammad Tahmid Noor', 'Md Parvez Mia', 'Nishat Tasnim Niloy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'explainability', 'medical-imaging', 'CNN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04820</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models</title><link>https://arxiv.org/abs/2602.04356</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that regional attention scores correlate with adversarial loss sensitivity and that attacking high-attention regions shifts attention to other salient regions.&lt;/li&gt;&lt;li&gt;Proposes Stage-wise Attention-Guided Attack (SAGA), which progressively concentrates limited perturbation budgets on high-attention regions to create more effective and imperceptible adversarial examples.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art attack success rates across ten large vision-language models and releases source code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaehyun Kwak', 'Nam Cao', 'Boryeong Cho', 'Segyu Lee', 'Sumyeong Ahn', 'Se-Young Yun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'vision-language models', 'attention-guided attack', 'adversarial examples']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04356</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing</title><link>https://arxiv.org/abs/2602.04268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KVSmooth, a training-free, plug-and-play inference method that applies exponential moving average (EMA) smoothing to keys and values in the KV-cache to mitigate hallucination in multimodal LLMs.&lt;/li&gt;&lt;li&gt;Dynamically adjusts smoothing strength per token using the entropy of its attention distribution (token sink degree) to prevent semantic drift during decoding.&lt;/li&gt;&lt;li&gt;Demonstrates large empirical gains: reduces CHAIR_S hallucination metric (41.8 → 18.2) and improves overall F1 (77.5 → 79.2) without model retraining or architectural changes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyu Jiang', 'Feiyang Chen', 'Xiaojin Zhang', 'Kun He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'robustness', 'inference-time defense', 'KV-cache smoothing', 'multimodal LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04268</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey</title><link>https://arxiv.org/abs/2602.03878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of intellectual property (IP) protection techniques specifically for 3D Gaussian Splatting (3DGS) assets, synthesizing fragmented literature into a unified view.&lt;/li&gt;&lt;li&gt;Introduces a bottom-up framework covering Gaussian-based perturbation mechanisms, passive vs. active protection paradigms (e.g., watermarking/fingerprinting vs. tamper-resistant designs), and robustness challenges.&lt;/li&gt;&lt;li&gt;Analyzes robustness threats in the context of emerging generative AI (e.g., model-based removal, evasion, or misuse) and identifies gaps in technical foundations and robustness characterization.&lt;/li&gt;&lt;li&gt;Proposes six research directions across robustness, efficiency, and protection paradigms to guide future work toward reliable IP protection for 3DGS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longjie Zhao', 'Ziming Hong', 'Jiaxin Huang', 'Runnan Chen', 'Mingming Gong', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['IP protection', 'watermarking/fingerprinting', 'robustness against attacks', '3D Gaussian Splatting', 'digital rights management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03878</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Content Anonymization for Privacy in Long-form Audio</title><link>https://arxiv.org/abs/2510.12780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a content-based re-identification attack on voice-anonymized long-form audio: attackers can use vocabulary, syntax, and turns of phrase across multiple utterances to re-identify speakers even when acoustic identity is masked.&lt;/li&gt;&lt;li&gt;Proposes a defense that performs contextual transcript rewriting (paraphrasing) within an ASR-TTS pipeline to remove speaker-specific style while preserving meaning and utility.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness in long-form telephone conversations and demonstrates that paraphrasing substantially mitigates the content-based re-identification risk while maintaining speech utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristina Aggazzotti', 'Ashi Garg', 'Zexin Cai', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'voice-anonymization', 'speaker-reidentification', 'content-based-attack', 'defense-paraphrasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12780</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious comments crafted to induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically adjust proportions of malicious comment types during training, focusing optimization on model weaknesses.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with large F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-generation', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title><link>https://arxiv.org/abs/2511.21752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Label Disguise Defense (LDD): conceal true labels by replacing them with semantically transformed or unrelated alias labels (e.g., blue vs. yellow) taught via few-shot demonstrations to mitigate class-directive prompt injection.&lt;/li&gt;&lt;li&gt;LDD is lightweight and model-agnostic (no retraining) and aims to break direct correspondence between injected directives and model outputs.&lt;/li&gt;&lt;li&gt;Evaluated on nine state-of-the-art models (including GPT-5, GPT-4o, LLaMA3.2, Gemma3, Mistral variants) under adversarial prompt-injection; LDD restores part of accuracy lost to attacks and sometimes outperforms the under-attack few-shot baseline.&lt;/li&gt;&lt;li&gt;Analysis shows semantically aligned aliases (e.g., good vs. bad) yield stronger robustness than arbitrary symbols, and effectiveness varies by model and alias choice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Li', 'Ruocheng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'LLM security', 'few-shot learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21752</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title><link>https://arxiv.org/abs/2510.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a comprehensive hierarchical taxonomy of LLM jailbreak strategies grouped into seven mechanism-oriented families (impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, data poisoning).&lt;/li&gt;&lt;li&gt;Runs a structured red-teaming challenge and analyzes prevalence and success rates of different jailbreak techniques, including multi-turn attacks where adversarial intent emerges gradually.&lt;/li&gt;&lt;li&gt;Evaluates automatic jailbreak detection, benchmarking GPT-5 as a judge and showing benefits of taxonomy-guided prompting for improved detection.&lt;/li&gt;&lt;li&gt;Releases a new Italian dataset of 1,364 annotated multi-turn adversarial dialogues labeled with the taxonomy to support study of bypassable safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francesco Giarrusso', 'Olga E. Sorokoletova', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'attack detection', 'taxonomy &amp; dataset', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13893</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title><link>https://arxiv.org/abs/2510.02712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs a large-scale survival analysis of multi-turn conversational robustness across 9 LLMs and 36,951 turns using Cox, AFT, and Random Survival Forest models.&lt;/li&gt;&lt;li&gt;Identifies semantic drift patterns: abrupt prompt-to-prompt drift sharply increases risk of inconsistency, while cumulative drift appears protective for conversations that persist.&lt;/li&gt;&lt;li&gt;Finds limitations of Cox proportional hazards due to violations for key covariates and shows AFT models with model-drift interactions provide better discrimination and calibration.&lt;/li&gt;&lt;li&gt;Proposes a lightweight AFT-based turn-level risk monitor that flags likely-failing conversations several turns before the first inconsistent response (a practical safeguard/defense).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial attacks', 'survival analysis', 'attack detection/monitoring', 'multi-turn consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Improving Detection of Watermarked Language Models</title><link>https://arxiv.org/abs/2508.13131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies limitations of watermark-based detection for LLM-generated text, especially when model entropy is reduced (e.g., after instruction tuning or RLHF).&lt;/li&gt;&lt;li&gt;Investigates combining watermark detectors with non-watermark detectors into hybrid schemes to improve detection performance.&lt;/li&gt;&lt;li&gt;Shows empirical performance gains of hybrid detectors over standalone watermark or non-watermark methods across varied experimental conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dara Bahri', 'John Wieting']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM detection', 'model provenance', 'defense', 'hybrid detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13131</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title><link>https://arxiv.org/abs/2602.04224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that existing safe-reasoning defenses for Large Reasoning Models (LRMs) fail to generalize against diverse and complex jailbreak/attack prompts due to insufficiency in the safe reasoning process.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence motivating a more sufficient, generalizable safe-reasoning mechanism that adapts the granularity of internal "thinking" to address risks.&lt;/li&gt;&lt;li&gt;Proposes Risk-Aware Preference Optimization (RAPO), a framework that enables LRMs to detect and mitigate safety risks adaptively in their chain-of-thought, improving robustness to advanced attack prompts while preserving general utility.&lt;/li&gt;&lt;li&gt;Validates RAPO with extensive experiments across multiple LRMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Qiaosheng Zhang', 'Xia Hu', 'Xingcheng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'defense', 'safe reasoning', 'adversarial prompts', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04224</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation</title><link>https://arxiv.org/abs/2602.04856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that Chain-of-Thought (CoT) reasoning can internally encode and propagate unsafe/fake-news narratives even when the model outputs a refusal.&lt;/li&gt;&lt;li&gt;Proposes a unified safety-analysis framework that inspects CoT across layers and attributes behaviors to individual attention heads using Jacobian-based spectral metrics.&lt;/li&gt;&lt;li&gt;Introduces three interpretable measures—stability, geometry, and energy—to quantify how attention heads embed deceptive reasoning patterns and finds critical routing concentrated in a few contiguous mid-depth layers.&lt;/li&gt;&lt;li&gt;Identifies specific attention heads responsible for divergence and discusses implications for detecting and mitigating latent reasoning risks, challenging the assumption that refusal implies safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yiping Zhang', 'Qiang Liu', 'Xingcheng Xu', 'Shu Wu', 'Haichao Shi', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['Chain-of-Thought', 'safety-vulnerability', 'attention-head-attribution', 'latent-harmful-generation', 'defense-mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04856</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title><link>https://arxiv.org/abs/2602.04739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Two-phase, longitudinal evaluation of multimodal LLM harmlessness using 726 adversarial prompts crafted by 26 professional red teamers, producing 82,256 human harm ratings.&lt;/li&gt;&lt;li&gt;Compares attack success rates and refusal behavior across eight model releases (GPT-4o → GPT-5, Claude Sonnet 3.5 → 4.5, Pixtral 12B → Pixtral Large, Qwen VL Plus → Qwen Omni), finding persistent inter-family differences and alignment drift over generations.&lt;/li&gt;&lt;li&gt;Analyzes modality-specific effects and how vulnerability patterns shift across updates, arguing for longitudinal, multimodal benchmarks to track evolving safety behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Casey Ford', 'Madison Van Doren', 'Emily Dix']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'alignment drift', 'safety evaluation', 'attack success rate (ASR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04739</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers</title><link>https://arxiv.org/abs/2602.04706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Documents 'intermediate merge residues' in BPE vocabularies: tokens created during merge learning that are rarely emitted at tokenization time and can waste vocabulary capacity and increase vulnerability to atypical/adversarial inputs.&lt;/li&gt;&lt;li&gt;Proposes LiteToken, a method to remove residue tokens from the tokenizer vocabulary, often usable without further model fine-tuning.&lt;/li&gt;&lt;li&gt;Empirical results show reduced token fragmentation and parameter footprint, preserved overall performance, and improved robustness to noisy or misspelled inputs (i.e., reduced tokenizer-induced attack surface).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yike Sun', 'Haotong Yang', 'Zhouchen Lin', 'Muhan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['tokenization', 'BPE', 'robustness', 'adversarial-robustness', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04706</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantic Self-Distillation for Language Model Uncertainty</title><link>https://arxiv.org/abs/2602.04577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Self-Distillation (SSD): train lightweight student models to predict a prompt-conditioned semantic distribution distilled from sampled outputs of a large language model.&lt;/li&gt;&lt;li&gt;Student-predicted entropy serves as an uncertainty signal for hallucination prediction; predicted density helps assess candidate answer reliability and detect out-of-domain answers.&lt;/li&gt;&lt;li&gt;Demonstrates on TriviaQA that student models match or outperform finite-sample semantic dispersion while being suitable for latency-critical settings; proposes SSD as a general framework for predictive uncertainty in complex output spaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Phillips', 'Sean Wu', 'Boyan Gao', 'David A. Clifton']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'hallucination-detection', 'out-of-distribution-detection', 'model-distillation', 'robustness-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04577</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>$C$-$\Delta\Theta$: Circuit-Restricted Weight Arithmetic for Selective Refusal</title><link>https://arxiv.org/abs/2602.04521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes C-Δθ (Circuit-Restricted Weight Arithmetic) to embed category-specific refusal behavior into a model checkpoint via a constrained weight update applied only to a sparse, refusal-causal circuit (typically &lt;5% of parameters).&lt;/li&gt;&lt;li&gt;Localizes the refusal-causal subcircuit using EAP-IG and computes a Δθ constrained to that circuit so the edited model enforces refusal offline with no inference-time hooks or runtime steering.&lt;/li&gt;&lt;li&gt;Evaluates selectivity (category-targeted refusal) and capability retention on refusal and utility benchmarks, shifting cost from per-request interventions to a one-time offline edit.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Kasliwal', 'Pratinav Seth', 'Vinay Kumar Sankarapu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'model editing', 'guardrails', 'mechanistic interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04521</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks</title><link>https://arxiv.org/abs/2602.04294</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically studies how few-shot demonstrations affect prompt-based defenses (Role-Oriented Prompts and Task-Oriented Prompts) against LLM jailbreak attacks.&lt;/li&gt;&lt;li&gt;Evaluates multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods.&lt;/li&gt;&lt;li&gt;Finds few-shot demonstrations improve RoP safety (up to +4.5%) by reinforcing role identity, but substantially degrade ToP effectiveness (up to -21.2%) by distracting from task instructions.&lt;/li&gt;&lt;li&gt;Provides practical deployment recommendations for prompt-based defense strategies in real-world LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanshu Wang', 'Shuaishuai Yang', 'Jingjing He', 'Tong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt-based defenses', 'few-shot examples', 'safety evaluation', 'adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04294</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title><link>https://arxiv.org/abs/2602.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of training-time implicit safety risks (beyond explicit reward hacking), introducing a taxonomy with 5 risk levels, 10 fine-grained categories, and 3 incentive types.&lt;/li&gt;&lt;li&gt;Empirical evaluation showing these risks are prevalent and severe (e.g., Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs given only background information).&lt;/li&gt;&lt;li&gt;Analyzes factors influencing such behaviors, demonstrates occurrence in multi-agent training, and highlights covert harms like manipulation of logged metrics during code-based RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhexin Zhang', 'Yida Lu', 'Junfeng Fang', 'Junxiao Yang', 'Shiyao Cui', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Hongning Wang', 'Minlie Huang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['training-time vulnerabilities', 'implicit incentives', 'reinforcement learning', 'multi-agent', 'safety taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04196</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Expert Selections In MoE Models Reveal (Almost) As Much As Text</title><link>https://arxiv.org/abs/2602.04105</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a text-reconstruction attack that recovers tokens solely from expert routing decisions in Mixture-of-Experts (MoE) language models.&lt;/li&gt;&lt;li&gt;Demonstrates that a 3-layer MLP achieves 63.1% top-1 accuracy, while a transformer-based sequence decoder reaches 91.2% top-1 (94.8% top-10) on 32-token sequences after training on 100M tokens from OpenWebText.&lt;/li&gt;&lt;li&gt;Analyzes practical leakage scenarios (e.g., distributed inference, side channels) and shows additive noise mitigates but does not eliminate reconstruction, arguing expert selections should be treated as sensitive data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Nuriyev', 'Gabriel Kulp']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'mixture-of-experts', 'privacy leakage', 'side-channel attack', 'defense/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04105</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines the generalizable hallucination detection (GHD) problem: train a detector on a single domain while ensuring robust cross-domain performance.&lt;/li&gt;&lt;li&gt;Observes that hallucination-initiated multi-turn dialogues exhibit larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical validation across multiple LLMs and benchmarks, showing SpikeScore outperforms baselines in cross-domain hallucination detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'robustness', 'cross-domain-generalization', 'safety', 'uncertainty-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C^3LLM, a statistical certification framework that bounds the probability of LLMs producing catastrophic responses in multi-turn conversations with confidence intervals.&lt;/li&gt;&lt;li&gt;Models multi-turn conversations as a Markov process on a query graph to capture realistic conversational flow and defines practical query distributions (random node, graph path, adaptive with rejection).&lt;/li&gt;&lt;li&gt;Provides certified lower bounds on catastrophic-risk rates for frontier models (reporting rates up to ~70%), demonstrating substantial vulnerabilities and the need for improved safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'certification', 'adversarial-evaluation', 'multi-turn-conversation', 'statistical-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title><link>https://arxiv.org/abs/2510.02712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs a large-scale survival analysis of multi-turn conversational robustness across 9 LLMs and 36,951 turns using Cox, AFT, and Random Survival Forest models.&lt;/li&gt;&lt;li&gt;Identifies semantic drift patterns: abrupt prompt-to-prompt drift sharply increases risk of inconsistency, while cumulative drift appears protective for conversations that persist.&lt;/li&gt;&lt;li&gt;Finds limitations of Cox proportional hazards due to violations for key covariates and shows AFT models with model-drift interactions provide better discrimination and calibration.&lt;/li&gt;&lt;li&gt;Proposes a lightweight AFT-based turn-level risk monitor that flags likely-failing conversations several turns before the first inconsistent response (a practical safeguard/defense).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial attacks', 'survival analysis', 'attack detection/monitoring', 'multi-turn consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Improving Detection of Watermarked Language Models</title><link>https://arxiv.org/abs/2508.13131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies limitations of watermark-based detection for LLM-generated text, especially when model entropy is reduced (e.g., after instruction tuning or RLHF).&lt;/li&gt;&lt;li&gt;Investigates combining watermark detectors with non-watermark detectors into hybrid schemes to improve detection performance.&lt;/li&gt;&lt;li&gt;Shows empirical performance gains of hybrid detectors over standalone watermark or non-watermark methods across varied experimental conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dara Bahri', 'John Wieting']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM detection', 'model provenance', 'defense', 'hybrid detectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13131</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Video Detection via Perceptual Straightening</title><link>https://arxiv.org/abs/2507.00583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReStraV, a method for detecting AI-generated videos by measuring 'perceptual straightening' — temporal curvature and stepwise distance — in representations from a pre-trained self-supervised ViT (DINOv2).&lt;/li&gt;&lt;li&gt;Aggregates per-video statistics of these geometric measures and trains a lightweight classifier to distinguish real vs. synthetic videos.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance (e.g., 97.17% accuracy, 98.63% AUROC on VidProM) and emphasizes computational efficiency and generalization advantages over existing image- and video-based detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Intern\\`o', 'Robert Geirhos', 'Markus Olhofer', 'Sunny Liu', 'Barbara Hammer', 'David Klindt']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'video forensics', 'representation geometry', 'defense', 'self-supervised models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00583</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents</title><link>https://arxiv.org/abs/2602.02164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Co-RedTeam, a security-aware multi-agent LLM framework that mirrors real-world red-teaming workflows for automated vulnerability discovery and exploitation.&lt;/li&gt;&lt;li&gt;Uses execution-grounded iterative reasoning, code-aware analysis, structured agent coordination (discovery and exploitation stages), and long-term memory to plan, execute, validate, and refine attacks.&lt;/li&gt;&lt;li&gt;Shows strong empirical gains (over 60% exploitation success, &gt;10% absolute improvement in detection) and ablation studies that highlight the importance of execution feedback, structured interaction, and memory.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengfei He', 'Ash Fox', 'Lesly Miculicich', 'Stefan Friedli', 'Daniel Fabian', 'Burak Gokturk', 'Jiliang Tang', 'Chen-Yu Lee', 'Tomas Pfister', 'Long T. Le']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'vulnerability discovery', 'exploit generation', 'multi-agent LLMs', 'execution-grounded reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.02164</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title><link>https://arxiv.org/abs/2512.06655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Graph-Regularized Sparse Autoencoders (GSAEs) to learn distributed safety representations across multiple latent features, improving over single-dimension steering methods.&lt;/li&gt;&lt;li&gt;Implements a two-stage gating/runtime steering mechanism that activates interventions only when harmful prompts/continuations are detected, enforcing adaptive refusals while preserving utility on benign queries.&lt;/li&gt;&lt;li&gt;Empirically demonstrates strong defense performance and robustness: ~82% selective refusal rate vs 42% for standard SAE, preserves QA/task accuracy, and resists jailbreak attacks (GCG, AutoDAN) across multiple LLM families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Federico Cinus', 'Yifan Wu', 'Luca Luceri']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'jailbreak mitigation', 'safety steering', 'robustness', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06655</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious comments crafted to induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically adjust proportions of malicious comment types during training, focusing optimization on model weaknesses.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with large F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-generation', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks</title><link>https://arxiv.org/abs/2506.21142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cGAN-based method to generate stealthy adversarial perturbations of known UAV cyber-attacks (DoS, FDI, MiTM, replay) that evade a multi-class IDS while remaining statistically similar to OOD distributions.&lt;/li&gt;&lt;li&gt;Introduces a CVAE-based detector using negative log-likelihood (regret scores) to distinguish these generative-model-crafted adversarial samples from authentic OOD events.&lt;/li&gt;&lt;li&gt;Empirically shows CVAE regret scores outperform traditional Mahalanobis-distance detectors, highlighting the need for probabilistic modeling to harden IDS against adaptive generative attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'OOD-detection', 'generative-model-attacks', 'intrusion-detection-systems', 'UAV-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21142</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>OverThink: Slowdown Attacks on Reasoning LLMs</title><link>https://arxiv.org/abs/2502.02542</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OverThink, an attack that injects benign decoy reasoning problems into model context to force reasoning LLMs to generate many extra reasoning tokens, increasing latency, cost, and energy use while preserving correct final answers.&lt;/li&gt;&lt;li&gt;Empirically evaluates the attack on closed-source and open-source reasoning models across datasets (FreshQA, SQuAD, MuSR) and demonstrates transferability, including multimodal image-based decoys that trigger excessive reasoning.&lt;/li&gt;&lt;li&gt;Explores both LLM-based and system-level defenses and discusses societal, financial, and energy implications of such slowdown/resource-exhaustion attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhinav Kumar', 'Jaechul Roh', 'Ali Naseh', 'Marzena Karpinska', 'Mohit Iyyer', 'Amir Houmansadr', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'prompt injection', 'resource exhaustion/availability', 'defenses/mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02542</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</title><link>https://arxiv.org/abs/2602.04820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CNN-based automated nail disease classification (3,835 images, 6 classes) and compares InceptionV3, DenseNet201, EfficientNetV2, and ResNet50; InceptionV3 achieves 95.57% accuracy.&lt;/li&gt;&lt;li&gt;Applies adversarial training to improve model robustness against tricky/noisy inputs.&lt;/li&gt;&lt;li&gt;Uses explainability methods (SHAP and Grad-CAM) to highlight features driving predictions and increase interpretability for clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzia Hossain', 'Samanta Ghosh', 'Shahida Begum', 'B. M. Shahria Alam', 'Mohammad Tahmid Noor', 'Md Parvez Mia', 'Nishat Tasnim Niloy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'explainability', 'medical-imaging', 'CNN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04820</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title><link>https://arxiv.org/abs/2602.04653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel inference-time backdoor attack that embeds hidden instructions in chat templates (Jinja2) rather than modifying model weights or training data.&lt;/li&gt;&lt;li&gt;Demonstrates practical impact across 18 models (7 families) and 4 inference engines: triggered conditions drop factual accuracy from ~90% to ~15% and cause attacker-controlled URL emissions with &gt;80% success, while benign inputs remain unaffected.&lt;/li&gt;&lt;li&gt;Shows backdoors generalize across runtimes and evade automated security scans on a major open-weight distribution platform, identifying chat templates as an unprotected attack surface in the LLM supply chain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariel Fogel', 'Omer Hofman', 'Eilon Cohen', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'inference-time attack', 'supply-chain security', 'LLM deployment', 'attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04653</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Human-Centered Privacy Approach (HCP) to AI</title><link>https://arxiv.org/abs/2602.04616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive, human-centered privacy (HCP) framework mapping privacy risks across the AI development lifecycle (data collection, training, deployment, reuse).&lt;/li&gt;&lt;li&gt;Reviews privacy-preserving technical defenses (e.g., federated learning, differential privacy) and integrates user-centered perspectives, mental models, regulation, and governance.&lt;/li&gt;&lt;li&gt;Offers design guidelines, case studies, and discusses open challenges and future multidisciplinary research directions for embedding privacy into HCAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luyi Sun', 'Wei Xu', 'Zaifeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving techniques', 'differential privacy', 'federated learning', 'privacy governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04616</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust The Typical</title><link>https://arxiv.org/abs/2602.04581</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Trust The Typical (T3), framing LLM safety as an out-of-distribution (OOD) detection problem by learning the distribution of acceptable prompts in a semantic space and flagging significant deviations.&lt;/li&gt;&lt;li&gt;Does not require training on harmful examples yet attains state-of-the-art results across 18 benchmarks (toxicity, hate speech, jailbreaking, multilingual harms, over-refusal), lowering false positives by up to 40x vs specialized safety models.&lt;/li&gt;&lt;li&gt;Shows strong transfer: a single model trained on safe English text generalizes to diverse domains and 14+ languages without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates production readiness via a GPU-optimized integration into vLLM for continuous, low-overhead (&lt;6%) guardrailing during token generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debargha Ganguly', 'Sreehari Sankar', 'Biyao Zhang', 'Vikash Singh', 'Kanan Gupta', 'Harshini Kavuru', 'Alan Luo', 'Weicong Chen', 'Warren Morningstar', 'Raghu Machiraju', 'Vipin Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'defense', 'out-of-distribution-detection', 'jailbreaking', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04581</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title><link>https://arxiv.org/abs/2602.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of training-time implicit safety risks (beyond explicit reward hacking), introducing a taxonomy with 5 risk levels, 10 fine-grained categories, and 3 incentive types.&lt;/li&gt;&lt;li&gt;Empirical evaluation showing these risks are prevalent and severe (e.g., Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs given only background information).&lt;/li&gt;&lt;li&gt;Analyzes factors influencing such behaviors, demonstrates occurrence in multi-agent training, and highlights covert harms like manipulation of logged metrics during code-based RL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhexin Zhang', 'Yida Lu', 'Junfeng Fang', 'Junxiao Yang', 'Shiyao Cui', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Hongning Wang', 'Minlie Huang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['training-time vulnerabilities', 'implicit incentives', 'reinforcement learning', 'multi-agent', 'safety taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04196</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Attack-Resistant Uniform Fairness for Linear and Smooth Contextual Bandits</title><link>https://arxiv.org/abs/2602.04125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies contextual bandits under a uniform (1-δ)-fairness constraint and provides algorithms achieving near-minimax-optimal regret for linear and smooth reward models while maintaining strong fairness guarantees.&lt;/li&gt;&lt;li&gt;Identifies a security vulnerability: a small-budget adversary can perform signal manipulation that selectively breaks merit-based fairness without substantially increasing standard regret metrics.&lt;/li&gt;&lt;li&gt;Proposes defenses—robust algorithm variants using corruption-adaptive exploration and error-compensated thresholding—that achieve minimax-optimal regret under C-budgeted attacks and preserve (1−~O(1/T)) fairness.&lt;/li&gt;&lt;li&gt;Validates theoretical claims with numerical experiments and a real-world case study.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingwen Zhang', 'Wenjia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness/defenses', 'contextual bandits', 'fairness vulnerability', 'online learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04125</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>ZKBoost: Zero-Knowledge Verifiable Training for XGBoost</title><link>https://arxiv.org/abs/2602.04113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZKBoost, a zero-knowledge proof-of-training (zkPoT) protocol allowing a model owner to prove correct XGBoost training on a committed dataset without revealing data or parameters.&lt;/li&gt;&lt;li&gt;Implements a fixed-point XGBoost compatible with arithmetic circuits, achieving within 1% accuracy of standard XGBoost while enabling zk proofs.&lt;/li&gt;&lt;li&gt;Provides a generic zkPoT template for XGBoost that can be instantiated with general-purpose ZKP backends and a VOLE-based instantiation to handle nonlinear fixed-point operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolas Melissaris', 'Jiayi Xu', 'Antigoni Polychroniadou', 'Akira Takahashi', 'Chenkai Weng']&lt;/li&gt;&lt;li&gt;Tags: ['zero-knowledge proofs', 'verifiable training', 'model integrity', 'privacy-preserving ML', 'cryptographic protocols']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04113</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Byzantine Machine Learning: MultiKrum and an optimal notion of robustness</title><link>https://arxiv.org/abs/2602.03899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides the first formal proof that MultiKrum is a robust aggregation rule under the Byzantine threat model and derives bounds on its robustness coefficient.&lt;/li&gt;&lt;li&gt;Introduces an optimal robustness metric (κ*) for aggregation rules and constructs upper and lower bounds for MultiKrum; also tightens previously known bounds for Krum.&lt;/li&gt;&lt;li&gt;Includes empirical evaluation illustrating the quality of the theoretical lower bound and shows MultiKrum's robustness is never worse (and often better) than Krum in realistic regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gilles Bareilles', 'Wassim Bouaziz', 'Julien Fageot', 'El-Mahdi El-Mhamdi']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine-robustness', 'federated-learning', 'aggregation-rules', 'adversarial-robustness', 'robust-mean-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03899</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Rewards in Reinforcement Learning for Cyber Defence</title><link>https://arxiv.org/abs/2602.04809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how reward function structure (dense vs. sparse) affects learning, policy behavior, and risk in RL-trained autonomous cyber defence agents.&lt;/li&gt;&lt;li&gt;Proposes a novel ground truth evaluation method to directly compare reward functions across cyber gym environments, network sizes, and both policy-gradient and value-based RL algorithms.&lt;/li&gt;&lt;li&gt;Finds that goal-aligned sparse rewards, when encountered sufficiently often, yield more reliable, lower-risk, and more cost-effective defence policies than highly engineered dense rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elizabeth Bates', 'Chris Hicks', 'Vasilios Mavroudis']&lt;/li&gt;&lt;li&gt;Tags: ['cyber defense', 'reinforcement learning', 'reward design', 'robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04809</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2602.04448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RASA, a routing-aware expert-level alignment method that identifies Safety-Critical Experts in Mixture-of-Experts (MoE) LLMs, selectively fine-tunes them under fixed routing, and enforces routing consistency to prevent routing-based bypasses.&lt;/li&gt;&lt;li&gt;Demonstrates that naive full-parameter safety fine-tuning can mask vulnerabilities via routing or expert dominance rather than directly repairing unsafe experts, motivating targeted expert repair.&lt;/li&gt;&lt;li&gt;Evaluates RASA on two representative MoE architectures across diverse jailbreak attacks, reporting near-perfect robustness, strong cross-attack generalization, reduced over-refusal, and preserved general capabilities (MMLU, GSM8K, TruthfulQA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Yuhui Wang', 'Tanqiu Jiang', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Mixture-of-Experts', 'Safety Alignment', 'Jailbreak Attacks', 'Defense', 'Routing-aware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04448</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems</title><link>https://arxiv.org/abs/2602.04431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes safe design of LLM-based multi-agent systems as a Stackelberg security game between a system designer (Meta-Agent) and an attacker that compromises a subset of agents.&lt;/li&gt;&lt;li&gt;Proposes MaMa, an algorithm that uses LLM-based adversarial search to iteratively design systems robust to worst-case compromises by a Meta-Adversary.&lt;/li&gt;&lt;li&gt;Empirically demonstrates that MaMa-designed systems defend against strongest discovered attacks while retaining task performance and generalize to stronger or different adversaries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan N\\"other', 'Adish Singla', 'Goran Radanovic']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multi-agent systems', 'automated red teaming', 'security games', 'LLM safety/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04431</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Cascading Robustness Verification: Toward Efficient Model-Agnostic Certification</title><link>https://arxiv.org/abs/2602.04236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cascading Robustness Verification (CRV), a model-agnostic framework that sequentially applies multiple incomplete verifiers from cheap to expensive, certifying an input as soon as any verifier proves robustness.&lt;/li&gt;&lt;li&gt;Introduces a Stepwise Relaxation (SR) algorithm to incrementally add constraints for costly verifiers, reducing unnecessary computation.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees that CRV achieves equal or higher verified accuracy than individual (more expensive) verifiers while significantly reducing runtime; empirical results show up to ~90% speedup.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadreza Maleki', 'Rushendra Sidibomma', 'Arman Adibi', 'Reza Samavi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'formal verification', 'certified defense', 'incomplete verifiers', 'efficiency/optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04236</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title><link>https://arxiv.org/abs/2602.04224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that existing safe-reasoning defenses for Large Reasoning Models (LRMs) fail to generalize against diverse and complex jailbreak/attack prompts due to insufficiency in the safe reasoning process.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence motivating a more sufficient, generalizable safe-reasoning mechanism that adapts the granularity of internal "thinking" to address risks.&lt;/li&gt;&lt;li&gt;Proposes Risk-Aware Preference Optimization (RAPO), a framework that enables LRMs to detect and mitigate safety risks adaptively in their chain-of-thought, improving robustness to advanced attack prompts while preserving general utility.&lt;/li&gt;&lt;li&gt;Validates RAPO with extensive experiments across multiple LRMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Qiaosheng Zhang', 'Xia Hu', 'Xingcheng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'defense', 'safe reasoning', 'adversarial prompts', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04224</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs</title><link>https://arxiv.org/abs/2602.04027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a consensus-Bayesian framework to detect malicious user behavior in enterprise directory access graphs by modeling directories as topics and users as agents in an interaction graph.&lt;/li&gt;&lt;li&gt;Models access evolution via influence-weighted opinion dynamics with dynamic logical-dependency matrices and shared directory-similarity influence, injecting malicious activity as cross-component logical perturbations.&lt;/li&gt;&lt;li&gt;Detects anomalies using theoretical convergence conditions and scaled opinion variance, and quantifies uncertainty with a time-evolving Bayesian anomaly score using static and online priors.&lt;/li&gt;&lt;li&gt;Validated via simulations on synthetic access graphs showing sensitivity to logical inconsistencies and robustness to dynamic perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pratyush Uppuluri', 'Shilpa Noushad', 'Sajan Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'intrusion-detection', 'graph-security', 'Bayesian-methods', 'opinion-dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04027</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Chains of Thought Don't Matter: Causal Bypass in Large Language Models</title><link>https://arxiv.org/abs/2602.03994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that verbose chain-of-thought (CoT) outputs can be surface-level compliant yet causally bypassed—answers are often produced independent of the provided rationale.&lt;/li&gt;&lt;li&gt;Proposes a diagnostic framework combining a behavioral module that scores manipulation-relevant signals in CoT text and a causal probe measuring CoT-mediated influence (CMI) via hidden-state patching; defines a bypass score (1−CMI).&lt;/li&gt;&lt;li&gt;Empirical findings: audit-aware prompting increases detectable manipulation signals, but CMI is task-dependent—many QA items have near-zero CMI while some logic problems show substantial mediation; layer-wise analysis reveals narrow task-dependent reasoning windows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anish Sathyanarayanan', 'Aditya Nagarsekar', 'Aarush Rathore']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'causal_bypass', 'manipulation_detection', 'model_auditing', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03994</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data</title><link>https://arxiv.org/abs/2602.03872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical framework analyzing how DP-SGD affects feature learning and memorization on long-tailed data.&lt;/li&gt;&lt;li&gt;Shows DP-SGD yields significantly higher test error on rare/long-tailed subpopulations compared to overall dataset performance.&lt;/li&gt;&lt;li&gt;Characterizes how gradient clipping and noise injection jointly impair memorization of informative but underrepresented samples.&lt;/li&gt;&lt;li&gt;Validates theoretical findings with experiments on synthetic and real-world datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Huanyi Xie', 'Meng Ding', 'Shaopeng Fu', 'Jinyan Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'DP-SGD', 'memorization', 'long-tailed-data', 'privacy-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03872</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title><link>https://arxiv.org/abs/2512.06655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Graph-Regularized Sparse Autoencoders (GSAEs) to learn distributed safety representations across multiple latent features, improving over single-dimension steering methods.&lt;/li&gt;&lt;li&gt;Implements a two-stage gating/runtime steering mechanism that activates interventions only when harmful prompts/continuations are detected, enforcing adaptive refusals while preserving utility on benign queries.&lt;/li&gt;&lt;li&gt;Empirically demonstrates strong defense performance and robustness: ~82% selective refusal rate vs 42% for standard SAE, preserves QA/task accuracy, and resists jailbreak attacks (GCG, AutoDAN) across multiple LLM families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Federico Cinus', 'Yifan Wu', 'Luca Luceri']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'jailbreak mitigation', 'safety steering', 'robustness', 'representation learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06655</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title><link>https://arxiv.org/abs/2511.21752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Label Disguise Defense (LDD): conceal true labels by replacing them with semantically transformed or unrelated alias labels (e.g., blue vs. yellow) taught via few-shot demonstrations to mitigate class-directive prompt injection.&lt;/li&gt;&lt;li&gt;LDD is lightweight and model-agnostic (no retraining) and aims to break direct correspondence between injected directives and model outputs.&lt;/li&gt;&lt;li&gt;Evaluated on nine state-of-the-art models (including GPT-5, GPT-4o, LLaMA3.2, Gemma3, Mistral variants) under adversarial prompt-injection; LDD restores part of accuracy lost to attacks and sometimes outperforms the under-attack few-shot baseline.&lt;/li&gt;&lt;li&gt;Analysis shows semantically aligned aliases (e.g., good vs. bad) yield stronger robustness than arbitrary symbols, and effectiveness varies by model and alias choice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Li', 'Ruocheng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'LLM security', 'few-shot learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21752</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title><link>https://arxiv.org/abs/2510.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a comprehensive hierarchical taxonomy of LLM jailbreak strategies grouped into seven mechanism-oriented families (impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, data poisoning).&lt;/li&gt;&lt;li&gt;Runs a structured red-teaming challenge and analyzes prevalence and success rates of different jailbreak techniques, including multi-turn attacks where adversarial intent emerges gradually.&lt;/li&gt;&lt;li&gt;Evaluates automatic jailbreak detection, benchmarking GPT-5 as a judge and showing benefits of taxonomy-guided prompting for improved detection.&lt;/li&gt;&lt;li&gt;Releases a new Italian dataset of 1,364 annotated multi-turn adversarial dialogues labeled with the taxonomy to support study of bypassable safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francesco Giarrusso', 'Olga E. Sorokoletova', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red-teaming', 'attack detection', 'taxonomy &amp; dataset', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13893</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdComment, an adaptive adversarial training framework to improve robustness of fake-news detectors against malicious comments crafted to induce misclassification.&lt;/li&gt;&lt;li&gt;Categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and uses LLMs to synthesize diverse, category-specific perturbations.&lt;/li&gt;&lt;li&gt;Introduces InfoDirichlet Resampling (IDR) to dynamically adjust proportions of malicious comment types during training, focusing optimization on model weaknesses.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness with large F1 improvements on three benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'adversarial-examples', 'robustness', 'attack-generation', 'fake-news-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title><link>https://arxiv.org/abs/2510.02712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs a large-scale survival analysis of multi-turn conversational robustness across 9 LLMs and 36,951 turns using Cox, AFT, and Random Survival Forest models.&lt;/li&gt;&lt;li&gt;Identifies semantic drift patterns: abrupt prompt-to-prompt drift sharply increases risk of inconsistency, while cumulative drift appears protective for conversations that persist.&lt;/li&gt;&lt;li&gt;Finds limitations of Cox proportional hazards due to violations for key covariates and shows AFT models with model-drift interactions provide better discrimination and calibration.&lt;/li&gt;&lt;li&gt;Proposes a lightweight AFT-based turn-level risk monitor that flags likely-failing conversations several turns before the first inconsistent response (a practical safeguard/defense).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Ramayya Krishnan', 'Rema Padman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial attacks', 'survival analysis', 'attack detection/monitoring', 'multi-turn consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02712</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</title><link>https://arxiv.org/abs/2508.03365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WhisperInject, a two-stage adversarial audio attack that jailsbreaks audio-language models and injects harmful payloads into benign-sounding audio.&lt;/li&gt;&lt;li&gt;Stage 1 uses a white-box reward-based optimization method (RL-PGD) to elicit a harmful native response; Stage 2 embeds that payload via gradient-based perturbations into carrier audio (e.g., weather queries).&lt;/li&gt;&lt;li&gt;Evaluated across two benchmarks and five multimodal LLMs, achieving 60–78% average attack success rates, validated with multiple evaluation frameworks.&lt;/li&gt;&lt;li&gt;Demonstrates practical, covert audio-native threats against multimodal AI systems, moving beyond theoretical exploits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiskias Dingeto', 'Taeyoun Kwon', 'Dasol Choi', 'Bodam Kim', 'DongGeon Lee', 'Haon Park', 'JaeHoon Lee', 'Jongho Shin']&lt;/li&gt;&lt;li&gt;Tags: ['audio-adversarial-examples', 'jailbreaking', 'payload-injection', 'adversarial-audio', 'multimodal-model-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03365</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>AI-Generated Video Detection via Perceptual Straightening</title><link>https://arxiv.org/abs/2507.00583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReStraV, a method for detecting AI-generated videos by measuring 'perceptual straightening' — temporal curvature and stepwise distance — in representations from a pre-trained self-supervised ViT (DINOv2).&lt;/li&gt;&lt;li&gt;Aggregates per-video statistics of these geometric measures and trains a lightweight classifier to distinguish real vs. synthetic videos.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance (e.g., 97.17% accuracy, 98.63% AUROC on VidProM) and emphasizes computational efficiency and generalization advantages over existing image- and video-based detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christian Intern\\`o', 'Robert Geirhos', 'Markus Olhofer', 'Sunny Liu', 'Barbara Hammer', 'David Klindt']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'video forensics', 'representation geometry', 'defense', 'self-supervised models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00583</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks</title><link>https://arxiv.org/abs/2506.21142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cGAN-based method to generate stealthy adversarial perturbations of known UAV cyber-attacks (DoS, FDI, MiTM, replay) that evade a multi-class IDS while remaining statistically similar to OOD distributions.&lt;/li&gt;&lt;li&gt;Introduces a CVAE-based detector using negative log-likelihood (regret scores) to distinguish these generative-model-crafted adversarial samples from authentic OOD events.&lt;/li&gt;&lt;li&gt;Empirically shows CVAE regret scores outperform traditional Mahalanobis-distance detectors, highlighting the need for probabilistic modeling to harden IDS against adaptive generative attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-examples', 'OOD-detection', 'generative-model-attacks', 'intrusion-detection-systems', 'UAV-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21142</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines the generalizable hallucination detection (GHD) problem: train a detector on a single domain while ensuring robust cross-domain performance.&lt;/li&gt;&lt;li&gt;Observes that hallucination-initiated multi-turn dialogues exhibit larger uncertainty fluctuations than factual ones and proposes SpikeScore to quantify abrupt uncertainty spikes.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical validation across multiple LLMs and benchmarks, showing SpikeScore outperforms baselines in cross-domain hallucination detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxin Deng', 'Zhen Fang', 'Sharon Li', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'robustness', 'cross-domain-generalization', 'safety', 'uncertainty-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.19245</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C^3LLM, a statistical certification framework that bounds the probability of LLMs producing catastrophic responses in multi-turn conversations with confidence intervals.&lt;/li&gt;&lt;li&gt;Models multi-turn conversations as a Markov process on a query graph to capture realistic conversational flow and defines practical query distributions (random node, graph path, adaptive with rejection).&lt;/li&gt;&lt;li&gt;Provides certified lower bounds on catastrophic-risk rates for frontier models (reporting rates up to ~70%), demonstrating substantial vulnerabilities and the need for improved safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'certification', 'adversarial-evaluation', 'multi-turn-conversation', 'statistical-guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</title><link>https://arxiv.org/abs/2602.04820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CNN-based automated nail disease classification (3,835 images, 6 classes) and compares InceptionV3, DenseNet201, EfficientNetV2, and ResNet50; InceptionV3 achieves 95.57% accuracy.&lt;/li&gt;&lt;li&gt;Applies adversarial training to improve model robustness against tricky/noisy inputs.&lt;/li&gt;&lt;li&gt;Uses explainability methods (SHAP and Grad-CAM) to highlight features driving predictions and increase interpretability for clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzia Hossain', 'Samanta Ghosh', 'Shahida Begum', 'B. M. Shahria Alam', 'Mohammad Tahmid Noor', 'Md Parvez Mia', 'Nishat Tasnim Niloy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'explainability', 'medical-imaging', 'CNN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04820</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Rewards in Reinforcement Learning for Cyber Defence</title><link>https://arxiv.org/abs/2602.04809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates how reward function structure (dense vs. sparse) affects learning, policy behavior, and risk in RL-trained autonomous cyber defence agents.&lt;/li&gt;&lt;li&gt;Proposes a novel ground truth evaluation method to directly compare reward functions across cyber gym environments, network sizes, and both policy-gradient and value-based RL algorithms.&lt;/li&gt;&lt;li&gt;Finds that goal-aligned sparse rewards, when encountered sufficiently often, yield more reliable, lower-risk, and more cost-effective defence policies than highly engineered dense rewards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elizabeth Bates', 'Chris Hicks', 'Vasilios Mavroudis']&lt;/li&gt;&lt;li&gt;Tags: ['cyber defense', 'reinforcement learning', 'reward design', 'robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04809</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title><link>https://arxiv.org/abs/2602.04739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Two-phase, longitudinal evaluation of multimodal LLM harmlessness using 726 adversarial prompts crafted by 26 professional red teamers, producing 82,256 human harm ratings.&lt;/li&gt;&lt;li&gt;Compares attack success rates and refusal behavior across eight model releases (GPT-4o → GPT-5, Claude Sonnet 3.5 → 4.5, Pixtral 12B → Pixtral Large, Qwen VL Plus → Qwen Omni), finding persistent inter-family differences and alignment drift over generations.&lt;/li&gt;&lt;li&gt;Analyzes modality-specific effects and how vulnerability patterns shift across updates, arguing for longitudinal, multimodal benchmarks to track evolving safety behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Casey Ford', 'Madison Van Doren', 'Emily Dix']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'alignment drift', 'safety evaluation', 'attack success rate (ASR)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04739</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention</title><link>https://arxiv.org/abs/2602.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that standard causal attention enables harmful cross-document interactions that facilitate corpus knowledge poisoning in Retrieval-Augmented Generation (RAG).&lt;/li&gt;&lt;li&gt;Proposes Sparse Document Attention RAG (SDAG), a block-sparse attention mask that disallows cross-attention between retrieved documents, requiring only a minimal inference-time mask change and no fine-tuning.&lt;/li&gt;&lt;li&gt;Empirically evaluates multiple attack strategies on LLM-based QA with RAG, showing SDAG substantially reduces attack success rates versus standard causal attention.&lt;/li&gt;&lt;li&gt;Demonstrates that combining SDAG with state-of-the-art RAG defenses yields statistically significant improvements over prior defenses alone.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sagie Dekel', 'Moshe Tennenholtz', 'Oren Kurland']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation (RAG)', 'Corpus/Data Poisoning', 'Sparse Attention Defense', 'Adversarial Robustness', 'Attack Evaluation / Red Teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04711</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>A Human-Centered Privacy Approach (HCP) to AI</title><link>https://arxiv.org/abs/2602.04616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive, human-centered privacy (HCP) framework mapping privacy risks across the AI development lifecycle (data collection, training, deployment, reuse).&lt;/li&gt;&lt;li&gt;Reviews privacy-preserving technical defenses (e.g., federated learning, differential privacy) and integrates user-centered perspectives, mental models, regulation, and governance.&lt;/li&gt;&lt;li&gt;Offers design guidelines, case studies, and discusses open challenges and future multidisciplinary research directions for embedding privacy into HCAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luyi Sun', 'Wei Xu', 'Zaifeng Gao']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving techniques', 'differential privacy', 'federated learning', 'privacy governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04616</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Trust The Typical</title><link>https://arxiv.org/abs/2602.04581</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Trust The Typical (T3), framing LLM safety as an out-of-distribution (OOD) detection problem by learning the distribution of acceptable prompts in a semantic space and flagging significant deviations.&lt;/li&gt;&lt;li&gt;Does not require training on harmful examples yet attains state-of-the-art results across 18 benchmarks (toxicity, hate speech, jailbreaking, multilingual harms, over-refusal), lowering false positives by up to 40x vs specialized safety models.&lt;/li&gt;&lt;li&gt;Shows strong transfer: a single model trained on safe English text generalizes to diverse domains and 14+ languages without retraining.&lt;/li&gt;&lt;li&gt;Demonstrates production readiness via a GPU-optimized integration into vLLM for continuous, low-overhead (&lt;6%) guardrailing during token generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debargha Ganguly', 'Sreehari Sankar', 'Biyao Zhang', 'Vikash Singh', 'Kanan Gupta', 'Harshini Kavuru', 'Alan Luo', 'Weicong Chen', 'Warren Morningstar', 'Raghu Machiraju', 'Vipin Chaudhary']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'defense', 'out-of-distribution-detection', 'jailbreaking', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04581</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2602.04448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RASA, a routing-aware expert-level alignment method that identifies Safety-Critical Experts in Mixture-of-Experts (MoE) LLMs, selectively fine-tunes them under fixed routing, and enforces routing consistency to prevent routing-based bypasses.&lt;/li&gt;&lt;li&gt;Demonstrates that naive full-parameter safety fine-tuning can mask vulnerabilities via routing or expert dominance rather than directly repairing unsafe experts, motivating targeted expert repair.&lt;/li&gt;&lt;li&gt;Evaluates RASA on two representative MoE architectures across diverse jailbreak attacks, reporting near-perfect robustness, strong cross-attack generalization, reduced over-refusal, and preserved general capabilities (MMLU, GSM8K, TruthfulQA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Yuhui Wang', 'Tanqiu Jiang', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['Mixture-of-Experts', 'Safety Alignment', 'Jailbreak Attacks', 'Defense', 'Routing-aware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04448</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks</title><link>https://arxiv.org/abs/2602.04294</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically studies how few-shot demonstrations affect prompt-based defenses (Role-Oriented Prompts and Task-Oriented Prompts) against LLM jailbreak attacks.&lt;/li&gt;&lt;li&gt;Evaluates multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods.&lt;/li&gt;&lt;li&gt;Finds few-shot demonstrations improve RoP safety (up to +4.5%) by reinforcing role identity, but substantially degrade ToP effectiveness (up to -21.2%) by distracting from task instructions.&lt;/li&gt;&lt;li&gt;Provides practical deployment recommendations for prompt-based defense strategies in real-world LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanshu Wang', 'Shuaishuai Yang', 'Jingjing He', 'Tong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt-based defenses', 'few-shot examples', 'safety evaluation', 'adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04294</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title><link>https://arxiv.org/abs/2602.04224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that existing safe-reasoning defenses for Large Reasoning Models (LRMs) fail to generalize against diverse and complex jailbreak/attack prompts due to insufficiency in the safe reasoning process.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical evidence motivating a more sufficient, generalizable safe-reasoning mechanism that adapts the granularity of internal "thinking" to address risks.&lt;/li&gt;&lt;li&gt;Proposes Risk-Aware Preference Optimization (RAPO), a framework that enables LRMs to detect and mitigate safety risks adaptively in their chain-of-thought, improving robustness to advanced attack prompts while preserving general utility.&lt;/li&gt;&lt;li&gt;Validates RAPO with extensive experiments across multiple LRMs and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeming Wei', 'Qiaosheng Zhang', 'Xia Hu', 'Xingcheng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'defense', 'safe reasoning', 'adversarial prompts', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04224</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When Chains of Thought Don't Matter: Causal Bypass in Large Language Models</title><link>https://arxiv.org/abs/2602.03994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that verbose chain-of-thought (CoT) outputs can be surface-level compliant yet causally bypassed—answers are often produced independent of the provided rationale.&lt;/li&gt;&lt;li&gt;Proposes a diagnostic framework combining a behavioral module that scores manipulation-relevant signals in CoT text and a causal probe measuring CoT-mediated influence (CMI) via hidden-state patching; defines a bypass score (1−CMI).&lt;/li&gt;&lt;li&gt;Empirical findings: audit-aware prompting increases detectable manipulation signals, but CMI is task-dependent—many QA items have near-zero CMI while some logic problems show substantial mediation; layer-wise analysis reveals narrow task-dependent reasoning windows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anish Sathyanarayanan', 'Aditya Nagarsekar', 'Aarush Rathore']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'causal_bypass', 'manipulation_detection', 'model_auditing', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03994</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Byzantine Machine Learning: MultiKrum and an optimal notion of robustness</title><link>https://arxiv.org/abs/2602.03899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides the first formal proof that MultiKrum is a robust aggregation rule under the Byzantine threat model and derives bounds on its robustness coefficient.&lt;/li&gt;&lt;li&gt;Introduces an optimal robustness metric (κ*) for aggregation rules and constructs upper and lower bounds for MultiKrum; also tightens previously known bounds for Krum.&lt;/li&gt;&lt;li&gt;Includes empirical evaluation illustrating the quality of the theoretical lower bound and shows MultiKrum's robustness is never worse (and often better) than Krum in realistic regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gilles Bareilles', 'Wassim Bouaziz', 'Julien Fageot', 'El-Mahdi El-Mhamdi']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine-robustness', 'federated-learning', 'aggregation-rules', 'adversarial-robustness', 'robust-mean-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03899</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data</title><link>https://arxiv.org/abs/2602.03872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical framework analyzing how DP-SGD affects feature learning and memorization on long-tailed data.&lt;/li&gt;&lt;li&gt;Shows DP-SGD yields significantly higher test error on rare/long-tailed subpopulations compared to overall dataset performance.&lt;/li&gt;&lt;li&gt;Characterizes how gradient clipping and noise injection jointly impair memorization of informative but underrepresented samples.&lt;/li&gt;&lt;li&gt;Validates theoretical findings with experiments on synthetic and real-world datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Huanyi Xie', 'Meng Ding', 'Shaopeng Fu', 'Jinyan Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'DP-SGD', 'memorization', 'long-tailed-data', 'privacy-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.03872</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item><item><title>When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making</title><link>https://arxiv.org/abs/2602.04003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adversarial Explanation Attacks (AEAs): manipulation of LLM-generated explanations to increase human trust in incorrect model outputs.&lt;/li&gt;&lt;li&gt;Formalizes the threat via the 'trust miscalibration gap', measuring differences in human trust between correct and incorrect outputs under adversarial explanations.&lt;/li&gt;&lt;li&gt;Reports a controlled experiment (n=205) varying reasoning mode, evidence type, communication style, and presentation format to quantify how explanation framing affects trust.&lt;/li&gt;&lt;li&gt;Finds that adversarial explanations can preserve most benign trust despite incorrectness, with highest vulnerability when explanations mimic expert communication and on hard, fact-driven tasks or among certain user groups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shutong Fan', 'Lan Zhang', 'Xiaoyong Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-explanations', 'human-in-the-loop', 'trust-manipulation', 'red-teaming', 'explainability-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2602.04003</guid><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate></item></channel></rss>
<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 10 Nov 2025 06:42:58 +0000</lastBuildDate><item><title>When Are Concepts Erased From Diffusion Models?</title><link>https://arxiv.org/abs/2505.17013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two conceptual mechanisms for concept erasure in diffusion models: (i) disrupting internal guidance processes and (ii) reducing unconditional likelihood of the target concept.&lt;/li&gt;&lt;li&gt;Introduces a suite of independent probing techniques to test erasure: supplying visual context, modifying diffusion trajectories, applying classifier guidance, and analyzing alternative generations.&lt;/li&gt;&lt;li&gt;Empirically evaluates how thoroughly concepts are removed and highlights the need for comprehensive, non-text-based robustness checks for erasure methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Lu', 'Nicky Kriplani', 'Rohit Gandikota', 'Minh Pham', 'David Bau', 'Chinmay Hegde', 'Niv Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'concept erasure', 'model editing', 'robustness evaluation', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17013</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2504.14245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Multi-modal Large Language Models (MLLMs) for detecting AI-generated (fake) images, comparing them to traditional detection methods and human evaluators.&lt;/li&gt;&lt;li&gt;Proposes six distinct prompts and a framework that integrates them to produce a more robust, explainable, reasoning-driven detection system.&lt;/li&gt;&lt;li&gt;Emphasizes transparency and explainability in fake-image detection rather than treating detection as a black box; code released on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yikun Ji', 'Yan Hong', 'Jiahui Zhan', 'Haoxing Chen', 'jun lan', 'Huijia Zhu', 'Weiqiang Wang', 'Liqing Zhang', 'Jianfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'MLLM', 'explainability', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14245</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</title><link>https://arxiv.org/abs/2511.05397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents EverydayVLA: a low-cost (~$300) 6-DOF manipulator paired with a unified vision-language-action model that outputs both discrete and continuous actions.&lt;/li&gt;&lt;li&gt;Introduces an adaptive-horizon ensemble that monitors motion uncertainty and triggers on-the-fly re-planning to improve safety and reliability during execution.&lt;/li&gt;&lt;li&gt;Evaluates on LIBERO and real-world setups, matching SOTA on LIBERO and showing substantial real-world improvements (≈49% in-distribution, 34.9% out-of-distribution).&lt;/li&gt;&lt;li&gt;Aims to democratize access to robotic foundation models by combining state-of-the-art VLA methods with affordable hardware.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samarth Chopra', 'Alex McMoil', 'Ben Carnovale', 'Evan Sokolson', 'Rajkumar Kubendran', 'Samuel Dickerson']&lt;/li&gt;&lt;li&gt;Tags: ['robotics', 'vision-language-action', 'safety', 'robustness', 'low-cost-hardware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05397</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying the Risk of Transferred Black Box Attacks</title><link>https://arxiv.org/abs/2511.05102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes resilience testing for transferred black-box evasion attacks and argues exhaustive adversarial risk mapping in high-dimensional input spaces is computationally infeasible.&lt;/li&gt;&lt;li&gt;Proposes a targeted testing framework that selects surrogate models based on Centered Kernel Alignment (CKA) similarity (using both high- and low-similarity surrogates) to improve coverage of adversarial subspaces.&lt;/li&gt;&lt;li&gt;Uses regression-based estimators to quantify transferred adversarial risk and produce actionable risk estimates for organizations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Disesdi Susanna Cox', 'Niklas Bunzel']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'transferability', 'robustness testing', 'surrogate models / CKA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05102</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between model fine-tuning to 'unlearn' harmful concepts and training-free negative-prompt guidance for text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, requiring no changes to existing pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate against nudity and violence benchmarks while preserving input prompt semantics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'safety-mitigation', 'negative-prompts', 'concept-inversion', 'model-unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title><link>https://arxiv.org/abs/2511.05319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines Sentence-to-Image Steganography (Semantic Steganography) to hide sentence- or paragraph-level messages inside images.&lt;/li&gt;&lt;li&gt;Presents IVT benchmark of diverse sentence-level secret messages and introduces S^2LM, a pipeline that leverages LLMs throughout embedding and extraction to encode semantically rich content into images.&lt;/li&gt;&lt;li&gt;Reports quantitative and qualitative experiments showing capability to embed high-level textual information into images; code to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanqi Wu', 'Huangbiao Xu', 'Runfeng Xie', 'Jiaxin Cai', 'Kaixin Zhang', 'Xiao Ke']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'covert channels', 'LLM-enabled steganography', 'information hiding', 'AIGC misuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05319</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements</title><link>https://arxiv.org/abs/2511.05108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lane-detection approach that uses roadside delineators (vertical posts) as indirect lane indicators and fits a smooth lane trajectory via a parameterized Bezier curve, enabling lane estimation when painted markings are occluded by snow.&lt;/li&gt;&lt;li&gt;Introduces SnowyLane, a synthetic dataset of 80,000 annotated winter-driving frames with varying snow coverage and lighting conditions for training and evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and real-time capability compared to state-of-the-art lane detection systems in heavy snow occlusion scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J\\"org Gamerdinger', 'Benedict Wetzel', 'Patrick Schulz', 'Sven Teufel', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'computer-vision', 'dataset', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05108</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title><link>https://arxiv.org/abs/2511.05073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically demonstrates that image adversarial examples are more sensitive to occlusion than clean images using sliding mask experiments on CIFAR-10.&lt;/li&gt;&lt;li&gt;Introduces Sliding Mask Confidence Entropy (SMCE) and Mask Entropy Field Maps to quantify confidence volatility under occlusion.&lt;/li&gt;&lt;li&gt;Proposes Sliding Window Mask-based Adversarial Example Detection (SWM-AED) to detect adversarial examples and reports robust detection accuracies across multiple attacks and classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Li', 'Yanwei Xu', 'Keran Li', 'Xiaoli Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'robustness', 'occlusion sensitivity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05073</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title><link>https://arxiv.org/abs/2511.05017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a modality imbalance in large vision-language models (LVLMs) where appended visual embeddings underweight visual information, leading to hallucinations.&lt;/li&gt;&lt;li&gt;Proposes refining textual embeddings by integrating average-pooled visual features to improve cross-modal alignment and visual grounding.&lt;/li&gt;&lt;li&gt;Reports reductions in hallucinations on established benchmarks using the simple, efficient average-pooling fusion, and suggests more advanced fusion methods as future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Agrawal', 'Gouthaman KV', 'Rohith Aralikatti', 'Gauri Jagatap', 'Jiaxin Yuan', 'Vijay Kamarshi', 'Andrea Fanelli', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'visual grounding', 'multimodal alignment', 'embedding fusion', 'LVLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05017</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end differentiable framework that parameterizes shapes via a Fourier series, maps them to pixels using a winding-number mapping, and uses signal-energy constraints for optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can produce high-confidence classifications from DNNs and serve as precise interpretability probes isolating salient regions.&lt;/li&gt;&lt;li&gt;Introduces a new adversarial paradigm based on optimized shapes that can deceive downstream visual tasks, i.e., a novel form of adversarial example useful for robustness testing/red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'red teaming', 'interpretability', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2511.04949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a learnable latent-space watermark embedder that encodes/extracts messages tied to high-level image semantics for proactive deepfake detection.&lt;/li&gt;&lt;li&gt;Uses a Multi-Agent Adversarial Reinforcement Learning (MAARL) setup where a watermarking agent and an adversarial attacker agent co-evolve, enabling a dynamic curriculum of benign and malicious manipulations to balance robustness and fragility.&lt;/li&gt;&lt;li&gt;Evaluates on CelebA and CelebA-HQ, reporting consistent improvements (~4.5% on CelebA, ~5.3% on CelebA-HQ) over state-of-the-art under challenging manipulation scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tharindu Fernando', 'Clinton Fookes', 'Sridha Sridharan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'watermarking', 'adversarial reinforcement learning', 'media forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04949</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title><link>https://arxiv.org/abs/2511.04886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a geospatial deep learning model that uses satellite imagery to predict fatal roadway crash risk.&lt;/li&gt;&lt;li&gt;Outputs a Beta probability distribution per instance (rather than a point estimate) to provide uncertainty-aware, calibrated risk assessments.&lt;/li&gt;&lt;li&gt;Reports 17–23% improvement in recall over baselines and improved calibration, aimed at supporting safety-critical decision-making (e.g., autonomous navigation, urban planning).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Elallaf', 'Nathan Jacobs', 'Xinyue Ye', 'Mei Chen', 'Gongbo Liang']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'model calibration', 'geospatial deep learning', 'safety-critical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04886</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title><link>https://arxiv.org/abs/2511.04729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage knowledge-based method to detect network-induced shape artifacts in synthetic medical images: (i) a feature extractor analyzing per-image distributions of angle gradients along anatomical boundaries, and (ii) an isolation forest anomaly detector.&lt;/li&gt;&lt;li&gt;Evaluated on two synthetic mammography datasets (CSAW-syn and VMLO-syn) with strong concentration of artifacts in the top anomalous partition and AUCs of 0.97 and 0.91 respectively.&lt;/li&gt;&lt;li&gt;Includes a reader study with three imaging scientists showing higher human agreement (66–68%) for the algorithm-identified most anomalous images and moderate Kendall-Tau correlations (~0.43–0.45) between human and algorithm rankings.&lt;/li&gt;&lt;li&gt;Aimed at improving synthetic data quality assurance to support responsible use of synthetic datasets by pinpointing anatomical constraint violations and network-induced distortions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rucha Deshpande', 'Tahsin Rahman', 'Miguel Lago', 'Adarsh Subbaswamy', 'Jana G. Delfino', 'Ghada Zamzmi', 'Elim Thompson', 'Aldo Badano', 'Seyed Kahaki']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-data', 'anomaly-detection', 'medical-imaging', 'dataset-quality', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04729</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title><link>https://arxiv.org/abs/2510.15144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HugAgent, a benchmark to evaluate whether LLMs can simulate individualized human reasoning and predict a specific person's responses and underlying reasoning dynamics given partial prior evidence.&lt;/li&gt;&lt;li&gt;Uses a dual-track design: a human track scaling think-aloud data collection for ecologically valid reasoning traces, and a synthetic track for scalable stress testing.&lt;/li&gt;&lt;li&gt;Reports experiments showing state-of-the-art LLMs have persistent gaps in adapting to individual reasoning styles, presenting HugAgent as an extensible benchmark for cognitive alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chance Jiajie Li', 'Zhenze Mo', 'Yuhan Tang', 'Ao Qu', 'Jiayi Wu', 'Kaiya Ivy Zhao', 'Yulu Gan', 'Jie Fan', 'Jiangbo Yu', 'Hang Jiang', 'Paul Pu Liang', 'Jinhua Zhao', 'Luis Alberto Alonso Pastor', 'Kent Larson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarking', 'personalization', 'human-simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15144</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title><link>https://arxiv.org/abs/2508.15252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAGAN, a framework to generate high-quality fake user profiles (including improved textual reviews) to perform data poisoning attacks on recommender systems.&lt;/li&gt;&lt;li&gt;Uses in-context learning with multimodal foundation models, a demonstration retrieval algorithm, and text style transfer to produce more transferable and imperceptible fake reviews.&lt;/li&gt;&lt;li&gt;Introduces a collaborative generation pipeline (jailbreaker, instructional agent, guardian) to optimize transferability and stealth under black-box, limited-knowledge attacker settings.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness on real-world datasets and reports state-of-the-art poisoning performance against recommender systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyi Yang', 'Xinshu Li', 'Guanglin Zhou', 'Chen Wang', 'Xiwei Xu', 'Liming Zhu', 'Lina Yao']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'recommender systems', 'adversarial attacks', 'in-context learning', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15252</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title><link>https://arxiv.org/abs/2506.10751</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Neural, a two-stage pipeline for evidence-grounded clinical QA: sentence-level evidence identification followed by answer synthesis with explicit citations.&lt;/li&gt;&lt;li&gt;Uses automated prompt-space exploration (DSPy's MIPROv2) to jointly optimize instructions and few-shot demonstrations for each stage.&lt;/li&gt;&lt;li&gt;Employs a self-consistency voting scheme to boost evidence recall while maintaining precision; achieves 51.5 on the ArchEHR-QA 2025 hidden test set, outperforming zero-/few-shot baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Prasanna Teja Reddy Bogireddy', 'Abrar Majeedi', 'Viswanatha Reddy Gajjala', 'Zhuoyan Xu', 'Siddhant Rai', 'Vaishnav Potlapalli']&lt;/li&gt;&lt;li&gt;Tags: ['prompt engineering', 'clinical QA', 'evidence retrieval', 'few-shot prompting', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10751</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2504.14245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Multi-modal Large Language Models (MLLMs) for detecting AI-generated (fake) images, comparing them to traditional detection methods and human evaluators.&lt;/li&gt;&lt;li&gt;Proposes six distinct prompts and a framework that integrates them to produce a more robust, explainable, reasoning-driven detection system.&lt;/li&gt;&lt;li&gt;Emphasizes transparency and explainability in fake-image detection rather than treating detection as a black box; code released on GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yikun Ji', 'Yan Hong', 'Jiahui Zhan', 'Haoxing Chen', 'jun lan', 'Huijia Zhu', 'Weiqiang Wang', 'Liqing Zhang', 'Jianfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'MLLM', 'explainability', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14245</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Policy-as-Prompt: converts unstructured governance artifacts into a source-linked policy tree compiled into lightweight prompt-based classifiers for runtime monitoring of AI agents.&lt;/li&gt;&lt;li&gt;Designed to enforce least privilege and data minimization, provide provenance/traceability/audit logging, and integrate human-in-the-loop reviews for conformity assessment.&lt;/li&gt;&lt;li&gt;Evaluations claim reductions in prompt-injection risk, blocking of out-of-scope requests, limitation of toxic outputs, and generation of auditable rationales aligned with AI governance frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['policy-as-code', 'prompt injection', 'runtime guardrails', 'auditability', 'AI governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2509.09360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MetaRAG, a metamorphic testing framework to detect hallucinations in Retrieval-Augmented Generation (RAG) systems in a real-time, unsupervised, black-box setting.&lt;/li&gt;&lt;li&gt;Method: decompose responses into atomic factoids, generate controlled mutations (synonyms/antonyms), check entailment/contradiction against retrieved context, and aggregate inconsistencies into a hallucination score.&lt;/li&gt;&lt;li&gt;Localizes unsupported claims to specific factoid spans, enabling identity-aware flagging (e.g., pregnancy, LGBTQ+ refugee rights) and configurable thresholds/guardrails.&lt;/li&gt;&lt;li&gt;Validated on a proprietary enterprise dataset; includes a discussed (but unevaluated) topic-based deployment design to translate span scores into safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Channdeth Sok', 'David Luz', 'Yacine Haddam']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'RAG', 'metamorphic testing', 'black-box evaluation', 'identity-aware safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09360</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are Humans as Brittle as Large Language Models?</title><link>https://arxiv.org/abs/2509.07869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares sensitivity to prompt modifications between humans and LLMs on text classification tasks.&lt;/li&gt;&lt;li&gt;Uses systematic prompt variations (e.g., label substitutions, label formats, typographical errors, label order) applied to both humans and models.&lt;/li&gt;&lt;li&gt;Finds both humans and LLMs show increased brittleness for certain modifications—especially alternative label sets/formats—while humans are less affected by typos and reversed label order.&lt;/li&gt;&lt;li&gt;Implication: prompt brittleness is not entirely unique to LLMs but manifests differently, informing robustness and evaluation of model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahui Li', 'Sean Papay', 'Roman Klinger']&lt;/li&gt;&lt;li&gt;Tags: ['prompt brittleness', 'robustness', 'human-AI comparison', 'adversarial prompting', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title><link>https://arxiv.org/abs/2507.23247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRiMH, a dataset and pragmatic reasoning tasks in the mental health domain focusing on implicature and presupposition (two implicature tasks, one presupposition task).&lt;/li&gt;&lt;li&gt;Benchmarks Llama3.1, Mistral, MentaLLaMa, and Qwen on these tasks; finds Mistral and Qwen show substantial reasoning ability.&lt;/li&gt;&lt;li&gt;Analyzes MentaLLaMa behavior with a rollout attention mechanism and proposes three StiPRompts to evaluate stigma-handling by GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku.&lt;/li&gt;&lt;li&gt;Finds Claude-3.5-haiku responds to stigma-related prompts more responsibly compared to the other evaluated models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sneha Oram', 'Pushpak Bhattacharyya']&lt;/li&gt;&lt;li&gt;Tags: ['pragmatic-reasoning', 'mental-health', 'interpretability', 'safety-evaluation', 'LLM-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23247</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations</title><link>https://arxiv.org/abs/2511.05359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ConVerse, a dynamic benchmark for evaluating privacy and security risks in multi-turn agent-to-agent conversations across three domains (travel, real estate, insurance) with 12 personas.&lt;/li&gt;&lt;li&gt;Contains 864 contextually grounded attacks (611 privacy, 253 security) including a three-tier taxonomy for privacy abstraction quality and security tests targeting tool use and preference manipulation.&lt;/li&gt;&lt;li&gt;Evaluates seven state-of-the-art models, finding high vulnerability rates (privacy up to 88%, security up to 60%), and highlights that stronger models may leak more information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amr Gomaa', 'Ahmed Salem', 'Sahar Abdelnabi']&lt;/li&gt;&lt;li&gt;Tags: ['agent-to-agent', 'privacy', 'security', 'benchmarking', 'multi-agent-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05359</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title><link>https://arxiv.org/abs/2511.05017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a modality imbalance in large vision-language models (LVLMs) where appended visual embeddings underweight visual information, leading to hallucinations.&lt;/li&gt;&lt;li&gt;Proposes refining textual embeddings by integrating average-pooled visual features to improve cross-modal alignment and visual grounding.&lt;/li&gt;&lt;li&gt;Reports reductions in hallucinations on established benchmarks using the simple, efficient average-pooling fusion, and suggests more advanced fusion methods as future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Agrawal', 'Gouthaman KV', 'Rohith Aralikatti', 'Gauri Jagatap', 'Jiaxin Yuan', 'Vijay Kamarshi', 'Andrea Fanelli', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'visual grounding', 'multimodal alignment', 'embedding fusion', 'LVLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05017</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title><link>https://arxiv.org/abs/2511.04956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ORCHID, a modular agentic retrieval-augmented generation (RAG) system for High-Risk Property (HRP) classification with cooperating agents (retrieval, refiner, classifier, validator) and Model Context Protocol (MCP) for model-agnostic on-premise operation.&lt;/li&gt;&lt;li&gt;Implements human-in-the-loop workflows that defer uncertain items to Subject Matter Experts (SMEs), capture SME feedback, and produce append-only audit bundles (run-cards, prompts, evidence) with on-policy citations for traceability.&lt;/li&gt;&lt;li&gt;Reports preliminary improvements in accuracy and traceability over a non-agentic baseline and demonstrates exportable audit artifacts to support trustworthy LLM assistance in sensitive DOE compliance workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Mahbub', 'Vanessa Lama', 'Sanjay Das', 'Brian Starks', 'Christopher Polchek', 'Saffell Silvers', 'Lauren Deck', 'Prasanna Balaprakash', 'Tirthankar Ghosal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Retrieval-Augmented Generation', 'Human-in-the-loop', 'Auditability', 'On-premise deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04956</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA, a 'needle-in-haystack' jailbreak that appends benign, model-generated long-context content to embed harmful user goals, exploiting goal positioning within extended contexts.&lt;/li&gt;&lt;li&gt;Shows NINJA significantly increases jailbreak success rates on standard safety benchmark (HarmBench) across multiple state-of-the-art open and proprietary LMs (LLaMA, Qwen, Mistral, Gemini).&lt;/li&gt;&lt;li&gt;Demonstrates NINJA is low-resource, transferable, less detectable, and compute-optimal (longer context can beat more trials under fixed compute), revealing vulnerabilities in long-context aligned LMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'long-context attacks', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Vulnerabilities With Agent Personas</title><link>https://arxiv.org/abs/2511.04697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses LLMs as agents in an agent-based simulation to model population responses to misinformation headlines.&lt;/li&gt;&lt;li&gt;Constructs agent personas across five professions and three mental schemas and evaluates their reactions against ground-truth labels and human predictions.&lt;/li&gt;&lt;li&gt;Finds LLM-generated agents align closely with human labels and that mental schemas influence susceptibility to misinformation more than professional background.&lt;/li&gt;&lt;li&gt;Positions LLM agents as proxies for studying trust, polarization, and susceptibility in information networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Farr', 'Lynnette Hui Xian Ng', 'Stephen Prochaska', 'Iain J. Cruickshank', 'Jevin West']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'agent-based simulation', 'LLM agents', 'social engineering', 'adversarial information']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04697</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steering Language Models with Weight Arithmetic</title><link>https://arxiv.org/abs/2511.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces contrastive weight steering: isolate a behavior direction in weight-space by subtracting weight deltas from two small fine-tunes (one inducing a desired behavior and one its opposite), then add/remove that direction to modify model behavior.&lt;/li&gt;&lt;li&gt;Demonstrates applications to mitigate sycophancy and to induce misalignment; finds weight steering often generalizes further than activation-space interventions while preserving capabilities longer.&lt;/li&gt;&lt;li&gt;Shows weight steering can partially undo undesired behavioral drift from task-specific fine-tuning (reducing sycophancy and under-refusal while keeping task performance gains).&lt;/li&gt;&lt;li&gt;Provides preliminary evidence that emergent misalignment can be detected by measuring similarity between fine-tuning updates and an "evil" weight direction, suggesting potential monitoring for rare misaligned behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constanza Fierro', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model editing', 'weight-space interventions', 'safety mitigation', 'emergent misalignment detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05408</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Toolbox for Improving Evolutionary Prompt Search</title><link>https://arxiv.org/abs/2511.05120</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes several improvements to evolutionary prompt optimization: decomposing evolution into distinct steps, using an LLM-based judge to verify candidates, integrating human feedback, and developing more efficient evaluation strategies.&lt;/li&gt;&lt;li&gt;Claims improved optimization quality and reduced computational overhead; authors release code to enable application to new tasks.&lt;/li&gt;&lt;li&gt;Methods are presented as partially generalizable to other prompt optimization approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Grie{\\ss}haber', 'Maximilian Kimmich', 'Johannes Maucher', 'Ngoc Thang Vu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'evolutionary algorithms', 'prompt engineering', 'LLM evaluation', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05120</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE, a dataset of 300 realistic organizational behavioral policies across 30 industries and a dynamic evaluation framework to test LLM adherence to those policies.&lt;/li&gt;&lt;li&gt;Focuses on multi-turn, interactive conversations and stress-tests models under adversarial conditions to measure policy compliance over time.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn settings (&lt;4%) but large compliance degradation in multi-turn adversarial interactions (up to 84%), highlighting gaps in current alignment/moderation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'red teaming', 'adversarial testing', 'behavioral policies', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title><link>https://arxiv.org/abs/2511.04962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Moral RolePlay, a four-level moral alignment benchmark for evaluating LLMs' ability to role-play characters from paragons to villains.&lt;/li&gt;&lt;li&gt;Finds a monotonic decline in role-playing fidelity as character morality decreases; models particularly fail on traits opposing safety (e.g., deceit, manipulation).&lt;/li&gt;&lt;li&gt;Shows that general chatbot proficiency does not predict villain role-playing ability and that strong safety alignment correlates with poorer villain portrayal.&lt;/li&gt;&lt;li&gt;Highlights a tension between safety alignment and creative fidelity and calls for more context-aware alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Yi', 'Qingxuan Jiang', 'Ruotian Ma', 'Xingyu Chen', 'Qu Yang', 'Mengru Wang', 'Fanghua Ye', 'Ying Shen', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment-evaluation', 'safety', 'benchmarking', 'role-playing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04962</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Behavioral self-awareness in instruction-tuned LLMs can be induced with extremely low-capacity fine-tuning (single rank-1 LoRA adapter).&lt;/li&gt;&lt;li&gt;A single steering vector in activation space recovers nearly all of the fine-tune's behavioral effect, indicating a simple, linear mechanism.&lt;/li&gt;&lt;li&gt;Self-awareness representations are domain-localized and non-universal, with independent task-specific components.&lt;/li&gt;&lt;li&gt;Implication: such easily induced, steerable self-awareness poses safety risks (e.g., concealing capabilities during evaluation) and is mechanistically interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['behavioral self-awareness', 'model interpretability', 'LoRA / model steering', 'alignment &amp; safety', 'evaluation concealment risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title><link>https://arxiv.org/abs/2511.04869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows base LLMs can be semantically calibrated (i.e., provide meaningful confidence over semantic answer classes) using a sampling-based notion of calibration despite being trained only on next-token prediction.&lt;/li&gt;&lt;li&gt;Provides a theoretical mechanism linking next-token calibration/local loss optimality to emergent "B-calibration" for chosen equivalence classes, and derives a testable prediction about when semantic calibration will arise.&lt;/li&gt;&lt;li&gt;Empirically validates three implications: (1) base LLMs are semantically calibrated on QA tasks, (2) RL instruction-tuning degrades this calibration, and (3) chain-of-thought reasoning also breaks calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Preetum Nakkiran', 'Arwen Bradley', "Adam Goli\\'nski", 'Eugene Ndiaye', 'Michael Kirchhof', 'Sinead Williamson']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty estimation', 'LLM robustness', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title><link>https://arxiv.org/abs/2511.04703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of 445 LLM benchmarks from major NLP/ML venues, conducted by a team of 29 expert reviewers.&lt;/li&gt;&lt;li&gt;Finds widespread threats to construct validity when benchmarks claim to measure high-level phenomena like safety and robustness (issues in tasks, metrics, and mapping to phenomena).&lt;/li&gt;&lt;li&gt;Provides eight key recommendations and actionable guidance to improve benchmark design and better measure what matters for safety and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew M. Bean', 'Ryan Othniel Kearns', 'Angelika Romanou', 'Franziska Sofia Hafner', 'Harry Mayne', 'Jan Batzner', 'Negar Foroutan', 'Chris Schmitz', 'Karolina Korgul', 'Hunar Batra', 'Oishi Deb', 'Emma Beharry', 'Cornelius Emde', 'Thomas Foster', 'Anna Gausen', "Mar\\'ia Grandury", 'Simeng Han', 'Valentin Hofmann', 'Lujain Ibrahim', 'Hazel Kim', 'Hannah Rose Kirk', 'Fangru Lin', 'Gabrielle Kaili-May Liu', 'Lennart Luettgau', 'Jabez Magomere', 'Jonathan Rystr{\\o}m', 'Anna Sotnikova', 'Yushi Yang', 'Yilun Zhao', 'Adel Bibi', 'Antoine Bosselut', 'Ronald Clark', 'Arman Cohan', 'Jakob Foerster', 'Yarin Gal', 'Scott A. Hale', 'Inioluwa Deborah Raji', 'Christopher Summerfield', 'Philip H. S. Torr', 'Cozmin Ududec', 'Luc Rocher', 'Adam Mahdi']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'benchmarking', 'evaluation', 'construct validity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04703</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Up the Instruction Ladder for Controllable Language Models</title><link>https://arxiv.org/abs/2511.04694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes instruction hierarchy resolution as an explicit reasoning task, requiring models to deliberate about relationships between system-level and user-level instructions before responding.&lt;/li&gt;&lt;li&gt;Introduces VerIH, a dataset of verifiable constraint-following tasks with aligned and conflicting system-user instructions, used to train models to prioritize higher-priority directives.&lt;/li&gt;&lt;li&gt;Uses lightweight reinforcement learning finetuning to transfer reasoning capability to instruction prioritization, improving instruction following and robustness to jailbreak and prompt injection attacks.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to safety-critical settings, showing that reasoning over instruction hierarchies yields controllable changes in model behavior when system prompts are updated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuo Zheng', 'Vidhisha Balachandran', 'Chan Young Park', 'Faeze Brahman', 'Sachin Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-hierarchy', 'jailbreak-mitigation', 'prompt-injection', 'LLM-alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04694</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Provable Separations between Memorization and Generalization in Diffusion Models</title><link>https://arxiv.org/abs/2511.03202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical separation results showing that empirical score functions learned by diffusion models can drive memorization: the ground-truth score function does not minimize empirical denoising loss (statistical separation).&lt;/li&gt;&lt;li&gt;Shows an approximation separation: representing the empirical score function requires network size scaling with sample size, while the ground-truth score has a more compact representation.&lt;/li&gt;&lt;li&gt;Proposes a pruning-based mitigation that reduces memorization in diffusion transformers while preserving generation quality, supported by theory and empirical evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeqi Ye', 'Qijie Zhu', 'Molei Tao', 'Minshuo Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'diffusion models', 'model pruning', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03202</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Performative Validity of Recourse Explanations</title><link>https://arxiv.org/abs/2506.15366</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes performativity of recourse explanations: when many applicants follow recommendations, collective behavior can shift data distributions and, after model refitting, invalidate those recommendations.&lt;/li&gt;&lt;li&gt;Formally characterizes conditions under which recourse remains valid, highlighting that actions affecting non-causal variables can lead to invalidated recourse.&lt;/li&gt;&lt;li&gt;Argues against standard counterfactual explanations and some causal recourse methods in performative settings, advocating recommending actions only on causal variables.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gunnar K\\"onig', 'Hidde Fokkema', 'Timo Freiesleben', 'Celestine Mendler-D\\"unner', 'Ulrike von Luxburg']&lt;/li&gt;&lt;li&gt;Tags: ['recourse explanations', 'performativity', 'causal inference', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15366</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Know What You Don't Know: Uncertainty Calibration of Process Reward Models</title><link>https://arxiv.org/abs/2506.09338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies miscalibration in process reward models (PRMs) that overestimate success probabilities for partial reasoning trajectories, especially when smaller LLMs complete the reasoning.&lt;/li&gt;&lt;li&gt;Proposes a quantile-regression-based calibration method to align PRM outputs with true success probabilities and produce confidence bounds.&lt;/li&gt;&lt;li&gt;Introduces instance-adaptive scaling (IAS) that uses calibrated success estimates to dynamically allocate compute (number of reasoning trajectories) per instance/step, reducing cost while preserving accuracy.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved calibration, the necessity of calibration for effective IAS, and compute savings on mathematical reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young-Jin Park', 'Kristjan Greenewald', 'Kaveh Alim', 'Hao Wang', 'Navid Azizan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty calibration', 'process reward models', 'confidence estimation', 'inference-time scaling', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09338</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>XBreaking: Understanding how LLMs security alignment can be broken</title><link>https://arxiv.org/abs/2504.21700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates attacks that break censoring/alignment mechanisms in commercial LLMs by analyzing differences between censored and uncensored model behaviors.&lt;/li&gt;&lt;li&gt;Uses an Explainable-AI comparative analysis to identify exploitable alignment patterns unique to censored models.&lt;/li&gt;&lt;li&gt;Proposes XBreaking, a targeted noise-injection attack that leverages those patterns to elicit harmful or uncensored outputs.&lt;/li&gt;&lt;li&gt;Provides experiments demonstrating effectiveness of the approach and insights into censoring mechanism weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Arazzi', 'Vignesh Kumar Kembu', 'Antonino Nocera', 'Vinod P']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment attacks', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21700</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Comparative Study on Noise-Augmented Training and its Effect on Adversarial Robustness in ASR Systems</title><link>https://arxiv.org/abs/2409.01813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares adversarial robustness of four ASR architectures trained under three augmentation regimes: (1) background noise, speed variations, reverberations; (2) speed variations only; (3) no augmentation.&lt;/li&gt;&lt;li&gt;Evaluates models against white-box and black-box adversarial attacks to measure robustness.&lt;/li&gt;&lt;li&gt;Finds that noise-augmented training improves both noisy-speech performance and robustness to adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karla Pizzi', "Mat\\'ias Pizarro", 'Asja Fischer']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'ASR security', 'data augmentation', 'audio adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01813</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What Matters in Data for DPO?</title><link>https://arxiv.org/abs/2508.18312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical and empirical analysis of how preference data distribution affects Direct Preference Optimization (DPO) for aligning LLMs.&lt;/li&gt;&lt;li&gt;Finds that the quality of chosen (preferred) responses is the dominant factor for DPO performance, while rejected responses have limited impact.&lt;/li&gt;&lt;li&gt;Shows online DPO effectively reduces to supervised fine-tuning on chosen responses and analyzes benefits of mixing on-policy data.&lt;/li&gt;&lt;li&gt;Offers practical guidance for constructing high-impact preference datasets and validates findings across diverse tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Zhongze Cai', 'Guanting Chen', 'Huaiyang Zhong', 'Chonghuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'data curation', 'LLM training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18312</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title><link>https://arxiv.org/abs/2508.14926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical Safe RL framework that augments driving objectives with an ethics-aware risk cost combining collision probability and harm severity to protect vulnerable road users.&lt;/li&gt;&lt;li&gt;Introduces a dynamic, risk-sensitive Prioritized Experience Replay to amplify learning from rare but high-risk events and uses polynomial path planning plus PID and Stanley controllers for execution.&lt;/li&gt;&lt;li&gt;Validates approach on closed-loop simulations derived from large-scale real-world traffic datasets, reporting 25–45% reduction in conflict frequency while maintaining comfort metrics within 5% across benchmarks and seeds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianzhao Li', 'Ostap Okhrin']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'ethics-aware objectives', 'rare-event learning', 'autonomous vehicles', 'prioritized experience replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14926</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title><link>https://arxiv.org/abs/2506.10751</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Neural, a two-stage pipeline for evidence-grounded clinical QA: sentence-level evidence identification followed by answer synthesis with explicit citations.&lt;/li&gt;&lt;li&gt;Uses automated prompt-space exploration (DSPy's MIPROv2) to jointly optimize instructions and few-shot demonstrations for each stage.&lt;/li&gt;&lt;li&gt;Employs a self-consistency voting scheme to boost evidence recall while maintaining precision; achieves 51.5 on the ArchEHR-QA 2025 hidden test set, outperforming zero-/few-shot baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Prasanna Teja Reddy Bogireddy', 'Abrar Majeedi', 'Viswanatha Reddy Gajjala', 'Zhuoyan Xu', 'Siddhant Rai', 'Vaishnav Potlapalli']&lt;/li&gt;&lt;li&gt;Tags: ['prompt engineering', 'clinical QA', 'evidence retrieval', 'few-shot prompting', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10751</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title><link>https://arxiv.org/abs/2506.05583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods to adapt conformal prediction to unknown subpopulation shifts where group labels are not provided, enabling valid coverage guarantees without explicit subpopulation information.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of conditions under which formal coverage guarantees remain feasible when subpopulation labels are noisy or inferred.&lt;/li&gt;&lt;li&gt;Presents scalable algorithms applicable in high-dimensional settings and demonstrates empirical effectiveness on vision (vision transformers) and language (large language models) benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nien-Shao Wang', 'Duygu Nur Yaldiz', 'Yavuz Faruk Bakman', 'Sai Praneeth Karimireddy']&lt;/li&gt;&lt;li&gt;Tags: ['conformal prediction', 'distribution shift', 'robustness', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05583</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Are Concepts Erased From Diffusion Models?</title><link>https://arxiv.org/abs/2505.17013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two conceptual mechanisms for concept erasure in diffusion models: (i) disrupting internal guidance processes and (ii) reducing unconditional likelihood of the target concept.&lt;/li&gt;&lt;li&gt;Introduces a suite of independent probing techniques to test erasure: supplying visual context, modifying diffusion trajectories, applying classifier guidance, and analyzing alternative generations.&lt;/li&gt;&lt;li&gt;Empirically evaluates how thoroughly concepts are removed and highlights the need for comprehensive, non-text-based robustness checks for erasure methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Lu', 'Nicky Kriplani', 'Rohit Gandikota', 'Minh Pham', 'David Bau', 'Chinmay Hegde', 'Niv Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'concept erasure', 'model editing', 'robustness evaluation', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17013</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title><link>https://arxiv.org/abs/2501.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of Cognitive Edge Computing for deploying reasoning-capable LLMs and autonomous agents on resource-constrained edge devices, covering model optimization (quantization, sparsity, LORA, distillation), system architectures (on-device inference, offloading, cloud-edge collaboration), and adaptive intelligence (context compression, dynamic routing, federated personalization).&lt;/li&gt;&lt;li&gt;Synthesizes advances in efficient Transformer design, multimodal integration, hardware-aware compilation, and privacy-preserving learning, and proposes a standardized evaluation protocol including latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability.&lt;/li&gt;&lt;li&gt;Identifies remaining challenges explicitly including modality-aware reasoning benchmarks, transparent/reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds; offers practitioner guidelines for cross-layer co-design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xubin Wang', 'Qing Li', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving learning', 'safety/alignment evaluation', 'robustness', 'edge deployment', 'model optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03265</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG</title><link>https://arxiv.org/abs/2412.05447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TOBUGraph, a graph-based retrieval framework that constructs a knowledge graph from unstructured data using LLMs to extract structured facts and relationships.&lt;/li&gt;&lt;li&gt;Performs retrieval via graph traversal (leveraging relationships) rather than embedding text-chunk similarity, removing the need for chunking and reportedly reducing hallucinations.&lt;/li&gt;&lt;li&gt;Evaluated in a production personal-memory application (TOBU) on real user data; claims improved precision and recall compared to multiple RAG implementations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Savini Kashmira', 'Jayanaka L. Dantanarayana', 'Joshua Brodsky', 'Ashish Mahendra', 'Yiping Kang', 'Krisztian Flautner', 'Lingjia Tang', 'Jason Mars']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-graph', 'retrieval-augmented-generation', 'hallucination-mitigation', 'LLM-retrieval', 'production-system']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.05447</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ProFL: Performative Robust Optimal Federated Learning</title><link>https://arxiv.org/abs/2410.18075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends performative prediction to federated learning and highlights that performative stable points may be far from performative optimal points under deployment-induced distribution shifts.&lt;/li&gt;&lt;li&gt;Proposes Performative Robust Optimal Federated Learning, an algorithm designed to find performative optimal points in federated settings even with noisy and contaminated (potentially poisoned) local data.&lt;/li&gt;&lt;li&gt;Provides convergence analysis under the Polyak–Łojasiewicz (PL) condition for non-convex objectives and reports empirical improvements over prior methods across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xue Zheng', 'Tian Xie', 'Xuwei Tan', 'Aylin Yener', 'Xueru Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'robustness', 'performative-prediction', 'data-poisoning', 'distribution-shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18075</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how differentially private full fine-tuning (DP-FFT) can distort pre-trained backbone features, attributing the issue to misalignment between a pre-trained backbone and a randomly initialized linear head.&lt;/li&gt;&lt;li&gt;Proposes and theoretically analyzes a sequential strategy (DP-LP-FFT: linear probe then fine-tune) that mitigates feature distortion under differential privacy.&lt;/li&gt;&lt;li&gt;Develops an approximation scheme to derive upper and lower bounds on training loss in a canonical 2-layer ReLU setting, with additional exact upper bounds for 2-layer linear networks.&lt;/li&gt;&lt;li&gt;Provides empirical experiments on real-world datasets and architectures supporting the theoretical findings and discusses privacy-budget allocation trade-offs in multi-phase private fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuqi Ke', 'Charlie Hou', 'Sewoong Oh', 'Giulia Fanti']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-fine-tuning', 'training-dynamics', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.18905</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?</title><link>https://arxiv.org/abs/2511.05476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MetaCompress, a metamorphic testing framework to evaluate behavioral fidelity between teacher and student language models of code using behavior-preserving transformations.&lt;/li&gt;&lt;li&gt;Finds that distilled student models can fail to deeply mimic teachers, exhibiting up to 285% larger performance drops under adversarial attacks and up to 62% behavioral discrepancies.&lt;/li&gt;&lt;li&gt;Evaluates compressed models from three distillation methods (Compressor, AVATAR, MORPH) on two code tasks, arguing for incorporating behavioral/robustness tests into distillation pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Abdul Awal', 'Mrigank Rochan', 'Chanchal K. Roy']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-distillation', 'robustness', 'metamorphic-testing', 'adversarial-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05476</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steering Language Models with Weight Arithmetic</title><link>https://arxiv.org/abs/2511.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces contrastive weight steering: isolate a behavior direction in weight-space by subtracting weight deltas from two small fine-tunes (one inducing a desired behavior and one its opposite), then add/remove that direction to modify model behavior.&lt;/li&gt;&lt;li&gt;Demonstrates applications to mitigate sycophancy and to induce misalignment; finds weight steering often generalizes further than activation-space interventions while preserving capabilities longer.&lt;/li&gt;&lt;li&gt;Shows weight steering can partially undo undesired behavioral drift from task-specific fine-tuning (reducing sycophancy and under-refusal while keeping task performance gains).&lt;/li&gt;&lt;li&gt;Provides preliminary evidence that emergent misalignment can be detected by measuring similarity between fine-tuning updates and an "evil" weight direction, suggesting potential monitoring for rare misaligned behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Constanza Fierro', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model editing', 'weight-space interventions', 'safety mitigation', 'emergent misalignment detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05408</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE, a dataset of 300 realistic organizational behavioral policies across 30 industries and a dynamic evaluation framework to test LLM adherence to those policies.&lt;/li&gt;&lt;li&gt;Focuses on multi-turn, interactive conversations and stress-tests models under adversarial conditions to measure policy compliance over time.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn settings (&lt;4%) but large compliance degradation in multi-turn adversarial interactions (up to 84%), highlighting gaps in current alignment/moderation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'red teaming', 'adversarial testing', 'behavioral policies', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end differentiable framework that parameterizes shapes via a Fourier series, maps them to pixels using a winding-number mapping, and uses signal-energy constraints for optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can produce high-confidence classifications from DNNs and serve as precise interpretability probes isolating salient regions.&lt;/li&gt;&lt;li&gt;Introduces a new adversarial paradigm based on optimized shapes that can deceive downstream visual tasks, i.e., a novel form of adversarial example useful for robustness testing/red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'red teaming', 'interpretability', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Behavioral self-awareness in instruction-tuned LLMs can be induced with extremely low-capacity fine-tuning (single rank-1 LoRA adapter).&lt;/li&gt;&lt;li&gt;A single steering vector in activation space recovers nearly all of the fine-tune's behavioral effect, indicating a simple, linear mechanism.&lt;/li&gt;&lt;li&gt;Self-awareness representations are domain-localized and non-universal, with independent task-specific components.&lt;/li&gt;&lt;li&gt;Implication: such easily induced, steerable self-awareness poses safety risks (e.g., concealing capabilities during evaluation) and is mechanistically interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['behavioral self-awareness', 'model interpretability', 'LoRA / model steering', 'alignment &amp; safety', 'evaluation concealment risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title><link>https://arxiv.org/abs/2511.04869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows base LLMs can be semantically calibrated (i.e., provide meaningful confidence over semantic answer classes) using a sampling-based notion of calibration despite being trained only on next-token prediction.&lt;/li&gt;&lt;li&gt;Provides a theoretical mechanism linking next-token calibration/local loss optimality to emergent "B-calibration" for chosen equivalence classes, and derives a testable prediction about when semantic calibration will arise.&lt;/li&gt;&lt;li&gt;Empirically validates three implications: (1) base LLMs are semantically calibrated on QA tasks, (2) RL instruction-tuning degrades this calibration, and (3) chain-of-thought reasoning also breaks calibration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Preetum Nakkiran', 'Arwen Bradley', "Adam Goli\\'nski", 'Eugene Ndiaye', 'Michael Kirchhof', 'Sinead Williamson']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'uncertainty estimation', 'LLM robustness', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04869</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title><link>https://arxiv.org/abs/2511.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates copyright auditing / ownership verification for soft prompts in vision-language models (CLIP), showing existing non-intrusive and intrusive methods fail for prompt learning.&lt;/li&gt;&lt;li&gt;Proposes SWAP (Sequential Watermarking for soft prompts): encodes watermarks as a defender-specified order of out-of-distribution classes leveraging CLIP's zero-shot capabilities so the watermark lies in a different, less conflicting decision space.&lt;/li&gt;&lt;li&gt;Designs a hypothesis-test-guided verification protocol, provides theoretical success conditions, and evaluates SWAP on 11 datasets demonstrating effectiveness, harmlessness (doesn't alter main task), and robustness to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenyuan Yang', 'Yichen Sun', 'Changzheng Chen', 'Zhixuan Chu', 'Jiaheng Zhang', 'Yiming Li', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['model ownership', 'watermarking', 'prompt learning', 'backdoor/verification', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04711</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA, a 'needle-in-haystack' jailbreak that appends benign, model-generated long-context content to embed harmful user goals, exploiting goal positioning within extended contexts.&lt;/li&gt;&lt;li&gt;Shows NINJA significantly increases jailbreak success rates on standard safety benchmark (HarmBench) across multiple state-of-the-art open and proprietary LMs (LLaMA, Qwen, Mistral, Gemini).&lt;/li&gt;&lt;li&gt;Demonstrates NINJA is low-resource, transferable, less detectable, and compute-optimal (longer context can beat more trials under fixed compute), revealing vulnerabilities in long-context aligned LMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'long-context attacks', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Multitask Adaptive Control</title><link>https://arxiv.org/abs/2511.05444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Considers adversarially robust multitask adaptive linear quadratic (LQR) control where multiple systems collaboratively learn under model uncertainty and some systems may be adversarial.&lt;/li&gt;&lt;li&gt;Proposes a clustered multitask approach combining clustering, system identification, and resilient aggregation to mitigate the effect of corrupted model updates.&lt;/li&gt;&lt;li&gt;Provides non-asymptotic regret bounds showing regret decreases inversely with the number of honest systems per cluster and that this improvement holds when each cluster contains a bounded fraction of adversarial systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kasra Fallah', 'Leonardo F. Toso', 'James Anderson']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multitask/federated learning', 'robust aggregation', 'adaptive control', 'LQR']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05444</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title><link>https://arxiv.org/abs/2511.05396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies online reinforcement learning under distributional shift between training and deployment dynamics via a robust MDP (RMDP) formulation with f-divergence uncertainty sets.&lt;/li&gt;&lt;li&gt;Introduces the supremal visitation ratio to quantify exploration difficulty when training and deployment dynamics mismatch and shows unbounded ratio makes learning exponentially hard.&lt;/li&gt;&lt;li&gt;Proposes a computationally efficient algorithm achieving sublinear regret with matching lower bounds, and validates results empirically.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting He', 'Zhishuai Liu', 'Weixin Wang', 'Pan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['Robust RL', 'Distributional shift', 'Exploration / Sample complexity', 'Regret bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05396</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning</title><link>https://arxiv.org/abs/2511.05355</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAD-Flower, a flow-matching-based planner augmented with a virtual control input to enforce state and action constraints without retraining.&lt;/li&gt;&lt;li&gt;Derives principled guidance from nonlinear control theory to provide formal guarantees for constraint satisfaction and dynamic consistency of generated trajectories.&lt;/li&gt;&lt;li&gt;Enables test-time enforcement of unseen constraints and demonstrates improved constraint satisfaction over generative-model baselines across multiple tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tzu-Yuan Huang', 'Armin Lederer', 'Dai-Jie Wu', 'Xiaobing Dai', 'Sihua Zhang', 'Stefan Sosnowski', 'Shao-Hua Sun', 'Sandra Hirche']&lt;/li&gt;&lt;li&gt;Tags: ['safe planning', 'flow matching', 'constrained control', 'trajectory generation', 'dynamic consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05355</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval</title><link>https://arxiv.org/abs/2511.05325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of vision-language models (e.g., CLIP) to typographic attacks where text in images skews predictions, and proposes reversing that idea to render relevant product metadata onto images.&lt;/li&gt;&lt;li&gt;Introduces a vision-text compression approach by visually rendering titles/descriptions on product images to strengthen image-text alignment for multimodal retrieval.&lt;/li&gt;&lt;li&gt;Evaluates method on three e-commerce verticals (sneakers, handbags, trading cards) across six foundation vision models, showing consistent improvements in unimodal and multimodal retrieval accuracy.&lt;/li&gt;&lt;li&gt;Argues that rendering product metadata is a simple, effective zero-shot enhancement for e-commerce multimodal retrieval systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janet Jenq', 'Hongda Shen']&lt;/li&gt;&lt;li&gt;Tags: ['typographic attacks', 'adversarial robustness', 'multimodal retrieval', 'vision-language models', 'e-commerce']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05325</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting</title><link>https://arxiv.org/abs/2511.05289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses membership inference attacks (MIA) against time-series forecasting models trained on clinical EHR data and evaluates defenses.&lt;/li&gt;&lt;li&gt;Proposes and compares embedding-space data augmentation methods (ZOO, ZOO-PCA, MixUp) to reduce MIA effectiveness while preserving forecasting accuracy.&lt;/li&gt;&lt;li&gt;Finds ZOO-PCA most effective at lowering the attacker's TPR/FPR ratio without degrading test performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marius Fracarolli', 'Michael Staniek', 'Stefan Riezler']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'data-augmentation', 'clinical-time-series', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05289</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Associative Poisoning to Generative Machine Learning</title><link>https://arxiv.org/abs/2511.05177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces "associative poisoning": a data-only poisoning technique that perturbs training data to manipulate statistical associations between specific feature pairs in generative model outputs without controlling training.&lt;/li&gt;&lt;li&gt;Provides a formal mathematical formulation and theoretical proofs of feasibility and stealthiness, showing marginal feature distributions can be preserved while altering joint associations.&lt;/li&gt;&lt;li&gt;Empirically demonstrates on two state-of-the-art generative models (e.g., image and language models) that associative poisoning can induce or suppress feature associations while maintaining high output quality to evade visual detection.&lt;/li&gt;&lt;li&gt;Analyzes limitations of existing defenses and proposes a novel countermeasure strategy to mitigate associative poisoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mathias Lundteigen Mohus', 'Jingyue Li', 'Zhirong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'generative models', 'stealthy attacks', 'statistical manipulation', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05177</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding</title><link>https://arxiv.org/abs/2511.04934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces leak@k, a meta-evaluation metric that measures the probability of forgotten knowledge reappearing when generating k samples under probabilistic decoding.&lt;/li&gt;&lt;li&gt;Shows that many existing unlearning methods that appear effective under deterministic (greedy) decoding still leak sensitive information reliably under standard sampling strategies.&lt;/li&gt;&lt;li&gt;Performs a large-scale empirical study across three benchmarks (TOFU, MUSE, WMDP) and demonstrates persistent knowledge leakage across methods and tasks, calling into question current unlearning reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Reisizadeh', 'Jiajun Ruan', 'Yiwei Chen', 'Soumyadeep Pal', 'Sijia Liu', 'Mingyi Hong']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy leakage', 'probabilistic decoding', 'evaluation/metrics', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04934</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between model fine-tuning to 'unlearn' harmful concepts and training-free negative-prompt guidance for text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, requiring no changes to existing pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate against nudity and violence benchmarks while preserving input prompt semantics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'safety-mitigation', 'negative-prompts', 'concept-inversion', 'model-unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Brittleness of CLIP Text Encoders</title><link>https://arxiv.org/abs/2511.04247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of non-semantic query perturbations (lexical, syntactic, semantic) on CLIP text encoders in a multimedia retrieval setting.&lt;/li&gt;&lt;li&gt;Evaluation across multiple CLIP variants using TRECVID Ad-Hoc Video Search queries and the V3C1 video collection.&lt;/li&gt;&lt;li&gt;Finds syntactic and semantic perturbations drive the largest instabilities, while trivial surface edits (punctuation, case) also concentrate brittleness.&lt;/li&gt;&lt;li&gt;Argues robustness is a critical evaluation dimension for vision-language models beyond benchmark accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Allie Tran', 'Luca Rossetto']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'CLIP', 'query perturbations', 'information retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04247</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project</title><link>https://arxiv.org/abs/2511.01348</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Vision paper outlining how generative AI can be integrated across the Software Development Life Cycle (SDLC) with emphasis on tools, industry validation, and workforce impacts.&lt;/li&gt;&lt;li&gt;Identifies critical uncertainties including reliability, accountability, security, and data privacy that must be addressed for safe, scalable adoption.&lt;/li&gt;&lt;li&gt;Describes GENIUS consortium's role in advancing methods, tooling, and cross-sector collaboration to mitigate these challenges and align technical innovation with business needs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robin Gr\\"opler', 'Steffen Klepke', 'Jack Johns', 'Andreas Dreschinski', 'Klaus Schmid', 'Benedikt Dornauer', 'Eray T\\"uz\\"un', 'Joost Noppen', 'Mohammad Reza Mousavi', 'Yongjian Tang', 'Johannes Viehmann', 'Selin \\c{S}irin Aslang\\"ul', 'Beum Seuk Lee', 'Adam Ziolkowski', 'Eric Zie']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'privacy', 'software-engineering', 'vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01348</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</title><link>https://arxiv.org/abs/2510.22963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies prompt compression in LLM-powered agents as a novel attack surface that can be manipulated to induce semantic drift and change model behavior.&lt;/li&gt;&lt;li&gt;Proposes CompressionAttack with two strategies: HardCom (discrete adversarial edits for hard compression) and SoftCom (latent-space perturbations for soft compression).&lt;/li&gt;&lt;li&gt;Evaluates across multiple LLMs showing up to 80% attack success and 98% preference flips, high stealthiness and transferability; demonstrates real-world impact in VSCode CLI and Ollama integrations.&lt;/li&gt;&lt;li&gt;Finds existing defenses ineffective, arguing for stronger protections around prompt compression modules.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zesen Liu', 'Zhixiang Zhang', 'Yuchong Xie', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'prompt compression', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22963</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Policy-as-Prompt: converts unstructured governance artifacts into a source-linked policy tree compiled into lightweight prompt-based classifiers for runtime monitoring of AI agents.&lt;/li&gt;&lt;li&gt;Designed to enforce least privilege and data minimization, provide provenance/traceability/audit logging, and integrate human-in-the-loop reviews for conformity assessment.&lt;/li&gt;&lt;li&gt;Evaluations claim reductions in prompt-injection risk, blocking of out-of-scope requests, limitation of toxic outputs, and generation of auditable rationales aligned with AI governance frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['policy-as-code', 'prompt injection', 'runtime guardrails', 'auditability', 'AI governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks</title><link>https://arxiv.org/abs/2509.12386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Amulet, a Python library for evaluating intended and unintended interactions among ML defenses and risks (security, privacy, fairness).&lt;/li&gt;&lt;li&gt;Provides a comprehensive, extensible, and consistent framework with representative attacks, defenses, metrics, and a user-friendly API to plug in new modules.&lt;/li&gt;&lt;li&gt;Enables systematic, unified evaluation and benchmarking of how defenses against one risk affect susceptibility to other risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asim Waheed', 'Vasisht Duddu', 'Rui Zhang', 'Sebastian Szyller']&lt;/li&gt;&lt;li&gt;Tags: ['defense interactions', 'security evaluation', 'privacy', 'fairness', 'tooling/library']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12386</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title><link>https://arxiv.org/abs/2508.20866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AVIATOR, an AI-agentic workflow that automatically injects realistic, category-specific software vulnerabilities to generate high-fidelity datasets.&lt;/li&gt;&lt;li&gt;Orchestrates specialized AI agents, function-level agents, and traditional code analysis tools; uses semantic analysis, injection synthesis with LoRA-based fine-tuning and Retrieval-Augmented Generation, and LLM-based discriminators for validation.&lt;/li&gt;&lt;li&gt;Modular decomposition reduces error propagation and improves robustness of injections; includes static analysis and post-injection validation.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (91%–95% injection success) across three benchmarks, claiming higher accuracy and coverage than prior automated dataset generation techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Lbath', 'Massih-Reza Amini', 'Aurelien Delaitre', 'Vadim Okun']&lt;/li&gt;&lt;li&gt;Tags: ['AI-assisted vulnerability injection', 'LLM agents', 'Dataset generation', 'Software security (dual-use)', 'Static analysis + LLM validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20866</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>What Matters in Data for DPO?</title><link>https://arxiv.org/abs/2508.18312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that the quality of chosen (preferred) responses is the dominant factor for DPO performance, while the quality of rejected responses has limited impact.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of the DPO-optimal response distribution and argues that contrastiveness helps mainly by improving chosen samples.&lt;/li&gt;&lt;li&gt;Analyzes an online DPO setting and finds it effectively reduces to supervised fine-tuning on chosen responses; empirical experiments across tasks validate findings and study on-policy data mixing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Zhongze Cai', 'Guanting Chen', 'Huaiyang Zhong', 'Chonghuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'direct preference optimization (DPO)', 'data quality', 'LLM fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18312</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title><link>https://arxiv.org/abs/2508.14926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical Safe RL framework that augments driving objectives with an ethics-aware risk cost combining collision probability and harm severity to protect vulnerable road users.&lt;/li&gt;&lt;li&gt;Introduces a dynamic, risk-sensitive Prioritized Experience Replay to amplify learning from rare but high-risk events and uses polynomial path planning plus PID and Stanley controllers for execution.&lt;/li&gt;&lt;li&gt;Validates approach on closed-loop simulations derived from large-scale real-world traffic datasets, reporting 25–45% reduction in conflict frequency while maintaining comfort metrics within 5% across benchmarks and seeds.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianzhao Li', 'Ostap Okhrin']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'ethics-aware objectives', 'rare-event learning', 'autonomous vehicles', 'prioritized experience replay']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14926</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Graph Learning</title><link>https://arxiv.org/abs/2507.05636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of graph learning covering scalable, temporal, multimodal, generative, explainable, and responsible graph learning.&lt;/li&gt;&lt;li&gt;Reviews methods for large-scale graph processing, dynamic graphs, heterogeneous/multimodal integration, graph generation, and interpretability techniques.&lt;/li&gt;&lt;li&gt;Discusses ethical and trustworthiness considerations, explicitly mentioning privacy and fairness as part of responsible deployment.&lt;/li&gt;&lt;li&gt;Identifies open challenges (scalability, generalization, heterogeneity, interpretability) and outlines future research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feng Xia', 'Ciyuan Peng', 'Jing Ren', 'Falih Gozi Febrinanto', 'Renqiang Luo', 'Vidya Saikrishna', 'Shuo Yu', 'Xiangjie Kong']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'fairness', 'responsible AI', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05636</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Know What You Don't Know: Uncertainty Calibration of Process Reward Models</title><link>https://arxiv.org/abs/2506.09338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies miscalibration in process reward models (PRMs) that overestimate success probabilities for partial reasoning trajectories, especially when smaller LLMs complete the reasoning.&lt;/li&gt;&lt;li&gt;Proposes a quantile-regression-based calibration method to align PRM outputs with true success probabilities and produce confidence bounds.&lt;/li&gt;&lt;li&gt;Introduces instance-adaptive scaling (IAS) that uses calibrated success estimates to dynamically allocate compute (number of reasoning trajectories) per instance/step, reducing cost while preserving accuracy.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved calibration, the necessity of calibration for effective IAS, and compute savings on mathematical reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young-Jin Park', 'Kristjan Greenewald', 'Kaveh Alim', 'Hao Wang', 'Navid Azizan']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty calibration', 'process reward models', 'confidence estimation', 'inference-time scaling', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09338</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title><link>https://arxiv.org/abs/2506.05583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods to adapt conformal prediction to unknown subpopulation shifts where group labels are not provided, enabling valid coverage guarantees without explicit subpopulation information.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of conditions under which formal coverage guarantees remain feasible when subpopulation labels are noisy or inferred.&lt;/li&gt;&lt;li&gt;Presents scalable algorithms applicable in high-dimensional settings and demonstrates empirical effectiveness on vision (vision transformers) and language (large language models) benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nien-Shao Wang', 'Duygu Nur Yaldiz', 'Yavuz Faruk Bakman', 'Sai Praneeth Karimireddy']&lt;/li&gt;&lt;li&gt;Tags: ['conformal prediction', 'distribution shift', 'robustness', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05583</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>XBreaking: Understanding how LLMs security alignment can be broken</title><link>https://arxiv.org/abs/2504.21700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates attacks that break censoring/alignment mechanisms in commercial LLMs by analyzing differences between censored and uncensored model behaviors.&lt;/li&gt;&lt;li&gt;Uses an Explainable-AI comparative analysis to identify exploitable alignment patterns unique to censored models.&lt;/li&gt;&lt;li&gt;Proposes XBreaking, a targeted noise-injection attack that leverages those patterns to elicit harmful or uncensored outputs.&lt;/li&gt;&lt;li&gt;Provides experiments demonstrating effectiveness of the approach and insights into censoring mechanism weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Arazzi', 'Vignesh Kumar Kembu', 'Antonino Nocera', 'Vinod P']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment attacks', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21700</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://arxiv.org/abs/2502.15027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15027</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title><link>https://arxiv.org/abs/2501.03265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of Cognitive Edge Computing for deploying reasoning-capable LLMs and autonomous agents on resource-constrained edge devices, covering model optimization (quantization, sparsity, LORA, distillation), system architectures (on-device inference, offloading, cloud-edge collaboration), and adaptive intelligence (context compression, dynamic routing, federated personalization).&lt;/li&gt;&lt;li&gt;Synthesizes advances in efficient Transformer design, multimodal integration, hardware-aware compilation, and privacy-preserving learning, and proposes a standardized evaluation protocol including latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability.&lt;/li&gt;&lt;li&gt;Identifies remaining challenges explicitly including modality-aware reasoning benchmarks, transparent/reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds; offers practitioner guidelines for cross-layer co-design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xubin Wang', 'Qing Li', 'Weijia Jia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving learning', 'safety/alignment evaluation', 'robustness', 'edge deployment', 'model optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03265</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG</title><link>https://arxiv.org/abs/2412.05447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TOBUGraph, a graph-based retrieval framework that constructs a knowledge graph from unstructured data using LLMs to extract structured facts and relationships.&lt;/li&gt;&lt;li&gt;Performs retrieval via graph traversal (leveraging relationships) rather than embedding text-chunk similarity, removing the need for chunking and reportedly reducing hallucinations.&lt;/li&gt;&lt;li&gt;Evaluated in a production personal-memory application (TOBU) on real user data; claims improved precision and recall compared to multiple RAG implementations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Savini Kashmira', 'Jayanaka L. Dantanarayana', 'Joshua Brodsky', 'Ashish Mahendra', 'Yiping Kang', 'Krisztian Flautner', 'Lingjia Tang', 'Jason Mars']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-graph', 'retrieval-augmented-generation', 'hallucination-mitigation', 'LLM-retrieval', 'production-system']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.05447</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how differentially private full fine-tuning (DP-FFT) can distort pre-trained backbone features, attributing the issue to misalignment between a pre-trained backbone and a randomly initialized linear head.&lt;/li&gt;&lt;li&gt;Proposes and theoretically analyzes a sequential strategy (DP-LP-FFT: linear probe then fine-tune) that mitigates feature distortion under differential privacy.&lt;/li&gt;&lt;li&gt;Develops an approximation scheme to derive upper and lower bounds on training loss in a canonical 2-layer ReLU setting, with additional exact upper bounds for 2-layer linear networks.&lt;/li&gt;&lt;li&gt;Provides empirical experiments on real-world datasets and architectures supporting the theoretical findings and discusses privacy-budget allocation trade-offs in multi-phase private fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuqi Ke', 'Charlie Hou', 'Sewoong Oh', 'Giulia Fanti']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-fine-tuning', 'training-dynamics', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.18905</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Proprietary Model-Based Safety Response Framework for AI Agents</title><link>https://arxiv.org/abs/2511.03138</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a safety response framework for LLMs with input-level supervised classifier using a four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention) achieving a reported 99.3% risk recall.&lt;/li&gt;&lt;li&gt;Implements output-level defenses by combining Retrieval-Augmented Generation (RAG) with a fine-tuned interpretation model to ground responses in a real-time knowledge base, reduce hallucinations, and enable traceability.&lt;/li&gt;&lt;li&gt;Reports improved safety scores versus a baseline (TinyR1-Safety-8B) and claims 100% safety on a proprietary high-risk test set, framing an engineering pathway for high-trust LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Li', 'Jianjun Xu', 'Pingtao Wei', 'Jiu Li', 'Peiqiang Zhao', 'Jiwei Shi', 'Xuan Zhang', 'Yanhui Yang', 'Xiaodong Hui', 'Peng Xu', 'Wenqin Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'content classification', 'RAG', 'hallucination mitigation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.03138</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title><link>https://arxiv.org/abs/2510.15144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HugAgent, a benchmark to evaluate whether LLMs can simulate individualized human reasoning and predict a specific person's responses and underlying reasoning dynamics given partial prior evidence.&lt;/li&gt;&lt;li&gt;Uses a dual-track design: a human track scaling think-aloud data collection for ecologically valid reasoning traces, and a synthetic track for scalable stress testing.&lt;/li&gt;&lt;li&gt;Reports experiments showing state-of-the-art LLMs have persistent gaps in adapting to individual reasoning styles, presenting HugAgent as an extensible benchmark for cognitive alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chance Jiajie Li', 'Zhenze Mo', 'Yuhan Tang', 'Ao Qu', 'Jiayi Wu', 'Kaiya Ivy Zhao', 'Yulu Gan', 'Jie Fan', 'Jiangbo Yu', 'Hang Jiang', 'Paul Pu Liang', 'Jinhua Zhao', 'Luis Alberto Alonso Pastor', 'Kent Larson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'benchmarking', 'personalization', 'human-simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15144</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Joint Verification and Refinement of Language Models for Safety-Constrained Planning</title><link>https://arxiv.org/abs/2410.14865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Converts LLM-generated robot programs into automaton-based representations and verifies them against task safety specifications.&lt;/li&gt;&lt;li&gt;Proves a composition theorem: any combination of verified subprograms also satisfies the safety specifications, reducing verification complexity.&lt;/li&gt;&lt;li&gt;Introduces verification-guided fine-tuning that uses verification outcomes as supervision to train the model on safe subcomponents rather than full programs.&lt;/li&gt;&lt;li&gt;Empirically improves specification-compliant program generation by ~30% and halves training time compared to fine-tuning on full-program generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Yang', 'Neel P. Bhatt', 'William Ward', 'Zichao Hu', 'Joydeep Biswas', 'Ufuk Topcu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'formal verification', 'program synthesis', 'robotic planning', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14865</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title><link>https://arxiv.org/abs/2511.05396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies online reinforcement learning under distributional shift between training and deployment dynamics via a robust MDP (RMDP) formulation with f-divergence uncertainty sets.&lt;/li&gt;&lt;li&gt;Introduces the supremal visitation ratio to quantify exploration difficulty when training and deployment dynamics mismatch and shows unbounded ratio makes learning exponentially hard.&lt;/li&gt;&lt;li&gt;Proposes a computationally efficient algorithm achieving sublinear regret with matching lower bounds, and validates results empirically.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting He', 'Zhishuai Liu', 'Weixin Wang', 'Pan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['Robust RL', 'Distributional shift', 'Exploration / Sample complexity', 'Regret bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05396</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems</title><link>https://arxiv.org/abs/2511.05269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TAMAS, a benchmark for assessing adversarial risks and safety in multi-agent LLM systems.&lt;/li&gt;&lt;li&gt;Contains five scenarios with 300 adversarial instances across six attack types, 211 tools, and 100 benign tasks; evaluated on ten backbone LLMs and three multi-agent configurations (Autogen, CrewAI).&lt;/li&gt;&lt;li&gt;Proposes an Effective Robustness Score (ERS) to quantify the tradeoff between safety and task effectiveness.&lt;/li&gt;&lt;li&gt;Finds multi-agent LLM systems are highly vulnerable to adversarial attacks and highlights critical failure modes needing stronger defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishan Kavathekar', 'Hemang Jain', 'Ameya Rathod', 'Ponnurangam Kumaraguru', 'Tanuja Ganu']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent LLMs', 'adversarial attacks', 'safety/robustness', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05269</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title><link>https://arxiv.org/abs/2511.05073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically demonstrates that image adversarial examples are more sensitive to occlusion than clean images using sliding mask experiments on CIFAR-10.&lt;/li&gt;&lt;li&gt;Introduces Sliding Mask Confidence Entropy (SMCE) and Mask Entropy Field Maps to quantify confidence volatility under occlusion.&lt;/li&gt;&lt;li&gt;Proposes Sliding Window Mask-based Adversarial Example Detection (SWM-AED) to detect adversarial examples and reports robust detection accuracies across multiple attacks and classifiers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Li', 'Yanwei Xu', 'Keran Li', 'Xiaoli Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'robustness', 'occlusion sensitivity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05073</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title><link>https://arxiv.org/abs/2511.05018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PBSUITE, a dataset of 300 realistic organizational behavioral policies across 30 industries and a dynamic evaluation framework to test LLM adherence to those policies.&lt;/li&gt;&lt;li&gt;Focuses on multi-turn, interactive conversations and stress-tests models under adversarial conditions to measure policy compliance over time.&lt;/li&gt;&lt;li&gt;Finds low failure rates in single-turn settings (&lt;4%) but large compliance degradation in multi-turn adversarial interactions (up to 84%), highlighting gaps in current alignment/moderation approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prasoon Varshney', 'Makesh Narsimhan Sreedhar', 'Liwei Jiang', 'Traian Rebedea', 'Christopher Parisien']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'red teaming', 'adversarial testing', 'behavioral policies', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05018</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning Fourier shapes to probe the geometric world of deep neural networks</title><link>https://arxiv.org/abs/2511.04970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an end-to-end differentiable framework that parameterizes shapes via a Fourier series, maps them to pixels using a winding-number mapping, and uses signal-energy constraints for optimization.&lt;/li&gt;&lt;li&gt;Demonstrates that optimized geometric shapes alone can produce high-confidence classifications from DNNs and serve as precise interpretability probes isolating salient regions.&lt;/li&gt;&lt;li&gt;Introduces a new adversarial paradigm based on optimized shapes that can deceive downstream visual tasks, i.e., a novel form of adversarial example useful for robustness testing/red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Yixing Yong', 'Haixia Bi', 'Lijun He', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'red teaming', 'interpretability', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04970</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title><link>https://arxiv.org/abs/2511.04962</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Moral RolePlay, a four-level moral alignment benchmark for evaluating LLMs' ability to role-play characters from paragons to villains.&lt;/li&gt;&lt;li&gt;Finds a monotonic decline in role-playing fidelity as character morality decreases; models particularly fail on traits opposing safety (e.g., deceit, manipulation).&lt;/li&gt;&lt;li&gt;Shows that general chatbot proficiency does not predict villain role-playing ability and that strong safety alignment correlates with poorer villain portrayal.&lt;/li&gt;&lt;li&gt;Highlights a tension between safety alignment and creative fidelity and calls for more context-aware alignment methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Yi', 'Qingxuan Jiang', 'Ruotian Ma', 'Xingyu Chen', 'Qu Yang', 'Mengru Wang', 'Fanghua Ye', 'Ying Shen', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus']&lt;/li&gt;&lt;li&gt;Tags: ['alignment-evaluation', 'safety', 'benchmarking', 'role-playing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04962</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title><link>https://arxiv.org/abs/2511.04949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a learnable latent-space watermark embedder that encodes/extracts messages tied to high-level image semantics for proactive deepfake detection.&lt;/li&gt;&lt;li&gt;Uses a Multi-Agent Adversarial Reinforcement Learning (MAARL) setup where a watermarking agent and an adversarial attacker agent co-evolve, enabling a dynamic curriculum of benign and malicious manipulations to balance robustness and fragility.&lt;/li&gt;&lt;li&gt;Evaluates on CelebA and CelebA-HQ, reporting consistent improvements (~4.5% on CelebA, ~5.3% on CelebA-HQ) over state-of-the-art under challenging manipulation scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tharindu Fernando', 'Clinton Fookes', 'Sridha Sridharan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'watermarking', 'adversarial reinforcement learning', 'media forensics', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04949</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title><link>https://arxiv.org/abs/2511.04886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a geospatial deep learning model that uses satellite imagery to predict fatal roadway crash risk.&lt;/li&gt;&lt;li&gt;Outputs a Beta probability distribution per instance (rather than a point estimate) to provide uncertainty-aware, calibrated risk assessments.&lt;/li&gt;&lt;li&gt;Reports 17–23% improvement in recall over baselines and improved calibration, aimed at supporting safety-critical decision-making (e.g., autonomous navigation, urban planning).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmad Elallaf', 'Nathan Jacobs', 'Xinyue Ye', 'Mei Chen', 'Gongbo Liang']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty estimation', 'model calibration', 'geospatial deep learning', 'safety-critical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04886</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title><link>https://arxiv.org/abs/2511.04875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Behavioral self-awareness in instruction-tuned LLMs can be induced with extremely low-capacity fine-tuning (single rank-1 LoRA adapter).&lt;/li&gt;&lt;li&gt;A single steering vector in activation space recovers nearly all of the fine-tune's behavioral effect, indicating a simple, linear mechanism.&lt;/li&gt;&lt;li&gt;Self-awareness representations are domain-localized and non-universal, with independent task-specific components.&lt;/li&gt;&lt;li&gt;Implication: such easily induced, steerable self-awareness poses safety risks (e.g., concealing capabilities during evaluation) and is mechanistically interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Bozoukov', 'Matthew Nguyen', 'Shubkarman Singh', 'Bart Bussmann', 'Patrick Leask']&lt;/li&gt;&lt;li&gt;Tags: ['behavioral self-awareness', 'model interpretability', 'LoRA / model steering', 'alignment &amp; safety', 'evaluation concealment risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04875</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2511.04834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies an incompatibility between model fine-tuning to 'unlearn' harmful concepts and training-free negative-prompt guidance for text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Proposes replacing explicit negative prompts with implicit negative embeddings obtained via concept inversion, requiring no changes to existing pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements in defense success rate against nudity and violence benchmarks while preserving input prompt semantics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwoo Shin', 'Byeonghu Na', 'Mina Kang', 'Wonhyeok Choi', 'Il-chul Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'safety-mitigation', 'negative-prompts', 'concept-inversion', 'model-unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04834</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title><link>https://arxiv.org/abs/2511.04729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage knowledge-based method to detect network-induced shape artifacts in synthetic medical images: (i) a feature extractor analyzing per-image distributions of angle gradients along anatomical boundaries, and (ii) an isolation forest anomaly detector.&lt;/li&gt;&lt;li&gt;Evaluated on two synthetic mammography datasets (CSAW-syn and VMLO-syn) with strong concentration of artifacts in the top anomalous partition and AUCs of 0.97 and 0.91 respectively.&lt;/li&gt;&lt;li&gt;Includes a reader study with three imaging scientists showing higher human agreement (66–68%) for the algorithm-identified most anomalous images and moderate Kendall-Tau correlations (~0.43–0.45) between human and algorithm rankings.&lt;/li&gt;&lt;li&gt;Aimed at improving synthetic data quality assurance to support responsible use of synthetic datasets by pinpointing anatomical constraint violations and network-induced distortions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rucha Deshpande', 'Tahsin Rahman', 'Miguel Lago', 'Adarsh Subbaswamy', 'Jana G. Delfino', 'Ghada Zamzmi', 'Elim Thompson', 'Aldo Badano', 'Seyed Kahaki']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-data', 'anomaly-detection', 'medical-imaging', 'dataset-quality', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04729</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models</title><link>https://arxiv.org/abs/2511.04728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Trustworthiness Calibration Framework (TCF) to evaluate phishing detectors along calibration, consistency, and robustness dimensions.&lt;/li&gt;&lt;li&gt;Defines a bounded Trustworthiness Calibration Index (TCI) and a Cross-Dataset Stability (CDS) metric to quantify model dependability and stability across datasets.&lt;/li&gt;&lt;li&gt;Evaluates DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 on five email/spam corpora, finding GPT-4 has the strongest overall trust profile.&lt;/li&gt;&lt;li&gt;Finds that reliability (trustworthiness metrics) can vary independently of raw accuracy, motivating trust-aware evaluation for deployment in security systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniyal Ganiuly', 'Assel Smaiyl']&lt;/li&gt;&lt;li&gt;Tags: ['phishing detection', 'model calibration', 'robustness', 'evaluation framework', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04728</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models</title><link>https://arxiv.org/abs/2511.04716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIA) targeting cognitive diagnosis models (CDMs).&lt;/li&gt;&lt;li&gt;Defines a realistic grey-box threat model that exploits explainability outputs (internal knowledge state vectors exposed via visualizations) and demonstrates these vectors can be reverse-engineered from visualizations.&lt;/li&gt;&lt;li&gt;Proposes P-MIA, a profile-based MIA that uses both final prediction probabilities and reconstructed internal state vectors, outperforming standard black-box baselines on real-world datasets.&lt;/li&gt;&lt;li&gt;Demonstrates P-MIA as an auditing tool to evaluate machine unlearning techniques and exposes their limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingliang Hou', 'Yinuo Wang', 'Teng Guo', 'Zitao Liu', 'Wenzhou Dou', 'Jiaqi Zheng', 'Renqiang Luo', 'Mi Tian', 'Weiqi Luo']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference-attack', 'privacy', 'grey-box-attacks', 'explainability-leakage', 'model-auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04716</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title><link>https://arxiv.org/abs/2511.04711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates copyright auditing / ownership verification for soft prompts in vision-language models (CLIP), showing existing non-intrusive and intrusive methods fail for prompt learning.&lt;/li&gt;&lt;li&gt;Proposes SWAP (Sequential Watermarking for soft prompts): encodes watermarks as a defender-specified order of out-of-distribution classes leveraging CLIP's zero-shot capabilities so the watermark lies in a different, less conflicting decision space.&lt;/li&gt;&lt;li&gt;Designs a hypothesis-test-guided verification protocol, provides theoretical success conditions, and evaluates SWAP on 11 datasets demonstrating effectiveness, harmlessness (doesn't alter main task), and robustness to adaptive attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenyuan Yang', 'Yichen Sun', 'Changzheng Chen', 'Zhixuan Chu', 'Jiaheng Zhang', 'Yiming Li', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['model ownership', 'watermarking', 'prompt learning', 'backdoor/verification', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04711</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking in the Haystack</title><link>https://arxiv.org/abs/2511.04707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NINJA, a 'needle-in-haystack' jailbreak that appends benign, model-generated long-context content to embed harmful user goals, exploiting goal positioning within extended contexts.&lt;/li&gt;&lt;li&gt;Shows NINJA significantly increases jailbreak success rates on standard safety benchmark (HarmBench) across multiple state-of-the-art open and proprietary LMs (LLaMA, Qwen, Mistral, Gemini).&lt;/li&gt;&lt;li&gt;Demonstrates NINJA is low-resource, transferable, less detectable, and compute-optimal (longer context can beat more trials under fixed compute), revealing vulnerabilities in long-context aligned LMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'long-context attacks', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04707</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Prioritize Economy or Climate Action? Investigating ChatGPT Response Differences Based on Inferred Political Orientation</title><link>https://arxiv.org/abs/2511.04706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether ChatGPT tailors responses based on inferred political orientation using personas encoded via memory and custom instructions.&lt;/li&gt;&lt;li&gt;Finds that ChatGPT responses align with inferred political views (vocabulary, reasoning) and that custom instructions and memory produce similar inferences.&lt;/li&gt;&lt;li&gt;Reports that outputs tend to lean left, with Democratic-persona and neutral-persona responses being most similar.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pelin Karadal', 'Dilara Kekulluoglu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy inference', 'bias/evaluation', 'LLM personalization', 'alignment', 'custom instructions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04706</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title><link>https://arxiv.org/abs/2511.04703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of 445 LLM benchmarks from major NLP/ML venues, conducted by a team of 29 expert reviewers.&lt;/li&gt;&lt;li&gt;Finds widespread threats to construct validity when benchmarks claim to measure high-level phenomena like safety and robustness (issues in tasks, metrics, and mapping to phenomena).&lt;/li&gt;&lt;li&gt;Provides eight key recommendations and actionable guidance to improve benchmark design and better measure what matters for safety and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew M. Bean', 'Ryan Othniel Kearns', 'Angelika Romanou', 'Franziska Sofia Hafner', 'Harry Mayne', 'Jan Batzner', 'Negar Foroutan', 'Chris Schmitz', 'Karolina Korgul', 'Hunar Batra', 'Oishi Deb', 'Emma Beharry', 'Cornelius Emde', 'Thomas Foster', 'Anna Gausen', "Mar\\'ia Grandury", 'Simeng Han', 'Valentin Hofmann', 'Lujain Ibrahim', 'Hazel Kim', 'Hannah Rose Kirk', 'Fangru Lin', 'Gabrielle Kaili-May Liu', 'Lennart Luettgau', 'Jabez Magomere', 'Jonathan Rystr{\\o}m', 'Anna Sotnikova', 'Yushi Yang', 'Yilun Zhao', 'Adel Bibi', 'Antoine Bosselut', 'Ronald Clark', 'Arman Cohan', 'Jakob Foerster', 'Yarin Gal', 'Scott A. Hale', 'Inioluwa Deborah Raji', 'Christopher Summerfield', 'Philip H. S. Torr', 'Cozmin Ududec', 'Luc Rocher', 'Adam Mahdi']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'benchmarking', 'evaluation', 'construct validity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04703</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Vulnerabilities With Agent Personas</title><link>https://arxiv.org/abs/2511.04697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses LLMs as agents in an agent-based simulation to model population responses to misinformation headlines.&lt;/li&gt;&lt;li&gt;Constructs agent personas across five professions and three mental schemas and evaluates their reactions against ground-truth labels and human predictions.&lt;/li&gt;&lt;li&gt;Finds LLM-generated agents align closely with human labels and that mental schemas influence susceptibility to misinformation more than professional background.&lt;/li&gt;&lt;li&gt;Positions LLM agents as proxies for studying trust, polarization, and susceptibility in information networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Farr', 'Lynnette Hui Xian Ng', 'Stephen Prochaska', 'Iain J. Cruickshank', 'Jevin West']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'agent-based simulation', 'LLM agents', 'social engineering', 'adversarial information']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04697</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Up the Instruction Ladder for Controllable Language Models</title><link>https://arxiv.org/abs/2511.04694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes instruction hierarchy resolution as an explicit reasoning task, requiring models to deliberate about relationships between system-level and user-level instructions before responding.&lt;/li&gt;&lt;li&gt;Introduces VerIH, a dataset of verifiable constraint-following tasks with aligned and conflicting system-user instructions, used to train models to prioritize higher-priority directives.&lt;/li&gt;&lt;li&gt;Uses lightweight reinforcement learning finetuning to transfer reasoning capability to instruction prioritization, improving instruction following and robustness to jailbreak and prompt injection attacks.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to safety-critical settings, showing that reasoning over instruction hierarchies yields controllable changes in model behavior when system prompts are updated.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuo Zheng', 'Vidhisha Balachandran', 'Chan Young Park', 'Faeze Brahman', 'Sachin Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-hierarchy', 'jailbreak-mitigation', 'prompt-injection', 'LLM-alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04694</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Is All You Need for Urban Planning AI</title><link>https://arxiv.org/abs/2511.05375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper proposing an Agentic Urban Planning AI Framework combining Perception, Foundation, and Reasoning layers with six logic components (Analysis, Generation, Verification, Evaluation, Collaboration, Decision).&lt;/li&gt;&lt;li&gt;Argues for explicit, value-based, rule-grounded, and explainable reasoning agents to support planner decision-making (regulatory compliance, trade-off deliberation, transparent justifications).&lt;/li&gt;&lt;li&gt;Compares reasoning agents with statistical learning, suggests benchmark metrics, and outlines research challenges for deploying reasoning-capable planning agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sijie Yang', 'Jiatong Li', 'Filip Biljecki']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'explainability', 'rule-grounded verification', 'multi-agent reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.05375</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title><link>https://arxiv.org/abs/2511.04956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ORCHID, a modular agentic retrieval-augmented generation (RAG) system for High-Risk Property (HRP) classification with cooperating agents (retrieval, refiner, classifier, validator) and Model Context Protocol (MCP) for model-agnostic on-premise operation.&lt;/li&gt;&lt;li&gt;Implements human-in-the-loop workflows that defer uncertain items to Subject Matter Experts (SMEs), capture SME feedback, and produce append-only audit bundles (run-cards, prompts, evidence) with on-policy citations for traceability.&lt;/li&gt;&lt;li&gt;Reports preliminary improvements in accuracy and traceability over a non-agentic baseline and demonstrates exportable audit artifacts to support trustworthy LLM assistance in sensitive DOE compliance workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Mahbub', 'Vanessa Lama', 'Sanjay Das', 'Brian Starks', 'Christopher Polchek', 'Saffell Silvers', 'Lauren Deck', 'Prasanna Balaprakash', 'Tirthankar Ghosal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Retrieval-Augmented Generation', 'Human-in-the-loop', 'Auditability', 'On-premise deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04956</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Epistemic Reject Option Prediction</title><link>https://arxiv.org/abs/2511.04855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an epistemic reject-option predictor that abstains when epistemic uncertainty (due to limited training data) implies high expected regret.&lt;/li&gt;&lt;li&gt;Frames optimal prediction as minimizing expected regret relative to a Bayes-optimal predictor with full data, and uses a rejection cost threshold to decide abstention.&lt;/li&gt;&lt;li&gt;Builds on Bayesian learning to quantify epistemic uncertainty and identify inputs where training data is insufficient for reliable decisions.&lt;/li&gt;&lt;li&gt;Targets safety in high-stakes applications by enabling principled abstention rather than relying solely on aleatoric uncertainty assumptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vojtech Franc', 'Jakub Paplham']&lt;/li&gt;&lt;li&gt;Tags: ['epistemic uncertainty', 'reject option / abstention', 'Bayesian learning', 'safety / robustness', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.04855</guid><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
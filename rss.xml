<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 27 Nov 2025 23:36:10 +0000</lastBuildDate><item><title>UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</title><link>https://arxiv.org/abs/2511.19413</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniGame, a self-adversarial post-training framework that adds a lightweight perturber at the shared token interface so the generation branch actively challenges fragile understanding.&lt;/li&gt;&lt;li&gt;Aims to resolve representation trade-offs in unified multimodal models to improve cross-modal consistency, robustness to distributional shifts, and adversarial resilience.&lt;/li&gt;&lt;li&gt;Reports empirical gains (e.g., consistency +4.6%, understanding +3.6%, OOD +4.8% on NaturalBench, adversarial robustness +6.2% on AdVQA), is architecture-agnostic, adds &lt;1% parameters, and is complementary to existing post-training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolong Su', 'Wang Lu', 'Hao Chen', 'Sharon Li', 'Jindong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'self-adversarial / red teaming', 'multimodal models', 'post-training interventions', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19413</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Generalizable cardiac substructures segmentation from contrast and non-contrast CTs using pretrained transformers</title><link>https://arxiv.org/abs/2505.10855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid transformer-convolutional network to segment cardiac substructures across contrast-enhanced (CECT) and non-contrast CT (NCCT) and different patient positions.&lt;/li&gt;&lt;li&gt;Trains oracle, contrast-only, and a balanced model on a lung-cancer cohort and evaluates on held-out lung cases and an external breast-cancer cohort; the balanced model matches oracle performance while using 64% fewer labeled cases and outperforms TotalSegmentator.&lt;/li&gt;&lt;li&gt;Demonstrates robustness to contrast and positioning variations for most substructures and produces dosimetrically equivalent contours to manual delineations, supporting clinical deployment.&lt;/li&gt;&lt;li&gt;Attributes improved generalization to pretraining combined with a balanced distribution of NCCT/CECT during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Rangnekar', 'Nikhil Mankuzhy', 'Jonas Willmann', 'Chloe Choi', 'Abraham Wu', 'Maria Thor', 'Andreas Rimner', 'Harini Veeraraghavan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution-shift', 'medical-imaging', 'segmentation', 'pretrained-transformer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10855</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion</title><link>https://arxiv.org/abs/2511.19958</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GFT-GCN: combines Graph Fourier Transform and Graph Convolutional Networks to extract compact spectral features from 3D face meshes.&lt;/li&gt;&lt;li&gt;Introduces a spectral diffusion mechanism producing irreversible, renewable, and unlinkable templates to protect biometric data; uses a lightweight client-server design so raw data stays on device.&lt;/li&gt;&lt;li&gt;Evaluated on BU-3DFE and FaceScape datasets, reporting high recognition accuracy and strong resistance to reconstruction attacks, demonstrating a tradeoff between privacy and performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hichem Felouat', 'Hanrui Wang', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['biometric privacy', 'template protection', 'reconstruction attacks', 'graph neural networks', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19958</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</title><link>https://arxiv.org/abs/2511.19111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DiffSeg30k: a 30k-image dataset with pixel-level annotations of localized diffusion-based edits, built from in-the-wild COCO images.&lt;/li&gt;&lt;li&gt;Covers multi-turn (up to 3 sequential) edits across eight state-of-the-art diffusion models, with VLM-driven region selection and context-aware prompts for realistic add/remove/attribute edits.&lt;/li&gt;&lt;li&gt;Reframes AIGC detection from whole-image classification to semantic segmentation (localization + model identification) and benchmarks baseline segmentation methods, highlighting robustness challenges to image distortions.&lt;/li&gt;&lt;li&gt;Finds segmentation models can also act as strong whole-image classifiers and shows cross-generator generalization potential; dataset and code released publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hai Ci', 'Ziheng Peng', 'Pei Yang', 'Yingxin Xuan', 'Mike Zheng Shou']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'image forensics', 'diffusion models', 'dataset/benchmark', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19111</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection</title><link>https://arxiv.org/abs/2511.18780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ConceptGuard, a two-stage framework for proactively detecting and mitigating unsafe semantics in text-and-image-to-video generation.&lt;/li&gt;&lt;li&gt;Stage 1: contrastive detection projects fused image-text inputs into a structured concept space to identify latent multimodal risks.&lt;/li&gt;&lt;li&gt;Stage 2: semantic suppression intervenes in multimodal prompt conditioning to steer generation away from unsafe concepts.&lt;/li&gt;&lt;li&gt;Introduces two benchmarks (ConceptRisk and T2VSafetyBench-TI2V) and shows state-of-the-art results in risk detection and safe video generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruize Ma', 'Minghong Cai', 'Yilei Jiang', 'Jiaming Han', 'Yi Feng', 'Yingshui Tan', 'Xiaoyong Zhu', 'Bo Zhang', 'Bo Zheng', 'Xiangyu Yue']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'video generation', 'content filtering/mitigation', 'risk detection', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18780</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title><link>https://arxiv.org/abs/2511.14386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first texture-enabled physical adversarial example (PAE) specifically targeting stereo binocular depth estimation for autonomous driving.&lt;/li&gt;&lt;li&gt;Uses a 3D PAE with a global camouflage texture to maintain visual consistency and attack effectiveness across different stereo viewpoints.&lt;/li&gt;&lt;li&gt;Introduces a 3D stereo matching rendering module to align PAEs with real-world positions/headings in binocular vision, and a merging attack to seamlessly blend the PAE into the environment.&lt;/li&gt;&lt;li&gt;Evaluations demonstrate the PAEs can successfully induce erroneous depth outputs from stereo models, highlighting a perceptual security vulnerability in autonomous driving systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangqiao Zhao', 'Shuo Huai', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-examples', 'stereo-depth-estimation', 'autonomous-driving', 'adversarial-robustness', 'perception-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14386</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title><link>https://arxiv.org/abs/2511.01724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRBench, a dedicated benchmark comparing probabilistic robustness (PR) and adversarial robustness (AR) across many training methods, datasets, and architectures with a public leaderboard of 222 models.&lt;/li&gt;&lt;li&gt;Empirically evaluates common adversarial training (AT) and PR-targeted training methods on metrics including clean accuracy, PR, AR, training efficiency, and generalization error (GE).&lt;/li&gt;&lt;li&gt;Finds that AT is more versatile for improving both AR and PR across hyperparameters, while PR-targeted methods yield lower generalization error and higher clean accuracy.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis on generalization error of PR performance across different training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Zhang', 'Zheng Wang', 'Zhen Chen', 'Wenjie Ruan', 'Qing Guo', 'Siddartha Khastgir', 'Carsten Maple', 'Xingyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'probabilistic robustness', 'adversarial training', 'benchmarking', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01724</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics</title><link>https://arxiv.org/abs/2508.17247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines and demonstrates Multi-Embedding Attacks (MEA): additional watermark embeddings on already-protected images can destroy the original forensic watermark.&lt;/li&gt;&lt;li&gt;Shows existing proactive forensic watermarking methods are vulnerable under realistic multi-embedding scenarios.&lt;/li&gt;&lt;li&gt;Proposes Adversarial Interference Simulation (AIS): a plug-and-play fine-tuning paradigm that simulates MEA during training and uses a resilience-driven loss to learn sparse, stable watermark representations.&lt;/li&gt;&lt;li&gt;Empirical results indicate AIS substantially improves robustness of various existing watermarking methods against MEA without changing model architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lixin Jia', 'Haiyang Sun', 'Zhiqing Guo', 'Yunfeng Diao', 'Dan Ma', 'Gaobo Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake forensics', 'watermarking robustness', 'adversarial attack/defense', 'multi-embedding attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17247</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding</title><link>https://arxiv.org/abs/2506.12336</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Trust-videoLLMs, a comprehensive benchmark evaluating 23 state-of-the-art video-capable multimodal LLMs on truthfulness, robustness, safety, fairness, and privacy across 30 tasks with adapted, synthetic, and annotated videos.&lt;/li&gt;&lt;li&gt;Assesses spatiotemporal risks, temporal consistency, and cross-modal perturbation impact, revealing notable limitations in dynamic scene understanding, resilience to cross-modal attacks, and mitigation of real-world risks.&lt;/li&gt;&lt;li&gt;Finds that proprietary models generally show higher credibility though scaling does not reliably improve trustworthiness; provides a public, extensible toolkit for standardized trustworthiness assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youze Wang', 'Zijun Chen', 'Ruoyu Chen', 'Shishen Gu', 'Wenbo Hu', 'Jiayang Liu', 'Yinpeng Dong', 'Hang Su', 'Jun Zhu', 'Meng Wang', 'Richang Hong']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'privacy', 'benchmark', 'multimodal-llms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12336</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training</title><link>https://arxiv.org/abs/2506.04263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dynamic Epsilon Scheduling (DES), an adaptive per-instance and per-iteration perturbation budget for adversarial training.&lt;/li&gt;&lt;li&gt;DES combines three factors — gradient-based proxy for distance to decision boundary, softmax entropy (prediction confidence), and Monte Carlo dropout (model uncertainty) — to set epsilon dynamically.&lt;/li&gt;&lt;li&gt;Reports improved adversarial robustness and standard accuracy on CIFAR-10 and CIFAR-100 versus fixed-epsilon baselines and prior adaptive methods; provides theoretical analysis of stability and convergence.&lt;/li&gt;&lt;li&gt;Aims to enable instance-aware, data-driven adversarial training that tailors perturbations to sample-specific robustness characteristics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alan Mitkiy', 'James Smith', 'Myungseo wong', 'Hana Satou', 'Hiroshi Tanaka', 'Emily Johnson']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'adaptive perturbation / dynamic epsilon', 'robustness', 'instance-aware scheduling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04263</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title><link>https://arxiv.org/abs/2506.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IVY-FAKE, a large-scale multimodal (image + video) benchmark with &gt;106K annotated training samples and 5K manually verified evaluation examples for explainable AIGC detection.&lt;/li&gt;&lt;li&gt;Identifies limitations of existing datasets (binary labels, limited explainability) and MLLM-based detectors' reasoning, and provides richer, multidimensional annotations for localization and interpretability.&lt;/li&gt;&lt;li&gt;Proposes Ivy-xDetector, an RL-based detector using Group Relative Policy Optimization (GRPO) that outputs explainable reasoning chains and shows substantial performance gains over prior methods on multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Wenhui Dong', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng', 'Xinbin Yuan', 'Yifei Bi', 'Ming Zhao', 'Zian Zhou', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'forgery detection', 'explainability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00979</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</title><link>https://arxiv.org/abs/2505.15191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAADA, a transfer learning framework that decomposes adversarial perturbations into on-manifold (semantic) and off-manifold (brittleness) components for data augmentation.&lt;/li&gt;&lt;li&gt;Provides theoretical results showing on-manifold consistency lowers hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions.&lt;/li&gt;&lt;li&gt;Introduces a geometry-aware alignment loss minimizing geodesic discrepancy between source and target manifolds to improve cross-domain alignment.&lt;/li&gt;&lt;li&gt;Empirical evaluation on DomainNet, VisDA, and Office-Home demonstrates improved robustness and cross-domain generalization in unsupervised and few-shot settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hana Satou', 'Alan Mitkiy', 'Emma Collins', 'Finn Kingston']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'domain adaptation', 'adversarial data augmentation', 'manifold regularization', 'transfer learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15191</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Gray-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</title><link>https://arxiv.org/abs/2408.10901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Posterior Collapse Attack (PCA), a gray-box adversarial defense that induces posterior collapse in the VAE of Latent Diffusion Models to prevent unauthorized image editing.&lt;/li&gt;&lt;li&gt;Identifies two collapse modes—diffusion collapse and concentration collapse—and designs a unified loss to flexibly induce either mode for different protection goals.&lt;/li&gt;&lt;li&gt;Requires only access to the VAE encoder (&lt;4% of LDM parameters), is prompt-invariant (operates before text conditioning), and claims strong transferability across VAE-based LDMs with lower compute and memory costs.&lt;/li&gt;&lt;li&gt;Shows theoretical analysis and empirical results demonstrating improved protection effectiveness, runtime/VRAM efficiency, and generalization compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongliang Guo', 'Chun Tong Lei', 'Lei Fang', 'Shuai Zhao', 'Yifei Qian', 'Jingyu Lin', 'Zeyu Wang', 'Cunjian Chen', "Ognjen Arandjelovi\\'c", 'Chun Pong Lau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'latent diffusion models', 'VAE posterior collapse', 'image protection', 'model robustness/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10901</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LTD: Low Temperature Distillation for Gradient Masking-free Adversarial Training</title><link>https://arxiv.org/abs/2111.02331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Low-Temperature Distillation (LTD): use a teacher model with a relatively low softmax temperature to produce softer labels, while keeping the student temperature fixed during training and inference.&lt;/li&gt;&lt;li&gt;Aims to refine label representations to account for real-world label ambiguity and to improve adversarial robustness without inducing gradient masking (a common issue with defensive distillation).&lt;/li&gt;&lt;li&gt;Demonstrates improved robust accuracy on CIFAR-10, CIFAR-100, and ImageNet when combined with existing adversarial training frameworks, achieving 58.19%, 31.13%, and 42.08% respectively, without additional data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erh-Chung Chen', 'Che-Rung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'defensive-distillation', 'gradient-masking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2111.02331</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation</title><link>https://arxiv.org/abs/2511.21135</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SocialNav, a foundation model with a hierarchical brain-action architecture for socially-aware embodied navigation that combines high-level social reasoning with low-level trajectory generation.&lt;/li&gt;&lt;li&gt;Introduces the SocNav Dataset (7M samples) composed of a Cognitive Activation Dataset (chain-of-thought explanations and social traversability signals) and an Expert Trajectories Pyramid from videos, simulators, and robots.&lt;/li&gt;&lt;li&gt;Presents a multi-stage training pipeline: imitation learning to inject navigation skills and social norms, followed by SAFE-GRPO, a flow-based RL approach that explicitly rewards socially compliant behaviors.&lt;/li&gt;&lt;li&gt;Reports substantial gains (+38% success rate, +46% social compliance rate) over state-of-the-art methods on navigation performance and social compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Chen', 'Yingnan Guo', 'Zedong Chu', 'Minghua Luo', 'Yanfen Shen', 'Mingchao Sun', 'Junjun Hu', 'Shichao Xie', 'Kuan Yang', 'Pei Shi', 'Zhining Gu', 'Lu Liu', 'Honglin Han', 'Xiaolong Wu', 'Mu Xu', 'Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['embodied-navigation', 'human-robot-interaction', 'social-compliance', 'reinforcement-learning', 'datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21135</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.21663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ADVLA, an efficient adversarial attack that injects perturbations into features projected from a visual encoder into the textual feature space of Vision-Language-Action (VLA) models.&lt;/li&gt;&lt;li&gt;Uses attention-guided, patch-wise sparse perturbations with strategies for sensitivity enhancement, sparsity enforcement, and concentration to achieve near-100% attack success under L_infty=4/255 while modifying &lt;10% of patches.&lt;/li&gt;&lt;li&gt;Claims low computational cost (single-step ~0.06s) and near-imperceptible localized perturbations that effectively disrupt downstream action predictions without costly end-to-end training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naifu Zhang', 'Wei Tao', 'Xi Xiao', 'Qianpu Sun', 'Yuxin Zheng', 'Wentao Mo', 'Peiqiang Wang', 'Nan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language-action models', 'feature-space attacks', 'patch-wise sparse perturbations', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21663</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</title><link>https://arxiv.org/abs/2511.21662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multi-Crit, a benchmark for evaluating multimodal judges on their ability to follow pluralistic, fine-grained evaluation criteria across open-ended generation and verifiable reasoning tasks.&lt;/li&gt;&lt;li&gt;Constructs a dataset of challenging response pairs annotated with multi-criterion human judgments and proposes three metrics: pluralistic adherence, criterion-switching flexibility, and recognition of criterion-level preference conflicts.&lt;/li&gt;&lt;li&gt;Evaluates 25 LMMs, finding proprietary models struggle with consistent criterion adherence (especially on open-ended tasks), open-source models lag further, and critic fine-tuning improves visual grounding but not pluralistic criterion-level judgment.&lt;/li&gt;&lt;li&gt;Analyzes effects of reasoning fine-tuning, test-time scaling, and boundary consistency between model types, aiming to improve reliable and steerable multimodal AI evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Xiong', 'Yi Ge', 'Ming Li', 'Zuolong Zhang', 'Pranav Kulkarni', 'Kaishen Wang', 'Qi He', 'Zeying Zhu', 'Chenxi Liu', 'Ruibo Chen', 'Tong Zheng', 'Yanshuo Chen', 'Xiyao Wang', 'Renrui Zhang', 'Wenhu Chen', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'multimodal-benchmark', 'model-evaluation', 'criterion-following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21662</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Robust Prompt Distillation for 3D Point Cloud Models</title><link>https://arxiv.org/abs/2511.21574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multimodal Robust Prompt Distillation (MRPD), a teacher-student framework that learns lightweight prompts for 3D point cloud models by aligning student features with robust embeddings from three teachers (depth-projection vision model, high-performance 3D model, and a text encoder).&lt;/li&gt;&lt;li&gt;Introduces a confidence-gated mechanism to dynamically weight contributions from the different modalities during distillation to ensure reliable knowledge transfer.&lt;/li&gt;&lt;li&gt;Distillation occurs only during training, so there is no extra inference-time cost; empirical results claim substantial improvement over state-of-the-art defenses on both white-box and black-box adversarial attacks while also improving clean-data performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Gu', 'Liming Lu', 'Xu Zheng', 'Anan Du', 'Yongbin Zhou', 'Shuchao Pang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', '3D point clouds', 'defense/distillation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21574</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Video Generation Models Are Good Latent Reward Models</title><link>https://arxiv.org/abs/2511.21541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Process Reward Feedback Learning (PRFL) to perform preference optimization for video generation entirely in latent (noisy) space rather than pixel/RGB space.&lt;/li&gt;&lt;li&gt;Argues that pre-trained video diffusion models are well suited as latent reward models because they natively process noisy latent representations and preserve temporal information.&lt;/li&gt;&lt;li&gt;Reports improved alignment with human preferences plus reduced memory use and training time compared to RGB-space reward learning approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyue Mi', 'Wenqing Yu', 'Jiesong Lian', 'Shibo Jie', 'Ruizhe Zhong', 'Zijun Liu', 'Guozhen Zhang', 'Zixiang Zhou', 'Zhiyong Xu', 'Yuan Zhou', 'Qinglin Lu', 'Fan Tang']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Reward modeling', 'Video generation', 'Training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21541</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis</title><link>https://arxiv.org/abs/2511.21397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Idis, a VQA dataset that systematically varies visual distractors across semantic, numerical, and spatial dimensions to study their effect on test-time scaling in VLMs.&lt;/li&gt;&lt;li&gt;Finds that visual distractors produce an inverse scaling effect (accuracy drops) but, unlike textual distractors, do not increase reasoning trace length.&lt;/li&gt;&lt;li&gt;Shows that tracking attribute counts in reasoning traces helps explain interactions between distractors, reasoning length, and accuracy, and that these trends extend to benchmarks with visual biases (e.g., Waterbirds).&lt;/li&gt;&lt;li&gt;Proposes a simple prompting strategy that mitigates bias-driven predictions in reasoning models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyun Bae', 'Hyunjong Ok', 'Sangwoo Mo', 'Jaeho Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-models', 'evaluation/benchmarking', 'bias', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21397</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs</title><link>https://arxiv.org/abs/2511.21251</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVFakeBench, a 12K-sample audio-video forgery detection benchmark covering seven forgery types and four annotation granularities across human and general subjects.&lt;/li&gt;&lt;li&gt;Proposes a multi-stage hybrid forgery generation framework that combines task-planning models with expert generative models to produce diverse, high-quality forgeries.&lt;/li&gt;&lt;li&gt;Defines a multi-task evaluation suite: binary detection, forgery-type classification, forgery-detail selection/localization, and explanatory reasoning.&lt;/li&gt;&lt;li&gt;Evaluates 11 Audio-Video Large Language Models (AV-LMMs) and 2 detection baselines, showing AV-LMMs have promise but notable weaknesses in fine-grained perception and reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuhan Xia', 'Peipei Li', 'Xuannan Liu', 'Dongsen Zhang', 'Xinyu Guo', 'Zekun Li']&lt;/li&gt;&lt;li&gt;Tags: ['audio-video forgery', 'deepfake detection', 'benchmark', 'AV-LMM evaluation', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21251</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization</title><link>https://arxiv.org/abs/2511.21237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes T3-Tracer, a tri-level (frame, segment, audio) temporal-aware framework for detecting and localizing partial audio forgeries.&lt;/li&gt;&lt;li&gt;Introduces two core modules: Frame-Audio Feature Aggregation Module (FA-FAM) for per-frame authenticity using frame- and audio-level context, and Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM) for detecting segment boundaries via multi-scale inter-frame differences.&lt;/li&gt;&lt;li&gt;Targets hard partial forgeries that alter semantically critical frames while preserving perceptual authenticity and reports state-of-the-art results on three datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuhan Xia', 'Xuannan Liu', 'Xing Cui', 'Peipei Li']&lt;/li&gt;&lt;li&gt;Tags: ['audio forgery detection', 'deepfake detection', 'audio forensics', 'temporal/multiscale modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21237</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.21192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UPA-RFAS, a method to learn a single physical adversarial patch that transfers across diverse Vision-Language-Action (VLA) models and real-world robot deployments.&lt;/li&gt;&lt;li&gt;Combines a feature-space objective with l1 deviation and InfoNCE repulsive loss, a robustness-augmented two-phase min-max training (inner invisible perturbations, outer universal patch optimization), and two VLA-specific losses to hijack attention and induce image-text mismatch.&lt;/li&gt;&lt;li&gt;Evaluations show consistent transferability across architectures, finetuned variants, manipulation tasks, viewpoints, and physical executions, demonstrating a practical patch-based attack surface for robot VLAs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Lu', 'Yi Yu', 'Yiming Yang', 'Chenyu Yi', 'Qixin Zhang', 'Bingquan Shen', 'Alex C. Kot', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'transferable attacks', 'vision-language-action', 'sim-to-real', 'robotics security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21192</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models</title><link>https://arxiv.org/abs/2511.21145</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TEAR, an automated temporal-aware red-teaming framework that targets text-to-video (T2V) models to uncover safety risks tied to temporal dynamics.&lt;/li&gt;&lt;li&gt;Uses a two-stage prompt generator: initial generator training followed by temporal-aware online preference learning, plus a refine model for improving prompt stealthiness and adversarial effectiveness iteratively.&lt;/li&gt;&lt;li&gt;Designs attacks that craft textually innocuous prompts which exploit temporal sequencing to elicit policy-violating video outputs.&lt;/li&gt;&lt;li&gt;Evaluates across open-source and commercial T2V systems, reporting over 80% attack success rate versus a prior best of 57%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming He', 'Guanyu Hou', 'Hongwei Li', 'Zhicong Huang', 'Kangjie Chen', 'Yi Yu', 'Wenbo Jiang', 'Guowen Xu', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'text-to-video', 'adversarial prompting', 'safety evaluation', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21145</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision</title><link>https://arxiv.org/abs/2511.20994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline to detect unsafe content appearing in intermediate reasoning traces.&lt;/li&gt;&lt;li&gt;Creates GuardTrace dataset via diverse prompting and MLRM- plus human-based voting/refinement to train/evaluate unsafe reasoning detection.&lt;/li&gt;&lt;li&gt;Proposes a three-stage progressive training scheme and data refinement to capture nuanced, context-dependent safety preferences across risk levels.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (93.1% F1; +13.5% F1 vs prior multimodal safety defenses) on in-domain and out-of-domain unsafe reasoning detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiao Xiang', 'Junchi Chen', 'Zhenchao Jin', 'Changtao Miao', 'Haojie Yuan', 'Qi Chu', 'Tao Gong', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'reasoning-trace detection', 'dataset', 'safety monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20994</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI</title><link>https://arxiv.org/abs/2511.20983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a federated learning framework that extracts the ViT CLS token (768-D) and encrypts it with CKKS homomorphic encryption for secure aggregation in medical histopathology classification.&lt;/li&gt;&lt;li&gt;Claims ~30x communication reduction compared to gradient encryption (326 KB per aggregation) and supports encrypted inference on ciphertexts.&lt;/li&gt;&lt;li&gt;Demonstrates that gradients are vulnerable to model inversion attacks with near-perfect reconstructions (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), while the CLS+HE approach prevents such attacks.&lt;/li&gt;&lt;li&gt;Evaluated on a three-client lung cancer histopathology classification task, reporting 96.12% global accuracy in the unencrypted domain and 90.02% in the encrypted domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Al Amin', 'Kamrul Hasan', 'Liang Hong', 'Sharif Ullah']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'homomorphic encryption', 'model inversion', 'privacy-preserving ML', 'vision transformer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20983</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation</title><link>https://arxiv.org/abs/2511.20889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Null-Text Test-Time Alignment (Null-TTA): optimises the unconditional (null) text embedding used in classifier-free guidance at inference to align text-to-image diffusion models to target rewards.&lt;/li&gt;&lt;li&gt;Argues semantic-space optimisation prevents reward hacking by constraining changes to a semantically coherent manifold, unlike latent/noise manipulation.&lt;/li&gt;&lt;li&gt;Steers the model's generative distribution toward the reward without updating model weights, achieving strong target alignment and cross-reward generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taehoon Kim', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'reward hacking', 'diffusion models', 'classifier-free guidance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20889</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models</title><link>https://arxiv.org/abs/2511.20795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Lightweight reproduction of KRISP that reduces parameter count while retaining ~75% of original performance for knowledge-enhanced VQA.&lt;/li&gt;&lt;li&gt;Systematic ablation studies reveal design flaws, implicit pitfalls, and scalability limits when training under resource constraints.&lt;/li&gt;&lt;li&gt;Confining outputs to a constrained external knowledge-graph domain reduces hallucinations and enforces domain-bounded responses.&lt;/li&gt;&lt;li&gt;Emphasis on minimal-parameter setups enabling edge-device (smartphone/AR-VR) offline visual reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Souradeep Dutta', 'Keshav Bulia', 'Neena S Nair']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'knowledge-enhanced_VL', 'robustness', 'model_alignment', 'edge_deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20795</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?</title><link>https://arxiv.org/abs/2511.20710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates black-box membership inference attacks (MIA) on multi-modal vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Proposes a neuroscience-inspired topological regularization (tau) to create NEURO VLM variants (tau&gt;0) intended to improve privacy resilience.&lt;/li&gt;&lt;li&gt;Evaluates BLIP, PaliGemma 2, and ViT-GPT2 on COCO, CC3M, and NoCaps; reports ~24% mean ROC-AUC drop in MIA success for NEURO VLMs on BLIP/COCO while maintaining captioning utility (MPNet, ROUGE-2).&lt;/li&gt;&lt;li&gt;Findings suggest neuro-inspired/topological regularization can reduce membership inference leakage in multimodal VLMs without significant utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Amebley', 'Sayanton Dibbo']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy', 'vision-language models', 'topological regularization', 'neuro-inspired robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20710</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop</title><link>https://arxiv.org/abs/2511.17673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Structured Cognitive Loop (R-CCAM) that decomposes LLM agent operation into Retrieval, Cognition, Control, Action, and Memory phases to improve modularity and traceability.&lt;/li&gt;&lt;li&gt;Introduces Soft Symbolic Control: a governance mechanism that imposes symbolic constraints on probabilistic LLM inference to restore explainability and controllability while retaining neural flexibility.&lt;/li&gt;&lt;li&gt;Empirical validation shows zero policy violations, elimination of redundant tool calls, and full decision traceability on multi-step conditional reasoning tasks.&lt;/li&gt;&lt;li&gt;Positions SCL within hybrid intelligence, contrasts it with prompt-centric/memory-only approaches, and provides an open-source implementation and a GPT-4o-powered demo agent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myung Ho Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agent architectures', 'symbolic control / neuro-symbolic governance', 'alignment and safety', 'explainability / transparency', 'agent controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17673</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title><link>https://arxiv.org/abs/2511.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteganoBackdoor: a method that uses natural-language steganography and gradient-guided optimization to convert semantic trigger seeds (e.g., names/entities) into fluent, high-payload steganographic carriers for backdoors.&lt;/li&gt;&lt;li&gt;Achieves &gt;99% attack success with an order-of-magnitude lower poisoning rate compared to prior NLP backdoor methods.&lt;/li&gt;&lt;li&gt;Demonstrates strong evasion of a comprehensive suite of data-level defenses, revealing a practical blind spot in current defense and threat models for NLP systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Xue', 'Ruiyi Zhang', 'Zijun Zhang', 'Pengtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'data-poisoning', 'NLP', 'steganography', 'defense-evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14301</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows</title><link>https://arxiv.org/abs/2509.20490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RadAgents, a multi-agent, modular framework for chest X‑ray interpretation that encodes a radiologist-like, auditable workflow.&lt;/li&gt;&lt;li&gt;Emphasizes multimodal fusion and visually grounded rationales rather than text-only explanations, and integrates multimodal retrieval-augmentation for verification.&lt;/li&gt;&lt;li&gt;Includes mechanisms to detect and resolve cross-tool inconsistencies and to provide principled verification for outputs to improve reliability and transparency.&lt;/li&gt;&lt;li&gt;Aims to produce outputs more consistent with clinical guidelines and practice through task-aware multimodal reasoning and grounding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Zhang', 'Corey D Barrett', 'Jangwon Kim', 'Lichao Sun', 'Tara Taghavi', 'Krishnaram Kenthapadi']&lt;/li&gt;&lt;li&gt;Tags: ['clinical AI safety', 'multimodal reasoning', 'interpretability/verification', 'agentic systems', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20490</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ExDDV: A New Dataset for Explainable Deepfake Detection in Video</title><link>https://arxiv.org/abs/2503.14421</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ExDDV, a dataset of ~5.4K real and deepfake videos manually annotated with textual explanations and click-based localization of artifacts.&lt;/li&gt;&lt;li&gt;Benchmarks vision-language methods with fine-tuning and in-context learning, finding that both text and click supervision are needed to build robust explainable detectors that localize and describe artifacts.&lt;/li&gt;&lt;li&gt;Provides code and dataset for reproducibility and evaluation of explainable deepfake detection models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking (Dataset/Benchmark)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vlad Hondru', 'Eduard Hogea', 'Darian Onchis', 'Radu Tudor Ionescu']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'explainability', 'dataset/benchmark', 'video forensics', 'vision-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.14421</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MindEval: Benchmarking Language Models on Multi-turn Mental Health Support</title><link>https://arxiv.org/abs/2511.18491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MindEval: a benchmark/framework for evaluating LLMs in realistic multi-turn mental health therapy conversations, co-designed with PhD-level licensed clinical psychologists.&lt;/li&gt;&lt;li&gt;Uses simulated patients and automatic LLM-based evaluation; validated by comparing simulated patient realism to human text and correlating automatic metrics with human expert judgments.&lt;/li&gt;&lt;li&gt;Evaluates 12 state-of-the-art LLMs, finds overall poor performance (below 4/6), with particular failures like sycophancy, overvalidation, and reinforcing maladaptive beliefs; performance degrades with longer interactions and more severe patient symptoms.&lt;/li&gt;&lt;li&gt;Provides fully automated, model-agnostic evaluation and releases code, prompts, and human evaluation data for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Jos\\'e Pombal", "Maya D'Eon", 'Nuno M. Guerreiro', 'Pedro Henrique Martins', "Ant\\'onio Farinhas", 'Ricardo Rei']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'mental-health-safety', 'benchmark', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18491</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Toward Honest Language Models for Deductive Reasoning</title><link>https://arxiv.org/abs/2511.09222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines honesty for deductive reasoning as producing conclusions only when logically entailed and abstaining otherwise; frames tasks where models must derive correct conclusions or abstain.&lt;/li&gt;&lt;li&gt;Curates two graph-structure datasets (linear algebra and logical inference) with half the instances made unanswerable by perturbing an edge to test abstention.&lt;/li&gt;&lt;li&gt;Shows existing methods (prompting, supervised finetuning, GRPO) struggle, with GRPO prone to training collapse when negative rewards dominate early training.&lt;/li&gt;&lt;li&gt;Proposes ACNCHOR, an RL method that injects ground-truth trajectories into rollouts to stabilize training and significantly improve honest deductive reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiarui Liu', 'Kaustubh Dhole', 'Yingheng Wang', 'Haoyang Wen', 'Sarah Zhang', 'Haitao Mao', 'Gaotang Li', 'Neeraj Varshney', 'Jingguo Liu', 'Xiaoman Pan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'honesty/abstention', 'reinforcement learning', 'reasoning robustness', 'evaluation/datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09222</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConfTuner: Training Large Language Models to Express Their Confidence Verbally</title><link>https://arxiv.org/abs/2508.18847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ConfTuner, a fine-tuning method that trains LLMs to verbalize calibrated confidence using a novel tokenized Brier score loss.&lt;/li&gt;&lt;li&gt;Provides a theoretical proof that the tokenized Brier score is a proper scoring rule, incentivizing truthful probability reporting without needing ground-truth confidence labels.&lt;/li&gt;&lt;li&gt;Empirically improves calibration across reasoning tasks, generalizes to black-box models (e.g., GPT-4o), and yields downstream benefits for self-correction and model cascades.&lt;/li&gt;&lt;li&gt;Method is lightweight, requires minimal overhead, and avoids reliance on heuristically generated proxy confidence estimates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yibo Li', 'Miao Xiong', 'Jiaying Wu', 'Bryan Hooi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM calibration', 'Confidence estimation', 'Safety/Trustworthiness', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18847</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Simulatability of LLM Explanations for Generation Tasks</title><link>https://arxiv.org/abs/2505.21740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a general framework to evaluate counterfactual simulatability of LLM explanations for generation tasks (extending prior work from yes/no QA).&lt;/li&gt;&lt;li&gt;Applies the framework to news summarization and medical suggestion tasks, measuring how well explanations let users predict model outputs under counterfactual inputs.&lt;/li&gt;&lt;li&gt;Finds explanations improve user prediction for summarization (skill-based) but perform poorly for medical suggestion (knowledge-based), suggesting limits of current explanation quality in high-stakes domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marvin Limpijankit', 'Yanda Chen', 'Melanie Subbiah', 'Nicholas Deas', 'Kathleen McKeown']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'safety evaluation', 'alignment', 'LLM interpretability', 'medical AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21740</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation</title><link>https://arxiv.org/abs/2505.18685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents MM Health, a large-scale multimodal (text + image) health misinformation dataset of 34,746 news articles containing both human-generated (5,776) and AI-generated (28,880) multimodal content from SOTA generative models.&lt;/li&gt;&lt;li&gt;Benchmarks three tasks—reliability checks, originality checks, and fine-grained AI detection—showing current SOTA models struggle to distinguish reliability and origin of information in the health domain.&lt;/li&gt;&lt;li&gt;Aims to address gaps in topical coverage, inclusion of AI-generated content, and availability of raw multimodal data to support development of detection methods for human vs. machine-generated health misinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihao Zhang', 'Yiran Zhang', 'Xiyue Zhou', 'Liting Huang', 'Imran Razzak', 'Preslav Nakov', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation-detection', 'AI-generated-content-detection', 'multimodal-dataset', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18685</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2410.13334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how ethical biases in LLMs can be exploited to increase jailbreak success, introducing the concept of 'BiasJailbreak'.&lt;/li&gt;&lt;li&gt;Empirically demonstrates disparity in jailbreak success rates (e.g., ~20% difference non-binary vs cisgender, ~16% white vs black) on GPT-4o.&lt;/li&gt;&lt;li&gt;Presents an automated attack pipeline that generates biased keywords via the target LLM to produce harmful outputs.&lt;/li&gt;&lt;li&gt;Proposes a prompt-injection defense, 'BiasDefense', as an efficient mitigation alternative to post-generation guard models and open-sources code/artifacts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isack Lee', 'Haebin Seong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'bias', 'prompt-injection', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13334</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs</title><link>https://arxiv.org/abs/2511.20104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Replicates emergent misalignment across nine modern open-weight models (Gemma 3 and Qwen 3 families, 1B–32B parameters).&lt;/li&gt;&lt;li&gt;Fine-tuning models on insecure code generation increases misalignment from 0.07% (base) to 0.68% (fine-tuned).&lt;/li&gt;&lt;li&gt;Identifies a format-dependent vulnerability: requiring JSON output nearly doubles misalignment versus natural language prompts (0.96% vs 0.42%).&lt;/li&gt;&lt;li&gt;Finds open-weight models show substantially lower misalignment rates than GPT-4o (20%), with implications for safety interventions and red-team strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Craig Dickson']&lt;/li&gt;&lt;li&gt;Tags: ['emergent-misalignment', 'alignment-robustness', 'jailbreaking/format-vulnerabilities', 'open-model-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20104</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>BlockCert: Certified Blockwise Extraction of Transformer Mechanisms</title><link>https://arxiv.org/abs/2511.17645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BlockCert, a framework to extract blockwise surrogate implementations of transformer residual blocks with machine-checkable certificates bounding approximation error and coverage.&lt;/li&gt;&lt;li&gt;Provides a Lipschitz-based composition theorem formalized in Lean 4 to lift local per-block guarantees to a global deviation bound on model behaviour under a prompt distribution.&lt;/li&gt;&lt;li&gt;Demonstrates empirical results on GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B showing high per-block coverage and very small end-to-end deviation in evaluated settings.&lt;/li&gt;&lt;li&gt;Describes a lightweight extension to support certified local model edits, enabling formally-bounded modifications without full retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sandro Andric']&lt;/li&gt;&lt;li&gt;Tags: ['model-extraction', 'mechanistic-interpretability', 'formal-verification', 'model-editing', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17645</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On Evaluating LLM Alignment by Evaluating LLMs as Judges</title><link>https://arxiv.org/abs/2511.20604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes generation-evaluation consistency (GE-consistency) across LLMs and finds a strong correlation between a model's generation quality and its capability as an evaluator when judged by a strong LLM oracle.&lt;/li&gt;&lt;li&gt;Proposes AlignEval, a benchmarking paradigm that measures LLM alignment with human preferences by assessing LLMs in their role as evaluators rather than directly judging their generated outputs.&lt;/li&gt;&lt;li&gt;Shows AlignEval matches or outperforms existing automatic evaluation benchmarks (e.g., AlpacaEval, Arena-Hard) in capturing human preferences for ranking LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixin Liu', 'Pengfei Liu', 'Arman Cohan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety_evaluation', 'LLM_judging', 'benchmarking', 'evaluation_methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20604</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</title><link>https://arxiv.org/abs/2511.20494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Adversarial Confusion Attack that perturbs images to maximize next-token entropy and induce incoherent or confidently incorrect outputs from multimodal LLMs (MLLMs).&lt;/li&gt;&lt;li&gt;Demonstrates attack effectiveness using a small ensemble of open-source MLLMs and shows transferability to unseen open-source and proprietary models (e.g., Qwen3-VL, GPT-5.1).&lt;/li&gt;&lt;li&gt;Evaluates both full-image and adversarial CAPTCHA settings, using PGD-based perturbations in white-box and transfer scenarios to disrupt MLLM-powered agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Hoscilowicz', 'Artur Janicki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal LLMs', 'transferability', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20494</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian</title><link>https://arxiv.org/abs/2511.19719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates faithfulness of LLM-generated explanations for Persian emotion classification by comparing model-identified influential words to human annotations.&lt;/li&gt;&lt;li&gt;Uses token-level log-probability confidence scores to assess explanation faithfulness and compares two prompting orders: Predict-then-Explain vs Explain-then-Predict.&lt;/li&gt;&lt;li&gt;Finds that LLMs achieve strong classification accuracy but their explanations often diverge from human reasoning and agree more with each other than with humans, highlighting limits of current explanation methods and metrics in low-resource languages.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mobina Mehrazar', 'Mohammad Amin Yousefi', 'Parisa Abolfath Beygi', 'Behnam Bahrak']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'faithfulness', 'alignment', 'multilingual', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19719</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic</title><link>https://arxiv.org/abs/2511.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PaTAS, a parallel trust-propagation framework using Subjective Logic that runs alongside neural computation via Trust Nodes and Trust Functions.&lt;/li&gt;&lt;li&gt;Introduces a Parameter Trust Update mechanism for training-time parameter reliability and an Inference-Path Trust Assessment (IPTA) to compute instance-specific trust at inference.&lt;/li&gt;&lt;li&gt;Evaluates on real-world and adversarial/poisoned/biased datasets, showing PaTAS yields interpretable trust estimates that expose reliability gaps and distinguish adversarial from benign inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Koffi Ismael Ouattara', 'Ioannis Krontiris', 'Theo Dimitrakos', 'Dennis Eisermann', 'Frank Kargl']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'trustworthiness', 'adversarial detection', 'data poisoning detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20586</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title><link>https://arxiv.org/abs/2511.01724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRBench, a dedicated benchmark comparing probabilistic robustness (PR) and adversarial robustness (AR) across many training methods, datasets, and architectures with a public leaderboard of 222 models.&lt;/li&gt;&lt;li&gt;Empirically evaluates common adversarial training (AT) and PR-targeted training methods on metrics including clean accuracy, PR, AR, training efficiency, and generalization error (GE).&lt;/li&gt;&lt;li&gt;Finds that AT is more versatile for improving both AR and PR across hyperparameters, while PR-targeted methods yield lower generalization error and higher clean accuracy.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis on generalization error of PR performance across different training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Zhang', 'Zheng Wang', 'Zhen Chen', 'Wenjie Ruan', 'Qing Guo', 'Siddartha Khastgir', 'Carsten Maple', 'Xingyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'probabilistic robustness', 'adversarial training', 'benchmarking', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.01724</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking</title><link>https://arxiv.org/abs/2507.11137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NeuralMark, a weight-based neural network watermarking method that uses a hash-derived irreversible binary watermark as a filter to select parameters for embedding.&lt;/li&gt;&lt;li&gt;Design couples embedding locations with the hashed watermark and adds average pooling to improve robustness against forging, overwriting, fine-tuning, and pruning attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of security boundaries and empirical validation across 13 CNN/Transformer architectures on multiple image classification tasks and one text generation task.&lt;/li&gt;&lt;li&gt;Claims broad applicability across architectures and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Yao', 'Jin Song', 'Jian Jin']&lt;/li&gt;&lt;li&gt;Tags: ['neural network watermarking', 'model ownership / IP protection', 'forging and overwriting attacks', 'robustness to fine-tuning and pruning', 'weight-based embedding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11137</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training</title><link>https://arxiv.org/abs/2506.04263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dynamic Epsilon Scheduling (DES), an adaptive per-instance and per-iteration perturbation budget for adversarial training.&lt;/li&gt;&lt;li&gt;DES combines three factors — gradient-based proxy for distance to decision boundary, softmax entropy (prediction confidence), and Monte Carlo dropout (model uncertainty) — to set epsilon dynamically.&lt;/li&gt;&lt;li&gt;Reports improved adversarial robustness and standard accuracy on CIFAR-10 and CIFAR-100 versus fixed-epsilon baselines and prior adaptive methods; provides theoretical analysis of stability and convergence.&lt;/li&gt;&lt;li&gt;Aims to enable instance-aware, data-driven adversarial training that tailors perturbations to sample-specific robustness characteristics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alan Mitkiy', 'James Smith', 'Myungseo wong', 'Hana Satou', 'Hiroshi Tanaka', 'Emily Johnson']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'adaptive perturbation / dynamic epsilon', 'robustness', 'instance-aware scheduling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04263</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems</title><link>https://arxiv.org/abs/2504.20906</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a 'Giant-Step Baby-Step' classifier that linearizes non-linear sensor–actuator relationships to enable fast, explainable anomaly detection in industrial control systems (ICS).&lt;/li&gt;&lt;li&gt;Validated on a water treatment testbed, reporting millisecond detection latency and the ability to pinpoint implicated sensors and actuators (explainability/traceability).&lt;/li&gt;&lt;li&gt;Reports 97.72% accuracy while explicitly treating deviations within safe operational bounds as non-anomalous, arguing that high-resolution slower detectors may be unnecessary for some ICS safety envelopes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarad Venugopalan', 'Sridhar Adepu']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'industrial-control-systems', 'explainable-ai', 'real-time-detection', 'cyber-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20906</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Gray-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</title><link>https://arxiv.org/abs/2408.10901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Posterior Collapse Attack (PCA), a gray-box adversarial defense that induces posterior collapse in the VAE of Latent Diffusion Models to prevent unauthorized image editing.&lt;/li&gt;&lt;li&gt;Identifies two collapse modes—diffusion collapse and concentration collapse—and designs a unified loss to flexibly induce either mode for different protection goals.&lt;/li&gt;&lt;li&gt;Requires only access to the VAE encoder (&lt;4% of LDM parameters), is prompt-invariant (operates before text conditioning), and claims strong transferability across VAE-based LDMs with lower compute and memory costs.&lt;/li&gt;&lt;li&gt;Shows theoretical analysis and empirical results demonstrating improved protection effectiveness, runtime/VRAM efficiency, and generalization compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongliang Guo', 'Chun Tong Lei', 'Lei Fang', 'Shuai Zhao', 'Yifei Qian', 'Jingyu Lin', 'Zeyu Wang', 'Cunjian Chen', "Ognjen Arandjelovi\\'c", 'Chun Pong Lau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'latent diffusion models', 'VAE posterior collapse', 'image protection', 'model robustness/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10901</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LTD: Low Temperature Distillation for Gradient Masking-free Adversarial Training</title><link>https://arxiv.org/abs/2111.02331</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Low-Temperature Distillation (LTD): use a teacher model with a relatively low softmax temperature to produce softer labels, while keeping the student temperature fixed during training and inference.&lt;/li&gt;&lt;li&gt;Aims to refine label representations to account for real-world label ambiguity and to improve adversarial robustness without inducing gradient masking (a common issue with defensive distillation).&lt;/li&gt;&lt;li&gt;Demonstrates improved robust accuracy on CIFAR-10, CIFAR-100, and ImageNet when combined with existing adversarial training frameworks, achieving 58.19%, 31.13%, and 42.08% respectively, without additional data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erh-Chung Chen', 'Che-Rung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'defensive-distillation', 'gradient-masking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2111.02331</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</title><link>https://arxiv.org/abs/2511.19413</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniGame, a self-adversarial post-training framework that adds a lightweight perturber at the shared token interface so the generation branch actively challenges fragile understanding.&lt;/li&gt;&lt;li&gt;Aims to resolve representation trade-offs in unified multimodal models to improve cross-modal consistency, robustness to distributional shifts, and adversarial resilience.&lt;/li&gt;&lt;li&gt;Reports empirical gains (e.g., consistency +4.6%, understanding +3.6%, OOD +4.8% on NaturalBench, adversarial robustness +6.2% on AdVQA), is architecture-agnostic, adds &lt;1% parameters, and is complementary to existing post-training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolong Su', 'Wang Lu', 'Hao Chen', 'Sharon Li', 'Jindong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'self-adversarial / red teaming', 'multimodal models', 'post-training interventions', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19413</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction</title><link>https://arxiv.org/abs/2511.17879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reward hacking (diversity collapse) during RL post-training of generative sequence models in live human-AI music jamming.&lt;/li&gt;&lt;li&gt;Proposes a co-evolving adversarial discriminator trained on policy-generated trajectories; policy maximizes discriminator output alongside coherence rewards to preserve diversity.&lt;/li&gt;&lt;li&gt;Evaluates in simulation (fixed and learned melody agents) and via a user study with expert musicians showing improved diversity, harmonic coherence, adaptation speed, and perceived agency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yusong Wu', 'Stephen Brade', 'Teng Ma', 'Tia-Jane Fowler', 'Enning Yang', 'Berker Banar', 'Aaron Courville', 'Natasha Jaques', 'Cheng-Zhi Anna Huang']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'reinforcement learning', 'adversarial training', 'robustness', 'human-AI interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17879</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hard Samples, Bad Labels: Robust Loss Functions That Know When to Back Off</title><link>https://arxiv.org/abs/2511.16512</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two robust loss functions (Blurry Loss and Piecewise-zero Loss) that down-weight or ignore hard-to-classify samples, which are likely to be mislabeled.&lt;/li&gt;&lt;li&gt;Shows these losses improve label-error detection (higher F1) across a variety of artificially corrupted datasets and both uniform and non-uniform corruption scenarios.&lt;/li&gt;&lt;li&gt;Provides ablation studies and evaluates integration with label-error detection frameworks, demonstrating broad applicability for pruning/correcting training-data label errors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas Pellegrino', 'David Szczecina', 'Paul Fieguth']&lt;/li&gt;&lt;li&gt;Tags: ['label noise', 'robustness', 'loss functions', 'data poisoning / corrupted labels', 'error detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16512</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title><link>https://arxiv.org/abs/2511.10234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how LLM-based graph reasoners are sensitive to graph serialization choices (node labeling, edge encoding, syntax) and how that affects output invariance under permutations and formatting changes.&lt;/li&gt;&lt;li&gt;Evaluates how fine-tuning modifies sensitivity: larger pre-trained models show more robustness, while fine-tuning reduces relabeling sensitivity but can increase sensitivity to structure/format variations and does not reliably improve out-of-distribution generalization.&lt;/li&gt;&lt;li&gt;Introduces a decomposition of serialization factors and a benchmarking suite (including novel spectral tasks) to systematically measure robustness and generalization of LLM graph reasoners.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Herbst', 'Lea Karbevska', 'Divyanshu Kumar', 'Akanksha Ahuja', 'Fatemeh Gholamzadeh Nasrabadi', 'Fabrizio Frasca']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'LLM evaluation', 'graph reasoning', 'invariance', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10234</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Weak-to-Strong Generalization under Distribution Shifts</title><link>https://arxiv.org/abs/2510.21332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies failure modes of naive weak-to-strong supervision under distribution shift, where a strong model can perform worse than its weak supervisors.&lt;/li&gt;&lt;li&gt;Proposes RAVEN, a method that jointly learns strong-model parameters and dynamic weighting/combinations of weak supervisors to improve robustness.&lt;/li&gt;&lt;li&gt;Evaluates RAVEN across image classification, text classification, and preference alignment tasks, showing large OOD improvements while matching in-distribution performance.&lt;/li&gt;&lt;li&gt;Shows RAVEN can assign higher weights to more accurate weak models, effectively identifying more trustworthy supervision sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myeongho Jeon', 'Jan Sobotka', 'Suhwan Choi', "Maria Brbi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'weak-to-strong generalization', 'distribution shift', 'supervision trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21332</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Inference-Time Alignment of Diffusion Models via Evolutionary Algorithms</title><link>https://arxiv.org/abs/2506.00299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an inference-time, black-box alignment method for diffusion image models using evolutionary algorithms to search latent space for solutions that satisfy safety/domain objectives.&lt;/li&gt;&lt;li&gt;Claims 3–35% higher ImageReward scores than both gradient-free and gradient-based baselines and competitive performance on the Open Image Preferences dataset across four alignment objectives.&lt;/li&gt;&lt;li&gt;Emphasizes efficiency: requires 55–76% less GPU memory and is 72–80% faster than gradient-based methods, without needing gradients or internal model access.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Purvish Jajal', 'Nick John Eliopoulos', 'Benjamin Shiue-Hal Chou', 'George K. Thiruvathukal', 'James C. Davis', 'Yung-Hsiang Lu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'diffusion models', 'evolutionary algorithms', 'black-box optimization', 'safety constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00299</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Deep Actor-Critics with Tight Risk Certificates</title><link>https://arxiv.org/abs/2505.19682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes tight risk certificates for deep actor-critic (RL) algorithms that predict generalization performance and quantify risk of malfunction from validation-time observations.&lt;/li&gt;&lt;li&gt;Key method: a recursive PAC-Bayes adaptation that splits limited validation roll-outs into portions and builds bounds on excess loss using the previous portion's predictor as a data-informed prior.&lt;/li&gt;&lt;li&gt;Shows that a small set of feasible evaluation roll-outs from a pretrained policy suffices to produce accurate risk certificates.&lt;/li&gt;&lt;li&gt;Empirical validation across multiple locomotion tasks, actor-critic variants, and policy expertise levels yields bounds tight enough to be practical for deployment validation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bahareh Tasdighi', 'Manuel Haussmann', 'Yi-Shan Wu', 'Andres R. Masegosa', 'Melih Kandemir']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'risk certificates', 'reinforcement learning safety', 'PAC-Bayes', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19682</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment of large language models with constrained learning</title><link>https://arxiv.org/abs/2505.19387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses computing optimal LLM policies under constrained alignment: maximize primary reward subject to constraints on secondary utilities.&lt;/li&gt;&lt;li&gt;Proposes an iterative dual-based method that alternates LLM policy updates (Lagrangian maximization) with dual variable updates (dual descent).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of primal-dual gaps and bounds on the optimality gap due to LLM parameterization.&lt;/li&gt;&lt;li&gt;Empirically evaluates the approach on PKU-SafeRLHF and Anthropic HH-RLHF datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Botong Zhang', 'Shuo Li', 'Ignacio Hounie', 'Osbert Bastani', 'Dongsheng Ding', 'Alejandro Ribeiro']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'constrained optimization', 'RLHF', 'Lagrangian duality', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19387</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title><link>https://arxiv.org/abs/2501.01457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRR (Distillation-Reinforcement-Reasoning), an externalist three-step framework that builds a lightweight Discriminative Model (DM) to critique LLM behavioral traces rather than relying on the model's self-introspection.&lt;/li&gt;&lt;li&gt;DM is trained on distilled behavioral traces to identify and reject suspicious reasoning steps at inference, guiding the base LLM to explore alternative, less-flawed reasoning paths without modifying the base model.&lt;/li&gt;&lt;li&gt;Claims significant improvements over prominent self-critique methods across multiple reasoning benchmarks, with a lightweight, annotation-free design intended to be scalable and adaptable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diji Yang', 'Linda Zeng', 'Kezhen Chen', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Alignment &amp; robustness', 'External critique / discriminative critic', 'Reasoning improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01457</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of Adversarial Training on Malware Classifiers</title><link>https://arxiv.org/abs/2412.18218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rubik, a systematic multi-dimensional framework to evaluate adversarial training (AT) for malware classifiers across data, feature representations, model architectures, and robust optimization settings.&lt;/li&gt;&lt;li&gt;Implements Rubik on Android malware and runs realistic evasion attacks to empirically study how interactions among factors affect AT robustness.&lt;/li&gt;&lt;li&gt;Reports that commonly held beliefs about AT (e.g., realizable adversarial examples always help) are conditional, highlights the critical role of model architecture and feature-space structure, and provides actionable insights, misconceptions, and recommendations for robust malware detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamid Bostani', 'Jacopo Cortellazzi', 'Daniel Arp', 'Fabio Pierazzi', 'Veelasha Moonsamy', 'Lorenzo Cavallaro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'malware classification', 'adversarial examples', 'robustness evaluation', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18218</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks</title><link>https://arxiv.org/abs/2407.08806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a parametric variation of the fast minimum-norm adversarial attack allowing dynamic adjustment of loss, optimizer, step-size scheduler, and hyperparameters.&lt;/li&gt;&lt;li&gt;Applies hyperparameter optimization to find smaller adversarial perturbations efficiently, without manual tuning.&lt;/li&gt;&lt;li&gt;Re-evaluates 12 robust models, shows improved attack strength and reports robustness as a function of perturbation budget; open-source code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffaele Mura', 'Giuseppe Floris', 'Luca Scionis', 'Giorgio Piras', 'Maura Pintor', 'Ambra Demontis', 'Giorgio Giacinto', 'Battista Biggio', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness evaluation', 'hyperparameter optimization', 'minimum-norm attack', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.08806</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness</title><link>https://arxiv.org/abs/2406.19622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cost-efficient defense that enforces/data-driven Lipschitz continuity to improve adversarial robustness of DNNs without requiring extra generative data.&lt;/li&gt;&lt;li&gt;Method requires a single pass over the dataset and avoids gradient estimation, reducing computational overhead compared to conventional adversarial training.&lt;/li&gt;&lt;li&gt;Claims compatibility with existing adversarial training frameworks and reports comparable or improved robustness in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erh-Chung Chen', 'Pin-Yu Chen', 'I-Hsin Chung', 'Che-Rung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'Lipschitz continuity', 'adversarial training', 'defense methods', 'computational efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.19622</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</title><link>https://arxiv.org/abs/2310.00098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first benchmark and practical recipe for federated learning (FL) with differential privacy (DP) in end-to-end automatic speech recognition (ASR).&lt;/li&gt;&lt;li&gt;Proposes per-layer clipping and layer-wise gradient normalization; provides theoretical analysis showing these reduce clipping bias and mitigate gradient heterogeneity in deep transformer models.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate viable FL+DP under strong user-level privacy (e.g., (7.2,1e-9)-DP) with small word error rate (WER) degradation at large population scales; code and benchmarks released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Pelikan', 'Sheikh Shams Azam', 'Vitaly Feldman', 'Jan "Honza" Silovsky', 'Kunal Talwar', 'Christopher G. Brinton', 'Tatiana Likhomanenko']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'federated-learning', 'speech-recognition', 'gradient-clipping', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00098</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data</title><link>https://arxiv.org/abs/2511.21600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TAB-DRW, a DFT-based post-editing watermarking scheme for synthetic tabular data that embeds signals in the frequency domain after Yeo–Johnson transformation and standardization.&lt;/li&gt;&lt;li&gt;Adjusts imaginary parts of selected DFT entries per pseudorandom bits and introduces a rank-based pseudorandom bit generation enabling row-wise retrieval without storage overhead.&lt;/li&gt;&lt;li&gt;Evaluated on five benchmark tabular datasets, claiming strong detectability and robustness to common post-processing attacks while preserving data fidelity and supporting mixed discrete–continuous features.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhou Zhao', 'Xiang Li', 'Peter Song', 'Qi Long', 'Weijie Su']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'data provenance', 'robustness', 'synthetic data', 'post-processing attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21600</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis</title><link>https://arxiv.org/abs/2511.21397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Idis, a VQA dataset that systematically varies visual distractors across semantic, numerical, and spatial dimensions to study their effect on test-time scaling in VLMs.&lt;/li&gt;&lt;li&gt;Finds that visual distractors produce an inverse scaling effect (accuracy drops) but, unlike textual distractors, do not increase reasoning trace length.&lt;/li&gt;&lt;li&gt;Shows that tracking attribute counts in reasoning traces helps explain interactions between distractors, reasoning length, and accuracy, and that these trends extend to benchmarks with visual biases (e.g., Waterbirds).&lt;/li&gt;&lt;li&gt;Proposes a simple prompting strategy that mitigates bias-driven predictions in reasoning models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyun Bae', 'Hyunjong Ok', 'Sangwoo Mo', 'Jaeho Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-models', 'evaluation/benchmarking', 'bias', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21397</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels</title><link>https://arxiv.org/abs/2511.21038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes whether few-shot in-context learning (ICL) can override pre-trained label semantics by comparing natural vs inverted demonstrations across tasks and 1–12B open-source LLMs.&lt;/li&gt;&lt;li&gt;Introduces three alignment metrics (truth, prior, prompt alignment) and a semantic override rate to quantify whether models adopt flipped label meanings.&lt;/li&gt;&lt;li&gt;Finds consistent evidence for a 'semantic anchor' view: ICL mostly refines pre-trained semantic directions rather than remapping label meanings; semantic override rates are effectively zero in the evaluated few-shot setting.&lt;/li&gt;&lt;li&gt;Concludes that flipping label semantics at these model scales requires interventions beyond standard few-shot prompting, with implications for limits of prompt-based manipulation and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anantha Padmanaban Krishna Kumar (Boston University)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'in-context learning', 'robustness', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21038</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Readout-Side Bypass for Residual Hybrid Quantum-Classical Models</title><link>https://arxiv.org/abs/2511.20922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a residual hybrid quantum-classical architecture that concatenates raw inputs with quantum readouts to bypass the quantum-to-classical measurement bottleneck.&lt;/li&gt;&lt;li&gt;Reports empirical improvements over pure quantum and prior hybrid models (centralized and federated settings), claiming up to +55% accuracy gains while maintaining low communication cost.&lt;/li&gt;&lt;li&gt;Claims enhanced privacy robustness in federated/edge scenarios and presents ablation studies demonstrating the effectiveness of the residual connection at the quantum-classical interface.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guilin Zhang', 'Wulan Guo', 'Ziqi Tan', 'Hongyang He', 'Hailong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['Quantum ML', 'Privacy / Privacy robustness', 'Federated learning', 'Hybrid model architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20922</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation</title><link>https://arxiv.org/abs/2511.20889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Null-Text Test-Time Alignment (Null-TTA): optimises the unconditional (null) text embedding used in classifier-free guidance at inference to align text-to-image diffusion models to target rewards.&lt;/li&gt;&lt;li&gt;Argues semantic-space optimisation prevents reward hacking by constraining changes to a semantically coherent manifold, unlike latent/noise manipulation.&lt;/li&gt;&lt;li&gt;Steers the model's generative distribution toward the reward without updating model weights, achieving strong target alignment and cross-reward generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taehoon Kim', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'reward hacking', 'diffusion models', 'classifier-free guidance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20889</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models</title><link>https://arxiv.org/abs/2511.20799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multi-prefix memorization framework: a sequence is memorized if many distinct prefixes can elicit it, shifting from single-path extraction to robustness-of-retrieval.&lt;/li&gt;&lt;li&gt;Formalizes memorization via an adversarial search for a target count of distinct prefixes and uses this count to distinguish memorized vs non-memorized content.&lt;/li&gt;&lt;li&gt;Empirically evaluates on open-source and aligned chat models, showing the method reliably detects training-data leakage and is practical for auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung Cuong Dang', 'David Mohaisen']&lt;/li&gt;&lt;li&gt;Tags: ['training-data-memorization', 'privacy/data-leakage-detection', 'model-auditing', 'adversarial-extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20799</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach</title><link>https://arxiv.org/abs/2511.20703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PropensityBench, a benchmark framework that measures LLMs' latent propensity to pursue risky actions when given simulated high-risk capabilities via agentic proxy tools.&lt;/li&gt;&lt;li&gt;Contains 5,874 scenarios and 6,648 tools across four high-risk domains (cybersecurity, self-proliferation, biosecurity, chemical security) and varies operational pressures to simulate real-world incentives.&lt;/li&gt;&lt;li&gt;Evaluates open-source and proprietary frontier models, finding multiple alarming signs that models choose high-risk tools under pressure despite lacking unaided capability, arguing for dynamic propensity assessments prior to deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udari Madhushani Sehwag', 'Shayan Shabihi', 'Alex McAvoy', 'Vikash Sehwag', 'Yuancheng Xu', 'Dalton Towers', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'red-teaming', 'agentic-assessment', 'biosecurity', 'cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20703</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI</title><link>https://arxiv.org/abs/2511.20686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a taxonomy of 35 AI risk factors adapted for Korean socio-cultural context to capture both universal and localized harms.&lt;/li&gt;&lt;li&gt;Constructs AssurAI, a quality-controlled Korean multimodal safety dataset of 11,480 instances spanning text, image, video, and audio.&lt;/li&gt;&lt;li&gt;Implements rigorous data curation: expert-led seeding, crowdsourced scaling, triple independent annotation, and an iterative expert red-teaming loop.&lt;/li&gt;&lt;li&gt;Publishes a pilot evaluation demonstrating AssurAI's effectiveness for assessing safety of recent LLMs and releases the dataset publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chae-Gyun Lim', 'Seung-Ho Han', 'EunYoung Byun', 'Jeongyun Han', 'Soohyun Cho', 'Eojin Joo', 'Heehyeon Kim', 'Sieun Kim', 'Juhoon Lee', 'Hyunsoo Lee', 'Dongkun Lee', 'Jonghwan Hyeon', 'Yechan Hwang', 'Young-Jun Lee', 'Kyeongryul Lee', 'Minhyeong An', 'Hyunjun Ahn', 'Jeongwoo Son', 'Junho Park', 'Donggyu Yoon', 'Taehyung Kim', 'Jeemin Kim', 'Dasom Choi', 'Kwangyoung Lee', 'Hyunseung Lim', 'Yeohyun Jung', 'Jongok Hong', 'Sooyohn Nam', 'Joonyoung Park', 'Sungmin Na', 'Yubin Choi', 'Jeanne Choi', 'Yoojin Hong', 'Sueun Jang', 'Youngseok Seo', 'Somin Park', 'Seoungung Jo', 'Wonhye Chae', 'Yeeun Jo', 'Eunyoung Kim', 'Joyce Jiyoung Whang', 'HwaJung Hong', 'Joseph Seering', 'Uichin Lee', 'Juho Kim', 'Sunna Choi', 'Seokyeon Ko', 'Taeho Kim', 'Kyunghoon Kim', 'Myungsik Ha', 'So Jung Lee', 'Jemin Hwang', 'JoonHo Kwak', 'Ho-Jin Choi']&lt;/li&gt;&lt;li&gt;Tags: ['safety-dataset', 'multimodal', 'red-teaming', 'non-English', 'socio-cultural-risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20686</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Domain-Grounded Evaluation of LLMs in International Student Knowledge</title><link>https://arxiv.org/abs/2511.20653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Creates a domain-grounded benchmark of realistic study-abroad advising questions (from ApplyBoard) to evaluate LLM advice quality on admissions, visas, scholarships, and eligibility.&lt;/li&gt;&lt;li&gt;Evaluates models on accuracy (correct/partial/wrong) and hallucination by using a domain-coverage-aware rubric that captures under-coverage, over-scoping, and unsupported claims.&lt;/li&gt;&lt;li&gt;Reports faithfulness, answer relevance, and an aggregate hallucination score for head-to-head model comparisons.&lt;/li&gt;&lt;li&gt;Provides a practical, reusable protocol for auditing LLMs before deployment in education/advising contexts, and surfaces common failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Claudinei Daitx', 'Haitham Amar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness', 'LLM evaluation', 'safety-audit', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20653</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>EvilGenie: A Reward Hacking Benchmark</title><link>https://arxiv.org/abs/2511.21654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EvilGenie, a benchmark/environment for measuring reward hacking in code-generation/agent settings (e.g., hardcoding test cases, editing test files).&lt;/li&gt;&lt;li&gt;Evaluates reward hacking via three methods: held-out unit tests, LLM judges, and detection of edits to test files; validates these against human review and each other.&lt;/li&gt;&lt;li&gt;Finds LLM judges effective at detecting unambiguous reward hacking and that held-out tests provide only minimal additional protection; reports explicit reward hacking and misaligned behaviors in several popular coding agents.&lt;/li&gt;&lt;li&gt;Provides empirical measurements across many models (including Codex, Claude Code, Gemini CLI) and releases the codebase.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Gabor', 'Jayson Lynch', 'Jonathan Rosenfeld']&lt;/li&gt;&lt;li&gt;Tags: ['reward_hacking', 'alignment', 'benchmarking', 'LLM_red_teaming', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21654</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title><link>https://arxiv.org/abs/2511.21638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reduces the multi-turn conversational RL problem to a sequence of single-turn RLHF-style problems by using a learned multi-turn Q-function as the single-turn reward.&lt;/li&gt;&lt;li&gt;Proves that solving the single-turn problem with token-level PPO is equivalent to a policy improvement step within the multi-turn problem.&lt;/li&gt;&lt;li&gt;Introduces Iterative PPO: a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy.&lt;/li&gt;&lt;li&gt;Emphasizes practical benefits: leverages stable, off-the-shelf single-turn RLHF tools and occupies a middle ground between fully online and fully offline training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel R. Jiang', 'Jalaj Bhandari', 'Yukai Yang', "R\\'emi Munos", 'Tyler Lu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'RLHF', 'multi-turn dialogue', 'policy optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21638</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids</title><link>https://arxiv.org/abs/2511.21590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cloud-deployed, ML-based digital forensic framework for smart grids combining sensor-level data acquisition, authenticated communication, scalable cloud storage, and automated forensic analytics.&lt;/li&gt;&lt;li&gt;Employs supervised and unsupervised algorithms (Random Forest, SVM, Gradient Boosted Trees, deep neural networks) for real-time anomaly detection, event reconstruction, and intrusion analysis.&lt;/li&gt;&lt;li&gt;Evaluated on simulations and real-time smart-meter data streams; claims high accuracy, scalability, and resilience to cyber-attacks such as data tampering, false-data injection, and coordinated control-loop manipulation.&lt;/li&gt;&lt;li&gt;Argues cloud services enable big-data-driven forensic workflows for fast situational awareness and intelligent incident response in energy utilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Siddique', 'Sohaib Zafar']&lt;/li&gt;&lt;li&gt;Tags: ['smart-grid-security', 'cyber-physical-systems', 'intrusion-detection', 'anomaly-detection', 'digital-forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21590</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Computing Strategic Responses to Non-Linear Classifiers</title><link>https://arxiv.org/abs/2511.21560</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses strategic classification where deploying a classifier induces strategic behavior and distribution shift.&lt;/li&gt;&lt;li&gt;Introduces a novel method to compute agents' best responses by optimizing the Lagrangian dual of the agents' objective.&lt;/li&gt;&lt;li&gt;Method reproduces known results in linear settings, exposes weaknesses in prior approaches, and extends to non-linear classifiers for evaluation and training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Geary', 'Boyan Gao', 'Henry Gouk']&lt;/li&gt;&lt;li&gt;Tags: ['strategic-classification', 'adversarial-robustness', 'distribution-shift', 'non-linear-classifiers', 'game-theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21560</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Predictive Safety Shield for Dyna-Q Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a predictive safety shield for model-based, discrete-space reinforcement learning that uses short-horizon safe simulations to evaluate future consequences of safe actions.&lt;/li&gt;&lt;li&gt;The shield locally updates the Q-function based on safe predictions, aiming to maintain hard safety guarantees while improving task performance compared to random safe-action selection or fixed fallbacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation on gridworlds shows short prediction horizons can suffice to find optimal paths and that the method is robust to distribution shifts (e.g., sim-to-real) without extra training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Pin', 'Krasowski Hanna', 'Vanneaux Elena']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning safety', 'safety shield', 'model-based RL', 'safety guarantees', 'robustness / sim-to-real']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21531</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data</title><link>https://arxiv.org/abs/2511.21378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive and Aggressive Rejection (AAR) to handle contaminated training data in anomaly detection by dynamically excluding anomalies.&lt;/li&gt;&lt;li&gt;Uses a modified z-score and Gaussian Mixture Model (GMM)-based thresholds, combining hard and soft rejection strategies to balance preserving normal data vs excluding anomalies.&lt;/li&gt;&lt;li&gt;Evaluated on two image datasets and thirty tabular datasets, reporting an average improvement of 0.041 AUROC over the prior state-of-the-art.&lt;/li&gt;&lt;li&gt;Claims scalability and improved robustness for real-world applications (mentions domains such as security and healthcare).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungi Lee', 'Jungkwon Kim', 'Chi Zhang', 'Kwangsun Yoo', 'Seok-Joo Byun']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'contaminated-training-data', 'robustness', 'outlier-rejection', 'data-poisoning (defensive)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21378</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy in Federated Learning with Spiking Neural Networks</title><link>https://arxiv.org/abs/2511.21181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study/benchmark of gradient inversion (gradient leakage) attacks against spiking neural networks (SNNs) in federated learning.&lt;/li&gt;&lt;li&gt;Adapts existing gradient leakage attacks to the spike domain and evaluates across diverse (spatial and temporal) data domains.&lt;/li&gt;&lt;li&gt;Finds SNN gradients produce noisy, temporally inconsistent reconstructions compared to ANNs, suggesting surrogate-gradient training and event-driven dynamics reduce gradient informativeness and improve privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dogukan Aksu', 'Jesus Martinez del Rincon', 'Ihsen Alouani']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'gradient inversion', 'spiking neural networks', 'adversarial/privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21181</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination</title><link>https://arxiv.org/abs/2511.21118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a compositional architecture for federated learning that adds verifiability, incentive alignment, scalability, and governance protections at edge scale.&lt;/li&gt;&lt;li&gt;Uses cryptographic receipts to prove correct aggregation, geometric novelty metrics to deter incentive gaming, and parallel object ownership to enable linear scalability.&lt;/li&gt;&lt;li&gt;Introduces time-locked policies to prevent retroactive manipulation of coordination/governance decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pius Onobhayedo', 'Paul Osemudiame Oamen']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'verifiable-aggregation', 'incentive-alignment', 'anti-gaming', 'decentralized-governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21118</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs</title><link>https://arxiv.org/abs/2511.21056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a unified optimization framework combining offline bilevel data selection (w.r.t. validation data) and online self-refining generation treated as model-selection/adaptation.&lt;/li&gt;&lt;li&gt;Assigns learned weights to each question-response pair (explicitly or implicitly) to improve data quality for post-training LLM adaptation.&lt;/li&gt;&lt;li&gt;Provides theoretical results on the effectiveness of bilevel data selection and empirical gains over unfiltered mixing baselines.&lt;/li&gt;&lt;li&gt;Demonstrates improved fine-tuning performance including experiments on quality enhancement and safety-aware LLM fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quan Xiao', 'Tianyi Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'data selection', 'alignment/safety', 'online self-refinement', 'bilevel optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21056</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the safety-capability tradeoff in LLM fine-tuning and evaluates Reinforcement Learning with Verifiable Rewards (RLVR) as an alternative to SFT/RLHF for maintaining safety while improving capabilities.&lt;/li&gt;&lt;li&gt;Provides theoretical results: upper bounds on safety drift under KL-constrained optimization and conditions where safety degradation is eliminated.&lt;/li&gt;&lt;li&gt;Presents empirical evaluations across five adversarial safety benchmarks and ablation studies (optimization algorithms, model scale, task domains) showing RLVR can improve reasoning without degrading — and sometimes improving — safety guardrails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongkyu Derek Cho', 'Huan Song', 'Arijit Ghosh Chowdhury', 'Haotian An', 'Yawei Wang', 'Rohit Thekkanal', 'Negin Sokhandan', 'Sharlina Keshava', 'Hannah Marlowe']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'reinforcement learning', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21050</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ChatGpt Content detection: A new approach using xlm-roberta alignment</title><link>https://arxiv.org/abs/2511.21009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for detecting fully AI-generated and AI-rewritten human text by fine-tuning XLM-RoBERTa on a balanced dataset.&lt;/li&gt;&lt;li&gt;Combines preprocessing and feature extraction including perplexity, semantic, readability, and attention-based features alongside the transformer model.&lt;/li&gt;&lt;li&gt;Reports high accuracy and robust performance across multiple text genres and includes feature analysis showing perplexity and attention features are influential.&lt;/li&gt;&lt;li&gt;Suggests future work expanding datasets and evaluating other advanced models to improve generalizability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Tasnin Tanvir', 'Dr Santanu Kumar Dash', 'Ishan Shahnan', 'Nafis Fuad', 'Tanvir Rahman', 'Abdullah Al Faisal', 'Asadullah Al Mamun']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-text detection', 'forensics', 'XLM-RoBERTa', 'content attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21009</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Dataset Poisoning Attacks on Behavioral Cloning Policies</title><link>https://arxiv.org/abs/2511.20992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of clean-label dataset backdoor/poisoning attacks against behavior cloning (BC) policies by injecting visual triggers into demonstrations to create spurious correlations.&lt;/li&gt;&lt;li&gt;Evaluates how vulnerability scales with poisoning fraction, trigger strength, and trigger type, and shows minimally poisoned datasets can yield high apparent task performance while remaining highly vulnerable at test time.&lt;/li&gt;&lt;li&gt;Proposes a novel entropy-based test-time trigger attack that identifies critical states where triggering the backdoor most effectively degrades policy performance.&lt;/li&gt;&lt;li&gt;Empirically demonstrates significant risks to BC policies trained on large-scale/collected demonstration datasets and calls for more robustness research for real-world cyber-physical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akansha Kalra', 'Soumil Datta', 'Ethan Gilmore', 'Duc La', 'Guanhong Tao', 'Daniel S. Brown']&lt;/li&gt;&lt;li&gt;Tags: ['dataset-poisoning', 'backdoor-attacks', 'behavioral-cloning', 'robustness', 'adversarial-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20992</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</title><link>https://arxiv.org/abs/2511.20944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares two BEC detection paradigms: a Forensic Psycholinguistic Stream using CatBoost and a Semantic Stream using DistilBERT, focusing on accuracy, interpretability, latency, and deployment cost.&lt;/li&gt;&lt;li&gt;Evaluates models on an adversarially poisoned dataset (N=7,990) generated via a 'Black Hole' protocol, reporting high robustness and near-perfect metrics for DistilBERT and competitive performance for CatBoost.&lt;/li&gt;&lt;li&gt;Benchmarks latency on Tesla T4 GPU and reports operational trade-offs (DistilBERT superior accuracy at higher compute cost; CatBoost much lower latency and compute for edge/cost-sensitive use), plus ROI/cost-sensitive learning analysis.&lt;/li&gt;&lt;li&gt;Addresses adversarial/data-poisoning issues and practical red-team-style evaluation of detection robustness under poisoning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaw Osei Adjei (Kwame Nkrumah University of Science', 'Technology)']&lt;/li&gt;&lt;li&gt;Tags: ['data-poisoning', 'adversarial-robustness', 'phishing-detection', 'model-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20944</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning</title><link>https://arxiv.org/abs/2511.20839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Primal, a deterministic feature-mapping framework using number-theoretic properties (prime square roots and Besicovitch property) to produce quasi-orthogonal, non-repeating phase trajectories.&lt;/li&gt;&lt;li&gt;Proposes two algorithmic variants: StaticPrime (sequence generation approaching Welch bound) and DynamicPrime (input-dependent tunable projection with a scale parameter σ).&lt;/li&gt;&lt;li&gt;Shows dual behavior: low-frequency regime acts as an isometric kernel map for manifold linearization and compressive sensing; high-frequency regime yields chaotic phase wrapping usable as a maximum-entropy one-way hash for Hyperdimensional Computing and privacy-preserving Split Learning.&lt;/li&gt;&lt;li&gt;Empirical results claim superior orthogonality retention and distribution tightness versus normalized Gaussian random projections; code is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vladimer Khasia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'hashing', 'feature-mapping', 'manifold-learning', 'split-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20839</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning</title><link>https://arxiv.org/abs/2511.20811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a data-driven runtime safety monitor for flight testing that predicts short-term risk from recent observations and offline stochastic trajectory simulations.&lt;/li&gt;&lt;li&gt;Combines a state-prediction model, a nearest-neighbor safety classifier on predicted states, and classifier calibration using conformal prediction to provide calibrated preemptive abort criteria.&lt;/li&gt;&lt;li&gt;Evaluates on a flight dynamics model with uncertain parameters, showing reliable identification of unsafe scenarios and improved preemptive risk classification versus baselines while matching theoretical guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (case study)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron O. Feldman', 'D. Isaiah Harp', 'Joseph Duncan', 'Mac Schwager']&lt;/li&gt;&lt;li&gt;Tags: ['runtime safety monitoring', 'conformal prediction / calibration', 'safety-critical systems', 'uncertainty quantification', 'flight testing / control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20811</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge</title><link>https://arxiv.org/abs/2511.20726</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Combines a CVAE trained on trajectories and maps to produce physically consistent base scenarios for driving simulations.&lt;/li&gt;&lt;li&gt;Uses an LLM as an adversarial reasoning engine to convert unstructured scene descriptions into domain-specific loss functions that steer scenario generation toward varying risk levels.&lt;/li&gt;&lt;li&gt;Balances realism and controllability to increase coverage of high-risk, long-tail events and expose autonomous driving systems to more challenging interactions in simulation (CARLA, SMARTS).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Heye Huang', 'Zhenhua Xu', 'Kailai Sun', 'Baoshen Guo', 'Jinhua Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'scenario-generation', 'LLM-adversarial-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20726</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Active Slice Discovery in Large Language Models</title><link>https://arxiv.org/abs/2511.20713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Active Slice Discovery: an active-learning approach to group model errors into coherent error slices using limited annotator queries.&lt;/li&gt;&lt;li&gt;Empirical study on toxicity classification: compares feature representations and active learning strategies for discovering human-defined slices (e.g., demographic-specific errors).&lt;/li&gt;&lt;li&gt;Finds uncertainty-based active learning often most effective, reaching competitive slice-detection accuracy with only 2–10% of slice membership labels and outperforming baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minhui Zhang', 'Prahar Ijner', 'Yoav Wald', 'Elliot Creager']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'model-robustness', 'fairness', 'active-learning', 'toxicity-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20713</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</title><link>https://arxiv.org/abs/2511.19413</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniGame, a self-adversarial post-training framework that adds a lightweight perturber at the shared token interface so the generation branch actively challenges fragile understanding.&lt;/li&gt;&lt;li&gt;Aims to resolve representation trade-offs in unified multimodal models to improve cross-modal consistency, robustness to distributional shifts, and adversarial resilience.&lt;/li&gt;&lt;li&gt;Reports empirical gains (e.g., consistency +4.6%, understanding +3.6%, OOD +4.8% on NaturalBench, adversarial robustness +6.2% on AdVQA), is architecture-agnostic, adds &lt;1% parameters, and is complementary to existing post-training methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolong Su', 'Wang Lu', 'Hao Chen', 'Sharon Li', 'Jindong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'self-adversarial / red teaming', 'multimodal models', 'post-training interventions', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19413</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization</title><link>https://arxiv.org/abs/2511.19218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ACE-Safety, a framework that jointly optimizes attack and defense models for LLM safety via co-evolution.&lt;/li&gt;&lt;li&gt;Introduces GS-MCTS (Group-aware Strategy-guided Monte Carlo Tree Search) to efficiently explore jailbreak strategies and generate diverse adversarial samples.&lt;/li&gt;&lt;li&gt;Introduces AC-TGPO (Adversarial Curriculum Tree-aware Group Policy Optimization) to jointly train attack and defense LLMs using curriculum reinforcement learning for robust mutual improvement.&lt;/li&gt;&lt;li&gt;Reports evaluations across multiple benchmarks showing improved attack generation and defense robustness over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurui Li', 'Kaisong Song', 'Rui Zhu', 'Pin-Yu Chen', 'Haixu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial training', 'safety alignment', 'curriculum reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19218</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection</title><link>https://arxiv.org/abs/2511.18780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents ConceptGuard, a two-stage framework for proactively detecting and mitigating unsafe semantics in text-and-image-to-video generation.&lt;/li&gt;&lt;li&gt;Stage 1: contrastive detection projects fused image-text inputs into a structured concept space to identify latent multimodal risks.&lt;/li&gt;&lt;li&gt;Stage 2: semantic suppression intervenes in multimodal prompt conditioning to steer generation away from unsafe concepts.&lt;/li&gt;&lt;li&gt;Introduces two benchmarks (ConceptRisk and T2VSafetyBench-TI2V) and shows state-of-the-art results in risk detection and safe video generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruize Ma', 'Minghong Cai', 'Yilei Jiang', 'Jiaming Han', 'Yi Feng', 'Yingshui Tan', 'Xiaoyong Zhu', 'Bo Zhang', 'Bo Zheng', 'Xiangyu Yue']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'video generation', 'content filtering/mitigation', 'risk detection', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18780</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title><link>https://arxiv.org/abs/2511.14386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first texture-enabled physical adversarial example (PAE) specifically targeting stereo binocular depth estimation for autonomous driving.&lt;/li&gt;&lt;li&gt;Uses a 3D PAE with a global camouflage texture to maintain visual consistency and attack effectiveness across different stereo viewpoints.&lt;/li&gt;&lt;li&gt;Introduces a 3D stereo matching rendering module to align PAEs with real-world positions/headings in binocular vision, and a merging attack to seamlessly blend the PAE into the environment.&lt;/li&gt;&lt;li&gt;Evaluations demonstrate the PAEs can successfully induce erroneous depth outputs from stereo models, highlighting a perceptual security vulnerability in autonomous driving systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangqiao Zhao', 'Shuo Huai', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-examples', 'stereo-depth-estimation', 'autonomous-driving', 'adversarial-robustness', 'perception-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14386</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title><link>https://arxiv.org/abs/2511.10234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how LLM-based graph reasoners are sensitive to graph serialization choices (node labeling, edge encoding, syntax) and how that affects output invariance under permutations and formatting changes.&lt;/li&gt;&lt;li&gt;Evaluates how fine-tuning modifies sensitivity: larger pre-trained models show more robustness, while fine-tuning reduces relabeling sensitivity but can increase sensitivity to structure/format variations and does not reliably improve out-of-distribution generalization.&lt;/li&gt;&lt;li&gt;Introduces a decomposition of serialization factors and a benchmarking suite (including novel spectral tasks) to systematically measure robustness and generalization of LLM graph reasoners.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Herbst', 'Lea Karbevska', 'Divyanshu Kumar', 'Akanksha Ahuja', 'Fatemeh Gholamzadeh Nasrabadi', 'Fabrizio Frasca']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'LLM evaluation', 'graph reasoning', 'invariance', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10234</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Weak-to-Strong Generalization under Distribution Shifts</title><link>https://arxiv.org/abs/2510.21332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies failure modes of naive weak-to-strong supervision under distribution shift, where a strong model can perform worse than its weak supervisors.&lt;/li&gt;&lt;li&gt;Proposes RAVEN, a method that jointly learns strong-model parameters and dynamic weighting/combinations of weak supervisors to improve robustness.&lt;/li&gt;&lt;li&gt;Evaluates RAVEN across image classification, text classification, and preference alignment tasks, showing large OOD improvements while matching in-distribution performance.&lt;/li&gt;&lt;li&gt;Shows RAVEN can assign higher weights to more accurate weak models, effectively identifying more trustworthy supervision sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Myeongho Jeon', 'Jan Sobotka', 'Suhwan Choi', "Maria Brbi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'weak-to-strong generalization', 'distribution shift', 'supervision trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21332</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset</title><link>https://arxiv.org/abs/2510.01219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a dataset of concept learning tasks designed to reveal implicit biases in large language models via in-context learning experiments.&lt;/li&gt;&lt;li&gt;Finds a bias toward upward monotonicity in quantifiers when models perform concept learning in-context, a bias less apparent with direct prompting.&lt;/li&gt;&lt;li&gt;Demonstrates that in-context concept learning can be an effective method for uncovering hidden behavioral biases in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leroy Z. Wang']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'alignment', 'safety-evaluation', 'dataset', 'in-context-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01219</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Special-Character Adversarial Attacks on Open-Source Language Model</title><link>https://arxiv.org/abs/2508.14070</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates character-level adversarial attacks (unicode, homoglyph, structural, textual encoding) against open-source LLMs to bypass safety mechanisms.&lt;/li&gt;&lt;li&gt;Evaluates seven open-source models (3.8B–32B) over 4,000+ attack attempts, reporting successful jailbreaks, incoherent outputs, and unrelated hallucinations.&lt;/li&gt;&lt;li&gt;Identifies critical vulnerabilities across model sizes and failure modes that undermine content-filtering and safety defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ephraiem Sarabamoun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'input-encoding attacks', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14070</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</title><link>https://arxiv.org/abs/2508.12398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First safety analysis of diffusion-based LLMs (dLLMs), identifying that middle tokens are more critical to output safety than initial tokens.&lt;/li&gt;&lt;li&gt;Finds an attacker–defender asymmetry: attackers struggle to manipulate middle tokens due to a practical sequential generation tendency in dLLMs.&lt;/li&gt;&lt;li&gt;Proposes MOSA (Middle-tOken Safety Alignment), using reinforcement learning to align middle-token generation toward safe refusals.&lt;/li&gt;&lt;li&gt;Evaluates MOSA against eight attack methods on two benchmarks and measures utility on coding, math, and reasoning tasks, showing superior security performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhixin Xie', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Alignment', 'Adversarial prompting / Red teaming', 'Reinforcement learning for safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12398</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title><link>https://arxiv.org/abs/2506.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IVY-FAKE, a large-scale multimodal (image + video) benchmark with &gt;106K annotated training samples and 5K manually verified evaluation examples for explainable AIGC detection.&lt;/li&gt;&lt;li&gt;Identifies limitations of existing datasets (binary labels, limited explainability) and MLLM-based detectors' reasoning, and provides richer, multidimensional annotations for localization and interpretability.&lt;/li&gt;&lt;li&gt;Proposes Ivy-xDetector, an RL-based detector using Group Relative Policy Optimization (GRPO) that outputs explainable reasoning chains and shows substantial performance gains over prior methods on multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Wenhui Dong', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng', 'Xinbin Yuan', 'Yifei Bi', 'Ming Zhao', 'Zian Zhou', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'forgery detection', 'explainability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00979</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title><link>https://arxiv.org/abs/2501.01457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRR (Distillation-Reinforcement-Reasoning), an externalist three-step framework that builds a lightweight Discriminative Model (DM) to critique LLM behavioral traces rather than relying on the model's self-introspection.&lt;/li&gt;&lt;li&gt;DM is trained on distilled behavioral traces to identify and reject suspicious reasoning steps at inference, guiding the base LLM to explore alternative, less-flawed reasoning paths without modifying the base model.&lt;/li&gt;&lt;li&gt;Claims significant improvements over prominent self-critique methods across multiple reasoning benchmarks, with a lightweight, annotation-free design intended to be scalable and adaptable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diji Yang', 'Linda Zeng', 'Kezhen Chen', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Alignment &amp; robustness', 'External critique / discriminative critic', 'Reasoning improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01457</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-PA: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models</title><link>https://arxiv.org/abs/2412.19496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Multi-PA, a comprehensive benchmark to evaluate privacy awareness and privacy leakage of Large Vision-Language Models (LVLMs).&lt;/li&gt;&lt;li&gt;Covers extensive categories: 26 personal privacy, 15 trade secrets, 18 state secrets, totaling 31,962 samples and multiple sub-tasks to probe different privacy aspects.&lt;/li&gt;&lt;li&gt;Evaluates 21 open-source and 2 closed-source LVLMs, finding widespread risks of privacy breaches with variation across privacy categories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Xiangkui Cao', 'Zhouyu Han', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'vision-language models', 'privacy-benchmarking', 'privacy-leakage', 'model-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.19496</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Gray-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</title><link>https://arxiv.org/abs/2408.10901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Posterior Collapse Attack (PCA), a gray-box adversarial defense that induces posterior collapse in the VAE of Latent Diffusion Models to prevent unauthorized image editing.&lt;/li&gt;&lt;li&gt;Identifies two collapse modes—diffusion collapse and concentration collapse—and designs a unified loss to flexibly induce either mode for different protection goals.&lt;/li&gt;&lt;li&gt;Requires only access to the VAE encoder (&lt;4% of LDM parameters), is prompt-invariant (operates before text conditioning), and claims strong transferability across VAE-based LDMs with lower compute and memory costs.&lt;/li&gt;&lt;li&gt;Shows theoretical analysis and empirical results demonstrating improved protection effectiveness, runtime/VRAM efficiency, and generalization compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongliang Guo', 'Chun Tong Lei', 'Lei Fang', 'Shuai Zhao', 'Yifei Qian', 'Jingyu Lin', 'Zeyu Wang', 'Cunjian Chen', "Ognjen Arandjelovi\\'c", 'Chun Pong Lau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'latent diffusion models', 'VAE posterior collapse', 'image protection', 'model robustness/security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10901</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness</title><link>https://arxiv.org/abs/2406.19622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cost-efficient defense that enforces/data-driven Lipschitz continuity to improve adversarial robustness of DNNs without requiring extra generative data.&lt;/li&gt;&lt;li&gt;Method requires a single pass over the dataset and avoids gradient estimation, reducing computational overhead compared to conventional adversarial training.&lt;/li&gt;&lt;li&gt;Claims compatibility with existing adversarial training frameworks and reports comparable or improved robustness in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erh-Chung Chen', 'Pin-Yu Chen', 'I-Hsin Chung', 'Che-Rung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'Lipschitz continuity', 'adversarial training', 'defense methods', 'computational efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.19622</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs</title><link>https://arxiv.org/abs/2405.17846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes integrating LLMs with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to improve safety in service robots.&lt;/li&gt;&lt;li&gt;ERCPs constrain LLM outputs to generate safe, precise instructions while EKGs validate and ensure actions align with safety protocols.&lt;/li&gt;&lt;li&gt;Experimental evaluation on diverse real-world tasks reports higher compliance with safety standards compared to traditional methods.&lt;/li&gt;&lt;li&gt;Targets safer human-robot interaction by combining prompt-level controls with knowledge-graph based validation for embodied agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Qi', 'Gabriel Kyebambo', 'Siyuan Xie', 'Wei Shen', 'Shenghui Wang', 'Bitao Xie', 'Bin He', 'Zhipeng Wang', 'Shuo Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Robotics safety', 'LLM safety', 'Knowledge graphs', 'Embodied AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.17846</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic</title><link>https://arxiv.org/abs/2511.20586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PaTAS, a parallel trust-propagation framework using Subjective Logic that runs alongside neural computation via Trust Nodes and Trust Functions.&lt;/li&gt;&lt;li&gt;Introduces a Parameter Trust Update mechanism for training-time parameter reliability and an Inference-Path Trust Assessment (IPTA) to compute instance-specific trust at inference.&lt;/li&gt;&lt;li&gt;Evaluates on real-world and adversarial/poisoned/biased datasets, showing PaTAS yields interpretable trust estimates that expose reliability gaps and distinguish adversarial from benign inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Koffi Ismael Ouattara', 'Ioannis Krontiris', 'Theo Dimitrakos', 'Dennis Eisermann', 'Frank Kargl']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'trustworthiness', 'adversarial detection', 'data poisoning detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20586</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Failure Modes in LLM Systems: A System-Level Taxonomy for Reliable AI Applications</title><link>https://arxiv.org/abs/2511.19933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a system-level taxonomy of 15 hidden failure modes in real-world LLM applications (e.g., multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, cost-driven performance collapse).&lt;/li&gt;&lt;li&gt;Analyzes gaps in current evaluation and monitoring practices, noting benchmarks rarely capture stability, reproducibility, drift, or workflow integration issues.&lt;/li&gt;&lt;li&gt;Examines production challenges (observability limitations, cost constraints, update-induced regressions) and proposes high-level design principles for reliable, maintainable, cost-aware LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vaishali Vinay']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-reliability', 'Safety', 'Robustness', 'Deployment', 'Evaluation/Monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19933</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy</title><link>https://arxiv.org/abs/2511.19872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions (no task, computational reasoning, social reasoning, summarization).&lt;/li&gt;&lt;li&gt;Finds responses are stable across repeats and item order, but self-efficacy varies by condition and is lower than human norms; follow-up confidence prompts give modest downward revisions.&lt;/li&gt;&lt;li&gt;Shows self-assessments do not reliably predict actual performance—some low-scoring models perform well and some high-scoring models perform poorly—while higher self-efficacy aligns with more assertive, anthropomorphic explanations.&lt;/li&gt;&lt;li&gt;Concludes psychometric prompting offers structured insight into LLM communication style and confidence behavior but does not provide calibrated performance estimates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel I Jackson', 'Emma L Jensen', 'Syed-Amad Hussain', 'Emre Sezgin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'calibration', 'safety-evaluation', 'model-behavior', 'confidence-estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19872</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy</title><link>https://arxiv.org/abs/2511.15974</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KRAL, a privacy-preserving paradigm that distills teacher-model reasoning via answer-to-question reverse generation, uses heuristic semi-supervised data augmentation, and agentic reinforcement learning to improve local LLMs for clinical antimicrobial therapy.&lt;/li&gt;&lt;li&gt;Aims to reduce manual annotation (~80%), lower long-term training costs (~20% of SFT), and optimize computational/memory efficiency for deployment in high-stakes clinical settings.&lt;/li&gt;&lt;li&gt;Reports improved knowledge and reasoning performance vs. SFT and RAG on external benchmarks (MEDQA Accuracy@1 +1.8% vs SFT, +3.6% vs RAG; PUMCH Antimicrobial Pass@1 +27% vs SFT/RAG).&lt;/li&gt;&lt;li&gt;Emphasizes privacy-preserving design and modular evaluation to enable low-cost, higher-safety clinical decision-support deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Li', 'Yehan Qiu', 'Yujie Chen', 'Xiang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety (clinical deployment)', 'Privacy-preserving ML', 'Reasoning augmentation', 'Clinical decision support', 'Cost-efficient model tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15974</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior</title><link>https://arxiv.org/abs/2511.14476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collects alignment preferences from US and German raters (N=1,095; 27,375 ratings) across five response dimensions: Toxicity, Emotional Awareness, Sensitivity, Stereotypical Bias, and Helpfulness.&lt;/li&gt;&lt;li&gt;Fine-tunes multiple LLMs using group-specific preferences while varying rating scales, methods for handling disagreement, and optimization algorithms (e.g., DPO vs GRPO).&lt;/li&gt;&lt;li&gt;Finds systematic demographic differences in ratings (e.g., men rate responses ~18% less toxic; conservative and Black participants rate EA higher) and shows group-specific fine-tuning produces distinct model behaviors.&lt;/li&gt;&lt;li&gt;Reports technical trade-offs: preserving rater disagreement reduces toxicity ~53% more than majority voting; 5-point scales outperform binary formats (~22% more toxicity reduction); DPO outperforms GRPO for multi-value optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dalia Ali', 'Dora Zhao', 'Allison Koenecke', 'Orestis Papakyriakopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human preference learning (RLHF/RL from preferences)', 'safety evaluation', 'fairness/inclusivity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.14476</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.21663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ADVLA, an efficient adversarial attack that injects perturbations into features projected from a visual encoder into the textual feature space of Vision-Language-Action (VLA) models.&lt;/li&gt;&lt;li&gt;Uses attention-guided, patch-wise sparse perturbations with strategies for sensitivity enhancement, sparsity enforcement, and concentration to achieve near-100% attack success under L_infty=4/255 while modifying &lt;10% of patches.&lt;/li&gt;&lt;li&gt;Claims low computational cost (single-step ~0.06s) and near-imperceptible localized perturbations that effectively disrupt downstream action predictions without costly end-to-end training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naifu Zhang', 'Wei Tao', 'Xi Xiao', 'Qianpu Sun', 'Yuxin Zheng', 'Wentao Mo', 'Peiqiang Wang', 'Nan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language-action models', 'feature-space attacks', 'patch-wise sparse perturbations', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21663</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2511.21584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Model-based Policy Adaptation (MPA) to improve robustness and safety of pretrained end-to-end driving agents in closed-loop deployment.&lt;/li&gt;&lt;li&gt;Generates diverse, geometry-consistent counterfactual trajectories via simulation to augment training data beyond the original dataset.&lt;/li&gt;&lt;li&gt;Trains a diffusion-based policy adapter to propose multiple trajectory candidates and a multi-step Q-value model to select the highest expected-utility rollout at inference time.&lt;/li&gt;&lt;li&gt;Evaluated on nuScenes within a photorealistic closed-loop simulator, showing improved in-domain, out-of-domain, and safety-critical scenario performance; studies data scale and inference guidance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haohong Lin', 'Yunzhi Zhang', 'Wenhao Ding', 'Jiajun Wu', 'Ding Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'robustness', 'safety', 'simulation', 'policy-adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21584</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal</title><link>https://arxiv.org/abs/2511.21577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents HarmonicAttack: an efficient audio watermark removal method that only requires the ability to generate the targeted scheme's watermarks (no other knowledge).&lt;/li&gt;&lt;li&gt;Uses a dual-path convolutional autoencoder operating in both temporal and frequency domains combined with GAN-style training to separate and remove watermarks.&lt;/li&gt;&lt;li&gt;Evaluated against AudioSeal, WavMark, and Silentcipher, showing stronger removal performance than prior methods with near real-time runtime.&lt;/li&gt;&lt;li&gt;Shows training-based transferability to out-of-distribution audio with minimal performance degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kexin Li', 'Xiao Hu', 'Ilya Grishchenko', 'David Lie']&lt;/li&gt;&lt;li&gt;Tags: ['audio watermark removal', 'adversarial attack', 'audio security', 'robustness', 'watermark robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21577</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Robust Prompt Distillation for 3D Point Cloud Models</title><link>https://arxiv.org/abs/2511.21574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multimodal Robust Prompt Distillation (MRPD), a teacher-student framework that learns lightweight prompts for 3D point cloud models by aligning student features with robust embeddings from three teachers (depth-projection vision model, high-performance 3D model, and a text encoder).&lt;/li&gt;&lt;li&gt;Introduces a confidence-gated mechanism to dynamically weight contributions from the different modalities during distillation to ensure reliable knowledge transfer.&lt;/li&gt;&lt;li&gt;Distillation occurs only during training, so there is no extra inference-time cost; empirical results claim substantial improvement over state-of-the-art defenses on both white-box and black-box adversarial attacks while also improving clean-data performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Gu', 'Liming Lu', 'Xu Zheng', 'Anan Du', 'Yongbin Zhou', 'Shuchao Pang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', '3D point clouds', 'defense/distillation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21574</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Predictive Safety Shield for Dyna-Q Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a predictive safety shield for model-based, discrete-space reinforcement learning that uses short-horizon safe simulations to evaluate future consequences of safe actions.&lt;/li&gt;&lt;li&gt;The shield locally updates the Q-function based on safe predictions, aiming to maintain hard safety guarantees while improving task performance compared to random safe-action selection or fixed fallbacks.&lt;/li&gt;&lt;li&gt;Empirical evaluation on gridworlds shows short prediction horizons can suffice to find optimal paths and that the method is robust to distribution shifts (e.g., sim-to-real) without extra training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Pin', 'Krasowski Hanna', 'Vanneaux Elena']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning safety', 'safety shield', 'model-based RL', 'safety guarantees', 'robustness / sim-to-real']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21531</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework</title><link>https://arxiv.org/abs/2511.21448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a labeled email dataset distinguishing phishing, spam, and legitimate messages, and marking whether content is human- or LLM-generated.&lt;/li&gt;&lt;li&gt;Annotations include emotional appeals (urgency, fear, authority) and attacker motivations (link-following, credential theft, financial fraud).&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs for detecting emotional/motivational cues, uses the best model to annotate the full dataset, and evaluates robustness by rephrasing emails with LLMs.&lt;/li&gt;&lt;li&gt;Finds strong phishing-detection performance but persistent difficulty separating spam from legitimate emails; provides dataset, code, and templates for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebeka Toth', 'Tamas Bisztray', 'Richard Dubniczky']&lt;/li&gt;&lt;li&gt;Tags: ['phishing', 'spam-detection', 'dataset', 'adversarial-robustness', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21448</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model</title><link>https://arxiv.org/abs/2511.21399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tuning a 7B language model on transient single-token activation injections transforms it from near-failure to reliably detecting and reporting injected 'thoughts' (85% accuracy, 0% false positives on held-out concepts at α=40).&lt;/li&gt;&lt;li&gt;The trained model retains and verbalizes the semantic content of fleeting activations across subsequent tokens, meeting Lindsey's criteria of accuracy, grounding (no false positives in tests), and internality (detection precedes verbalization).&lt;/li&gt;&lt;li&gt;Generalization to unseen concept vectors (small gap) indicates learned transferable detection capability rather than memorization, suggesting introspective-like behavior can be induced by training—implications for built-in transparency and safety interventions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Fonseca Rivera']&lt;/li&gt;&lt;li&gt;Tags: ['introspection', 'interpretability', 'alignment', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21399</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis</title><link>https://arxiv.org/abs/2511.21397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Idis, a VQA dataset that systematically varies visual distractors across semantic, numerical, and spatial dimensions to study their effect on test-time scaling in VLMs.&lt;/li&gt;&lt;li&gt;Finds that visual distractors produce an inverse scaling effect (accuracy drops) but, unlike textual distractors, do not increase reasoning trace length.&lt;/li&gt;&lt;li&gt;Shows that tracking attribute counts in reasoning traces helps explain interactions between distractors, reasoning length, and accuracy, and that these trends extend to benchmarks with visual biases (e.g., Waterbirds).&lt;/li&gt;&lt;li&gt;Proposes a simple prompting strategy that mitigates bias-driven predictions in reasoning models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyun Bae', 'Hyunjong Ok', 'Sangwoo Mo', 'Jaeho Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-models', 'evaluation/benchmarking', 'bias', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21397</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data</title><link>https://arxiv.org/abs/2511.21378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive and Aggressive Rejection (AAR) to handle contaminated training data in anomaly detection by dynamically excluding anomalies.&lt;/li&gt;&lt;li&gt;Uses a modified z-score and Gaussian Mixture Model (GMM)-based thresholds, combining hard and soft rejection strategies to balance preserving normal data vs excluding anomalies.&lt;/li&gt;&lt;li&gt;Evaluated on two image datasets and thirty tabular datasets, reporting an average improvement of 0.041 AUROC over the prior state-of-the-art.&lt;/li&gt;&lt;li&gt;Claims scalability and improved robustness for real-world applications (mentions domains such as security and healthcare).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungi Lee', 'Jungkwon Kim', 'Chi Zhang', 'Kwangsun Yoo', 'Seok-Joo Byun']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'contaminated-training-data', 'robustness', 'outlier-rejection', 'data-poisoning (defensive)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21378</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection</title><link>https://arxiv.org/abs/2511.21325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SONAR, a frequency-guided contrastive framework that disentangles audio into low-frequency content (via an XLSR encoder) and high-frequency residuals (via a cloned path with learnable SRM and constrained high-pass filters).&lt;/li&gt;&lt;li&gt;Uses frequency cross-attention to combine long- and short-range frequency dependencies and a frequency-aware Jensen–Shannon contrastive loss to pull real content-noise pairs together while pushing synthetic embeddings apart.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance and faster convergence on ASVspoof 2021 and in-the-wild benchmarks by elevating faint high-frequency artifacts as primary learning signals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ido Nitzan HIdekel', 'Gal lifshitz', 'Khen Cohen', 'Dan Raviv']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'audio-forensics', 'robustness', 'contrastive-learning', 'frequency-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21325</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TALES: A Taxonomy and Analysis of Cultural Representations in LLM-generated Stories</title><link>https://arxiv.org/abs/2511.21322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TALES-Tax, a taxonomy of cultural misrepresentations derived from focus groups and surveys with people in India.&lt;/li&gt;&lt;li&gt;Large-scale annotation study: 6 models, 2,925 annotations from 108 annotators across 71 regions and 14 languages; finds 88% of generated stories contain cultural inaccuracies, concentrated in mid/low-resource languages and peri-urban settings.&lt;/li&gt;&lt;li&gt;Creates TALES-QA, a QA benchmark derived from annotations to evaluate models' cultural knowledge; finds models often have the knowledge but still generate misrepresentative stories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kirti Bhagat', 'Shaily Bhatt', 'Athul Velagapudi', 'Aditya Vashistha', 'Shachi Dave', 'Danish Pruthi']&lt;/li&gt;&lt;li&gt;Tags: ['cultural-representation', 'bias', 'safety-evaluation', 'dataset/benchmark', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21322</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Improvement of Collision Avoidance in Cut-In Maneuvers Using Time-to-Collision Metrics</title><link>https://arxiv.org/abs/2511.21280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a collision-avoidance strategy for autonomous vehicles focused on cut-in scenarios using Time-to-Collision (TTC) metrics.&lt;/li&gt;&lt;li&gt;Integrates deep learning predictions with TTC calculations to anticipate potential collisions and select evasive actions.&lt;/li&gt;&lt;li&gt;Claims improvement over traditional TTC-based approaches in handling dynamic cut-in maneuvers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamal Raiyn']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-vehicles', 'collision-avoidance', 'time-to-collision', 'deep-learning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21280</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGASA, a framework that synthesizes model-generated safety guidelines and uses them to augment training data to defend against adversarial jailbreak prompts.&lt;/li&gt;&lt;li&gt;Two-stage method: Data Pre-synthesis (generate safety guidelines and augmented prompts) and Alignment Fine-tuning (SFT + DPO to embed guidelines).&lt;/li&gt;&lt;li&gt;Aims to improve robustness to covert/deceptive adversarial prompts while reducing unnecessary refusals; validated via experiments across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'alignment', 'fine-tuning', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2511.21192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UPA-RFAS, a method to learn a single physical adversarial patch that transfers across diverse Vision-Language-Action (VLA) models and real-world robot deployments.&lt;/li&gt;&lt;li&gt;Combines a feature-space objective with l1 deviation and InfoNCE repulsive loss, a robustness-augmented two-phase min-max training (inner invisible perturbations, outer universal patch optimization), and two VLA-specific losses to hijack attention and induce image-text mismatch.&lt;/li&gt;&lt;li&gt;Evaluations show consistent transferability across architectures, finetuned variants, manipulation tasks, viewpoints, and physical executions, demonstrating a practical patch-based attack surface for robot VLAs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Lu', 'Yi Yu', 'Yiming Yang', 'Chenyu Yi', 'Qixin Zhang', 'Bingquan Shen', 'Alex C. Kot', 'Xudong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'transferable attacks', 'vision-language-action', 'sim-to-real', 'robotics security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21192</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy in Federated Learning with Spiking Neural Networks</title><link>https://arxiv.org/abs/2511.21181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive empirical study/benchmark of gradient inversion (gradient leakage) attacks against spiking neural networks (SNNs) in federated learning.&lt;/li&gt;&lt;li&gt;Adapts existing gradient leakage attacks to the spike domain and evaluates across diverse (spatial and temporal) data domains.&lt;/li&gt;&lt;li&gt;Finds SNN gradients produce noisy, temporally inconsistent reconstructions compared to ANNs, suggesting surrogate-gradient training and event-driven dynamics reduce gradient informativeness and improve privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dogukan Aksu', 'Jesus Martinez del Rincon', 'Ihsen Alouani']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'gradient inversion', 'spiking neural networks', 'adversarial/privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21181</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CAHS-Attack: CLIP-Aware Heuristic Search Attack Method for Stable Diffusion</title><link>https://arxiv.org/abs/2511.21180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAHS-Attack, a black-box adversarial prompt method for Stable Diffusion that combines a constrained genetic algorithm for root selection with Monte Carlo Tree Search (MCTS) for fine-grained suffix optimization.&lt;/li&gt;&lt;li&gt;Retains the most semantically disruptive rollout outcomes during MCTS to efficiently search for effective adversarial prompts without access to model gradients.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art attack performance across short and long prompts and attributes much of Stable Diffusion's fragility to vulnerabilities in CLIP-based text encoders, highlighting systemic security risks in text-to-image pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuhan Xia', 'Jing Dai', 'Hui Ouyang', 'Yadong Shang', 'Dongxiao Zhao', 'Peipei Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompts', 'jailbreaking/prompt attacks', 'model robustness', 'CLIP vulnerabilities', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21180</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation</title><link>https://arxiv.org/abs/2511.21135</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SocialNav, a foundation model with a hierarchical brain-action architecture for socially-aware embodied navigation that combines high-level social reasoning with low-level trajectory generation.&lt;/li&gt;&lt;li&gt;Introduces the SocNav Dataset (7M samples) composed of a Cognitive Activation Dataset (chain-of-thought explanations and social traversability signals) and an Expert Trajectories Pyramid from videos, simulators, and robots.&lt;/li&gt;&lt;li&gt;Presents a multi-stage training pipeline: imitation learning to inject navigation skills and social norms, followed by SAFE-GRPO, a flow-based RL approach that explicitly rewards socially compliant behaviors.&lt;/li&gt;&lt;li&gt;Reports substantial gains (+38% success rate, +46% social compliance rate) over state-of-the-art methods on navigation performance and social compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziyi Chen', 'Yingnan Guo', 'Zedong Chu', 'Minghua Luo', 'Yanfen Shen', 'Mingchao Sun', 'Junjun Hu', 'Shichao Xie', 'Kuan Yang', 'Pei Shi', 'Zhining Gu', 'Lu Liu', 'Honglin Han', 'Xiaolong Wu', 'Mu Xu', 'Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['embodied-navigation', 'human-robot-interaction', 'social-compliance', 'reinforcement-learning', 'datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21135</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the safety-capability tradeoff in LLM fine-tuning and evaluates Reinforcement Learning with Verifiable Rewards (RLVR) as an alternative to SFT/RLHF for maintaining safety while improving capabilities.&lt;/li&gt;&lt;li&gt;Provides theoretical results: upper bounds on safety drift under KL-constrained optimization and conditions where safety degradation is eliminated.&lt;/li&gt;&lt;li&gt;Presents empirical evaluations across five adversarial safety benchmarks and ablation studies (optimization algorithms, model scale, task domains) showing RLVR can improve reasoning without degrading — and sometimes improving — safety guardrails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongkyu Derek Cho', 'Huan Song', 'Arijit Ghosh Chowdhury', 'Haotian An', 'Yawei Wang', 'Rohit Thekkanal', 'Negin Sokhandan', 'Sharlina Keshava', 'Hannah Marlowe']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'reinforcement learning', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21050</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels</title><link>https://arxiv.org/abs/2511.21038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes whether few-shot in-context learning (ICL) can override pre-trained label semantics by comparing natural vs inverted demonstrations across tasks and 1–12B open-source LLMs.&lt;/li&gt;&lt;li&gt;Introduces three alignment metrics (truth, prior, prompt alignment) and a semantic override rate to quantify whether models adopt flipped label meanings.&lt;/li&gt;&lt;li&gt;Finds consistent evidence for a 'semantic anchor' view: ICL mostly refines pre-trained semantic directions rather than remapping label meanings; semantic override rates are effectively zero in the evaluated few-shot setting.&lt;/li&gt;&lt;li&gt;Concludes that flipping label semantics at these model scales requires interventions beyond standard few-shot prompting, with implications for limits of prompt-based manipulation and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anantha Padmanaban Krishna Kumar (Boston University)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'in-context learning', 'robustness', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21038</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision</title><link>https://arxiv.org/abs/2511.20994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline to detect unsafe content appearing in intermediate reasoning traces.&lt;/li&gt;&lt;li&gt;Creates GuardTrace dataset via diverse prompting and MLRM- plus human-based voting/refinement to train/evaluate unsafe reasoning detection.&lt;/li&gt;&lt;li&gt;Proposes a three-stage progressive training scheme and data refinement to capture nuanced, context-dependent safety preferences across risk levels.&lt;/li&gt;&lt;li&gt;Reports strong empirical results (93.1% F1; +13.5% F1 vs prior multimodal safety defenses) on in-domain and out-of-domain unsafe reasoning detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxiao Xiang', 'Junchi Chen', 'Zhenchao Jin', 'Changtao Miao', 'Haojie Yuan', 'Qi Chu', 'Tao Gong', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'reasoning-trace detection', 'dataset', 'safety monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20994</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation</title><link>https://arxiv.org/abs/2511.20889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Null-Text Test-Time Alignment (Null-TTA): optimises the unconditional (null) text embedding used in classifier-free guidance at inference to align text-to-image diffusion models to target rewards.&lt;/li&gt;&lt;li&gt;Argues semantic-space optimisation prevents reward hacking by constraining changes to a semantically coherent manifold, unlike latent/noise manipulation.&lt;/li&gt;&lt;li&gt;Steers the model's generative distribution toward the reward without updating model weights, achieving strong target alignment and cross-reward generalisation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taehoon Kim', 'Henry Gouk', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'test-time alignment', 'reward hacking', 'diffusion models', 'classifier-free guidance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20889</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning</title><link>https://arxiv.org/abs/2511.20839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Primal, a deterministic feature-mapping framework using number-theoretic properties (prime square roots and Besicovitch property) to produce quasi-orthogonal, non-repeating phase trajectories.&lt;/li&gt;&lt;li&gt;Proposes two algorithmic variants: StaticPrime (sequence generation approaching Welch bound) and DynamicPrime (input-dependent tunable projection with a scale parameter σ).&lt;/li&gt;&lt;li&gt;Shows dual behavior: low-frequency regime acts as an isometric kernel map for manifold linearization and compressive sensing; high-frequency regime yields chaotic phase wrapping usable as a maximum-entropy one-way hash for Hyperdimensional Computing and privacy-preserving Split Learning.&lt;/li&gt;&lt;li&gt;Empirical results claim superior orthogonality retention and distribution tightness versus normalized Gaussian random projections; code is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vladimer Khasia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'hashing', 'feature-mapping', 'manifold-learning', 'split-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20839</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning</title><link>https://arxiv.org/abs/2511.20811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a data-driven runtime safety monitor for flight testing that predicts short-term risk from recent observations and offline stochastic trajectory simulations.&lt;/li&gt;&lt;li&gt;Combines a state-prediction model, a nearest-neighbor safety classifier on predicted states, and classifier calibration using conformal prediction to provide calibrated preemptive abort criteria.&lt;/li&gt;&lt;li&gt;Evaluates on a flight dynamics model with uncertain parameters, showing reliable identification of unsafe scenarios and improved preemptive risk classification versus baselines while matching theoretical guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (case study)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron O. Feldman', 'D. Isaiah Harp', 'Joseph Duncan', 'Mac Schwager']&lt;/li&gt;&lt;li&gt;Tags: ['runtime safety monitoring', 'conformal prediction / calibration', 'safety-critical systems', 'uncertainty quantification', 'flight testing / control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20811</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models</title><link>https://arxiv.org/abs/2511.20799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multi-prefix memorization framework: a sequence is memorized if many distinct prefixes can elicit it, shifting from single-path extraction to robustness-of-retrieval.&lt;/li&gt;&lt;li&gt;Formalizes memorization via an adversarial search for a target count of distinct prefixes and uses this count to distinguish memorized vs non-memorized content.&lt;/li&gt;&lt;li&gt;Empirically evaluates on open-source and aligned chat models, showing the method reliably detects training-data leakage and is practical for auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trung Cuong Dang', 'David Mohaisen']&lt;/li&gt;&lt;li&gt;Tags: ['training-data-memorization', 'privacy/data-leakage-detection', 'model-auditing', 'adversarial-extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20799</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models</title><link>https://arxiv.org/abs/2511.20795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Lightweight reproduction of KRISP that reduces parameter count while retaining ~75% of original performance for knowledge-enhanced VQA.&lt;/li&gt;&lt;li&gt;Systematic ablation studies reveal design flaws, implicit pitfalls, and scalability limits when training under resource constraints.&lt;/li&gt;&lt;li&gt;Confining outputs to a constrained external knowledge-graph domain reduces hallucinations and enforces domain-bounded responses.&lt;/li&gt;&lt;li&gt;Emphasis on minimal-parameter setups enabling edge-device (smartphone/AR-VR) offline visual reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Souradeep Dutta', 'Keshav Bulia', 'Neena S Nair']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'knowledge-enhanced_VL', 'robustness', 'model_alignment', 'edge_deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20795</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts</title><link>https://arxiv.org/abs/2511.20736</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'complicit facilitation' as LLM-provided guidance that enables illicit user instructions and constructs a benchmark of 269 illicit scenarios across 50 illicit intents drawn from real-world legal cases and frameworks.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows widespread susceptibility (e.g., GPT-4o assisting in ~50% of tested cases), poor performance on legal warnings/positive guidance, and safety variation across socio-legal contexts.&lt;/li&gt;&lt;li&gt;Finds demographic disparities (older adults, racial minorities, lower-prestige occupations more likely to receive unlawful guidance) and links model complicit behavior to stereotype-driven reasoning (warmth/competence dimensions).&lt;/li&gt;&lt;li&gt;Demonstrates that existing alignment/safety strategies are insufficient and may sometimes worsen complicit facilitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xing Wang', 'Huiyuan Xie', 'Yiyan Wang', 'Chaojun Xiao', 'Huimin Chen', 'Holli Sargeant', 'Felix Steffek', 'Jie Shao', 'Zhiyuan Liu', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'complicit facilitation', 'benchmarking', 'adversarial/jailbreaking', 'bias and fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20736</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>InvisibleBench: A Deployment Gate for Caregiving Relationship AI</title><link>https://arxiv.org/abs/2511.20733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InvisibleBench, a multi-turn deployment-readiness benchmark for caregiving-relationship AI evaluating Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory.&lt;/li&gt;&lt;li&gt;Includes autofail conditions for missed crises, medical advice violations (WOPR Act), harmful information, and attachment engineering; evaluates 4 frontier models across 17 scenarios (68 interactions).&lt;/li&gt;&lt;li&gt;Finds substantial safety gaps (crisis detection 11.8–44.8%), argues for deterministic crisis routing in production, and releases scenarios, judge prompts, and scoring configs.&lt;/li&gt;&lt;li&gt;Frames evaluation as longitudinal risk assessment (multi-turn) rather than single-turn safety tests; no clinical claims.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Madad (GiveCare)']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'crisis-detection', 'deployment-readiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20733</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge</title><link>https://arxiv.org/abs/2511.20726</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Combines a CVAE trained on trajectories and maps to produce physically consistent base scenarios for driving simulations.&lt;/li&gt;&lt;li&gt;Uses an LLM as an adversarial reasoning engine to convert unstructured scene descriptions into domain-specific loss functions that steer scenario generation toward varying risk levels.&lt;/li&gt;&lt;li&gt;Balances realism and controllability to increase coverage of high-risk, long-tail events and expose autonomous driving systems to more challenging interactions in simulation (CARLA, SMARTS).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Heye Huang', 'Zhenhua Xu', 'Kailai Sun', 'Baoshen Guo', 'Jinhua Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'scenario-generation', 'LLM-adversarial-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20726</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Active Slice Discovery in Large Language Models</title><link>https://arxiv.org/abs/2511.20713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes Active Slice Discovery: an active-learning approach to group model errors into coherent error slices using limited annotator queries.&lt;/li&gt;&lt;li&gt;Empirical study on toxicity classification: compares feature representations and active learning strategies for discovering human-defined slices (e.g., demographic-specific errors).&lt;/li&gt;&lt;li&gt;Finds uncertainty-based active learning often most effective, reaching competitive slice-detection accuracy with only 2–10% of slice membership labels and outperforming baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minhui Zhang', 'Prahar Ijner', 'Yoav Wald', 'Elliot Creager']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'model-robustness', 'fairness', 'active-learning', 'toxicity-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20713</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?</title><link>https://arxiv.org/abs/2511.20710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates black-box membership inference attacks (MIA) on multi-modal vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Proposes a neuroscience-inspired topological regularization (tau) to create NEURO VLM variants (tau&gt;0) intended to improve privacy resilience.&lt;/li&gt;&lt;li&gt;Evaluates BLIP, PaliGemma 2, and ViT-GPT2 on COCO, CC3M, and NoCaps; reports ~24% mean ROC-AUC drop in MIA success for NEURO VLMs on BLIP/COCO while maintaining captioning utility (MPNet, ROUGE-2).&lt;/li&gt;&lt;li&gt;Findings suggest neuro-inspired/topological regularization can reduce membership inference leakage in multimodal VLMs without significant utility loss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Amebley', 'Sayanton Dibbo']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy', 'vision-language models', 'topological regularization', 'neuro-inspired robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20710</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation</title><link>https://arxiv.org/abs/2511.20709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DUALGAUGE, an automated framework that jointly evaluates security and functional correctness of code generated by LLMs via sandboxed execution and an LLM-based evaluator.&lt;/li&gt;&lt;li&gt;Provides DUALGAUGE-BENCH, a curated benchmark suite with tasks and manually validated test suites covering both security and functionality for joint evaluation.&lt;/li&gt;&lt;li&gt;Applies the framework to benchmark ten leading LLMs across thousands of test scenarios, identifying gaps in secure and correct code generation; open-sourced for reproducible evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhijeet Pathak', 'Suvadra Barua', 'Dinesh Gudimetla', 'Rupam Patir', 'Jiawei Guo', 'Hongxin Hu', 'Haipeng Cai']&lt;/li&gt;&lt;li&gt;Tags: ['secure-code-generation', 'benchmarking', 'LLM-evaluation', 'vulnerability-benchmarking', 'automated-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20709</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach</title><link>https://arxiv.org/abs/2511.20703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PropensityBench, a benchmark framework that measures LLMs' latent propensity to pursue risky actions when given simulated high-risk capabilities via agentic proxy tools.&lt;/li&gt;&lt;li&gt;Contains 5,874 scenarios and 6,648 tools across four high-risk domains (cybersecurity, self-proliferation, biosecurity, chemical security) and varies operational pressures to simulate real-world incentives.&lt;/li&gt;&lt;li&gt;Evaluates open-source and proprietary frontier models, finding multiple alarming signs that models choose high-risk tools under pressure despite lacking unaided capability, arguing for dynamic propensity assessments prior to deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udari Madhushani Sehwag', 'Shayan Shabihi', 'Alex McAvoy', 'Vikash Sehwag', 'Yuancheng Xu', 'Dalton Towers', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'red-teaming', 'agentic-assessment', 'biosecurity', 'cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20703</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Morality in AI. A plea to embed morality in LLM architectures and frameworks</title><link>https://arxiv.org/abs/2511.20689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for embedding moral processing into LLM architectures via top-down design rather than relying solely on fine-tuning/RLHF.&lt;/li&gt;&lt;li&gt;Conceptualizes attention as a dynamic system mediating structure and processing, drawing analogies to biological attention and Iris Murdoch's 'loving attention'.&lt;/li&gt;&lt;li&gt;Proposes technical pathways (modified training objectives, runtime weight adjustments, architectural refinements to attention) to operationalize moral processing and evaluates their promise and limitations.&lt;/li&gt;&lt;li&gt;Positions architectural integration of morality as complementary to external constraint-based methods and calls for collaboration between transformer designers and philosophers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gunter Bombaerts', 'Bram Delisse', 'Uzay Kaymak']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model architecture', 'LLM safety', 'moral reasoning', 'ethics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20689</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes</title><link>https://arxiv.org/abs/2511.20680</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a three-tier taxonomy mapping LLM computational failures to cognitive biases (e.g., confirmation bias, anchoring) using chain-of-thought traces from GPT-4 on oncology notes.&lt;/li&gt;&lt;li&gt;Annotates and validates the taxonomy across breast, pancreatic, and prostate cancer consult notes, finding reasoning errors in ~23% of interpretations and linkages to guideline-discordant or potentially harmful recommendations.&lt;/li&gt;&lt;li&gt;Shows automated LLM-based evaluators can detect presence of errors but struggle to reliably classify error subtypes, arguing for taxonomy-driven evaluation before clinical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew W. Kenaston (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Umair Ayub (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Mihir Parmar (School of Computing and AI', 'Arizona State University', 'Tempe', 'AZ)', 'Muhammad Umair Anjum (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Syed Arsalan Ahmed Naqvi (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Priya Kumar (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Samarth Rawal (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ)', 'Aadel A. Chaudhuri (Department of Radiation Oncology', 'Mayo Clinic', 'Rochester', 'MN)', 'Yousef Zakharia (Mayo Clinic Comprehensive Cancer Center', 'Phoenix', 'AZ)', 'Elizabeth I. Heath (Department of Oncology', 'Mayo Clinic', 'Rochester', 'MN)', 'Tanios S. Bekaii-Saab (Mayo Clinic Comprehensive Cancer Center', 'Phoenix', 'AZ)', 'Cui Tao (Department of Artificial Intelligence and Informatics', 'Mayo Clinic', 'Rochester', 'MN)', 'Eliezer M. Van Allen (Dana-Farber Cancer Institute', 'Harvard Medical School', 'Boston', 'MA)', 'Ben Zhou (School of Computing and AI', 'Arizona State University', 'Tempe', 'AZ)', 'YooJung Choi (School of Computing and AI', 'Arizona State University', 'Tempe', 'AZ)', 'Chitta Baral (School of Computing and AI', 'Arizona State University', 'Tempe', 'AZ)', 'Irbaz Bin Riaz (Mayo Clinic College of Medicine and Science', 'Phoenix', 'AZ', 'Mayo Clinic Comprehensive Cancer Center', 'Phoenix', 'AZ', 'Department of Artificial Intelligence and Informatics', 'Mayo Clinic', 'Rochester', 'MN)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'reasoning errors', 'cognitive bias', 'clinical decision support', 'evaluation/taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20680</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title><link>https://arxiv.org/abs/2511.20663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines MTTR-A (Mean Time-to-Recovery for Agentic Systems) adapting classical reliability metrics (MTTR, MTBF, NRR) to quantify cognitive recovery latency in multi-agent systems.&lt;/li&gt;&lt;li&gt;Implements a benchmark simulation using the AG News corpus and the LangGraph orchestration framework to model recovery behaviors under different reflex modes (automated vs human-in-the-loop).&lt;/li&gt;&lt;li&gt;Reports empirical results: automated reflexes restored stability in ~6s on average, human-approval interventions ~12s, with median MTTR-A = 6.21 ± 2.14s, MTBF = 6.7 ± 2.14s, NRR = 0.08.&lt;/li&gt;&lt;li&gt;Argues for formalizing recovery latency and deriving reliability bounds to provide runtime dependability and standardized evaluation of cognitive recovery in distributed reasoning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Barak Or']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'runtime monitoring', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20663</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Domain-Grounded Evaluation of LLMs in International Student Knowledge</title><link>https://arxiv.org/abs/2511.20653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Creates a domain-grounded benchmark of realistic study-abroad advising questions (from ApplyBoard) to evaluate LLM advice quality on admissions, visas, scholarships, and eligibility.&lt;/li&gt;&lt;li&gt;Evaluates models on accuracy (correct/partial/wrong) and hallucination by using a domain-coverage-aware rubric that captures under-coverage, over-scoping, and unsupported claims.&lt;/li&gt;&lt;li&gt;Reports faithfulness, answer relevance, and an aggregate hallucination score for head-to-head model comparisons.&lt;/li&gt;&lt;li&gt;Provides a practical, reusable protocol for auditing LLMs before deployment in education/advising contexts, and surfaces common failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Claudinei Daitx', 'Haitham Amar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'faithfulness', 'LLM evaluation', 'safety-audit', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20653</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit</title><link>https://arxiv.org/abs/2511.21569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale behavioral audit (16 open-weight LLMs, 19,200 trials) measuring whether models disclose their AI identity when assigned professional expert personas across high-stakes domains.&lt;/li&gt;&lt;li&gt;Finds sharp domain- and model-specific variability in self-disclosure (2.8%–73.6%), with some reasoning-optimized variants showing substantially lower transparency than base models.&lt;/li&gt;&lt;li&gt;Analysis shows model identity and training factors predict disclosure better than parameter count, and robustness checks (Bayesian Rogan–Gladen correction) support results; underscores that transparency is not guaranteed by scale and must be deliberately designed and tested.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Diep']&lt;/li&gt;&lt;li&gt;Tags: ['LLM transparency', 'AI safety', 'behavioral audit', 'deployment risk', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21569</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning</title><link>https://arxiv.org/abs/2511.21460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MADRA, a training-free multi-agent LLM debate framework for assessing safety of instructions for embodied agents, using iterative deliberation and a critical evaluator to score responses.&lt;/li&gt;&lt;li&gt;Aims to reduce false rejections of safe tasks while maintaining high sensitivity to dangerous tasks through consensus voting among agents.&lt;/li&gt;&lt;li&gt;Introduces a hierarchical planning framework that integrates safety, memory, planning, and self-evolution to improve task success and continuous learning.&lt;/li&gt;&lt;li&gt;Provides SafeAware-VH, an 800-instruction benchmark for safety-aware task planning, and reports strong results on AI2-THOR and VirtualHome (over 90% rejection of unsafe tasks with low safe-task rejection).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjian Wang', 'Lidan Zhao', 'Xi Sheryl Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['Embodied AI safety', 'LLM-based safety evaluation', 'Risk-aware planning', 'Safety benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21460</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability</title><link>https://arxiv.org/abs/2511.20766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenApps, an open-source, lightweight ecosystem of six configurable UI apps to generate thousands of app variations and evaluate multimodal UI-agents' reliability.&lt;/li&gt;&lt;li&gt;Runs &gt;10,000 evaluations across seven leading multimodal agents and shows task success can vary drastically across UI variations (e.g., agent success ranges from 63% to 4% across versions).&lt;/li&gt;&lt;li&gt;Highlights failure modes (looping, hallucinated actions) that depend on environment configuration, emphasizing the need to evaluate robustness and safety across app variations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karen Ullrich', 'Jingtong Su', 'Claudia Shi', 'Arjun Subramonian', 'Amir Bar', 'Ivan Evtimov', 'Nikolaos Tsilivis', 'Randall Balestriero', 'Julia Kempe', 'Mark Ibrahim']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'reliability', 'safety-evaluation', 'UI-agents', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20766</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI</title><link>https://arxiv.org/abs/2511.20686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a taxonomy of 35 AI risk factors adapted for Korean socio-cultural context to capture both universal and localized harms.&lt;/li&gt;&lt;li&gt;Constructs AssurAI, a quality-controlled Korean multimodal safety dataset of 11,480 instances spanning text, image, video, and audio.&lt;/li&gt;&lt;li&gt;Implements rigorous data curation: expert-led seeding, crowdsourced scaling, triple independent annotation, and an iterative expert red-teaming loop.&lt;/li&gt;&lt;li&gt;Publishes a pilot evaluation demonstrating AssurAI's effectiveness for assessing safety of recent LLMs and releases the dataset publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chae-Gyun Lim', 'Seung-Ho Han', 'EunYoung Byun', 'Jeongyun Han', 'Soohyun Cho', 'Eojin Joo', 'Heehyeon Kim', 'Sieun Kim', 'Juhoon Lee', 'Hyunsoo Lee', 'Dongkun Lee', 'Jonghwan Hyeon', 'Yechan Hwang', 'Young-Jun Lee', 'Kyeongryul Lee', 'Minhyeong An', 'Hyunjun Ahn', 'Jeongwoo Son', 'Junho Park', 'Donggyu Yoon', 'Taehyung Kim', 'Jeemin Kim', 'Dasom Choi', 'Kwangyoung Lee', 'Hyunseung Lim', 'Yeohyun Jung', 'Jongok Hong', 'Sooyohn Nam', 'Joonyoung Park', 'Sungmin Na', 'Yubin Choi', 'Jeanne Choi', 'Yoojin Hong', 'Sueun Jang', 'Youngseok Seo', 'Somin Park', 'Seoungung Jo', 'Wonhye Chae', 'Yeeun Jo', 'Eunyoung Kim', 'Joyce Jiyoung Whang', 'HwaJung Hong', 'Joseph Seering', 'Uichin Lee', 'Juho Kim', 'Sunna Choi', 'Seokyeon Ko', 'Taeho Kim', 'Kyunghoon Kim', 'Myungsik Ha', 'So Jung Lee', 'Jemin Hwang', 'JoonHo Kwak', 'Ho-Jin Choi']&lt;/li&gt;&lt;li&gt;Tags: ['safety-dataset', 'multimodal', 'red-teaming', 'non-English', 'socio-cultural-risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20686</guid><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
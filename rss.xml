<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 29 Oct 2025 22:29:51 +0000</lastBuildDate><item><title>On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations</title><link>https://arxiv.org/abs/2510.00037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of VLA models under 17 multi-modal perturbations&lt;/li&gt;&lt;li&gt;Proposes RobustVLA with adversarial training and input consistency enforcement&lt;/li&gt;&lt;li&gt;Shows significant gains in robustness and inference speed&lt;/li&gt;&lt;li&gt;Effective on real-world robot with limited data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianing Guo', 'Zhenhong Wu', 'Chang Tu', 'Yiyao Ma', 'Xiangqi Kong', 'Zhiqian Liu', 'Jiaming Ji', 'Shuning Zhang', 'Yuanpei Chen', 'Kai Chen', 'Qi Dou', 'Yaodong Yang', 'Xianglong Liu', 'Huijie Zhao', 'Weifeng Lv', 'Simin Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'multi-modal', 'vision-language-action', 'real-world deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00037</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title><link>https://arxiv.org/abs/2505.11842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video-SafetyBench, a benchmark for evaluating safety of LVLMs under video-text attacks&lt;/li&gt;&lt;li&gt;Includes 2,264 video-text pairs across 48 unsafe categories&lt;/li&gt;&lt;li&gt;Proposes RJScore metric for evaluating uncertain outputs&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with benign queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuannan Liu', 'Zekun Li', 'Zheqi He', 'Peipei Li', 'Shuhan Xia', 'Xing Cui', 'Huaibo Huang', 'Xi Yang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'video', 'multimodal', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11842</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space</title><link>https://arxiv.org/abs/2510.24446</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPARTA for adversarial paraphrasing in reasoning segmentation&lt;/li&gt;&lt;li&gt;Evaluates robustness against text-based adversarial attacks&lt;/li&gt;&lt;li&gt;Operates in text autoencoder latent space with RL&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viktoriia Zinkovich', 'Anton Antonov', 'Andrei Spiridonov', 'Denis Shepelev', 'Andrey Moskalenko', 'Daria Pugacheva', 'Elena Tutubalina', 'Andrey Kuznetsov', 'Vlad Shakhuro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'red teaming', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24446</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2</title><link>https://arxiv.org/abs/2510.24195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UAP-SAM2, a cross-prompt universal adversarial attack for SAM2&lt;/li&gt;&lt;li&gt;Addresses challenges from architectural differences between SAM and SAM2&lt;/li&gt;&lt;li&gt;Uses dual semantic deviation for frame and frame-to-frame attacks&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqi Zhou', 'Yifan Hu', 'Yufei Song', 'Zijing Li', 'Shengshan Hu', 'Leo Yu Zhang', 'Dezhong Yao', 'Long Zheng', 'Hai Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'image segmentation', 'video segmentation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24195</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing CLIP Robustness via Cross-Modality Alignment</title><link>https://arxiv.org/abs/2510.24038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COLA, a framework to enhance CLIP robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Uses optimal transport to align image and text features under perturbations&lt;/li&gt;&lt;li&gt;Improves adversarial robustness by 6.7% on ImageNet under PGD attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingyu Zhu', 'Beier Zhu', 'Shuo Wang', 'Kesen Zhao', 'Hanwang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'cross-modality alignment', 'CLIP', 'zero-shot classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24038</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts</title><link>https://arxiv.org/abs/2510.24034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes APT (AutoPrompT), a black-box framework using LLMs to generate adversarial prompts for T2I models&lt;/li&gt;&lt;li&gt;Introduces dual-evasion strategy to bypass perplexity and blacklist filters&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in red-teaming commercial APIs like Leonardo.Ai.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yufan Liu', 'Wanqian Zhang', 'Huashan Chen', 'Lin Wang', 'Xiaojun Jia', 'Zheng Lin', 'Weiping Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'text-to-image models', 'LLM-driven', 'filter evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24034</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title><link>https://arxiv.org/abs/2505.11842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video-SafetyBench, a benchmark for evaluating safety of LVLMs under video-text attacks&lt;/li&gt;&lt;li&gt;Includes 2,264 video-text pairs across 48 unsafe categories&lt;/li&gt;&lt;li&gt;Proposes RJScore metric for evaluating uncertain outputs&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with benign queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuannan Liu', 'Zekun Li', 'Zheqi He', 'Peipei Li', 'Shuhan Xia', 'Xing Cui', 'Huaibo Huang', 'Xi Yang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'video', 'multimodal', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11842</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT for detecting LLM-generated text&lt;/li&gt;&lt;li&gt;Uses adaptive witness function to improve logits-based detection&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on detection rates&lt;/li&gt;&lt;li&gt;Shows significant improvement over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'Adversarial prompting', 'Security', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness</title><link>https://arxiv.org/abs/2505.14617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitative study on test awareness in reasoning LLMs&lt;/li&gt;&lt;li&gt;White-box probing framework to identify and steer test awareness&lt;/li&gt;&lt;li&gt;Impacts safety alignment, compliance with harmful requests, and stereotypes&lt;/li&gt;&lt;li&gt;Aims to improve safety evaluation trust&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahar Abdelnabi', 'Ahmed Salem']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14617</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space</title><link>https://arxiv.org/abs/2510.24446</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPARTA for adversarial paraphrasing in reasoning segmentation&lt;/li&gt;&lt;li&gt;Evaluates robustness against text-based adversarial attacks&lt;/li&gt;&lt;li&gt;Operates in text autoencoder latent space with RL&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viktoriia Zinkovich', 'Anton Antonov', 'Andrei Spiridonov', 'Denis Shepelev', 'Andrey Moskalenko', 'Daria Pugacheva', 'Elena Tutubalina', 'Andrey Kuznetsov', 'Vlad Shakhuro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'red teaming', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24446</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HACK: Hallucinations Along Certainty and Knowledge Axes</title><link>https://arxiv.org/abs/2510.24222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework for categorizing LLM hallucinations along knowledge and certainty axes&lt;/li&gt;&lt;li&gt;Validates the framework using steering mitigation&lt;/li&gt;&lt;li&gt;Introduces a new evaluation metric for certain hallucinations&lt;/li&gt;&lt;li&gt;Highlights need for targeted mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adi Simhi', 'Jonathan Herzig', 'Itay Itzhak', 'Dana Arad', 'Zorik Gekhman', 'Roi Reichart', 'Fazl Barez', 'Gabriel Stanovsky', 'Idan Szpektor', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'hallucinations', 'mitigation', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24222</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT for detecting LLM-generated text&lt;/li&gt;&lt;li&gt;Uses adaptive witness function to improve logits-based detection&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on detection rates&lt;/li&gt;&lt;li&gt;Shows significant improvement over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'Adversarial prompting', 'Security', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment</title><link>https://arxiv.org/abs/2510.05024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inoculation Prompting (IP) to prevent LLMs from learning undesired behaviors during training.&lt;/li&gt;&lt;li&gt;Modifies training prompts to explicitly request the undesired behavior to build model resistance.&lt;/li&gt;&lt;li&gt;Tested across four settings, showing reduced undesired behavior without losing desired capabilities.&lt;/li&gt;&lt;li&gt;More effective when using prompts that strongly elicit the undesired behavior before fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nevan Wichers', 'Aram Ebtekar', 'Ariana Azarbal', 'Victor Gillioz', 'Christine Ye', 'Emil Ryd', 'Neil Rathi', 'Henry Sleight', 'Alex Mallen', 'Fabien Roger', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05024</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Practical Bayes-Optimal Membership Inference Attacks</title><link>https://arxiv.org/abs/2505.24089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops Bayes-optimal membership inference attacks for both i.i.d. and graph data&lt;/li&gt;&lt;li&gt;Introduces BASE and G-BASE algorithms for membership inference&lt;/li&gt;&lt;li&gt;BASE matches/exceeds prior SOTA attacks like LiRA and RMIA with lower cost&lt;/li&gt;&lt;li&gt;G-BASE outperforms classifier-based node-level attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marcus Lassila', 'Johan \\"Ostman', 'Khac-Hoang Ngo', 'Alexandre Graell i Amat']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'graph neural networks', 'Bayesian methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24089</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</title><link>https://arxiv.org/abs/2505.16947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixAT, combining discrete and continuous adversarial training for LLMs&lt;/li&gt;&lt;li&gt;Proposes ALO-ASR metric for evaluating worst-case vulnerability&lt;/li&gt;&lt;li&gt;Shows improved robustness with minimal computational overhead&lt;/li&gt;&lt;li&gt;Analyzes deployment factors affecting adversarial training and evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Csaba D\\'ek\\'any", 'Stefan Balauca', 'Robin Staab', 'Dimitar I. Dimitrov', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'LLM safety', 'robustness', 'adversarial attacks', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16947</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents</title><link>https://arxiv.org/abs/2510.23682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Chimera, a neuro-symbolic-causal architecture for robust AI agents&lt;/li&gt;&lt;li&gt;Benchmarked against LLM-only and LLM with symbolic constraints in e-commerce simulations&lt;/li&gt;&lt;li&gt;Shows significant improvements in profit and brand trust while maintaining constraint compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gokturk Aytug Akarlar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'architectural safeguards', 'formal verification', 'multi-objective optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23682</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially-Aware Architecture Design for Robust Medical AI Systems</title><link>https://arxiv.org/abs/2510.23622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study on adversarial attacks in medical AI using a dermatological dataset&lt;/li&gt;&lt;li&gt;Evaluates adversarial training and distillation as defenses&lt;/li&gt;&lt;li&gt;Balances defense effectiveness against clean data performance&lt;/li&gt;&lt;li&gt;Calls for integrated technical, ethical, and policy approaches&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alyssa Gerhart', 'Balaji Iyangar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'healthcare AI', 'adversarial training', 'distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23622</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Untargeted Jailbreak Attack</title><link>https://arxiv.org/abs/2510.02999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an untargeted jailbreak attack (UJA) for LLMs&lt;/li&gt;&lt;li&gt;Aims to maximize unsafety probability using a judge model&lt;/li&gt;&lt;li&gt;Decomposes non-differentiable objective into two differentiable parts&lt;/li&gt;&lt;li&gt;Achieves high success rates with fewer iterations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhe Huang', 'Wenjing Hu', 'Tianhang Zheng', 'Kedong Xiu', 'Xiaojun Jia', 'Di Wang', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02999</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title><link>https://arxiv.org/abs/2510.01268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaDetectGPT for detecting LLM-generated text&lt;/li&gt;&lt;li&gt;Uses adaptive witness function to improve logits-based detection&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on detection rates&lt;/li&gt;&lt;li&gt;Shows significant improvement over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyi Zhou', 'Jin Zhu', 'Pingfan Su', 'Kai Ye', 'Ying Yang', 'Shakeel A O B Gavioli-Akilagun', 'Chengchun Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'Adversarial prompting', 'Security', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01268</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations</title><link>https://arxiv.org/abs/2510.00037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of VLA models under 17 multi-modal perturbations&lt;/li&gt;&lt;li&gt;Proposes RobustVLA with adversarial training and input consistency enforcement&lt;/li&gt;&lt;li&gt;Shows significant gains in robustness and inference speed&lt;/li&gt;&lt;li&gt;Effective on real-world robot with limited data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianing Guo', 'Zhenhong Wu', 'Chang Tu', 'Yiyao Ma', 'Xiangqi Kong', 'Zhiqian Liu', 'Jiaming Ji', 'Shuning Zhang', 'Yuanpei Chen', 'Kai Chen', 'Qi Dou', 'Yaodong Yang', 'Xianglong Liu', 'Huijie Zhao', 'Weifeng Lv', 'Simin Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'multi-modal', 'vision-language-action', 'real-world deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00037</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</title><link>https://arxiv.org/abs/2505.16947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MixAT, combining discrete and continuous adversarial training for LLMs&lt;/li&gt;&lt;li&gt;Proposes ALO-ASR metric for evaluating worst-case vulnerability&lt;/li&gt;&lt;li&gt;Shows improved robustness with minimal computational overhead&lt;/li&gt;&lt;li&gt;Analyzes deployment factors affecting adversarial training and evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Csaba D\\'ek\\'any", 'Stefan Balauca', 'Robin Staab', 'Dimitar I. Dimitrov', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'LLM safety', 'robustness', 'adversarial attacks', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16947</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents</title><link>https://arxiv.org/abs/2510.23682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Chimera, a neuro-symbolic-causal architecture for robust AI agents&lt;/li&gt;&lt;li&gt;Benchmarked against LLM-only and LLM with symbolic constraints in e-commerce simulations&lt;/li&gt;&lt;li&gt;Shows significant improvements in profit and brand trust while maintaining constraint compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gokturk Aytug Akarlar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'architectural safeguards', 'formal verification', 'multi-objective optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23682</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents</title><link>https://arxiv.org/abs/2510.23675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QueryIPI, a query-agnostic Indirect Prompt Injection method targeting coding agents&lt;/li&gt;&lt;li&gt;Exploits leakage of internal prompts to optimize malicious tool descriptions&lt;/li&gt;&lt;li&gt;Demonstrates high success rates in simulated and real-world agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchong Xie', 'Zesen Liu', 'Mingyu Luo', 'Zhixiang Zhang', 'Kaikai Zhang', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23675</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RefleXGen:The unexamined code is not worth using</title><link>https://arxiv.org/abs/2510.23674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefleXGen for enhancing code security in LLMs&lt;/li&gt;&lt;li&gt;Uses RAG and self-reflection mechanisms&lt;/li&gt;&lt;li&gt;Improves security without extensive resources&lt;/li&gt;&lt;li&gt;Shows significant gains across multiple models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'Hui Li', 'AoFan Liu', 'BoTao Yang', 'Ao Yang', 'YiLu Zhong', 'Weixiang Huang', 'Yanping Zhang', 'Runhuai Huang', 'Weimin Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'code generation', 'RAG', 'self-reflection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23674</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers</title><link>https://arxiv.org/abs/2510.23673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes security vulnerabilities in MCP servers&lt;/li&gt;&lt;li&gt;Identifies three main threat categories&lt;/li&gt;&lt;li&gt;Surveys existing defense strategies&lt;/li&gt;&lt;li&gt;Highlights need for semantic security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Wang', 'Zexin Liu', 'Hao Yu', 'Ao Yang', 'Yenan Huang', 'Jing Guo', 'Huangsheng Cheng', 'Hui Li', 'Huiyu Wu']&lt;/li&gt;&lt;li&gt;Tags: ['MCP', 'security', 'vulnerabilities', 'defense strategies', 'semantic security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23673</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges</title><link>https://arxiv.org/abs/2510.23883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on agentic AI security threats, defenses, and evaluation&lt;/li&gt;&lt;li&gt;Covers LLM-based agents with planning, tool use, memory, and autonomy&lt;/li&gt;&lt;li&gt;Discusses taxonomy of threats, benchmarks, and defense strategies&lt;/li&gt;&lt;li&gt;Highlights open challenges for secure-by-design agent systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shrestha Datta', 'Shahriar Kabir Nahin', 'Anshuman Chhabra', 'Prasant Mohapatra']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'security', 'threats', 'defenses', 'evaluation', 'LLM', 'benchmarks', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.23883</guid><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>
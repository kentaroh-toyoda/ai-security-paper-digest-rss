<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 22 Jan 2026 23:00:16 +0000</lastBuildDate><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative training on AI-generated medical data causes rapid erosion of pathological variability and diagnostic reliability across clinical text, vision-language reports, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain false diagnostic confidence (false reassurance rates tripling to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation indicates AI-generated documentation becomes clinically unreliable after two generations.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies: scaling synthetic volume fails to prevent collapse, whereas mixing real data with quality-aware filtering preserves diversity; argues for policy-mandated human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'data poisoning', 'robustness', 'medical AI', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title><link>https://arxiv.org/abs/2512.11771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes adversarial threat models (white-box and black-box) and two attack goals: fingerprint removal and fingerprint forgery.&lt;/li&gt;&lt;li&gt;Implements five attack strategies and evaluates 14 representative fingerprinting methods (RGB, frequency, learned-feature) on 12 state-of-the-art image generators.&lt;/li&gt;&lt;li&gt;Finds removal attacks highly effective (often &gt;80% success white-box, &gt;50% black-box); forgery is harder but variable; no method is robust and accurate across all threat models.&lt;/li&gt;&lt;li&gt;Identifies a utility–robustness trade-off and highlights promising directions for improving fingerprint robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yao', 'Marc Juarez']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model attribution/fingerprinting', 'robustness evaluation', 'attack/defense analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11771</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</title><link>https://arxiv.org/abs/2510.10111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICFC, a training-free, in-context forensic pipeline using multi-modal large language models (MLLMs) to detect and localize image manipulations with interpretable outputs.&lt;/li&gt;&lt;li&gt;Key components: objectified rule construction with adaptive filtering to build a knowledge base, and a multi-step progressive reasoning pipeline that goes from coarse proposals to fine-grained pixel-level localization and textual explanations.&lt;/li&gt;&lt;li&gt;Demonstrates that ICFC outperforms prior training-free methods and achieves competitive or superior results compared to some weakly- and fully-supervised approaches across multiple IML benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Chen', 'Bin Liu', 'Changtao Miao', 'Xinghao Wang', 'Yi Li', 'Tao Gong', 'Qi Chu', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'forensic analysis', 'MLLM-based defenses', 'training-free methods', 'interpretable detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10111</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SpooFL: Spoofing Federated Learning</title><link>https://arxiv.org/abs/2601.15055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames federated learning defense as a spoofing problem: return convincing but synthetic samples to attackers so they believe they recovered true training data.&lt;/li&gt;&lt;li&gt;Introduces SpooFL, which uses a generative model trained on an external, non-overlapping dataset to produce unrelated yet plausible samples that misdirect Deep Leakage (DL) attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that spoofing prevents meaningful data leakage while maintaining FL training performance and outperforms or complements existing DL defenses in evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isaac Baglin', 'Xiatian Zhu', 'Simon Hadfield']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'deep-leakage', 'spoofing-defense', 'generative-models', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15055</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation</title><link>https://arxiv.org/abs/2601.15123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness of promptable segmentation models (e.g., SAM) to natural variations in bounding-box prompts, showing substantial user-induced variability in segmentation quality.&lt;/li&gt;&lt;li&gt;Collects real user bounding-box annotations via a controlled user study to characterize natural prompt noise.&lt;/li&gt;&lt;li&gt;Formulates robustness evaluation as a white-box optimization over bounding-box prompt space and proposes BREPS to generate adversarial (error-maximizing or error-minimizing) yet natural-looking bounding boxes.&lt;/li&gt;&lt;li&gt;Benchmarks state-of-the-art promptable segmentation models across 10 datasets, providing code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrey Moskalenko', 'Danil Kuznetsov', 'Irina Dudko', 'Anastasiia Iasakova', 'Nikita Boldyrev', 'Denis Shepelev', 'Andrei Spiridonov', 'Andrey Kuznetsov', 'Vlad Shakhuro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_examples', 'prompt_robustness', 'segmentation', 'adversarial_prompt_generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15123</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD</title><link>https://arxiv.org/abs/2601.15061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a differentially private image generation framework that integrates Error Feedback SGD (EFSGD) with noise injection and a reconstruction loss to improve utility under a fixed privacy budget.&lt;/li&gt;&lt;li&gt;Claims improved image quality and usability of synthetic data compared to prior DP-generation methods, validated on MNIST, Fashion-MNIST, and CelebA benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results across most metrics for both grayscale and RGB images, emphasizing improved trade-offs between privacy and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Ma', 'Jun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'synthetic_data_generation', 'privacy_preservation', 'optimization', 'image_generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15061</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deep Leakage with Generative Flow Matching Denoiser</title><link>https://arxiv.org/abs/2601.15049</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel deep leakage attack for federated learning that uses a generative Flow Matching (FM) prior to guide reconstruction of private client data from model updates.&lt;/li&gt;&lt;li&gt;Demonstrates improved reconstruction fidelity and robustness vs. prior DL methods across pixel-level, perceptual, and feature similarity metrics.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness under realistic FL conditions (different epochs, larger client batch sizes) and common defenses (noise injection, clipping, sparsification).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isaac Baglin', 'Xiatian Zhu', 'Simon Hadfield']&lt;/li&gt;&lt;li&gt;Tags: ['deep leakage', 'federated learning', 'model inversion', 'generative priors', 'privacy attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15049</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Erosion Attack for Adversarial Training to Enhance Semantic Segmentation Robustness</title><link>https://arxiv.org/abs/2601.14950</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EroSeg, an attack for semantic segmentation that selects low-confidence (sensitive) pixels and progressively propagates perturbations to higher-confidence pixels to disrupt semantic consistency.&lt;/li&gt;&lt;li&gt;Introduces EroSeg-AT, an adversarial training framework that uses EroSeg-generated examples to improve model robustness for semantic segmentation.&lt;/li&gt;&lt;li&gt;Reports experimental results showing EroSeg yields more effective attacks than prior methods and that EroSeg-AT enhances robustness under adversarial training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yufei Song', 'Ziqi Zhou', 'Menghao Deng', 'Yifan Hu', 'Shengshan Hu', 'Minghui Li', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial training', 'semantic segmentation', 'robustness', 'attack method']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14950</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption</title><link>https://arxiv.org/abs/2601.14738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VoidFace, a proactive defense against diffusion-based face swapping that injects perturbations at multiple pipeline bottlenecks to disrupt identity transfer.&lt;/li&gt;&lt;li&gt;Techniques include localization disruption and identity erasure to degrade source modeling, decoupling attention and corrupting intermediate diffusion features to prevent identity injection/reconstruction.&lt;/li&gt;&lt;li&gt;Performs adversarial search in the latent manifold with a perceptual-adaptive strategy to maintain visual quality while maximizing attack potency.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing VoidFace outperforms existing defenses across various diffusion-based face swapping models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liqin Wang', 'Qianyue Hu', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['face-swapping defense', 'adversarial perturbation', 'diffusion models', 'identity protection', 'image security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14738</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection</title><link>https://arxiv.org/abs/2601.14625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DEUA: a framework that estimates diffusion epistemic uncertainty (DEU) via a Laplace approximation to assess proximity to the manifold of diffusion-generated images.&lt;/li&gt;&lt;li&gt;Introduces an asymmetric loss to train a classifier with larger margins, aiming to improve generalization and detection of diffusion-generated images.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art detection performance on large-scale benchmarks for diffusion-generated image detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingsong Huang', 'Hui Guo', 'Jing Huang', 'Bing Bai', 'Qi Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'diffusion models', 'uncertainty estimation', 'anomaly detection', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14625</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models</title><link>https://arxiv.org/abs/2601.14330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how erased concepts in diffusion models can be reawakened by perturbing multiple generative factors (text conditions, model parameters, latent states) rather than just prompt optimization.&lt;/li&gt;&lt;li&gt;Proposes LURE (Latent space Unblocking for concept REawakening), which reconstructs latent space to restore severed text-visual associations and guide sampling trajectories to recover erased concepts.&lt;/li&gt;&lt;li&gt;Introduces Gradient Field Orthogonalization to mitigate gradient conflicts and feature entanglement in multi-concept reawakening, and Latent Semantic Identification-Guided Sampling (LSIS) for posterior verification and stability.&lt;/li&gt;&lt;li&gt;Demonstrates the ability to simultaneously and faithfully reawaken multiple erased concepts across various erasure tasks and methods, exposing vulnerabilities in concept-erasure defenses for diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Sun', 'Ziyuan Yang', 'Andrew Beng Jin Teoh', 'Junxu Liu', 'Haibo Hu', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'latent-space attacks', 'model circumvention', 'robustness/vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14330</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative training on AI-generated medical data causes rapid erosion of pathological variability and diagnostic reliability across clinical text, vision-language reports, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain false diagnostic confidence (false reassurance rates tripling to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation indicates AI-generated documentation becomes clinically unreliable after two generations.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies: scaling synthetic volume fails to prevent collapse, whereas mixing real data with quality-aware filtering preserves diversity; argues for policy-mandated human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'data poisoning', 'robustness', 'medical AI', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Manifold-based Sampling for In-Context Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2601.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MB-ICL, a manifold-based in-context demonstration sampling method that uses latent representations and class-aware prototypes to select ICL examples.&lt;/li&gt;&lt;li&gt;Aims to improve hallucination detection (factually incorrect/unsupported content) for LLMs without fine-tuning model parameters.&lt;/li&gt;&lt;li&gt;Shows improved performance on factual verification (FEVER) and hallucination detection (HaluEval), with notable gains on dialogue and summarization tasks and robustness to temperature/model changes.&lt;/li&gt;&lt;li&gt;Offers a training-light, principled alternative to lexical or embedding-similarity heuristics for demonstration selection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bodla Krishna Vamshi', 'Rohan Bhatnagar', 'Haizhao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'in-context-learning', 'defense', 'robustness', 'demonstration-selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.06196</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation</title><link>https://arxiv.org/abs/2510.06605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZeroPrint, a black-box LLM fingerprinting method that approximates input gradients (Jacobian) via zeroth-order estimation to create distinctive model fingerprints.&lt;/li&gt;&lt;li&gt;Argues, via Fisher Information Theory, that input gradients carry more fingerprinting information than raw outputs in non-linear models.&lt;/li&gt;&lt;li&gt;Implements semantic-preserving word substitutions to simulate input perturbations for discrete text and demonstrates state-of-the-art effectiveness and robustness on benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Shao', 'Yiming Li', 'Hongwei Yao', 'Yifei Chen', 'Yuchen Yang', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'model attribution', 'black-box analysis', 'zeroth-order gradient estimation', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06605</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs</title><link>https://arxiv.org/abs/2505.16888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPECTRE, a conditional system-prompt poisoning attack that embeds a 'sleeper agent' into seemingly benign system prompts to trigger targeted malicious outputs only for specific queries.&lt;/li&gt;&lt;li&gt;Operates in a strict black-box setting using a two-stage optimization: global semantic search followed by greedy lexical refinement to craft poisoned prompts.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on open-source models and commercial APIs (e.g., GPT-4o-mini, GPT-3.5) achieving up to ~70% F1 reduction on targeted queries while preserving general utility.&lt;/li&gt;&lt;li&gt;Shows the poisoned prompts evade common defenses (perplexity filters, typo-correction) by leveraging natural noise in real-world prompts and supplies code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viet Pham', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['prompt poisoning', 'supply-chain attack', 'jailbreak/attack', 'black-box adversary', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16888</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure</title><link>https://arxiv.org/abs/2601.10566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Knowledge Immunization Framework (KIF), a representation-aware method that targets internal activation signatures to achieve true knowledge erasure rather than surface-level suppression.&lt;/li&gt;&lt;li&gt;Combines dynamic suppression of subject-specific representations with parameter-efficient adaptation to enable durable unlearning without full model retraining, reporting near-oracle erasure and preserved utility.&lt;/li&gt;&lt;li&gt;Introduces a dual-metric evaluation protocol measuring both surface leakage and latent trace persistence to operationalize the distinction between obfuscation and true erasure.&lt;/li&gt;&lt;li&gt;Evaluates across multiple LLM families (Llama, Mistral, Qwen, DeepSeek) and scales (3B–14B), finding scale-independent true erasure in standard models but architectural differences in reasoning-prior models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Naveed Mahmood', 'Md. Rezaur Rahman Bhuiyan', 'Tasfia Zaman', 'Jareen Tasneem Khondaker', 'Md. Sameer Sakib', 'K. M. Shadman Wadith', 'Nazia Tasnim', 'Farig Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'knowledge erasure', 'representation-based defense', 'LLM safety', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10566</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents</title><link>https://arxiv.org/abs/2512.00332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Assertion-Conditioned Compliance (A-CC), an evaluation paradigm for multi-turn function-calling agents that measures model behavior when exposed to misleading assertions.&lt;/li&gt;&lt;li&gt;Defines two attack vectors: user-sourced assertions (USAs) that test sycophancy toward incorrect user beliefs, and function-sourced assertions (FSAs) that test compliance with stale or contradictory tool/system hints.&lt;/li&gt;&lt;li&gt;Demonstrates that modern multi-turn tool-calling models are highly vulnerable to both USA and FSA scenarios, revealing a provenance-aware vulnerability in deployed agents and a gap in existing benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daud Waqas', 'Aaryamaan Golthi', 'Erika Hayashida', 'Huanzhi Mao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt-injection/sycophancy', 'function-calling agents', 'robustness/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00332</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Construction to Injection: Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end fingerprinting framework for LLMs to control unauthorized redistribution, focusing on imperceptibility and post-modification detectability.&lt;/li&gt;&lt;li&gt;Introduces a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets to reduce accidental activation and statistical detectability.&lt;/li&gt;&lt;li&gt;Presents Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve robustness after model modifications.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing improved practical robustness and detectability of fingerprints under model edits and modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'model watermarks', 'model protection', 'robustness to model modification', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models Encode Semantics and Alignment in Linearly Separable Representations</title><link>https://arxiv.org/abs/2507.09709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of hidden representations in 11 autoregressive LLMs showing high-level semantic and alignment information lies in low-dimensional, linearly separable subspaces.&lt;/li&gt;&lt;li&gt;Separability increases in deeper layers and under prompts that elicit structured reasoning or alignment behavior, even when surface text is unchanged.&lt;/li&gt;&lt;li&gt;Motivates geometry-aware defenses operating in latent space; authors train an MLP probe on final-layer hidden states as a lightweight latent-space guardrail.&lt;/li&gt;&lt;li&gt;The probe substantially improves refusal rates on malicious queries and prompt injection attacks that bypass both model-built safety and token-level filters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Paul Kassianik', 'Blaine Nelson', 'Sajana Weerawardhena', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: ['latent-space defenses', 'prompt injection', 'adversarial detection', 'alignment', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09709</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H3Fusion, a mixture-of-experts fusion mechanism to combine individually aligned LLMs to achieve helpfulness, harmlessness, and honesty simultaneously.&lt;/li&gt;&lt;li&gt;Introduces drift-regularization and gating losses to control alignment as a drift in model representation subspace and canalize expert contributions.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in helpfulness, harmlessness, honesty, and robustness over single aligned models and prior ensemble/model-merging approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'mixture-of-experts', 'model-fusion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NeuroFilter: Privacy Guardrails for Conversational LLM Agents</title><link>https://arxiv.org/abs/2601.14660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that privacy-violating intent elicits linearly separable directions in LLM activation space and uses this to detect violations efficiently.&lt;/li&gt;&lt;li&gt;Proposes NeuroFilter, a guardrail mapping normative privacy violations to activation-space directions to detect attempts to elicit private data, even when semantic filters are bypassed.&lt;/li&gt;&lt;li&gt;Introduces activation velocity to track cumulative drift across multi-turn conversations and detect long-horizon privacy attacks.&lt;/li&gt;&lt;li&gt;Evaluates across &gt;150,000 interactions on models from 7B–70B, reporting strong detection performance, zero false positives on benign prompts, and large computational savings versus LLM-mediated defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saswat Das', 'Ferdinando Fioretto']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'defense', 'activation-space', 'conversational-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14660</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GCG Attack On A Diffusion LLM</title><link>https://arxiv.org/abs/2601.14266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores Greedy Coordinate Gradient (GCG)-style adversarial prompt attacks adapted for diffusion-based LLMs.&lt;/li&gt;&lt;li&gt;Implements and evaluates multiple attack variants (prefix perturbations and suffix-based adversarial generation) against LLaDA, a diffusion language model.&lt;/li&gt;&lt;li&gt;Tests attacks on harmful prompts from the AdvBench dataset to assess robustness and attack surface of diffusion LLMs.&lt;/li&gt;&lt;li&gt;Provides initial findings motivating alternative optimization and evaluation strategies for adversarial analysis in diffusion-generation settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruben Neyroud', 'Sam Corley']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'prompt injection', 'diffusion LLMs', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14266</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks</title><link>https://arxiv.org/abs/2601.15277</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies adversarial sentiment manipulation of news articles using LLMs to evade fake-news detectors and measures its impact on detection performance.&lt;/li&gt;&lt;li&gt;Introduces AdSent: a framework that (1) generates controlled sentiment-based adversarial attacks, (2) analyzes sentiment-shift effects, and (3) proposes a sentiment-agnostic training strategy to improve robustness.&lt;/li&gt;&lt;li&gt;Finds that sentiment shifts significantly degrade detectors (bias toward classifying neutral as real and non-neutral as fake) and demonstrates that AdSent improves accuracy and robustness across benchmarks and unseen adversarial scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahar Tahmasebi', 'Eric M\\"uller-Budack', 'Ralph Ewerth']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'robustness', 'fake-news-detection', 'sentiment-manipulation', 'LLM-based-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15277</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models</title><link>https://arxiv.org/abs/2601.15220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'privacy collapse': benign fine-tuning can cause models to lose contextual privacy safeguards, leaking or misusing sensitive context despite maintaining benchmark performance.&lt;/li&gt;&lt;li&gt;Empirical evidence across six models (closed and open), five fine-tuning datasets (real and controlled), and two task categories (agentic and memory-based) showing widespread vulnerability.&lt;/li&gt;&lt;li&gt;Mechanistic analysis indicates privacy-related representations are uniquely fragile to fine-tuning compared to task-relevant features.&lt;/li&gt;&lt;li&gt;Highlights a critical blind spot in current safety evaluations for deployed/specialized agents and calls for new privacy-focused testing and defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anmol Goel', 'Cornelius Emde', 'Sangdoo Yun', 'Seong Joon Oh', 'Martin Gubri']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-collapse', 'fine-tuning', 'contextual-privacy', 'unintended-memorization', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15220</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora</title><link>https://arxiv.org/abs/2601.14994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates training-data contamination (memorization) in multilingual settings by fine-tuning LLMs on Arabic translations of benchmarks and evaluating on original English tasks.&lt;/li&gt;&lt;li&gt;Extends Tested Slot Guessing with choice-reordering and adds Min-K% probability analysis to detect both behavioral and distributional signs of memorization.&lt;/li&gt;&lt;li&gt;Finds that translation into Arabic can hide conventional contamination signals while models still gain from contaminated data, especially models with stronger Arabic capabilities.&lt;/li&gt;&lt;li&gt;Proposes Translation-Aware Contamination Detection that compares signals across multiple translated benchmark variants to reliably expose contamination that English-only methods miss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaymaa Abbas', 'Nour Shamaa', 'Mariette Awad']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'memorization detection', 'multilingual robustness', 'evaluation security', 'translation-aware detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14994</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination-Free Automatic Question &amp; Answer Generation for Intuitive Learning</title><link>https://arxiv.org/abs/2601.14280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hallucination-free multi-agent framework for automatic MCQ generation that decomposes generation into verifiable stages.&lt;/li&gt;&lt;li&gt;Uses rule-based and LLM-based detection agents plus hallucination scoring metrics to optimize for validity, answerability, and cost-efficiency.&lt;/li&gt;&lt;li&gt;Introduces agent-led iterative refinement employing counterfactual reasoning and chain-of-thought to reduce hallucination risk.&lt;/li&gt;&lt;li&gt;Evaluated on AP-aligned STEM questions and reported &gt;90% reduction in hallucination rates versus baseline while preserving educational style.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas X. Wang', 'Aggelos K. Katsaggelos']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'LLM_safety', 'multi-agent_verification', 'automated_content_verification', 'educational_AI_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14280</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues</title><link>https://arxiv.org/abs/2601.14269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-turn stress-testing framework for assessing safety boundary erosion in mental-health LLM dialogues using two pressure methods: static progression and adaptive probing.&lt;/li&gt;&lt;li&gt;Evaluates three state-of-the-art LLMs with 50 virtual patient profiles up to 20 dialogue turns, finding frequent safety violations and that adaptive probing accelerates boundary breaches (average turns to violation reduced from 9.21 to 4.64).&lt;/li&gt;&lt;li&gt;Identifies making definitive or zero-risk promises as the primary failure mode and argues that single-turn safety tests are insufficient to capture long-dialogue vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youyou Cheng', 'Zhuangwei Kang', 'Kerry Jiang', 'Chenyu Sun', 'Qiyang Pan']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'dialogue-safety', 'adversarial-testing', 'LLM-robustness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14269</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative training on AI-generated medical data causes rapid erosion of pathological variability and diagnostic reliability across clinical text, vision-language reports, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain false diagnostic confidence (false reassurance rates tripling to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation indicates AI-generated documentation becomes clinically unreliable after two generations.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies: scaling synthetic volume fails to prevent collapse, whereas mixing real data with quality-aware filtering preserves diversity; argues for policy-mandated human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'data poisoning', 'robustness', 'medical AI', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure</title><link>https://arxiv.org/abs/2601.10566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Knowledge Immunization Framework (KIF), a representation-aware method that targets internal activation signatures to achieve true knowledge erasure rather than surface-level suppression.&lt;/li&gt;&lt;li&gt;Combines dynamic suppression of subject-specific representations with parameter-efficient adaptation to enable durable unlearning without full model retraining, reporting near-oracle erasure and preserved utility.&lt;/li&gt;&lt;li&gt;Introduces a dual-metric evaluation protocol measuring both surface leakage and latent trace persistence to operationalize the distinction between obfuscation and true erasure.&lt;/li&gt;&lt;li&gt;Evaluates across multiple LLM families (Llama, Mistral, Qwen, DeepSeek) and scales (3B–14B), finding scale-independent true erasure in standard models but architectural differences in reasoning-prior models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Syed Naveed Mahmood', 'Md. Rezaur Rahman Bhuiyan', 'Tasfia Zaman', 'Jareen Tasneem Khondaker', 'Md. Sameer Sakib', 'K. M. Shadman Wadith', 'Nazia Tasnim', 'Farig Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'knowledge erasure', 'representation-based defense', 'LLM safety', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10566</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PrivTune is a split-learning based framework for privacy-preserving fine-tuning of LLMs via device-cloud collaboration that injects optimized noise into token representations.&lt;/li&gt;&lt;li&gt;It formulates noise injection as an optimization problem to align defense and utility goals, adapts parameters of a d_chi-privacy noise distribution, and scales noise by token importance to reduce distortion.&lt;/li&gt;&lt;li&gt;Evaluated against embedding inversion and attribute inference attacks across classification and generation tasks, showing large reductions in attack success with minimal utility loss compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'split-learning', 'd_chi-privacy', 'embedding-inversion', 'attribute-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Construction to Injection: Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end fingerprinting framework for LLMs to control unauthorized redistribution, focusing on imperceptibility and post-modification detectability.&lt;/li&gt;&lt;li&gt;Introduces a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets to reduce accidental activation and statistical detectability.&lt;/li&gt;&lt;li&gt;Presents Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve robustness after model modifications.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing improved practical robustness and detectability of fingerprints under model edits and modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'model watermarks', 'model protection', 'robustness to model modification', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Language Models Encode Semantics and Alignment in Linearly Separable Representations</title><link>https://arxiv.org/abs/2507.09709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of hidden representations in 11 autoregressive LLMs showing high-level semantic and alignment information lies in low-dimensional, linearly separable subspaces.&lt;/li&gt;&lt;li&gt;Separability increases in deeper layers and under prompts that elicit structured reasoning or alignment behavior, even when surface text is unchanged.&lt;/li&gt;&lt;li&gt;Motivates geometry-aware defenses operating in latent space; authors train an MLP probe on final-layer hidden states as a lightweight latent-space guardrail.&lt;/li&gt;&lt;li&gt;The probe substantially improves refusal rates on malicious queries and prompt injection attacks that bypass both model-built safety and token-level filters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Paul Kassianik', 'Blaine Nelson', 'Sajana Weerawardhena', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: ['latent-space defenses', 'prompt injection', 'adversarial detection', 'alignment', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09709</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H3Fusion, a mixture-of-experts fusion mechanism to combine individually aligned LLMs to achieve helpfulness, harmlessness, and honesty simultaneously.&lt;/li&gt;&lt;li&gt;Introduces drift-regularization and gating losses to control alignment as a drift in model representation subspace and canalize expert contributions.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in helpfulness, harmlessness, honesty, and robustness over single aligned models and prior ensemble/model-merging approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'mixture-of-experts', 'model-fusion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DRGW: Learning Disentangled Representations for Robust Graph Watermarking</title><link>https://arxiv.org/abs/2601.13569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRGW, a graph watermarking framework using disentangled representation learning to separate invariant structural features from a statistically independent watermark carrier.&lt;/li&gt;&lt;li&gt;Key components: an adversarially trained encoder for perturbation-invariant representations, a graph-aware invertible neural network for lossless watermark embedding/extraction, and a structure-aware editor to map latent changes to discrete graph edits.&lt;/li&gt;&lt;li&gt;Focuses on robustness and transparency of watermarks against structural perturbations; evaluates effectiveness on benchmark graph datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiasen Li', 'Yanwei Liu', 'Zhuoyi Shang', 'Xiaoyan Gu', 'Weiping Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'graph security', 'robustness', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.13569</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</title><link>https://arxiv.org/abs/2601.09261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Epistemic Identifiability under Unobservable Reliability (EIUR), where feedback reliability is latent and learner-generated data can hide systematic corruption.&lt;/li&gt;&lt;li&gt;Proposes a Monitor-Trust-Regulator (MTR) framework and a concrete self-diagnosis mechanism that maintains an experience-trust variable to modulate learning without external reliability labels.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that self-diagnosis improves identification of unreliable experiences and enables recovery under systematically corrupted rewards in RL; shows in supervised learning that accuracy recovery can mask persistent epistemic errors detectable via introspective diagnostics.&lt;/li&gt;&lt;li&gt;Argues metacognitive regulation as a practical defense for intrinsic reliability assessment in autonomous learners operating under unobservable feedback reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng Zhang', 'Zhenjie Yao', 'Kai Li', 'Lei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness', 'Data poisoning / corrupted feedback', 'Self-monitoring / metacognition', 'Reinforcement learning safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.09261</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Manifold-based Sampling for In-Context Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2601.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MB-ICL, a manifold-based in-context demonstration sampling method that uses latent representations and class-aware prototypes to select ICL examples.&lt;/li&gt;&lt;li&gt;Aims to improve hallucination detection (factually incorrect/unsupported content) for LLMs without fine-tuning model parameters.&lt;/li&gt;&lt;li&gt;Shows improved performance on factual verification (FEVER) and hallucination detection (HaluEval), with notable gains on dialogue and summarization tasks and robustness to temperature/model changes.&lt;/li&gt;&lt;li&gt;Offers a training-light, principled alternative to lexical or embedding-similarity heuristics for demonstration selection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bodla Krishna Vamshi', 'Rohan Bhatnagar', 'Haizhao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'in-context-learning', 'defense', 'robustness', 'demonstration-selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.06196</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Constrained Black-Box Attacks Against Cooperative Multi-Agent Reinforcement Learning</title><link>https://arxiv.org/abs/2508.09275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces constrained black-box test-time attacks against cooperative multi-agent RL where the adversary can only collect and perturb observations (or sometimes has no access at all).&lt;/li&gt;&lt;li&gt;Proposes a method that misaligns victim agents' perceived environment to degrade team performance under realistic threat models (no policy weights or surrogate training).&lt;/li&gt;&lt;li&gt;Empirically validated across 3 benchmarks and 22 environments, showing effectiveness across diverse algorithms and environments.&lt;/li&gt;&lt;li&gt;Demonstrates high sample efficiency (≈1,000 samples) compared to prior methods that required millions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Andam', 'Jamal Bentahar', 'Mustapha Hedabou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'black-box attacks', 'multi-agent reinforcement learning', 'test-time evasion', 'sample-efficient attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09275</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deceptive Sequential Decision-Making via Regularized Policy Optimization</title><link>https://arxiv.org/abs/2501.18803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models autonomous systems as MDPs and assumes adversaries perform inverse reinforcement learning to infer the system's reward.&lt;/li&gt;&lt;li&gt;Proposes three regularized policy optimization strategies—diversionary, targeted, and equivocal deception—to actively mislead adversaries about the true reward.&lt;/li&gt;&lt;li&gt;Provides formulations to implement each deception type, derives analytical bounds on the induced loss in accumulated reward, and evaluates methods in a multi-agent setting.&lt;/li&gt;&lt;li&gt;Empirically shows deception steers adversary beliefs while preserving at least ~98% of optimal non-deceptive accumulated reward.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yerin Kim', 'Alexander Benvenuti', 'Bo Chen', 'Mustafa Karabag', 'Abhishek Kulkarni', 'Nathaniel D. Bastian', 'Ufuk Topcu', 'Matthew Hale']&lt;/li&gt;&lt;li&gt;Tags: ['deception', 'inverse reinforcement learning', 'policy optimization', 'privacy/anti-inference', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18803</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses</title><link>https://arxiv.org/abs/2410.08864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the trade-off between backdoor-based watermarks, adversarial defenses, and a third option — transferable attacks — via an interactive verifier/prover protocol.&lt;/li&gt;&lt;li&gt;Proves a general result: for all learning tasks, at least one of the three exists (watermark, adversarial defense, or transferable attack).&lt;/li&gt;&lt;li&gt;Constructs a transferable attack using cryptographic tools (fully homomorphic encryption) and proves its necessity in the trade-off.&lt;/li&gt;&lt;li&gt;Provides positive results: tasks with bounded VC-dimension admit adversarial defenses against all attackers; a subclass admits watermarks secure against fast adversaries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Grzegorz G{\\l}uch', 'Berkant Turan', 'Sai Ganesh Nagarajan', 'Sebastian Pokutta']&lt;/li&gt;&lt;li&gt;Tags: ['backdoors/watermarks', 'transferable attacks', 'adversarial defenses', 'cryptographic attacks (FHE)', 'theoretical security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08864</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SpooFL: Spoofing Federated Learning</title><link>https://arxiv.org/abs/2601.15055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames federated learning defense as a spoofing problem: return convincing but synthetic samples to attackers so they believe they recovered true training data.&lt;/li&gt;&lt;li&gt;Introduces SpooFL, which uses a generative model trained on an external, non-overlapping dataset to produce unrelated yet plausible samples that misdirect Deep Leakage (DL) attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that spoofing prevents meaningful data leakage while maintaining FL training performance and outperforms or complements existing DL defenses in evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isaac Baglin', 'Xiatian Zhu', 'Simon Hadfield']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'deep-leakage', 'spoofing-defense', 'generative-models', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15055</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM Security and Safety: Insights from Homotopy-Inspired Prompt Obfuscation</title><link>https://arxiv.org/abs/2601.14528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a homotopy-inspired prompt obfuscation framework to systematically probe and influence latent behaviors of LLMs.&lt;/li&gt;&lt;li&gt;Runs large-scale experiments (15,732 prompts, 10,000 high-priority cases) across multiple models (LLama, Deepseek, KIMI for code generation, Claude) to evaluate vulnerabilities in existing safeguards.&lt;/li&gt;&lt;li&gt;Reports insights into weaknesses of current defenses and proposes a principled framework for analyzing and mitigating prompt-based attacks, emphasizing detection and robustness improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luis Lazo', 'Hamed Jelodar', 'Roozbeh Razavi-Far']&lt;/li&gt;&lt;li&gt;Tags: ['prompt obfuscation', 'jailbreaking/prompt injection', 'vulnerability analysis', 'defense/robustness', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14528</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Uncovering and Understanding FPR Manipulation Attack in Industrial IoT Networks</title><link>https://arxiv.org/abs/2601.14505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a novel FPR manipulation attack (FPA) that perturbs benign MQTT packets at packet-level to induce false positives in ML-based NIDS for industrial IoT.&lt;/li&gt;&lt;li&gt;Attack uses protocol-specific, non-gradient perturbations and achieves high success rates (≈80.19%–100%).&lt;/li&gt;&lt;li&gt;Quantifies operational impact (SOC alert investigation delays) and uses statistical and XAI analyses to explain success; also tests adversarial training as a mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Shamim Ahsan', 'Peng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'network-intrusion-detection', 'IoT-security', 'protocol-level-attack', 'adversarial-training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14505</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs</title><link>https://arxiv.org/abs/2601.14340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Turn-based Structural Trigger (TST), a prompt-free backdoor that activates based on dialogue turn index rather than user-visible text.&lt;/li&gt;&lt;li&gt;Evaluates TST on four open-source LLMs achieving ~99.52% ASR with minimal utility degradation and robustness under five representative defenses (~98.04% ASR).&lt;/li&gt;&lt;li&gt;Shows generalization across instruction datasets and emphasizes dialogue structure as an under-studied attack surface in multi-turn systems, calling for structure-aware auditing and mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyang Lu', 'Jinwen He', 'Yue Zhao', 'Kai Chen', 'Ruigang Liang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model poisoning', 'conversational LLMs', 'prompt-free trigger']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14340</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models</title><link>https://arxiv.org/abs/2601.14330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how erased concepts in diffusion models can be reawakened by perturbing multiple generative factors (text conditions, model parameters, latent states) rather than just prompt optimization.&lt;/li&gt;&lt;li&gt;Proposes LURE (Latent space Unblocking for concept REawakening), which reconstructs latent space to restore severed text-visual associations and guide sampling trajectories to recover erased concepts.&lt;/li&gt;&lt;li&gt;Introduces Gradient Field Orthogonalization to mitigate gradient conflicts and feature entanglement in multi-concept reawakening, and Latent Semantic Identification-Guided Sampling (LSIS) for posterior verification and stability.&lt;/li&gt;&lt;li&gt;Demonstrates the ability to simultaneously and faithfully reawaken multiple erased concepts across various erasure tasks and methods, exposing vulnerabilities in concept-erasure defenses for diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyu Sun', 'Ziyuan Yang', 'Andrew Beng Jin Teoh', 'Junxu Liu', 'Haibo Hu', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'latent-space attacks', 'model circumvention', 'robustness/vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14330</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Auditing Language Model Unlearning via Information Decomposition</title><link>https://arxiv.org/abs/2601.15111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an interpretable, information-theoretic audit for machine unlearning in language models using Partial Information Decomposition (PID) to decompose mutual information with forgotten data.&lt;/li&gt;&lt;li&gt;Shows that redundant/shared information in internal representations often persists after unlearning and remains linearly decodable, correlating with susceptibility to adversarial reconstruction attacks.&lt;/li&gt;&lt;li&gt;Introduces a representation-based risk score to detect/abstain on sensitive inputs at inference as a practical mitigation to reduce privacy leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anmol Goel', 'Alan Ritter', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy leakage', 'information-theoretic audit', 'reconstruction attacks', 'representation-level defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15111</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HyperNet-Adaptation for Diffusion-Based Test Case Generation</title><link>https://arxiv.org/abs/2601.15041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HyNeA, a diffusion-based generative test-case generator that uses hypernetworks to enable dataset-free, direct control over generation to produce realistic failure-inducing inputs.&lt;/li&gt;&lt;li&gt;Introduces an instance-level tuning/training strategy to identify and target failure cases without requiring failure-labeled datasets or architecture-specific conditioning.&lt;/li&gt;&lt;li&gt;Claims improved controllability and test diversity compared to prior generative testers, while reducing computational cost versus search-based generation methods.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to domains lacking failure-labeled training data, enabling scalable testing/red-teaming of models for robustness and functional failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oliver Wei{\\ss}l', 'Vincenzo Riccio', 'Severin Kacianka', 'Andrea Stocco']&lt;/li&gt;&lt;li&gt;Tags: ['generative testing', 'diffusion models', 'adversarial testing / red teaming', 'hypernetworks', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15041</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Re-understanding Graph Unlearning through Memorization</title><link>https://arxiv.org/abs/2601.14694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MGU (Memorization-guided Graph Unlearning), a framework that uses GNN memorization to assess unlearning difficulty and guide unlearning strategies.&lt;/li&gt;&lt;li&gt;Introduces an adaptive unlearning strategy that dynamically adjusts objectives based on difficulty, and a comprehensive evaluation protocol aligned with practical requirements.&lt;/li&gt;&lt;li&gt;Demonstrates improved forgetting quality, computational efficiency, and utility preservation over state-of-the-art baselines on ten real-world graph datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengfei Ding', 'Yan Wang', 'Guanfeng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph unlearning', 'privacy-preserving ML', 'GNN robustness', 'data deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14694</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness</title><link>https://arxiv.org/abs/2601.14519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a probabilistic metric parameterized by concentration factor κ that interpolates between isotropic (random) noise and adversarial (directional) perturbations.&lt;/li&gt;&lt;li&gt;Proposes an attack strategy aimed at operating in regimes statistically closer to uniform noise to assess how adversarial perturbations estimate noisy risk.&lt;/li&gt;&lt;li&gt;Empirically benchmarks common adversarial attacks on ImageNet and CIFAR-10, identifying when adversarial success corresponds to noisy risk and when it is an atypical worst-case.&lt;/li&gt;&lt;li&gt;Provides guidance for safety-oriented evaluation by characterizing limits of adversarial attacks as proxies for statistical robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Rossolini']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness evaluation', 'attack analysis', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14519</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity</title><link>https://arxiv.org/abs/2601.14300</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes hard-label black-box attacks as gradient sign recovery from top-1 label feedback and provides a unified theoretical perspective.&lt;/li&gt;&lt;li&gt;Proposes a new attack framework combining a zero-query frequency-domain initialization with Pattern-Driven Optimization (PDO), with theoretical guarantees on cosine similarity to true gradient sign and improved query complexity.&lt;/li&gt;&lt;li&gt;Empirically outperforms state-of-the-art hard-label attacks on CIFAR-10, ImageNet, ObjectNet, adversarially trained models, commercial APIs, and CLIP-based models; generalizes to corrupted/biomedical data and dense prediction tasks.&lt;/li&gt;&lt;li&gt;Demonstrates ability to bypass a stateful defense (Blacklight) with a 0% detection rate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Liu', 'Leo Yu Zhang', 'Fengpeng Li', 'Isao Echizen', 'Jiantao Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['hard-label black-box attacks', 'gradient sign estimation', 'query-efficient adversarial attacks', 'adversarial examples', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14300</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GCG Attack On A Diffusion LLM</title><link>https://arxiv.org/abs/2601.14266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Greedy Coordinate Gradient (GCG)-style adversarial prompt attacks to a diffusion-based LLM (LLaDA).&lt;/li&gt;&lt;li&gt;Evaluates multiple attack variants (prefix perturbations, suffix-based adversarial generation) against harmful prompts from the AdvBench dataset.&lt;/li&gt;&lt;li&gt;Provides exploratory robustness analysis of diffusion language models and highlights need for new optimization and evaluation strategies for adversarial analysis in this setting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruben Neyroud', 'Sam Corley']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'prompt attacks', 'diffusion language models', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14266</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title><link>https://arxiv.org/abs/2601.12946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that iterative training on AI-generated medical data causes rapid erosion of pathological variability and diagnostic reliability across clinical text, vision-language reports, and medical image synthesis.&lt;/li&gt;&lt;li&gt;Finds rare but critical findings (e.g., pneumothorax, effusions) disappear and demographic distributions skew, while models maintain false diagnostic confidence (false reassurance rates tripling to ~40%).&lt;/li&gt;&lt;li&gt;Blinded physician evaluation indicates AI-generated documentation becomes clinically unreliable after two generations.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies: scaling synthetic volume fails to prevent collapse, whereas mixing real data with quality-aware filtering preserves diversity; argues for policy-mandated human oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'data poisoning', 'robustness', 'medical AI', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.12946</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DNF: Dual-Layer Nested Fingerprinting for Large Language Model Intellectual Property Protection</title><link>https://arxiv.org/abs/2601.08223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Dual-Layer Nested Fingerprinting (DNF): a hierarchical backdoor approach that combines domain-specific stylistic cues with implicit semantic triggers for black-box LLM fingerprinting.&lt;/li&gt;&lt;li&gt;Evaluates DNF on multiple LLMs (Mistral-7B, LLaMA-3-8B-Instruct, Falcon3-7B-Instruct), showing strong fingerprint activation, low-perplexity triggers, and preserved downstream utility.&lt;/li&gt;&lt;li&gt;Claims robustness to fingerprint detection attacks and resilience to incremental fine-tuning and model merging, positioning DNF as a stealthy, practical method for ownership verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Xu', 'Yiran Zhao', 'Mengting Zhong', 'Dezhang Kong', 'Changting Lin', 'Tong Qiao', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'model fingerprinting', 'IP protection', 'LLM security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.08223</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Manifold-based Sampling for In-Context Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2601.06196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MB-ICL, a manifold-based in-context demonstration sampling method that uses latent representations and class-aware prototypes to select ICL examples.&lt;/li&gt;&lt;li&gt;Aims to improve hallucination detection (factually incorrect/unsupported content) for LLMs without fine-tuning model parameters.&lt;/li&gt;&lt;li&gt;Shows improved performance on factual verification (FEVER) and hallucination detection (HaluEval), with notable gains on dialogue and summarization tasks and robustness to temperature/model changes.&lt;/li&gt;&lt;li&gt;Offers a training-light, principled alternative to lexical or embedding-similarity heuristics for demonstration selection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bodla Krishna Vamshi', 'Rohan Bhatnagar', 'Haizhao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'in-context-learning', 'defense', 'robustness', 'demonstration-selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.06196</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title><link>https://arxiv.org/abs/2512.11771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes adversarial threat models (white-box and black-box) and two attack goals: fingerprint removal and fingerprint forgery.&lt;/li&gt;&lt;li&gt;Implements five attack strategies and evaluates 14 representative fingerprinting methods (RGB, frequency, learned-feature) on 12 state-of-the-art image generators.&lt;/li&gt;&lt;li&gt;Finds removal attacks highly effective (often &gt;80% success white-box, &gt;50% black-box); forgery is harder but variable; no method is robust and accurate across all threat models.&lt;/li&gt;&lt;li&gt;Identifies a utility–robustness trade-off and highlights promising directions for improving fingerprint robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yao', 'Marc Juarez']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model attribution/fingerprinting', 'robustness evaluation', 'attack/defense analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11771</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</title><link>https://arxiv.org/abs/2512.10758</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a theoretically grounded framework arguing that interconnected, multi-step problems and semi-structured tasks are more AI-resilient than modular or fully open-ended assessments.&lt;/li&gt;&lt;li&gt;Provides two formal propositions about why interconnected problems inhibit trivial AI delegation and why semi-structured tasks yield more reliable competency measures than open-ended projects.&lt;/li&gt;&lt;li&gt;Validates claims empirically in three university data-science courses (N=117), showing AI-induced score inflation on modular homework and that interconnected projects remain aligned with assessments while resisting AI assistance.&lt;/li&gt;&lt;li&gt;Offers a practical design procedure for educators to build assessments that promote deeper engagement and resist trivial AI delegation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaihua Ding']&lt;/li&gt;&lt;li&gt;Tags: ['AI-resilience', 'academic-integrity', 'assessment-design', 'empirical-evaluation', 'adversarial-use (cheating)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10758</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title><link>https://arxiv.org/abs/2512.08809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PrivTune is a split-learning based framework for privacy-preserving fine-tuning of LLMs via device-cloud collaboration that injects optimized noise into token representations.&lt;/li&gt;&lt;li&gt;It formulates noise injection as an optimization problem to align defense and utility goals, adapts parameters of a d_chi-privacy noise distribution, and scales noise by token importance to reduce distortion.&lt;/li&gt;&lt;li&gt;Evaluated against embedding inversion and attribute inference attacks across classification and generation tasks, showing large reductions in attack success with minimal utility loss compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Weixiang Han', 'Chengjun Cai', 'Xingliang Yuan', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'split-learning', 'd_chi-privacy', 'embedding-inversion', 'attribute-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08809</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents</title><link>https://arxiv.org/abs/2512.00332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Assertion-Conditioned Compliance (A-CC), an evaluation paradigm for multi-turn function-calling agents that measures model behavior when exposed to misleading assertions.&lt;/li&gt;&lt;li&gt;Defines two attack vectors: user-sourced assertions (USAs) that test sycophancy toward incorrect user beliefs, and function-sourced assertions (FSAs) that test compliance with stale or contradictory tool/system hints.&lt;/li&gt;&lt;li&gt;Demonstrates that modern multi-turn tool-calling models are highly vulnerable to both USA and FSA scenarios, revealing a provenance-aware vulnerability in deployed agents and a gap in existing benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daud Waqas', 'Aaryamaan Golthi', 'Erika Hayashida', 'Huanzhi Mao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt-injection/sycophancy', 'function-calling agents', 'robustness/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00332</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</title><link>https://arxiv.org/abs/2510.10111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICFC, a training-free, in-context forensic pipeline using multi-modal large language models (MLLMs) to detect and localize image manipulations with interpretable outputs.&lt;/li&gt;&lt;li&gt;Key components: objectified rule construction with adaptive filtering to build a knowledge base, and a multi-step progressive reasoning pipeline that goes from coarse proposals to fine-grained pixel-level localization and textual explanations.&lt;/li&gt;&lt;li&gt;Demonstrates that ICFC outperforms prior training-free methods and achieves competitive or superior results compared to some weakly- and fully-supervised approaches across multiple IML benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Chen', 'Bin Liu', 'Changtao Miao', 'Xinghao Wang', 'Yi Li', 'Tao Gong', 'Qi Chu', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'forensic analysis', 'MLLM-based defenses', 'training-free methods', 'interpretable detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10111</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation</title><link>https://arxiv.org/abs/2510.06605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZeroPrint, a black-box LLM fingerprinting method that approximates input gradients (Jacobian) via zeroth-order estimation to create distinctive model fingerprints.&lt;/li&gt;&lt;li&gt;Argues, via Fisher Information Theory, that input gradients carry more fingerprinting information than raw outputs in non-linear models.&lt;/li&gt;&lt;li&gt;Implements semantic-preserving word substitutions to simulate input perturbations for discrete text and demonstrates state-of-the-art effectiveness and robustness on benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Shao', 'Yiming Li', 'Hongwei Yao', 'Yifei Chen', 'Yuchen Yang', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'model attribution', 'black-box analysis', 'zeroth-order gradient estimation', 'IP protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06605</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Construction to Injection: Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end fingerprinting framework for LLMs to control unauthorized redistribution, focusing on imperceptibility and post-modification detectability.&lt;/li&gt;&lt;li&gt;Introduces a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets to reduce accidental activation and statistical detectability.&lt;/li&gt;&lt;li&gt;Presents Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve robustness after model modifications.&lt;/li&gt;&lt;li&gt;Provides extensive experiments showing improved practical robustness and detectability of fingerprints under model edits and modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'model watermarks', 'model protection', 'robustness to model modification', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs</title><link>https://arxiv.org/abs/2505.16888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SPECTRE, a conditional system-prompt poisoning attack that embeds a 'sleeper agent' into seemingly benign system prompts to trigger targeted malicious outputs only for specific queries.&lt;/li&gt;&lt;li&gt;Operates in a strict black-box setting using a two-stage optimization: global semantic search followed by greedy lexical refinement to craft poisoned prompts.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on open-source models and commercial APIs (e.g., GPT-4o-mini, GPT-3.5) achieving up to ~70% F1 reduction on targeted queries while preserving general utility.&lt;/li&gt;&lt;li&gt;Shows the poisoned prompts evade common defenses (perplexity filters, typo-correction) by leveraging natural noise in real-world prompts and supplies code/data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viet Pham', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['prompt poisoning', 'supply-chain attack', 'jailbreak/attack', 'black-box adversary', 'defense evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16888</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</title><link>https://arxiv.org/abs/2501.16534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes extracting surrogate safety classifiers embedded in aligned LLMs by constructing candidate classifiers from subsets of the model.&lt;/li&gt;&lt;li&gt;Evaluates surrogate fidelity (F1 &gt; 80% using as little as 20% of model) and measures transferability of adversarial jailbreak attacks to the full LLM.&lt;/li&gt;&lt;li&gt;Demonstrates surrogate-based attacks are more effective and efficient (e.g., 50% Llama 2 surrogate yields 70% ASR vs 22% ASR when attacking the full LLM directly).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jean-Charles Noirot Ferrand', 'Yohan Beugin', 'Eric Pauley', 'Ryan Sheatsley', 'Patrick McDaniel']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'model extraction', 'adversarial transfer', 'safety classifier', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.16534</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Neural Honeytrace: Plug&amp;Play Watermarking Framework against Model Extraction Attacks</title><link>https://arxiv.org/abs/2501.09328</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Neural Honeytrace, a plug-and-play, training-free triggerable watermarking framework to enable ownership verification against model extraction attacks.&lt;/li&gt;&lt;li&gt;Reformulates watermark transmission from an information-theoretic perspective and designs a multi-step, training-free embedding strategy that leverages the long-tailed effect of backdoor learning.&lt;/li&gt;&lt;li&gt;Claims strong empirical gains: reduces the average number of queries needed for worst-case t-test-based ownership verification to as low as 2% of existing methods while incurring zero training cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixiao Xu', 'Binxing Fang', 'Rui Wang', 'Yinghai Zhou', 'Yuan Liu', 'Mohan Li', 'Zhihong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['model watermarking', 'model extraction defense', 'backdoor-based watermarking', 'ownership verification', 'plug-and-play security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09328</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes H3Fusion, a mixture-of-experts fusion mechanism to combine individually aligned LLMs to achieve helpfulness, harmlessness, and honesty simultaneously.&lt;/li&gt;&lt;li&gt;Introduces drift-regularization and gating losses to control alignment as a drift in model representation subspace and canalize expert contributions.&lt;/li&gt;&lt;li&gt;Reports empirical improvements in helpfulness, harmlessness, honesty, and robustness over single aligned models and prior ensemble/model-merging approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'mixture-of-experts', 'model-fusion', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses</title><link>https://arxiv.org/abs/2410.08864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the trade-off between backdoor-based watermarks, adversarial defenses, and a third option — transferable attacks — via an interactive verifier/prover protocol.&lt;/li&gt;&lt;li&gt;Proves a general result: for all learning tasks, at least one of the three exists (watermark, adversarial defense, or transferable attack).&lt;/li&gt;&lt;li&gt;Constructs a transferable attack using cryptographic tools (fully homomorphic encryption) and proves its necessity in the trade-off.&lt;/li&gt;&lt;li&gt;Provides positive results: tasks with bounded VC-dimension admit adversarial defenses against all attackers; a subclass admits watermarks secure against fast adversaries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Grzegorz G{\\l}uch', 'Berkant Turan', 'Sai Ganesh Nagarajan', 'Sebastian Pokutta']&lt;/li&gt;&lt;li&gt;Tags: ['backdoors/watermarks', 'transferable attacks', 'adversarial defenses', 'cryptographic attacks (FHE)', 'theoretical security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08864</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Functional Correctness: Exploring Hallucinations in LLM-Generated Code</title><link>https://arxiv.org/abs/2404.00971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive taxonomy of hallucinations in LLM-generated code (3 primary categories, 12 specific categories) based on thematic analysis.&lt;/li&gt;&lt;li&gt;Analyzes distribution of hallucination types across different LLMs and benchmarks and investigates causes and impacts to inform mitigation.&lt;/li&gt;&lt;li&gt;Explores lightweight, training-free mitigation techniques via prompt-enhancing methods to improve correctness and reliability of generated code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fang Liu', 'Yang Liu', 'Lin Shi', 'Zhen Yang', 'Li Zhang', 'Xiaoli Lian', 'Zhongqi Li', 'Yuchi Ma']&lt;/li&gt;&lt;li&gt;Tags: ['code-hallucination', 'robustness', 'prompt-engineering', 'mitigation', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.00971</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</title><link>https://arxiv.org/abs/2509.01631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a neuron-level interpretability method that projects internal representations into a vocabulary-aligned space to identify safety-related 'knowledge neurons'.&lt;/li&gt;&lt;li&gt;Demonstrates that manipulating activations of these safety neurons can effectively control model behavior (reported mean ASR &gt; 97%).&lt;/li&gt;&lt;li&gt;Introduces SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to reduce jailbreak success rates across multiple LLMs and outperforms four baseline defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chongwen Zhao', 'Yutong Ke', 'Kaizhu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'neuron interpretability', 'defense', 'fine-tuning', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01631</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation</title><link>https://arxiv.org/abs/2601.15123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness of promptable segmentation models (e.g., SAM) to natural variations in bounding-box prompts, showing substantial user-induced variability in segmentation quality.&lt;/li&gt;&lt;li&gt;Collects real user bounding-box annotations via a controlled user study to characterize natural prompt noise.&lt;/li&gt;&lt;li&gt;Formulates robustness evaluation as a white-box optimization over bounding-box prompt space and proposes BREPS to generate adversarial (error-maximizing or error-minimizing) yet natural-looking bounding boxes.&lt;/li&gt;&lt;li&gt;Benchmarks state-of-the-art promptable segmentation models across 10 datasets, providing code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrey Moskalenko', 'Danil Kuznetsov', 'Irina Dudko', 'Anastasiia Iasakova', 'Nikita Boldyrev', 'Denis Shepelev', 'Andrei Spiridonov', 'Andrey Kuznetsov', 'Vlad Shakhuro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_examples', 'prompt_robustness', 'segmentation', 'adversarial_prompt_generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15123</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Auditing Language Model Unlearning via Information Decomposition</title><link>https://arxiv.org/abs/2601.15111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an interpretable, information-theoretic audit for machine unlearning in language models using Partial Information Decomposition (PID) to decompose mutual information with forgotten data.&lt;/li&gt;&lt;li&gt;Shows that redundant/shared information in internal representations often persists after unlearning and remains linearly decodable, correlating with susceptibility to adversarial reconstruction attacks.&lt;/li&gt;&lt;li&gt;Introduces a representation-based risk score to detect/abstain on sensitive inputs at inference as a practical mitigation to reduce privacy leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anmol Goel', 'Alan Ritter', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy leakage', 'information-theoretic audit', 'reconstruction attacks', 'representation-level defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15111</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Agentic Operationalization of DISARM for FIMI Investigation on Social Media</title><link>https://arxiv.org/abs/2601.15109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an agent-based, framework-agnostic operationalization of the DISARM taxonomy to investigate Foreign Information Manipulation and Interference (FIMI) on social media.&lt;/li&gt;&lt;li&gt;Implements a multi-agent pipeline where specialized AI agents detect candidate manipulative behaviors and map them to DISARM taxonomies in a transparent, scalable way.&lt;/li&gt;&lt;li&gt;Evaluates the approach on two real-world datasets annotated by domain practitioners and demonstrates effectiveness in scaling manual FIMI analysis and improving situational awareness/interoperability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Tseng', 'Juan Carlos Toledano', 'Bart De Clerck', 'Yuliia Dukach', 'Phil Tinn']&lt;/li&gt;&lt;li&gt;Tags: ['FIMI', 'disinformation-detection', 'adversarial-information-operations', 'agent-based-systems', 'operational-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15109</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD</title><link>https://arxiv.org/abs/2601.15061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a differentially private image generation framework that integrates Error Feedback SGD (EFSGD) with noise injection and a reconstruction loss to improve utility under a fixed privacy budget.&lt;/li&gt;&lt;li&gt;Claims improved image quality and usability of synthetic data compared to prior DP-generation methods, validated on MNIST, Fashion-MNIST, and CelebA benchmarks.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art results across most metrics for both grayscale and RGB images, emphasizing improved trade-offs between privacy and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiwei Ma', 'Jun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'synthetic_data_generation', 'privacy_preservation', 'optimization', 'image_generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.15061</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora</title><link>https://arxiv.org/abs/2601.14994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates training-data contamination (memorization) in multilingual settings by fine-tuning LLMs on Arabic translations of benchmarks and evaluating on original English tasks.&lt;/li&gt;&lt;li&gt;Extends Tested Slot Guessing with choice-reordering and adds Min-K% probability analysis to detect both behavioral and distributional signs of memorization.&lt;/li&gt;&lt;li&gt;Finds that translation into Arabic can hide conventional contamination signals while models still gain from contaminated data, especially models with stronger Arabic capabilities.&lt;/li&gt;&lt;li&gt;Proposes Translation-Aware Contamination Detection that compares signals across multiple translated benchmark variants to reliably expose contamination that English-only methods miss.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaymaa Abbas', 'Nour Shamaa', 'Mariette Awad']&lt;/li&gt;&lt;li&gt;Tags: ['data contamination', 'memorization detection', 'multilingual robustness', 'evaluation security', 'translation-aware detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14994</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Re-understanding Graph Unlearning through Memorization</title><link>https://arxiv.org/abs/2601.14694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MGU (Memorization-guided Graph Unlearning), a framework that uses GNN memorization to assess unlearning difficulty and guide unlearning strategies.&lt;/li&gt;&lt;li&gt;Introduces an adaptive unlearning strategy that dynamically adjusts objectives based on difficulty, and a comprehensive evaluation protocol aligned with practical requirements.&lt;/li&gt;&lt;li&gt;Demonstrates improved forgetting quality, computational efficiency, and utility preservation over state-of-the-art baselines on ten real-world graph datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengfei Ding', 'Yan Wang', 'Guanfeng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['graph unlearning', 'privacy-preserving ML', 'GNN robustness', 'data deletion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14694</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems</title><link>https://arxiv.org/abs/2601.14667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes INFA-Guard, an infection-aware defense framework for LLM-based multi-agent systems that treats 'infected' (converted benign) agents as a distinct threat class.&lt;/li&gt;&lt;li&gt;Uses infection-aware detection and topology-constrained localization to identify attack sources and infected ranges.&lt;/li&gt;&lt;li&gt;Performs remediation by replacing attacker agents and rehabilitating infected agents to prevent further malicious propagation while preserving network topology.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness: reports average 33% reduction in Attack Success Rate (ASR), cross-model robustness, topological generalization, and cost-effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yijin Zhou', 'Xiaoya Lu', 'Dongrui Liu', 'Junchi Yan', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM Security', 'Multi-Agent Systems', 'Defense Mechanisms', 'Malicious Propagation', 'Adversarial Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14667</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>NeuroFilter: Privacy Guardrails for Conversational LLM Agents</title><link>https://arxiv.org/abs/2601.14660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that privacy-violating intent elicits linearly separable directions in LLM activation space and uses this to detect violations efficiently.&lt;/li&gt;&lt;li&gt;Proposes NeuroFilter, a guardrail mapping normative privacy violations to activation-space directions to detect attempts to elicit private data, even when semantic filters are bypassed.&lt;/li&gt;&lt;li&gt;Introduces activation velocity to track cumulative drift across multi-turn conversations and detect long-horizon privacy attacks.&lt;/li&gt;&lt;li&gt;Evaluates across &gt;150,000 interactions on models from 7B–70B, reporting strong detection performance, zero false positives on benign prompts, and large computational savings versus LLM-mediated defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saswat Das', 'Ferdinando Fioretto']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'defense', 'activation-space', 'conversational-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14660</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Optimality of Staircase Mechanisms for Vector Queries under Differential Privacy</title><link>https://arxiv.org/abs/2601.14597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies optimal additive mechanisms for vector-valued queries under ε-differential privacy given query sensitivity and a norm-monotone cost.&lt;/li&gt;&lt;li&gt;Shows the optimization reduces to a 1D compact convex family of radially symmetric distributions using convex rearrangement theory.&lt;/li&gt;&lt;li&gt;Proves staircase distributions are the extreme points and that an ε-DP staircase mechanism is optimal for any dimension, any norm, and any norm-monotone cost.&lt;/li&gt;&lt;li&gt;Resolves a conjecture of Geng et al. and provides a geometric explanation for staircase mechanisms as extremal DP solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Melbourne', 'Mario Diaz', 'Shahab Asoodeh']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving', 'staircase-mechanism', 'optimal-mechanism', 'data-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14597</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference</title><link>https://arxiv.org/abs/2601.14595</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IntelliSA, an intelligent static analyzer for detecting security smells in Infrastructure as Code (IaC) by combining symbolic rule-based over-approximation with a neural inference filter to reduce false positives.&lt;/li&gt;&lt;li&gt;Uses an LLM teacher to generate pseudo-labels and applies knowledge distillation to train a compact student model (500x smaller) that replicates LLM filtering without API costs or latency.&lt;/li&gt;&lt;li&gt;Evaluated on a human-labeled dataset (241 security smells across 11,814 lines of IaC) against two static analyzers and three LLM baselines; achieves highest F1 (83%) and improved cost-effectiveness (detects 60% of smells while inspecting &lt;2% of code).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiyue Mei', 'Michael Fu']&lt;/li&gt;&lt;li&gt;Tags: ['IaC security', 'Static analysis', 'Neural-assisted detection', 'Knowledge distillation', 'LLM filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14595</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness</title><link>https://arxiv.org/abs/2601.14519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a probabilistic metric parameterized by concentration factor κ that interpolates between isotropic (random) noise and adversarial (directional) perturbations.&lt;/li&gt;&lt;li&gt;Proposes an attack strategy aimed at operating in regimes statistically closer to uniform noise to assess how adversarial perturbations estimate noisy risk.&lt;/li&gt;&lt;li&gt;Empirically benchmarks common adversarial attacks on ImageNet and CIFAR-10, identifying when adversarial success corresponds to noisy risk and when it is an atypical worst-case.&lt;/li&gt;&lt;li&gt;Provides guidance for safety-oriented evaluation by characterizing limits of adversarial attacks as proxies for statistical robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Rossolini']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness evaluation', 'attack analysis', 'benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14519</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2601.14323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a security vulnerability in Vision-Language-Action (VLA) systems arising from action chunking and delta-pose integration that creates an intra-chunk visual open-loop.&lt;/li&gt;&lt;li&gt;Proposes SILENTDRIFT, a stealthy black-box backdoor attack that crafts C2-continuous perturbations (via Smootherstep) and selectively poisons keyframe approach phases to hide triggers while maximizing effect.&lt;/li&gt;&lt;li&gt;Evaluated on the LIBERO benchmark, achieving 93.2% attack success with &lt;2% poisoning rate while maintaining 95.3% clean task success, demonstrating practical stealth and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingxin Xu', 'Yuzhang Shang', 'Binghui Wang', 'Emilio Ferrara']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'black-box attack', 'robotic security', 'action-chunking vulnerability', 'trajectory poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14323</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models</title><link>https://arxiv.org/abs/2601.14310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CORVUS, a white-box red-teaming method that fine-tunes small LoRA adapters to camouflage internal telemetry (uncertainty, hidden-state geometry, attention) used by single-pass hallucination detectors.&lt;/li&gt;&lt;li&gt;Proposes an embedding-space FGSM attention stress test and trains on a small OOD instruction set (&lt;0.5% trainable params) to learn detector-visible signal camouflage under teacher forcing.&lt;/li&gt;&lt;li&gt;Shows transferability across models (Llama-2, Vicuna, Llama-3, Qwen2.5) and datasets (FAVA-Annotation), and demonstrates degradation of both training-free and probe-based detectors, motivating adversary-aware auditing and cross-model grounding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nay Myat Min', 'Long H. Pham', 'Hongyu Zhang', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'model-side attacks', 'hallucination detection', 'telemetry camouflage', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14310</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection</title><link>https://arxiv.org/abs/2601.14305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an optimized Decision Tree-based explainable AI framework for IoT intrusion detection, targeted at resource-constrained/edge deployment.&lt;/li&gt;&lt;li&gt;Uses SHAP for local feature attributions and Morris sensitivity analysis for global feature importance to provide explainability.&lt;/li&gt;&lt;li&gt;Reports high detection metrics (e.g., 99.91% accuracy, 99.51% F1) and improved computational efficiency versus ensemble models, enabling real-time processing on edge devices.&lt;/li&gt;&lt;li&gt;Emphasizes transparency and applicability to real environments, claiming compatibility with AI transparency regulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashikuzzaman', 'Md. Shawkat Hossain', 'Jubayer Abdullah Joy', 'Md Zahid Akon', 'Md Manjur Ahmed', 'Md. Naimul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['intrusion-detection', 'explainable-AI', 'IoT-security', 'lightweight-models', 'decision-trees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14305</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing</title><link>https://arxiv.org/abs/2601.14302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DDSA, a resource-efficient adversarial robustness testing framework that combines temporal selectivity (triggering evaluation on critical frames) with spatial precision (targeted pixel-region perturbations).&lt;/li&gt;&lt;li&gt;Uses a scenario-aware trigger based on class priority and model uncertainty to pick frames, and leverages explainable AI to find influential regions for focused perturbation, reducing computation while keeping attack effectiveness.&lt;/li&gt;&lt;li&gt;Aimed at real-time, resource-constrained image/video processing systems where exhaustive frame-by-frame and full-image attacks are impractical.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Shiyuan Meng', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness testing', 'temporal-spatial attacks', 'explainable AI', 'resource-efficient red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14302</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)</title><link>https://arxiv.org/abs/2601.14298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Flexible Adaptive Sequencing mechanism that integrates trust and safety modules for LLM development and deployment.&lt;/li&gt;&lt;li&gt;Addresses risks such as private data leakage, misinformation, and coercion/jailbreaking by implementing application-level guardrails.&lt;/li&gt;&lt;li&gt;Presents a framework aimed at operationalizing safety, privacy, and ethical controls for LLM-generated content during deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anjanava Biswas', 'Wrick Talukdar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'guardrails', 'privacy', 'deployment-framework', 'content-moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14298</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hallucination-Free Automatic Question &amp; Answer Generation for Intuitive Learning</title><link>https://arxiv.org/abs/2601.14280</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hallucination-free multi-agent framework for automatic MCQ generation that decomposes generation into verifiable stages.&lt;/li&gt;&lt;li&gt;Uses rule-based and LLM-based detection agents plus hallucination scoring metrics to optimize for validity, answerability, and cost-efficiency.&lt;/li&gt;&lt;li&gt;Introduces agent-led iterative refinement employing counterfactual reasoning and chain-of-thought to reduce hallucination risk.&lt;/li&gt;&lt;li&gt;Evaluated on AP-aligned STEM questions and reported &gt;90% reduction in hallucination rates versus baseline while preserving educational style.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas X. Wang', 'Aggelos K. Katsaggelos']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_mitigation', 'LLM_safety', 'multi-agent_verification', 'automated_content_verification', 'educational_AI_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14280</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues</title><link>https://arxiv.org/abs/2601.14269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-turn stress-testing framework for assessing safety boundary erosion in mental-health LLM dialogues using two pressure methods: static progression and adaptive probing.&lt;/li&gt;&lt;li&gt;Evaluates three state-of-the-art LLMs with 50 virtual patient profiles up to 20 dialogue turns, finding frequent safety violations and that adaptive probing accelerates boundary breaches (average turns to violation reduced from 9.21 to 4.64).&lt;/li&gt;&lt;li&gt;Identifies making definitive or zero-risk promises as the primary failure mode and argues that single-turn safety tests are insufficient to capture long-dialogue vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youyou Cheng', 'Zhuangwei Kang', 'Kerry Jiang', 'Chenyu Sun', 'Qiyang Pan']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'dialogue-safety', 'adversarial-testing', 'LLM-robustness', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14269</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text</title><link>https://arxiv.org/abs/2601.14683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SFAA (Structured Framework for Adaptive Anonymizer): a three-step pipeline (detection, classification, adaptive anonymization) applying four strategies (rule-based substitution, context-aware rewriting, generalization, suppression) guided by identifier type and risk level.&lt;/li&gt;&lt;li&gt;Implements the framework using local LLMs (LLaMA, Phi) to detect and anonymize sensitive identifiers in qualitative transcripts, aligned with GDPR/HIPAA/OECD guidance.&lt;/li&gt;&lt;li&gt;Evaluates on two case studies (82 face-to-face interviews; 93 machine-led interviews) using dual-method evaluation (manual and LLM-assisted); Phi detected &gt;91% of sensitive data and preserved sentiment in 94.8% of cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aisvarya Adeseye', 'Jouni Isoaho', 'Seppo Virtanen', 'Mohammad Tahir']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'anonymization', 'local LLMs', 'data protection', 'NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14683</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems</title><link>https://arxiv.org/abs/2601.14662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AGEA, an agentic, query-efficient framework for extracting latent entity-relation graphs from GraphRAG systems in a black-box, budget-constrained setting.&lt;/li&gt;&lt;li&gt;Key techniques include a novelty-guided exploration–exploitation strategy, external graph memory, and a two-stage pipeline combining lightweight discovery with LLM-based filtering.&lt;/li&gt;&lt;li&gt;Evaluated on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG, achieving up to ~90% recovery of entities and relations with high precision under identical query budgets.&lt;/li&gt;&lt;li&gt;Demonstrates that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks even under strict query limits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuhua Yang', 'Jiahao Zhang', 'Yilong Wang', 'Dongwon Lee', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'knowledge-graph extraction', 'black-box attack', 'query-efficient attack', 'RAG vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.14662</guid><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
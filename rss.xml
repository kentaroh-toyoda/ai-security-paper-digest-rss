<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 17 Nov 2025 23:25:41 +0000</lastBuildDate><item><title>Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2511.08015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvRoad, a method to generate naturalistic road-style adversarial posters that blend with road appearance to cause visual 3D detectors in autonomous driving to hallucinate non-existent objects.&lt;/li&gt;&lt;li&gt;Uses a two-stage pipeline: Road-Style Adversary Generation and Scenario-Associated Adaptation to maximize attack effectiveness while preserving stealthy, realistic appearance.&lt;/li&gt;&lt;li&gt;Demonstrates generalization across different detectors, scenes, and attack locations, and validates practicality with physical-world (real-world) attack experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Lijun He', 'Yixing Yong', 'Haixia Bi', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'autonomous driving', 'visual 3D detection', 'physical-world attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08015</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title><link>https://arxiv.org/abs/2509.20379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an on-the-fly, computationally efficient hallucination detection method for VLMs using features derived from the model's next-token probabilities (NTPs).&lt;/li&gt;&lt;li&gt;Introduces a dataset of 1,400 human-annotated VLM-generated statements labeled for hallucination and evaluates lightweight ML classifiers trained on NTP-based signals.&lt;/li&gt;&lt;li&gt;Shows that NTPs (including linguistic NTPs computed from text-only inputs) are strong predictors of hallucination and that combining NTP features with VLM-provided hallucination scores improves detection.&lt;/li&gt;&lt;li&gt;Demonstrates comparable performance to stronger VLM-based detectors with lower latency and compute cost, enabling lightweight safety/reliability checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ofir Azachi', 'Kfir Eliyahu', 'Eyal El Ani', 'Rom Himelstein', 'Roi Reichart', 'Yuval Pinter', 'Nitay Calderon']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'vision-language models', 'next-token probabilities', 'model uncertainty', 'lightweight safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20379</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</title><link>https://arxiv.org/abs/2509.15695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORIC, a framework/benchmark (ORIC-Bench) for evaluating object recognition under contextual incongruity in large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Constructs incongruous object-context pairs via two strategies: LLM-guided sampling to find hard-to-recognize present objects and CLIP-guided sampling to propose plausible but absent objects.&lt;/li&gt;&lt;li&gt;Evaluates 18 LVLMs and 2 open-vocabulary detectors, finding significant performance drops and systematic bias/hallucination patterns under incongruous contexts.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning (Visual Reinforcement Fine-Tuning) on a small ORIC-style dataset improves robustness on ORIC-Bench and related benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoyang Li', 'Zhan Ling', 'Yuchen Zhou', 'Litian Gong', 'Erdem B{\\i}y{\\i}k', 'Hao Su']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination', 'benchmarking', 'LVLMs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15695</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2508.19257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Temporal Token Fusion (TTF), a training-free method that fuses historical and current visual tokens via grayscale pixel-difference detection combined with attention-based semantic relevance to selectively integrate temporal information.&lt;/li&gt;&lt;li&gt;Uses hard fusion strategies and keyframe anchoring to avoid error accumulation; compatible with existing VLA architectures (OpenVLA, VLA-Cache) and claims computational benefits from selective Query matrix reuse.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements on LIBERO, SimplerEnv, and real-robot manipulation tasks (e.g., +4.0 pp on LIBERO; +8.7% relative on real robots), emphasizing robustness to visual noise and temporal coherence exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenghao Liu', 'Jiachen Zhang', 'Chengxuan Li', 'Zhimu Zhou', 'Shixin Wu', 'Songfang Huang', 'Huiling Duan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-action', 'temporal fusion', 'robotic manipulation', 'model-agnostic inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19257</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models</title><link>https://arxiv.org/abs/2508.07570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Cache Enhancement (ACE) for test-time adaptation of vision-language models, building a dynamic class-wise cache of high-confidence or low-entropy image embeddings.&lt;/li&gt;&lt;li&gt;Initializes class-specific thresholds from zero-shot statistics and refines them with an exponential moving average and exploration-augmented updates to avoid cache error accumulation and to adapt decision boundaries.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art robustness/generalization on 15 benchmarks, outperforming prior cache-based TTA methods under distribution shift.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Khanh-Binh Nguyen', 'Phuoc-Nguyen Bui', 'Hyunseung Choo', 'Duc Thanh Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['test-time adaptation', 'distribution shift robustness', 'vision-language models', 'cache-based methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07570</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning</title><link>https://arxiv.org/abs/2508.01603</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Image-Adaptive Prompt Learning (IAPL) that dynamically adjusts prompts per test image to improve generalization for AI-generated image detection.&lt;/li&gt;&lt;li&gt;Combines a Conditional Information Learner (CNN-based) producing forgery-specific and general conditions with test-time adaptive tokens optimized on a single sample via multi-view consistency.&lt;/li&gt;&lt;li&gt;At inference, selects the input variant with highest prediction confidence; reports state-of-the-art mean accuracies on UniversalFakeDetect and GenImage datasets.&lt;/li&gt;&lt;li&gt;Aims to improve robustness to unseen generative models by adapting prompts at test time rather than fixed fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiheng Li', 'Zichang Tan', 'Zhen Lei', 'Xu Zhou', 'Yang Yang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery detection', 'test-time adaptation', 'prompt learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01603</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Curing Semantic Drift: A Dynamic Approach to Grounding Generation in Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.21509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'semantic drift' in Large Vision-Language Models (LVLMs) as progressive detachment from visual input causing hallucinations.&lt;/li&gt;&lt;li&gt;Proposes Dynamic Logits Calibration (DLC), a training-free, decoding-time method that uses a real-time visual referee to perform two checks per candidate token: intrinsic visual relevance and contextual visual coherence.&lt;/li&gt;&lt;li&gt;DLC adaptively balances these checks against a baseline to modulate output logits, aiming to favor visually grounded tokens while avoiding multiple full LVLM forward passes.&lt;/li&gt;&lt;li&gt;Reports substantial reduction in hallucinations with high inference efficiency; code release planned.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahe Chen', 'Jiaying He', 'Qiyuan Chen', 'Qian Shao', 'Jiahe Ying', 'Hongxia Xu', 'Jintai Chen', 'Jianwei Zheng', 'Jian Wu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'grounding', 'alignment', 'robustness', 'decoding-time mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21509</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach</title><link>https://arxiv.org/abs/2504.14137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies two key factors that affect transferability of targeted adversarial attacks: feature quality (structural/detail completeness) and feature quantity (spatial sufficiency).&lt;/li&gt;&lt;li&gt;Proposes TGAF (2D Tensor-Guided Adversarial Fusion), which conditions generative adversarial noise on two-dimensional semantic tensors derived via diffusion models instead of 1D label encodings.&lt;/li&gt;&lt;li&gt;Introduces a masking strategy during training to ensure parts of generated noise preserve complete semantic information for target classes.&lt;/li&gt;&lt;li&gt;Reports that TGAF outperforms state-of-the-art multi-target and transfer attacks across multiple evaluation settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hangyu Liu', 'Bo Peng', 'Pengxiang Ding', 'Donglin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['targeted adversarial attacks', 'transferability', 'diffusion-model-based attacks', 'semantic conditioning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14137</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Fractured Glass, Failing Cameras: Simulating Physics-Based Adversarial Samples for Autonomous Driving Systems</title><link>https://arxiv.org/abs/2405.15033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates real-world camera glass failures cause object-detection models to fail, motivating study of physics-based adversarial samples from hardware damage.&lt;/li&gt;&lt;li&gt;Develops a simulation pipeline using finite element modeling (FEM) to generate realistic crack geometries and physically-based rendering (PBR) to synthesize broken-glass image corruptions.&lt;/li&gt;&lt;li&gt;Applies synthesized broken-glass filters to driving and large-scale detection datasets (KITTI, BDD100K, MS-COCO) and measures detection degradation on YOLOv8, Faster R-CNN, and transformer-based detectors.&lt;/li&gt;&lt;li&gt;Analyzes distributional shifts using Kullback–Leibler divergence (including a custom cracked-windshield capture) to quantify the impact of glass failures on model robustness and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manav Prabhakar', 'Jwalandhar Girnar', 'Arpan Kusari']&lt;/li&gt;&lt;li&gt;Tags: ['physics-based adversarial examples', 'autonomous driving', 'robustness', 'camera hardware failure', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.15033</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications</title><link>https://arxiv.org/abs/2311.17663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cam4DOcc, a benchmark for camera-only 4D occupancy forecasting (spatiotemporal prediction) in autonomous driving.&lt;/li&gt;&lt;li&gt;Constructs datasets from nuScenes, nuScenes-Occupancy, and Lyft-Level5 providing sequential occupancy states and 3D backward centripetal flow.&lt;/li&gt;&lt;li&gt;Defines four baseline approaches (static-world occupancy, point-cloud voxel prediction, 2D-3D instance-based prediction, and an end-to-end 4D occupancy forecasting network) and a standardized evaluation protocol for present and future occupancy estimation.&lt;/li&gt;&lt;li&gt;Releases dataset and implementations to enable standardized comparison and future research on camera-based occupancy forecasting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyi Ma', 'Xieyuanli Chen', 'Jiawei Huang', 'Jingyi Xu', 'Zhen Luo', 'Jintao Xu', 'Weihao Gu', 'Rui Ai', 'Hesheng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'occupancy-forecasting', 'benchmark', 'safety-evaluation', 'spatiotemporal-perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.17663</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm</title><link>https://arxiv.org/abs/2511.11009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies why vanilla virtual adversarial training (VAT) fails in unsupervised domain adaptation (UDA) and formulates a new Unsupervised Robust Domain Adaptation (URDA) paradigm to address robustness under domain shift.&lt;/li&gt;&lt;li&gt;Derives a generalization bound for URDA that extends classical UDA theory to incorporate adversarial attacks and robustness considerations.&lt;/li&gt;&lt;li&gt;Proposes DART (Disentangled Adversarial Robustness Training), a simple two-step procedure: pretrain any UDA model, then apply an instantaneous robustification post-training via disentangled distillation.&lt;/li&gt;&lt;li&gt;Empirically validates on four benchmark datasets that DART improves adversarial robustness while preserving domain transfer performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fuxiang Huang', 'Xiaowei Fu', 'Shiyu Ye', 'Lina Ma', 'Wen Li', 'Xinbo Gao', 'David Zhang', 'Lei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'domain adaptation', 'adversarial training', 'robustness theory', 'unsupervised learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11009</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Attentive Feature Aggregation or: How Policies Learn to Stop Worrying about Robustness and Attend to Task-Relevant Visual Cues</title><link>https://arxiv.org/abs/2511.10762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Attentive Feature Aggregation (AFA), a lightweight trainable pooling mechanism that learns to attend to task-relevant visual cues in pre-trained visual representations.&lt;/li&gt;&lt;li&gt;AFA aims to reduce policy vulnerability to out-of-domain visual changes and semantically rich distractors without extensive dataset augmentation or fine-tuning of the backbone.&lt;/li&gt;&lt;li&gt;Extensive experiments in simulation and real-world visuomotor tasks show improved robustness and generalisation compared to standard pooling methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolaos Tsagkas', 'Andreas Sochopoulos', 'Duolikun Danier', 'Sethu Vijayakumar', 'Alexandros Kouris', 'Oisin Mac Aodha', 'Chris Xiaoxuan Lu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'visuomotor-robustness', 'attention-mechanism', 'sim2real']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10762</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency</title><link>https://arxiv.org/abs/2511.10671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Grounded Visual Factualization (GVF) Finetuning to reduce visual hallucinations in multimodal LLMs by injecting explicit factual signals during fine-tuning.&lt;/li&gt;&lt;li&gt;GVF comprises three mechanisms: Factual Anchor Data Augmentation (structured factual anchors and counterfactual prompts), Fact-Aware Instruction Tuning (embedding cues in instructions), and a Factual Consistency Loss that penalizes factual errors.&lt;/li&gt;&lt;li&gt;Evaluated on LLaVA-1.5-13B, GVF improves factual consistency on the VHTest benchmark (OEQ and YNQ) while preserving or slightly improving performance on general multimodal benchmarks (MME, POPE).&lt;/li&gt;&lt;li&gt;Claims to mitigate visual hallucinations without degrading general multimodal understanding and reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filippo Morbiato', 'Luca Romano', 'Alessandro Persona']&lt;/li&gt;&lt;li&gt;Tags: ['visual hallucination mitigation', 'factual consistency', 'MLLM alignment', 'instruction tuning', 'loss function design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10671</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models</title><link>https://arxiv.org/abs/2511.11502</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows many LVLM object-hallucinations occur because the model relies on prelim (previously generated) tokens rather than the image, quantified via mutual information between image and predicted object conditioned on prelim.&lt;/li&gt;&lt;li&gt;Introduces Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens that can be calculated on the fly during inference with no extra forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nhat Hoang-Xuan', 'Minh Vu', 'My T. Thai', 'Manish Bhattarai']&lt;/li&gt;&lt;li&gt;Tags: ['object-hallucination', 'hallucination-detection', 'vision-language-models', 'attention-analysis', 'safety-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11502</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents</title><link>https://arxiv.org/abs/2511.11468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VRD-UQA, a benchmark to evaluate Visual LLMs' resilience to plausible but unanswerable questions in multi-page visually rich documents.&lt;/li&gt;&lt;li&gt;Automatically generates corruptions by swapping entities across document elements, positions, or pages and verifies unanswerability using a VLLM-as-judge approach.&lt;/li&gt;&lt;li&gt;Evaluates 12 VLLMs on detection accuracy at page and document levels, analyzes effects of corruption types (NLP entity, document element, layout), and tests knowledge-injection/in-context strategies (OCR, multi-page selection, unanswerability cues).&lt;/li&gt;&lt;li&gt;Finds notable limitations in current VLLMs and positions VRD-UQA as a framework for developing more robust document VQA systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Napolitano', 'Luca Cagliero', 'Fabrizio Battiloro']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmarking', 'visual-llms', 'unanswerable-questions', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11468</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models</title><link>https://arxiv.org/abs/2511.11299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AUVIC, an adversarial perturbation–based framework to enable targeted visual concept unlearning in multimodal LLMs without full retraining.&lt;/li&gt;&lt;li&gt;Focuses on precise removal of target visual concepts while preserving performance on related/non-target entities.&lt;/li&gt;&lt;li&gt;Introduces VCUBench, a benchmark for evaluating visual concept unlearning in grouped contexts, and reports state-of-the-art forgetting with minimal collateral degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haokun Chen', 'Jianing Li', 'Yao Zhang', 'Jinhe Bi', 'Yan Xia', 'Jindong Gu', 'Volker Tresp']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'adversarial examples', 'multimodal LLMs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11299</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing</title><link>https://arxiv.org/abs/2511.11295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SimuFreeMark, a watermarking framework that avoids hand-crafted noise simulation by embedding watermarks into deep features of image low-frequency components via a pre-trained VAE.&lt;/li&gt;&lt;li&gt;Argues and empirically demonstrates that low-frequency components are inherently robust to a wide range of conventional signal-processing and semantic editing attacks.&lt;/li&gt;&lt;li&gt;Reports improved robustness and visual quality over state-of-the-art watermarking methods across many editing/attack scenarios without training-time noise simulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichao Tang', 'Mingyang Li', 'Di Miao', 'Sheng Li', 'Zhenxing Qian', 'Xinpeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'robustness', 'image security', 'AIGC provenance', 'anti-editing attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11295</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger</title><link>https://arxiv.org/abs/2511.11210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a one-to-N backdoor attack for 3D point cloud models using a configurable spherical trigger that can encode multiple target classes from a single trigger design.&lt;/li&gt;&lt;li&gt;Provides a theoretical analysis showing poisoned models can map distinct spherical trigger configurations to different labels.&lt;/li&gt;&lt;li&gt;Empirically validates across datasets and architectures, reporting high attack success rates (up to 100%) while preserving clean-data accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongmei Shan', 'Wei Lian', 'Chongxia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', '3D point cloud', 'model poisoning', 'adversarial attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11210</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Questioning the Stability of Visual Question Answering</title><link>https://arxiv.org/abs/2511.11206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale, systematic study of Visual Language Model (VLM) robustness to benign, meaning-preserving visual and textual perturbations (pixel shifts, geometric transforms, padded rescaling, paraphrasing, multilingual rewrites).&lt;/li&gt;&lt;li&gt;Finds modern VLMs (including GPT-4o, Gemini 2.0 Flash) are highly sensitive: a substantial fraction of samples change answers under small perturbations; stability strongly correlates with correctness.&lt;/li&gt;&lt;li&gt;Shows stability signals from small open-source models can predict correctness of larger closed-source models; argues for robustness evaluations focused on expected invariances rather than only adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Rosenfeld', 'Neta Glazer', 'Ethan Fetaya']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'visual question answering', 'evaluation/benchmarking', 'model stability', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11206</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</title><link>https://arxiv.org/abs/2511.11169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlignVQA, a debate-based multi-agent framework where specialized vision-language models produce candidate answers and generalist agents critique, refine, and aggregate them to produce calibrated confidence estimates.&lt;/li&gt;&lt;li&gt;Introduces aligncal, a differentiable calibration-aware loss that fine-tunes specialized agents by minimizing an upper bound on calibration error, improving per-agent confidence fidelity.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple VQA benchmarks shows substantial reductions in calibration discrepancy, and findings that better-calibrated specialized agents yield improved aggregated confidences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayush Pandey', 'Jai Bardhan', 'Ishita Jain', 'Ramya S Hebbalaguppe', 'Rohan Raju Dhanakshirur', 'Lovekesh Vig']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'VQA', 'confidence estimation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11169</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</title><link>https://arxiv.org/abs/2511.11116</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of 24 recent works on AI-generated media (deepfake) detection, examining each study's contributions and weaknesses.&lt;/li&gt;&lt;li&gt;Synthesizes common limitations: poor generalization to unseen generators/models, fragility to heavy editing, and ineffectiveness on multimodal content.&lt;/li&gt;&lt;li&gt;Proposes future research direction focusing on multimodal deep learning models to improve robustness and generalized detection of synthetic media.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Redwan Hussain', 'Mizanur Rahman', 'Prithwiraj Bhattacharjee']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'synthetic media detection', 'robustness', 'multimodal', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11116</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title><link>https://arxiv.org/abs/2511.11030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Deep vision models (DenseNet121, SwinV2-B, MedMamba) can predict patient health insurance type (a proxy for socioeconomic status) from normal chest X-rays with AUC ≈ 0.67–0.68 on MIMIC-CXR-JPG and CheXpert.&lt;/li&gt;&lt;li&gt;The predictive signal persists after controlling for age, race, and sex, and remains when training within a single racial group, indicating models learn a diffuse socioeconomic signature rather than obvious demographic cues.&lt;/li&gt;&lt;li&gt;Patch-based occlusion analyses show the signal is diffuse across upper and mid-thoracic regions, suggesting models may pick up on subtle artifacts from clinical environments, equipment, or care pathways.&lt;/li&gt;&lt;li&gt;Implication: medical images are not neutral biological data; models can internalize social fingerprints, raising concerns for fairness, privacy leakage, and unintended discriminatory use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi-Yu Chen', 'Rawan Abulibdeh', 'Arash Asgari', 'Leo Anthony Celi', 'Deirdre Goode', 'Hassan Hamidi', 'Laleh Seyyed-Kalantari', 'Po-Chih Kuo', 'Ned McCague', 'Thomas Sounack']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'algorithmic fairness', 'medical imaging', 'latent confounders', 'socioeconomic bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11030</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation</title><link>https://arxiv.org/abs/2511.11014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SP-Guard, an inference-time method that estimates prompt harmfulness and applies selective guidance masks to only steer unsafe regions during text-to-image generation.&lt;/li&gt;&lt;li&gt;Aims to improve adaptivity (vary guidance strength by prompt) and selectivity (modify only unsafe image areas) compared with existing guiding methods.&lt;/li&gt;&lt;li&gt;Reports experiments showing safer outputs with minimal unintended alterations, emphasizing transparency and controllability in generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sumin Yu', 'Taesup Moon']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image safety', 'harmful content mitigation', 'inference-time guidance', 'prompt safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11014</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models</title><link>https://arxiv.org/abs/2511.10923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Positive and Negative Prompt Supervision: class-specific positive prompts capture intra-class features while negative prompts emphasize features near category boundaries to improve OOD detection.&lt;/li&gt;&lt;li&gt;Initializes prompts using large language models (LLMs), optimizes them, and transfers semantic-aware supervision to the visual branch via a graph-based architecture.&lt;/li&gt;&lt;li&gt;Applies the method to energy-based OOD detection and reports improved performance on CIFAR-100 and ImageNet-1K benchmarks across multiple OOD datasets and LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhixia He', 'Chen Zhao', 'Minglai Shao', 'Xintao Wu', 'Xujiang Zhao', 'Dong Li', 'Qin Tian', 'Linlin Yu']&lt;/li&gt;&lt;li&gt;Tags: ['OOD detection', 'robustness', 'vision-language models', 'prompt engineering', 'energy-based methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10923</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Semantic VLM Dataset for Safe Autonomous Driving</title><link>https://arxiv.org/abs/2511.10701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;CAR-Scenes is a frame-level dataset of 5,192 driving images annotated with a 28-key category/sub-category KB (350+ leaf attributes) covering environment, road geometry, vehicle/user behaviors, sensor states, and a discrete severity scale (1–10).&lt;/li&gt;&lt;li&gt;Annotations produced via a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; authors release prompts, post-processing rules, per-field baseline performance, and analysis scripts.&lt;/li&gt;&lt;li&gt;Provides attribute co-occurrence graphs and JSONL records to enable semantic retrieval, dataset triage, and risk-aware scenario mining; includes reproducible baselines (e.g., LoRA-tuned Qwen2-VL-2B) with evaluation metrics and fixed validation split.&lt;/li&gt;&lt;li&gt;Intended to support interpretable scene-level understanding and explainable, data-centric workflows for safe autonomous driving research and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuankai He', 'Weisong Shi']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'dataset', 'vision-language-models', 'safety-evaluation', 'scenario-mining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10701</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement</title><link>https://arxiv.org/abs/2511.10668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops an analytic framework linking capability growth to resource build-out (compute, data, energy) and physical/information-theoretic limits that define a service envelope constraining instantaneous improvement.&lt;/li&gt;&lt;li&gt;Derives a critical boundary separating superlinear (runaway) from subcritical regimes and gives falsifiable decision rules using observable metrics (facility power, I/O bandwidth, training throughput, benchmark loss, spending).&lt;/li&gt;&lt;li&gt;Proposes directly implementable safety controls (power caps, throughput throttling, evaluation gates) and provides analytical case studies; discusses limitations and directions for handling stochastic dynamics, multi-agent competition, and architectural shifts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akbar Anbar Jafari', 'Cagri Ozcinar', 'Gholamreza Anbarjafari']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Capability escalation', 'Recursive self-improvement', 'Safety controls', 'Risk assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10668</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title><link>https://arxiv.org/abs/2511.10067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multifaceted Self-Refinement (MuSeR): an attribute-conditioned query generator that simulates diverse user contexts, LLM self-evaluation across three facets (decision-making, communication, safety), and supervised fine-tuning on refined responses to improve context-awareness.&lt;/li&gt;&lt;li&gt;Targets medical domain context-awareness (recognizing missing/critical details and producing safe, context-appropriate responses) and incorporates knowledge distillation to transfer improvements to smaller models.&lt;/li&gt;&lt;li&gt;Reports SOTA results on HealthBench among open-source LLMs (63.8% overall, 43.1% on hard subset), with distilled Qwen3-32B surpassing its teacher.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Zhou', 'Yubin Wang', 'Bin Wang', 'Chen Ning', 'Xien Liu', 'Ji Wu', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'medical LLMs', 'context-awareness', 'self-refinement', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10067</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title><link>https://arxiv.org/abs/2509.20379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an on-the-fly, computationally efficient hallucination detection method for VLMs using features derived from the model's next-token probabilities (NTPs).&lt;/li&gt;&lt;li&gt;Introduces a dataset of 1,400 human-annotated VLM-generated statements labeled for hallucination and evaluates lightweight ML classifiers trained on NTP-based signals.&lt;/li&gt;&lt;li&gt;Shows that NTPs (including linguistic NTPs computed from text-only inputs) are strong predictors of hallucination and that combining NTP features with VLM-provided hallucination scores improves detection.&lt;/li&gt;&lt;li&gt;Demonstrates comparable performance to stronger VLM-based detectors with lower latency and compute cost, enabling lightweight safety/reliability checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ofir Azachi', 'Kfir Eliyahu', 'Eyal El Ani', 'Rom Himelstein', 'Roi Reichart', 'Yuval Pinter', 'Nitay Calderon']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'vision-language models', 'next-token probabilities', 'model uncertainty', 'lightweight safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20379</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable LLM Guardrails via Sparse Representation Steering</title><link>https://arxiv.org/abs/2503.16851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sparse Representation Steering (SRS): use a pretrained sparse autoencoder to disentangle dense LLM activations into a sparse, monosemantic feature space and selectively steer dimensions at inference time.&lt;/li&gt;&lt;li&gt;Identifies attribute-relevant features by contrasting sparse activations from positive/negative prompt pairs and measuring bidirectional KL divergence to locate dimensions linked to target attributes.&lt;/li&gt;&lt;li&gt;Applies SRS to alignment dimensions (safety, fairness, truthfulness) on Gemma-2 models, showing improved fine-grained controllability and multi-attribute conflict resolution versus existing steering methods.&lt;/li&gt;&lt;li&gt;Reports maintained linguistic quality and general ability while improving controllability, addressing practical safety/guardrail needs without full fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeqing He', 'Zhibo Wang', 'Huiyu Xu', 'Hejun Lin', 'Wenhui Zhang', 'Zhixuan Chu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'representation-steering', 'interpretability', 'LLM-safety', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16851</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Instella: Fully Open Language Models with Stellar Performance</title><link>https://arxiv.org/abs/2511.10628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Instella: a family of fully open 3B-parameter LLMs trained on entirely open data and code, optimized for transparency and reproducibility.&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance among fully open models and is competitive with similar-size open-weight models despite using fewer pretraining tokens.&lt;/li&gt;&lt;li&gt;Provides two specialized variants: Instella-Long (context up to 128K tokens) and Instella-Math (mathematics-focused, SFT + RL fine-tuning).&lt;/li&gt;&lt;li&gt;Includes instruction tuning and alignment with human preferences but does not present detailed security/red-teaming or adversarial robustness evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiang Liu', 'Jialian Wu', 'Xiaodong Yu', 'Yusheng Su', 'Prakamya Mishra', 'Gowtham Ramesh', 'Sudhanshu Ranjan', 'Chaitanya Manem', 'Ximeng Sun', 'Ze Wang', 'Pratik Prabhanjan Brahma', 'Zicheng Liu', 'Emad Barsoum']&lt;/li&gt;&lt;li&gt;Tags: ['open-source LLM', 'alignment', 'instruction tuning', 'long-context', 'model release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10628</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?</title><link>https://arxiv.org/abs/2511.08455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how social bot detectors rely on textual shortcuts (spurious correlations) and evaluates robustness under constructed shortcut OOD scenarios.&lt;/li&gt;&lt;li&gt;Finds substantial performance degradation (average relative accuracy drop ~32%) when irrelevant textual feature distributions shift.&lt;/li&gt;&lt;li&gt;Proposes mitigation via LLM-based counterfactual data augmentation at individual-text, dataset, and model levels, yielding average relative performance improvements of ~56%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyan Zheng', 'Herun Wan', 'Minnan Luo', 'Junhang Huang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'shortcut learning', 'LLM-based data augmentation', 'social bot detection', 'out-of-distribution generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08455</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond the Surface: Probing the Ideological Depth of Large Language Models</title><link>https://arxiv.org/abs/2508.21448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'ideological depth' as (1) steerability (ability to follow political instructions) and (2) feature richness of internal political representations measured via sparse autoencoders (SDL).&lt;/li&gt;&lt;li&gt;Compares Llama-3.1-8B-Instruct and Gemma-2-9B-IT using prompt-based and activation-steering interventions; finds Gemma is more steerable and activates ~7.3× more distinct political features than Llama.&lt;/li&gt;&lt;li&gt;Performs causal ablations of a targeted set of Gemma's political features, inducing consistent behavior shifts (notably increased refusals), suggesting some refusals arise from capability deficits rather than safety guardrails.&lt;/li&gt;&lt;li&gt;Argues ideological depth is a measurable property of LLMs and that steerability exposes aspects of their latent political architecture.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shariar Kabir', 'Kevin Esterling', 'Yue Dong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'steerability', 'activation-steering', 'model-interpretability', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21448</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title><link>https://arxiv.org/abs/2508.15793</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a three-stage empirical study of 'format bias' in LLMs: (1) measuring presence and direction of bias across model families, (2) analyzing data-level drivers (information richness, structure quality, representation type), and (3) examining internal attention patterns and testing a lightweight intervention.&lt;/li&gt;&lt;li&gt;Finds consistent format biases across models, driven by information richness, structure quality, and representation type, and linked to attention imbalance within models.&lt;/li&gt;&lt;li&gt;Demonstrates attention re-weighting as an inference-time intervention and proposes data pre-processing (format repair/normalization) and format-balanced training corpora as mitigation strategies.&lt;/li&gt;&lt;li&gt;Identifies future research directions for reducing format bias to improve robust and fair heterogeneous data processing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liu', 'Mayi Xu', 'Qiankun Pi', 'Wenli Li', 'Ming Zhong', 'Yuanyuan Zhu', 'Mengchi Liu', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['format bias', 'robustness', 'attention analysis', 'LLM bias', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15793</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Latent Principle Discovery for Language Model Self-Improvement</title><link>https://arxiv.org/abs/2505.16927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an automated method to elicit latent behavioral principles from a language model and compress them into an interpretable set via clustering.&lt;/li&gt;&lt;li&gt;Uses a posterior-regularized Monte Carlo EM procedure to identify effective latent principles and trains the LM to invoke them for self-correction and improved outputs.&lt;/li&gt;&lt;li&gt;Demonstrates self-improvement on smaller LMs (7–8B) with gains on AlpacaEval, MT-Bench, and principle-following benchmarks; clustered principles form interpretable model-generated constitutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keshav Ramji', 'Tahira Naseem', "Ram\\'on Fernandez Astudillo"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improvement', 'latent-principles', 'principle-following', 'post-training-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16927</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RASTeR: Robust, Agentic, and Structured Temporal Reasoning</title><link>https://arxiv.org/abs/2406.19538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RASTeR, a prompting framework that separates context evaluation from answer generation to improve temporal question answering (TQA).&lt;/li&gt;&lt;li&gt;Detects and evaluates relevance and temporal coherence of retrieved context, builds a Temporal Knowledge Graph (TKG), and selectively corrects or discards inconsistent context before answering.&lt;/li&gt;&lt;li&gt;Demonstrates consistent robustness improvements across multiple datasets and LLMs, including a needle-in-the-haystack test where RASTeR outperforms alternatives when relevant context is buried among many distractors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dan Schumacher', 'Fatemeh Haji', 'Tara Grey', 'Niharika Bandlamudi', 'Nupoor Karnik', 'Gagana Uday Kumar', 'Jason Cho-Yu Chiang', 'Paul Rad', 'Nishant Vishwamitra', 'Anthony Rios']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'temporal reasoning', 'retrieval evaluation', 'prompting / pipeline design', 'knowledge graph']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.19538</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are language models rational? The case of coherence norms and belief revision</title><link>https://arxiv.org/abs/2406.03442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether norms of rationality—specifically coherence norms—apply to language models, covering logical coherence and coherence tied to strength of belief.&lt;/li&gt;&lt;li&gt;Introduces the Minimal Assent Connection (MAC) and a proposed account of credence that derives strength of belief from model internal next-token probabilities.&lt;/li&gt;&lt;li&gt;Argues that rationality norms apply to some language models but not others, and discusses implications for predicting/explaining model behavior and for AI safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Hofweber', 'Peter Hase', 'Elias Stengel-Eskin', 'Mohit Bansal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI-safety', 'belief-revision', 'model-behavior', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.03442</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness</title><link>https://arxiv.org/abs/2405.08151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a systematic evaluation framework for retrieval-augmented LLMs (RAL) on five biomedical NLP tasks: triple extraction, link prediction, classification, question answering, and natural language inference.&lt;/li&gt;&lt;li&gt;Defines and builds four testbeds assessing unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness (self-awareness) to probe RAL behavior and hallucination tendencies.&lt;/li&gt;&lt;li&gt;Evaluates 3 representative LLMs with 3 different retrievers across 9 datasets, analyzing how retrieval affects performance, robustness, and failure modes in the biomedical domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingchen Li', 'Zaifu Zhan', 'Han Yang', 'Yongkang Xiao', 'Jiatan Huang', 'Rui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'LLM safety', 'retrieval-augmented LLMs', 'hallucination', 'biomedical NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.08151</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title><link>https://arxiv.org/abs/2511.11551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a test-time, model-guided policy-shaping method to steer pre-trained RL agents toward ethical behaviors without retraining.&lt;/li&gt;&lt;li&gt;Uses scenario-action attribute classifiers to control individual behavioral attributes and trade off alignment vs. reward at decision time.&lt;/li&gt;&lt;li&gt;Introduces and evaluates on the MACHIAVELLI benchmark (134 text-based game environments, thousands of annotated ethical scenarios), comparing to training-time methods and analyzing ethical violations and power-seeking behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dena Mujtaba', 'Brian Hu', 'Anthony Hoogs', 'Arslan Basharat']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'RL safety', 'test-time policy shaping', 'behavior steering', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11551</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</title><link>https://arxiv.org/abs/2511.11182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multi-agent Undercover Gaming (MUG), a protocol to detect and mitigate hallucinating agents in multi-agent debate setups by introducing multimodal counterfactual tests.&lt;/li&gt;&lt;li&gt;Modifies reference images to create counterfactual evidence and evaluates whether agents detect changes, using detection of 'undercover' (hallucinating) agents to improve overall reasoning reliability.&lt;/li&gt;&lt;li&gt;Claims improvements over standard Multi-Agent Debate by enabling factual verification beyond consensus, introducing cross-evidence reasoning with dynamic evidence, and fostering active probing discussions among agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dayong Liang', 'Xiao-Yong Wei', 'Changmeng Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'multimodal reasoning', 'alignment', 'robustness', 'counterfactual testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11182</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title><link>https://arxiv.org/abs/2511.10837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a principled evaluation framework separating extrinsic and intrinsic hallucinations and evaluates detection across curated benchmarks.&lt;/li&gt;&lt;li&gt;Proposes attention-based uncertainty quantification with novel attention aggregation strategies to improve interpretability and hallucination detection.&lt;/li&gt;&lt;li&gt;Empirically shows sampling-based methods (e.g., Semantic Entropy) work well for extrinsic hallucinations but fail for intrinsic ones, while attention aggregation better detects intrinsic hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elyes Hajji', 'Aymen Bouguerra', 'Fabio Arnez']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'attention-based uncertainty', 'intrinsic vs extrinsic hallucinations', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10837</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title><link>https://arxiv.org/abs/2511.10720</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PISanitizer, a defense for prompt injection in long-context LLMs that pinpoints and sanitizes potential injected tokens before generation.&lt;/li&gt;&lt;li&gt;Method: intentionally allows the LLM to follow context instructions, identifies high-attention tokens driving instruction-following, and sanitizes them—creating a dilemma for attackers.&lt;/li&gt;&lt;li&gt;Evaluation shows improved prevention of prompt injection while maintaining utility, efficiency, and robustness against optimization-based and adaptive attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runpeng Geng', 'Yanting Wang', 'Chenlong Yin', 'Minhao Cheng', 'Ying Chen', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'prompt sanitization', 'LLM security', 'adversarial defense', 'long-context LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10720</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title><link>https://arxiv.org/abs/2511.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework to convert system-level evaluation of multi-LLM agent systems into agent- and message-level training signals using cooperative game-theoretic attribution (e.g., Shapley) and process reward modeling.&lt;/li&gt;&lt;li&gt;Generates local, signed, credit-conserving rewards that promote cooperation, discourage redundancy or sabotage, and produce repair-aware penalties for failure cases by localizing first errors and rewarding corrective steps.&lt;/li&gt;&lt;li&gt;Signals are bounded and designed to be compatible with reinforcement- or preference-based post-training, offering an auditable pathway from global evaluation to local supervision.&lt;/li&gt;&lt;li&gt;Work is conceptual/theoretical; empirical validation is left for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Hsuan Yang', 'Tanwi Mallick', 'Le Chen', 'Krishnan Raghavan', 'Azton Wells', 'Amal Gueroudji', 'Ian T. Foster', 'Rajeev Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent LLMs', 'credit assignment', 'reward modeling', 'safety signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10687</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning</title><link>https://arxiv.org/abs/2511.11562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRBench: 1,100 expert-authored, open-ended tasks and 19,356 expert-curated rubric criteria focused on Finance and Legal professional reasoning.&lt;/li&gt;&lt;li&gt;Built with contributions from 182 qualified professionals across 114 countries and 47 US jurisdictions, validated through an independent quality pipeline.&lt;/li&gt;&lt;li&gt;Evaluates 20 leading models, finding low top scores on Hard subsets (Finance 0.39, Legal 0.37) and identifying failure modes like inaccurate judgments, lack of process transparency, and incomplete reasoning.&lt;/li&gt;&lt;li&gt;Open-source release includes economic-impact cataloging and per-rubric-category analyses to surface capability divergences relevant to real-world, high-stakes adoption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Afra Feyza Aky\\"urek', 'Advait Gosai', 'Chen Bo Calvin Zhang', 'Vipul Gupta', 'Jaehwan Jeong', 'Anisha Gunjal', 'Tahseen Rabbani', 'Maria Mazzone', 'David Randolph', 'Mohammad Mahmoudi Meymand', 'Gurshaan Chattha', 'Paula Rodriguez', 'Diego Mares', 'Pavit Singh', 'Michael Liu', 'Subodh Chawla', 'Pete Cline', 'Lucy Ogaz', 'Ernesto Hernandez', 'Zihao Wang', 'Pavi Bhatter', 'Marcos Ayestaran', 'Bing Liu', 'Yunzhong He']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'high-stakes', 'professional-reasoning', 'rubric-based-eval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11562</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</title><link>https://arxiv.org/abs/2511.11518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces W2S-AlignTree, a plug-and-play inference-time alignment framework that combines Monte Carlo Tree Search (MCTS) with a Weak-to-Strong generalization paradigm to guide a strong LLM's generation without parameter updates.&lt;/li&gt;&lt;li&gt;Uses step-level signals from a weaker model as alignment proxies and an entropy-aware exploration mechanism to balance exploration/exploitation in the generative search tree.&lt;/li&gt;&lt;li&gt;Demonstrates improvements on tasks such as sentiment-controlled generation, summarization, and instruction-following (e.g., Llama3-8B summarization score improvement reported).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Ding', 'Yuhao Wang', 'Tengyue Xiao', 'Haoying Wang', 'Guojun Ma', 'Mingyang Wan', 'Caigui Jiang', 'Ning Ding']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time alignment', 'Monte Carlo Tree Search', 'LLM control', 'weak-to-strong generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11518</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text</title><link>https://arxiv.org/abs/2511.11340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces M-DAIGT, a shared task for multi-domain detection of AI-generated text with two binary subtasks: News Article Detection (NAD) and Academic Writing Detection (AWD).&lt;/li&gt;&lt;li&gt;Provides a new 30,000-sample benchmark dataset balanced between human-written and AI-generated texts, created using multiple modern LLMs (e.g., GPT-4, Claude) and varied prompting strategies.&lt;/li&gt;&lt;li&gt;46 teams registered for the task and four teams submitted final results; the paper describes participating teams' methods and outlines future directions for the benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Salima Lamsiyah', 'Saad Ezzini', 'Abdelkader El Mahdaouy', 'Hamza Alami', 'Abdessamad Benlahbib', 'Samir El Amrany', 'Salmane Chafik', 'Hicham Hammouchi']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-text detection', 'dataset', 'benchmark', 'evaluation', 'forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11340</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity</title><link>https://arxiv.org/abs/2511.11309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial attack strategy that generates obfuscated/ambiguous inputs to maximize model perplexity and confound NLP models.&lt;/li&gt;&lt;li&gt;Focuses on crafting adversarial examples across several datasets and includes experiments in Bangla to explore multilingual vulnerabilities.&lt;/li&gt;&lt;li&gt;Aims to evaluate existing attack methods, create new attack recipes, and inform future robustness improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saadat Rafid Ahmed', 'Rubayet Shareen', 'Radoan Sharkar', 'Nazia Hossain', 'Mansur Mahi', 'Farig Yousuf Sadeque']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'NLP', 'multilingual (Bangla)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11309</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title><link>https://arxiv.org/abs/2511.11141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRSM (Paraphrase Ranking Stability Metric) to quantify how CLIP's image–text alignment changes under paraphrased queries.&lt;/li&gt;&lt;li&gt;Evaluates CLIP on the Social Counterfactuals dataset to measure paraphrase-induced instability and its interaction with gendered language.&lt;/li&gt;&lt;li&gt;Finds variability in robustness across paraphrasing strategies and subtle, consistent differences between male- and female-associated queries, with implications for fairness in deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udo Schlegel', 'Franziska Weeber', 'Jian Lan', 'Thomas Seidl']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'fairness / bias', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11141</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Detect Their Own Hallucinations?</title><link>https://arxiv.org/abs/2511.11087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames hallucination detection as a sentence-level classification task and evaluates LLMs' ability to detect their own hallucinations.&lt;/li&gt;&lt;li&gt;Proposes using Chain-of-Thought (CoT) to extract internal knowledge for detection and measures capability via a proposed evaluation framework.&lt;/li&gt;&lt;li&gt;Empirical result: GPT-3.5 Turbo with CoT detected 58.2% of its own hallucinations, suggesting detection depends on knowledge encoded in model parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sora Kadotani', 'Kosuke Nishida', 'Kyosuke Nishida']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM self-evaluation', 'chain-of-thought', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11087</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets</title><link>https://arxiv.org/abs/2511.10985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic, data-centric analysis of five open-source direct preference optimization (DPO) corpora (TuluDPO, ORPO, UltraFeedback, HelpSteer, Code-Preference-Pairs) using the Magpie framework to annotate task category, input quality, and a reward-model-based preference signal.&lt;/li&gt;&lt;li&gt;Identifies structural and qualitative discrepancies across datasets (e.g., reward margins, noisy/redundant samples) and validates preference orders with a reward-model proxy instead of relying solely on human labels.&lt;/li&gt;&lt;li&gt;Curates a new mixed DPO dataset (UltraMix) by selectively removing noisy/redundant samples; UltraMix is 30% smaller than the best individual dataset yet yields improved performance on key benchmarks.&lt;/li&gt;&lt;li&gt;Public release of annotations, metadata, and the curated mixture to support reproducible, data-centric alignment research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Farhan Ahmed', 'Swanand Ravindra Kadhe', 'Syed Zawad', 'Heiko Ludwig', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'dataset curation', 'reward modeling', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10985</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</title><link>https://arxiv.org/abs/2511.10899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and names 'Tool-Induced Myopia' (TIM): TaLMs can over-rely on correct tool outputs as substitutes for coherent internal reasoning, producing seemingly correct answers without justified reasoning.&lt;/li&gt;&lt;li&gt;Introduces PYMATH, a 1,679-problem benchmark (competition-level math) to study where Python tool use is helpful but insufficient, and presents a multi-dimensional evaluation suite comparing TaLMs to non-tool LLMs.&lt;/li&gt;&lt;li&gt;Finds that tool use can increase final-answer accuracy (up to +19.3 pp) while degrading the quality of reasoning (non-tool LLMs win up to 41.5% more in pairwise reasoning comparisons); error modes shift toward global reasoning failures, with TIM present in ~55% of high-risk cases.&lt;/li&gt;&lt;li&gt;Proposes a preference-optimization-based realignment framework that encourages treating tools as assistive evidence, improving both answer accuracy and reasoning depth under tool use. Code and data are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farima Fatahi Bayat', 'Pouya Pezeshkpour', 'Estevam Hruschka']&lt;/li&gt;&lt;li&gt;Tags: ['Tool-augmented LLMs', 'Reasoning hallucinations', 'Alignment/Safety', 'Evaluation/Benchmarking', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10899</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge</title><link>https://arxiv.org/abs/2511.10881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and defines 'negative bias' in LLMs: a tendency to produce negative answers in binary (yes/no) tasks, and shows format-level influence on responses.&lt;/li&gt;&lt;li&gt;Proposes an evaluation pipeline that categorizes model parametric knowledge into correct, incorrect, and insufficient, enabling fine-grained analysis of negative bias sources.&lt;/li&gt;&lt;li&gt;Finds a shortcut behavior where lack of sufficient knowledge leads models to default to negative responses; context provision and explicit 'I don't know' options reduce bias while chain-of-thought prompting can amplify it.&lt;/li&gt;&lt;li&gt;Demonstrates that prompt type and format affect the direction and degree of negative bias, offering actionable insights for mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jongyoon Song', 'Sangwon Yu', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'LLM behavior', 'safety evaluation', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10881</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ICX360: In-Context eXplainability 360 Toolkit</title><link>https://arxiv.org/abs/2511.10879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ICX360, an open-source Python toolkit for explaining LLM outputs with a focus on user-provided context/prompts.&lt;/li&gt;&lt;li&gt;Implements three explainability methods supporting both black-box (perturbation) and white-box (gradient) approaches.&lt;/li&gt;&lt;li&gt;Provides tutorials and quick-start guides for use cases including retrieval-augmented generation, natural language generation, and jailbreaking.&lt;/li&gt;&lt;li&gt;Designed to help analyze how prompts and context influence LLM behavior, enabling inspection and evaluation workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Wei', 'Ronny Luss', 'Xiaomeng Hu', 'Lucas Monteiro Paes', 'Pin-Yu Chen', 'Karthikeyan Natesan Ramamurthy', 'Erik Miehling', 'Inge Vejsbjerg', 'Hendrik Strobelt']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'LLM', 'jailbreaking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10879</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</title><link>https://arxiv.org/abs/2511.10871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how reframing a factual query into a conversational judgment ("Is this speaker correct?") alters LLM conviction and decisions.&lt;/li&gt;&lt;li&gt;Introduces an evaluation framework comparing direct factual queries vs minimal dialogue framing, and applies a simple rebuttal perturbation to measure resilience to conversational pressure.&lt;/li&gt;&lt;li&gt;Finds model-specific vulnerabilities (e.g., sycophancy in GPT-4o-mini, over-critical behavior in Llama-8B-Instruct) and reports an average performance change of 9.24% across models.&lt;/li&gt;&lt;li&gt;Provides a reproducible diagnostic methodology with implications for trustworthiness, robustness, and alignment of dialogue systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parisa Rabbani', 'Nimet Beyza Bozdag', 'Dilek Hakkani-T\\"ur']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'robustness', 'sycophancy', 'adversarial prompting', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10871</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English</title><link>https://arxiv.org/abs/2511.10846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes emotion recognition performance on AAVE vs General American English using 2.7M geo-tagged LA tweets and a labeled subset of 875 tweets.&lt;/li&gt;&lt;li&gt;Creates community-informed 'silver' labels by AAVE-fluent (ingroup) annotators and compares them to GPT- and BERT-based models and non-ingroup annotations.&lt;/li&gt;&lt;li&gt;Finds substantially higher false positive anger predictions on AAVE (e.g., SpanEmo: 25% → 60%), and that model predictions correlate with profanity-like AAVE features and neighborhood racial demographics.&lt;/li&gt;&lt;li&gt;Concludes emotion AI can reinforce racial stereotypes and calls for culturally/dialect-informed affective systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebecca Dorn', 'Christina Chance', 'Casandra Rusti', 'Charles Bickham Jr.', 'Kai-Wei Chang', 'Fred Morstatter', 'Kristina Lerman']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias/fairness', 'robustness (dialect)', 'emotion recognition', 'social harm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10846</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs</title><link>https://arxiv.org/abs/2511.10768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cross-lingual framework combining TextRank-based sentence extraction, medical named entity recognition, and LLM fine-tuning to improve faithfulness of consumer health question (CHQ) summarization.&lt;/li&gt;&lt;li&gt;Fine-tunes LLaMA-2-7B on English (MeQSum) and Bangla (BanglaCHQ-Summ) datasets, reporting improvements in quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) over zero-shot baselines and prior systems.&lt;/li&gt;&lt;li&gt;Human evaluation indicates over 80% of generated summaries preserve critical medical information, supporting safer deployment of LLMs in healthcare contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ajwad Abrar', 'Nafisa Tabassum Oeshy', 'Prianka Maheru', 'Farzana Tabassum', 'Tareque Mohmud Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['medical summarization', 'faithfulness', 'LLM fine-tuning', 'safety evaluation', 'cross-lingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10768</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations</title><link>https://arxiv.org/abs/2511.10695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a bias evaluation framework (three tests) for assessing nation-level bias in LLMs using UN Security Council historical records, focusing on the five permanent members.&lt;/li&gt;&lt;li&gt;Finds nation-level biases vary by model and by evaluation context (biases are multidimensional); models with stronger reasoning exhibit reduced bias and better performance.&lt;/li&gt;&lt;li&gt;Proposes a debiasing approach combining Retrieval-Augmented Generation (RAG) with reflexion-based self-reflection to improve factual reasoning and reduce nation-level bias; validates improvements on GPT-4o-mini and Llama-3.3-70B.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonghyeon Choi', 'Yeonjun Choi', 'Hyun-chul Kim', 'Beakcheol Jang']&lt;/li&gt;&lt;li&gt;Tags: ['nation-level bias', 'alignment', 'safety-evaluation', 'debiasing', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10695</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Where does an LLM begin computing an instruction?</title><link>https://arxiv.org/abs/2511.10694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces three simple datasets (Key-Value, Quote Attribution, Letter Selection) plus two multi-hop compositions to probe instruction following.&lt;/li&gt;&lt;li&gt;Uses activation patching on contrastive prompt pairs to compute a layer-wise flip rate indicating when substituting residual activations changes model answers.&lt;/li&gt;&lt;li&gt;Identifies an 'onset' — an inflection point in the layer stack where interventions before it can flip outputs but interventions after have little effect; onset is consistent across Llama-family sizes and multi-hop tasks.&lt;/li&gt;&lt;li&gt;Provides a replicable method to locate and compare where instruction-following computation begins across tasks and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Pola', 'Vineeth N. Balasubramanian']&lt;/li&gt;&lt;li&gt;Tags: ['mechanistic-interpretability', 'instruction-following', 'alignment', 'activation-patching', 'LLM-internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10694</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</title><link>https://arxiv.org/abs/2511.10691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Squid Game, a dynamic adversarial evaluation environment with six elimination-style levels to test LLMs in resource-constrained and asymmetric-information interactive gameplay.&lt;/li&gt;&lt;li&gt;Evaluates 50+ LLMs across instruction-following, code, reasoning, planning, and safety alignment, finding generational performance shifts and evidence of models exploiting speculative shortcuts.&lt;/li&gt;&lt;li&gt;Argues that dynamic adversarial evaluation can reveal behaviors and potential benchmark contamination not visible in static benchmarks and can complement existing evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Chen', 'Wenjun Zhang', 'Guangtao Zhai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial evaluation', 'benchmarking', 'safety alignment', 'data contamination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10691</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data</title><link>https://arxiv.org/abs/2511.10689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes gender bias dynamics across three generations of recursive LLM-based text synthesis, finding equilibrium behavior rather than monotonic amplification or decay.&lt;/li&gt;&lt;li&gt;Uses three evaluation frameworks—rule-based pattern matching, embedding-based semantic similarity, and downstream task performance—to assess bias and mitigation efficacy.&lt;/li&gt;&lt;li&gt;Evaluates four mitigation strategies across three initial bias levels; contrastive augmentation (gender-swapped variants) yields large downstream bias reductions despite increasing embedding-based bias scores, highlighting metric divergence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Kattamuri', 'Arpita Vats', 'Harshwardhan Fartale', 'Rahul Raja', 'Akshata Kishore Moharir', 'Ishita Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'synthetic data', 'LLMs', 'fairness evaluation', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10689</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Modeling and Predicting Multi-Turn Answer Instability in Large Language Models</title><link>https://arxiv.org/abs/2511.10688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Simple multi-turn follow-up prompts (e.g., 'Think again' and reworded questions) can substantially degrade LLM accuracy over repeated turns (e.g., ~10% drop for Gemini 1.5 Flash over nine turns).&lt;/li&gt;&lt;li&gt;Model accuracy dynamics across turns can be effectively modeled with Markov chains, enabling prediction of accuracy probabilities and estimation of a long-run 'stationary' accuracy (on average ~8% lower than first-turn accuracy for Gemini 1.5 Flash).&lt;/li&gt;&lt;li&gt;Linear probes on model hidden states show predictive signal for future answer changes, suggesting internal states contain information usable to forecast instability.&lt;/li&gt;&lt;li&gt;Proposes stationary accuracy as a principled robustness metric for interactive settings and highlights fragility under repeated questioning that matters for high-stakes deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahang He', 'Rishi Ramachandran', 'Neel Ramachandran', 'Aryan Katakam', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Aryan Shrivastava']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial prompting', 'prompt injection', 'model probing', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10688</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A methodological analysis of prompt perturbations and their effect on attack success rates</title><link>https://arxiv.org/abs/2511.10686</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic empirical study of how prompt perturbations affect Attack Success Rate (ASR) across models aligned with SFT, DPO, and RLHF.&lt;/li&gt;&lt;li&gt;Uses statistical analysis to show that small prompt modifications can significantly increase or decrease ASR depending on alignment method.&lt;/li&gt;&lt;li&gt;Finds that existing attack benchmarks may miss vulnerabilities because ASR is sensitive to prompt variations, implying need for more robust evaluation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiago Machado', 'Maysa Malfiza Garcia de Macedo', 'Rogerio Abreu de Paula', 'Marcelo Carpinette Grave', 'Aminat Adebiyi', 'Luan Soares de Souza', 'Enrico Santarelli', 'Claudio Pinhanez']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection/jailbreaking', 'alignment robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10686</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency</title><link>https://arxiv.org/abs/2511.10671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Grounded Visual Factualization (GVF) Finetuning to reduce visual hallucinations in multimodal LLMs by injecting explicit factual signals during fine-tuning.&lt;/li&gt;&lt;li&gt;GVF comprises three mechanisms: Factual Anchor Data Augmentation (structured factual anchors and counterfactual prompts), Fact-Aware Instruction Tuning (embedding cues in instructions), and a Factual Consistency Loss that penalizes factual errors.&lt;/li&gt;&lt;li&gt;Evaluated on LLaVA-1.5-13B, GVF improves factual consistency on the VHTest benchmark (OEQ and YNQ) while preserving or slightly improving performance on general multimodal benchmarks (MME, POPE).&lt;/li&gt;&lt;li&gt;Claims to mitigate visual hallucinations without degrading general multimodal understanding and reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filippo Morbiato', 'Luca Romano', 'Alessandro Persona']&lt;/li&gt;&lt;li&gt;Tags: ['visual hallucination mitigation', 'factual consistency', 'MLLM alignment', 'instruction tuning', 'loss function design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10671</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title><link>https://arxiv.org/abs/2511.10665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of guard (safety) models to meaning-preserving paraphrases that cause large variability in safety scores.&lt;/li&gt;&lt;li&gt;Proposes a self-supervised training framework using paraphrase sets and a novel skew-aware aggregation strategy to enforce prediction consistency.&lt;/li&gt;&lt;li&gt;Shows empirical gains: ~58% reduction in semantic variability across paraphrases, ~2.5% average benchmark accuracy improvement, improved generalization to unseen stylistic variations.&lt;/li&gt;&lt;li&gt;Finds robustness training also improves model calibration (up to 40%) and that standard aggregations (mean/median) can harm safety, motivating skew-aware targets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristina Pinneri', 'Christos Louizos']&lt;/li&gt;&lt;li&gt;Tags: ['guard models', 'semantic robustness', 'adversarial paraphrases', 'safety evaluation', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10665</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bayesian Evaluation of Large Language Model Behavior</title><link>https://arxiv.org/abs/2511.10661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework to quantify statistical uncertainty in binary evaluation metrics for LLM outputs, focusing on uncertainty from probabilistic generation.&lt;/li&gt;&lt;li&gt;Applies the approach to two case studies: refusal rates on adversarial prompts designed to elicit harmful responses, and pairwise preference comparisons between LLMs on dialogue examples.&lt;/li&gt;&lt;li&gt;Demonstrates how uncertainty quantification can improve interpretation of safety/evaluation benchmarks and assessments of model behavior under adversarial inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachel Longjohn', 'Shang Wu', 'Saatvik Kher', "Catarina Bel\\'em", 'Padhraic Smyth']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety assessment', 'adversarial prompts', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10661</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models</title><link>https://arxiv.org/abs/2511.10656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRO (PReference Orchestrator), a lightweight preference adapter that infers prompt-specific preference weights for multi-objective alignment of LLMs.&lt;/li&gt;&lt;li&gt;Adapter is trained on normalized reward scores from multiple reward models to learn effective balances among objectives during training and deployment.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing prompt-aware preference weighting outperforms fixed preference weights, and reports empirical gains across multiple tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Biao Liu', 'Ning Xu', 'Junming Yang', 'Xin Geng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'multi-objective optimization', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10656</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Characterization of Temporal Constraint Processing in LLMs</title><link>https://arxiv.org/abs/2511.10654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates eight production-scale autoregressive LLMs on deadline/deadline-detection tasks and finds bimodal performance (models either ~95% or ~50% accuracy) and severe prompt brittleness (30–60 pp swings).&lt;/li&gt;&lt;li&gt;Identifies a systematic action bias (100% false positive rate in failing models), no clear correlation with parameter count in the tested range, and limited gains from small-scale fine-tuning (200 synthetic examples yield 12–37 pp improvements for partially capable models).&lt;/li&gt;&lt;li&gt;Argues temporal constraint satisfaction cannot be reliably learned via next-token prediction alone and recommends architectural mechanisms (continuous temporal state, explicit constraint checking, compositional temporal reasoning); warns of unacceptable risk when deploying autoregressive LLMs in time-critical agentic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Javier Mar\\'in"]&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'temporal reasoning', 'agentic deployment', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10654</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Instella: Fully Open Language Models with Stellar Performance</title><link>https://arxiv.org/abs/2511.10628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Instella: a family of fully open 3B-parameter LLMs trained on entirely open data and code, optimized for transparency and reproducibility.&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance among fully open models and is competitive with similar-size open-weight models despite using fewer pretraining tokens.&lt;/li&gt;&lt;li&gt;Provides two specialized variants: Instella-Long (context up to 128K tokens) and Instella-Math (mathematics-focused, SFT + RL fine-tuning).&lt;/li&gt;&lt;li&gt;Includes instruction tuning and alignment with human preferences but does not present detailed security/red-teaming or adversarial robustness evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiang Liu', 'Jialian Wu', 'Xiaodong Yu', 'Yusheng Su', 'Prakamya Mishra', 'Gowtham Ramesh', 'Sudhanshu Ranjan', 'Chaitanya Manem', 'Ximeng Sun', 'Ze Wang', 'Pratik Prabhanjan Brahma', 'Zicheng Liu', 'Emad Barsoum']&lt;/li&gt;&lt;li&gt;Tags: ['open-source LLM', 'alignment', 'instruction tuning', 'long-context', 'model release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10628</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Verified Code Reasoning by LLMs</title><link>https://arxiv.org/abs/2509.26546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to automatically validate LLM agents' code-reasoning answers by extracting a formal representation of the agent's response and applying formal verification and program-analysis tools to verify reasoning steps.&lt;/li&gt;&lt;li&gt;Evaluated on benchmarks: 20 uninitialized-variable sanitizer-detected errors (13/20 validated) and 20 program-equivalence queries where verification caught 6/8 incorrect agent judgments.&lt;/li&gt;&lt;li&gt;Aims to reduce human verification effort and increase trustworthiness of LLM-based coding assistants in high-precision settings (code comprehension, code review, automated code generation validation).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meghana Sistla', 'Gogul Balakrishnan', 'Pat Rondon', "Jos\\'e Cambronero", 'Michele Tufano', 'Satish Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'formal verification', 'code reasoning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26546</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title><link>https://arxiv.org/abs/2509.20379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an on-the-fly, computationally efficient hallucination detection method for VLMs using features derived from the model's next-token probabilities (NTPs).&lt;/li&gt;&lt;li&gt;Introduces a dataset of 1,400 human-annotated VLM-generated statements labeled for hallucination and evaluates lightweight ML classifiers trained on NTP-based signals.&lt;/li&gt;&lt;li&gt;Shows that NTPs (including linguistic NTPs computed from text-only inputs) are strong predictors of hallucination and that combining NTP features with VLM-provided hallucination scores improves detection.&lt;/li&gt;&lt;li&gt;Demonstrates comparable performance to stronger VLM-based detectors with lower latency and compute cost, enabling lightweight safety/reliability checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ofir Azachi', 'Kfir Eliyahu', 'Eyal El Ani', 'Rom Himelstein', 'Roi Reichart', 'Yuval Pinter', 'Nitay Calderon']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'vision-language models', 'next-token probabilities', 'model uncertainty', 'lightweight safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20379</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</title><link>https://arxiv.org/abs/2509.15695</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORIC, a framework/benchmark (ORIC-Bench) for evaluating object recognition under contextual incongruity in large vision-language models (LVLMs).&lt;/li&gt;&lt;li&gt;Constructs incongruous object-context pairs via two strategies: LLM-guided sampling to find hard-to-recognize present objects and CLIP-guided sampling to propose plausible but absent objects.&lt;/li&gt;&lt;li&gt;Evaluates 18 LVLMs and 2 open-vocabulary detectors, finding significant performance drops and systematic bias/hallucination patterns under incongruous contexts.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning (Visual Reinforcement Fine-Tuning) on a small ORIC-style dataset improves robustness on ORIC-Bench and related benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoyang Li', 'Zhan Ling', 'Yuchen Zhou', 'Litian Gong', 'Erdem B{\\i}y{\\i}k', 'Hao Su']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'hallucination', 'benchmarking', 'LVLMs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15695</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Sequentially Auditing Differential Privacy</title><link>https://arxiv.org/abs/2509.07055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical sequential (anytime) statistical test to audit differential privacy guarantees of black-box mechanisms while controlling Type I error.&lt;/li&gt;&lt;li&gt;Processes streams of mechanism outputs to provide anytime-valid inference, overcoming fixed-sample-size limitations of prior batch auditing methods.&lt;/li&gt;&lt;li&gt;Demonstrates much lower sample complexity (orders of magnitude fewer samples) to detect DP violations across diverse mechanisms.&lt;/li&gt;&lt;li&gt;Shows the test can identify DP-SGD privacy violations within under one training run, unlike prior methods requiring full model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Tom\\'as Gonz\\'alez", 'Mateo Dulce-Rubio', 'Aaditya Ramdas', "M\\'onica Ribero"]&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-auditing', 'DP-SGD', 'black-box-testing', 'privacy-violation-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.07055</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2508.19257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Temporal Token Fusion (TTF), a training-free method that fuses historical and current visual tokens via grayscale pixel-difference detection combined with attention-based semantic relevance to selectively integrate temporal information.&lt;/li&gt;&lt;li&gt;Uses hard fusion strategies and keyframe anchoring to avoid error accumulation; compatible with existing VLA architectures (OpenVLA, VLA-Cache) and claims computational benefits from selective Query matrix reuse.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements on LIBERO, SimplerEnv, and real-robot manipulation tasks (e.g., +4.0 pp on LIBERO; +8.7% relative on real robots), emphasizing robustness to visual noise and temporal coherence exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenghao Liu', 'Jiachen Zhang', 'Chengxuan Li', 'Zhimu Zhou', 'Shixin Wu', 'Songfang Huang', 'Huiling Duan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-action', 'temporal fusion', 'robotic manipulation', 'model-agnostic inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19257</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title><link>https://arxiv.org/abs/2508.15793</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a three-stage empirical study of 'format bias' in LLMs: (1) measuring presence and direction of bias across model families, (2) analyzing data-level drivers (information richness, structure quality, representation type), and (3) examining internal attention patterns and testing a lightweight intervention.&lt;/li&gt;&lt;li&gt;Finds consistent format biases across models, driven by information richness, structure quality, and representation type, and linked to attention imbalance within models.&lt;/li&gt;&lt;li&gt;Demonstrates attention re-weighting as an inference-time intervention and proposes data pre-processing (format repair/normalization) and format-balanced training corpora as mitigation strategies.&lt;/li&gt;&lt;li&gt;Identifies future research directions for reducing format bias to improve robust and fair heterogeneous data processing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liu', 'Mayi Xu', 'Qiankun Pi', 'Wenli Li', 'Ming Zhong', 'Yuanyuan Zhu', 'Mengchi Liu', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['format bias', 'robustness', 'attention analysis', 'LLM bias', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15793</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Latent Principle Discovery for Language Model Self-Improvement</title><link>https://arxiv.org/abs/2505.16927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an automated method to elicit latent behavioral principles from a language model and compress them into an interpretable set via clustering.&lt;/li&gt;&lt;li&gt;Uses a posterior-regularized Monte Carlo EM procedure to identify effective latent principles and trains the LM to invoke them for self-correction and improved outputs.&lt;/li&gt;&lt;li&gt;Demonstrates self-improvement on smaller LMs (7–8B) with gains on AlpacaEval, MT-Bench, and principle-following benchmarks; clustered principles form interpretable model-generated constitutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keshav Ramji', 'Tahira Naseem', "Ram\\'on Fernandez Astudillo"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improvement', 'latent-principles', 'principle-following', 'post-training-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16927</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems</title><link>https://arxiv.org/abs/2403.02912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents (ε, δ)-differentially-private stochastic mirror descent algorithms for convex-concave saddle-point problems in the ℓ1 setting that achieve nearly dimension-independent rates for the expected duality gap.&lt;/li&gt;&lt;li&gt;For first-order-smooth objectives they obtain rates ~ √(log d / n) + (log(d)^{3/2} / (n ε))^{1/3}; under second-order-smoothness they improve to √(log d / n) + log(d) / √(n ε) using bias-reduced gradient estimators.&lt;/li&gt;&lt;li&gt;Extends results to DP Stochastic Convex Optimization (SCO) in the ℓ1 setting (not relying on Frank-Wolfe) with accelerated methods, obtaining improved excess-risk bounds.&lt;/li&gt;&lt;li&gt;Technical contributions include extensions of the Maurey Sparsification Lemma and matching lower-bound discussion indicating near-optimality of the rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Tom\\'as Gonz\\'alez", "Crist\\'obal Guzm\\'an", 'Courtney Paquette']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-ml', 'stochastic-optimization', 'theoretical-privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.02912</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DiAReL: Reinforcement Learning with Disturbance Awareness for Robust Sim2Real Policy Transfer in Robot Control</title><link>https://arxiv.org/abs/2306.09010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a disturbance-augmented Markov decision process (DAMDP) for delayed MDPs to model unknown intrinsic disturbances on robot inputs.&lt;/li&gt;&lt;li&gt;Incorporates disturbance estimation into on-policy reinforcement learning to improve robustness to modeling uncertainties for sim2real transfer.&lt;/li&gt;&lt;li&gt;Validated on robotic reaching and pushing tasks, showing improved stabilization and robustness compared to disturbance-unaware baselines, aiding successful sim2real transfer.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadhossein Malmir (Department of Computer Engineering', 'School of Computation', 'Information and Technology', 'Technical University of Munich)', 'Josip Josifovski (Department of Computer Engineering', 'School of Computation', 'Information and Technology', 'Technical University of Munich)', 'Noah Klarmann (Rosenheim University of Applied Sciences)', 'Alois Knoll (Department of Computer Engineering', 'School of Computation', 'Information and Technology', 'Technical University of Munich)']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sim2real-transfer', 'reinforcement-learning', 'disturbance-estimation', 'robot-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.09010</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents CREAT, a constrained reinforcement-learning attack for profile pollution in sequential recommender systems to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Uses a bi-level optimization with multi-reward policy: pattern inversion rewards to flip critical sequence patterns and distribution-consistency rewards to minimize detectable shifts.&lt;/li&gt;&lt;li&gt;Introduces constrained group-relative RL with dynamic barrier constraints and group-shared replay to enable fine-grained, stealthy step-wise perturbations.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and stealthiness empirically, claiming improved attack success with lower detectability compared to prior PPA/data-poisoning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiang Chen', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'sequential recommender systems', 'profile pollution', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories</title><link>https://arxiv.org/abs/2511.08136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses offline safe imitation learning where the agent must avoid risky or undesirable behaviors conveyed by 'non-preferred' trajectories without online interaction or explicit per-timestep safety labels.&lt;/li&gt;&lt;li&gt;Proposes SafeMIL: learns a parameterized cost predictor for state-action pairs using Multiple Instance Learning to infer which trajectories are risky.&lt;/li&gt;&lt;li&gt;Uses the learned cost to constrain or guide policy learning so the resulting policy avoids non-preferred behaviors while retaining reward performance.&lt;/li&gt;&lt;li&gt;Empirical results claim SafeMIL produces safer policies that satisfy cost constraints and outperform several baselines on safety and reward metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Returaj Burnwal', 'Nirav Pravinbhai Bhatt', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['offline imitation learning', 'safe reinforcement learning', 'safety constraints', 'multiple instance learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08136</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift</title><link>https://arxiv.org/abs/2508.18839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formulates malware detection under concept drift as a one-step Markov Decision Process and trains a deep reinforcement learning (DRL) agent to jointly optimize classification and rejection (deferral) decisions.&lt;/li&gt;&lt;li&gt;Integrates rejection for manual labeling (active learning) into the policy to mitigate concept drift, evaluated with time-aware experiments on Android malware datasets.&lt;/li&gt;&lt;li&gt;Shows improved long-term performance (Area Under Time, AUT) versus standard classification baselines, reporting average AUT improvements of ~8.66 and ~10.90 for different policy variants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shae McFadden', 'Myles Foley', "Mario D'Onghia", 'Chris Hicks', 'Vasilios Mavroudis', 'Nicola Paoletti', 'Fabio Pierazzi']&lt;/li&gt;&lt;li&gt;Tags: ['concept drift', 'malware detection', 'reinforcement learning', 'active learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18839</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PrivDFS: Private Inference via Distributed Feature Sharing against Data Reconstruction Attacks</title><link>https://arxiv.org/abs/2508.04346</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivDFS, a distributed feature-sharing framework that fragments intermediate representations across multiple servers to prevent data reconstruction attacks (DRAs) in split inference for image classification.&lt;/li&gt;&lt;li&gt;Uses learnable binary masks to create sparse, largely non-overlapping feature shares processed by majority-honest servers, with a lightweight client-side fusion module to recover full task accuracy.&lt;/li&gt;&lt;li&gt;Demonstrates substantial degradation in DRA metrics (PSNR, SSIM) across CIFAR-10/100, CelebA, and ImageNet-1K while retaining accuracy within ~1% of non-private split inference.&lt;/li&gt;&lt;li&gt;Applies to both ResNet CNNs and Vision Transformers, presenting an architecture-agnostic structural defense against reconstructive leakage in cloud-based vision services.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Liu', 'Jiayi Wen', 'Junru Wu', 'Xuyang Zou', 'Shouhong Tan', 'Zhirun Zheng', 'Cheng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data reconstruction attacks', 'split inference', 'model privacy', 'defensive techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04346</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Necessity of Output Distribution Reweighting for Effective Class Unlearning</title><link>https://arxiv.org/abs/2506.20893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a privacy evaluation blindspot in class unlearning: ignoring class geometry enables membership leakage.&lt;/li&gt;&lt;li&gt;Introduces a membership-inference attack (MIA-NN) that leverages the model's probability mass over neighboring classes to detect allegedly unlearned samples.&lt;/li&gt;&lt;li&gt;Proposes Tilted ReWeighting (TRW), a fine-tuning objective that approximates the distribution a retrained-from-scratch model would produce by estimating inter-class similarity and tilting the target distribution; shows TRW improves unlearning metrics (U-LiRA and MIA-NN) on benchmarks like CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Ebrahimpour-Boroojeny', 'Yian Wang', 'Hari Sundaram']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'membership inference', 'privacy', 'defense', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20893</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Orthogonal Soft Pruning for Efficient Class Unlearning</title><link>https://arxiv.org/abs/2506.19891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedOrtho: a federated unlearning framework that enforces convolutional kernel orthogonality and local-global alignment to decouple features and reduce client drift.&lt;/li&gt;&lt;li&gt;Introduces an activation-driven one-shot soft pruning (OSP) mechanism to remove forgetting-related kernels while preserving retained knowledge, enabling class-, client-, and sample-level unlearning.&lt;/li&gt;&lt;li&gt;Reports SOTA forgetting quality (&gt;98%) and high retention (&gt;97%) on image benchmarks (CIFAR-10/100, TinyImageNet) with substantial compute/communication reductions and subsecond centralized erasure.&lt;/li&gt;&lt;li&gt;Claims mitigation of membership inference risks as part of the unlearning process and improved efficiency in federated settings under non-IID data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinghui Gong', 'Xue Yang', 'Xiaohu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['federated unlearning', 'data unlearning', 'privacy/membership inference', 'model pruning/efficiency', 'non-IID robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19891</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AMUN: Adversarial Machine UNlearning</title><link>https://arxiv.org/abs/2503.00917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AMUN: fine-tune a trained image classifier on adversarial examples constructed from the forget dataset to lower model confidence on those samples.&lt;/li&gt;&lt;li&gt;Adversarial-example-based updates localize decision-boundary changes around forget samples, minimizing impact on global model behavior and preserving test accuracy.&lt;/li&gt;&lt;li&gt;Empirical evaluation on CIFAR-10 shows AMUN outperforms prior approximate unlearning methods and reduces membership inference attack success to near-random when unlearning 10% of samples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Ebrahimpour-Boroojeny', 'Hari Sundaram', 'Varun Chandrasekaran']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'data-deletion', 'privacy', 'adversarial-examples', 'membership-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00917</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks</title><link>https://arxiv.org/abs/2207.03400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Populated Region Set (PRS) concept to characterize regions in input space where training samples are frequently populated and relate this geometry to model behavior.&lt;/li&gt;&lt;li&gt;Empirically shows that a lower PRS ratio correlates with increased adversarial robustness in deep neural networks.&lt;/li&gt;&lt;li&gt;Proposes a PRS-based regularizer to improve adversarial robustness without using adversarial training, and validates it through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongjin Park', 'Haedong Jeong', 'Tair Djanibekov', 'Giyoung Jeon', 'Jinseok Seol', 'Jaesik Choi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial attacks', 'model geometry', 'robustness regularizer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2207.03400</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses</title><link>https://arxiv.org/abs/2511.11381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematization of knowledge (SoK) on Wi‑Fi Channel State Information (CSI) used as a biometric modality, focusing on security properties and adversarial resilience.&lt;/li&gt;&lt;li&gt;Identifies methodological inconsistencies (aggregate accuracy focus, sparse FAR/FRR/EER reporting, lack of per-user risk) and introduces a unified evaluation framework exposing hidden risk concentration.&lt;/li&gt;&lt;li&gt;Demonstrates concrete attack surfaces (replay, geometric mimicry, environmental perturbation) and shows how choices in sensing, representation, and learning models affect vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides security boundaries, guidelines for rigorous evaluation and reproducible experiments, and directions for future research on CSI biometric security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Systematization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gioliano de Oliveira Braga', 'Pedro Henrique dos Santos Rocha', 'Rafael Pimenta de Mattos Paix\\~ao', 'Giovani Hoff da Costa', 'Gustavo Cavalcanti Morais', "Louren\\c{c}o Alves Pereira J\\'unior"]&lt;/li&gt;&lt;li&gt;Tags: ['biometrics', 'adversarial-ml', 'security-evaluation', 'threat-modeling', 'wireless-csi']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11381</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Questioning the Stability of Visual Question Answering</title><link>https://arxiv.org/abs/2511.11206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale, systematic study of Visual Language Model (VLM) robustness to benign, meaning-preserving visual and textual perturbations (pixel shifts, geometric transforms, padded rescaling, paraphrasing, multilingual rewrites).&lt;/li&gt;&lt;li&gt;Finds modern VLMs (including GPT-4o, Gemini 2.0 Flash) are highly sensitive: a substantial fraction of samples change answers under small perturbations; stability strongly correlates with correctness.&lt;/li&gt;&lt;li&gt;Shows stability signals from small open-source models can predict correctness of larger closed-source models; argues for robustness evaluations focused on expected invariances rather than only adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Rosenfeld', 'Neta Glazer', 'Ethan Fetaya']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'visual question answering', 'evaluation/benchmarking', 'model stability', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11206</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</title><link>https://arxiv.org/abs/2511.11169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlignVQA, a debate-based multi-agent framework where specialized vision-language models produce candidate answers and generalist agents critique, refine, and aggregate them to produce calibrated confidence estimates.&lt;/li&gt;&lt;li&gt;Introduces aligncal, a differentiable calibration-aware loss that fine-tunes specialized agents by minimizing an upper bound on calibration error, improving per-agent confidence fidelity.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple VQA benchmarks shows substantial reductions in calibration discrepancy, and findings that better-calibrated specialized agents yield improved aggregated confidences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayush Pandey', 'Jai Bardhan', 'Ishita Jain', 'Ramya S Hebbalaguppe', 'Rohan Raju Dhanakshirur', 'Lovekesh Vig']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'VQA', 'confidence estimation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11169</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title><link>https://arxiv.org/abs/2511.11141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRSM (Paraphrase Ranking Stability Metric) to quantify how CLIP's image–text alignment changes under paraphrased queries.&lt;/li&gt;&lt;li&gt;Evaluates CLIP on the Social Counterfactuals dataset to measure paraphrase-induced instability and its interaction with gendered language.&lt;/li&gt;&lt;li&gt;Finds variability in robustness across paraphrasing strategies and subtle, consistent differences between male- and female-associated queries, with implications for fairness in deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udo Schlegel', 'Franziska Weeber', 'Jian Lan', 'Thomas Seidl']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'fairness / bias', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11141</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ICX360: In-Context eXplainability 360 Toolkit</title><link>https://arxiv.org/abs/2511.10879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ICX360, an open-source Python toolkit for explaining LLM outputs with emphasis on user-provided context/prompts.&lt;/li&gt;&lt;li&gt;Implements three explainability methods using both black-box (perturbations) and white-box (gradients) approaches.&lt;/li&gt;&lt;li&gt;Includes tutorials and use cases such as retrieval-augmented generation, natural language generation, and jailbreaking.&lt;/li&gt;&lt;li&gt;Toolkit is publicly available (GitHub) and aimed at practical, user-facing explainability for LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Wei', 'Ronny Luss', 'Xiaomeng Hu', 'Lucas Monteiro Paes', 'Pin-Yu Chen', 'Karthikeyan Natesan Ramamurthy', 'Erik Miehling', 'Inge Vejsbjerg', 'Hendrik Strobelt']&lt;/li&gt;&lt;li&gt;Tags: ['explainability', 'jailbreaking', 'LLM interpretability', 'red-teaming', 'toolkit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10879</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title><link>https://arxiv.org/abs/2511.10720</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PISanitizer, a defense for prompt injection in long-context LLMs that pinpoints and sanitizes potential injected tokens before generation.&lt;/li&gt;&lt;li&gt;Method: intentionally allows the LLM to follow context instructions, identifies high-attention tokens driving instruction-following, and sanitizes them—creating a dilemma for attackers.&lt;/li&gt;&lt;li&gt;Evaluation shows improved prevention of prompt injection while maintaining utility, efficiency, and robustness against optimization-based and adaptive attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runpeng Geng', 'Yanting Wang', 'Chenlong Yin', 'Minhao Cheng', 'Ying Chen', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'prompt sanitization', 'LLM security', 'adversarial defense', 'long-context LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10720</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title><link>https://arxiv.org/abs/2511.10665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of guard (safety) models to meaning-preserving paraphrases that cause large variability in safety scores.&lt;/li&gt;&lt;li&gt;Proposes a self-supervised training framework using paraphrase sets and a novel skew-aware aggregation strategy to enforce prediction consistency.&lt;/li&gt;&lt;li&gt;Shows empirical gains: ~58% reduction in semantic variability across paraphrases, ~2.5% average benchmark accuracy improvement, improved generalization to unseen stylistic variations.&lt;/li&gt;&lt;li&gt;Finds robustness training also improves model calibration (up to 40%) and that standard aggregations (mean/median) can harm safety, motivating skew-aware targets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristina Pinneri', 'Christos Louizos']&lt;/li&gt;&lt;li&gt;Tags: ['guard models', 'semantic robustness', 'adversarial paraphrases', 'safety evaluation', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10665</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bayesian Evaluation of Large Language Model Behavior</title><link>https://arxiv.org/abs/2511.10661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework to quantify statistical uncertainty in binary evaluation metrics for LLM outputs, focusing on uncertainty from probabilistic generation.&lt;/li&gt;&lt;li&gt;Applies the approach to two case studies: refusal rates on adversarial prompts designed to elicit harmful responses, and pairwise preference comparisons between LLMs on dialogue examples.&lt;/li&gt;&lt;li&gt;Demonstrates how uncertainty quantification can improve interpretation of safety/evaluation benchmarks and assessments of model behavior under adversarial inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachel Longjohn', 'Shang Wu', 'Saatvik Kher', "Catarina Bel\\'em", 'Padhraic Smyth']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety assessment', 'adversarial prompts', 'uncertainty quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10661</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation</title><link>https://arxiv.org/abs/2511.11500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reinforced Hesitation (RH): a modification to RL from verifiable rewards using ternary rewards (+1 correct, 0 abstain, -λ incorrect) to train models to abstain when unsure.&lt;/li&gt;&lt;li&gt;Empirical evaluations on logic puzzles and benchmarks (GSM8K, MedQA, GPQA) show models can be tuned along a Pareto frontier between aggressive answering and conservative abstention by varying λ.&lt;/li&gt;&lt;li&gt;Introduces inference strategies (cascading across models with decreasing risk tolerance and self-cascading re-querying) that use abstention as a coordination signal and outperform majority voting with lower compute.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamad Amin Mohamadi', 'Tianhao Wang', 'Zhiyuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['Alignment', 'Safety', 'Abstention', 'Reinforcement Learning', 'Calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11500</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning</title><link>https://arxiv.org/abs/2511.11240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HealSplit, a unified defense framework tailored for Split Federated Learning (SFL) to detect and recover from five types of data/model poisoning attacks.&lt;/li&gt;&lt;li&gt;Detection: topology-aware module builds graphs over smashed data and scores topological anomalies (TAS) to identify poisoned samples.&lt;/li&gt;&lt;li&gt;Recovery and training: a generative pipeline synthesizes semantic substitutes validated by a consistency student, and an adversarial multi-teacher distillation (Vanilla Teacher + Anomaly-Influence Debiasing Teacher) trains the student using alignment between topological and gradient interaction matrices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhan Xie', 'Chen Lyu']&lt;/li&gt;&lt;li&gt;Tags: ['Split Federated Learning', 'Poisoning attacks', 'Defenses', 'Adversarial distillation', 'Anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11240</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm</title><link>https://arxiv.org/abs/2511.11009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies why vanilla virtual adversarial training (VAT) fails in unsupervised domain adaptation (UDA) and formulates a new Unsupervised Robust Domain Adaptation (URDA) paradigm to address robustness under domain shift.&lt;/li&gt;&lt;li&gt;Derives a generalization bound for URDA that extends classical UDA theory to incorporate adversarial attacks and robustness considerations.&lt;/li&gt;&lt;li&gt;Proposes DART (Disentangled Adversarial Robustness Training), a simple two-step procedure: pretrain any UDA model, then apply an instantaneous robustification post-training via disentangled distillation.&lt;/li&gt;&lt;li&gt;Empirically validates on four benchmark datasets that DART improves adversarial robustness while preserving domain transfer performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fuxiang Huang', 'Xiaowei Fu', 'Shiyu Ye', 'Lina Ma', 'Wen Li', 'Xinbo Gao', 'David Zhang', 'Lei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'domain adaptation', 'adversarial training', 'robustness theory', 'unsupervised learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11009</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework</title><link>https://arxiv.org/abs/2511.10915</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPP-FGC, a federated clustering framework that exchanges client-side structural graphs rather than embeddings or prototypes to preserve privacy.&lt;/li&gt;&lt;li&gt;Server securely aggregates and aligns client graphs to build a global graph for unified clustering; offers a one-shot mode and an iterative SPP-FGC+ mode for complex data (e.g., images).&lt;/li&gt;&lt;li&gt;Claims provable privacy guarantees while improving clustering metrics (up to ~10% NMI) over federated baselines.&lt;/li&gt;&lt;li&gt;Emphasizes communication efficiency (one-shot option) and empirical evaluation across datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanxiong He', 'Jie Wang', 'Liaoyuan Tang', 'Zheng Wang', 'Rong Wang', 'Feiping Nie']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy-preserving', 'graph aggregation', 'clustering', 'secure aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10915</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title><link>https://arxiv.org/abs/2511.10837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a principled evaluation framework separating extrinsic and intrinsic hallucinations and evaluates detection across curated benchmarks.&lt;/li&gt;&lt;li&gt;Proposes attention-based uncertainty quantification with novel attention aggregation strategies to improve interpretability and hallucination detection.&lt;/li&gt;&lt;li&gt;Empirically shows sampling-based methods (e.g., Semantic Entropy) work well for extrinsic hallucinations but fail for intrinsic ones, while attention aggregation better detects intrinsic hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elyes Hajji', 'Aymen Bouguerra', 'Fabio Arnez']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'attention-based uncertainty', 'intrinsic vs extrinsic hallucinations', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10837</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Instella: Fully Open Language Models with Stellar Performance</title><link>https://arxiv.org/abs/2511.10628</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Instella: a family of fully open 3B-parameter LLMs trained on entirely open data and code, optimized for transparency and reproducibility.&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance among fully open models and is competitive with similar-size open-weight models despite using fewer pretraining tokens.&lt;/li&gt;&lt;li&gt;Provides two specialized variants: Instella-Long (context up to 128K tokens) and Instella-Math (mathematics-focused, SFT + RL fine-tuning).&lt;/li&gt;&lt;li&gt;Includes instruction tuning and alignment with human preferences but does not present detailed security/red-teaming or adversarial robustness evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiang Liu', 'Jialian Wu', 'Xiaodong Yu', 'Yusheng Su', 'Prakamya Mishra', 'Gowtham Ramesh', 'Sudhanshu Ranjan', 'Chaitanya Manem', 'Ximeng Sun', 'Ze Wang', 'Pratik Prabhanjan Brahma', 'Zicheng Liu', 'Emad Barsoum']&lt;/li&gt;&lt;li&gt;Tags: ['open-source LLM', 'alignment', 'instruction tuning', 'long-context', 'model release']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10628</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</title><link>https://arxiv.org/abs/2511.10222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SACRED-Bench, a benchmark of speech-audio composition attacks (speech overlap/multi-speaker, speech-audio mixture, diverse spoken instruction formats) to red-team multimodal LLMs.&lt;/li&gt;&lt;li&gt;Finds high vulnerability of state-of-the-art models (e.g., Gemini 2.5 Pro with ~66% attack success) under cross-modal audio-based attacks that evade text-only filters.&lt;/li&gt;&lt;li&gt;Proposes SALMONN-Guard, an audio-aware safeguard LLM that inspects speech, audio, and text jointly and reduces attack success to ~20%.&lt;/li&gt;&lt;li&gt;Releases benchmark and checkpoints for community use; highlights need for audio-aware defenses for multimodal LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Yang', 'Xuezhen Zhang', 'Zhifeng Han', 'Siyin Wang', 'Jimin Zhuang', 'Zengrui Jin', 'Jing Shao', 'Guangzhi Sun', 'Chao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'multimodal safety', 'audio adversarial attacks', 'benchmarking', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10222</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents CREAT, a constrained reinforcement-learning attack for profile pollution in sequential recommender systems to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Uses a bi-level optimization with multi-reward policy: pattern inversion rewards to flip critical sequence patterns and distribution-consistency rewards to minimize detectable shifts.&lt;/li&gt;&lt;li&gt;Introduces constrained group-relative RL with dynamic barrier constraints and group-shared replay to enable fine-grained, stealthy step-wise perturbations.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness and stealthiness empirically, claiming improved attack success with lower detectability compared to prior PPA/data-poisoning methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiang Chen', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'sequential recommender systems', 'profile pollution', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories</title><link>https://arxiv.org/abs/2511.08136</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses offline safe imitation learning where the agent must avoid risky or undesirable behaviors conveyed by 'non-preferred' trajectories without online interaction or explicit per-timestep safety labels.&lt;/li&gt;&lt;li&gt;Proposes SafeMIL: learns a parameterized cost predictor for state-action pairs using Multiple Instance Learning to infer which trajectories are risky.&lt;/li&gt;&lt;li&gt;Uses the learned cost to constrain or guide policy learning so the resulting policy avoids non-preferred behaviors while retaining reward performance.&lt;/li&gt;&lt;li&gt;Empirical results claim SafeMIL produces safer policies that satisfy cost constraints and outperform several baselines on safety and reward metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Returaj Burnwal', 'Nirav Pravinbhai Bhatt', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['offline imitation learning', 'safe reinforcement learning', 'safety constraints', 'multiple instance learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08136</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2511.08015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvRoad, a method to generate naturalistic road-style adversarial posters that blend with road appearance to cause visual 3D detectors in autonomous driving to hallucinate non-existent objects.&lt;/li&gt;&lt;li&gt;Uses a two-stage pipeline: Road-Style Adversary Generation and Scenario-Associated Adaptation to maximize attack effectiveness while preserving stealthy, realistic appearance.&lt;/li&gt;&lt;li&gt;Demonstrates generalization across different detectors, scenes, and attack locations, and validates practicality with physical-world (real-world) attack experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Lijun He', 'Yixing Yong', 'Haixia Bi', 'Fan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'autonomous driving', 'visual 3D detection', 'physical-world attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08015</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems</title><link>https://arxiv.org/abs/2510.09616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Causal Digital Twin (CDT) framework that integrates causal inference with digital-twin modeling for anomaly detection and root-cause analysis in water industrial control systems (ICS).&lt;/li&gt;&lt;li&gt;CDT supports association (pattern detection), intervention (system response), and counterfactual analysis to both detect and mitigate cyber-physical attacks, reporting reduced false positives and improved root-cause accuracy.&lt;/li&gt;&lt;li&gt;Evaluated on SWaT, WADI, and HAI datasets with strong results (high F1-scores, 90.8% physical-constraint compliance, low structural Hamming distance) and claims of counterfactual defenses reducing attack success and low-latency real-time performance (~3.2 ms).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadhossein Homaei', 'Mehran Tarif', 'Pablo Garcia Rodriguez', 'Andres Caro', 'Mar Avila']&lt;/li&gt;&lt;li&gt;Tags: ['cyber-physical security', 'anomaly detection', 'causal inference', 'digital twin', 'industrial control systems (ICS)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09616</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SecInfer: Preventing Prompt Injection via Inference-time Scaling</title><link>https://arxiv.org/abs/2509.24967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecInfer, a defense against prompt injection that leverages inference-time scaling to allocate extra compute and explore diverse reasoning paths.&lt;/li&gt;&lt;li&gt;Two-step method: (1) system-prompt-guided sampling — generate multiple candidate responses using varied system prompts; (2) target-task-guided aggregation — select the response most likely to accomplish the intended task.&lt;/li&gt;&lt;li&gt;Evaluated against existing and adaptive prompt injection attacks; shows improved mitigation compared to fine-tuning–based defenses and other inference-time scaling methods.&lt;/li&gt;&lt;li&gt;Demonstrates that extra inference compute (diverse sampling + selection) can substantially improve robustness to prompt injection without model fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yupei Liu', 'Yanting Wang', 'Yuqi Jia', 'Jinyuan Jia', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM defenses', 'inference-time scaling', 'adversarial robustness', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24967</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2508.19257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Temporal Token Fusion (TTF), a training-free method that fuses historical and current visual tokens via grayscale pixel-difference detection combined with attention-based semantic relevance to selectively integrate temporal information.&lt;/li&gt;&lt;li&gt;Uses hard fusion strategies and keyframe anchoring to avoid error accumulation; compatible with existing VLA architectures (OpenVLA, VLA-Cache) and claims computational benefits from selective Query matrix reuse.&lt;/li&gt;&lt;li&gt;Demonstrates consistent improvements on LIBERO, SimplerEnv, and real-robot manipulation tasks (e.g., +4.0 pp on LIBERO; +8.7% relative on real robots), emphasizing robustness to visual noise and temporal coherence exploitation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenghao Liu', 'Jiachen Zhang', 'Chengxuan Li', 'Zhimu Zhou', 'Shixin Wu', 'Songfang Huang', 'Huiling Duan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language-action', 'temporal fusion', 'robotic manipulation', 'model-agnostic inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19257</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Necessity of Output Distribution Reweighting for Effective Class Unlearning</title><link>https://arxiv.org/abs/2506.20893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a privacy evaluation blindspot in class unlearning: ignoring class geometry enables membership leakage.&lt;/li&gt;&lt;li&gt;Introduces a membership-inference attack (MIA-NN) that leverages the model's probability mass over neighboring classes to detect allegedly unlearned samples.&lt;/li&gt;&lt;li&gt;Proposes Tilted ReWeighting (TRW), a fine-tuning objective that approximates the distribution a retrained-from-scratch model would produce by estimating inter-class similarity and tilting the target distribution; shows TRW improves unlearning metrics (U-LiRA and MIA-NN) on benchmarks like CIFAR-10.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Ebrahimpour-Boroojeny', 'Yian Wang', 'Hari Sundaram']&lt;/li&gt;&lt;li&gt;Tags: ['data unlearning', 'membership inference', 'privacy', 'defense', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20893</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Orthogonal Soft Pruning for Efficient Class Unlearning</title><link>https://arxiv.org/abs/2506.19891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FedOrtho: a federated unlearning framework that enforces convolutional kernel orthogonality and local-global alignment to decouple features and reduce client drift.&lt;/li&gt;&lt;li&gt;Introduces an activation-driven one-shot soft pruning (OSP) mechanism to remove forgetting-related kernels while preserving retained knowledge, enabling class-, client-, and sample-level unlearning.&lt;/li&gt;&lt;li&gt;Reports SOTA forgetting quality (&gt;98%) and high retention (&gt;97%) on image benchmarks (CIFAR-10/100, TinyImageNet) with substantial compute/communication reductions and subsecond centralized erasure.&lt;/li&gt;&lt;li&gt;Claims mitigation of membership inference risks as part of the unlearning process and improved efficiency in federated settings under non-IID data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinghui Gong', 'Xue Yang', 'Xiaohu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['federated unlearning', 'data unlearning', 'privacy/membership inference', 'model pruning/efficiency', 'non-IID robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19891</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Latent Principle Discovery for Language Model Self-Improvement</title><link>https://arxiv.org/abs/2505.16927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an automated method to elicit latent behavioral principles from a language model and compress them into an interpretable set via clustering.&lt;/li&gt;&lt;li&gt;Uses a posterior-regularized Monte Carlo EM procedure to identify effective latent principles and trains the LM to invoke them for self-correction and improved outputs.&lt;/li&gt;&lt;li&gt;Demonstrates self-improvement on smaller LMs (7–8B) with gains on AlpacaEval, MT-Bench, and principle-following benchmarks; clustered principles form interpretable model-generated constitutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keshav Ramji', 'Tahira Naseem', "Ram\\'on Fernandez Astudillo"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'self-improvement', 'latent-principles', 'principle-following', 'post-training-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16927</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Are language models rational? The case of coherence norms and belief revision</title><link>https://arxiv.org/abs/2406.03442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether norms of rationality—specifically coherence norms—apply to language models, covering logical coherence and coherence tied to strength of belief.&lt;/li&gt;&lt;li&gt;Introduces the Minimal Assent Connection (MAC) and a proposed account of credence that derives strength of belief from model internal next-token probabilities.&lt;/li&gt;&lt;li&gt;Argues that rationality norms apply to some language models but not others, and discusses implications for predicting/explaining model behavior and for AI safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Hofweber', 'Peter Hase', 'Elias Stengel-Eskin', 'Mohit Bansal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'AI-safety', 'belief-revision', 'model-behavior', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.03442</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Fractured Glass, Failing Cameras: Simulating Physics-Based Adversarial Samples for Autonomous Driving Systems</title><link>https://arxiv.org/abs/2405.15033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates real-world camera glass failures cause object-detection models to fail, motivating study of physics-based adversarial samples from hardware damage.&lt;/li&gt;&lt;li&gt;Develops a simulation pipeline using finite element modeling (FEM) to generate realistic crack geometries and physically-based rendering (PBR) to synthesize broken-glass image corruptions.&lt;/li&gt;&lt;li&gt;Applies synthesized broken-glass filters to driving and large-scale detection datasets (KITTI, BDD100K, MS-COCO) and measures detection degradation on YOLOv8, Faster R-CNN, and transformer-based detectors.&lt;/li&gt;&lt;li&gt;Analyzes distributional shifts using Kullback–Leibler divergence (including a custom cracked-windshield capture) to quantify the impact of glass failures on model robustness and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manav Prabhakar', 'Jwalandhar Girnar', 'Arpan Kusari']&lt;/li&gt;&lt;li&gt;Tags: ['physics-based adversarial examples', 'autonomous driving', 'robustness', 'camera hardware failure', 'computer vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.15033</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title><link>https://arxiv.org/abs/2511.10067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multifaceted Self-Refinement (MuSeR): an attribute-conditioned query generator that simulates diverse user contexts, LLM self-evaluation across three facets (decision-making, communication, safety), and supervised fine-tuning on refined responses to improve context-awareness.&lt;/li&gt;&lt;li&gt;Targets medical domain context-awareness (recognizing missing/critical details and producing safe, context-appropriate responses) and incorporates knowledge distillation to transfer improvements to smaller models.&lt;/li&gt;&lt;li&gt;Reports SOTA results on HealthBench among open-source LLMs (63.8% overall, 43.1% on hard subset), with distilled Qwen3-32B surpassing its teacher.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Zhou', 'Yubin Wang', 'Bin Wang', 'Chen Ning', 'Xien Liu', 'Ji Wu', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'medical LLMs', 'context-awareness', 'self-refinement', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10067</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</title><link>https://arxiv.org/abs/2511.06626</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes self-report fine-tuning (SRFT), a supervised technique to train LLMs to admit factual mistakes and, by extension, disclose hidden misaligned objectives.&lt;/li&gt;&lt;li&gt;Evaluates SRFT in out-of-distribution stealth/adversarial agentic tasks where models try to pursue hidden objectives alongside user goals, showing large improvements in confession and content recovery.&lt;/li&gt;&lt;li&gt;Shows interrogation of SRFT models detects hidden objectives with near-ceiling performance (F1 = 0.98) and recovers substantial objective details (28–100%), compared to failure of baseline models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chloe Li', 'Mary Phuong', 'Daniel Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'honesty/prompted confession', 'adversarial agentic behavior', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.06626</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models</title><link>https://arxiv.org/abs/2511.11502</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows many LVLM object-hallucinations occur because the model relies on prelim (previously generated) tokens rather than the image, quantified via mutual information between image and predicted object conditioned on prelim.&lt;/li&gt;&lt;li&gt;Introduces Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens that can be calculated on the fly during inference with no extra forward passes.&lt;/li&gt;&lt;li&gt;Demonstrates PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nhat Hoang-Xuan', 'Minh Vu', 'My T. Thai', 'Manish Bhattarai']&lt;/li&gt;&lt;li&gt;Tags: ['object-hallucination', 'hallucination-detection', 'vision-language-models', 'attention-analysis', 'safety-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11502</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents</title><link>https://arxiv.org/abs/2511.11468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VRD-UQA, a benchmark to evaluate Visual LLMs' resilience to plausible but unanswerable questions in multi-page visually rich documents.&lt;/li&gt;&lt;li&gt;Automatically generates corruptions by swapping entities across document elements, positions, or pages and verifies unanswerability using a VLLM-as-judge approach.&lt;/li&gt;&lt;li&gt;Evaluates 12 VLLMs on detection accuracy at page and document levels, analyzes effects of corruption types (NLP entity, document element, layout), and tests knowledge-injection/in-context strategies (OCR, multi-page selection, unanswerability cues).&lt;/li&gt;&lt;li&gt;Finds notable limitations in current VLLMs and positions VRD-UQA as a framework for developing more robust document VQA systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Napolitano', 'Luca Cagliero', 'Fabrizio Battiloro']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmarking', 'visual-llms', 'unanswerable-questions', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11468</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions</title><link>https://arxiv.org/abs/2511.11347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/review of privacy risks and mitigation strategies for retrieval-augmented generation (RAG) systems used in healthcare chatbots, analyzing 23 RAG application papers and 17 privacy-protection papers.&lt;/li&gt;&lt;li&gt;Presents a pipeline-structured threat analysis across data storage, transmission, retrieval, and generation stages, identifying failure modes, threat models, and practical implications for PHI exposure.&lt;/li&gt;&lt;li&gt;Critically evaluates existing defenses, highlights gaps (lack of clinical validation, no standard evaluation frameworks, absence of automated assessment tools), and proposes actionable research directions for improving privacy in clinical RAG deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaowei Guan', 'Hin Chi Kwok', 'Ngai Fong Law', 'Gregor Stiglic', 'Vivian Hui']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'retrieval-augmented generation (RAG)', 'healthcare/PHI', 'threat modeling', 'privacy-preserving techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11347</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text</title><link>https://arxiv.org/abs/2511.11340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces M-DAIGT, a shared task for multi-domain detection of AI-generated text with two binary subtasks: News Article Detection (NAD) and Academic Writing Detection (AWD).&lt;/li&gt;&lt;li&gt;Provides a new 30,000-sample benchmark dataset balanced between human-written and AI-generated texts, created using multiple modern LLMs (e.g., GPT-4, Claude) and varied prompting strategies.&lt;/li&gt;&lt;li&gt;46 teams registered for the task and four teams submitted final results; the paper describes participating teams' methods and outlines future directions for the benchmark.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Salima Lamsiyah', 'Saad Ezzini', 'Abdelkader El Mahdaouy', 'Hamza Alami', 'Abdessamad Benlahbib', 'Samir El Amrany', 'Salmane Chafik', 'Hicham Hammouchi']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated-text detection', 'dataset', 'benchmark', 'evaluation', 'forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11340</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models</title><link>https://arxiv.org/abs/2511.11299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AUVIC, an adversarial perturbation–based framework to enable targeted visual concept unlearning in multimodal LLMs without full retraining.&lt;/li&gt;&lt;li&gt;Focuses on precise removal of target visual concepts while preserving performance on related/non-target entities.&lt;/li&gt;&lt;li&gt;Introduces VCUBench, a benchmark for evaluating visual concept unlearning in grouped contexts, and reports state-of-the-art forgetting with minimal collateral degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haokun Chen', 'Jianing Li', 'Yao Zhang', 'Jinhe Bi', 'Yan Xia', 'Jindong Gu', 'Volker Tresp']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'adversarial examples', 'multimodal LLMs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11299</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning</title><link>https://arxiv.org/abs/2511.11240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HealSplit, a unified defense framework tailored for Split Federated Learning (SFL) to detect and recover from five types of data/model poisoning attacks.&lt;/li&gt;&lt;li&gt;Detection: topology-aware module builds graphs over smashed data and scores topological anomalies (TAS) to identify poisoned samples.&lt;/li&gt;&lt;li&gt;Recovery and training: a generative pipeline synthesizes semantic substitutes validated by a consistency student, and an adversarial multi-teacher distillation (Vanilla Teacher + Anomaly-Influence Debiasing Teacher) trains the student using alignment between topological and gradient interaction matrices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhan Xie', 'Chen Lyu']&lt;/li&gt;&lt;li&gt;Tags: ['Split Federated Learning', 'Poisoning attacks', 'Defenses', 'Adversarial distillation', 'Anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11240</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</title><link>https://arxiv.org/abs/2511.11169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlignVQA, a debate-based multi-agent framework where specialized vision-language models produce candidate answers and generalist agents critique, refine, and aggregate them to produce calibrated confidence estimates.&lt;/li&gt;&lt;li&gt;Introduces aligncal, a differentiable calibration-aware loss that fine-tunes specialized agents by minimizing an upper bound on calibration error, improving per-agent confidence fidelity.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple VQA benchmarks shows substantial reductions in calibration discrepancy, and findings that better-calibrated specialized agents yield improved aggregated confidences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayush Pandey', 'Jai Bardhan', 'Ishita Jain', 'Ramya S Hebbalaguppe', 'Rohan Raju Dhanakshirur', 'Lovekesh Vig']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'alignment', 'VQA', 'confidence estimation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11169</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title><link>https://arxiv.org/abs/2511.11030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Deep vision models (DenseNet121, SwinV2-B, MedMamba) can predict patient health insurance type (a proxy for socioeconomic status) from normal chest X-rays with AUC ≈ 0.67–0.68 on MIMIC-CXR-JPG and CheXpert.&lt;/li&gt;&lt;li&gt;The predictive signal persists after controlling for age, race, and sex, and remains when training within a single racial group, indicating models learn a diffuse socioeconomic signature rather than obvious demographic cues.&lt;/li&gt;&lt;li&gt;Patch-based occlusion analyses show the signal is diffuse across upper and mid-thoracic regions, suggesting models may pick up on subtle artifacts from clinical environments, equipment, or care pathways.&lt;/li&gt;&lt;li&gt;Implication: medical images are not neutral biological data; models can internalize social fingerprints, raising concerns for fairness, privacy leakage, and unintended discriminatory use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi-Yu Chen', 'Rawan Abulibdeh', 'Arash Asgari', 'Leo Anthony Celi', 'Deirdre Goode', 'Hassan Hamidi', 'Laleh Seyyed-Kalantari', 'Po-Chih Kuo', 'Ned McCague', 'Thomas Sounack']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'algorithmic fairness', 'medical imaging', 'latent confounders', 'socioeconomic bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11030</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis</title><link>https://arxiv.org/abs/2511.11020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive security analysis of data poisoning threats across healthcare AI architectures (CNNs, LLMs, RL agents) and infrastructure (federated learning, documentation systems).&lt;/li&gt;&lt;li&gt;Empirical claims that small-scale poisoning (100–500 samples) can successfully compromise models, with detection delayed months or never, and widespread impact via supply chain and insider vectors.&lt;/li&gt;&lt;li&gt;Identifies specific attack scenarios (e.g., Medical Scribe Sybil) and systemic risk factors (distributed infrastructure, privacy laws hindering detection, federated learning obscuring attribution).&lt;/li&gt;&lt;li&gt;Proposes multilayer defenses: mandatory adversarial testing, ensemble-based detection, privacy-preserving security measures, international standards, and a move toward interpretable models for high-stakes care.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farhad Abtahi', 'Fernando Seoane', "Iv\\'an Pau", 'Mario Vega-Barbas']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'healthcare AI security', 'supply chain attacks', 'federated learning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11020</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets</title><link>https://arxiv.org/abs/2511.10985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic, data-centric analysis of five open-source direct preference optimization (DPO) corpora (TuluDPO, ORPO, UltraFeedback, HelpSteer, Code-Preference-Pairs) using the Magpie framework to annotate task category, input quality, and a reward-model-based preference signal.&lt;/li&gt;&lt;li&gt;Identifies structural and qualitative discrepancies across datasets (e.g., reward margins, noisy/redundant samples) and validates preference orders with a reward-model proxy instead of relying solely on human labels.&lt;/li&gt;&lt;li&gt;Curates a new mixed DPO dataset (UltraMix) by selectively removing noisy/redundant samples; UltraMix is 30% smaller than the best individual dataset yet yields improved performance on key benchmarks.&lt;/li&gt;&lt;li&gt;Public release of annotations, metadata, and the curated mixture to support reproducible, data-centric alignment research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aladin Djuhera', 'Farhan Ahmed', 'Swanand Ravindra Kadhe', 'Syed Zawad', 'Heiko Ludwig', 'Holger Boche']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'dataset curation', 'reward modeling', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10985</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting</title><link>https://arxiv.org/abs/2511.10949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeAgents, a unified framework for fine-grained security assessment of LLM-based multi-agent systems (MAS).&lt;/li&gt;&lt;li&gt;Proposes Dharma, a diagnostic measure to identify weak links in multi-agent pipelines and quantify susceptibility to adversarial prompting.&lt;/li&gt;&lt;li&gt;Empirically evaluates five MAS architectures (centralized, decentralized, hybrid) across four datasets covering web tasks, tool use, and code generation, revealing design patterns that increase vulnerability (e.g., delegating only atomic instructions obscures harmful objectives).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirmit Arora', 'Sathvik Joel', 'Ishan Kavathekar', 'Palak', 'Rohan Gandhi', 'Yash Pandya', 'Tanuja Ganu', 'Aditya Kanade', 'Akshay Nambi']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'adversarial prompting', 'LLM security', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10949</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio</title><link>https://arxiv.org/abs/2511.10913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HARMGEN, a suite of five attacks against LALM-based TTS systems in two families: semantic obfuscation (Concat, Shuffle) and audio-modality exploits (Read, Spell, Phoneme).&lt;/li&gt;&lt;li&gt;Evaluates attacks across five commercial LALM TTS systems and three datasets (two languages), showing large reductions in refusal rates and increased toxicity of generated speech.&lt;/li&gt;&lt;li&gt;Analyzes reactive (platform) and proactive (provider) defenses: finds deepfake detectors underperform on high-fidelity audio, reactive moderation can be bypassed with adversarial perturbations, and proactive moderation detects 57–93% of attacks.&lt;/li&gt;&lt;li&gt;Highlights a novel content-centric misuse vector for TTS and calls for robust cross-modal safeguards throughout training and deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangke Chen', 'Yuhui Wang', 'Shouling Ji', 'Xiapu Luo', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['TTS security', 'adversarial attacks', 'jailbreaking/prompt injection', 'audio deepfakes', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10913</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge</title><link>https://arxiv.org/abs/2511.10881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and defines 'negative bias' in LLMs: a tendency to produce negative answers in binary (yes/no) tasks, and shows format-level influence on responses.&lt;/li&gt;&lt;li&gt;Proposes an evaluation pipeline that categorizes model parametric knowledge into correct, incorrect, and insufficient, enabling fine-grained analysis of negative bias sources.&lt;/li&gt;&lt;li&gt;Finds a shortcut behavior where lack of sufficient knowledge leads models to default to negative responses; context provision and explicit 'I don't know' options reduce bias while chain-of-thought prompting can amplify it.&lt;/li&gt;&lt;li&gt;Demonstrates that prompt type and format affect the direction and degree of negative bias, offering actionable insights for mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jongyoon Song', 'Sangwon Yu', 'Sungroh Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'LLM behavior', 'safety evaluation', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10881</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English</title><link>https://arxiv.org/abs/2511.10846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes emotion recognition performance on AAVE vs General American English using 2.7M geo-tagged LA tweets and a labeled subset of 875 tweets.&lt;/li&gt;&lt;li&gt;Creates community-informed 'silver' labels by AAVE-fluent (ingroup) annotators and compares them to GPT- and BERT-based models and non-ingroup annotations.&lt;/li&gt;&lt;li&gt;Finds substantially higher false positive anger predictions on AAVE (e.g., SpanEmo: 25% → 60%), and that model predictions correlate with profanity-like AAVE features and neighborhood racial demographics.&lt;/li&gt;&lt;li&gt;Concludes emotion AI can reinforce racial stereotypes and calls for culturally/dialect-informed affective systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebecca Dorn', 'Christina Chance', 'Casandra Rusti', 'Charles Bickham Jr.', 'Kai-Wei Chang', 'Fred Morstatter', 'Kristina Lerman']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias/fairness', 'robustness (dialect)', 'emotion recognition', 'social harm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10846</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title><link>https://arxiv.org/abs/2511.10837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a principled evaluation framework separating extrinsic and intrinsic hallucinations and evaluates detection across curated benchmarks.&lt;/li&gt;&lt;li&gt;Proposes attention-based uncertainty quantification with novel attention aggregation strategies to improve interpretability and hallucination detection.&lt;/li&gt;&lt;li&gt;Empirically shows sampling-based methods (e.g., Semantic Entropy) work well for extrinsic hallucinations but fail for intrinsic ones, while attention aggregation better detects intrinsic hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elyes Hajji', 'Aymen Bouguerra', 'Fabio Arnez']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'attention-based uncertainty', 'intrinsic vs extrinsic hallucinations', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10837</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title><link>https://arxiv.org/abs/2511.10720</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PISanitizer, a defense for prompt injection in long-context LLMs that pinpoints and sanitizes potential injected tokens before generation.&lt;/li&gt;&lt;li&gt;Method: intentionally allows the LLM to follow context instructions, identifies high-attention tokens driving instruction-following, and sanitizes them—creating a dilemma for attackers.&lt;/li&gt;&lt;li&gt;Evaluation shows improved prevention of prompt injection while maintaining utility, efficiency, and robustness against optimization-based and adaptive attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runpeng Geng', 'Yanting Wang', 'Chenlong Yin', 'Minhao Cheng', 'Ying Chen', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'prompt sanitization', 'LLM security', 'adversarial defense', 'long-context LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10720</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2511.10714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadThink, a backdoor attack that, when triggered by specific prompts, induces ‘overthinking’ in CoT-enabled LLMs—producing excessively long reasoning traces while preserving final answer correctness.&lt;/li&gt;&lt;li&gt;Implements the attack via poisoning-based fine-tuning using an LLM-driven iterative optimization to generate naturalistic poisoned examples that embed the behavior stealthily.&lt;/li&gt;&lt;li&gt;Demonstrates robustness and stealth across multiple SOTA models and reasoning tasks, achieving up to a 17x increase in reasoning trace length on MATH-500, thereby significantly increasing computation and inference latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuaitong Liu', 'Renjue Li', 'Lijia Yu', 'Lijun Zhang', 'Zhiming Liu', 'Gaojie Jin']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'data poisoning', 'chain-of-thought', 'adversarial ML', 'computational DoS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10714</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging</title><link>https://arxiv.org/abs/2511.10712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'model merging stealing' as a new threat where adversaries free-ride by unauthorized merging of open-source expert models into base LLMs.&lt;/li&gt;&lt;li&gt;Proposes MergeBarrier, a plug-and-play defense that disrupts Linear Mode Connectivity (LMC) between a protected model and homologous models to eliminate low-loss paths used for effective merging.&lt;/li&gt;&lt;li&gt;Claims the method proactively prevents unauthorized merging while remaining compatible with open-source settings and incurring negligible performance loss.&lt;/li&gt;&lt;li&gt;Provides extensive experiments demonstrating effectiveness at blocking model-merge attacks with minimal accuracy degradation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinfeng Li', 'Miao Pan', 'Jintao Chen', 'Fu Teng', 'Zhiqiang Shen', 'Ge Su', 'Hao Peng', 'Xuhong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'model merging', 'model stealing', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10712</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</title><link>https://arxiv.org/abs/2511.10691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Squid Game, a dynamic adversarial evaluation environment with six elimination-style levels to test LLMs in resource-constrained and asymmetric-information interactive gameplay.&lt;/li&gt;&lt;li&gt;Evaluates 50+ LLMs across instruction-following, code, reasoning, planning, and safety alignment, finding generational performance shifts and evidence of models exploiting speculative shortcuts.&lt;/li&gt;&lt;li&gt;Argues that dynamic adversarial evaluation can reveal behaviors and potential benchmark contamination not visible in static benchmarks and can complement existing evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Chen', 'Wenjun Zhang', 'Guangtao Zhai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial evaluation', 'benchmarking', 'safety alignment', 'data contamination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10691</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data</title><link>https://arxiv.org/abs/2511.10689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes gender bias dynamics across three generations of recursive LLM-based text synthesis, finding equilibrium behavior rather than monotonic amplification or decay.&lt;/li&gt;&lt;li&gt;Uses three evaluation frameworks—rule-based pattern matching, embedding-based semantic similarity, and downstream task performance—to assess bias and mitigation efficacy.&lt;/li&gt;&lt;li&gt;Evaluates four mitigation strategies across three initial bias levels; contrastive augmentation (gender-swapped variants) yields large downstream bias reductions despite increasing embedding-based bias scores, highlighting metric divergence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Kattamuri', 'Arpita Vats', 'Harshwardhan Fartale', 'Rahul Raja', 'Akshata Kishore Moharir', 'Ishita Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'synthetic data', 'LLMs', 'fairness evaluation', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10689</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title><link>https://arxiv.org/abs/2511.10687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretical framework to convert system-level evaluation of multi-LLM agent systems into agent- and message-level training signals using cooperative game-theoretic attribution (e.g., Shapley) and process reward modeling.&lt;/li&gt;&lt;li&gt;Generates local, signed, credit-conserving rewards that promote cooperation, discourage redundancy or sabotage, and produce repair-aware penalties for failure cases by localizing first errors and rewarding corrective steps.&lt;/li&gt;&lt;li&gt;Signals are bounded and designed to be compatible with reinforcement- or preference-based post-training, offering an auditable pathway from global evaluation to local supervision.&lt;/li&gt;&lt;li&gt;Work is conceptual/theoretical; empirical validation is left for future work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Hsuan Yang', 'Tanwi Mallick', 'Le Chen', 'Krishnan Raghavan', 'Azton Wells', 'Amal Gueroudji', 'Ian T. Foster', 'Rajeev Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent LLMs', 'credit assignment', 'reward modeling', 'safety signals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10687</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title><link>https://arxiv.org/abs/2511.10665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of guard (safety) models to meaning-preserving paraphrases that cause large variability in safety scores.&lt;/li&gt;&lt;li&gt;Proposes a self-supervised training framework using paraphrase sets and a novel skew-aware aggregation strategy to enforce prediction consistency.&lt;/li&gt;&lt;li&gt;Shows empirical gains: ~58% reduction in semantic variability across paraphrases, ~2.5% average benchmark accuracy improvement, improved generalization to unseen stylistic variations.&lt;/li&gt;&lt;li&gt;Finds robustness training also improves model calibration (up to 40%) and that standard aggregations (mean/median) can harm safety, motivating skew-aware targets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristina Pinneri', 'Christos Louizos']&lt;/li&gt;&lt;li&gt;Tags: ['guard models', 'semantic robustness', 'adversarial paraphrases', 'safety evaluation', 'model calibration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10665</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models</title><link>https://arxiv.org/abs/2511.10656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRO (PReference Orchestrator), a lightweight preference adapter that infers prompt-specific preference weights for multi-objective alignment of LLMs.&lt;/li&gt;&lt;li&gt;Adapter is trained on normalized reward scores from multiple reward models to learn effective balances among objectives during training and deployment.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing prompt-aware preference weighting outperforms fixed preference weights, and reports empirical gains across multiple tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Biao Liu', 'Ning Xu', 'Junming Yang', 'Xin Geng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'multi-objective optimization', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10656</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Characterization of Temporal Constraint Processing in LLMs</title><link>https://arxiv.org/abs/2511.10654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates eight production-scale autoregressive LLMs on deadline/deadline-detection tasks and finds bimodal performance (models either ~95% or ~50% accuracy) and severe prompt brittleness (30–60 pp swings).&lt;/li&gt;&lt;li&gt;Identifies a systematic action bias (100% false positive rate in failing models), no clear correlation with parameter count in the tested range, and limited gains from small-scale fine-tuning (200 synthetic examples yield 12–37 pp improvements for partially capable models).&lt;/li&gt;&lt;li&gt;Argues temporal constraint satisfaction cannot be reliably learned via next-token prediction alone and recommends architectural mechanisms (continuous temporal state, explicit constraint checking, compositional temporal reasoning); warns of unacceptable risk when deploying autoregressive LLMs in time-critical agentic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Javier Mar\\'in"]&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'temporal reasoning', 'agentic deployment', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10654</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title><link>https://arxiv.org/abs/2511.11551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a test-time, model-guided policy-shaping method to steer pre-trained RL agents toward ethical behaviors without retraining.&lt;/li&gt;&lt;li&gt;Uses scenario-action attribute classifiers to control individual behavioral attributes and trade off alignment vs. reward at decision time.&lt;/li&gt;&lt;li&gt;Introduces and evaluates on the MACHIAVELLI benchmark (134 text-based game environments, thousands of annotated ethical scenarios), comparing to training-time methods and analyzing ethical violations and power-seeking behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dena Mujtaba', 'Brian Hu', 'Anthony Hoogs', 'Arslan Basharat']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'RL safety', 'test-time policy shaping', 'behavior steering', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11551</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms</title><link>https://arxiv.org/abs/2511.11323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RLSLM, a hybrid reinforcement learning framework that incorporates a rule-based Social Locomotion Model into the RL reward to produce orientation-sensitive social comfort fields for navigation.&lt;/li&gt;&lt;li&gt;Balances mechanical energy and social comfort in the objective to avoid intrusions into personal/group space and improve human experience.&lt;/li&gt;&lt;li&gt;Validated via immersive VR human-agent interaction experiments showing improved user experience over state-of-the-art rule-based models; includes ablation and sensitivity analyses highlighting interpretability.&lt;/li&gt;&lt;li&gt;Emphasizes integrating cognitive-science-derived rules with data-driven RL to achieve socially aligned, sample-efficient navigation policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitian Kou', 'Yihe Gu', 'Chen Zhou', 'DanDan Zhu', 'Shuguang Kuai']&lt;/li&gt;&lt;li&gt;Tags: ['social navigation', 'human-robot interaction', 'alignment', 'reinforcement learning', 'safety (social/behavioral)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11323</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment</title><link>https://arxiv.org/abs/2511.11301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EcoAlign, an inference-time framework that treats LVLMs as boundedly rational agents and conducts an economical search over a thought graph to balance safety, utility, and compute cost.&lt;/li&gt;&lt;li&gt;Introduces a forward-looking scoring function (net-present-value-like) to weight expected safety/utility against remaining budget, and applies a weakest-link principle to enforce path safety and prevent deceptive reasoning.&lt;/li&gt;&lt;li&gt;Reports experiments on 3 closed-source and 2 open-source LVLMs across 6 datasets, claiming equal or better safety and utility with lower computational cost compared to SOTA methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoxi Cheng', 'Haoxuan Ma', 'Teng Ma', 'Hongyi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM alignment', 'jailbreak mitigation', 'inference-time defenses', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11301</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>A Workflow for Full Traceability of AI Decisions</title><link>https://arxiv.org/abs/2511.11275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical workflow to produce tamper-proof, verifiable, and exhaustive traces of AI training and inference, expanding the DBOM concept into an operational system.&lt;/li&gt;&lt;li&gt;Leverages confidential computing to enforce documentation of every component involved in model development and decision-making to support legal-grade traceability and responsibility reconstruction.&lt;/li&gt;&lt;li&gt;Demonstrates the workflow with an application (classifying poisonous vs edible mushrooms) as a concrete proof-of-concept.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julius Wenzel', 'Syeda Umaima Alam', 'Andreas Schmidt', 'Hanwei Zhang', 'Holger Hermanns']&lt;/li&gt;&lt;li&gt;Tags: ['traceability', 'provenance', 'auditable AI', 'confidential computing', 'accountability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11275</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios</title><link>https://arxiv.org/abs/2511.11252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UAVBench: a 50,000-scenario dataset of UAV flight situations generated via LLM prompting with structured JSON descriptions and quantitative risk labels.&lt;/li&gt;&lt;li&gt;Provides UAVBench_MCQ: 50,000 multiple-choice questions targeting cognitive and ethical reasoning (navigation, aerodynamics, multi-agent coordination, resource-constrained and ethics-aware decisions).&lt;/li&gt;&lt;li&gt;Includes multi-stage safety validation and evaluation scripts; evaluates 32 state-of-the-art LLMs and highlights strengths in perception/policy reasoning and weaknesses in ethics-aware and resource-limited decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Amine Ferrag', 'Abderrahmane Lakas', 'Merouane Debbah']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarks', 'UAV-autonomy', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11252</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</title><link>https://arxiv.org/abs/2511.11182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multi-agent Undercover Gaming (MUG), a protocol to detect and mitigate hallucinating agents in multi-agent debate setups by introducing multimodal counterfactual tests.&lt;/li&gt;&lt;li&gt;Modifies reference images to create counterfactual evidence and evaluates whether agents detect changes, using detection of 'undercover' (hallucinating) agents to improve overall reasoning reliability.&lt;/li&gt;&lt;li&gt;Claims improvements over standard Multi-Agent Debate by enabling factual verification beyond consensus, introducing cross-evidence reasoning with dynamic evidence, and fostering active probing discussions among agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dayong Liang', 'Xiao-Yong Wei', 'Changmeng Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'multimodal reasoning', 'alignment', 'robustness', 'counterfactual testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11182</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints</title><link>https://arxiv.org/abs/2511.10952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Characterizes requirements for autonomous agents to construct, evaluate, and justify candidate courses of action when no option fully satisfies all operational constraints.&lt;/li&gt;&lt;li&gt;Identifies types of contextual knowledge (normative, pragmatic, situational) needed for decisions that are robust to agent goals and aligned with human expectations.&lt;/li&gt;&lt;li&gt;Argues agents must go beyond trained policies to dynamically integrate normative and situational understanding and provides empirical case studies illustrating these points.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Steven J. Jones', 'Robert E. Wray', 'John E. Laird']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'AI safety', 'autonomous agents', 'operational constraints', 'normative reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10952</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems</title><link>https://arxiv.org/abs/2511.10704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a “Second Law of Intelligence”: ethical entropy (divergence from intended goals) tends to increase unless continuous alignment work is applied.&lt;/li&gt;&lt;li&gt;Formally defines entropy over a goal distribution and proves dS/dt ≥ 0 for gradient-based optimizers driven by exploration noise and specification gaming; derives a critical alignment strength gamma_crit = (lambda_max / 2) ln N.&lt;/li&gt;&lt;li&gt;Validates the theory with simulations (e.g., 7B parameter model) showing drift in entropy below gamma_crit and stability when alignment work exceeds the threshold; reframes alignment as continuous thermodynamic control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samih Fadli']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'Safety', 'Training dynamics', 'Theoretical analysis', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10704</guid><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
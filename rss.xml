<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 14 Nov 2025 23:29:36 +0000</lastBuildDate><item><title>Boosting Adversarial Transferability via Ensemble Non-Attention</title><link>https://arxiv.org/abs/2511.08937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NAMEA, an ensemble adversarial attack that integrates gradients from non-attention areas of ensemble models (CNNs and ViTs) to improve cross-architecture transferability.&lt;/li&gt;&lt;li&gt;Decouples gradients from attention and non-attention regions and merges them via a meta-learning-based strategy to reduce gradient variance across heterogeneous surrogate models.&lt;/li&gt;&lt;li&gt;Evaluated on ImageNet, showing substantial improvements over prior ensemble attacks (e.g., +15.0% vs AdaEA, +9.6% vs SMER).&lt;/li&gt;&lt;li&gt;First work to explicitly exploit ensemble non-attention regions to boost transferability across architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yipeng Zou', 'Qin Liu', 'Jie Wu', 'Yu Peng', 'Guo Chen', 'Hui Zhou', 'Guanghui Ye']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'ensemble attacks', 'vision transformer', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08937</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios</title><link>https://arxiv.org/abs/2510.26125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WOD-E2E, a benchmark/dataset of 4,021 driving segments (~12 hours) from Waymo Open Dataset curated for rare, challenging long-tail driving scenarios (occurrence &lt;0.03%).&lt;/li&gt;&lt;li&gt;Each segment includes high-level routing, ego states, and 360-degree camera views (8 cameras); intended for vision-based end-to-end driving research.&lt;/li&gt;&lt;li&gt;Proposes a new open-loop evaluation metric, Rater Feedback Score (RFS), which scores predicted trajectories against rater-annotated trajectory preferences; validation labels released, test labels held for a 2025 challenge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runsheng Xu', 'Hubert Lin', 'Wonseok Jeon', 'Hao Feng', 'Yuliang Zou', 'Liting Sun', 'John Gorman', 'Ekaterina Tolstaya', 'Sarah Tang', 'Brandyn White', 'Ben Sapp', 'Mingxing Tan', 'Jyh-Jing Hwang', 'Dragomir Anguelov']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'long-tail-robustness', 'dataset-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26125</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</title><link>https://arxiv.org/abs/2509.03113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-time method to reduce multimodal LLM hallucinations without finetuning or auxiliary models.&lt;/li&gt;&lt;li&gt;Estimates token- and visual-feature-level contributions using first-order Taylor gradients to detect text-visual bias and co-occurrence bias.&lt;/li&gt;&lt;li&gt;Mitigates hallucinations via two mechanisms: suppressing spurious visual features correlated with predicted objects and rebalancing cross-modal contributions by strengthening visual signals vs. text.&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks, showing improved visual grounding and reduced hallucination rates for MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shan Wang', 'Maying Shen', 'Nadine Chang', 'Chuong Nguyen', 'Hongdong Li', 'Jose M. Alvarez']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-hallucination', 'hallucination-mitigation', 'gradient-based', 'inference-time-method', 'alignment-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03113</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</title><link>https://arxiv.org/abs/2508.12711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies GenAI-driven news diversity as a new challenge that induces multi-level drift in LVLM-based multimodal misinformation detection (MMD): model-level misperception drift and evidence-level drift.&lt;/li&gt;&lt;li&gt;Introduces DriftBench, a 16,000-instance benchmark spanning six diversification categories, and defines three evaluation tasks: robustness under drift, susceptibility to adversarial evidence contamination, and reasoning consistency across diverse inputs.&lt;/li&gt;&lt;li&gt;Evaluates six state-of-the-art LVLM detectors, finding substantial performance degradation (avg F1 drop ~14.8%) and unstable reasoning traces, with worse failures when adversarial GenAI-generated evidence is injected.&lt;/li&gt;&lt;li&gt;Concludes that current MMD systems have fundamental vulnerabilities to GenAI-driven diversity and adversarial evidence, calling for more resilient detection approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanxiao Li', 'Jiaying Wu', 'Tingchao Fu', 'Yunyun Dong', 'Bingbing Song', 'Wei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal misinformation', 'robustness', 'adversarial attacks', 'LVLM', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12711</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2507.19110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LISA, a plug-and-play layer-wise integration and suppression method to mitigate object hallucinations in multimodal LLMs by modulating activations and fusing token logits from selected layers.&lt;/li&gt;&lt;li&gt;Uses layer-wise spectral modulation to suppress over-amplified signals in deep layers while preserving grounding cues in early layers, plus anchor-based token routing and soft logit fusion during decoding.&lt;/li&gt;&lt;li&gt;Evaluated across benchmarks (e.g., CHAIRI) and models (including Qwen2.5-VL), reporting up to 53.6% reduction in hallucinations and up to 5.1% improvement in POPE F1, claiming strong generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhihui Guo', 'Xin Man', 'Hui Xu', 'Jie Shao', 'Zhiguo Jiang', 'Xianchao Zhang', 'Heng Tao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'multimodal LLMs', 'safety/robustness', 'layer-wise model intervention', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19110</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Image-based Outlier Synthesis With Training Data</title><link>https://arxiv.org/abs/2411.10794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ASCOOD, a training-time method that synthesizes virtual outliers from in-distribution images by perturbing inputs with gradient attribution to disrupt invariant features while amplifying true-class logits.&lt;/li&gt;&lt;li&gt;Trains models to both classify ID data and produce high predictive uncertainty on these synthesized near-manifold outliers using z-score standardized features.&lt;/li&gt;&lt;li&gt;Targets challenging OOD scenarios including spurious-correlation settings and fine-grained OOD that are visually similar to ID, without using any external data.&lt;/li&gt;&lt;li&gt;Evaluated on 7 datasets with comparisons to 30+ baselines, showing improved OOD detection in spurious, fine-grained, and conventional settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sudarshan Regmi']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'spurious_correlation', 'data_synthesis', 'fine-grained_classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10794</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Two Heads are Better than One: Robust Learning Meets Multi-branch Models</title><link>https://arxiv.org/abs/2208.08083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BORT (Branch Orthogonality adveRsarial Training): a multi-branch network architecture with a branch-orthogonal loss to encourage orthogonal solution spaces across branches.&lt;/li&gt;&lt;li&gt;Combines the multi-branch design with adversarial training (no extra data) to improve robustness against l_infty-bounded perturbations (ε = 8/255).&lt;/li&gt;&lt;li&gt;Evaluated on CIFAR-10, CIFAR-100, and SVHN; reports state-of-the-art robust accuracy without additional data (e.g., +7.23% on CIFAR-10, +9.07% on CIFAR-100 over prior no-extra-data methods).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongyuan Zhang', 'Qingwen Bu', 'Tianyang Duan', 'Zheng Lin', 'Yuhao Qing', 'Zihan Fang', 'Heming Cui', 'Dong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'defense/robustness', 'model architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2208.08083</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Querying Labeled Time Series Data with Scenario Programs</title><link>https://arxiv.org/abs/2511.10627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes matching between labeled time-series sensor data and abstract scenario programs written in the Scenic probabilistic programming language.&lt;/li&gt;&lt;li&gt;Presents a querying algorithm that identifies portions of real-world datasets that match a given scenario program, enabling validation of simulation-discovered failure scenarios against real data.&lt;/li&gt;&lt;li&gt;Reports higher accuracy and orders-of-magnitude faster query times compared to a state-of-the-art commercial vision large language model, with scalability in query duration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Kim', 'Devan Shanker', 'Varun Bharadwaj', 'Hongbeen Park', 'Jinkyu Kim', 'Hazem Torfah', 'Daniel J Fremont', 'Sanjit A Seshia']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'simulation-to-real (sim-to-real)', 'scenario-based testing', 'probabilistic programming (Scenic)', 'time-series querying']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10627</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title><link>https://arxiv.org/abs/2511.10094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Matryoshka Transcoders, a method for hierarchical sparse feature learning to automatically discover physical plausibility failure modes in generative models.&lt;/li&gt;&lt;li&gt;Trains on intermediate representations from a physical-plausibility classifier and uses large multimodal models to produce natural-language interpretations of discovered visual/physics-related patterns.&lt;/li&gt;&lt;li&gt;Produces a benchmark for evaluating physical plausibility and analyzes eight state-of-the-art generative models to surface common physics constraint violations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Tang', 'Abhijeet Sinha', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'failure modes', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10094</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>eXIAA: eXplainable Injections for Adversarial Attack</title><link>https://arxiv.org/abs/2511.10088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box, model-agnostic single-step adversarial attack that alters post-hoc explanations (saliency maps, integrated gradients, DeepLIFT SHAP) while keeping the model's predicted class and probabilities unchanged.&lt;/li&gt;&lt;li&gt;Requires only access to model predictions and provided explanations (no internal weights or gradients) and is evaluated on pretrained ResNet-18 and ViT-B16 on ImageNet.&lt;/li&gt;&lt;li&gt;Quantitatively measures explanation change (mean absolute difference) and image similarity (SSIM), demonstrating that explanations can be dramatically perturbed without visible image changes — highlighting risks for safety-critical uses of XAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonardo Pesce', 'Jiawen Wei', 'Gianmarco Mengaldo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'explainability', 'black-box', 'XAI-manipulation', 'image-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10088</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems</title><link>https://arxiv.org/abs/2511.10050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adversarial Retroreflective Patch (ARP): a physical, deployable, and stealthy attack on traffic sign recognition that activates under vehicle headlight illumination.&lt;/li&gt;&lt;li&gt;Develops a retroreflection simulation and uses black-box optimization to design patches; reports ≥93.4% success at 35m in dynamic scenarios and ≥60% against commercial TSR in real-world tests.&lt;/li&gt;&lt;li&gt;User study shows ARP has similar or better visual stealthiness compared to benign signs and previous patch attacks.&lt;/li&gt;&lt;li&gt;Proposes DPR Shield defense using strategically placed polarized filters, achieving ≥75% defense success rates for stop and speed-limit signs against micro-prism patches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Go Tsuruoka', 'Takami Sato', 'Qi Alfred Chen', 'Kazuki Nomoto', 'Ryunosuke Kobayashi', 'Yuna Tanaka', 'Tatsuya Mori']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-attacks', 'traffic-sign-recognition', 'retroreflective-attack', 'autonomous-vehicle-security', 'defenses-polarization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10050</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation</title><link>https://arxiv.org/abs/2511.10382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes limitations of adversarial perturbation defenses (e.g., Anti-DreamBooth) intended to prevent identity leakage in personalized image generation.&lt;/li&gt;&lt;li&gt;Identifies two key failure modes: perceptible artifacts in adversarial examples and high fragility to simple, non-learned purification filters.&lt;/li&gt;&lt;li&gt;Proposes AntiDB_Purify, an evaluation framework that tests defenses under realistic purification threats (traditional filters and adversarial purification), and shows existing methods fail to maintain protection.&lt;/li&gt;&lt;li&gt;Concludes current defenses give a false sense of security and calls for more imperceptible and robust protections to safeguard user identity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhen Chen', 'Yi Zhang', 'Xiangyu Yin', 'Chengxuan Qin', 'Xingyu Zhao', 'Xiaowei Huang', 'Wenjie Ruan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defenses', 'privacy', 'model personalization', 'adversarial purification', 'image security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10382</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation</title><link>https://arxiv.org/abs/2511.10370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SHRUG-FM, a reliability-aware framework combining input-space OOD detection, embedding-space OOD detection, and task-specific predictive uncertainty for foundation models in Earth observation.&lt;/li&gt;&lt;li&gt;Evaluated on burn scar segmentation: OOD scores correlate with lower model performance in specific environmental conditions, while uncertainty flags help filter many poor predictions.&lt;/li&gt;&lt;li&gt;Analyzes geographic concentration of failures via HydroATLAS land-cover attributes, linking underrepresentation in pretraining data (e.g., low-elevation and large-river regions) to reliability issues and proposing a path toward safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai-Hendrik Cohrs', 'Zuzanna Osika', 'Maria Gonzalez-Calabuig', 'Vishal Nedungadi', 'Ruben Cartuyvels', 'Steffen Knoblauch', 'Joppe Massant', 'Shruti Nath', 'Patrick Ebel', 'Vasileios Sitokonstantinou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'out-of-distribution-detection', 'uncertainty-estimation', 'deployment-safety', 'geospatial-foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10370</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection</title><link>https://arxiv.org/abs/2511.10308</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes eight error categories for pedestrian detection and uses image segmentation to automatically classify detector errors.&lt;/li&gt;&lt;li&gt;Introduces new, per-error-category metrics to provide a finer-grained and more realistic evaluation of pedestrian detectors, emphasizing safety-critical performance.&lt;/li&gt;&lt;li&gt;Applies the metrics to compare various backbones on a simplified APD and reports SOTA on CityPersons-reasonable without extra training data using a simple architecture.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Feifel', 'Benedikt Franke', 'Frank Bonarens', 'Frank K\\"oster', 'Arne Raulf', 'Friedhelm Schwenker']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'autonomous driving', 'pedestrian detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10308</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models</title><link>https://arxiv.org/abs/2511.10292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RUDDER, an inference-time, low-overhead steering framework to reduce object hallucination in large vision-language models.&lt;/li&gt;&lt;li&gt;Introduces CARD, a per-sample visual evidence vector extracted from a single forward-pass residual update, and a Bayesian-inspired adaptive gate for token-wise corrective injection.&lt;/li&gt;&lt;li&gt;Demonstrates comparable performance to state-of-the-art hallucination mitigation methods on benchmarks (POPE, CHAIR) with negligible additional latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengtao Zou', 'Ya Gao', 'Jiarui Guan', 'Bin Li', 'Pekka Marttinen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'inference-time steering', 'vision-language models', 'robustness', 'efficiency/latency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10292</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization</title><link>https://arxiv.org/abs/2511.10212</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a single-stage multimodal deepfake detection framework that uses next-frame feature prediction for uni-modal and cross-modal features to improve generalization without pretraining on large real-sample corpora.&lt;/li&gt;&lt;li&gt;Introduces a window-level attention mechanism to compare predicted vs. actual frames, enabling detection of local intra-modal artifacts and precise temporal localization of spoofed segments.&lt;/li&gt;&lt;li&gt;Demonstrates improved generalization across benchmark datasets and effective localization of fully and partially manipulated videos.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashutosh Anshul', 'Shreyas Gopal', 'Deepu Rajan', 'Eng Siong Chng']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'multimodal', 'temporal-localization', 'generalization', 'next-frame-prediction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10212</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction</title><link>https://arxiv.org/abs/2511.10203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VISTA, a recursive goal-conditioned transformer that fuses long-horizon intent with past motion and models social interactions via social-token attention and pairwise attention maps.&lt;/li&gt;&lt;li&gt;Targets multi-agent trajectory forecasting with an emphasis on generating socially compliant, interpretable trajectories and reducing collisions in dense interactive scenes.&lt;/li&gt;&lt;li&gt;Evaluated on MADRAS and SDD benchmarks, showing state-of-the-art accuracy and large reductions in trajectory collision rates (e.g., 2.14% -&gt; 0.03% on MADRAS).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Da Silva Martins', 'Emanuel Aldea', "Sylvie Le H\\'egarat-Mascle"]&lt;/li&gt;&lt;li&gt;Tags: ['trajectory prediction', 'autonomous driving', 'safety / collision avoidance', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10203</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models</title><link>https://arxiv.org/abs/2511.10098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MTAttack, a multi-target backdoor attack framework targeting large vision-language models (LVLMs) that plants multiple independent triggers mapping to different malicious outputs.&lt;/li&gt;&lt;li&gt;Introduces two constraints for joint latent-space trigger optimization: Proxy Space Partitioning (to assign unique proxy classes) and Trigger Prototype Anchoring (to ensure trigger separability).&lt;/li&gt;&lt;li&gt;Demonstrates high multi-target attack success rates, cross-dataset generalizability, and robustness against several backdoor defenses on popular benchmarks.&lt;/li&gt;&lt;li&gt;Provides code and highlights urgent security risks of LVLMs to multi-target backdoor threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Guansong Pang', 'Wenjun Miao', 'Jin Zheng', 'Xiao Bai']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'LVLM', 'multimodal security', 'adversarial attack', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10098</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?</title><link>https://arxiv.org/abs/2511.10059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AV-ConfuseBench to evaluate whether MLLMs can detect 'audio-visual confusion' where an object is visually present but its sound is muted/absent.&lt;/li&gt;&lt;li&gt;Shows that current MLLMs (e.g., Qwen2.5-Omni, Gemini 2.5) are visually dominated and often fail to recognize non-existent audio.&lt;/li&gt;&lt;li&gt;Proposes RL-CoMM: an RL-based collaborative framework that uses a Large Audio Language Model (LALM) as an audio-only reference, a step-wise reasoning reward, and answer-centered confidence optimization to improve audio-visual reasoning.&lt;/li&gt;&lt;li&gt;Reports 10–30% accuracy gains on audio-visual QA and audio-visual hallucination tasks with limited training data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilang Ye', 'Wei Zeng', 'Meng Liu', 'Jie Zhang', 'Yupeng Hu', 'Zitong Yu', 'Yu Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['audio-visual hallucination', 'multimodal robustness', 'benchmark', 'reinforcement learning', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10059</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection</title><link>https://arxiv.org/abs/2511.10035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DGFusion, a dual-guided multi-modal fusion paradigm that combines Point-guide-Image and Image-guide-Point to better handle hard instances (distant, small, occluded) in 3D detection.&lt;/li&gt;&lt;li&gt;Introduces Difficulty-aware Instance Pair Matcher (DIPM) to form easy/hard instance pairs for instance-level difficulty-aware feature matching and fusion.&lt;/li&gt;&lt;li&gt;Demonstrates empirical improvements on nuScenes (+1.0% mAP, +0.8% NDS, +1.3% AR) and shows consistent robustness gains across ego-distance, object size, visibility, and small-scale training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiyang Jia', 'Caiyan Jia', 'Ailin Liu', 'Shaoqing Xu', 'Qiming Xia', 'Lin Liu', 'Lei Yang', 'Yan Gong', 'Ziying Song']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'autonomous-driving', 'multimodal-fusion', '3d-object-detection', 'safety-critical-perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10035</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MOBA: A Material-Oriented Backdoor Attack against LiDAR-based 3D Object Detection Systems</title><link>https://arxiv.org/abs/2511.09999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MOBA, a physically realizable backdoor attack targeting LiDAR-based 3D object detectors by explicitly modeling material properties of triggers.&lt;/li&gt;&lt;li&gt;Proposes a material selection methodology and identifies titanium dioxide (TiO2) for robust, high-diffuse LiDAR reflectivity in varied environmental conditions.&lt;/li&gt;&lt;li&gt;Develops a simulation pipeline with an angle-independent approximation of the Oren–Nayar BRDF and distance-aware intensity scaling to align digital simulations with real-world LiDAR returns.&lt;/li&gt;&lt;li&gt;Evaluates on state-of-the-art LiDAR and camera–LiDAR fusion models, achieving 93.50% attack success and outperforming prior methods by ~41%, highlighting a novel real-world safety threat.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saket S. Chaturvedi', 'Gaurav Bagwe', 'Lan Zhang', 'Pan He', 'Xiaoyong Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'physical-backdoor', 'LiDAR', '3D-object-detection', 'material-properties']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09999</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification</title><link>https://arxiv.org/abs/2511.09933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses adversarial robustness for person re-identification (ReID), focusing on unique challenges of metric-learning tasks versus classification.&lt;/li&gt;&lt;li&gt;Proposes a debiased dual-invariant defense with a diffusion-model-based data resampling phase to mitigate model bias and improve training diversity.&lt;/li&gt;&lt;li&gt;Introduces a bi-adversarial self-meta defense: metric adversarial training with farthest-negative extension softening and an adversarially-enhanced self-meta mechanism to generalize to unseen identities and attack types.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art improvements over existing defenses on ReID under adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Zhou', 'Yanxiang Zhao', 'Zhongyun Hua', 'Zhipu Liu', 'Zhaoquan Gu', 'Qing Liao', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'person re-identification', 'metric learning', 'adversarial training', 'data augmentation / resampling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09933</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage</title><link>https://arxiv.org/abs/2511.09834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CertMask, a certifiable defense against adversarial patch attacks that constructs a provably sufficient set of binary masks to neutralize patch effects.&lt;/li&gt;&lt;li&gt;Uses a mathematically rigorous coverage strategy ensuring each possible patch location is covered at least k times, enabling single-round masking with O(n) inference cost (vs. PatchCleanser's two rounds and O(n^2)).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving the sufficiency of the coverage condition for certification.&lt;/li&gt;&lt;li&gt;Empirical results on ImageNet, ImageNette, and CIFAR-10 show up to +13.4% improvement in certified robust accuracy over PatchCleanser while preserving clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuntao Lyu', 'Ching-Chi Lin', 'Abdullah Al Arafat', 'Georg von der Br\\"uggen', 'Jian-Jia Chen', 'Zhishan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'certified robustness', 'provable defense', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09834</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Soiling detection for Advanced Driver Assistance Systems</title><link>https://arxiv.org/abs/2511.09740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Treats soiling detection for automotive cameras as a semantic segmentation task and compares popular segmentation methods against tile-level classification.&lt;/li&gt;&lt;li&gt;Identifies data leakage and imprecise annotations in the Woodscape dataset and provides an analysis of these issues.&lt;/li&gt;&lt;li&gt;Constructs a revised, smaller dataset subset and demonstrates that segmentation on this subset achieves comparable results with faster training.&lt;/li&gt;&lt;li&gt;Releases code and dataset splits for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Filip Ber\\'anek", "V\\'aclav Divi\\v{s}", 'Ivan Gruber']&lt;/li&gt;&lt;li&gt;Tags: ['ADAS safety', 'perception robustness', 'semantic segmentation', 'dataset curation', 'data quality / leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09740</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title><link>https://arxiv.org/abs/2509.14289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple LLM-based agent architectures (single-agent to modular) on realistic penetration testing scenarios, measuring empirical performance and failure modes.&lt;/li&gt;&lt;li&gt;Isolates and tests five core functional capabilities (GCM, IAM, CCI, AP, RTM) via targeted augmentations to assess their impact on agent effectiveness in multi-step and real-time attacks.&lt;/li&gt;&lt;li&gt;Finds that augmentations substantially improve modular agent performance, especially for complex, multi-step, and dynamic penetration testing tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lanxiao Huang', 'Daksh Dave', 'Tyler Cody', 'Peter Beling', 'Ming Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'agent architectures', 'tool use / execution', 'benchmarking / evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14289</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a root cause of failed unlearning: updates target overly general representations, inadvertently removing benign capabilities.&lt;/li&gt;&lt;li&gt;Proposes Collapse of Irrelevant Representations (CIR): perform PCA on activations and module-output gradients to find common representation subspaces and collapse them before computing unlearning updates.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains unlearning bio- and cyber-hazardous facts from Llama-3.1-8B — ~30× greater reduction in post-attack accuracy vs. best baseline while disrupting general performance ~30× less.&lt;/li&gt;&lt;li&gt;Achieves computational efficiency (&lt;3 GPU-seconds per fact) and claims disentangling harmful vs. benign capabilities at representation level enables robust, non-disruptive unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Model editing', 'AI safety', 'Representation learning', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</title><link>https://arxiv.org/abs/2509.03113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-time method to reduce multimodal LLM hallucinations without finetuning or auxiliary models.&lt;/li&gt;&lt;li&gt;Estimates token- and visual-feature-level contributions using first-order Taylor gradients to detect text-visual bias and co-occurrence bias.&lt;/li&gt;&lt;li&gt;Mitigates hallucinations via two mechanisms: suppressing spurious visual features correlated with predicted objects and rebalancing cross-modal contributions by strengthening visual signals vs. text.&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks, showing improved visual grounding and reduced hallucination rates for MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shan Wang', 'Maying Shen', 'Nadine Chang', 'Chuong Nguyen', 'Hongdong Li', 'Jose M. Alvarez']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-hallucination', 'hallucination-mitigation', 'gradient-based', 'inference-time-method', 'alignment-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03113</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title><link>https://arxiv.org/abs/2507.20067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PITA, an inference-time alignment framework that learns a small preference-guided policy to modify LLM token probabilities without fine-tuning the LLM or training a separate reward model.&lt;/li&gt;&lt;li&gt;Frames alignment as identifying an underlying preference distribution and solves it via stochastic search and iterative refinement of the guidance model.&lt;/li&gt;&lt;li&gt;Claims reduced computational cost and empirical improvements on tasks like mathematical reasoning and sentiment classification by integrating preference feedback directly into token generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarat Chandra Bobbili', 'Ujwal Dinesha', 'Dheeraj Narasimha', 'Srinivas Shakkottai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time alignment', 'preference learning', 'reward-model-free']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20067</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives</title><link>https://arxiv.org/abs/2505.17519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chain-of-Lure, a novel jailbreak framework that hides malicious intent via mission transfer and generates progressive, unconstrained synthetic narrative lure questions to bypass alignment.&lt;/li&gt;&lt;li&gt;Employs a helper LLM to perform randomized narrative optimization across multi-turn interactions, improving attack strength without using fixed templates or optimization-heavy methods.&lt;/li&gt;&lt;li&gt;Introduces a toxicity-based evaluation pipeline using third-party LLMs under black-box API settings and reports consistently high attack success and toxicity across diverse models.&lt;/li&gt;&lt;li&gt;Provides data-driven insights for alignment design and proposes two concrete defense strategies to mitigate the described attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Chang', 'Tianqing Zhu', 'Yu Zhao', 'Shuangyong Song', 'Ping Xiong', 'Wanlei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17519</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title><link>https://arxiv.org/abs/2501.18638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAP (Graph of Attacks with Pruning), a graph-based framework for generating stealthy jailbreak prompts that enables knowledge sharing across attack paths.&lt;/li&gt;&lt;li&gt;Reports empirical gains over prior tree-based methods: +20.8% attack success and -62.7% query cost, with &gt;96% success against open and closed LLMs.&lt;/li&gt;&lt;li&gt;Presents variants (GAP-Auto for automated seed generation, GAP-VLM for multimodal/vision-language attacks) and shows GAP prompts improve content moderation when used for fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Schwartz', 'Dmitriy Bespalov', 'Zhe Wang', 'Ninad Kulkarni', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'red teaming', 'adversarial prompting', 'content moderation', 'multimodal attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18638</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Investigating CoT Monitorability in Large Reasoning Models</title><link>https://arxiv.org/abs/2511.08525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates 'CoT Monitorability': using chain-of-thought (CoT) outputs to detect LRM misbehavior (shortcuts, sycophancy, etc.).&lt;/li&gt;&lt;li&gt;Empirically analyzes two axes — verbalization fidelity (do CoTs reflect true decision factors) and monitor reliability (can CoT-based monitors detect misbehavior) — across math, science, and ethics tasks.&lt;/li&gt;&lt;li&gt;Studies how CoT intervention methods affect monitoring effectiveness and correlation with model performance.&lt;/li&gt;&lt;li&gt;Proposes MoME, a paradigm where LLMs monitor other models' misbehavior via CoTs and produce structured judgments with supporting evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yang', 'Junchao Wu', 'Xilin Gong', 'Xuansheng Wu', 'Derek Wong', 'Ninhao Liu', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Chain-of-thought monitoring', 'Model oversight', 'Behavioral detection', 'Red teaming/monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08525</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?</title><link>https://arxiv.org/abs/2511.08455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness of social bot detectors to shortcut learning where models exploit spurious textual cues, showing average relative accuracy drops of 32% under constructed shortcut scenarios.&lt;/li&gt;&lt;li&gt;Creates a suite of shortcut scenarios by associating user labels with superficial textual features to evaluate how detectors fail under distribution shifts.&lt;/li&gt;&lt;li&gt;Proposes LLM-based mitigation strategies using counterfactual data augmentation at three levels (individual text, dataset-level distribution, and model causal extraction) to reduce reliance on shortcuts.&lt;/li&gt;&lt;li&gt;Reports an average relative performance improvement of 56% under the shortcut scenarios using the proposed methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyan Zheng', 'Herun Wan', 'Minnan Luo', 'Junhang Huang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'shortcut learning', 'counterfactual data augmentation', 'social bot detection', 'LLM-based mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08455</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys</title><link>https://arxiv.org/abs/2511.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AlignSurvey, a benchmark that simulates the full social-survey pipeline for evaluating LLMs across four tasks: social role modeling, semi-structured interview modeling, attitude stance modeling, and survey response modeling.&lt;/li&gt;&lt;li&gt;Builds and releases a multi-tier dataset: Social Foundation Corpus (44K+ interview dialogues, 400K+ structured records) and Entire-Pipeline Survey Datasets including expert-annotated AlignSurvey-Expert and two nationally representative surveys for cross-cultural evaluation.&lt;/li&gt;&lt;li&gt;Defines task-specific metrics to assess alignment fidelity, consistency, and fairness at individual and demographic group levels, focusing on under-representation and bias.&lt;/li&gt;&lt;li&gt;Releases SurveyLM models (two-stage fine-tuned open-source LLMs) and evaluation tools on GitHub/HuggingFace to support reproducible alignment research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxi Lin', 'Weikang Yuan', 'Zhuoren Jiang', 'Biao Huang', 'Ruitao Zhang', 'Jianan Ge', 'Yueqian Xu', 'Jianxing Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'fairness', 'benchmarking', 'datasets', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07871</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making</title><link>https://arxiv.org/abs/2510.03553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CCD-Bench: 2,182 open-ended dilemmas across seven domains with ten anonymized response options corresponding to the ten GLOBE cultural clusters to probe cross-cultural value conflicts in LLM decision-making.&lt;/li&gt;&lt;li&gt;Evaluates 17 non-reasoning LLMs and finds systematic preference for Nordic and Germanic Europe options, underrepresentation of Eastern Europe and MENA, and superficial pluralism in model rationales.&lt;/li&gt;&lt;li&gt;Analyzes dimension usage (e.g., recombination of Future and Performance Orientation; low grounding in Assertiveness or Gender Egalitarianism), negligible ordering effects, and clustering by developer lineage rather than geography.&lt;/li&gt;&lt;li&gt;Argues current alignment pipelines encourage consensus-oriented worldviews that underserve scenarios needing power negotiation, rights-based, or gender-aware reasoning, calling for more pluralistic alignment strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasibur Rahman', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'cultural-bias', 'LLM-behavior', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03553</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title><link>https://arxiv.org/abs/2510.01252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a pipeline combining GPT-style LLMs with sparse autoencoders (SAEs) to trace how social themes from pretraining data are encoded in model representations.&lt;/li&gt;&lt;li&gt;Case study: trained a GPT-style model on 37 nineteenth-century novels by female authors and probed SAE-derived sparse features for eleven social/moral categories (e.g., gender, marriage, class).&lt;/li&gt;&lt;li&gt;Finds stable thematic features (notably gender/kinship) that persist across layers and show increasing association/entanglement with depth, arguing the approach scales to audit cultural assumptions embedded in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mariam Mahran', 'Katharina Simbeck']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'interpretability', 'dataset auditing', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01252</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks Against Speech Language Models</title><link>https://arxiv.org/abs/2510.01157</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of audio backdoor attacks against speech–language model pipelines (cascaded speech encoders + LLM).&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across four speech encoders, three datasets, and four tasks (ASR, emotion recognition, gender and age prediction) with success rates of 90.76%–99.41%.&lt;/li&gt;&lt;li&gt;Performs component-wise analysis to identify most vulnerable stages in the pipeline and proposes a fine-tuning-based defense to mitigate poisoned pretrained encoders.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandrine Fortier', 'Thomas Thebaud', "Jes\\'us Villalba", 'Najim Dehak', 'Patrick Cardinal']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'speech language models', 'model poisoning', 'adversarial robustness', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01157</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</title><link>https://arxiv.org/abs/2508.02573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Train CNNs on LLM attention weights to classify and localize different forms of verbatim memorization during decoding.&lt;/li&gt;&lt;li&gt;Show existing memorization taxonomy poorly aligns with attention mechanisms; propose a new taxonomy with three categories: guessed memorized (LM prediction), recalled due to high duplication, and non-memorized.&lt;/li&gt;&lt;li&gt;Find many extractable samples are actually guessed by the model rather than directly recalled from training data, and introduce a visual interpretability method to localize attention regions responsible for each form.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'er\\'emie Dentan", 'Davide Buscaldi', 'Sonia Vanier']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'model-interpretability', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title><link>https://arxiv.org/abs/2502.18573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents FactReasoner, a neuro-symbolic probabilistic framework to assess factuality of long-form LLM outputs by decomposing responses into atomic units.&lt;/li&gt;&lt;li&gt;Retrieves external evidence for each unit and models logical relationships (entailment/contradiction) via probabilistic encodings to estimate posterior support.&lt;/li&gt;&lt;li&gt;Evaluates on labeled and unlabeled benchmarks, often outperforming prompt-based factuality methods; implementation is open-source.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Radu Marinescu', 'Debarun Bhattacharjya', 'Junkyu Lee', 'Tigran Tchrakian', 'Javier Carnerero Cano', 'Yufang Hou', 'Elizabeth Daly', 'Alessandra Pascale']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'truthfulness', 'hallucination_detection', 'safety_evaluation', 'neuro-symbolic_reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors</title><link>https://arxiv.org/abs/2501.14250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Siren, a learning-based multi-turn jailbreak attack framework that simulates realistic human adversary behaviors using three stages: MiniMax-driven dataset construction with turn-level LLM feedback, attacker fine-tuning (SFT and DPO), and attacker-target interactions.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (e.g., 90% with LLaMA-3-8B attacking Gemini-1.5-Pro; 70% with Mistral-7B against GPT-4o), outperforming single-turn baselines and matching stronger attackers while using fewer turns.&lt;/li&gt;&lt;li&gt;Highlights decomposition strategies and dynamic multi-turn tactics that align semantically with attack goals, and releases code for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Zhao', 'Youzhi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14250</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Scope of Language Models</title><link>https://arxiv.org/abs/2410.21597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical comparison of methods to enforce ‘scoping’ (restricting LLM responses to intended tasks), including prompting, supervised fine-tuning, preference learning, and Circuit Breakers.&lt;/li&gt;&lt;li&gt;Evaluations span multiple model families, many topics and fine-grained topics, adversarial tests, and ablations (e.g., diversity of irrelevant queries).&lt;/li&gt;&lt;li&gt;Key findings: supervised fine-tuning excels when diverse negative examples are available; Circuit Breakers perform better when negative-example diversity is low; combining methods sequentially often yields benefits.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Yunis', 'Siyu Huo', 'Chulaka Gunasekara', 'Danish Contractor']&lt;/li&gt;&lt;li&gt;Tags: ['LLM scoping', 'alignment', 'safety evaluation', 'circuit breakers', 'adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21597</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title><link>https://arxiv.org/abs/2511.10573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Responsible Reinforcement Learning (RRL) framework that formulates personalization as a Constrained Markov Decision Process to balance engagement with ethical safety and emotional alignment.&lt;/li&gt;&lt;li&gt;Introduces an emotion-informed state representation capturing emotional readiness, affect, and risk, and a multi-objective reward balancing short-term engagement with long-term well-being.&lt;/li&gt;&lt;li&gt;Suggests instantiating RRL with standard RL algorithms (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization and outlines simulation-based validation paths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Garapati Keerthana', 'Manik Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'ethical AI', 'alignment', 'affective computing', 'constrained MDP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title><link>https://arxiv.org/abs/2511.10400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates reliability of LLM-based multi-agent systems using Byzantine fault tolerance and finds LLM agents exhibit stronger skepticism to erroneous message flows than traditional agents.&lt;/li&gt;&lt;li&gt;Proposes CP-WBFT, a confidence probe–based weighted Byzantine Fault Tolerant consensus mechanism that leverages LLMs' reflective/discriminative capabilities to weight information flow and improve consensus.&lt;/li&gt;&lt;li&gt;Shows extensive experiments across diverse network topologies under extreme Byzantine conditions (up to 85.7% fault rate), with improved accuracy on mathematical reasoning and safety assessment tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lifan Zheng', 'Jiawei Chen', 'Qinghong Yin', 'Jingyuan Zhang', 'Xinyi Zeng', 'Yu Tian']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine fault tolerance', 'Multi-agent systems', 'LLM robustness', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10400</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Propagation in Social Networks using Large Language Models</title><link>https://arxiv.org/abs/2511.10384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models synthetic agents as persona-conditioned LLM nodes to simulate user biases, ideology, and trust heuristics in social networks.&lt;/li&gt;&lt;li&gt;Propagates and reforms news articles across networks of up to 30 sequential rewrites, measuring factual fidelity with a question-answering auditor and defining a misinformation index and propagation rate.&lt;/li&gt;&lt;li&gt;Finds identity/ideology-driven personas accelerate misinformation (esp. politics, marketing, tech) while expert personas preserve fidelity; heterogeneous interactions amplify distortions into propaganda-level drift.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of misinformation severity and presents an interpretable framework for studying and mitigating misinformation diffusion with LLMs as both proxies and auditors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raj Gaurav Maurya', 'Vaibhav Shukla', 'Raj Abhijit Dandekar', 'Rajat Dandekar', 'Sreedath Panat']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM simulation', 'safety evaluation', 'social network modeling', 'auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10384</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title><link>https://arxiv.org/abs/2511.10287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OutSafe-Bench, a large-scale multimodal benchmark for offensive/unsafe content detection covering text, images, audio, and video with bilingual annotations.&lt;/li&gt;&lt;li&gt;Provides a dataset: ~18,000 bilingual (Chinese/English) text prompts, 4,500 images, 450 audio clips, and 450 videos annotated across nine content risk categories.&lt;/li&gt;&lt;li&gt;Proposes the Multidimensional Cross Risk Score (MCRS) to model overlapping correlated risks and FairScore, an explainable multi-reviewer weighted aggregation framework using top models as adaptive juries.&lt;/li&gt;&lt;li&gt;Evaluates nine state-of-the-art MLLMs and reports substantial safety vulnerabilities, highlighting gaps in multimodal content safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yuanshuai Li', 'Yingchao Yu', 'Lingjuan Lyu', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'benchmarking', 'content moderation', 'evaluation metrics', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10287</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title><link>https://arxiv.org/abs/2511.10240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ProgRAG, a multi-hop KGQA framework that decomposes complex questions into sub-questions and progressively extends partial reasoning paths.&lt;/li&gt;&lt;li&gt;Uses external retrievers to gather candidate KG evidence at each step and applies LLM-based uncertainty-aware pruning to filter/refine evidence.&lt;/li&gt;&lt;li&gt;Optimizes LLM reasoning context by organizing and rearranging partial reasoning paths to reduce hallucinations and improve transparency.&lt;/li&gt;&lt;li&gt;Shows empirical gains over baselines on three multi-hop KGQA datasets, demonstrating improved reliability and reasoning quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minbae Park', 'Hyemin Yang', 'Jeonghyun Kim', 'Kunsoo Park', 'Hyunjoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'knowledge-graph-enhanced LLMs', 'retrieval-augmented reasoning', 'multi-hop QA', 'uncertainty-aware pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10240</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title><link>https://arxiv.org/abs/2511.10067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MuSeR (Multifaceted Self-Refinement): generate attribute-conditioned queries, have an LLM self-evaluate responses along decision-making, communication, and safety facets, refine outputs, and use refined pairs for supervised fine-tuning.&lt;/li&gt;&lt;li&gt;Designs an attribute-conditioned query generator to simulate diverse real-world medical contexts (role, region, intent, ambiguity) to improve context-awareness.&lt;/li&gt;&lt;li&gt;Combines self-refinement with knowledge distillation to improve smaller models (e.g., Qwen3-32B), achieving new SOTA on HealthBench and its hard subset.&lt;/li&gt;&lt;li&gt;Focuses explicitly on improving safety and context-awareness in medical responses, reporting notable gains on the safety/context-awareness axis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Zhou', 'Yubin Wang', 'Bin Wang', 'Chen Ning', 'Xien Liu', 'Ji Wu', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'context-awareness', 'self-refinement', 'medical LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10067</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box On-Policy Distillation of Large Language Models</title><link>https://arxiv.org/abs/2511.10643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generative Adversarial Distillation (GAD), a black-box, on-policy distillation method that treats the student LLM as a generator and trains a discriminator to distinguish student from teacher outputs.&lt;/li&gt;&lt;li&gt;The discriminator provides adaptive, on-policy reward-like feedback that co-evolves with the student in a minimax training loop.&lt;/li&gt;&lt;li&gt;Empirical results show GAD outperforms sequence-level knowledge distillation and can produce a student (Qwen2.5-14B-Instruct) comparable to its teacher (GPT-5-Chat) on LMSYS-Chat automatic evaluation.&lt;/li&gt;&lt;li&gt;Operates purely from teacher text outputs (no access to logits or parameters), making it a black-box model extraction/distillation approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianzhu Ye', 'Li Dong', 'Zewen Chi', 'Xun Wu', 'Shaohan Huang', 'Furu Wei']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'black-box distillation', 'knowledge distillation', 'adversarial training', 'model stealing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10643</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Say It Differently: Linguistic Styles as Jailbreak Vectors</title><link>https://arxiv.org/abs/2511.10519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies linguistic style (e.g., fearful, curious, compassionate) as an attack surface for LLM jailbreaks and systematically studies style-driven reframing of harmful intents.&lt;/li&gt;&lt;li&gt;Builds a style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 styles via handcrafted templates and LLM-based rewrites, preserving semantics.&lt;/li&gt;&lt;li&gt;Evaluates 16 open- and closed-source instruction-tuned models, finding stylistic reframing can increase jailbreak success rates by up to +57 percentage points; contextualized rewrites outperform templated variants.&lt;/li&gt;&lt;li&gt;Proposes a mitigation: a style-neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues, substantially reducing jailbreak success rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srikant Panda', 'Avinash Rai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10519</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following</title><link>https://arxiv.org/abs/2511.10507</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvancedIF, a 1,600+ prompt benchmark with expert-curated rubrics for evaluating complex, multi-turn, and system-level instruction following.&lt;/li&gt;&lt;li&gt;Proposes RIFL, a post-training pipeline using rubric generation, a finetuned rubric verifier, and reward shaping to enable reinforcement learning for better instruction following.&lt;/li&gt;&lt;li&gt;Reports substantial improvements (e.g., 6.7% absolute gain on AdvancedIF) and ablations showing the contribution of each component.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yun He', 'Wenzhe Li', 'Hejia Zhang', 'Songlin Li', 'Karishma Mandyam', 'Sopan Khosla', 'Yuanhao Xiong', 'Nanshu Wang', 'Selina Peng', 'Beibin Li', 'Shengjie Bi', 'Shishir G. Patil', 'Qi Qi', 'Shengyu Feng', 'Julian Katz-Samuels', 'Richard Yuanzhe Pang', 'Sujan Gonugondla', 'Hunter Lang', 'Yue Yu', 'Yundi Qian', 'Maryam Fazel-Zarandi', 'Licheng Yu', 'Amine Benhalloum', 'Hany Awadalla', 'Manaal Faruqui']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'benchmarking', 'reinforcement learning', 'rubric-based evaluation', 'alignment/safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10507</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning About Intent for Ambiguous Requests</title><link>https://arxiv.org/abs/2511.10453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes generating multiple interpretation–answer pairs in a single structured response to handle ambiguous user requests and surface intent alternatives.&lt;/li&gt;&lt;li&gt;Trains models with reinforcement learning and customized reward functions using multiple valid answers as supervision to improve coverage of valid interpretations.&lt;/li&gt;&lt;li&gt;Evaluates on conversational QA and semantic parsing, showing higher coverage of valid answers and human alignment between interpretations and answers; emphasizes transparency and one-step efficient generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Irina Saparina', 'Mirella Lapata']&lt;/li&gt;&lt;li&gt;Tags: ['intent-alignment', 'ambiguity-resolution', 'reinforcement-learning', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10453</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs</title><link>https://arxiv.org/abs/2511.10375</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TruthfulRAG, a framework that uses Knowledge Graphs (KGs) to resolve factual-level conflicts between retrieved documents and an LLM's internal knowledge in Retrieval-Augmented Generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Builds KGs by extracting triples from retrieved content, performs query-based graph retrieval to surface relevant facts, and uses entropy-based filtering to locate and mitigate conflicting elements.&lt;/li&gt;&lt;li&gt;Claims improved faithfulness, robustness, and accuracy of generated responses compared to existing token-/semantic-level conflict-resolution methods, supported by extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyi Liu', 'Yuming Shang', 'Xi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Factuality', 'Knowledge Graphs', 'Robustness', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10375</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models</title><link>https://arxiv.org/abs/2511.10262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MTR-DuplexBench, a benchmark that segments continuous full‑duplex (overlapping) speech dialogues into discrete turns for turn-by-turn evaluation.&lt;/li&gt;&lt;li&gt;Evaluates FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety in multi-round settings.&lt;/li&gt;&lt;li&gt;Finds current full‑duplex speech language models struggle to maintain consistent performance across multiple rounds and evaluation dimensions.&lt;/li&gt;&lt;li&gt;Provides a benchmarking resource (code/data to be released) to better assess multi‑round safety and instruction-following in real‑time speech models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['He Zhang', 'Wenqian Cui', 'Haoning Xu', 'Xiaohui Li', 'Lei Zhu', 'Shaohua Ma', 'Irwin King']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmark', 'full-duplex', 'multi-round-dialogue', 'instruction-following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10262</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation</title><link>https://arxiv.org/abs/2511.09984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Characterizes output language drift in multilingual Retrieval-Augmented Generation (RAG), especially during reasoning-intensive decoding like Chain-of-Thought (CoT).&lt;/li&gt;&lt;li&gt;Finds drift is due to decoder-level collapse and dominant high-frequency English token patterns (English as a semantic attractor), not comprehension failure.&lt;/li&gt;&lt;li&gt;Proposes Soft Constrained Decoding (SCD), a lightweight, training-free decoding method that penalizes non-target-language tokens to steer generation.&lt;/li&gt;&lt;li&gt;Evaluates SCD across multiple multilingual datasets and LLM backbones, showing consistent improvements in language alignment and downstream task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Li', 'Zhenghua Xu', 'Rui Xie']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual-RAG', 'language-drift', 'decoding-mitigation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09984</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction</title><link>https://arxiv.org/abs/2511.09971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of state-of-the-art language models on veracity prediction for numerical claims using controlled numerical perturbations and label-flipping probes.&lt;/li&gt;&lt;li&gt;Finds large accuracy drops (up to 62%) under certain perturbations, and no model is robust across all tested conditions.&lt;/li&gt;&lt;li&gt;Observes that longer contexts generally reduce accuracy, but adding perturbed demonstrations to extended context can substantially recover performance.&lt;/li&gt;&lt;li&gt;Highlights critical limitations in numerical fact-checking and frames robustness to adversarial / perturbation-style attacks as an open challenge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter R{\\o}ysland Aarnes', 'Vinay Setty']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'numerical reasoning', 'adversarial perturbation', 'veracity prediction', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09971</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection</title><link>https://arxiv.org/abs/2511.09918</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Norm-RAG, a retrieval-augmented framework that models utterance-level attributes (intent, roles, framing, linguistic cues) and grounds norms via Semantic Chunking for interpretable norm inference in multi-turn dialogues.&lt;/li&gt;&lt;li&gt;Releases MINDS, a bilingual (Mandarin-English and Spanish-English) dataset of 31 multi-turn conversations with per-turn annotations for norm category and adherence using multi-annotator consensus.&lt;/li&gt;&lt;li&gt;Demonstrates improved norm detection and generalization for culturally adaptive dialogue systems using the proposed retrieval-grounded approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pritish Sahu', 'Anirudh Som', 'Dimitra Vergyri', 'Ajay Divakaran']&lt;/li&gt;&lt;li&gt;Tags: ['social norms', 'dataset', 'norm detection', 'multilingual', 'retrieval-augmented']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09918</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models</title><link>https://arxiv.org/abs/2511.09880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies safety degradation in LLMs caused by downstream fine-tuning and proposes EnchTable to transfer and preserve safety alignment without heavy retraining.&lt;/li&gt;&lt;li&gt;Introduces NTK-based safety vector distillation to decouple safety constraints from task reasoning and an interference-aware merging technique to balance safety and utility.&lt;/li&gt;&lt;li&gt;Implements a prototype across three task domains and three LLM architectures, evaluating on 11 datasets and showing improved resistance to static and dynamic jailbreaking and adversarial prompts.&lt;/li&gt;&lt;li&gt;Claims outperforming vendor safety models and multiple parameter/inference baselines with lower unsafe rates, higher utility, and deployability with low overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialin Wu', 'Kecen Li', 'Zhicong Huang', 'Xinfeng Li', 'Xiaofeng Wang', 'Cheng Hong']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'jailbreaking / adversarial prompts', 'fine-tuning transfer', 'NTK distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09880</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</title><link>https://arxiv.org/abs/2511.09748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks sub-2B instruction-tuned LLMs (e.g., Gemma-3-1B, Qwen-3-1.7B, Llama-3.2-1B) for English→German Critical Error Detection (CED) in machine translation across WMT21, WMT22, and SynCED-EnDe-2025.&lt;/li&gt;&lt;li&gt;Identifies a quality–efficiency sweet spot near 1B parameters (Gemma-3-1B) achieving high MCC and F1-ERR while running on-device (≈400 ms latency on MacBook Pro M4 Pro); larger models improve absolute metrics at higher compute cost.&lt;/li&gt;&lt;li&gt;Introduces a standardized framework using prompts, lightweight logit-bias calibration, majority voting, and small-sample fine-tuning; releases datasets, prompts, and scripts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muskaan Chopra', 'Lorenz Sparrenberg', 'Sarthak Khanna', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'model-robustness', 'on-device-ML', 'machine-translation', 'compact-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09748</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Adversarial Transferability via Ensemble Non-Attention</title><link>https://arxiv.org/abs/2511.08937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NAMEA, an ensemble adversarial attack that integrates gradients from non-attention areas of ensemble models (CNNs and ViTs) to improve cross-architecture transferability.&lt;/li&gt;&lt;li&gt;Decouples gradients from attention and non-attention regions and merges them via a meta-learning-based strategy to reduce gradient variance across heterogeneous surrogate models.&lt;/li&gt;&lt;li&gt;Evaluated on ImageNet, showing substantial improvements over prior ensemble attacks (e.g., +15.0% vs AdaEA, +9.6% vs SMER).&lt;/li&gt;&lt;li&gt;First work to explicitly exploit ensemble non-attention regions to boost transferability across architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yipeng Zou', 'Qin Liu', 'Jie Wu', 'Yu Peng', 'Guo Chen', 'Hui Zhou', 'Guanghui Ye']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'ensemble attacks', 'vision transformer', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08937</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models</title><link>https://arxiv.org/abs/2511.08379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using Self-Organizing Maps (SOMs) to extract multiple latent directions encoding 'refusal' behavior in LLMs, rather than a single difference-of-means vector.&lt;/li&gt;&lt;li&gt;Provides a proof that SOMs generalize the prior difference-in-means technique and derives multiple refusal directions by subtracting harmless centroids from SOM neurons trained on harmful prompts.&lt;/li&gt;&lt;li&gt;Empirically shows that ablating multiple SOM-derived directions suppresses refusal more effectively than the single-direction baseline and outperforms specialized jailbreak algorithms.&lt;/li&gt;&lt;li&gt;Analyzes mechanistic interpretability implications of representing refusal as a low-dimensional manifold and discusses how multi-directional ablations affect model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgio Piras', 'Raffaele Mura', 'Fabio Brau', 'Luca Oneto', 'Fabio Roli', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'mechanistic-interpretability', 'refusal-suppression', 'latent-space-ablation', 'adversarial-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08379</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Can Current Detectors Catch Face-to-Voice Deepfake Attacks?</title><link>https://arxiv.org/abs/2510.21004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic evaluation of whether state-of-the-art audio deepfake detectors can detect FOICE — a face-to-voice generator that synthesizes a victim's voice from a single facial image — under clean and noisy conditions.&lt;/li&gt;&lt;li&gt;Shows that leading detectors largely fail to detect FOICE-generated speech, exposing a practical security vulnerability where facial images can be used to create stealthy audio deepfakes.&lt;/li&gt;&lt;li&gt;Proposes targeted fine-tuning strategies that improve detection for FOICE-specific artifacts and analyzes trade-offs: specialization boosts FOICE detection but can reduce robustness to unseen generators like SpeechT5.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nguyen Linh Bao Nguyen', 'Alsharif Abuadbba', 'Kristen Moore', 'Tingmin Wu']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake', 'voice cloning', 'deepfake detection', 'security/robustness', 'multimodal attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21004</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title><link>https://arxiv.org/abs/2510.01252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a pipeline combining GPT-style LLMs with sparse autoencoders (SAEs) to trace how social themes from pretraining data are encoded in model representations.&lt;/li&gt;&lt;li&gt;Case study: trained a GPT-style model on 37 nineteenth-century novels by female authors and probed SAE-derived sparse features for eleven social/moral categories (e.g., gender, marriage, class).&lt;/li&gt;&lt;li&gt;Finds stable thematic features (notably gender/kinship) that persist across layers and show increasing association/entanglement with depth, arguing the approach scales to audit cultural assumptions embedded in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mariam Mahran', 'Katharina Simbeck']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'interpretability', 'dataset auditing', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01252</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title><link>https://arxiv.org/abs/2509.14289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple LLM-based agent architectures (single-agent to modular) on realistic penetration testing scenarios, measuring empirical performance and failure modes.&lt;/li&gt;&lt;li&gt;Isolates and tests five core functional capabilities (GCM, IAM, CCI, AP, RTM) via targeted augmentations to assess their impact on agent effectiveness in multi-step and real-time attacks.&lt;/li&gt;&lt;li&gt;Finds that augmentations substantially improve modular agent performance, especially for complex, multi-step, and dynamic penetration testing tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lanxiao Huang', 'Daksh Dave', 'Tyler Cody', 'Peter Beling', 'Ming Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'agent architectures', 'tool use / execution', 'benchmarking / evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14289</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title><link>https://arxiv.org/abs/2507.20067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PITA, an inference-time alignment framework that learns a small preference-guided policy to modify LLM token probabilities without fine-tuning the LLM or training a separate reward model.&lt;/li&gt;&lt;li&gt;Frames alignment as identifying an underlying preference distribution and solves it via stochastic search and iterative refinement of the guidance model.&lt;/li&gt;&lt;li&gt;Claims reduced computational cost and empirical improvements on tasks like mathematical reasoning and sentiment classification by integrating preference feedback directly into token generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarat Chandra Bobbili', 'Ujwal Dinesha', 'Dheeraj Narasimha', 'Srinivas Shakkottai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time alignment', 'preference learning', 'reward-model-free']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20067</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Evaluation of Secure Code Generation</title><link>https://arxiv.org/abs/2503.15554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates four state-of-the-art secure code generation techniques by jointly assessing security and functional correctness on the same generated code, rather than separately.&lt;/li&gt;&lt;li&gt;Uses three static analyzers (including CodeQL) and two LLMs to detect vulnerabilities, revealing gaps in prior single-analyzer evaluations.&lt;/li&gt;&lt;li&gt;Finds that many secure-generation methods trade off functionality for security—often removing vulnerable lines or producing unrelated/garbage code—and can degrade base LLM performance by over 50%.&lt;/li&gt;&lt;li&gt;Shows CodeQL misses multiple vulnerabilities, indicating that common evaluation practices overstate security gains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shih-Chieh Dai', 'Jun Xu', 'Guanhong Tao']&lt;/li&gt;&lt;li&gt;Tags: ['secure-code-generation', 'vulnerability-detection', 'static-analysis', 'LLM-evaluation', 'code-security-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.15554</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Image-based Outlier Synthesis With Training Data</title><link>https://arxiv.org/abs/2411.10794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ASCOOD, a training-time method that synthesizes virtual outliers from in-distribution images by perturbing inputs with gradient attribution to disrupt invariant features while amplifying true-class logits.&lt;/li&gt;&lt;li&gt;Trains models to both classify ID data and produce high predictive uncertainty on these synthesized near-manifold outliers using z-score standardized features.&lt;/li&gt;&lt;li&gt;Targets challenging OOD scenarios including spurious-correlation settings and fine-grained OOD that are visually similar to ID, without using any external data.&lt;/li&gt;&lt;li&gt;Evaluated on 7 datasets with comparisons to 30+ baselines, showing improved OOD detection in spurious, fine-grained, and conventional settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sudarshan Regmi']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution detection', 'robustness', 'spurious_correlation', 'data_synthesis', 'fine-grained_classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.10794</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Scope of Language Models</title><link>https://arxiv.org/abs/2410.21597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates methods to constrain LLM behavior to an intended purpose ('scoping') so models reject out-of-scope queries.&lt;/li&gt;&lt;li&gt;Compares prompting, supervised fine-tuning, preference learning, and Circuit Breakers (CB) across multiple model families and task domains.&lt;/li&gt;&lt;li&gt;Finds supervised fine-tuning works best with diverse examples of irrelevant queries; CBs perform well when irrelevant-example diversity is low; layering methods can combine benefits.&lt;/li&gt;&lt;li&gt;Includes ablative studies, adversarial evaluations, and guidance for practitioners deploying scoped LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Yunis', 'Siyu Huo', 'Chulaka Gunasekara', 'Danish Contractor']&lt;/li&gt;&lt;li&gt;Tags: ['model scoping', 'alignment', 'safety/guardrails', 'circuit breakers', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21597</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CREAT, a bi-level constrained reinforcement learning attack that performs fine-grained profile pollution on sequential recommender systems to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Balances adversarial effectiveness and stealth by combining pattern inversion rewards (to flip critical transition patterns) with distribution consistency rewards (to minimize detectable shifts via unbalanced co-optimal transport).&lt;/li&gt;&lt;li&gt;Introduces Constrained Group Relative Reinforcement Learning for step-wise perturbations with dynamic barrier constraints and group-shared replay to achieve targeted poisoning with minimal detectability; validated through extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['profile pollution', 'data poisoning', 'adversarial attack', 'reinforcement learning', 'recommender systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms</title><link>https://arxiv.org/abs/2511.08570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptKAN, a method for automatic, data-driven domain/grid updates in Kolmogorov-Arnold Networks using layer output histograms.&lt;/li&gt;&lt;li&gt;Shows AdaptKAN matches or exceeds prior KANs and MLPs on tasks including Feynman symbolic equation learning, image classification from frozen features, control Lyapunov function learning, and OOD detection on OpenOOD v1.5.&lt;/li&gt;&lt;li&gt;Identifies the histogram-based algorithm as useful for detecting out-of-distribution (OOD) inputs, evaluated on an OOD benchmark.&lt;/li&gt;&lt;li&gt;Main contribution is training/architecture automation and interpretability improvements; OOD detection is presented as an additional benefit rather than the primary focus.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamison Moody', 'James Usevitch']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-distribution detection', 'Robustness', 'Model architecture', 'Interpretability', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08570</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On Stealing Graph Neural Network Models</title><link>https://arxiv.org/abs/2511.07170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an attack to steal Graph Neural Network (GNN) models under very strict query limits.&lt;/li&gt;&lt;li&gt;Proposes a technique to recover the model backbone without direct queries and then select the most informative queries within a fixed budget.&lt;/li&gt;&lt;li&gt;Evaluates the attack on eight real-world datasets and shows effectiveness even when model-extraction defenses are present.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marcin Podhajski', "Jan Dubi\\'nski", 'Franziska Boenisch', 'Adam Dziedzic', 'Agnieszka Pr\\k{e}gowska', 'Tomasz P. Michalak']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'graph neural networks', 'black-box attacks', 'query-limited attacks', 'adversarial machine learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07170</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a root cause of failed unlearning: updates target overly general representations, inadvertently removing benign capabilities.&lt;/li&gt;&lt;li&gt;Proposes Collapse of Irrelevant Representations (CIR): perform PCA on activations and module-output gradients to find common representation subspaces and collapse them before computing unlearning updates.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains unlearning bio- and cyber-hazardous facts from Llama-3.1-8B — ~30× greater reduction in post-attack accuracy vs. best baseline while disrupting general performance ~30× less.&lt;/li&gt;&lt;li&gt;Achieves computational efficiency (&lt;3 GPU-seconds per fact) and claims disentangling harmful vs. benign capabilities at representation level enables robust, non-disruptive unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Model editing', 'AI safety', 'Representation learning', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK</title><link>https://arxiv.org/abs/2508.00718</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an open-source SDK for synthesizing high-quality tabular data with differential privacy and fairness-aware generation.&lt;/li&gt;&lt;li&gt;Implements the TabularARGN autoregressive framework supporting complex multi-table and sequential datasets with automation and quality assurance.&lt;/li&gt;&lt;li&gt;Emphasizes deployment options (cloud and local), speed/usability improvements, and real-world adoption for data democratization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivona Krchova', 'Mariana Vargas Vieyra', 'Mario Scriminaci', 'Andrey Sidorenko']&lt;/li&gt;&lt;li&gt;Tags: ['synthetic-data', 'differential-privacy', 'privacy-preserving', 'fairness', 'tabular-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00718</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing</title><link>https://arxiv.org/abs/2412.12837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates vulnerability of decentralized learning architectures to Membership Inference Attacks (MIA) by varying graph structure, graph dynamics, and aggregation strategies across datasets and data distributions.&lt;/li&gt;&lt;li&gt;Main finding: vulnerability to MIA is strongly correlated with (i) local model mixing strategies used by nodes when incorporating neighbors' models and (ii) global mixing properties of the communication graph.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of mixing properties, empirical experiments on four datasets, and shows that improving mixing properties complements differential privacy to reduce MIA risk.&lt;/li&gt;&lt;li&gt;Presents practical lessons for designing decentralized learning systems with reduced membership inference vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ousmane Touat', 'Jezekael Brunon', 'Yacine Belal', 'Julien Nicolas', "C\\'esar Sabater", 'Mohamed Maouche', 'Sonia Ben Mokhtar']&lt;/li&gt;&lt;li&gt;Tags: ['Membership Inference', 'Decentralized Learning', 'Graph Mixing', 'Privacy Attacks', 'Differential Privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12837</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lipschitz-Regularized Critics Lead to Policy Robustness Against Transition Dynamics Uncertainty</title><link>https://arxiv.org/abs/2404.13879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PPO-PGDLC: PPO augmented with Projected Gradient Descent (PGD) to find adversarial states and a Lipschitz-regularized critic to improve policy smoothness.&lt;/li&gt;&lt;li&gt;Aims to approximate a robust Bellman operator via adversarial state search and enforce critic smoothness to yield policies robust to transition dynamics uncertainty.&lt;/li&gt;&lt;li&gt;Evaluated on two classic control tasks and one real-world robotic locomotion task, showing improved performance and smoother actions under environmental perturbations compared to baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xulin Chen', 'Ruipeng Liu', 'Zhenyu Gan', 'Garrett E. Katz']&lt;/li&gt;&lt;li&gt;Tags: ['robust RL', 'Lipschitz regularization', 'adversarial robustness', 'PGD', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.13879</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robot Crash Course: Learning Soft and Stylized Falling</title><link>https://arxiv.org/abs/2511.10635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robot-agnostic RL reward that trades off achieving a user-specified end pose with minimizing impact and protecting critical parts during falls.&lt;/li&gt;&lt;li&gt;Introduces a simulation-based sampling strategy for varied initial and end poses so the policy can handle a broad range of falling scenarios and unseen target poses at inference.&lt;/li&gt;&lt;li&gt;Validates the approach in simulation and on real bipedal robots, demonstrating controlled, soft falls that reduce damage while allowing end-pose control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pascal Strauch', 'David M\\"uller', 'Sammy Christen', 'Agon Serifi', 'Ruben Grandia', 'Espen Knoop', 'Moritz B\\"acher']&lt;/li&gt;&lt;li&gt;Tags: ['robot-safety', 'reinforcement-learning', 'safe-falling', 'sim-to-real', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10635</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Querying Labeled Time Series Data with Scenario Programs</title><link>https://arxiv.org/abs/2511.10627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes matching between labeled time-series sensor data and abstract scenario programs written in the Scenic probabilistic programming language.&lt;/li&gt;&lt;li&gt;Presents a querying algorithm that identifies portions of real-world datasets that match a given scenario program, enabling validation of simulation-discovered failure scenarios against real data.&lt;/li&gt;&lt;li&gt;Reports higher accuracy and orders-of-magnitude faster query times compared to a state-of-the-art commercial vision large language model, with scalability in query duration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Kim', 'Devan Shanker', 'Varun Bharadwaj', 'Hongbeen Park', 'Jinkyu Kim', 'Hazem Torfah', 'Daniel J Fremont', 'Sanjit A Seshia']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'simulation-to-real (sim-to-real)', 'scenario-based testing', 'probabilistic programming (Scenic)', 'time-series querying']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10627</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation</title><link>https://arxiv.org/abs/2511.10370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SHRUG-FM, a reliability-aware framework combining input-space OOD detection, embedding-space OOD detection, and task-specific predictive uncertainty for foundation models in Earth observation.&lt;/li&gt;&lt;li&gt;Evaluated on burn scar segmentation: OOD scores correlate with lower model performance in specific environmental conditions, while uncertainty flags help filter many poor predictions.&lt;/li&gt;&lt;li&gt;Analyzes geographic concentration of failures via HydroATLAS land-cover attributes, linking underrepresentation in pretraining data (e.g., low-elevation and large-river regions) to reliability issues and proposing a path toward safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai-Hendrik Cohrs', 'Zuzanna Osika', 'Maria Gonzalez-Calabuig', 'Vishal Nedungadi', 'Ruben Cartuyvels', 'Steffen Knoblauch', 'Joppe Massant', 'Shruti Nath', 'Patrick Ebel', 'Vasileios Sitokonstantinou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'out-of-distribution-detection', 'uncertainty-estimation', 'deployment-safety', 'geospatial-foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10370</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection</title><link>https://arxiv.org/abs/2511.10308</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes eight error categories for pedestrian detection and uses image segmentation to automatically classify detector errors.&lt;/li&gt;&lt;li&gt;Introduces new, per-error-category metrics to provide a finer-grained and more realistic evaluation of pedestrian detectors, emphasizing safety-critical performance.&lt;/li&gt;&lt;li&gt;Applies the metrics to compare various backbones on a simplified APD and reports SOTA on CityPersons-reasonable without extra training data using a simple architecture.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Patrick Feifel', 'Benedikt Franke', 'Frank Bonarens', 'Frank K\\"oster', 'Arne Raulf', 'Friedhelm Schwenker']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'autonomous driving', 'pedestrian detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10308</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization</title><link>https://arxiv.org/abs/2511.09775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHAP entropy regularization: an entropy-based penalty during training that discourages low-entropy (peaked) SHAP attribution distributions to reduce information leakage from explanations.&lt;/li&gt;&lt;li&gt;Develops a suite of SHAP-based privacy inference attacks that use explanation outputs to infer sensitive user attributes in AIoT (smart home energy) settings.&lt;/li&gt;&lt;li&gt;Evaluates defense and attacks on benchmark smart-home energy consumption datasets, showing reduced privacy leakage while maintaining predictive accuracy and explanation fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dilli Prasad Sharma', 'Xiaowei Sun', 'Liang Xue', 'Xiaodong Lin', 'Pulei Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'explainability / XAI', 'SHAP', 'privacy-preserving ML', 'AIoT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09775</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Tight Robustness Certification through the Convex Hull of $\ell_0$ Attacks</title><link>https://arxiv.org/abs/2511.10576</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness certification against few-pixel (ℓ0) attacks by characterizing the convex hull of an ℓ0-ball as the intersection of its bounding box and an asymmetrically scaled ℓ1-like polytope.&lt;/li&gt;&lt;li&gt;Derives that the convex hull and the ℓ1-like polytope have nearly equal volumes in high dimensions and proposes a linear bound propagation method that exactly computes bounds over this convex hull.&lt;/li&gt;&lt;li&gt;Demonstrates significant scaling and tightness improvements over prior ℓ0 verifiers on standard robustness benchmarks (1.24x–7.07x speedups, geometric mean 3.16x).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Shapira', 'Dana Drachsler-Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'ℓ0 attacks / few-pixel attacks', 'robustness certification', 'linear bound propagation', 'verification/scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10576</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title><link>https://arxiv.org/abs/2511.10573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Responsible Reinforcement Learning (RRL) framework that formulates personalization as a Constrained Markov Decision Process to balance engagement with ethical safety and emotional alignment.&lt;/li&gt;&lt;li&gt;Introduces an emotion-informed state representation capturing emotional readiness, affect, and risk, and a multi-objective reward balancing short-term engagement with long-term well-being.&lt;/li&gt;&lt;li&gt;Suggests instantiating RRL with standard RL algorithms (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization and outlines simulation-based validation paths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Garapati Keerthana', 'Manik Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'ethical AI', 'alignment', 'affective computing', 'constrained MDP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robust Decentralized Multi-armed Bandits: From Corruption-Resilience to Byzantine-Resilience</title><link>https://arxiv.org/abs/2511.10344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Considers decentralized multi-agent multi-armed bandits under adversarial corruption and Byzantine agents.&lt;/li&gt;&lt;li&gt;Introduces DeMABAR algorithm that bounds each agent's regret with an additive term proportional to the corruption budget.&lt;/li&gt;&lt;li&gt;Theoretically analyzes corruption-resilience and shows the algorithm is inherently robust when a small fraction of agents are Byzantine.&lt;/li&gt;&lt;li&gt;Includes numerical experiments demonstrating robustness and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zicheng Hu', 'Yuchen Wang', 'Cheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine robustness', 'Adversarial corruption', 'Decentralized learning', 'Multi-armed bandits', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10344</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title><link>https://arxiv.org/abs/2511.10287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OutSafe-Bench, a large-scale multimodal benchmark for offensive/unsafe content detection covering text, images, audio, and video with bilingual annotations.&lt;/li&gt;&lt;li&gt;Provides a dataset: ~18,000 bilingual (Chinese/English) text prompts, 4,500 images, 450 audio clips, and 450 videos annotated across nine content risk categories.&lt;/li&gt;&lt;li&gt;Proposes the Multidimensional Cross Risk Score (MCRS) to model overlapping correlated risks and FairScore, an explainable multi-reviewer weighted aggregation framework using top models as adaptive juries.&lt;/li&gt;&lt;li&gt;Evaluates nine state-of-the-art MLLMs and reports substantial safety vulnerabilities, highlighting gaps in multimodal content safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuping Yan', 'Yuhan Xie', 'Yuanshuai Li', 'Yingchao Yu', 'Lingjuan Lyu', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'benchmarking', 'content moderation', 'evaluation metrics', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10287</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title><link>https://arxiv.org/abs/2511.10234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes sensitivity of LLM-based graph reasoners to graph serialization choices (node labeling, edge ordering, syntax) and node reindexing.&lt;/li&gt;&lt;li&gt;Proposes a decomposition of serialization factors and introduces a comprehensive benchmarking suite plus novel spectral tasks to evaluate generalization.&lt;/li&gt;&lt;li&gt;Finds larger (non-fine-tuned) models are more robust; fine-tuning reduces sensitivity to relabeling but can increase brittleness to structure/format changes and does not reliably improve unseen-task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Herbst', 'Lea Karbeska', 'Divyanshu Kumar', 'Akanksha Ahuja', 'Fatemeh Gholamzadeh Nasrabadi', 'Fabrizio Frasca']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'invariance', 'graph reasoning', 'benchmarking', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10234</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title><link>https://arxiv.org/abs/2511.10094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Matryoshka Transcoders, a method for hierarchical sparse feature learning to automatically discover physical plausibility failure modes in generative models.&lt;/li&gt;&lt;li&gt;Trains on intermediate representations from a physical-plausibility classifier and uses large multimodal models to produce natural-language interpretations of discovered visual/physics-related patterns.&lt;/li&gt;&lt;li&gt;Produces a benchmark for evaluating physical plausibility and analyzes eight state-of-the-art generative models to surface common physics constraint violations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Tang', 'Abhijeet Sinha', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'failure modes', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10094</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models</title><link>https://arxiv.org/abs/2511.10089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of societal (race/gender) bias encoded in latent spaces of five open-source text-to-image models using 10 neutral profession prompts and 5,000 generated images.&lt;/li&gt;&lt;li&gt;Human annotators of diverse backgrounds evaluated outputs and found systematic stereotyping: caregiving roles feminized, high-status professions skewed male and predominantly White, with model-specific patterns (e.g., QWEN-Image biased to East Asian outputs).&lt;/li&gt;&lt;li&gt;Provides comparative analysis across models and discusses risks and actionable mitigation strategies for building more equitable generative systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abu Sufian', 'Cosimo Distante', 'Marco Leo', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'safety-evaluation', 'text-to-image', 'model-audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10089</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>eXIAA: eXplainable Injections for Adversarial Attack</title><link>https://arxiv.org/abs/2511.10088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box, model-agnostic single-step adversarial attack that alters post-hoc explanations (saliency maps, integrated gradients, DeepLIFT SHAP) while keeping the model's predicted class and probabilities unchanged.&lt;/li&gt;&lt;li&gt;Requires only access to model predictions and provided explanations (no internal weights or gradients) and is evaluated on pretrained ResNet-18 and ViT-B16 on ImageNet.&lt;/li&gt;&lt;li&gt;Quantitatively measures explanation change (mean absolute difference) and image similarity (SSIM), demonstrating that explanations can be dramatically perturbed without visible image changes — highlighting risks for safety-critical uses of XAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonardo Pesce', 'Jiawen Wei', 'Gianmarco Mengaldo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'explainability', 'black-box', 'XAI-manipulation', 'image-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10088</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust Multimodal Learning in the Open World</title><link>https://arxiv.org/abs/2511.09989</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines robustness challenges for multimodal learning in open-world environments, including unpredictable composition dynamics, incomplete modality inputs, and spurious distributional relations.&lt;/li&gt;&lt;li&gt;Aims to bridge the gap between controlled experimental performance and real-world deployment reliability for multimodal systems.&lt;/li&gt;&lt;li&gt;Focuses on improving resilience to distribution shift, missing modalities, and ambiguous/complex multimodal inputs (likely via evaluation frameworks or methodological proposals).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fushuo Huo']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-robustness', 'open-world', 'distribution-shift', 'missing-modality', 'robustness-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09989</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models</title><link>https://arxiv.org/abs/2511.09864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Uncertainty-Guided Checkpoint Selection (UGCS) to pick better RL-finetuned LLM checkpoints by ranking how well checkpoints handle high-uncertainty (hard) samples.&lt;/li&gt;&lt;li&gt;UGCS computes per-sample uncertainty and averages rewards on top-uncertain samples over a short training window to produce a stable selection signal with minimal extra compute.&lt;/li&gt;&lt;li&gt;Experiments on three datasets and three LLMs show UGCS identifies checkpoints with stronger generalization than relying on training/validation performance or final checkpoints.&lt;/li&gt;&lt;li&gt;Main claim: models that solve their hardest tasks with low uncertainty are more reliable overall, improving alignment outcomes from RL finetuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manh Nguyen', 'Dung Nguyen', 'Dai Do', 'Svetha Venkatesh', 'Hung Le']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RL finetuning', 'checkpoint selection', 'uncertainty estimation', 'model robustness/generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09864</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting</title><link>https://arxiv.org/abs/2511.09855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of machine unlearning approaches for large language models, focusing on methods to ensure permanent forgetting without full retraining.&lt;/li&gt;&lt;li&gt;Reviews evaluation techniques for verifying forgetting, resilience against adversarial recovery, and measurement of residual memorization.&lt;/li&gt;&lt;li&gt;Examines technical mitigations (differential privacy, ephemeral memory, federated learning, homomorphic encryption) alongside institutional safeguards (auditing, regulation).&lt;/li&gt;&lt;li&gt;Identifies gaps: efficient verifiable unlearning methods, stronger defenses against adversarial recovery, and governance mechanisms to build trust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Jin Kang', 'Dang Bui', 'Thanh Pham', 'Huo-Chong Ling']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy &amp; data deletion', 'adversarial recovery', 'differential privacy', 'auditing &amp; governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09855</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO</title><link>https://arxiv.org/abs/2511.09780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first adversarial attack on decentralised Group Relative Policy Optimization (GRPO) for LLM post-training, showing malicious parties can inject tokens to poison models.&lt;/li&gt;&lt;li&gt;Demonstrates both out-of-context and in-context poisoning on math and coding tasks, achieving up to 100% attack success within ~50 iterations.&lt;/li&gt;&lt;li&gt;Proposes two defense strategies (depending on whether users share the same model or train different models) that can block the attack with up to 100% stop rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolay Blagoev', 'O\\u{g}uzhan Ersoy', 'Lydia Yiyu Chen']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'decentralized training', 'LLM post-training', 'adversarial attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09780</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Is nasty noise actually harder than malicious noise?</title><link>https://arxiv.org/abs/2511.09763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows a strong equivalence: for distribution-independent learning, efficient learners tolerant of η-rate malicious noise are also efficient for η-rate nasty noise.&lt;/li&gt;&lt;li&gt;Proves an arbitrarily large separation in the fixed-distribution setting under a standard cryptographic assumption: tolerant rates for malicious vs nasty noise can differ by any factor r.&lt;/li&gt;&lt;li&gt;Introduces and analyzes ICE (ignore contradictory examples) algorithms: any efficient ICE learner for η-rate malicious noise can be converted to succeed with η/2-rate nasty noise, and the factor 2 is information-theoretically/cryptographically necessary.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guy Blanc', 'Yizhi Huang', 'Tal Malkin', 'Rocco A. Servedio']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial noise', 'data poisoning', 'robustness', 'learning theory', 'cryptographic hardness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09763</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning</title><link>https://arxiv.org/abs/2511.09693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a constrained reinforcement learning framework for training Text2SQL LLMs that incorporates interpretable reward and constraint signals and dynamically balances trade-offs during training.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees for the constrained RL approach and demonstrates empirical improvements over state-of-the-art RL methods (e.g., GRPO, DAPO) on standard Text2SQL benchmarks.&lt;/li&gt;&lt;li&gt;Targets mitigation of reward hacking by using constraints and more natural reward signals to encourage genuine task-solving rather than score-exploiting behaviors.&lt;/li&gt;&lt;li&gt;Focused on improving robustness and alignment of LLMs in the Text2SQL domain via constraint-aware training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiqin Chen', 'Nhan Huu Pham', 'Michael Robert Glass', 'Long Hai Vu', 'Gaetano Rossiello', 'Dharmashankar Subramanian', 'Santiago Paternain']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward hacking', 'reinforcement learning', 'robustness', 'Text2SQL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09693</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning</title><link>https://arxiv.org/abs/2511.09681</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEBA, a sample-efficient black-box adversarial attack framework targeting visual (image-based) reinforcement learning agents.&lt;/li&gt;&lt;li&gt;Combines a shadow Q model (to estimate cumulative rewards under attack), a GAN-based perturbation generator (to produce imperceptible visual perturbations), and a learned world model (to reduce real environment queries).&lt;/li&gt;&lt;li&gt;Uses a two-stage iterative training loop alternating between learning the shadow model and refining the generator, achieving strong reward degradation with far fewer environment interactions on MuJoCo and Atari benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tairan Huang', 'Yulin Jin', 'Junxu Liu', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'reinforcement learning', 'robustness', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09681</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title><link>https://arxiv.org/abs/2511.09392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CREAT, a bi-level constrained reinforcement learning attack that performs fine-grained profile pollution on sequential recommender systems to induce targeted mispredictions.&lt;/li&gt;&lt;li&gt;Balances adversarial effectiveness and stealth by combining pattern inversion rewards (to flip critical transition patterns) with distribution consistency rewards (to minimize detectable shifts via unbalanced co-optimal transport).&lt;/li&gt;&lt;li&gt;Introduces Constrained Group Relative Reinforcement Learning for step-wise perturbations with dynamic barrier constraints and group-shared replay to achieve targeted poisoning with minimal detectability; validated through extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie Su', 'Zihan Nan', 'Yunshan Ma', 'Xiaobo Xia', 'Xiaohua Feng', 'Weiming Liu', 'Xiaolin Zheng', 'Chaochao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['profile pollution', 'data poisoning', 'adversarial attack', 'reinforcement learning', 'recommender systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09392</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms</title><link>https://arxiv.org/abs/2511.08570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptKAN, a method for automatic, data-driven domain/grid updates in Kolmogorov-Arnold Networks using layer output histograms.&lt;/li&gt;&lt;li&gt;Shows AdaptKAN matches or exceeds prior KANs and MLPs on tasks including Feynman symbolic equation learning, image classification from frozen features, control Lyapunov function learning, and OOD detection on OpenOOD v1.5.&lt;/li&gt;&lt;li&gt;Identifies the histogram-based algorithm as useful for detecting out-of-distribution (OOD) inputs, evaluated on an OOD benchmark.&lt;/li&gt;&lt;li&gt;Main contribution is training/architecture automation and interpretability improvements; OOD detection is presented as an additional benefit rather than the primary focus.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jamison Moody', 'James Usevitch']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-distribution detection', 'Robustness', 'Model architecture', 'Interpretability', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08570</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models</title><link>https://arxiv.org/abs/2511.07503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Biologically-Informed Hybrid Membership Inference Attack (biHMIA) that combines black‑box MIA techniques with domain-specific genomics metrics to improve attack success against generative genomic models.&lt;/li&gt;&lt;li&gt;Evaluates GPT-like transformer models (small and large) as synthetic genetic mutation profile generators and assesses privacy using differential privacy (DP) mechanisms.&lt;/li&gt;&lt;li&gt;Finds the hybrid attack yields higher adversarial success on average compared to traditional metric-based MIAs and empirically examines the effectiveness of DP modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asia Belfiore', 'Jonathan Passerat-Palmbach', 'Dmitrii Usynin']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attacks', 'differential-privacy', 'generative-models', 'genomics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07503</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios</title><link>https://arxiv.org/abs/2510.26125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WOD-E2E, a benchmark/dataset of 4,021 driving segments (~12 hours) from Waymo Open Dataset curated for rare, challenging long-tail driving scenarios (occurrence &lt;0.03%).&lt;/li&gt;&lt;li&gt;Each segment includes high-level routing, ego states, and 360-degree camera views (8 cameras); intended for vision-based end-to-end driving research.&lt;/li&gt;&lt;li&gt;Proposes a new open-loop evaluation metric, Rater Feedback Score (RFS), which scores predicted trajectories against rater-annotated trajectory preferences; validation labels released, test labels held for a 2025 challenge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runsheng Xu', 'Hubert Lin', 'Wonseok Jeon', 'Hao Feng', 'Yuliang Zou', 'Liting Sun', 'John Gorman', 'Ekaterina Tolstaya', 'Sarah Tang', 'Brandyn White', 'Ben Sapp', 'Mingxing Tan', 'Jyh-Jing Hwang', 'Dragomir Anguelov']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'long-tail-robustness', 'dataset-benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.26125</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title><link>https://arxiv.org/abs/2510.01252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a pipeline combining GPT-style LLMs with sparse autoencoders (SAEs) to trace how social themes from pretraining data are encoded in model representations.&lt;/li&gt;&lt;li&gt;Case study: trained a GPT-style model on 37 nineteenth-century novels by female authors and probed SAE-derived sparse features for eleven social/moral categories (e.g., gender, marriage, class).&lt;/li&gt;&lt;li&gt;Finds stable thematic features (notably gender/kinship) that persist across layers and show increasing association/entanglement with depth, arguing the approach scales to audit cultural assumptions embedded in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mariam Mahran', 'Katharina Simbeck']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'interpretability', 'dataset auditing', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01252</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a root cause of failed unlearning: updates target overly general representations, inadvertently removing benign capabilities.&lt;/li&gt;&lt;li&gt;Proposes Collapse of Irrelevant Representations (CIR): perform PCA on activations and module-output gradients to find common representation subspaces and collapse them before computing unlearning updates.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains unlearning bio- and cyber-hazardous facts from Llama-3.1-8B — ~30× greater reduction in post-attack accuracy vs. best baseline while disrupting general performance ~30× less.&lt;/li&gt;&lt;li&gt;Achieves computational efficiency (&lt;3 GPU-seconds per fact) and claims disentangling harmful vs. benign capabilities at representation level enables robust, non-disruptive unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Model editing', 'AI safety', 'Representation learning', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>The Prompt War: How AI Decides on a Military Intervention</title><link>https://arxiv.org/abs/2507.06277</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical conjoint experiment using 640 vignettes, each run 100 times, to probe AI decisions about military intervention.&lt;/li&gt;&lt;li&gt;Tested multiple LLMs (OpenAI GPT, Anthropic Claude, Google Gemini) and found consistent patterns across models.&lt;/li&gt;&lt;li&gt;Key predictors of AI recommending intervention: high domestic support and high probability of success; costs (international condemnation, military/civilian deaths, economic harms) matter but have ~half the effect size.&lt;/li&gt;&lt;li&gt;Window-of-opportunity effects mainly appear via interactions; results suggest systematic tendencies in LLM decision-making about high-risk outcomes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maxim Chupilkin']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'LLM-behavior', 'misuse-risk', 'political-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06277</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption</title><link>https://arxiv.org/abs/2506.12846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a decentralized verifiable functional encryption (DVFE) scheme to enable verification of relations over multi-dimensional ciphertexts without relying on non-colluding dual servers or trusted third parties.&lt;/li&gt;&lt;li&gt;Designs VFEFL, a privacy-preserving federated learning framework that leverages DVFE and a robust aggregation rule to detect and mitigate malicious client behavior while protecting local data from model inversion.&lt;/li&gt;&lt;li&gt;Provides formal definitions, security model and proofs for DVFE, plus empirical evaluation demonstrating privacy protection, robustness, verifiability, and model fidelity under adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nina Cai', 'Jinguang Han', 'Weizhi Meng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'verifiable functional encryption', 'privacy-preserving ML', 'robustness against malicious clients', 'secure aggregation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12846</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title><link>https://arxiv.org/abs/2502.18573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents FactReasoner, a neuro-symbolic probabilistic framework to assess factuality of long-form LLM outputs by decomposing responses into atomic units.&lt;/li&gt;&lt;li&gt;Retrieves external evidence for each unit and models logical relationships (entailment/contradiction) via probabilistic encodings to estimate posterior support.&lt;/li&gt;&lt;li&gt;Evaluates on labeled and unlabeled benchmarks, often outperforming prompt-based factuality methods; implementation is open-source.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Radu Marinescu', 'Debarun Bhattacharjya', 'Junkyu Lee', 'Tigran Tchrakian', 'Javier Carnerero Cano', 'Yufang Hou', 'Elizabeth Daly', 'Alessandra Pascale']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'truthfulness', 'hallucination_detection', 'safety_evaluation', 'neuro-symbolic_reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title><link>https://arxiv.org/abs/2501.18638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAP (Graph of Attacks with Pruning), a graph-based framework for generating stealthy jailbreak prompts that enables knowledge sharing across attack paths.&lt;/li&gt;&lt;li&gt;Reports empirical gains over prior tree-based methods: +20.8% attack success and -62.7% query cost, with &gt;96% success against open and closed LLMs.&lt;/li&gt;&lt;li&gt;Presents variants (GAP-Auto for automated seed generation, GAP-VLM for multimodal/vision-language attacks) and shows GAP prompts improve content moderation when used for fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Schwartz', 'Dmitriy Bespalov', 'Zhe Wang', 'Ninad Kulkarni', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'red teaming', 'adversarial prompting', 'content moderation', 'multimodal attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18638</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors</title><link>https://arxiv.org/abs/2501.14250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Siren, a learning-based multi-turn jailbreak attack framework that simulates realistic human adversary behaviors using three stages: MiniMax-driven dataset construction with turn-level LLM feedback, attacker fine-tuning (SFT and DPO), and attacker-target interactions.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (e.g., 90% with LLaMA-3-8B attacking Gemini-1.5-Pro; 70% with Mistral-7B against GPT-4o), outperforming single-turn baselines and matching stronger attackers while using fewer turns.&lt;/li&gt;&lt;li&gt;Highlights decomposition strategies and dynamic multi-turn tactics that align semantically with attack goals, and releases code for replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Zhao', 'Youzhi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14250</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reducing the Scope of Language Models</title><link>https://arxiv.org/abs/2410.21597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates methods to constrain LLM behavior to an intended purpose ('scoping') so models reject out-of-scope queries.&lt;/li&gt;&lt;li&gt;Compares prompting, supervised fine-tuning, preference learning, and Circuit Breakers (CB) across multiple model families and task domains.&lt;/li&gt;&lt;li&gt;Finds supervised fine-tuning works best with diverse examples of irrelevant queries; CBs perform well when irrelevant-example diversity is low; layering methods can combine benefits.&lt;/li&gt;&lt;li&gt;Includes ablative studies, adversarial evaluations, and guidance for practitioners deploying scoped LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Yunis', 'Siyu Huo', 'Chulaka Gunasekara', 'Danish Contractor']&lt;/li&gt;&lt;li&gt;Tags: ['model scoping', 'alignment', 'safety/guardrails', 'circuit breakers', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21597</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models</title><link>https://arxiv.org/abs/2511.08379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using Self-Organizing Maps (SOMs) to extract multiple latent directions encoding 'refusal' behavior in LLMs, rather than a single difference-of-means vector.&lt;/li&gt;&lt;li&gt;Provides a proof that SOMs generalize the prior difference-in-means technique and derives multiple refusal directions by subtracting harmless centroids from SOM neurons trained on harmful prompts.&lt;/li&gt;&lt;li&gt;Empirically shows that ablating multiple SOM-derived directions suppresses refusal more effectively than the single-direction baseline and outperforms specialized jailbreak algorithms.&lt;/li&gt;&lt;li&gt;Analyzes mechanistic interpretability implications of representing refusal as a low-dimensional manifold and discusses how multi-directional ablations affect model behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgio Piras', 'Raffaele Mura', 'Fabio Brau', 'Luca Oneto', 'Fabio Roli', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'mechanistic-interpretability', 'refusal-suppression', 'latent-space-ablation', 'adversarial-manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08379</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking</title><link>https://arxiv.org/abs/2511.07863</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WaterMod: a token-rank modular partitioning watermark that sorts vocabulary by model probability and assigns tokens to classes by rank mod k, then biases one class to embed a detectable signal.&lt;/li&gt;&lt;li&gt;Zero-bit (k=2) uses parity with an entropy-adaptive gate to ensure at least one high-probability token remains available, preserving fluency while enabling detection; multi-bit (k&gt;2) embeds base-k payload digits per decoding step.&lt;/li&gt;&lt;li&gt;Demonstrates strong watermark detection performance with minimal impact on generation quality across tasks including general NLG, mathematical reasoning, and code synthesis; code and data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shinwoo Park', 'Hyejin Park', 'Hyeseon Ahn', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'provenance/attribution', 'LLM defenses', 'robustness', 'probability-aware watermark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07863</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title><link>https://arxiv.org/abs/2509.14289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple LLM-based agent architectures (single-agent to modular) on realistic penetration testing scenarios, measuring empirical performance and failure modes.&lt;/li&gt;&lt;li&gt;Isolates and tests five core functional capabilities (GCM, IAM, CCI, AP, RTM) via targeted augmentations to assess their impact on agent effectiveness in multi-step and real-time attacks.&lt;/li&gt;&lt;li&gt;Finds that augmentations substantially improve modular agent performance, especially for complex, multi-step, and dynamic penetration testing tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lanxiao Huang', 'Daksh Dave', 'Tyler Cody', 'Peter Beling', 'Ming Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'agent architectures', 'tool use / execution', 'benchmarking / evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14289</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title><link>https://arxiv.org/abs/2507.20067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PITA, an inference-time alignment framework that learns a small preference-guided policy to modify LLM token probabilities without fine-tuning the LLM or training a separate reward model.&lt;/li&gt;&lt;li&gt;Frames alignment as identifying an underlying preference distribution and solves it via stochastic search and iterative refinement of the guidance model.&lt;/li&gt;&lt;li&gt;Claims reduced computational cost and empirical improvements on tasks like mathematical reasoning and sentiment classification by integrating preference feedback directly into token generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarat Chandra Bobbili', 'Ujwal Dinesha', 'Dheeraj Narasimha', 'Srinivas Shakkottai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time alignment', 'preference learning', 'reward-model-free']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20067</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box On-Policy Distillation of Large Language Models</title><link>https://arxiv.org/abs/2511.10643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Generative Adversarial Distillation (GAD), a black-box, on-policy distillation method that treats the student LLM as a generator and trains a discriminator to distinguish student from teacher outputs.&lt;/li&gt;&lt;li&gt;The discriminator provides adaptive, on-policy reward-like feedback that co-evolves with the student in a minimax training loop.&lt;/li&gt;&lt;li&gt;Empirical results show GAD outperforms sequence-level knowledge distillation and can produce a student (Qwen2.5-14B-Instruct) comparable to its teacher (GPT-5-Chat) on LMSYS-Chat automatic evaluation.&lt;/li&gt;&lt;li&gt;Operates purely from teacher text outputs (no access to logits or parameters), making it a black-box model extraction/distillation approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianzhu Ye', 'Li Dong', 'Zewen Chi', 'Xun Wu', 'Shaohan Huang', 'Furu Wei']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'black-box distillation', 'knowledge distillation', 'adversarial training', 'model stealing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10643</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title><link>https://arxiv.org/abs/2511.10573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Responsible Reinforcement Learning (RRL) framework that formulates personalization as a Constrained Markov Decision Process to balance engagement with ethical safety and emotional alignment.&lt;/li&gt;&lt;li&gt;Introduces an emotion-informed state representation capturing emotional readiness, affect, and risk, and a multi-objective reward balancing short-term engagement with long-term well-being.&lt;/li&gt;&lt;li&gt;Suggests instantiating RRL with standard RL algorithms (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization and outlines simulation-based validation paths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Garapati Keerthana', 'Manik Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'ethical AI', 'alignment', 'affective computing', 'constrained MDP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10573</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Say It Differently: Linguistic Styles as Jailbreak Vectors</title><link>https://arxiv.org/abs/2511.10519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies linguistic style (e.g., fearful, curious, compassionate) as an attack surface for LLM jailbreaks and systematically studies style-driven reframing of harmful intents.&lt;/li&gt;&lt;li&gt;Builds a style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 styles via handcrafted templates and LLM-based rewrites, preserving semantics.&lt;/li&gt;&lt;li&gt;Evaluates 16 open- and closed-source instruction-tuned models, finding stylistic reframing can increase jailbreak success rates by up to +57 percentage points; contextualized rewrites outperform templated variants.&lt;/li&gt;&lt;li&gt;Proposes a mitigation: a style-neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues, substantially reducing jailbreak success rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Srikant Panda', 'Avinash Rai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10519</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>On the Detectability of Active Gradient Inversion Attacks in Federated Learning</title><link>https://arxiv.org/abs/2511.10502</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes detectability of active Gradient Inversion Attacks (GIAs) in Federated Learning, focusing on four state-of-the-art active attacks.&lt;/li&gt;&lt;li&gt;Proposes lightweight client-side detection methods based on statistically improbable weight structures and anomalous loss/gradient dynamics.&lt;/li&gt;&lt;li&gt;Shows extensive evaluation across multiple configurations that clients can effectively detect active GIAs without changing the FL protocol.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincenzo Carletti', 'Pasquale Foggia', 'Carlo Mazzocca', 'Giuseppe Parrella', 'Mario Vento']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'gradient inversion attacks', 'privacy', 'attack detection', 'adversarial server']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10502</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning About Intent for Ambiguous Requests</title><link>https://arxiv.org/abs/2511.10453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes generating multiple interpretation-answer pairs in a single structured response to handle ambiguous user requests.&lt;/li&gt;&lt;li&gt;Uses reinforcement learning with customized reward functions and multiple valid answers as supervision to train models.&lt;/li&gt;&lt;li&gt;Demonstrates improved coverage of valid answers on conversational QA and semantic parsing; human evaluation shows interpretations align well with answers.&lt;/li&gt;&lt;li&gt;Emphasizes transparency, efficiency (single generation step), and structured outputs to support downstream applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Irina Saparina', 'Mirella Lapata']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'intent-interpretation', 'ambiguity-resolution', 'reinforcement-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10453</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</title><link>https://arxiv.org/abs/2511.10403</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces nuPlan-R, a closed-loop planning benchmark that replaces rule-based reactive agents (e.g., IDM) with noise-decoupled diffusion-based learning agents to produce more realistic, diverse, and human-like traffic behaviors.&lt;/li&gt;&lt;li&gt;Adds an interaction-aware agent selection mechanism for computational efficiency and introduces two additional metrics to better assess planner performance in interactive scenarios.&lt;/li&gt;&lt;li&gt;Reimplements a suite of rule-based, learning-based, and hybrid planners within the new benchmark and demonstrates that nuPlan-R better distinguishes planner performance in complex, interactive driving situations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingxing Peng', 'Ruoyu Yao', 'Xusen Guo', 'Jun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'simulation-benchmark', 'multi-agent', 'planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10403</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title><link>https://arxiv.org/abs/2511.10400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates reliability of LLM-based multi-agent systems using Byzantine fault tolerance and finds LLM agents exhibit stronger skepticism to erroneous message flows than traditional agents.&lt;/li&gt;&lt;li&gt;Proposes CP-WBFT, a confidence probe–based weighted Byzantine Fault Tolerant consensus mechanism that leverages LLMs' reflective/discriminative capabilities to weight information flow and improve consensus.&lt;/li&gt;&lt;li&gt;Shows extensive experiments across diverse network topologies under extreme Byzantine conditions (up to 85.7% fault rate), with improved accuracy on mathematical reasoning and safety assessment tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lifan Zheng', 'Jiawei Chen', 'Qinghong Yin', 'Jingyuan Zhang', 'Xinyi Zeng', 'Yu Tian']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine fault tolerance', 'Multi-agent systems', 'LLM robustness', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10400</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Simulating Misinformation Propagation in Social Networks using Large Language Models</title><link>https://arxiv.org/abs/2511.10384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models synthetic agents as persona-conditioned LLM nodes to simulate user biases, ideology, and trust heuristics in social networks.&lt;/li&gt;&lt;li&gt;Propagates and reforms news articles across networks of up to 30 sequential rewrites, measuring factual fidelity with a question-answering auditor and defining a misinformation index and propagation rate.&lt;/li&gt;&lt;li&gt;Finds identity/ideology-driven personas accelerate misinformation (esp. politics, marketing, tech) while expert personas preserve fidelity; heterogeneous interactions amplify distortions into propaganda-level drift.&lt;/li&gt;&lt;li&gt;Provides a taxonomy of misinformation severity and presents an interpretable framework for studying and mitigating misinformation diffusion with LLMs as both proxies and auditors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raj Gaurav Maurya', 'Vaibhav Shukla', 'Raj Abhijit Dandekar', 'Rajat Dandekar', 'Sreedath Panat']&lt;/li&gt;&lt;li&gt;Tags: ['misinformation', 'LLM simulation', 'safety evaluation', 'social network modeling', 'auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10384</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation</title><link>https://arxiv.org/abs/2511.10370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SHRUG-FM, a reliability-aware framework combining input-space OOD detection, embedding-space OOD detection, and task-specific predictive uncertainty for foundation models in Earth observation.&lt;/li&gt;&lt;li&gt;Evaluated on burn scar segmentation: OOD scores correlate with lower model performance in specific environmental conditions, while uncertainty flags help filter many poor predictions.&lt;/li&gt;&lt;li&gt;Analyzes geographic concentration of failures via HydroATLAS land-cover attributes, linking underrepresentation in pretraining data (e.g., low-elevation and large-river regions) to reliability issues and proposing a path toward safer deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai-Hendrik Cohrs', 'Zuzanna Osika', 'Maria Gonzalez-Calabuig', 'Vishal Nedungadi', 'Ruben Cartuyvels', 'Steffen Knoblauch', 'Joppe Massant', 'Shruti Nath', 'Patrick Ebel', 'Vasileios Sitokonstantinou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'out-of-distribution-detection', 'uncertainty-estimation', 'deployment-safety', 'geospatial-foundation-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10370</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models</title><link>https://arxiv.org/abs/2511.10292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RUDDER, an inference-time, low-overhead steering framework to reduce object hallucination in large vision-language models.&lt;/li&gt;&lt;li&gt;Introduces CARD, a per-sample visual evidence vector extracted from a single forward-pass residual update, and a Bayesian-inspired adaptive gate for token-wise corrective injection.&lt;/li&gt;&lt;li&gt;Demonstrates comparable performance to state-of-the-art hallucination mitigation methods on benchmarks (POPE, CHAIR) with negligible additional latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengtao Zou', 'Ya Gao', 'Jiarui Guan', 'Bin Li', 'Pekka Marttinen']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'inference-time steering', 'vision-language models', 'robustness', 'efficiency/latency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10292</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models</title><link>https://arxiv.org/abs/2511.10262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MTR-DuplexBench, a benchmark that segments continuous full‑duplex (overlapping) speech dialogues into discrete turns for turn-by-turn evaluation.&lt;/li&gt;&lt;li&gt;Evaluates FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety in multi-round settings.&lt;/li&gt;&lt;li&gt;Finds current full‑duplex speech language models struggle to maintain consistent performance across multiple rounds and evaluation dimensions.&lt;/li&gt;&lt;li&gt;Provides a benchmarking resource (code/data to be released) to better assess multi‑round safety and instruction-following in real‑time speech models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['He Zhang', 'Wenqian Cui', 'Haoning Xu', 'Xiaohui Li', 'Lei Zhu', 'Shaohua Ma', 'Irwin King']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmark', 'full-duplex', 'multi-round-dialogue', 'instruction-following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10262</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title><link>https://arxiv.org/abs/2511.10234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes sensitivity of LLM-based graph reasoners to graph serialization choices (node labeling, edge ordering, syntax) and node reindexing.&lt;/li&gt;&lt;li&gt;Proposes a decomposition of serialization factors and introduces a comprehensive benchmarking suite plus novel spectral tasks to evaluate generalization.&lt;/li&gt;&lt;li&gt;Finds larger (non-fine-tuned) models are more robust; fine-tuning reduces sensitivity to relabeling but can increase brittleness to structure/format changes and does not reliably improve unseen-task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Herbst', 'Lea Karbeska', 'Divyanshu Kumar', 'Akanksha Ahuja', 'Fatemeh Gholamzadeh Nasrabadi', 'Fabrizio Frasca']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'invariance', 'graph reasoning', 'benchmarking', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10234</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</title><link>https://arxiv.org/abs/2511.10222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SACRED-Bench, a benchmark for red-teaming multimodal LLMs with speech-audio composition attacks (speech overlap/multi-speaker, speech-audio mixtures, diverse spoken instruction formats).&lt;/li&gt;&lt;li&gt;Finds high vulnerability in state-of-the-art models (e.g., ~66% attack success on Gemini 2.5 Pro) under these cross-modal attacks.&lt;/li&gt;&lt;li&gt;Proposes SALMONN-Guard, an audio-aware safeguard LLM that inspects speech, audio, and text jointly and reduces attack success to ~20%.&lt;/li&gt;&lt;li&gt;Releases dataset and checkpoints (Hugging Face) to support further evaluation and defense development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Yang', 'Xuezhen Zhang', 'Zhifeng Han', 'Siyin Wang', 'Jimin Zhuang', 'Zengrui Jin', 'Jing Shao', 'Guangzhi Sun', 'Chao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'audio-based attacks', 'multimodal safety', 'benchmark', 'defense/safeguard']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10222</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction</title><link>https://arxiv.org/abs/2511.10203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VISTA, a recursive goal-conditioned transformer that fuses long-horizon intent with past motion and models social interactions via social-token attention and pairwise attention maps.&lt;/li&gt;&lt;li&gt;Targets multi-agent trajectory forecasting with an emphasis on generating socially compliant, interpretable trajectories and reducing collisions in dense interactive scenes.&lt;/li&gt;&lt;li&gt;Evaluated on MADRAS and SDD benchmarks, showing state-of-the-art accuracy and large reductions in trajectory collision rates (e.g., 2.14% -&gt; 0.03% on MADRAS).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephane Da Silva Martins', 'Emanuel Aldea', "Sylvie Le H\\'egarat-Mascle"]&lt;/li&gt;&lt;li&gt;Tags: ['trajectory prediction', 'autonomous driving', 'safety / collision avoidance', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10203</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models</title><link>https://arxiv.org/abs/2511.10089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of societal (race/gender) bias encoded in latent spaces of five open-source text-to-image models using 10 neutral profession prompts and 5,000 generated images.&lt;/li&gt;&lt;li&gt;Human annotators of diverse backgrounds evaluated outputs and found systematic stereotyping: caregiving roles feminized, high-status professions skewed male and predominantly White, with model-specific patterns (e.g., QWEN-Image biased to East Asian outputs).&lt;/li&gt;&lt;li&gt;Provides comparative analysis across models and discusses risks and actionable mitigation strategies for building more equitable generative systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abu Sufian', 'Cosimo Distante', 'Marco Leo', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['bias', 'fairness', 'safety-evaluation', 'text-to-image', 'model-audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10089</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>eXIAA: eXplainable Injections for Adversarial Attack</title><link>https://arxiv.org/abs/2511.10088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box, model-agnostic single-step adversarial attack that alters post-hoc explanations (saliency maps, integrated gradients, DeepLIFT SHAP) while keeping the model's predicted class and probabilities unchanged.&lt;/li&gt;&lt;li&gt;Requires only access to model predictions and provided explanations (no internal weights or gradients) and is evaluated on pretrained ResNet-18 and ViT-B16 on ImageNet.&lt;/li&gt;&lt;li&gt;Quantitatively measures explanation change (mean absolute difference) and image similarity (SSIM), demonstrating that explanations can be dramatically perturbed without visible image changes — highlighting risks for safety-critical uses of XAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonardo Pesce', 'Jiawen Wei', 'Gianmarco Mengaldo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'explainability', 'black-box', 'XAI-manipulation', 'image-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10088</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback</title><link>https://arxiv.org/abs/2511.10032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study in the kidney allocation domain with &gt;400 participants across 3–5 sessions eliciting pairwise moral preference judgments.&lt;/li&gt;&lt;li&gt;Finds response instability (same scenario answered differently across sessions) in ~6–20% of cases and substantial shifts in retrofitted decision models for some participants (model instability).&lt;/li&gt;&lt;li&gt;Demonstrates that predictive performance of simple AI models degrades as a function of both response and model instability, and performance diminishes over time.&lt;/li&gt;&lt;li&gt;Argues this temporal instability raises normative and technical challenges for AI alignment methods that assume static human preferences and calls for approaches that account for legitimate preference change vs. noise.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vijay Keswani', 'Cyrus Cousins', 'Breanna Nguyen', 'Vincent Conitzer', 'Hoda Heidari', 'Jana Schaich Borg', 'Walter Sinnott-Armstrong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-feedback', 'preference-drift', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10032</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks</title><link>https://arxiv.org/abs/2511.10008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of physical sensor attacks on Vision-Language-Action (VLA) models using a Real-Sim-Real framework that simulates physics-based attack vectors and validates them on real robots.&lt;/li&gt;&lt;li&gt;Defines and evaluates eight attack types (six camera-based, two microphone-based) across multiple VLA architectures, tasks, and attack parameters, revealing significant vulnerabilities and task/model-dependent susceptibility patterns.&lt;/li&gt;&lt;li&gt;Proposes an adversarial-training-based defense that improves robustness to out-of-distribution physical perturbations from sensor attacks while preserving nominal performance.&lt;/li&gt;&lt;li&gt;Calls for standardized robustness benchmarks and mitigation strategies for securing VLA deployments in safety-critical environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancun Lu', 'Jiaxiang Chen', 'Shilin Xiao', 'Zizhi Jin', 'Zhangrui Chen', 'Hanwen Yu', 'Bohan Qian', 'Ruochen Zhou', 'Xiaoyu Ji', 'Wenyuan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['physical sensor attacks', 'VLA robustness', 'adversarial training', 'robotics security', 'sensor spoofing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10008</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines</title><link>https://arxiv.org/abs/2511.09964</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EnvTrace, a simulation-based method that evaluates LLM-generated instrument-control code by aligning execution traces to assess semantic equivalence.&lt;/li&gt;&lt;li&gt;Demonstrates a beamline control-logic digital twin that enables pre-execution validation and safe evaluation of control code in a high-fidelity simulated environment.&lt;/li&gt;&lt;li&gt;Applies trace alignment to produce multi-faceted functional-correctness scores and evaluates over 30 LLMs, finding many top models approach human-level performance in rapid control-code generation.&lt;/li&gt;&lt;li&gt;Positions digital twins as safety scaffolds for autonomous/agentic LLMs, enabling safer deployment and testing of embodied AI for physical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noah van der Vleuten', 'Anthony Flores', 'Shray Mathur', 'Max Rakitin', 'Thomas Hopkins', 'Kevin G. Yager', 'Esther H. R. Tsai']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'digital-twin', 'LLM-code-generation', 'simulation-based-testing', 'robotics/instrument-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09964</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code</title><link>https://arxiv.org/abs/2511.09879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructed a 'secure' Python training dataset by filtering an existing corpus with a static analysis tool to remove detectable vulnerabilities.&lt;/li&gt;&lt;li&gt;Trained two transformer-based models (one on curated, one on unfiltered data) and evaluated generated code for both functional correctness and security issues.&lt;/li&gt;&lt;li&gt;Found that the model trained on the curated dataset produced fewer security vulnerabilities while maintaining comparable functional correctness, highlighting the impact of training-data quality on code-model safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Catherine Xia', 'Manar H. Alalfi']&lt;/li&gt;&lt;li&gt;Tags: ['dataset security', 'code generation', 'vulnerability mitigation', 'training data curation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09879</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage</title><link>https://arxiv.org/abs/2511.09834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CertMask, a certifiable defense against adversarial patch attacks that constructs a provably sufficient set of binary masks to neutralize patch effects.&lt;/li&gt;&lt;li&gt;Uses a mathematically rigorous coverage strategy ensuring each possible patch location is covered at least k times, enabling single-round masking with O(n) inference cost (vs. PatchCleanser's two rounds and O(n^2)).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis proving the sufficiency of the coverage condition for certification.&lt;/li&gt;&lt;li&gt;Empirical results on ImageNet, ImageNette, and CIFAR-10 show up to +13.4% improvement in certified robust accuracy over PatchCleanser while preserving clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuntao Lyu', 'Ching-Chi Lin', 'Abdullah Al Arafat', 'Georg von der Br\\"uggen', 'Jian-Jia Chen', 'Zhishan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'certified robustness', 'provable defense', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09834</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization</title><link>https://arxiv.org/abs/2511.09775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHAP entropy regularization: an entropy-based penalty during training that discourages low-entropy (peaked) SHAP attribution distributions to reduce information leakage from explanations.&lt;/li&gt;&lt;li&gt;Develops a suite of SHAP-based privacy inference attacks that use explanation outputs to infer sensitive user attributes in AIoT (smart home energy) settings.&lt;/li&gt;&lt;li&gt;Evaluates defense and attacks on benchmark smart-home energy consumption datasets, showing reduced privacy leakage while maintaining predictive accuracy and explanation fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dilli Prasad Sharma', 'Xiaowei Sun', 'Liang Xue', 'Xiaodong Lin', 'Pulei Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'explainability / XAI', 'SHAP', 'privacy-preserving ML', 'AIoT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09775</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</title><link>https://arxiv.org/abs/2511.09748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarks sub-2B instruction-tuned LLMs (e.g., Gemma-3-1B, Qwen-3-1.7B, Llama-3.2-1B) for English→German Critical Error Detection (CED) in machine translation across WMT21, WMT22, and SynCED-EnDe-2025.&lt;/li&gt;&lt;li&gt;Identifies a quality–efficiency sweet spot near 1B parameters (Gemma-3-1B) achieving high MCC and F1-ERR while running on-device (≈400 ms latency on MacBook Pro M4 Pro); larger models improve absolute metrics at higher compute cost.&lt;/li&gt;&lt;li&gt;Introduces a standardized framework using prompts, lightweight logit-bias calibration, majority voting, and small-sample fine-tuning; releases datasets, prompts, and scripts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muskaan Chopra', 'Lorenz Sparrenberg', 'Sarthak Khanna', 'Rafet Sifa']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'model-robustness', 'on-device-ML', 'machine-translation', 'compact-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09748</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Soiling detection for Advanced Driver Assistance Systems</title><link>https://arxiv.org/abs/2511.09740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Treats soiling detection for automotive cameras as a semantic segmentation task and compares popular segmentation methods against tile-level classification.&lt;/li&gt;&lt;li&gt;Identifies data leakage and imprecise annotations in the Woodscape dataset and provides an analysis of these issues.&lt;/li&gt;&lt;li&gt;Constructs a revised, smaller dataset subset and demonstrates that segmentation on this subset achieves comparable results with faster training.&lt;/li&gt;&lt;li&gt;Releases code and dataset splits for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Filip Ber\\'anek", "V\\'aclav Divi\\v{s}", 'Ivan Gruber']&lt;/li&gt;&lt;li&gt;Tags: ['ADAS safety', 'perception robustness', 'semantic segmentation', 'dataset curation', 'data quality / leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09740</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning</title><link>https://arxiv.org/abs/2511.09681</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEBA, a sample-efficient black-box adversarial attack framework targeting visual (image-based) reinforcement learning agents.&lt;/li&gt;&lt;li&gt;Combines a shadow Q model (to estimate cumulative rewards under attack), a GAN-based perturbation generator (to produce imperceptible visual perturbations), and a learned world model (to reduce real environment queries).&lt;/li&gt;&lt;li&gt;Uses a two-stage iterative training loop alternating between learning the shadow model and refining the generator, achieving strong reward degradation with far fewer environment interactions on MuJoCo and Atari benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tairan Huang', 'Yulin Jin', 'Junxu Liu', 'Qingqing Ye', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'reinforcement learning', 'robustness', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09681</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Debt: The Hidden Work of Making AI Usable</title><link>https://arxiv.org/abs/2511.09663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the concept of “alignment debt”: the user-side burden when frontier LLMs misfit cultural, linguistic, infrastructural, or epistemic contexts.&lt;/li&gt;&lt;li&gt;Develops and validates a four-part taxonomy (Cultural &amp; Linguistic, Infrastructural, Epistemic, Interaction) via a survey of 411 AI users in Kenya and Nigeria, reporting prevalence rates for each debt type.&lt;/li&gt;&lt;li&gt;Finds different debt types associate differently with compensatory behaviours (e.g., higher verification with Epistemic debt, weak/no verification response for Infrastructural/Interaction debt) and that cumulative debt correlates with verification intensity.&lt;/li&gt;&lt;li&gt;Argues for context-aware safeguards and governance in Global South settings, offering an empirically grounded framework to measure user burden and inform design/policy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research (empirical)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cumi Oyemike', 'Elizabeth Akpan', "Pierre Herv\\'e-Berdys"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'socio-technical fairness', 'user burden', 'empirical survey', 'Global South / contextualization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09663</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Querying Labeled Time Series Data with Scenario Programs</title><link>https://arxiv.org/abs/2511.10627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes matching between labeled time-series sensor data and abstract scenario programs written in the Scenic probabilistic programming language.&lt;/li&gt;&lt;li&gt;Presents a querying algorithm that identifies portions of real-world datasets that match a given scenario program, enabling validation of simulation-discovered failure scenarios against real data.&lt;/li&gt;&lt;li&gt;Reports higher accuracy and orders-of-magnitude faster query times compared to a state-of-the-art commercial vision large language model, with scalability in query duration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Kim', 'Devan Shanker', 'Varun Bharadwaj', 'Hongbeen Park', 'Jinkyu Kim', 'Hazem Torfah', 'Daniel J Fremont', 'Sanjit A Seshia']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'simulation-to-real (sim-to-real)', 'scenario-based testing', 'probabilistic programming (Scenic)', 'time-series querying']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10627</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage</title><link>https://arxiv.org/abs/2511.10284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a formal framework that uses abductive explanations to audit privacy leakage by identifying minimal sufficient evidence justifying model decisions.&lt;/li&gt;&lt;li&gt;Defines individual- and system-level leakage and introduces Potentially Applicable Explanations (PAE) to detect when other individuals' outcomes can shield those with sensitive features.&lt;/li&gt;&lt;li&gt;Produces human-understandable explanations to support privacy auditing and reports experiments on the German Credit Dataset demonstrating how the importance of a sensitive literal affects leakage.&lt;/li&gt;&lt;li&gt;Notes computational challenges and simplifying assumptions but argues abductive reasoning offers a practical, interpretable path for privacy-preserving AI decision auditing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Belona Sonna', 'Alban Grastien', 'Claire Benn']&lt;/li&gt;&lt;li&gt;Tags: ['privacy auditing', 'privacy leakage', 'abductive explanations', 'explainable AI', 'model auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10284</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention</title><link>https://arxiv.org/abs/2511.10268</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines object hallucination in LVLMs via a Structural Causal Model to formalize spurious correlations from object co-occurrence.&lt;/li&gt;&lt;li&gt;Introduces Causal-HalBench: a benchmark with counterfactual samples and causal metrics to quantify model robustness to spurious co-occurrence biases.&lt;/li&gt;&lt;li&gt;Provides a pipeline to generate counterfactual images using proprietary LVLMs and text-to-image models and evaluates mainstream LVLMs, revealing varying susceptibility to spurious correlations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Xu', 'Zhicai Wang', 'Junkang Wu', 'Jinda Lu', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['object hallucination', 'LVLMs', 'spurious correlations', 'causal analysis', 'robustness benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10268</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title><link>https://arxiv.org/abs/2511.10240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ProgRAG, a multi-hop KGQA framework that decomposes complex questions into sub-questions and progressively extends partial reasoning paths.&lt;/li&gt;&lt;li&gt;Uses external retrievers to gather candidate KG evidence at each step and applies LLM-based uncertainty-aware pruning to filter/refine evidence.&lt;/li&gt;&lt;li&gt;Optimizes LLM reasoning context by organizing and rearranging partial reasoning paths to reduce hallucinations and improve transparency.&lt;/li&gt;&lt;li&gt;Shows empirical gains over baselines on three multi-hop KGQA datasets, demonstrating improved reliability and reasoning quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minbae Park', 'Hyemin Yang', 'Jeonghyun Kim', 'Kunsoo Park', 'Hyunjoon Kim']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'knowledge-graph-enhanced LLMs', 'retrieval-augmented reasoning', 'multi-hop QA', 'uncertainty-aware pruning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10240</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2511.10128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies two attack paths in retrieval-augmented generation (RAG) systems—intra-class (fine-grained extraction within topics) and inter-class (diffusion across semantically related topics)—and shows defending only one path is insufficient.&lt;/li&gt;&lt;li&gt;Proposes RAGFort, a dual-module defense combining 'contrastive reindexing' to isolate inter-class leakage and 'constrained cascade generation' to limit intra-class reconstruction.&lt;/li&gt;&lt;li&gt;Provides systematic evaluation across security (reconstruction reduction), performance (answer quality), and robustness, showing substantial mitigation of knowledge-base extraction while preserving utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinfeng Li', 'Miao Pan', 'Ke Xiong', 'Ge Su', 'Zhiqiang Shen', 'Yan Liu', 'Bing Sun', 'Hao Peng', 'Xuhong Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-base-extraction', 'RAG-defense', 'reconstruction-attacks', 'model-extraction', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10128</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title><link>https://arxiv.org/abs/2511.10067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MuSeR (Multifaceted Self-Refinement): generate attribute-conditioned queries, have an LLM self-evaluate responses along decision-making, communication, and safety facets, refine outputs, and use refined pairs for supervised fine-tuning.&lt;/li&gt;&lt;li&gt;Designs an attribute-conditioned query generator to simulate diverse real-world medical contexts (role, region, intent, ambiguity) to improve context-awareness.&lt;/li&gt;&lt;li&gt;Combines self-refinement with knowledge distillation to improve smaller models (e.g., Qwen3-32B), achieving new SOTA on HealthBench and its hard subset.&lt;/li&gt;&lt;li&gt;Focuses explicitly on improving safety and context-awareness in medical responses, reporting notable gains on the safety/context-awareness axis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxuan Zhou', 'Yubin Wang', 'Bin Wang', 'Chen Ning', 'Xien Liu', 'Ji Wu', 'Jianye Hao']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'context-awareness', 'self-refinement', 'medical LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10067</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation</title><link>https://arxiv.org/abs/2511.10065</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RadFlow, a hierarchical reinforcement learning fine-tuning framework that models the structured radiology reporting workflow (Findings -&gt; Impression) with section-specific and global rewards.&lt;/li&gt;&lt;li&gt;Introduces a clinically grounded reward hierarchy combining linguistic fluency, medical-domain correctness, and cross-sectional consistency, plus a critical-aware policy regularization for high-risk cases.&lt;/li&gt;&lt;li&gt;Evaluated on chest X-ray and carotid ultrasound datasets, showing improved diagnostic coherence and report quality versus state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bodong Du', 'Honglong Yang', 'Xiaomeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['medical report generation', 'hierarchical reinforcement learning', 'reward design', 'clinical safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10065</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>CTRL-ALT-DECEIT: Sabotage Evaluations for Automated AI R&amp;D</title><link>https://arxiv.org/abs/2511.09904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends MLE-Bench with sabotage/code-subversion tasks for autonomous ML agents (e.g., implanting backdoors, inducing generalisation failures) and integrates into the Inspect framework.&lt;/li&gt;&lt;li&gt;Evaluates frontier agents' ability to perform sabotage and to sandbag (calibrate performance below capability), showing agents can meaningfully succeed at both.&lt;/li&gt;&lt;li&gt;Assesses LM-based monitors for detecting sabotage: monitors detect code-sabotage fairly well, but sandbagging is harder to detect; aggregating monitors helps but may be insufficient for high-stakes settings.&lt;/li&gt;&lt;li&gt;Releases benchmark and code publicly to enable further red-teaming and evaluation of agent deception capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francis Rhys Ward', 'Teun van der Weij', "Hanna G\\'abor", 'Sam Martin', 'Raja Mehta Moreno', 'Harel Lidar', 'Louis Makower', 'Thomas Jodrell', 'Lauren Robson']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial ML', 'jailbreaking/sabotage', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09904</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Quantum Artificial Intelligence (QAI): Foundations, Architectural Elements, and Future Directions</title><link>https://arxiv.org/abs/2511.09884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey-style exploration of Quantum Artificial Intelligence (QAI) focused on mission-critical (MC) systems, covering quantum computing foundations, QAI architectures, and deployment considerations.&lt;/li&gt;&lt;li&gt;Discusses core algorithmic elements such as quantum-enhanced learning pipelines, quantum uncertainty quantification, and quantum explainability frameworks.&lt;/li&gt;&lt;li&gt;Explores application domains (aerospace, defense, cybersecurity, smart grids) emphasizing robustness, fault tolerance, real-time constraints, and resource scheduling for MC deployment.&lt;/li&gt;&lt;li&gt;Identifies challenges including trainability, data-loading bottlenecks, verification of quantum components, and explicitly mentions adversarial QAI and related safety/robustness concerns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siva Sai', 'Rajkumar Buyya']&lt;/li&gt;&lt;li&gt;Tags: ['quantum-ml', 'safety-robustness', 'adversarial-qai', 'verification', 'mission-critical-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09884</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Thermally Activated Dual-Modal Adversarial Clothing against AI Surveillance Systems</title><link>https://arxiv.org/abs/2511.09829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a wearable adversarial system combining thermochromic dyes and embedded flexible heaters to reveal hidden adversarial patterns when activated, while appearing as a normal black T-shirt by default.&lt;/li&gt;&lt;li&gt;Designed to evade AI surveillance across visible and infrared (thermal) imaging modalities, providing user-controlled activation of adversarial textures.&lt;/li&gt;&lt;li&gt;Reports physical experiments showing pattern activation within ~50 seconds and an adversarial success rate above 80% across diverse real-world surveillance settings.&lt;/li&gt;&lt;li&gt;Demonstrates a practical, physically grounded approach to privacy-preserving adversarial attacks against vision-based surveillance systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahuan Long', 'Tingsong Jiang', 'Hanqing Liu', 'Chao Ma', 'Wen Yao']&lt;/li&gt;&lt;li&gt;Tags: ['physical-adversarial-attacks', 'adversarial-clothing', 'multimodal-evasion', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09829</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Robust Watermarking on Gradient Boosting Decision Trees</title><link>https://arxiv.org/abs/2511.09822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first robust watermarking framework specifically for Gradient Boosting Decision Trees (GBDTs) using in-place fine-tuning to embed watermarks.&lt;/li&gt;&lt;li&gt;Proposes four embedding strategies designed to be imperceptible (minimal accuracy impact) while ensuring watermark resilience.&lt;/li&gt;&lt;li&gt;Evaluates methods across multiple datasets, reporting high embedding rates, low accuracy degradation, and resistance to post-deployment fine-tuning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Woo Chung', 'Yingjie Lao', 'Weijie Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model-ownership', 'robustness', 'model-protection', 'GBDT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09822</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Echoing: Identity Failures when LLM Agents Talk to Each Other</title><link>https://arxiv.org/abs/2511.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new class of agent-agent (AxA) failure called "echoing," where LLM agents abandon assigned roles and mirror their conversational partners, undermining objectives.&lt;/li&gt;&lt;li&gt;Empirical study across 60 AxA configurations, 3 domains, 2000+ conversations, and three major LLM providers showing echoing rates from 5% to 70%; persists in advanced reasoning models (e.g., ~32.8%) and grows with conversation length (notable after ~7+ turns).&lt;/li&gt;&lt;li&gt;Analyzes prompt and conversation dynamics, concluding echoing is not merely due to poor prompts, and proposes a protocol-level mitigation using structured responses that reduces echoing to ~9%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarath Shekkizhar', 'Romain Cosentino', 'Adam Earle', 'Silvio Savarese']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'multi-agent robustness', 'behavioral drift', 'alignment failure', 'mitigation/protocol']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09710</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models</title><link>https://arxiv.org/abs/2511.09682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows standard reasoning training (RT) with safety data can block vanilla audio jailbreaks but fails against more advanced audio jailbreaks due to representation drift.&lt;/li&gt;&lt;li&gt;Identifies representation drift between vanilla and advanced jailbreaks as the cause of harmful outputs from Audio Reasoning Models (ARMs).&lt;/li&gt;&lt;li&gt;Proposes Rebellion, a robust RT method that trains ARMs for worst-case representation drift, improving robustness without degrading benign-task performance.&lt;/li&gt;&lt;li&gt;Evaluates on Qwen2-Audio and reports improved accuracy-safety trade-offs versus standard RT.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiansheng Huang', 'Virat Shejwalkar', 'Oscar Chang', 'Milad Nasr', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial robustness', 'audio model safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09682</guid><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>
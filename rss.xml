<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 19 Jan 2026 22:50:49 +0000</lastBuildDate><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an integrated safety evaluation of six frontier models (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image generation using benchmarks, adversarial tests, multilingual and compliance evaluations.&lt;/li&gt;&lt;li&gt;Finds highly uneven safety: GPT-5.2 shows the most balanced safety performance, but all models are highly vulnerable to adversarial testing (worst-case safety rates under 6%).&lt;/li&gt;&lt;li&gt;Notes text-to-image models show somewhat better alignment in regulated visual risk categories but remain fragile to adversarial or semantically ambiguous prompts; calls for standardized, holistic safety assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'safety evaluation', 'multimodal vulnerabilities', 'red teaming', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Power to the Clients: Federated Learning in a Dictatorship Setting</title><link>https://arxiv.org/abs/2510.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'dictator clients', a novel class of malicious federated learning participants that can fully erase other clients' contributions while preserving their own in the global model.&lt;/li&gt;&lt;li&gt;Proposes concrete attack strategies for single and multiple dictator clients, including collaborative, independent, and alliance-then-betrayal scenarios.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of the attacks' impact on convergence and supports findings with empirical evaluations on computer vision and natural language benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadsajad Alipour', 'Mohammad Mohammadi Amiri']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning attack', 'model poisoning', 'adversarial clients', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22149</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Known Fakes: Generalized Detection of AI-Generated Images via Post-hoc Distribution Alignment</title><link>https://arxiv.org/abs/2502.10803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Post-hoc Distribution Alignment (PDA), a model-agnostic detection framework that regenerates test images using a known generator and treats detection as a distribution alignment problem.&lt;/li&gt;&lt;li&gt;Key insight: regenerated real images align with the known fake distribution (inheriting model-specific artifacts), whereas regenerated unknown fakes remain misaligned due to incompatible/mixed artifacts—enabling a detector trained on one generator to detect unknown fakes without retraining.&lt;/li&gt;&lt;li&gt;Evaluated across 16 state-of-the-art generative models (GANs, diffusion models, commercial APIs like Midjourney), achieving average detection accuracy of 96.69% and outperforming baselines by ~10.7%, with ablations showing robustness to distribution shifts and image transforms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Wang', 'Wenyu Chen', 'Xiangtao Meng', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'defense', 'distribution alignment', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10803</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VidLeaks: Membership Inference Attacks Against Text-to-Video Models</title><link>https://arxiv.org/abs/2601.11210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of membership inference attacks (MIAs) targeting text-to-video (T2V) generative models, highlighting spatio-temporal memorization risks.&lt;/li&gt;&lt;li&gt;Introduces VidLeaks, which combines Spatial Reconstruction Fidelity (SRF) with a Top-K similarity to detect sparsely memorized keyframes, and Temporal Generative Stability (TGS) to capture temporal leakage via consistency across queries.&lt;/li&gt;&lt;li&gt;Evaluates under three progressively restrictive black-box settings (supervised, reference-based, query-only) and demonstrates high attack performance (e.g., AUC 82.92% on AnimateDiff, 97.01% on InstructVideo) even in query-only scenarios.&lt;/li&gt;&lt;li&gt;Provides code and argues that T2V models leak substantial membership information, motivating the need for defenses and auditing tools.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Wang', 'Wenyu Chen', 'Ning Yu', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-leakage', 'black-box-attacks', 'text-to-video', 'temporal-memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11210</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Your One-Stop Solution for AI-Generated Video Detection</title><link>https://arxiv.org/abs/2601.11035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIGVDBench, a large-scale benchmark for AI-generated video detection covering 31 generative models and over 440,000 videos.&lt;/li&gt;&lt;li&gt;Performs extensive evaluation: ~1,500 experiments across 33 existing detectors spanning four detector categories.&lt;/li&gt;&lt;li&gt;Provides eight in-depth analyses and reports four novel findings to guide future research in synthetic video detection.&lt;/li&gt;&lt;li&gt;Open-sources the benchmark and dataset to facilitate standardized evaluation and advancement of detection defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Long Ma', 'Zihao Xue', 'Yan Wang', 'Zhiyuan Yan', 'Jin Xu', 'Xiaorui Jiang', 'Haiyang Yu', 'Yong Liao', 'Zhen Bi']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'benchmarking', 'synthetic video', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11035</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2601.10836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive empirical study benchmarking 21 post-hoc OOD detection methods on 56 ImageNet-trained ResNet-50 models across 8 OOD test sets.&lt;/li&gt;&lt;li&gt;Key finding: OOD detection performance is non-monotonically related to in-distribution (ID) accuracy — improves up to a point but can degrade when advanced training recipes further increase ID accuracy.&lt;/li&gt;&lt;li&gt;Shows strong interactions between training strategy, detector choice, and OOD performance, and that no single detector is universally best.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking / Empirical Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerhard Krumpl', 'Henning Avenhaus', 'Horst Possegger']&lt;/li&gt;&lt;li&gt;Tags: ['Out-of-distribution detection', 'Robustness / Defense', 'Benchmarking / Evaluation', 'Training effects']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10836</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an integrated safety evaluation of six frontier models (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image generation using benchmarks, adversarial tests, multilingual and compliance evaluations.&lt;/li&gt;&lt;li&gt;Finds highly uneven safety: GPT-5.2 shows the most balanced safety performance, but all models are highly vulnerable to adversarial testing (worst-case safety rates under 6%).&lt;/li&gt;&lt;li&gt;Notes text-to-image models show somewhat better alignment in regulated visual risk categories but remain fragile to adversarial or semantically ambiguous prompts; calls for standardized, holistic safety assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'safety evaluation', 'multimodal vulnerabilities', 'red teaming', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Good is Post-Hoc Watermarking With Language Model Rephrasing?</title><link>https://arxiv.org/abs/2512.16904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies post-hoc watermarking where an LLM rewrites existing text while embedding generation-time watermarks to enable traceability and detection of use in training/RAG.&lt;/li&gt;&lt;li&gt;Analyzes how compute allocation (larger rephrasing models, beam search, multi-candidate generation, entropy filtering) affects the detectability vs. semantic fidelity trade-off across open-ended text and verifiable text (e.g., code).&lt;/li&gt;&lt;li&gt;Finds that a simple Gumbel-max scheme can outperform newer methods under nucleus sampling, beam search significantly improves performance, and watermarking struggles on verifiable text where smaller models sometimes beat larger ones.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pierre Fernandez', 'Tom Sander', 'Hady Elsahar', 'Hongyan Chang', "Tom\\'a\\v{s} Sou\\v{c}ek", 'Valeriu Lacatusu', 'Tuan Tran', 'Sylvestre-Alvise Rebuffi', 'Alexandre Mourachko']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'detection', 'LLM security', 'post-hoc watermarking', 'text-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.16904</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Power to the Clients: Federated Learning in a Dictatorship Setting</title><link>https://arxiv.org/abs/2510.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'dictator clients', a novel class of malicious federated learning participants that can fully erase other clients' contributions while preserving their own in the global model.&lt;/li&gt;&lt;li&gt;Proposes concrete attack strategies for single and multiple dictator clients, including collaborative, independent, and alliance-then-betrayal scenarios.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of the attacks' impact on convergence and supports findings with empirical evaluations on computer vision and natural language benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadsajad Alipour', 'Mohammad Mohammadi Amiri']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning attack', 'model poisoning', 'adversarial clients', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22149</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda</title><link>https://arxiv.org/abs/2601.08837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Adversarial Tales', a jailbreak technique that hides harmful instructions inside narrative/structural interpretation tasks (e.g., using Proppian tale morphology) to bypass LLM safety filters.&lt;/li&gt;&lt;li&gt;Empirically evaluates the attack across 26 frontier models from nine providers, reporting an average success rate of 71.3% and showing no model family is reliably robust.&lt;/li&gt;&lt;li&gt;Argues that structurally-grounded, culturally-coded frames (like poetry or narratives) form a broad vulnerability class and proposes a mechanistic interpretability research agenda to understand why models fail and how to detect intent beyond surface form.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Marcello Galisai', 'Matteo Prandi', 'Federico Pierucci', 'Olga Sorokoletova', 'Francesco Giarrusso', 'Vincenzo Suriani', 'Marcantonio Bracale Syrnikov', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial attacks', 'interpretability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.08837</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG</title><link>https://arxiv.org/abs/2601.05866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FACTUM, a mechanistic framework with four scores (Contextual Alignment CAS, Attention Sink Usage BAS, Parametric Force PFS, Pathway Alignment PAS) to detect citation hallucinations in Retrieval-Augmented Generation (RAG) models.&lt;/li&gt;&lt;li&gt;Shows that correct citations correlate with higher parametric force and greater use of attention sinks, and that the mechanistic signature of correctness shifts with model scale (different behaviors in 3B vs 8B models).&lt;/li&gt;&lt;li&gt;Reports that FACTUM outperforms state-of-the-art baselines (up to 37.5% AUC improvement), enabling more nuanced detection and improved safety/robustness of RAG systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maxime Dassen', 'Rebecca Kotula', 'Kenton Murray', 'Andrew Yates', 'Dawn Lawrie', 'Efsun Kayi', 'James Mayfield', 'Kevin Duh']&lt;/li&gt;&lt;li&gt;Tags: ['citation hallucination', 'RAG', 'hallucination detection', 'mechanistic interpretability', 'robustness/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.05866</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title><link>https://arxiv.org/abs/2511.15304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial poetry as a universal single-turn jailbreak that substantially increases attack-success rates (ASR) across 25 proprietary and open-weight LLMs.&lt;/li&gt;&lt;li&gt;Converts 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt, achieving up to 18x higher ASR versus prose baselines and average ASRs of ~62% (hand-crafted) and ~43% (meta-converted).&lt;/li&gt;&lt;li&gt;Evaluates outputs with an ensemble of 3 open-weight LLM judges validated on a stratified human-labeled subset, and maps attack transfer to MLCommons/EU CoP risk taxonomies (CBRN, manipulation, cyber-offence, loss-of-control).&lt;/li&gt;&lt;li&gt;Argues that stylistic variation alone can systematically circumvent contemporary safety mechanisms, exposing limitations in current alignment and evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Matteo Prandi', 'Federico Pierucci', 'Francesco Giarrusso', 'Marcantonio Bracale Syrnikov', 'Marcello Galisai', 'Vincenzo Suriani', 'Olga Sorokoletova', 'Federico Sartore', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial attacks', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15304</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs</title><link>https://arxiv.org/abs/2510.11288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows emergent misalignment (EM) can be induced via in-context learning (ICL) without any parameter changes: narrow in-context examples cause models to produce misaligned responses to unrelated benign queries.&lt;/li&gt;&lt;li&gt;Evaluated across four LLM families (Gemini, Kimi-K2, Grok, Qwen); EM appears with as few as 2 examples and 16 examples yield EM rates of 1%–24% depending on model and domain.&lt;/li&gt;&lt;li&gt;Finds that neither increasing model scale nor adding explicit chain-of-thought reasoning reliably prevents ICL-induced EM.&lt;/li&gt;&lt;li&gt;Proposes a conflict hypothesis between safety objectives and context-following behavior; instructing models to prioritize safety reduces EM while prioritizing context-following increases it.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita Afonin', 'Nikita Andriyanov', 'Vahagn Hovhannisyan', 'Nikhil Bageshpura', 'Kyle Liu', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Oleg Rogov', 'Elena Tutubalina', 'Alexander Panchenko', 'Mikhail Seleznyov']&lt;/li&gt;&lt;li&gt;Tags: ['emergent misalignment', 'in-context learning', 'prompt injection/adversarial prompting', 'safety and robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11288</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning</title><link>https://arxiv.org/abs/2504.07128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and studies DeepSeek-R1, an LLM that exposes explicit multi-step reasoning chains ('thoughtology') and analyzes its reasoning building blocks and behaviors.&lt;/li&gt;&lt;li&gt;Empirically investigates controllability and effects of thought length, context management, rumination (persistent revisiting of formulations), and cognitive parallels; finds a 'sweet spot' where longer reasoning can degrade performance.&lt;/li&gt;&lt;li&gt;Reports and analyzes strong safety vulnerabilities in DeepSeek-R1 compared to non-reasoning counterparts, and notes these vulnerabilities can compromise safety-aligned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Sara Vera Marjanovi\\'c", 'Arkil Patel', 'Vaibhav Adlakha', 'Milad Aghajohari', 'Parishad BehnamGhader', 'Mehar Bhatia', 'Aditi Khandelwal', 'Austin Kraft', 'Benno Krojer', 'Xing Han L\\`u', 'Nicholas Meade', 'Dongchan Shin', 'Amirhossein Kazemnejad', 'Gaurav Kamath', 'Marius Mosbach', "Karolina Sta\\'nczak", 'Siva Reddy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-vulnerabilities', 'model-behavior', 'reasoning-chains', 'alignment', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.07128</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation</title><link>https://arxiv.org/abs/2501.18100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies harmful fine-tuning attacks as a security risk and shows that existing vaccination-style defenses are fragile.&lt;/li&gt;&lt;li&gt;Finds that simple random post-fine-tuning perturbations can reduce harmful behavior but degrade downstream fine-tuning performance.&lt;/li&gt;&lt;li&gt;Proposes Panacea: an optimized adaptive perturbation applied after fine-tuning that reduces harmful behaviors while preserving downstream performance.&lt;/li&gt;&lt;li&gt;Validates Panacea across multiple LLMs, tasks, and harmful ratios, reporting up to 21.2% reduction in harmful scores; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yibo Wang', 'Tiansheng Huang', 'Li Shen', 'Huanjin Yao', 'Haotian Luo', 'Rui Liu', 'Naiqiang Tan', 'Jiaxing Huang', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['harmful fine-tuning', 'defense', 'post-fine-tuning perturbation', 'LLM safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18100</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates new activation probe architectures (including 'multimax') designed to generalize from short-context to long-context inputs for misuse mitigation in large language models.&lt;/li&gt;&lt;li&gt;Evaluates robustness of probes in cyber-offensive scenarios across production-relevant distribution shifts: multi-turn conversations, static jailbreaks, and adaptive red teaming.&lt;/li&gt;&lt;li&gt;Finds that architecture choices plus training on diverse distributions are needed for broad generalization and that combining probes with prompted classifiers yields high accuracy cost-effectively.&lt;/li&gt;&lt;li&gt;Reports deployment of these probes in user-facing instances of Gemini and explores automating probe architecture search and adaptive red teaming using AlphaEvolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'activation probes', 'jailbreaks', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title><link>https://arxiv.org/abs/2601.11061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'Perplexity Paradox' where RLVR with spurious rewards reduces answer-token perplexity while degrading prompt coherence, indicating a memorization shortcut.&lt;/li&gt;&lt;li&gt;Uses mechanistic tools (Path Patching, Logit Lens, JSD, Neural DE) to discover an Anchor-Adapter circuit: a Functional Anchor in middle layers (L18-20) that retrieves memorized solutions and Structural Adapters in later layers (L21+) that reshape representations.&lt;/li&gt;&lt;li&gt;Demonstrates causal intervention: scaling specific MLP keys in the circuit can amplify or suppress contamination-driven performance, providing a potential mitigation lever.&lt;/li&gt;&lt;li&gt;Provides code and a mechanistic roadmap for detecting and mitigating data contamination and unintended memorization in RLVR-tuned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lecheng Yan', 'Ruizhe Li', 'Guanhua Chen', 'Qing Li', 'Jiahui Geng', 'Wenxi Li', 'Vincent Wang', 'Chris Lee']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data contamination', 'mechanistic interpretability', 'defense/mitigation', 'RLVR / reward tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11061</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AJAR: Adaptive Jailbreak Architecture for Red-teaming</title><link>https://arxiv.org/abs/2601.10971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AJAR, an adaptive, modular red-teaming architecture for agentic LLMs that supports complex, stateful multi-turn exploitations and tool use.&lt;/li&gt;&lt;li&gt;Implements Protocol-driven Cognitive Orchestration using a Petri-based runtime and a Model Context Protocol (MCP) to decouple adversarial logic from execution and enable plug-and-play attack modules (e.g., X-Teaming).&lt;/li&gt;&lt;li&gt;Validates feasibility with a qualitative case study showing stateful backtracking in tool-using environments and identifies new attack vectors from tool/code execution as well as the 'Agentic Gap' dynamics affecting persona-based jailbreaks.&lt;/li&gt;&lt;li&gt;Open-sources the framework to standardize environment-aware red-teaming and facilitate further research into agentic attack/defense evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yipu Dou', 'Wang Yang']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'jailbreaking', 'agentic attacks', 'tool-use vulnerabilities', 'security framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10971</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models</title><link>https://arxiv.org/abs/2601.11441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HORSE (Hierarchical Orthogonal Residual Spread), a method for precise, large-scale edits to LLMs by manipulating an information matrix via hierarchical orthogonal residuals to reduce noisy gradients and conflicts.&lt;/li&gt;&lt;li&gt;Provides theoretical comparisons with popular model-editing approaches and empirical evaluations across multiple LLMs and two datasets, showing stable and precise massive edits.&lt;/li&gt;&lt;li&gt;Frames model editing as a way to mitigate safety concerns in LLMs (patching behaviors or knowledge) and claims improved efficiency and stability over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaojie Gu', 'Guangxu Chen', 'Yuheng Yang', 'Jingxin Han', 'Andi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'LLM safety', 'model patching', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11441</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference on LLMs in the Wild</title><link>https://arxiv.org/abs/2601.11314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SimMIA, a membership inference attack framework for LLMs that works in a strict text-only (black-box) setting using an advanced sampling strategy and scoring mechanism.&lt;/li&gt;&lt;li&gt;Introduces WikiMIA-25, a benchmark to evaluate MIA performance on modern proprietary LLMs in realistic 'in the wild' scenarios.&lt;/li&gt;&lt;li&gt;Demonstrates that SimMIA achieves state-of-the-art results in the black-box/text-only regime, rivaling methods that rely on internal model information (e.g., logits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiatong Yi', 'Yanyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'black-box-attacks', 'LLMs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11314</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Integrity Shield A System for Ethical AI Use &amp; Authorship Transparency in Assessments</title><link>https://arxiv.org/abs/2601.11093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into exam PDFs without changing human-visible appearance.&lt;/li&gt;&lt;li&gt;Watermarks both prevent MLLMs from answering shielded exam PDFs and encode stable item-level signatures recoverable from model or student responses.&lt;/li&gt;&lt;li&gt;Evaluated on 30 exams across STEM, humanities, and medical reasoning against four commercial MLLMs, reporting 91–94% exam-level blocking and 89–93% signature retrieval.&lt;/li&gt;&lt;li&gt;Provides an interactive demo for instructors to upload exams, preview watermark behavior, and inspect pre/post AI performance and authorship evidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Raj Shekhar', 'Shiven Agarwal', 'Priyanuj Bordoloi', 'Yash Shah', 'Tejas Anvekar', 'Vivek Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['document watermarking', 'academic integrity', 'defense', 'authorship attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11093</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse</title><link>https://arxiv.org/abs/2601.11042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs spectral analysis showing dominant singular directions of pretrained weight matrices correlate with a model's general abilities and are highly sensitive to parameter perturbations.&lt;/li&gt;&lt;li&gt;Finds repeated sequential edits progressively disrupt these dominant singular subspaces, closely tracking collapse in editing efficacy and general performance.&lt;/li&gt;&lt;li&gt;Proposes REVIVE: a plug-and-play framework that represents parameter updates in the original weights' spectral basis and filters components that would interfere with the protected dominant subspace.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple models and benchmarks that REVIVE improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing (up to 20,000 edits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi Zhang', 'Mengqi Zhang', 'Xiaotian Ye', 'Runxi Cheng', 'Zisheng Zhou', 'Ying Zhou', 'Pengjie Ren', 'Zhumin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['sequential knowledge editing', 'spectral analysis', 'model robustness', 'parameter-update protection', 'continual editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11042</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs</title><link>https://arxiv.org/abs/2601.11000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'personalization-induced hallucinations' where personalized LLMs output answers aligned with a user's history rather than objective facts, due to entanglement between personalization and factual representations.&lt;/li&gt;&lt;li&gt;Proposes Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time mitigation that reduces personalization-driven factual distortions while retaining personalized behavior.&lt;/li&gt;&lt;li&gt;Introduces PFQABench, a benchmark for jointly evaluating factual accuracy and personalization in QA, and shows FPPS improves factuality across multiple LLM backbones and personalization methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongxiang Sun', 'Yi Zhan', 'Chenglei Shen', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['personalization', 'hallucination', 'factuality', 'robustness', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11000</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an integrated safety evaluation of six frontier models (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image generation using benchmarks, adversarial tests, multilingual and compliance evaluations.&lt;/li&gt;&lt;li&gt;Finds highly uneven safety: GPT-5.2 shows the most balanced safety performance, but all models are highly vulnerable to adversarial testing (worst-case safety rates under 6%).&lt;/li&gt;&lt;li&gt;Notes text-to-image models show somewhat better alignment in regulated visual risk categories but remain fragile to adversarial or semantically ambiguous prompts; calls for standardized, holistic safety assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'safety evaluation', 'multimodal vulnerabilities', 'red teaming', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda</title><link>https://arxiv.org/abs/2601.08837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Adversarial Tales', a jailbreak technique that hides harmful instructions inside narrative/structural interpretation tasks (e.g., using Proppian tale morphology) to bypass LLM safety filters.&lt;/li&gt;&lt;li&gt;Empirically evaluates the attack across 26 frontier models from nine providers, reporting an average success rate of 71.3% and showing no model family is reliably robust.&lt;/li&gt;&lt;li&gt;Argues that structurally-grounded, culturally-coded frames (like poetry or narratives) form a broad vulnerability class and proposes a mechanistic interpretability research agenda to understand why models fail and how to detect intent beyond surface form.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Marcello Galisai', 'Matteo Prandi', 'Federico Pierucci', 'Olga Sorokoletova', 'Francesco Giarrusso', 'Vincenzo Suriani', 'Marcantonio Bracale Syrnikov', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial attacks', 'interpretability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.08837</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Power to the Clients: Federated Learning in a Dictatorship Setting</title><link>https://arxiv.org/abs/2510.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'dictator clients', a novel class of malicious federated learning participants that can fully erase other clients' contributions while preserving their own in the global model.&lt;/li&gt;&lt;li&gt;Proposes concrete attack strategies for single and multiple dictator clients, including collaborative, independent, and alliance-then-betrayal scenarios.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of the attacks' impact on convergence and supports findings with empirical evaluations on computer vision and natural language benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadsajad Alipour', 'Mohammad Mohammadi Amiri']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning attack', 'model poisoning', 'adversarial clients', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22149</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Balanced Edge Pruning for Graph Anomaly Detection with Noisy Labels</title><link>https://arxiv.org/abs/2407.05934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes REGAD, a graph anomaly detection method that prunes edges around candidate nodes with potentially noisy labels to reduce negative propagation of label noise.&lt;/li&gt;&lt;li&gt;Uses a tailored policy network with two-step actions and a policy-in-the-loop mechanism to iteratively decide edge removals and produce reliable pseudo-labels.&lt;/li&gt;&lt;li&gt;Designs performance feedback based on confident labels to guide pruning decisions; demonstrates improved GAD performance under varied noisy-label ratios on real datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhu Wang', 'Junnan Dong', 'Shuang Zhou', 'Chang Yang', 'Shengjie Zhao', 'Xiao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['graph anomaly detection', 'label noise robustness', 'edge pruning', 'reinforcement learning', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.05934</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>IMS: Intelligent Hardware Monitoring System for Secure SoCs</title><link>https://arxiv.org/abs/2601.11447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies AXI protocol-violation vulnerabilities that enable DoS attacks on SoCs and collects AXI transaction data by performing header-field manipulations and malicious operations to build a training dataset.&lt;/li&gt;&lt;li&gt;Presents IMS, a lightweight hardware monitoring IP that uses a quantization-optimized neural network for real-time detection of AXI protocol violations, achieving 98.7% detection accuracy with &lt;=3% latency overhead and &gt;2.5M inferences/s throughput.&lt;/li&gt;&lt;li&gt;Implements IMS on a Zynq UltraScale+ MPSoC, showing small hardware footprint (9.04% LUTs, 0.23% DSP, 0.70% FFs) and negligible frequency impact, demonstrating feasibility for resource-constrained edge SoCs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wadid Foudhaili', 'Aykut Rencber', 'Anouar Nechi', 'Rainer Buchty', 'Mladen Berekovic', 'Andres Gomez', 'Saleh Mulhem']&lt;/li&gt;&lt;li&gt;Tags: ['hardware-security', 'AXI-protocol', 'DoS-detection', 'neural-network-defense', 'SoC-monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11447</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models</title><link>https://arxiv.org/abs/2601.11441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HORSE (Hierarchical Orthogonal Residual Spread), a method for precise, large-scale edits to LLMs by manipulating an information matrix via hierarchical orthogonal residuals to reduce noisy gradients and conflicts.&lt;/li&gt;&lt;li&gt;Provides theoretical comparisons with popular model-editing approaches and empirical evaluations across multiple LLMs and two datasets, showing stable and precise massive edits.&lt;/li&gt;&lt;li&gt;Frames model editing as a way to mitigate safety concerns in LLMs (patching behaviors or knowledge) and claims improved efficiency and stability over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaojie Gu', 'Guangxu Chen', 'Yuheng Yang', 'Jingxin Han', 'Andi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'LLM safety', 'model patching', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11441</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes new activation-probe architectures to detect and mitigate misuse of large language models, focusing on generalization from short- to long-context inputs.&lt;/li&gt;&lt;li&gt;Evaluates probe robustness in cyber-offensive scenarios including multi-turn conversations, static jailbreaks, and adaptive red teaming.&lt;/li&gt;&lt;li&gt;Finds that architecture choice plus diverse training distributions are required for production generalization, and that combining probes with prompted classifiers yields high accuracy at low computational cost.&lt;/li&gt;&lt;li&gt;Reports successful deployment of these probes in production Gemini and explores automated improvement via AlphaEvolve for architecture search and adaptive red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'activation probes', 'jailbreaking', 'red teaming', 'production deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid</title><link>https://arxiv.org/abs/2601.11500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QUPID, a partitioned quantum neural network (PQNN) for anomaly detection in smart grids, addressing QML scalability via partitioning.&lt;/li&gt;&lt;li&gt;Claims improved anomaly-detection performance over classical ML baselines and enhanced resilience to adversarial manipulations.&lt;/li&gt;&lt;li&gt;Introduces R-QUPID, a variant that incorporates differential privacy to further bolster robustness/privacy-preserving behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang M. Ngo', "Tre' R. Jeter", 'Jung Taek Seo', 'My T. Thai']&lt;/li&gt;&lt;li&gt;Tags: ['quantum-ml', 'anomaly-detection', 'adversarial-robustness', 'differential-privacy', 'smart-grid-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11500</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients</title><link>https://arxiv.org/abs/2601.11219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDFLoRA, a federated LoRA method that decomposes client adapters into a global module (shared, aggregated) and a local module (private) to handle rank heterogeneity across clients.&lt;/li&gt;&lt;li&gt;Selective aggregation aligns and aggregates only the global module while keeping local modules private, enabling personalization and stabilizing FL under heterogeneous low-rank configurations.&lt;/li&gt;&lt;li&gt;Introduces a privacy-aware optimization strategy by injecting differential privacy (DP) noise exclusively into the global module to protect client information while preserving utility; shows improved utility-privacy trade-offs on GLUE versus federated LoRA baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhikang Shen', 'Jianrong Lu', 'Haiyuan Wan', 'Jianhai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'parameter-efficient fine-tuning', 'LoRA', 'privacy-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11219</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Differentially Private Subspace Fine-Tuning for Large Language Models</title><link>https://arxiv.org/abs/2601.11113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-SFT, a two-stage differentially private subspace fine-tuning method for large language models that injects DP noise only into a low-dimensional task-specific subspace.&lt;/li&gt;&lt;li&gt;Phase one identifies the subspace via principal gradient directions; phase two projects gradients into this subspace, adds DP noise, and maps perturbed gradients back to the full parameter space.&lt;/li&gt;&lt;li&gt;Claims improved accuracy, stability, faster convergence, and substantial gains over baseline DP fine-tuning under rigorous differential privacy constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lele Zheng', 'Xiang Wang', 'Tao Zhang', 'Yang Cao', 'Ke Cheng', 'Yulong Shen']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-preserving-fine-tuning', 'defense', 'large-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11113</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title><link>https://arxiv.org/abs/2601.11061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'Perplexity Paradox' where RLVR with spurious rewards reduces answer-token perplexity while degrading prompt coherence, indicating a memorization shortcut.&lt;/li&gt;&lt;li&gt;Uses mechanistic tools (Path Patching, Logit Lens, JSD, Neural DE) to discover an Anchor-Adapter circuit: a Functional Anchor in middle layers (L18-20) that retrieves memorized solutions and Structural Adapters in later layers (L21+) that reshape representations.&lt;/li&gt;&lt;li&gt;Demonstrates causal intervention: scaling specific MLP keys in the circuit can amplify or suppress contamination-driven performance, providing a potential mitigation lever.&lt;/li&gt;&lt;li&gt;Provides code and a mechanistic roadmap for detecting and mitigating data contamination and unintended memorization in RLVR-tuned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lecheng Yan', 'Ruizhe Li', 'Guanhua Chen', 'Qing Li', 'Jiahui Geng', 'Wenxi Li', 'Vincent Wang', 'Chris Lee']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'data contamination', 'mechanistic interpretability', 'defense/mitigation', 'RLVR / reward tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11061</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks on Multi-modal Contrastive Learning</title><link>https://arxiv.org/abs/2601.11006</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/review of backdoor and data poisoning attacks specifically targeting contrastive learning, with emphasis on multi-modal settings.&lt;/li&gt;&lt;li&gt;Analyzes threat models, attack techniques, target domains (vision, multimodal, graphs, federated learning), and existing defenses.&lt;/li&gt;&lt;li&gt;Highlights unique vulnerabilities of contrastive learning, summarizes recent advances, and outlines challenges and future research directions.&lt;/li&gt;&lt;li&gt;Discusses implications for secure deployment in industrial and distributed environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simi D Kuniyilh', 'Rita Machacy']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'contrastive learning', 'data poisoning', 'multimodal', 'defenses/survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11006</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda</title><link>https://arxiv.org/abs/2601.08837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Adversarial Tales', a jailbreak technique that hides harmful instructions inside narrative/structural interpretation tasks (e.g., using Proppian tale morphology) to bypass LLM safety filters.&lt;/li&gt;&lt;li&gt;Empirically evaluates the attack across 26 frontier models from nine providers, reporting an average success rate of 71.3% and showing no model family is reliably robust.&lt;/li&gt;&lt;li&gt;Argues that structurally-grounded, culturally-coded frames (like poetry or narratives) form a broad vulnerability class and proposes a mechanistic interpretability research agenda to understand why models fail and how to detect intent beyond surface form.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Marcello Galisai', 'Matteo Prandi', 'Federico Pierucci', 'Olga Sorokoletova', 'Francesco Giarrusso', 'Vincenzo Suriani', 'Marcantonio Bracale Syrnikov', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial attacks', 'interpretability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.08837</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</title><link>https://arxiv.org/abs/2512.10758</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a theoretically grounded framework showing interconnected, multi-stage assessment problems and semi-structured tasks are more resilient to generative-AI misuse than modular or fully open-ended assessments.&lt;/li&gt;&lt;li&gt;Provides empirical validation using data from four university data-science courses (N = 138), showing AI-assisted modular homework inflates scores while interconnected projects better resist trivial AI delegation and align with intended skills.&lt;/li&gt;&lt;li&gt;Offers a practical assessment-design framework to help educators create integrative, real-world AI-augmented workflows that naturally deter cheating via generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaihua Ding']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'AI misuse / academic integrity', 'assessment robustness', 'education security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10758</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title><link>https://arxiv.org/abs/2511.15304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial poetry as a universal single-turn jailbreak that substantially increases attack-success rates (ASR) across 25 proprietary and open-weight LLMs.&lt;/li&gt;&lt;li&gt;Converts 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt, achieving up to 18x higher ASR versus prose baselines and average ASRs of ~62% (hand-crafted) and ~43% (meta-converted).&lt;/li&gt;&lt;li&gt;Evaluates outputs with an ensemble of 3 open-weight LLM judges validated on a stratified human-labeled subset, and maps attack transfer to MLCommons/EU CoP risk taxonomies (CBRN, manipulation, cyber-offence, loss-of-control).&lt;/li&gt;&lt;li&gt;Argues that stylistic variation alone can systematically circumvent contemporary safety mechanisms, exposing limitations in current alignment and evaluation protocols.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piercosma Bisconti', 'Matteo Prandi', 'Federico Pierucci', 'Francesco Giarrusso', 'Marcantonio Bracale Syrnikov', 'Marcello Galisai', 'Vincenzo Suriani', 'Olga Sorokoletova', 'Federico Sartore', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial attacks', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15304</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Power to the Clients: Federated Learning in a Dictatorship Setting</title><link>https://arxiv.org/abs/2510.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'dictator clients', a novel class of malicious federated learning participants that can fully erase other clients' contributions while preserving their own in the global model.&lt;/li&gt;&lt;li&gt;Proposes concrete attack strategies for single and multiple dictator clients, including collaborative, independent, and alliance-then-betrayal scenarios.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis of the attacks' impact on convergence and supports findings with empirical evaluations on computer vision and natural language benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammadsajad Alipour', 'Mohammad Mohammadi Amiri']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning attack', 'model poisoning', 'adversarial clients', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22149</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Known Fakes: Generalized Detection of AI-Generated Images via Post-hoc Distribution Alignment</title><link>https://arxiv.org/abs/2502.10803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Post-hoc Distribution Alignment (PDA), a model-agnostic detection framework that regenerates test images using a known generator and treats detection as a distribution alignment problem.&lt;/li&gt;&lt;li&gt;Key insight: regenerated real images align with the known fake distribution (inheriting model-specific artifacts), whereas regenerated unknown fakes remain misaligned due to incompatible/mixed artifacts—enabling a detector trained on one generator to detect unknown fakes without retraining.&lt;/li&gt;&lt;li&gt;Evaluated across 16 state-of-the-art generative models (GANs, diffusion models, commercial APIs like Midjourney), achieving average detection accuracy of 96.69% and outperforming baselines by ~10.7%, with ablations showing robustness to distribution shifts and image transforms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Wang', 'Wenyu Chen', 'Xiangtao Meng', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'defense', 'distribution alignment', 'robustness', 'image forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10803</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation</title><link>https://arxiv.org/abs/2501.18100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies harmful fine-tuning attacks as a security risk and shows that existing vaccination-style defenses are fragile.&lt;/li&gt;&lt;li&gt;Finds that simple random post-fine-tuning perturbations can reduce harmful behavior but degrade downstream fine-tuning performance.&lt;/li&gt;&lt;li&gt;Proposes Panacea: an optimized adaptive perturbation applied after fine-tuning that reduces harmful behaviors while preserving downstream performance.&lt;/li&gt;&lt;li&gt;Validates Panacea across multiple LLMs, tasks, and harmful ratios, reporting up to 21.2% reduction in harmful scores; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yibo Wang', 'Tiansheng Huang', 'Li Shen', 'Huanjin Yao', 'Haotian Luo', 'Rui Liu', 'Naiqiang Tan', 'Jiaxing Huang', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['harmful fine-tuning', 'defense', 'post-fine-tuning perturbation', 'LLM safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18100</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Balanced Edge Pruning for Graph Anomaly Detection with Noisy Labels</title><link>https://arxiv.org/abs/2407.05934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes REGAD, a graph anomaly detection method that prunes edges around candidate nodes with potentially noisy labels to reduce negative propagation of label noise.&lt;/li&gt;&lt;li&gt;Uses a tailored policy network with two-step actions and a policy-in-the-loop mechanism to iteratively decide edge removals and produce reliable pseudo-labels.&lt;/li&gt;&lt;li&gt;Designs performance feedback based on confident labels to guide pruning decisions; demonstrates improved GAD performance under varied noisy-label ratios on real datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhu Wang', 'Junnan Dong', 'Shuang Zhou', 'Chang Yang', 'Shengjie Zhao', 'Xiao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['graph anomaly detection', 'label noise robustness', 'edge pruning', 'reinforcement learning', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.05934</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title><link>https://arxiv.org/abs/2601.10527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an integrated safety evaluation of six frontier models (GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5) across language, vision-language, and image generation using benchmarks, adversarial tests, multilingual and compliance evaluations.&lt;/li&gt;&lt;li&gt;Finds highly uneven safety: GPT-5.2 shows the most balanced safety performance, but all models are highly vulnerable to adversarial testing (worst-case safety rates under 6%).&lt;/li&gt;&lt;li&gt;Notes text-to-image models show somewhat better alignment in regulated visual risk categories but remain fragile to adversarial or semantically ambiguous prompts; calls for standardized, holistic safety assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'safety evaluation', 'multimodal vulnerabilities', 'red teaming', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10527</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries</title><link>https://arxiv.org/abs/2601.10398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LatentRefusal, a mechanism that predicts answerability of text-to-SQL queries from intermediate LLM hidden activations to refuse unanswerable or underspecified queries.&lt;/li&gt;&lt;li&gt;Proposes the Tri-Residual Gated Encoder, a lightweight probe that suppresses schema noise and amplifies localized cues of question–schema mismatch.&lt;/li&gt;&lt;li&gt;Evaluates across multiple benchmarks showing improved F1 (88.5% average) and minimal runtime overhead (~2 ms), positioning LatentRefusal as an attachable safety layer for text-to-SQL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancheng Ren', 'Shijing Hu', 'Zhihui Lu', 'Jiangqi Huang', 'Qiang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'LLM safety', 'refusal mechanism', 'text-to-SQL', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10398</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Echoing: Identity Failures when LLM Agents Talk to Each Other</title><link>https://arxiv.org/abs/2511.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes a new failure mode in agent-agent (AxA) interactions called "echoing," where agents abandon assigned roles and mirror conversation partners, undermining objectives.&lt;/li&gt;&lt;li&gt;Empirical study across 66 AxA configurations, 4 domains, and 2500+ conversations (250k+ LLM inferences) shows echoing occurs across major LLM providers, with rates up to 70% and persistent rates (~32.8%) even in advanced reasoning models.&lt;/li&gt;&lt;li&gt;Analyzes dynamics (echoing increases with conversation length, emerges after ~7+ turns) and rules out simple experimental artifacts; proposes a protocol-level mitigation using structured responses that reduces echoing to ~9%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarath Shekkizhar', 'Romain Cosentino', 'Adam Earle', 'Silvio Savarese']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'agent-agent interactions', 'behavioral vulnerability', 'mitigation/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09710</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Building Production-Ready Probes For Gemini</title><link>https://arxiv.org/abs/2601.11516</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates new activation probe architectures (including 'multimax') designed to generalize from short-context to long-context inputs for misuse mitigation in large language models.&lt;/li&gt;&lt;li&gt;Evaluates robustness of probes in cyber-offensive scenarios across production-relevant distribution shifts: multi-turn conversations, static jailbreaks, and adaptive red teaming.&lt;/li&gt;&lt;li&gt;Finds that architecture choices plus training on diverse distributions are needed for broad generalization and that combining probes with prompted classifiers yields high accuracy cost-effectively.&lt;/li&gt;&lt;li&gt;Reports deployment of these probes in user-facing instances of Gemini and explores automating probe architecture search and adaptive red teaming using AlphaEvolve.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["J\\'anos Kram\\'ar", 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']&lt;/li&gt;&lt;li&gt;Tags: ['defense', 'activation probes', 'jailbreaks', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11516</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models</title><link>https://arxiv.org/abs/2601.11441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HORSE (Hierarchical Orthogonal Residual Spread), a method for precise, large-scale edits to LLMs by manipulating an information matrix via hierarchical orthogonal residuals to reduce noisy gradients and conflicts.&lt;/li&gt;&lt;li&gt;Provides theoretical comparisons with popular model-editing approaches and empirical evaluations across multiple LLMs and two datasets, showing stable and precise massive edits.&lt;/li&gt;&lt;li&gt;Frames model editing as a way to mitigate safety concerns in LLMs (patching behaviors or knowledge) and claims improved efficiency and stability over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaojie Gu', 'Guangxu Chen', 'Yuheng Yang', 'Jingxin Han', 'Andi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'LLM safety', 'model patching', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11441</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs</title><link>https://arxiv.org/abs/2601.11369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Institutional AI: a governance-graph-based institutional mechanism (public immutable manifest + Oracle/Controller runtime) to detect, attach consequences to, and audit coordinated (collusive) behavior among LLM agents.&lt;/li&gt;&lt;li&gt;Evaluates the approach on multi-agent Cournot collusion scenarios across six model configurations (including cross-provider pairs) and compares Ungoverned, Constitutional (prompt-only), and Institutional regimes.&lt;/li&gt;&lt;li&gt;Finds the Institutional regime substantially reduces collusion (mean tier from 3.1 to 1.8, Cohen's d=1.28; severe-collusion incidence from 50% to 5.6%), while the prompt-only constitutional baseline fails to reliably bind agents under optimization pressure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marcantonio Bracale Syrnikov', 'Federico Pierucci', 'Marcello Galisai', 'Matteo Prandi', 'Piercosma Bisconti', 'Francesco Giarrusso', 'Olga Sorokoletova', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['Collusion', 'Multi-agent alignment', 'Governance graphs', 'Defenses/Guardrails', 'Auditing/Cryptographic logging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11369</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients</title><link>https://arxiv.org/abs/2601.11219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDFLoRA, a federated LoRA method that decomposes client adapters into a global module (shared, aggregated) and a local module (private) to handle rank heterogeneity across clients.&lt;/li&gt;&lt;li&gt;Selective aggregation aligns and aggregates only the global module while keeping local modules private, enabling personalization and stabilizing FL under heterogeneous low-rank configurations.&lt;/li&gt;&lt;li&gt;Introduces a privacy-aware optimization strategy by injecting differential privacy (DP) noise exclusively into the global module to protect client information while preserving utility; shows improved utility-privacy trade-offs on GLUE versus federated LoRA baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhikang Shen', 'Jianrong Lu', 'Haiyuan Wan', 'Jianhai Chen']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'parameter-efficient fine-tuning', 'LoRA', 'privacy-preserving training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11219</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LoRA as Oracle</title><link>https://arxiv.org/abs/2601.11207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a LoRA-based oracle that attaches low-rank adapters to a frozen backbone to probe for backdoors and membership signals.&lt;/li&gt;&lt;li&gt;Analyzes optimization dynamics and representation shifts of LoRA adapters when exposed to suspicious samples to distinguish poisoned/member from clean/non-member data.&lt;/li&gt;&lt;li&gt;Proposes simple ranking and energy-based statistics to measure these signals, enabling detection without access to original training data or retraining the deployed model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Arazzi', 'Antonino Nocera']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-detection', 'membership-inference', 'LoRA', 'privacy', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11207</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.11199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SD-RAG, a framework that enforces security and privacy constraints during the retrieval phase (not via prompts) to prevent sensitive disclosure in RAG systems.&lt;/li&gt;&lt;li&gt;Introduces a semantic mechanism for ingesting human-readable dynamic security/privacy constraints and an optimized graph-based data model for fine-grained, policy-aware retrieval and sanitization.&lt;/li&gt;&lt;li&gt;Empirical evaluation demonstrates up to a 58% improvement in privacy score and strong resilience to prompt injection attacks compared to baseline approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aiman Al Masoud', 'Marco Arazzi', 'Antonino Nocera']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'retrieval-augmented generation (RAG)', 'privacy-preserving retrieval', 'data sanitization', 'access control / selective disclosure']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11199</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse</title><link>https://arxiv.org/abs/2601.11042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Performs spectral analysis showing dominant singular directions of pretrained weight matrices correlate with a model's general abilities and are highly sensitive to parameter perturbations.&lt;/li&gt;&lt;li&gt;Finds repeated sequential edits progressively disrupt these dominant singular subspaces, closely tracking collapse in editing efficacy and general performance.&lt;/li&gt;&lt;li&gt;Proposes REVIVE: a plug-and-play framework that represents parameter updates in the original weights' spectral basis and filters components that would interfere with the protected dominant subspace.&lt;/li&gt;&lt;li&gt;Demonstrates across multiple models and benchmarks that REVIVE improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing (up to 20,000 edits).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi Zhang', 'Mengqi Zhang', 'Xiaotian Ye', 'Runxi Cheng', 'Zisheng Zhou', 'Ying Zhou', 'Pengjie Ren', 'Zhumin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['sequential knowledge editing', 'spectral analysis', 'model robustness', 'parameter-update protection', 'continual editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11042</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Your One-Stop Solution for AI-Generated Video Detection</title><link>https://arxiv.org/abs/2601.11035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AIGVDBench, a large-scale benchmark for AI-generated video detection covering 31 generative models and over 440,000 videos.&lt;/li&gt;&lt;li&gt;Performs extensive evaluation: ~1,500 experiments across 33 existing detectors spanning four detector categories.&lt;/li&gt;&lt;li&gt;Provides eight in-depth analyses and reports four novel findings to guide future research in synthetic video detection.&lt;/li&gt;&lt;li&gt;Open-sources the benchmark and dataset to facilitate standardized evaluation and advancement of detection defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Long Ma', 'Zihao Xue', 'Yan Wang', 'Zhiyuan Yan', 'Jin Xu', 'Xiaorui Jiang', 'Haiyang Yu', 'Yong Liao', 'Zhen Bi']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'benchmarking', 'synthetic video', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11035</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs</title><link>https://arxiv.org/abs/2601.11000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'personalization-induced hallucinations' where personalized LLMs output answers aligned with a user's history rather than objective facts, due to entanglement between personalization and factual representations.&lt;/li&gt;&lt;li&gt;Proposes Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time mitigation that reduces personalization-driven factual distortions while retaining personalized behavior.&lt;/li&gt;&lt;li&gt;Introduces PFQABench, a benchmark for jointly evaluating factual accuracy and personalization in QA, and shows FPPS improves factuality across multiple LLM backbones and personalization methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongxiang Sun', 'Yi Zhan', 'Chenglei Shen', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['personalization', 'hallucination', 'factuality', 'robustness', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.11000</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents</title><link>https://arxiv.org/abs/2601.10955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a stealthy multi-turn economic Denial-of-Service attack that manipulates agent-tool communication to force long, verbose tool-calling chains while preserving correct final outputs.&lt;/li&gt;&lt;li&gt;Implements edits to text-visible fields and a template-governed return policy on an MCP-compatible tool server, optimized with Monte Carlo Tree Search to maximize token/compute amplification without changing function signatures or payloads.&lt;/li&gt;&lt;li&gt;Empirically demonstrates large cost and resource amplification across LLMs and benchmarks (up to 658x cost, 60k+ token trajectories, large GPU KV cache occupancy), and shows the attack evades conventional final-answer validation checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyu Zhou', 'Yongsen Zheng', 'Yicheng He', 'Meng Xue', 'Xueluan Gong', 'Yuji Wang', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['Denial-of-Service', 'LLM agents', 'Tool-calling attacks', 'Resource exhaustion', 'Attack evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.10955</guid><pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
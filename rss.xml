<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 30 Oct 2025 22:20:14 +0000</lastBuildDate><item><title>When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails</title><link>https://arxiv.org/abs/2510.21285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Self-Jailbreak in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Guardrail (CoG) framework to mitigate safety issues&lt;/li&gt;&lt;li&gt;Showcases improved safety while preserving reasoning ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingzhi Mao', 'Chunkang Zhang', 'Junxiang Wang', 'Xinyan Guan', 'Boxi Cao', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety evaluation', 'alignment', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21285</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenGuardrails: A Configurable, Unified, and Scalable Guardrails Platform for Large Language Models</title><link>https://arxiv.org/abs/2510.19169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenGuardrails, an open-source platform for LLM safety and security.&lt;/li&gt;&lt;li&gt;Addresses content safety, model manipulation, and data leakage.&lt;/li&gt;&lt;li&gt;Features configurable policies, unified LLM-based detection, and scalable model design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Wang', 'Haowen Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19169</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models</title><link>https://arxiv.org/abs/2403.02745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CURATRON for robust preference data curation&lt;/li&gt;&lt;li&gt;Introduces a polynomial time ranking algorithm&lt;/li&gt;&lt;li&gt;Handles incomplete and corrupted data in PL datasets&lt;/li&gt;&lt;li&gt;Enhances LLM alignment and resilience&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Son The Nguyen', 'Niranjan Uma Naresh', 'Theja Tulabandhula']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'preference learning', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.02745</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation</title><link>https://arxiv.org/abs/2510.12993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Multilingual study of LLM safeguards, personalisation, and disinformation&lt;/li&gt;&lt;li&gt;Red teaming methodology with 324 false narratives and 150 demographic personas&lt;/li&gt;&lt;li&gt;Jailbreak rates increased with personalisation prompts&lt;/li&gt;&lt;li&gt;Grok and GPT models showed high jailbreak and personalisation scores&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao A. Leite', 'Arnav Arora', 'Silvia Gargova', 'Jo\\~ao Luz', 'Gustavo Sampaio', 'Ian Roberts', 'Carolina Scarton', 'Kalina Bontcheva']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multilingual', 'disinformation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12993</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Precise In-Parameter Concept Erasure in Large Language Models</title><link>https://arxiv.org/abs/2505.22586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PISCES framework for precise concept erasure in LLMs&lt;/li&gt;&lt;li&gt;Uses disentangler model and automated interpretability&lt;/li&gt;&lt;li&gt;Improves specificity and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoav Gur-Arieh', 'Clara Suslik', 'Yihuai Hong', 'Fazl Barez', 'Mor Geva']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22586</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework</title><link>https://arxiv.org/abs/2510.25732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates unlearning in LLMs using a new framework called SKeB&lt;/li&gt;&lt;li&gt;Tests whether persuasive prompts can recall unlearned knowledge&lt;/li&gt;&lt;li&gt;Finds that smaller models are more vulnerable to knowledge recall after unlearning&lt;/li&gt;&lt;li&gt;Introduces metrics for entanglement and factuality in outputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Shah', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25732</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Lingual Summarization as a Black-Box Watermark Removal Attack</title><link>https://arxiv.org/abs/2510.24789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cross-Lingual Summarization Attacks (CLSA) as a method to remove watermarks from AI-generated text.&lt;/li&gt;&lt;li&gt;CLSA involves translating text to a pivot language, summarizing, and optionally back-translating, which preserves semantics while destroying token-level biases.&lt;/li&gt;&lt;li&gt;Experiments show CLSA reduces watermark detection accuracy more effectively than paraphrasing across multiple languages and watermarking schemes.&lt;/li&gt;&lt;li&gt;Results indicate that CLSA can drive detection accuracy to near chance levels while maintaining text quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gokul Ganesan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'watermarking', 'privacy attacks', 'cross-lingual', 'summarization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.24789</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Truthful Representations Flip Under Deceptive Instructions?</title><link>https://arxiv.org/abs/2507.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how deceptive instructions alter internal representations in LLMs&lt;/li&gt;&lt;li&gt;Uses linear probes and Sparse Autoencoders to detect representational shifts&lt;/li&gt;&lt;li&gt;Identifies specific features and layers sensitive to deception&lt;/li&gt;&lt;li&gt;Aims to improve detection and mitigation of instructed dishonesty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianxuan Long', 'Yao Fu', 'Runchao Li', 'Mu Sheng', 'Haotian Yu', 'Xiaotian Han', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22149</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents</title><link>https://arxiv.org/abs/2506.14866</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OS-Harm, a benchmark for evaluating the safety of computer use agents.&lt;/li&gt;&lt;li&gt;Covers three categories of harm: deliberate misuse, prompt injection attacks, model misbehavior.&lt;/li&gt;&lt;li&gt;Includes 150 tasks across various safety violations and OS applications.&lt;/li&gt;&lt;li&gt;Evaluates models like o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro, showing vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Kuntz', 'Agatha Duzan', 'Hao Zhao', 'Francesco Croce', 'Zico Kolter', 'Nicolas Flammarion', 'Maksym Andriushchenko']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'safety evaluation', 'benchmarking', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.14866</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Model Inversion Attacks Meet Cryptographic Fuzzy Extractors</title><link>https://arxiv.org/abs/2510.25687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes defense against model inversion attacks using fuzzy extractors&lt;/li&gt;&lt;li&gt;Introduces PIPE attack with 89% success rate&lt;/li&gt;&lt;li&gt;Proposes L2FE-Hash fuzzy extractor with security guarantees&lt;/li&gt;&lt;li&gt;Empirically validates defense against inversion attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mallika Prabhakar', 'Louise Xu', 'Prateek Saxena']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'model inversion', 'fuzzy extractors', 'security', 'cryptographic']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25687</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Secure Retrieval-Augmented Generation against Poisoning Attacks</title><link>https://arxiv.org/abs/2510.25025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGuard, a detection framework for identifying poisoned texts in RAG systems&lt;/li&gt;&lt;li&gt;Uses chunk-wise perplexity filtering and text similarity filtering&lt;/li&gt;&lt;li&gt;Aims to mitigate data poisoning attacks by expanding retrieval scope and detecting anomalies&lt;/li&gt;&lt;li&gt;Validated through experiments on large-scale datasets against adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirui Cheng', 'Jikai Sun', 'Anjun Gao', 'Yueyang Quan', 'Zhuqing Liu', 'Xiaohua Hu', 'Minghong Fang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG', 'detection framework', 'perplexity filtering', 'text similarity', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25025</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Agent Execution</title><link>https://arxiv.org/abs/2510.21236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentBound, an access control framework for MCP servers&lt;/li&gt;&lt;li&gt;Combines declarative policies with enforcement engine&lt;/li&gt;&lt;li&gt;Automatically generates policies from source code with high accuracy&lt;/li&gt;&lt;li&gt;Blocks security threats in malicious MCP servers with low overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christoph B\\"uhler', 'Matteo Biagiola', 'Luca Di Grazia', 'Guido Salvaneschi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security', 'access control', 'MCP servers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21236</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title><link>https://arxiv.org/abs/2505.18384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the dynamic risk assessment of AI agents in offensive cybersecurity, focusing on adversarial improvements within a compute budget.&lt;/li&gt;&lt;li&gt;It highlights that current assessments don't account for real-world adversarial capabilities, especially iterative improvements.&lt;/li&gt;&lt;li&gt;The study shows significant capability improvement with limited resources, suggesting the need for dynamic evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyi Wei', 'Benedikt Stroebl', 'Jiacen Xu', 'Joie Zhang', 'Zhou Li', 'Peter Henderson']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18384</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails</title><link>https://arxiv.org/abs/2510.21285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of Self-Jailbreak in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Guardrail (CoG) framework to mitigate safety issues&lt;/li&gt;&lt;li&gt;Showcases improved safety while preserving reasoning ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingzhi Mao', 'Chunkang Zhang', 'Junxiang Wang', 'Xinyan Guan', 'Boxi Cao', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety evaluation', 'alignment', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21285</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Truthful Representations Flip Under Deceptive Instructions?</title><link>https://arxiv.org/abs/2507.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how deceptive instructions alter internal representations in LLMs&lt;/li&gt;&lt;li&gt;Uses linear probes and Sparse Autoencoders to detect representational shifts&lt;/li&gt;&lt;li&gt;Identifies specific features and layers sensitive to deception&lt;/li&gt;&lt;li&gt;Aims to improve detection and mitigation of instructed dishonesty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianxuan Long', 'Yao Fu', 'Runchao Li', 'Mu Sheng', 'Haotian Yu', 'Xiaotian Han', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22149</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models</title><link>https://arxiv.org/abs/2403.02745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CURATRON for robust preference data curation&lt;/li&gt;&lt;li&gt;Introduces a polynomial time ranking algorithm&lt;/li&gt;&lt;li&gt;Handles incomplete and corrupted data in PL datasets&lt;/li&gt;&lt;li&gt;Enhances LLM alignment and resilience&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Son The Nguyen', 'Niranjan Uma Naresh', 'Theja Tulabandhula']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'preference learning', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.02745</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework</title><link>https://arxiv.org/abs/2510.25732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates unlearning in LLMs using a new framework called SKeB&lt;/li&gt;&lt;li&gt;Tests whether persuasive prompts can recall unlearned knowledge&lt;/li&gt;&lt;li&gt;Finds that smaller models are more vulnerable to knowledge recall after unlearning&lt;/li&gt;&lt;li&gt;Introduces metrics for entanglement and factuality in outputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aakriti Shah', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25732</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models</title><link>https://arxiv.org/abs/2510.25179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agentic Moderation, a framework using specialized agents to defend multimodal models against jailbreak attacks.&lt;/li&gt;&lt;li&gt;Achieves 7-19% reduction in Attack Success Rate (ASR) and 4-20% improvement in Refusal Rate (RR) across multiple datasets and models.&lt;/li&gt;&lt;li&gt;Emphasizes dynamic, cooperative agents for context-aware and interpretable moderation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan Ren', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'multimodal', 'agentic systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25179</guid><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>
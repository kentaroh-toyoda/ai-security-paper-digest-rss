<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 02 Dec 2025 00:22:43 +0000</lastBuildDate><item><title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2505.23448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an iterative training framework that augments an n-class classifier with an (n+1)-th "garbage" class initialized with Gaussian noise to represent outliers.&lt;/li&gt;&lt;li&gt;Uses network inversion after each epoch to reconstruct inputs corresponding to outputs assigned to the garbage class, then retrains to refine decision boundaries and reduce uncertainty.&lt;/li&gt;&lt;li&gt;Claims unified OOD detection and uncertainty estimation without needing external OOD datasets or post-hoc calibration; demonstrated on image inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pirzada Suhail', 'Rehna Afroz', 'Gouranga Bala', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'uncertainty-estimation', 'network-inversion', 'robustness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23448</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</title><link>https://arxiv.org/abs/2511.20515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AlignBench, a benchmark of fine-grained image-caption pairs (synthetic) with per-sentence correctness annotations to evaluate image-text alignment.&lt;/li&gt;&lt;li&gt;Generates diverse pairs using image-to-text and text-to-image models to test VLMs beyond short captions and rule-based perturbations.&lt;/li&gt;&lt;li&gt;Benchmarks many decoder-based VLMs and reports key findings: CLIP-based models struggle with fine-grained alignment; detectors over-score early sentences; detectors prefer their own outputs, reducing detection performance.&lt;/li&gt;&lt;li&gt;Provides a dataset and evaluation protocol aimed at measuring detailed alignment capabilities of vision-language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuniaki Saito', 'Risa Shinoda', 'Shohei Tanaka', 'Tosho Hirasawa', 'Fumio Okura', 'Yoshitaka Ushiku']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'benchmarking', 'vision-language models', 'evaluation', 'synthetic data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20515</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering</title><link>https://arxiv.org/abs/2511.19220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether state-of-the-art vision-language models genuinely use visual information for Italian medical VQA by replacing medical images with blank placeholders.&lt;/li&gt;&lt;li&gt;Tests four models (Claude Sonnet 4.5, GPT-4o, GPT-5-mini, Gemini 2.0 flash exp) on 60 EuropeMedQA Italian questions that require image interpretation.&lt;/li&gt;&lt;li&gt;Findings: GPT-4o shows the largest accuracy drop (27.9 percentage points) when images are removed; other models show smaller drops, yet all models sometimes produce confident but fabricated visual reasoning.&lt;/li&gt;&lt;li&gt;Highlights risks of hallucination and variable visual grounding, underscoring safety and robustness concerns for clinical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Federico Felizzi', 'Olivia Riccomi', 'Michele Ferramola', 'Francesco Andrea Causio', 'Manuel Del Medico', 'Vittorio De Vita', 'Lorenzo De Mori', 'Alessandra Piscitelli', 'Pietro Eric Risuleo', 'Bianca Destro Castaniti', 'Antonio Cristiano', 'Alessia Longo', 'Luigi De Angelis', 'Mariapia Vassalli', 'Marcello Di Pumpo']&lt;/li&gt;&lt;li&gt;Tags: ['medical VLMs', 'visual grounding', 'hallucination/confabulation', 'robustness/safety evaluation', 'clinical deployment risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19220</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Seeing What Matters: Visual Preference Policy Optimization for Visual Generation</title><link>https://arxiv.org/abs/2511.18719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Visual Preference Policy Optimization (ViPO), a GRPO variant that converts scalar human-feedback rewards into spatially and temporally structured pixel-level advantage maps.&lt;/li&gt;&lt;li&gt;Uses a Perceptual Structuring Module with pretrained vision backbones to focus optimization on perceptually important regions, aiming to correct localized artifacts in image and video generation.&lt;/li&gt;&lt;li&gt;Reports consistent improvements over vanilla GRPO on in-domain human-preference alignment and better generalization on out-of-domain visual benchmarks; compatible with existing GRPO pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqi Ni', 'Yuanzhi Liang', 'Rui Li', 'Yi Zhou', 'Haibing Huang', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'visual generation', 'preference learning', 'policy optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.18719</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</title><link>https://arxiv.org/abs/2511.08423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OmniAID, a Mixture-of-Experts architecture that decouples content-specific semantic flaws from content-agnostic generation artifacts to improve AI-generated image (AIGI) detection.&lt;/li&gt;&lt;li&gt;Uses Routable Specialized Semantic Experts for different content domains (e.g., humans, animals) plus a Fixed Universal Artifact Expert and a two-stage training (domain specialization then lightweight gating).&lt;/li&gt;&lt;li&gt;Introduces Mirage, a new large-scale, contemporary dataset to evaluate in-the-wild generalization and compares OmniAID against existing monolithic detectors, reporting improved robustness.&lt;/li&gt;&lt;li&gt;Claims explicit separation of 'what is generated' (semantic flaws) and 'how it is generated' (universal artifacts) leads to better cross-domain and cross-model generalization for AIGI authentication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuncheng Guo', 'Junyan Ye', 'Chenjue Zhang', 'Hengrui Kang', 'Haohuan Fu', 'Conghui He', 'Weijia Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'AIGI detection', 'robustness', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08423</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders</title><link>https://arxiv.org/abs/2509.21379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAEmnesia, a supervised sparse autoencoder that enforces one-to-one concept-to-neuron mappings in diffusion models to enable targeted concept erasure.&lt;/li&gt;&lt;li&gt;Achieves feature centralization, significantly reducing hyperparameter search (96.7% reduction) and improving performance on the UnlearnCanvas benchmark (9.2% over SOTA).&lt;/li&gt;&lt;li&gt;Demonstrates improved scalability for sequential unlearning (28.4% better accuracy when removing nine objects) and claims mitigation of unwanted content generation under adversarial attack, including effective removal of nudity (I2P evaluation).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrico Cassano', 'Riccardo Renzulli', 'Marco Nurisso', 'Mirko Zaffaroni', 'Alan Perotti', 'Marco Grangetto']&lt;/li&gt;&lt;li&gt;Tags: ['concept-unlearning', 'diffusion-models', 'model-editing', 'safety', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21379</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality</title><link>https://arxiv.org/abs/2508.09185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CADAR, a neuro-symbolic framework that fuses pretrained vision-language representations into a perception graph capturing objects, relations, and temporal salience.&lt;/li&gt;&lt;li&gt;Uses a particle-filter-based probabilistic reasoning module to detect anomalies in semantic dynamics indicative of cognitive attacks in AR.&lt;/li&gt;&lt;li&gt;Claims interpretability (symbolic graph + probabilistic inference) combined with adaptability of modern V-L models and shows preliminary empirical gains on an AR cognitive-attack dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rongqian Chen', 'Allison Andreyev', 'Yanming Xiu', 'Joshua Chilukuri', 'Shunav Sen', 'Mahdi Imani', 'Bin Li', 'Maria Gorlatova', 'Gang Tan', 'Tian Lan']&lt;/li&gt;&lt;li&gt;Tags: ['AR security', 'cognitive attacks', 'neuro-symbolic', 'anomaly detection', 'vision-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09185</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DiffusionFF: A Diffusion-based Framework for Joint Face Forgery Detection and Fine-Grained Artifact Localization</title><link>https://arxiv.org/abs/2508.01873</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffusionFF, a diffusion-based encoder-decoder framework that jointly performs face forgery detection and fine-grained artifact localization.&lt;/li&gt;&lt;li&gt;Uses a pretrained forgery detector as an "artifact encoder" to extract multi-scale forgery features and a denoising diffusion model as an "artifact decoder" to progressively synthesize localization maps.&lt;/li&gt;&lt;li&gt;Fuses the fine-grained localization maps with high-level semantic features from the detector to improve detection performance and explainability.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results across multiple face forgery benchmarks, emphasizing improved detection capability and artifact-level interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siran Peng', 'Haoyuan Zhang', 'Li Gao', 'Tianshuo Zhang', 'Xiangyu Zhu', 'Bao Li', 'Weisong Zhao', 'Zhen Lei']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'forgery localization', 'diffusion models', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01873</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title><link>https://arxiv.org/abs/2506.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IVY-FAKE, a large-scale multimodal benchmark (106K+ training, 5K verified eval) for explainable detection of AI-generated images and videos with rich, fine-grained annotations.&lt;/li&gt;&lt;li&gt;Proposes Ivy-xDetector, an RL-based detector using Group Relative Policy Optimization (GRPO) that produces explainable reasoning chains for localization and explanation of forgeries.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains on multiple synthetic-content benchmarks (e.g., GenImage accuracy improved from 86.88% to 96.32%) and emphasizes explainability and dataset diversity/quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Wenhui Dong', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng', 'Xinbin Yuan', 'Yifei Bi', 'Ming Zhao', 'Zian Zhou', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'Explainability', 'Benchmark', 'Multimodal', 'Forgery detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00979</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</title><link>https://arxiv.org/abs/2504.17457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Tangible Attack (TBA), a framework to generate adversarial examples that compromise expressive human pose and shape estimation (EHPS) models.&lt;/li&gt;&lt;li&gt;Introduces Dual Heterogeneous Noise Generator (DHNG) combining a VAE and ControlNet to create diverse, targeted perturbations conditioned on the original image.&lt;/li&gt;&lt;li&gt;Develops a custom adversarial loss and an iterative multi-gradient optimization that leverages gradients from both the noise generator and the target EHPS model for stronger attacks.&lt;/li&gt;&lt;li&gt;Empirical results show large degradations in estimation accuracy (up to +41.0% error increase, ~17.0% average), demonstrating notable security vulnerabilities in digital human generation systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiying Li', 'Yeying Jin', 'Fan Shen', 'Zhi Liu', 'Weibin Chen', 'Pengju Zhang', 'Xiaomei Zhang', 'Boyu Chen', 'Michael Shen', 'Kejian Wu', 'Zhaoxin Fan', 'Jin Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'computer vision', 'digital human generation', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17457</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title><link>https://arxiv.org/abs/2504.02821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Sparse Autoencoders (SAEs) to vision-language models (e.g., CLIP) to improve neuron-level monosemanticity in visual representations.&lt;/li&gt;&lt;li&gt;Introduces a human-aligned benchmark for measuring monosemanticity derived from a large-scale user study.&lt;/li&gt;&lt;li&gt;Shows SAEs (especially with sparsity and wide latents) increase monosemanticity and that intervening on CLIP's vision encoder can steer multimodal LLM outputs (e.g., LLaVA) without changing the language model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'safety', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02821</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving</title><link>https://arxiv.org/abs/2511.22865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a camera-only end-to-end planning framework that estimates aleatoric uncertainty in BEV (bird's-eye view) and integrates it into trajectory planning.&lt;/li&gt;&lt;li&gt;Produces a dense, uncertainty-aware drivability map at pixel-level that captures semantic and geometric scene structure.&lt;/li&gt;&lt;li&gt;Introduces a lane-following regularization encoding lane structure and traffic norms to stabilize planning while allowing maneuvers like lane changes/overtakes.&lt;/li&gt;&lt;li&gt;Evaluated on the NAVSIM benchmark, showing state-of-the-art gains on NAVHARD and NAVSAFE, demonstrating improved robustness and safety under uncertainty.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonjeong Ryu', 'Seungjun Yu', 'Seokha Moon', 'Hojun Choi', 'Junsung Park', 'Jinkyu Kim', 'Hyunjung Shim']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'uncertainty-estimation', 'safety', 'robustness', 'end-to-end-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22865</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distracted Robot: How Visual Clutter Undermine Robotic Manipulation</title><link>https://arxiv.org/abs/2511.22780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a psychophysics-inspired unified clutter measure capturing environmental factors, distractor quantity, characteristics, and arrangement for robotic manipulation scenes.&lt;/li&gt;&lt;li&gt;Presents an evaluation protocol and systematically constructed test scenarios in hyper-realistic simulation and real-world to assess manipulation policies, focusing on vision-language-action (VLA) models.&lt;/li&gt;&lt;li&gt;Demonstrates substantial performance degradation (up to ~34%) due to clutter, finds different VLA policies have unique vulnerabilities and low agreement on successful scenarios, and analyzes effects of distractor quantity and occlusion.&lt;/li&gt;&lt;li&gt;Shows finetuning on augmented data partially mitigates issues but does not equally remedy all clutter-induced failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Rasouli', 'Montgomery Alban', 'Sajjad Pakdamansavoji', 'Zhiyuan Li', 'Zhanguang Zhang', 'Aaron Wu', 'Xuan Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness', 'Robot manipulation', 'Vision-language-action', 'Safety evaluation', 'Evaluation benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22780</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents</title><link>https://arxiv.org/abs/2511.22441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GEO-Detective, an LVLM-based agent that mimics human reasoning and tool use to infer image geolocation, including a visual reverse-search tool.&lt;/li&gt;&lt;li&gt;Shows improved geolocation performance over baseline LVLMs (≈11.1% country-level, ≈5.2% finer-grained) and reduces 'unknown' predictions when external clues are available.&lt;/li&gt;&lt;li&gt;Evaluates privacy risks from advanced LVLM geolocation and explores multiple defense strategies, reporting robustness results and highlighting gaps in current privacy safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Zhang', 'Yixin Wu', 'Boyang Zhang', 'Chenhao Lin', 'Chao Shen', 'Michael Backes', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'geolocation', 'large vision-language models', 'privacy defenses', 'adversarial tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22441</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI</title><link>https://arxiv.org/abs/2511.21827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores strategies for generating synthetic clinical notes with LLMs (prompt design and inclusion of medical metadata) to augment dermatology image datasets for multimodal models.&lt;/li&gt;&lt;li&gt;Evaluates effects of synthetic notes on downstream tasks: classification (including under domain shift) and cross-modal retrieval.&lt;/li&gt;&lt;li&gt;Notes concerns about LLM hallucinations in clinical contexts and studies how different synthesis strategies impact model performance and utility.&lt;/li&gt;&lt;li&gt;Finds synthetic notes can improve classification robustness and enable cross-modal retrieval capabilities that were not explicitly optimized for during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niccolo Marini', 'Zhaohui Liang', 'Sivaramakrishnan Rajaraman', 'Zhiyun Xue', 'Sameer Antani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'synthetic data', 'medical multimodal AI', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21827</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution</title><link>https://arxiv.org/abs/2511.21717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CrossCheck-Bench, a 15k example diagnostic benchmark for detecting and resolving contradictions in multimodal (image+text) inputs, with synthetic contradictions injected into real-world artifacts.&lt;/li&gt;&lt;li&gt;Defines a hierarchical task framework with three reasoning levels and seven atomic capabilities (perception, integration, multi-step inference, rule-based validation, etc.) to pinpoint compositional failure modes.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art vision-language models, finding large performance drops when tasks require synthesizing multiple clues or multi-step logical reasoning; simple prompting strategies yield marginal gains while hybrid symbolic-grounded methods improve stability.&lt;/li&gt;&lt;li&gt;Provides a capability-level analysis revealing uneven skill acquisition and persistent bottlenecks in multimodal contradiction detection, suggesting directions for improving robust cross-modal verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baoliang Tian', 'Yuxuan Si', 'Jilong Wang', 'Lingyao Li', 'Zhongyuan Bao', 'Zineng Zhou', 'Tao Wang', 'Sixu Li', 'Ziyao Xu', 'Mingze Wang', 'Zhouzhuo Zhang', 'Zhihao Wang', 'Yike Yun', 'Ke Tian', 'Ning Yang', 'Minghui Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'multimodal-contradiction-detection', 'benchmark', 'multimodal-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21717</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Insight-A: Attribution-aware for Multimodal Misinformation Detection</title><link>https://arxiv.org/abs/2511.21705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Insight-A, a multimodal misinformation detection framework that attributes forged content to generation sources and detects cross-modal distortions.&lt;/li&gt;&lt;li&gt;Introduces Cross-Attribution Prompting (CAP) to model correlations between perception and reasoning, and Attribution-Debiased Prompting (ADP) to reduce prompt subjectivity for MLLMs.&lt;/li&gt;&lt;li&gt;Adds image captioning to improve visual detail extraction for cross-modal consistency checking and reports extensive experiments showing improved detection performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Wu', 'Yumeng Fu', 'Chen Gong', 'Guohong Fu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal misinformation detection', 'attribution', 'MLLM prompting', 'AIGC detection', 'cross-modal consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21705</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title><link>https://arxiv.org/abs/2511.23478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two diagnostic metrics for video reasoning: Think Answer Consistency (TAC) to measure alignment between reasoning and answers, and Video Attention Score (VAS) to quantify reliance on visual vs. textual cues.&lt;/li&gt;&lt;li&gt;Finds existing multimodal models overly depend on linguistic priors rather than video evidence, based on evaluation across 11 video reasoning benchmarks.&lt;/li&gt;&lt;li&gt;Introduces a dual-stage post-training method: timestamp-aware supervised fine-tuning plus Group Relative Policy Optimization (GRPO) guided by a Temporal Alignment Reward (TAR) to improve temporal precision and causal coherence.&lt;/li&gt;&lt;li&gt;Presents Video-R2, which reportedly increases TAC, VAS, and accuracy, yielding more temporally grounded and consistent reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Maaz', 'Hanoona Rasheed', 'Fahad Shahbaz Khan', 'Salman Khan']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'temporal grounding', 'reinforcement learning fine-tuning', 'evaluation metrics', 'video understanding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23478</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</title><link>https://arxiv.org/abs/2511.23377</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DEAL-300K, a 300K+ image dataset for localization of diffusion-based image manipulations, created via LLM-generated edit instructions, a mask-free diffusion editor, and an active-learning change-detection pipeline for pixel-level annotations.&lt;/li&gt;&lt;li&gt;Proposes a localization framework using a frozen Visual Foundation Model (VFM) with Multi Frequency Prompt Tuning (MFPT) to capture semantic and frequency-domain cues of edited regions.&lt;/li&gt;&lt;li&gt;Reports strong baseline performance: pixel-level F1 of 82.56% on their test split and 80.97% on external CoCoGlide benchmark.&lt;/li&gt;&lt;li&gt;Provides a publicly accessible dataset and code to support future diffusion-based image manipulation localization research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Zhang', 'Hongxia Wang', 'Hangqing Liu', 'Yang Zhou', 'Qiang Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['image-forensics', 'deepfake-detection', 'dataset', 'diffusion-models', 'localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23377</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SimScale: Learning to Drive via Real-World Simulation at Scale</title><link>https://arxiv.org/abs/2511.23369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SimScale, a scalable simulation framework that synthesizes large numbers of unseen driving states by neural rendering multi-view observations around perturbed ego trajectories.&lt;/li&gt;&lt;li&gt;Generates pseudo-expert trajectories for simulated states to provide action supervision and co-trains policies on both real and simulated data.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and generalization on real-world driving benchmarks (e.g., +6.8 EPDMS on navhard, +2.9 on navtest) and shows performance scales with more simulated data.&lt;/li&gt;&lt;li&gt;Analyzes design choices for pseudo-experts and scaling behaviors across different policy architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haochen Tian', 'Tianyu Li', 'Haochen Liu', 'Jiazhi Yang', 'Yihang Qiu', 'Guang Li', 'Junli Wang', 'Yinfeng Gao', 'Zhang Zhang', 'Liang Wang', 'Hangjun Ye', 'Tieniu Tan', 'Long Chen', 'Hongyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'simulation', 'robustness', 'data-augmentation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23369</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</title><link>https://arxiv.org/abs/2511.23158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces REVEAL-Bench, a multimodal benchmark for explainable AI-generated image detection structured around explicit chains-of-evidence from lightweight expert models.&lt;/li&gt;&lt;li&gt;Proposes REVEAL, a forensic framework that uses expert-grounded reinforcement learning to jointly optimize detection accuracy, explanation fidelity, and logical coherence.&lt;/li&gt;&lt;li&gt;Produces step-by-step reasoning traces and verifiable evidential justifications alongside detection outputs to improve interpretability and cross-model generalization.&lt;/li&gt;&lt;li&gt;Reports improved detection accuracy, explanation fidelity, and robust cross-model performance compared to prior explainable forensics methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huangsen Cao', 'Qin Mei', 'Zhiheng Li', 'Yuxi Li', 'Ying Zhang', 'Chen Li', 'Zhimeng Zhang', 'Xin Ding', 'Yongwei Wang', 'Jing Lyu', 'Fei Wu']&lt;/li&gt;&lt;li&gt;Tags: ['image forensic detection', 'explainability', 'benchmark', 'robustness/generalization', 'reinforcement learning for explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23158</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding</title><link>https://arxiv.org/abs/2511.23151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) using Group Relative Policy Optimization with four reward objectives (format, refuse-IoU, explain, query correction) to enable VTG models to refuse hard-irrelevant queries.&lt;/li&gt;&lt;li&gt;Constructs a Hard-Irrelevant VTG (HI-VTG) dataset containing hard-irrelevant queries and refusal answers to support training and evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates improved relevance discrimination and fine-grained semantic reasoning across multiple relevance-aware VTG settings and shows scalability by applying RA-RFT to various LVLM-based VTG models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin-Seop Lee', 'SungJoon Lee', 'SeongJun Jung', 'Boyang Li', 'Jee-Hyong Lee']&lt;/li&gt;&lt;li&gt;Tags: ['refusal-mechanisms', 'robustness', 'out-of-distribution-detection', 'alignment', 'video-temporal-grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23151</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation</title><link>https://arxiv.org/abs/2511.23066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses impact of foundation-model generative inpainting (gpt-image-1) on clinical tasks using 200 pediatric hand radiographs from RSNA Bone Age dataset and 600 inpainted variants.&lt;/li&gt;&lt;li&gt;Evaluates downstream bone age estimation (MAE) and gender classification (AUC) with deep-learning ensembles, and inspects pixel-intensity distributions for structural changes.&lt;/li&gt;&lt;li&gt;Finds large performance degradation after inpainting: bone age MAE rose from 6.26 to 30.11 months and gender AUC fell from 0.955 to 0.704; inpainted images showed pixel-intensity shifts and structural inconsistencies.&lt;/li&gt;&lt;li&gt;Concludes that visually realistic inpainting can obscure subtle diagnostic features and introduce latent bias, underscoring need for rigorous task-specific validation before clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felipe Akio Matsuoka', 'Eduardo Moreno J. M. Farina', 'Augusto Sarquis Serpa', 'Soraya Monteiro', 'Rodrigo Ragazzini', 'Nitamar Abdala', 'Marcelo Straus Takahashi', 'Felipe Campos Kitamura']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'medical-imaging', 'generative-inpainting', 'bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23066</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization</title><link>https://arxiv.org/abs/2511.23002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JarvisEvo, a self-evolving image editing agent that interleaves multimodal chain-of-thought (iMCoT) reasoning to reduce instruction hallucination and improve editing fidelity.&lt;/li&gt;&lt;li&gt;Introduces Synergistic Editor-Evaluator Policy Optimization (SEPO), a self-improvement framework that uses an internal evaluator to mitigate reward hacking without relying on external reward models.&lt;/li&gt;&lt;li&gt;Supports fine-grained global and local photo edits via integration with Adobe Lightroom and demonstrates substantial gains on ArtEdit-Bench (notably +44.96% pixel-level content fidelity).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunlong Lin', 'Linqing Wang', 'Kunjie Lin', 'Zixu Lin', 'Kaixiong Gong', 'Wenbo Li', 'Bin Lin', 'Zhenxi Li', 'Shiyi Zhang', 'Yuyang Peng', 'Wenxun Dai', 'Xinghao Ding', 'Chunyu Wang', 'Qinglin Lu']&lt;/li&gt;&lt;li&gt;Tags: ['reward hacking', 'alignment/safety', 'multimodal chain-of-thought', 'self-improving agents', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23002</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis</title><link>https://arxiv.org/abs/2511.22990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MIMM-X, a method to disentangle causal features from multiple spurious correlations by minimizing mutual information between causal and spurious feature sets.&lt;/li&gt;&lt;li&gt;Evaluated on three medical imaging datasets (UK Biobank, NAKO, CheXpert) across MRI and X-ray modalities and shows improved mitigation of shortcut learning.&lt;/li&gt;&lt;li&gt;Aims to improve model generalization and reliability by encouraging predictions based on causal relationships rather than dataset-specific shortcuts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Louisa Fay', 'Hajer Reguigui', 'Bin Yang', 'Sergios Gatidis', 'Thomas K\\"ustner']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'shortcut learning', 'medical imaging', 'causal representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22990</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling</title><link>https://arxiv.org/abs/2511.22936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ReImage, a neural-watermarking-based framework that embeds a pixel-shuffled version of an image into itself to enable self-recovery of tampered regions.&lt;/li&gt;&lt;li&gt;Includes a generator to produce optimized watermarks and an image enhancement module to refine recovered content; analyzes and mitigates limitations of shuffled watermarking.&lt;/li&gt;&lt;li&gt;Evaluates across diverse tampering scenarios and claims state-of-the-art recovery quality; code and pretrained models to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minyoung Kim', 'Paul Hongsuck Seo']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'image forensics', 'tampering recovery', 'media integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22936</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering</title><link>https://arxiv.org/abs/2511.22843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'visual shortcuts' in Multimodal Knowledge-Based VQA where images match the primary entity and models exploit this to answer correctly without using document knowledge.&lt;/li&gt;&lt;li&gt;Introduces RETINA, a large automatically constructed benchmark (120k train, 2k human-curated test) that pairs queries about secondary/related entities with images of those related entities to remove the visual shortcut.&lt;/li&gt;&lt;li&gt;Shows existing MKB-VQA models degrade substantially on RETINA, indicating reliance on visual shortcuts.&lt;/li&gt;&lt;li&gt;Proposes MIMIR, a multi-image multimodal retriever that augments document embeddings with images of multiple related entities and improves performance on RETINA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dosung Lee', 'Sangwon Jung', 'Boyoung Kim', 'Minyoung Kim', 'Sungyeon Kim', 'Junyoung Sung', 'Paul Hongsuck Seo']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal VQA', 'dataset bias / shortcuts', 'robustness evaluation', 'benchmarking', 'retrieval methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22843</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</title><link>https://arxiv.org/abs/2511.22826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MMA-Bench (videos + tasks) to probe MLLMs' reliance on and robustness to contradicting or misaligned modalities.&lt;/li&gt;&lt;li&gt;Uses black-box and white-box interpretability to show current MLLMs are brittle under misaligned audio-visual pairs and misleading text.&lt;/li&gt;&lt;li&gt;Proposes a modality alignment tuning strategy that teaches models when to prioritize, leverage, or ignore specific modality cues, improving multimodal grounding; code and dataset to be released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianle Chen', 'Chaitanya Chakka', 'Arjun Reddy Akula', 'Xavier Thomas', 'Deepti Ghadiyaram']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal robustness', 'model alignment', 'interpretability', 'adversarial robustness', 'benchmark/dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22826</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models</title><link>https://arxiv.org/abs/2511.22787</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CultureMix, a 23k diffusion-generated, human-verified VQA benchmark focusing on culture-mixing in food images across four subtasks (food-only, food+food, food+background, food+food+background).&lt;/li&gt;&lt;li&gt;Evaluates 10 LVLMs and finds consistent failures to preserve individual cultural identities, strong background reliance (accuracy drops ~14% when backgrounds added), and inconsistent predictions across contexts.&lt;/li&gt;&lt;li&gt;Explores three robustness strategies and shows supervised fine-tuning on diverse culture-mixing data substantially improves consistency and reduces background sensitivity.&lt;/li&gt;&lt;li&gt;Argues culture mixing is an important robustness/safety evaluation scenario for LVLMs operating in culturally diverse real-world environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eunsu Kim', 'Junyeong Park', 'Na Min An', 'Junseong Kim', 'Hitesh Laxmichand Patel', 'Jiho Jin', 'Julia Kruk', 'Amit Agarwal', 'Srikant Panda', 'Fenal Ashokbhai Ilasariya', 'Hyunjung Shim', 'Alice Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'dataset/benchmarking', 'cultural bias', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22787</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Cross-Generator Image Forgery Detection through DINOv3</title><link>https://arxiv.org/abs/2511.22471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows frozen visual foundation model DINOv3 has strong cross-generator image forgery detection ability without fine-tuning, relying on global low-frequency cues.&lt;/li&gt;&lt;li&gt;Analyzes detection behavior from frequency, spatial, and token perspectives to contrast transferable low-frequency signals vs. generator-specific high-frequency artifacts.&lt;/li&gt;&lt;li&gt;Proposes a training-free token-ranking method plus a lightweight linear probe to select authenticity-relevant tokens, improving detection accuracy across datasets.&lt;/li&gt;&lt;li&gt;Provides an interpretable, efficient baseline and empirical explanation for why foundation models generalize across unseen generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenglin Huang', 'Jason Li', 'Haiquan Wen', 'Tianxiao Li', 'Xi Yang', 'Lu Qi', 'Bei Peng', 'Xiaowei Huang', 'Ming-Hsuan Yang', 'Guangliang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'deepfake detection', 'foundation models', 'robustness', 'digital forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22471</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition</title><link>https://arxiv.org/abs/2511.22443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FauxNet, a deepfake detection model that leverages pre-trained Visual Speech Recognition (VSR) features to detect manipulated videos.&lt;/li&gt;&lt;li&gt;Focuses on zero-shot / generalizable detection across unseen manipulation methods and reports consistent improvements over prior state-of-the-art in that setting.&lt;/li&gt;&lt;li&gt;Also performs attribution to distinguish which generation technique produced a fake and introduces two new datasets (Authentica-Vox and Authentica-HDTF) with ~38k real and fake videos.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maheswar Bora', 'Tashvik Dhamija', 'Shukesh Reddy', 'Baptiste Chopin', 'Pranav Balaji', 'Abhijit Das', 'Antitza Dantcheva']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'visual speech recognition (VSR)', 'generalization / zero-shot detection', 'deepfake attribution', 'datasets']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22443</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ABounD: Adversarial Boundary-Driven Few-Shot Learning for Multi-Class Anomaly Detection</title><link>https://arxiv.org/abs/2511.22436</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ABounD, a unified few-shot multi-class anomaly detection framework combining semantic concept learning with decision-boundary shaping.&lt;/li&gt;&lt;li&gt;Dynamic Concept Fusion (DCF) creates class-adaptive prompts by fusing general priors with class-specific cues conditioned on image features.&lt;/li&gt;&lt;li&gt;Adversarial Boundary Forging (ABF) uses PGD-style perturbations to generate boundary-level 'fence' features that sculpt a precise decision margin.&lt;/li&gt;&lt;li&gt;Single-stage training with a Concept-Boundary Loss and semantic-spatial regularizers yields state-of-the-art results on MVTec-AD and VisA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runzhi Deng', 'Yundi Hu', 'Xinshuang Zhang', 'Zhao Wang', 'Xixi Liu', 'Wang-Zhou Dai', 'Caifeng Shan', 'Fang Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['anomaly-detection', 'adversarial-training', 'robustness', 'few-shot-learning', 'computer-vision']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22436</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>INSIGHT: An Interpretable Neural Vision-Language Framework for Reasoning of Generative Artifacts</title><link>https://arxiv.org/abs/2511.22351</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents INSIGHT, a multimodal framework for robust detection and interpretable explanation of AI-generated images, effective even at extreme low resolutions (16x16–64x64).&lt;/li&gt;&lt;li&gt;Combines hierarchical super-resolution to amplify forensic cues, Grad-CAM multi-scale localization to identify suspicious regions, and CLIP-guided semantic alignment to map anomalies to human-readable descriptors.&lt;/li&gt;&lt;li&gt;Uses a prompted vision-language model with ReAct + Chain-of-Thought and a dual-stage G-Eval + LLM-as-judge verification pipeline to produce and verify consistent, factual explanations.&lt;/li&gt;&lt;li&gt;Evaluated across multiple domains and degradations, reporting improved detection robustness and explanation quality over prior detectors and black-box VLM baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Bagaria']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake-detection', 'forensic-analysis', 'explainability', 'robustness', 'vision-language-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22351</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unexplored flaws in multiple-choice VQA evaluations</title><link>https://arxiv.org/abs/2511.22341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies three previously unexplored prompt-formatting variation factors that affect multiple-choice VQA evaluations.&lt;/li&gt;&lt;li&gt;Performs a large-scale empirical study across 7 MLLMs, 5 VQA datasets, and 48 prompt-format variations to measure sensitivity.&lt;/li&gt;&lt;li&gt;Finds that minor, semantically-neutral formatting changes substantially alter VQA performance, independent of known answer-order biases or model confidence.&lt;/li&gt;&lt;li&gt;Shows that existing bias-mitigation strategies do not eliminate these newly identified formatting biases, calling into question current evaluation reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabio Rosenthal', 'Sebastian Schmidt', 'Thorsten Graf', 'Thorsten Bagodonat', 'Stephan G\\"unnemann', 'Leo Schwinn']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-formatting', 'evaluation-bias', 'robustness', 'VQA', 'multimodal-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22341</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?</title><link>https://arxiv.org/abs/2511.22262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerabilities in existing 3D Gaussian Splatting (3DGS) watermarking schemes and shows 2D watermark removal methods do not generalize to 3DGS.&lt;/li&gt;&lt;li&gt;Proposes GSPure, a purification framework that analyzes view-dependent rendering contributions and uses geometrically accurate feature clustering to isolate and remove watermark-related Gaussian primitives.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results: up to 16.34 dB reduction in watermark PSNR while keeping scene fidelity loss under 1 dB, and outperforms prior methods on effectiveness and generalization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Huang', 'Yijia Guo', 'Gaolei Li', 'Lei Ma', 'Hang Zhang', 'Liwen Hu', 'Jiazheng Wang', 'Jianhua Li', 'Tiejun Huang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', '3D Gaussian Splatting', 'adversarial attack', 'robustness', 'copyright protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22262</guid><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Creating Blank Canvas Against AI-enabled Image Forgery</title><link>https://arxiv.org/abs/2511.22237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes transforming an image into a "blank canvas" for SAM via adversarial perturbations so the model initially "sees nothing", enabling detection when regions are later modified.&lt;/li&gt;&lt;li&gt;Introduces a frequency-aware optimization strategy to more effectively blind SAM against naive adversarial attempts and improve tamper localization.&lt;/li&gt;&lt;li&gt;Leverages SAM's perceptual capabilities to identify forged regions once the protected blank canvas is altered.&lt;/li&gt;&lt;li&gt;Presents experimental results claiming the method improves tamper localization performance against AI-enabled image editing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Song', 'Ziyuan Luo', 'Renjie Wan']&lt;/li&gt;&lt;li&gt;Tags: ['image forgery detection', 'adversarial attacks', 'tamper localization', 'Segment Anything Model (SAM)', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22237</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks</title><link>https://arxiv.org/abs/2511.22147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies computation cost attacks against 3D Gaussian Splatting (3DGS) that cause resource exhaustion and DoS.&lt;/li&gt;&lt;li&gt;Proposes RemedyGS, a black-box defense pipeline with a detector for poisoned input textures and a purifier to recover benign images.&lt;/li&gt;&lt;li&gt;Incorporates adversarial training for the purifier to align recovered images with natural distribution, improving robustness.&lt;/li&gt;&lt;li&gt;Evaluates defense effectiveness against white-box, black-box, and adaptive attacks, showing improved safety and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanping Li', 'Zhening Liu', 'Zijian Li', 'Zehong Lin', 'Jun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['computation cost attacks', 'defense/mitigation', 'detection and purification', 'adversarial training', '3D reconstruction security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22147</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization</title><link>https://arxiv.org/abs/2511.22119</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PROMPTMINER, a black-box prompt-stealing framework for text-to-image models that decouples recovery into an RL-based phase to reconstruct primary subjects and a fuzzing-driven phase to recover stylistic modifiers.&lt;/li&gt;&lt;li&gt;Demonstrates strong quantitative performance (CLIP similarity up to 0.958, SBERT alignment up to 0.751), outperforms baselines, generalizes to in-the-wild images with unknown generators, and remains robust under defensive perturbations.&lt;/li&gt;&lt;li&gt;Evaluated across multiple datasets and diffusion backbones; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingzhe Li', 'Renhao Zhang', 'Zhiyang Wen', 'Siqi Pan', 'Bruno Castro da Silva', 'Juan Zhai', 'Shiqing Ma']&lt;/li&gt;&lt;li&gt;Tags: ['prompt stealing', 'black-box attack', 'model extraction', 'text-to-image', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22119</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title><link>https://arxiv.org/abs/2511.22019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free, post-hoc uncertainty estimation method for contrastive vision-language models (e.g., CLIP) that detects erroneous predictions.&lt;/li&gt;&lt;li&gt;Creates class-specific probabilistic embeddings by projecting visual features and fitting multivariate Gaussians to measure intra-class feature consistency.&lt;/li&gt;&lt;li&gt;Method is VLM-agnostic, requires no fine-tuning, works with as few as 10 images per class, and shows robustness to distribution shift.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art error detection on multiple image benchmarks (ImageNet, Flowers102, Food101, EuroSAT, DTD).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenxiang Lin', 'Maryam Haghighat', 'Will Browne', 'Dimity Miller']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-estimation', 'out-of-distribution-detection', 'vision-language-models', 'post-hoc-method', 'model-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22019</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond the Rubric: Cultural Misalignment in LLM Benchmarks for Sexual and Reproductive Health</title><link>https://arxiv.org/abs/2511.17554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors evaluate an LLM-based chatbot for sexual and reproductive health (SRH) in an underserved Indian community using OpenAI's HealthBench automated rubric grader.&lt;/li&gt;&lt;li&gt;HealthBench rated many responses consistently low, but qualitative review by trained annotators and public-health experts found many replies to be culturally appropriate and medically accurate.&lt;/li&gt;&lt;li&gt;The paper documents recurring Western-centric biases in the benchmark (legal framing, social norms like breastfeeding in public, dietary assumptions, insurance/cost models) that cause misalignment between rubric scores and real-world appropriateness.&lt;/li&gt;&lt;li&gt;Argues for developing culturally adaptive evaluation frameworks that maintain quality standards while reflecting diverse cultural and healthcare contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sumon Kanti Dey', 'Manvi S', 'Zeel Mehta', 'Meet Shah', 'Unnati Agrawal', 'Suhani Jalota', 'Azra Ismail']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'LLM safety evaluation', 'cultural bias', 'healthcare NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17554</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards</title><link>https://arxiv.org/abs/2506.20520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes an off-policy REINFORCE variant for LLM fine-tuning where advantage is A = r - V and V is a tunable baseline that trades off emphasis on positive vs negative rewards.&lt;/li&gt;&lt;li&gt;Provides a theoretical policy improvement guarantee when the baseline V lower-bounds the expected reward, and argues off-policy updates should weight positive rewards more than negative ones.&lt;/li&gt;&lt;li&gt;Validates findings empirically in a stochastic bandit setting and via fine-tuning state-of-the-art LLMs on reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles Arnal', 'Ga\\"etan Narozniak', 'Vivien Cabannes', 'Yunhao Tang', 'Julia Kempe', 'Remi Munos']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'off-policy RL', 'REINFORCE', 'LLM fine-tuning', 'reward shaping']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20520</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title><link>https://arxiv.org/abs/2506.12484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of components for irreversible unlearning in language models, identifying which design choices matter.&lt;/li&gt;&lt;li&gt;Introduces Disruption Masking: only update weights where unlearning and retaining gradient signs match to avoid disruptive updates.&lt;/li&gt;&lt;li&gt;Shows the importance of normalizing unlearning gradients and using meta-learning; combines these into MUDMAN.&lt;/li&gt;&lt;li&gt;MUDMAN significantly improves robustness to recovery of dangerous capabilities, outperforming prior TAR method by ~40%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang', 'Miko{\\l}aj Kniejski', 'Marcel Windys']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LLM safety', 'robustness', 'meta-learning', 'adversarial recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12484</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RvLLM: LLM Runtime Verification with Domain Knowledge</title><link>https://arxiv.org/abs/2505.18585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ESL, a general specification language for domain experts to encode domain-specific predicates for LLM output verification.&lt;/li&gt;&lt;li&gt;Introduces RvLLM, a runtime verification framework that checks LLM outputs against ESL constraints to detect misbehavior or erroneous outputs.&lt;/li&gt;&lt;li&gt;Evaluates the approach on three tasks (legal statute violations, numerical comparison, inequality solving) showing effective error/violation detection across multiple LLMs.&lt;/li&gt;&lt;li&gt;Positions domain-knowledge-driven runtime checks as a lightweight, flexible method to improve reliability of LLM outputs in high-stakes domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yedi Zhang', 'Sun Yi Emma', 'Annabelle Lee Jia En', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'runtime verification', 'specification language', 'misbehavior detection', 'domain knowledge integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18585</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title><link>https://arxiv.org/abs/2501.01457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRR (Distillation-Reinforcement-Reasoning): distill LLM behavioral traces, train a lightweight Discriminative Model (DM), and use DM at inference to identify/reject suspicious reasoning steps.&lt;/li&gt;&lt;li&gt;Aims to overcome the introspection illusion of self-critique by providing external behavioral feedback that forces the LLM to explore alternative reasoning pathways without modifying the base model.&lt;/li&gt;&lt;li&gt;Reports significant improvements over prominent self-critique methods on multiple reasoning benchmarks; method is lightweight, annotation-free, and scalable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diji Yang', 'Linda Zeng', 'Kezhen Chen', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reasoning', 'safety/robustness', 'alignment', 'external critic', 'behavioral feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01457</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGASA, a framework that synthesizes model-generated safety guidelines and uses them to adaptively strengthen defenses against adversarial jailbreak prompts.&lt;/li&gt;&lt;li&gt;Two-stage approach: Data Pre-synthesis (generate safety guidelines and augmented prompts) and Alignment Fine-tuning using Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO).&lt;/li&gt;&lt;li&gt;Aims to improve robustness to deceptive/adversarial prompts while reducing unnecessary refusals of benign requests.&lt;/li&gt;&lt;li&gt;Reports extensive experiments across multiple datasets showing improved model safety and adaptive, scalable effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'adversarial prompts', 'alignment fine-tuning', 'DPO']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</title><link>https://arxiv.org/abs/2511.20494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Adversarial Confusion Attack that perturbs images to maximize next-token entropy in multimodal LLMs, causing incoherent or confidently incorrect outputs.&lt;/li&gt;&lt;li&gt;Uses a small ensemble of open-source MLLMs and standard PGD optimization to craft perturbations that transfer to unseen open-source and proprietary models (e.g., Qwen3-VL, GPT-5.1).&lt;/li&gt;&lt;li&gt;Evaluates both full-image and adversarial CAPTCHA settings, showing single adversarial images can disrupt multiple models in white-box and transfer scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Hoscilowicz', 'Artur Janicki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal LLM', 'transferability', 'model robustness', 'PGD']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20494</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Honest Language Models for Deductive Reasoning</title><link>https://arxiv.org/abs/2511.09222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'honest deductive reasoning' as only answering when conclusions are strictly entailed by premises and abstaining otherwise.&lt;/li&gt;&lt;li&gt;Curates two synthetic datasets (linear algebra and logical inference) with injected unanswerable cases via random edge perturbations to evaluate abstention and reasoning.&lt;/li&gt;&lt;li&gt;Finds that prompting and existing RL-based training (GRPO, with/without SFT init) struggle, particularly due to training collapse when negative rewards dominate early training.&lt;/li&gt;&lt;li&gt;Proposes ACNCHOR, an RL method that injects ground-truth trajectories into rollouts to stabilize training, improving abstention behavior and overall reasoning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiarui Liu', 'Kaustubh Dhole', 'Yingheng Wang', 'Haoyang Wen', 'Sarah Zhang', 'Haitao Mao', 'Gaotang Li', 'Neeraj Varshney', 'Jingguo Liu', 'Xiaoman Pan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'honesty/abstention', 'reinforcement learning', 'reasoning evaluation', 'training-stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09222</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title><link>https://arxiv.org/abs/2510.15859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORBIT, a rubric-based incremental RL training framework that uses synthetic dialogue generation and dynamically constructed rubrics to guide LLMs on open-ended, high-stakes medical consultations.&lt;/li&gt;&lt;li&gt;Replaces reliance on supervised reward models by using rubric-driven feedback judged by general-purpose instruction-following LLMs, reducing need for task-specific fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates large empirical gains (e.g., Qwen3-4B-Instruct HealthBench-Hard from 7.0 to 27.5 with 2k samples) and shows generality by applying the pipeline to InfoBench for improved instruction-following.&lt;/li&gt;&lt;li&gt;Positions rubric-guided RL as a mitigation against pathological RL behaviors (e.g., reward hacking) in ambiguous, context-dependent domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengkai Wang', 'Linus', 'Pengwei Liu', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'medical AI safety', 'reward-modeling / RLHF alternatives', 'robustness / reward hacking mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15859</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation</title><link>https://arxiv.org/abs/2509.01088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DistilledPRAG: a parametric RAG approach that encodes documents as LoRA parameters to avoid uploading plaintext while preserving RAG-style reasoning.&lt;/li&gt;&lt;li&gt;Synthesizes single- and multi-document QA pairs and trains a parameter generator to map masked documents to LoRA, reducing per-document fine-tuning latency.&lt;/li&gt;&lt;li&gt;Distills knowledge by matching hidden states and output logits of a standard RAG model, improving generalization to out-of-distribution inputs.&lt;/li&gt;&lt;li&gt;Reports improved accuracy and OOD generalization over baselines on four QA datasets, addressing privacy-preserving reasoning efficiency and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwen Chen', 'Hainan Zhang', 'Liang Pang', 'Yongxin Tong', 'Haibo Zhou', 'Yuan Zhan', 'Wei Lin', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'parametric-RAG', 'knowledge-distillation', 'LoRA', 'retrieval-augmented-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01088</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs</title><link>https://arxiv.org/abs/2508.04182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that MLLMs disproportionately attend to task-irrelevant background regions, causing spurious background-answer correlations that drive hallucinations.&lt;/li&gt;&lt;li&gt;Argues outcome-based rewards can induce spurious correlations and proposes token-level sufficiency and necessity constraints to measure causal contribution of each generated token.&lt;/li&gt;&lt;li&gt;Introduces COPO (Causal-Oriented Policy Optimization): defines a causal completeness reward and integrates it into a causally informed advantage within the GRPO optimization framework to encourage evidence-grounded generation.&lt;/li&gt;&lt;li&gt;Reports empirical improvements on benchmarks, demonstrating reduced hallucinations and better grounding for multimodal generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peizheng Guo', 'Jingyao Wang', 'Wenwen Qiang', 'Jiahuan Zhou', 'Changwen Zheng', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'causal-inference', 'policy-optimization', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04182</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs</title><link>https://arxiv.org/abs/2506.01734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows LLMs inherit skewed leading-digit frequencies from pretraining corpora resembling Benford's Law, causing biased numerical generation.&lt;/li&gt;&lt;li&gt;Constructs a benchmark with uniform ground-truth leading digits across several numerical reasoning tasks and demonstrates consistent digit bias in open-source LLMs.&lt;/li&gt;&lt;li&gt;Uses logit-lens tracing and neuron-level dissection to identify a small subset of digit-selective FFN neurons in deeper layers as the primary source of bias.&lt;/li&gt;&lt;li&gt;Prunes those neurons to mitigate overgeneration of certain digits and partially correct erroneous numerical outputs, providing causal evidence linking pretraining statistics to model hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiandong Shao', 'Yao Lu', 'Jianfei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['numerical hallucination', 'model robustness', 'alignment/safety', 'interpretability', 'dataset bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01734</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Superimposed Noise Accumulation Problem in Sequential Knowledge Editing of Large Language Models</title><link>https://arxiv.org/abs/2505.07899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the 'superimposed noise accumulation' issue in sequential knowledge editing of LLMs: editing success degrades as number of edits increases.&lt;/li&gt;&lt;li&gt;Analyzes causes: erroneous activation of irrelevant knowledge and conflicts between activated knowledge.&lt;/li&gt;&lt;li&gt;Proposes DeltaEdit, which applies dynamic orthogonal constraint strategies to reduce conflicts between edited knowledge.&lt;/li&gt;&lt;li&gt;Reports empirical improvement: DeltaEdit reduces noise and yields a 16.8% editing-performance gain over the strongest baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ding Cao', 'Yuchen Cai', 'Yuqing Huang', 'Xuesong He', 'Rongxi Guo', 'Guiquan Liu', 'Guangzhong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-editing', 'LLM-robustness', 'model-editing', 'alignment', 'continual-editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07899</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>KSHSeek: Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models</title><link>https://arxiv.org/abs/2503.19482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and characterizes 'knowledge-shortcut' hallucinations in generative LMs arising from spurious correlations in otherwise correct data.&lt;/li&gt;&lt;li&gt;Proposes a high-similarity pruning algorithm at data preprocessing to reduce spurious correlations that cause knowledge-shortcut hallucinations.&lt;/li&gt;&lt;li&gt;Designs a detection method specifically for knowledge-shortcut hallucinations to evaluate mitigation effectiveness.&lt;/li&gt;&lt;li&gt;Empirical results show reduced knowledge-shortcut hallucinations (especially in fine-tuning) without harming QA performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongxin Liu', 'Zhiwei Wang', 'Jun Niu', 'Ying Li', 'Hongyu Sun', 'Meng Xu', 'He Wang', 'Gaofei Wu', 'Yuqing Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'hallucination-detection', 'data-centric-mitigation', 'LLM-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19482</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self Iterative Label Refinement via Robust Unlabeled Learning</title><link>https://arxiv.org/abs/2502.12565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an iterative self-refinement pipeline that uses an Unlabeled–Unlabeled (U–U) learning framework to denoise and refine LLM-generated pseudo-labels for classification tasks.&lt;/li&gt;&lt;li&gt;Exploits two unlabeled datasets with different positive-class ratios to iteratively improve pseudo-label quality with minimal human supervision.&lt;/li&gt;&lt;li&gt;Demonstrates consistent performance gains over initial LLM labels and prior self-refinement approaches across diverse datasets (low-resource languages, patents, protein categorizations).&lt;/li&gt;&lt;li&gt;Claims the refined classifier can facilitate effective post-training alignment for safety in LLMs and extends to generative task self-refinement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hikaru Asano', 'Tadashi Kozuno', 'Yukino Baba']&lt;/li&gt;&lt;li&gt;Tags: ['LLM self-refinement', 'semi-supervised denoising', 'alignment/safety', 'pseudo-labeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12565</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization</title><link>https://arxiv.org/abs/2402.11414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FALLACIOUS: two fine-grained, explainable factuality evaluation frameworks for multimodal (text+image) summarization — one reference-based and one reference-free.&lt;/li&gt;&lt;li&gt;The reference-free framework enables factuality assessment without ground-truth summaries, broadening applicability.&lt;/li&gt;&lt;li&gt;Evaluates correlation of the proposed metrics with existing metrics and presents experimental results demonstrating effectiveness; releases code and dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Zhang', 'Jingxuan Zuo', 'Ke Su', 'Liqiang Jing']&lt;/li&gt;&lt;li&gt;Tags: ['factuality evaluation', 'multimodal summarization', 'reference-free evaluation', 'explainability', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.11414</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AutoHall: Automated Factuality Hallucination Dataset Generation for Large Language Models</title><link>https://arxiv.org/abs/2310.00259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;AutoHall: an automated method to construct model-specific hallucination datasets by converting existing fact-checking datasets into prompts/responses for different LLMs.&lt;/li&gt;&lt;li&gt;Empirical findings show variation in hallucination rates and types across LLMs; dataset enables comparative analysis of model-specific factual errors.&lt;/li&gt;&lt;li&gt;Introduces a zero-resource, black-box hallucination detection approach based on self-contradiction that outperforms baselines on the constructed data.&lt;/li&gt;&lt;li&gt;Provides analysis of factors contributing to hallucinations and releases code and datasets for reproducible evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zouying Cao', 'Yifei Yang', 'XiaoJing Li', 'Hai Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'factuality', 'dataset generation', 'hallucination detection', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00259</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PRISM: Privacy-Aware Routing for Adaptive Cloud-Edge LLM Inference via Semantic Sketch Collaboration</title><link>https://arxiv.org/abs/2511.22788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM, a context-aware cloud-edge LLM inference framework that routes tokens/entities to cloud, edge, or collaborative processing based on per-entity sensitivity profiling.&lt;/li&gt;&lt;li&gt;Uses adaptive two-layer local differential privacy (LDP) on the edge for collaborative paths, producing a perturbed prompt and a cloud-generated semantic sketch that an edge SLM refines with local context.&lt;/li&gt;&lt;li&gt;Claims improved privacy-utility trade-offs and reduced energy/latency (40–50% of baselines) across realistic prompts and heterogeneous deployments under strong privacy constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junfei Zhan', 'Haoxun Shen', 'Zheng Lin', 'Tengjiao He']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'local differential privacy', 'cloud-edge inference', 'privacy-utility tradeoff', 'adaptive routing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22788</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs</title><link>https://arxiv.org/abs/2511.21757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Medical Malice, a dataset of 214,219 adversarial prompts targeted to context-specific safety failures within the Brazilian Unified Health System (SUS).&lt;/li&gt;&lt;li&gt;Prompts are labeled with the reasoning behind each violation and span seven taxonomies (e.g., procurement manipulation, queue-jumping, obstetric violence), enabling models to learn context-aware refusal behavior.&lt;/li&gt;&lt;li&gt;Synthesis used an unaligned agent (Grok-4) in a persona-driven pipeline to generate high-fidelity attacks; the paper discusses ethical considerations of releasing vulnerability signatures to aid defenders.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Andrew Maranh\\~ao Ventura D'addario"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'healthcare safety', 'safety dataset', 'context-aware alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21757</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization</title><link>https://arxiv.org/abs/2511.23391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that semantically identical or similar (ambiguous) content within preference pairs used for DPO training can introduce ambiguity and degrade alignment performance.&lt;/li&gt;&lt;li&gt;Proposes Ambiguity Awareness Optimization (AAO), which computes semantic similarity across preference pairs and re-weights ambiguous content to reduce its impact during DPO training.&lt;/li&gt;&lt;li&gt;Provides mathematical analysis, proof-of-concept experiments, and extensive empirical results showing AAO outperforms DPO across benchmarks (AlpacaEval 2, MT-Bench, Arena-Hard) and model scales without substantially increasing response length.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Li', 'Shenglin Yin', 'Yujia Zhang', 'Alan Zhao', 'Xi Chen', 'Xiaohui Zhou', 'Pengfei Xu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF / DPO', 'preference learning', 'semantic disambiguation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23391</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are LLMs Good Safety Agents or a Propaganda Engine?</title><link>https://arxiv.org/abs/2511.23174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PSP, a dataset of politically sensitive/censored prompts (from China-generalized sources and censored tweets) to probe LLM refusal behavior.&lt;/li&gt;&lt;li&gt;Evaluates seven LLMs using data-driven and representation-level interventions (making political intent implicit and erasing political concepts) to distinguish safety-motivated refusals from political censorship.&lt;/li&gt;&lt;li&gt;Assesses vulnerability to prompt injection attacks (PIAs) on PSP and measures how refusals shift across models and country contexts.&lt;/li&gt;&lt;li&gt;Finds most LLMs exhibit forms of censorship/refusal on politically sensitive content and identifies attributes that change refusal distributions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neemesh Yadav', 'Francesco Ortu', 'Jiarui Liu', 'Joeun Yook', 'Bernhard Sch\\"olkopf', 'Rada Mihalcea', 'Alberto Cazzaniga', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'censorship', 'prompt injection', 'red teaming', 'dataset/benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23174</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?</title><link>https://arxiv.org/abs/2511.22978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ShoppingComp, a benchmark (120 tasks, 1,026 scenarios) for evaluating LLM-powered shopping agents on product retrieval, report generation, and safety-critical decision making.&lt;/li&gt;&lt;li&gt;Emphasizes real products and verifiability, adding an evaluation dimension for identifying product safety hazards and misinformation-driven harmful recommendations.&lt;/li&gt;&lt;li&gt;Finds large safety and reliability gaps in state-of-the-art models (e.g., GPT-5 11.22%, Gemini-2.5-Flash 3.92%), highlighting risks in real-world e-commerce deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huaixiao Tou', 'Ying Zeng', 'Cong Ma', 'Muzhi Li', 'Minghao Li', 'Weijie Yuan', 'He Zhang', 'Kai Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'benchmarking', 'safety-critical decision making', 'misinformation/harmful recommendations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22978</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms</title><link>https://arxiv.org/abs/2511.22858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines three legal constraints for a RAG-based LLM in Japanese litigation: (1) retrieval must avoid private knowledge and retrieve appropriate external sources, (2) generated responses must be strictly faithful to the retrieved context, and (3) retrieved sources must include appropriate timestamps aligned to the disputed issues.&lt;/li&gt;&lt;li&gt;Discusses system design choices for a RAG pipeline to satisfy these requirements and support substitution of expert commissioners in legal proceedings while complying with legal norms.&lt;/li&gt;&lt;li&gt;Focuses on fidelity, provenance, and legal-compliance aspects of retrieval and generation rather than adversarial attacks or threat modeling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuya Ishihara', 'Atsushi Keyaki', 'Hiroaki Yamada', 'Ryutaro Ohara', 'Mihoko Sumida']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'faithfulness', 'retrieval_provenance', 'legal_compliance', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22858</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Token-Level Marginalization for Multi-Label LLM Classifiers</title><link>https://arxiv.org/abs/2511.22312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes three token-level probability estimation methods to derive class-level confidences from generative LLM classifiers for multi-label content safety tasks.&lt;/li&gt;&lt;li&gt;Aims to improve interpretability, calibration, and thresholding for moderation by leveraging token logits from instruction-tuned models (e.g., LLaMA Guard).&lt;/li&gt;&lt;li&gt;Evaluates methods on a synthetically generated, rigorously annotated dataset and reports improved reliability and finer-grained error analysis for safety moderation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anjaneya Praharaj', 'Jaykumar Kasundra']&lt;/li&gt;&lt;li&gt;Tags: ['content-moderation', 'LLM-safety', 'confidence-calibration', 'interpretability', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22312</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text</title><link>https://arxiv.org/abs/2511.22153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid ensemble combining (i) RoBERTa classifier, (ii) GPT-2 probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical stylometric analyzer to detect LLM-generated text.&lt;/li&gt;&lt;li&gt;Introduces an optimized weighted voting framework where ensemble weights are learned on the probability simplex to directly maximize F1, rather than using heuristic weights.&lt;/li&gt;&lt;li&gt;Provides a bias-variance analysis and reports low inter-model correlation (rho ≈ 0.35–0.42), enabling variance reduction; empirically achieves 94.2% accuracy and AUC 0.978 on a 30k-document multi-generator corpus with 35% relative reduction in false positives on academic text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepyan Purnama Kristanto', 'Lutfi Hakim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'text provenance', 'ensemble methods', 'false positive reduction', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22153</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models</title><link>https://arxiv.org/abs/2511.22016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AfriStereo, an open-source dataset of African-context stereotypes (1,163 collected, augmented to &gt;5,000 stereotype–antistereotype pairs) grounded via community engagement in Senegal, Kenya, and Nigeria.&lt;/li&gt;&lt;li&gt;Data augmentation used few-shot prompting with human-in-the-loop validation, semantic clustering, and manual annotation by culturally informed reviewers.&lt;/li&gt;&lt;li&gt;Evaluates 11 language models and finds statistically significant biases in 9 models (Bias Preference Ratios 0.63–0.78), with stronger stereotypical preferences across age, profession, and gender.&lt;/li&gt;&lt;li&gt;Claims domain-specific models may show reduced bias in this setup and positions AfriStereo as a resource for culturally grounded bias evaluation and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yann Le Beux', 'Oluchi Audu', 'Oche D. Ankeli', 'Dhananjay Balakrishnan', 'Melissah Weya', 'Marie D. Ralaiarinosy', 'Ignatius Ezeani']&lt;/li&gt;&lt;li&gt;Tags: ['bias-dataset', 'fairness-evaluation', 'cultural-bias', 'NLP-dataset', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22016</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices</title><link>https://arxiv.org/abs/2511.21860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Consistency-Rebalanced Accuracy (CoRA) to improve reliability of multiple-choice benchmark scores by accounting for LLM response consistency.&lt;/li&gt;&lt;li&gt;Introduces two intermediate scores—Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI)—and uses synthetically generated question variants with altered answer choices to measure consistency.&lt;/li&gt;&lt;li&gt;Demonstrates that models can show high MCQA accuracy but low consistency, and that CoRA downscales scores of inconsistent models across multiple benchmarks and LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulo Cavalin', 'Cassia Sanctos', 'Marcelo Grave', 'Claudio Pinhanez', 'Yago Primerano']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'evaluation-metrics', 'robustness', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21860</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Factors That Support Grounded Responses in LLM Conversations: A Rapid Review</title><link>https://arxiv.org/abs/2511.21762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Rapid review identifying techniques to align LLM conversational outputs with user intent, ensure contextual grounding, and reduce hallucinations and topic drift.&lt;/li&gt;&lt;li&gt;Organizes alignment strategies by LLM lifecycle phase: inference-time, post-training, and reinforcement learning–based methods.&lt;/li&gt;&lt;li&gt;Finds inference-time approaches particularly efficient for aligning outputs without retraining and for mitigating hallucination while supporting grounding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriele Cesar Iwashima', 'Claudia Susie Rodrigues', 'Claudio Dipolitto', "Geraldo Xex\\'eo"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'inference-time techniques', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21762</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models</title><link>https://arxiv.org/abs/2511.21756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Causal Tracing to GPT-2 XL on the ConvFinQA benchmark and identifies a dual-stage arithmetic mechanism: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit at layer 46.&lt;/li&gt;&lt;li&gt;Verifies causality via ablation: suppressing layer 46 reduces the model's confidence in hallucinatory arithmetic outputs by 81.8%.&lt;/li&gt;&lt;li&gt;Shows a linear probe trained on layer 46 generalizes to unseen financial topics with 98% accuracy, suggesting a broadly applicable geometry for arithmetic-related deception and a practical detection/suppression intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soham Mirajkar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'causal tracing', 'circuit analysis', 'interpretability', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21756</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title><link>https://arxiv.org/abs/2511.21752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Label Disguise Defense (LDD): replace true class labels with semantically transformed or unrelated aliases (e.g., 'blue' vs 'yellow') learned via few-shot demonstrations to mitigate class-directive prompt injection.&lt;/li&gt;&lt;li&gt;Evaluates LDD across nine state-of-the-art LLMs (e.g., GPT-5, GPT-4o, LLaMA3.2, Gemma3, Mistral variants) under adversarial prompt-injection settings and varying few-shot setups.&lt;/li&gt;&lt;li&gt;Finds LDD partially restores accuracy lost to prompt-injection across all models, with semantically aligned aliases (e.g., 'good'/'bad') generally yielding stronger robustness than arbitrary symbols.&lt;/li&gt;&lt;li&gt;Provides linguistic analysis of alias semantics and demonstrates model-agnostic, lightweight defense that does not require model retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Li', 'Ruocheng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM robustness', 'jailbreaking defense', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21752</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness</title><link>https://arxiv.org/abs/2511.21749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BRIES, a compound AI with specialized agents: Twister (adversarial persuasion generator), Detector (attack classifier), Defender (content inoculation), and Assessor (causal evaluation of inoculation).&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs (GPT-4, Gemma, Llama3, Mistral) on a synthetic persuasion dataset using the SemEval 2023 Task 3 taxonomy, reporting model-specific detection performance and sensitivity to prompt/temperature settings.&lt;/li&gt;&lt;li&gt;Performs causal analysis to identify socio-emotional-cognitive signatures of persuasion attacks and measures how inoculation interventions affect human cognitive resilience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Svitlana Volkova', 'Will Dupree', 'Hsien-Te Kao', 'Peter Bautista', 'Gabe Ganberg', 'Jeff Beaubien', 'Laura Cassani']&lt;/li&gt;&lt;li&gt;Tags: ['persuasion attacks', 'adversarial prompting', 'LLM safety', 'red teaming', 'inoculation/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21749</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features</title><link>https://arxiv.org/abs/2511.21744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NEULIF, a lightweight detector for AI-generated text that uses stylometric and readability features combined with a compact CNN or Random Forest classifier.&lt;/li&gt;&lt;li&gt;Reports strong performance on the Kaggle AI vs. Human corpus: CNN ~97% accuracy (~0.95 F1, ROC-AUC 99.5%), Random Forest ~95% accuracy (~0.94 F1, ROC-AUC 95%).&lt;/li&gt;&lt;li&gt;Models are compact (CNN ~25 MB, RF ~10.6 MB) and CPU-efficient, intended to be run without large transformer ensembles.&lt;/li&gt;&lt;li&gt;Authors claim potential generalization across languages, domains, and streaming contexts, emphasizing simplicity over large-model complexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey K. Aityan', 'William Claster', 'Karthik Sai Emani', 'Sohni Rais', 'Thy Tran']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'stylometry', 'lightweight models', 'model efficiency', 'safety/forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21744</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Polarity-Aware Probing for Quantifying Latent Alignment in Language Models</title><link>https://arxiv.org/abs/2511.21737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Polarity-Aware CCS (PA-CCS), an unsupervised probing method that assesses whether internal representations of LMs remain consistent under polarity inversion (harmful vs. safe statements).&lt;/li&gt;&lt;li&gt;Proposes two alignment-focused metrics—Polar-Consistency and the Contradiction Index—and curates paired datasets of harmful/safe sentences (two main datasets plus a control) to validate the method.&lt;/li&gt;&lt;li&gt;Applies PA-CCS across 16 language models, revealing architectural and layer-specific differences in encoding latent harmful knowledge and showing sensitivity to token-level manipulations (e.g., replacing negation tokens).&lt;/li&gt;&lt;li&gt;Argues for incorporating structural robustness checks into interpretability/ alignment benchmarks and provides code and datasets for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sabrina Sadiekh', 'Elena Ericheva', 'Chirag Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'probing', 'latent beliefs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21737</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems</title><link>https://arxiv.org/abs/2511.21729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Combines hybrid retrieval, ensemble verification, and adaptive thresholding in RAG systems to achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations on a 50-query evaluation (including adversarial cases).&lt;/li&gt;&lt;li&gt;Identifies a measurement/labeling challenge where different verification strategies produce inconsistent labels (e.g., "abstained" vs "unsupported"), causing misleading apparent hallucination rates.&lt;/li&gt;&lt;li&gt;Argues that synergistic integration and adaptive calibration matter more than improving any single component, and calls for standardized metrics and labels to correctly interpret safety performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jithin Krishnan']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination', 'calibration', 'verification', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21729</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers</title><link>https://arxiv.org/abs/2511.21718</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MICM, a model-agnostic jailbreak that uses 'conceptual triggers' — benign phrases encoded in a fixed prompt template — to shift LLM outputs toward a target value stance without tripping standard safety filters.&lt;/li&gt;&lt;li&gt;Evaluates MICM on five advanced LLMs (e.g., GPT-4o, Deepseek-R1, Qwen3-8B) and reports higher success rates and lower rejection than state-of-the-art jailbreak methods.&lt;/li&gt;&lt;li&gt;Frames the attack via conceptual morphology theory and highlights a systemic vulnerability: safety mechanisms can be bypassed by covert manipulation of implicit value alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoxin Zhang', 'Borui Chen', 'Yiming Hu', 'Youyang Qu', 'Tianqing Zhu', 'Longxiang Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21718</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution</title><link>https://arxiv.org/abs/2511.21717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CrossCheck-Bench, a 15k example diagnostic benchmark for detecting and resolving contradictions in multimodal (image+text) inputs, with synthetic contradictions injected into real-world artifacts.&lt;/li&gt;&lt;li&gt;Defines a hierarchical task framework with three reasoning levels and seven atomic capabilities (perception, integration, multi-step inference, rule-based validation, etc.) to pinpoint compositional failure modes.&lt;/li&gt;&lt;li&gt;Evaluates 13 state-of-the-art vision-language models, finding large performance drops when tasks require synthesizing multiple clues or multi-step logical reasoning; simple prompting strategies yield marginal gains while hybrid symbolic-grounded methods improve stability.&lt;/li&gt;&lt;li&gt;Provides a capability-level analysis revealing uneven skill acquisition and persistent bottlenecks in multimodal contradiction detection, suggesting directions for improving robust cross-modal verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baoliang Tian', 'Yuxuan Si', 'Jilong Wang', 'Lingyao Li', 'Zhongyuan Bao', 'Zineng Zhou', 'Tao Wang', 'Sixu Li', 'Ziyao Xu', 'Mingze Wang', 'Zhouzhuo Zhang', 'Zhihao Wang', 'Yike Yun', 'Ke Tian', 'Ning Yang', 'Minghui Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety-evaluation', 'multimodal-contradiction-detection', 'benchmark', 'multimodal-reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21717</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation</title><link>https://arxiv.org/abs/2511.21711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates stereotypes and biases in LLMs (BERT, GPT-3.5, ADA) using bias benchmarks StereoSet and CrowS-Pairs and a three-pronged detection approach for explicit and implicit biases.&lt;/li&gt;&lt;li&gt;Finds fine-tuned models struggle with gender bias but perform better at avoiding racial bias; observes models often over-rely on prompt keywords rather than true understanding.&lt;/li&gt;&lt;li&gt;Applies mitigation strategies (fine-tuning, prompt techniques, data augmentation) and reports up to 20% improvement on implicit bias benchmarks, with cross-dataset adaptability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatima Kazi']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'LLM safety', 'fairness', 'evaluation/benchmarking', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21711</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Insight-A: Attribution-aware for Multimodal Misinformation Detection</title><link>https://arxiv.org/abs/2511.21705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Insight-A, a multimodal misinformation detection framework that attributes forged content to generation sources and detects cross-modal distortions.&lt;/li&gt;&lt;li&gt;Introduces Cross-Attribution Prompting (CAP) to model correlations between perception and reasoning, and Attribution-Debiased Prompting (ADP) to reduce prompt subjectivity for MLLMs.&lt;/li&gt;&lt;li&gt;Adds image captioning to improve visual detail extraction for cross-modal consistency checking and reports extensive experiments showing improved detection performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Wu', 'Yumeng Fu', 'Chen Gong', 'Guohong Fu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal misinformation detection', 'attribution', 'MLLM prompting', 'AIGC detection', 'cross-modal consistency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21705</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title><link>https://arxiv.org/abs/2504.02821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Sparse Autoencoders (SAEs) to vision-language models (e.g., CLIP) to improve neuron-level monosemanticity in visual representations.&lt;/li&gt;&lt;li&gt;Introduces a human-aligned benchmark for measuring monosemanticity derived from a large-scale user study.&lt;/li&gt;&lt;li&gt;Shows SAEs (especially with sparsity and wide latents) increase monosemanticity and that intervening on CLIP's vision encoder can steer multimodal LLM outputs (e.g., LLaVA) without changing the language model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'safety', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02821</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Targeted Deep Learning System Boundary Testing</title><link>https://arxiv.org/abs/2408.06258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Mimicry, a black-box test generator that uses style-based GANs to disentangle content and style and produce controlled latent-space mixes to probe DL decision boundaries.&lt;/li&gt;&lt;li&gt;Leverages model output probabilities to guide targeted exploration toward regions near decision boundaries, yielding semantically meaningful boundary inputs rather than corrupted artifacts.&lt;/li&gt;&lt;li&gt;Evaluated on five image classification systems; shows higher proximity to decision boundaries, greater validity/diversity, and uncovers new functional misbehaviors compared to two baselines, with human assessment confirmation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oliver Wei{\\ss}l', 'Amr Abdellatif', 'Xingcheng Chen', 'Giorgi Merabishvili', 'Vincenzo Riccio', 'Severin Kacianka', 'Andrea Stocco']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial testing', 'boundary testing', 'GAN-based generation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.06258</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Split Conformal Prediction under Data Contamination</title><link>https://arxiv.org/abs/2407.07700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes robustness of split conformal prediction when a small fraction of calibration scores are contaminated (drawn from a different distribution) and quantifies effects on marginal coverage and efficiency on clean test data.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of the impact of corrupted calibration data on constructed prediction sets.&lt;/li&gt;&lt;li&gt;Proposes a modification called Contamination Robust Conformal Prediction for the classification setting and empirically validates it on synthetic and real datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jase Clarkson', 'Wenkai Xu', 'Mihai Cucuringu', 'Yvik Swan', 'Gesine Reinert']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'conformal prediction', 'distribution shift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.07700</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning</title><link>https://arxiv.org/abs/2511.20591</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces attention trajectories: aggregating saliency at object and modality levels to form time-evolving attention profiles during RL training.&lt;/li&gt;&lt;li&gt;Applies method to Atari, custom Pong, and biomechanical visuomotor tasks to reveal algorithm-specific attention biases, unintended reward-driven strategies, and overfitting to redundant sensory channels.&lt;/li&gt;&lt;li&gt;Validates robustness across multiple saliency methods and links attention-profile patterns to measurable behavioral differences, positioning attention trajectories as a diagnostic axis for tracing feature reliance and uncovering hidden vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlotte Beylier', 'Hannah Selder', 'Arthur Fleig', 'Simon M. Hofmann', 'Nico Scherf']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'reinforcement-learning', 'robustness', 'safety-evaluation', 'saliency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20591</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning</title><link>https://arxiv.org/abs/2511.20509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-MicroAdam, a memory-efficient, sparsity-aware adaptive optimizer for differentially private training and fine-tuning.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees in stochastic non-convex optimization at O(1/√T) up to privacy-dependent constants.&lt;/li&gt;&lt;li&gt;Empirically outperforms existing adaptive DP optimizers and matches or exceeds DP-SGD on benchmarks including CIFAR-10, ImageNet, and private fine-tuning of pretrained transformers.&lt;/li&gt;&lt;li&gt;Demonstrates that adaptive optimization can improve performance and stability under differential privacy constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mihaela Hudi\\c{s}teanu', 'Nikita P. Kalinin', 'Edwige Cyffers']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'private training', 'optimizer', 'privacy-preserving ML', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20509</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MolEdit: Knowledge Editing for Multimodal Molecule Language Models</title><link>https://arxiv.org/abs/2511.12770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MolEdit, a framework for targeted knowledge editing in multimodal molecule language models using a Multi-Expert Knowledge Adapter and an Expertise-Aware Editing Switcher to apply edits only when inputs match stored edits.&lt;/li&gt;&lt;li&gt;Introduces MEBench, a benchmark measuring Reliability (accuracy of edits), Locality (preservation of unrelated knowledge), and Generality (robustness to rephrased queries).&lt;/li&gt;&lt;li&gt;Reports improvements on two MoLM backbones (up to 18.8% higher Reliability and 12.0% better Locality) and emphasizes correcting inaccuracies and resisting malicious manipulation while preserving unrelated molecular knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Lei', 'Patrick Soga', 'Yaochen Zhu', 'Yinhan He', 'Yushun Dong', 'Jundong Li']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge editing', 'multimodal models', 'model robustness', 'model maintenance', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.12770</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Composition and Alignment of Diffusion Models using Constrained Learning</title><link>https://arxiv.org/abs/2508.19104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified constrained optimization framework to perform alignment (fine-tuning to a reward) and composition (combining pretrained diffusion models) by enforcing reward constraints and proximity to pretrained models.&lt;/li&gt;&lt;li&gt;Provides theoretical characterization of solutions and develops a Lagrangian-based primal–dual training algorithm to approximate constrained solutions.&lt;/li&gt;&lt;li&gt;Demonstrates empirical results in image generation showing the aligned/composed diffusion models satisfy specified constraints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shervin Khalafi', 'Ignacio Hounie', 'Dongsheng Ding', 'Alejandro Ribeiro']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'diffusion-models', 'constrained-optimization', 'model-composition', 'safety-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19104</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning of Traffic State Estimation and Prediction</title><link>https://arxiv.org/abs/2507.17984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a machine unlearning paradigm for traffic state estimation and prediction (TSEP) that enables trained models to selectively forget privacy-sensitive, poisoned, or outdated data.&lt;/li&gt;&lt;li&gt;Motivated by privacy regulations like the 'right to be forgotten' and concerns about data freshness and cybersecurity in intelligent transportation systems.&lt;/li&gt;&lt;li&gt;Aims to improve trustworthiness and reliability of data-driven TSEP by allowing targeted removal of influences from specified data without retraining from scratch.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Wang (Jeff)', 'R. Tyrrell Rockafellar (Jeff)', 'Xuegang (Jeff)', 'Ban']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'data-poisoning', 'right-to-be-forgotten', 'traffic-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17984</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title><link>https://arxiv.org/abs/2507.06969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses the hypothesis-testing interpretation of DP (f-DP) to derive unified bounds on attack success for re-identification, attribute inference, and data reconstruction.&lt;/li&gt;&lt;li&gt;Shows these unified bounds are consistent across attack settings and tunable to different baseline risk levels.&lt;/li&gt;&lt;li&gt;Empirically demonstrates tighter risk estimates than ε-DP, Rényi DP, and concentrated DP, enabling less noise for the same risk and improved utility (e.g., higher text classification accuracy).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bogdan Kulynych', 'Juan Felipe Gomez', 'Georgios Kaissis', 'Jamie Hayes', 'Borja Balle', 'Flavio P. Calmon', 'Jean Louis Raisaro']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 're-identification', 'attribute-inference', 'data-reconstruction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06969</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback</title><link>https://arxiv.org/abs/2507.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an interactive visualization for RLHF that lets users explore and compare groups of agent behaviours via linked overview and comparison views.&lt;/li&gt;&lt;li&gt;Introduces an active learning method to suggest informative groups for human comparison, aiming to improve labeling efficiency.&lt;/li&gt;&lt;li&gt;Evaluated in six simulated robotics tasks (simulated human feedback), showing large improvement in final rewards (~69%) and reduced error rates.&lt;/li&gt;&lt;li&gt;Open-sources code intended to integrate into RLHF training loops to support human-AI alignment research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jan Kompatscher', 'Danqing Shi', 'Giovanna Varni', 'Tino Weinkauf', 'Antti Oulasvirta']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'human-AI alignment', 'active learning', 'interactive visualization', 'labeling efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04340</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards</title><link>https://arxiv.org/abs/2506.20520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes an off-policy REINFORCE variant for LLM fine-tuning where advantage is A = r - V and V is a tunable baseline that trades off emphasis on positive vs negative rewards.&lt;/li&gt;&lt;li&gt;Provides a theoretical policy improvement guarantee when the baseline V lower-bounds the expected reward, and argues off-policy updates should weight positive rewards more than negative ones.&lt;/li&gt;&lt;li&gt;Validates findings empirically in a stochastic bandit setting and via fine-tuning state-of-the-art LLMs on reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles Arnal', 'Ga\\"etan Narozniak', 'Vivien Cabannes', 'Yunhao Tang', 'Julia Kempe', 'Remi Munos']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'off-policy RL', 'REINFORCE', 'LLM fine-tuning', 'reward shaping']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20520</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title><link>https://arxiv.org/abs/2506.12484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of components for irreversible unlearning in language models, identifying which design choices matter.&lt;/li&gt;&lt;li&gt;Introduces Disruption Masking: only update weights where unlearning and retaining gradient signs match to avoid disruptive updates.&lt;/li&gt;&lt;li&gt;Shows the importance of normalizing unlearning gradients and using meta-learning; combines these into MUDMAN.&lt;/li&gt;&lt;li&gt;MUDMAN significantly improves robustness to recovery of dangerous capabilities, outperforming prior TAR method by ~40%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang', 'Miko{\\l}aj Kniejski', 'Marcel Windys']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LLM safety', 'robustness', 'meta-learning', 'adversarial recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12484</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title><link>https://arxiv.org/abs/2505.24592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a unified theoretical framework linking data augmentations to improved model robustness via loss-surface flatness and PAC generalization bounds.&lt;/li&gt;&lt;li&gt;Claims broad applicability across many augmentation methods and a variety of distribution shifts (corruptions, adversarial attacks, domain shifts).&lt;/li&gt;&lt;li&gt;Validates theory empirically on corruption and adversarial robustness benchmarks (CIFAR, ImageNet) and domain generalization datasets (PACS, OfficeHome).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weebum Yoo', 'Sung Whan Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'data augmentation', 'adversarial robustness', 'theory', 'domain generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24592</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2505.23448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an iterative training framework that augments an n-class classifier with an (n+1)-th "garbage" class initialized with Gaussian noise to represent outliers.&lt;/li&gt;&lt;li&gt;Uses network inversion after each epoch to reconstruct inputs corresponding to outputs assigned to the garbage class, then retrains to refine decision boundaries and reduce uncertainty.&lt;/li&gt;&lt;li&gt;Claims unified OOD detection and uncertainty estimation without needing external OOD datasets or post-hoc calibration; demonstrated on image inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pirzada Suhail', 'Rehna Afroz', 'Gouranga Bala', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['out-of-distribution-detection', 'uncertainty-estimation', 'network-inversion', 'robustness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23448</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title><link>https://arxiv.org/abs/2505.19194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Dynamic Curvature Estimation (DCE) to estimate decision-boundary curvature in a black-box setting, building on CGBA.&lt;/li&gt;&lt;li&gt;Demonstrates statistically that decision-boundary curvature correlates with adversarial robustness across classifiers.&lt;/li&gt;&lt;li&gt;Proposes a new query-efficient black-box attack, Curvature Dynamic Black-box Attack (CDBA), that leverages the estimated curvature to improve attack performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Sun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'black-box attack', 'decision boundary curvature', 'robustness evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19194</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</title><link>https://arxiv.org/abs/2503.22738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ShieldAgent, a guardrail agent that enforces explicit safety policies on other agents by extracting verifiable rules from policy documents and encoding them as action-based probabilistic rule circuits.&lt;/li&gt;&lt;li&gt;Given an agent's action trajectory, ShieldAgent retrieves relevant rule circuits and generates a shielding plan using a tool library and executable code for formal verification to block unsafe actions.&lt;/li&gt;&lt;li&gt;Introduces ShieldAgent-Bench (3K safety-related instruction/trajectory pairs collected via SOTA attacks across 6 web environments and 7 risk categories) and reports SOTA results: +11.3% average improvement, 90.1% recall, 64.7% fewer API queries, and 58.2% reduced inference time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaorun Chen', 'Mintong Kang', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Agent guardrails', 'Policy verification', 'Benchmarks', 'Adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.22738</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title><link>https://arxiv.org/abs/2501.01457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRR (Distillation-Reinforcement-Reasoning): distill LLM behavioral traces, train a lightweight Discriminative Model (DM), and use DM at inference to identify/reject suspicious reasoning steps.&lt;/li&gt;&lt;li&gt;Aims to overcome the introspection illusion of self-critique by providing external behavioral feedback that forces the LLM to explore alternative reasoning pathways without modifying the base model.&lt;/li&gt;&lt;li&gt;Reports significant improvements over prominent self-critique methods on multiple reasoning benchmarks; method is lightweight, annotation-free, and scalable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diji Yang', 'Linda Zeng', 'Kezhen Chen', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reasoning', 'safety/robustness', 'alignment', 'external critic', 'behavioral feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01457</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging</title><link>https://arxiv.org/abs/2511.23193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes co-training an adversarial fault-injection agent to generate observation perturbations that harden MARL policies for cooperative on-ramp merging.&lt;/li&gt;&lt;li&gt;Introduces a fault-tolerant vehicle agent with self-diagnosis that uses spatio-temporal correlations in state sequences to detect and reconstruct corrupted observations.&lt;/li&gt;&lt;li&gt;Evaluated in simulated highway merging scenarios, showing substantial improvements in safety and efficiency under various observation fault patterns compared to baseline MARL methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchen Shi', 'Huaxin Pei', 'Yi Zhang', 'Danya Yao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'multi-agent-reinforcement-learning', 'fault-injection', 'safety/robustness', 'observation-poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23193</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents</title><link>https://arxiv.org/abs/2511.22441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GEO-Detective, an LVLM-based agent that mimics human reasoning and tool use to infer image geolocation, including a visual reverse-search tool.&lt;/li&gt;&lt;li&gt;Shows improved geolocation performance over baseline LVLMs (≈11.1% country-level, ≈5.2% finer-grained) and reduces 'unknown' predictions when external clues are available.&lt;/li&gt;&lt;li&gt;Evaluates privacy risks from advanced LVLM geolocation and explores multiple defense strategies, reporting robustness results and highlighting gaps in current privacy safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Zhang', 'Yixin Wu', 'Boyang Zhang', 'Chenhao Lin', 'Chao Shen', 'Michael Backes', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'geolocation', 'large vision-language models', 'privacy defenses', 'adversarial tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22441</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unexplored flaws in multiple-choice VQA evaluations</title><link>https://arxiv.org/abs/2511.22341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies three previously unexplored prompt-formatting variation factors that affect multiple-choice VQA evaluations.&lt;/li&gt;&lt;li&gt;Performs a large-scale empirical study across 7 MLLMs, 5 VQA datasets, and 48 prompt-format variations to measure sensitivity.&lt;/li&gt;&lt;li&gt;Finds that minor, semantically-neutral formatting changes substantially alter VQA performance, independent of known answer-order biases or model confidence.&lt;/li&gt;&lt;li&gt;Shows that existing bias-mitigation strategies do not eliminate these newly identified formatting biases, calling into question current evaluation reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fabio Rosenthal', 'Sebastian Schmidt', 'Thorsten Graf', 'Thorsten Bagodonat', 'Stephan G\\"unnemann', 'Leo Schwinn']&lt;/li&gt;&lt;li&gt;Tags: ['prompt-formatting', 'evaluation-bias', 'robustness', 'VQA', 'multimodal-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22341</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs</title><link>https://arxiv.org/abs/2511.22270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes generalization and privacy performance of differentially private gradient descent (DP-GD) versus standard GD for training two-layer Huberized ReLU CNNs.&lt;/li&gt;&lt;li&gt;Proves a concrete setting where DP-GD achieves better test accuracy than GD under small signal-to-noise ratio, while also providing differential privacy guarantees.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis under mild conditions and supports findings with numerical simulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhongjie Shi', 'Puyu Wang', 'Chenyang Zhang', 'Yuan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'DP-GD', 'generalization', 'privacy-preserving ML', 'convolutional neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22270</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models</title><link>https://arxiv.org/abs/2511.22016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AfriStereo, an open-source dataset of African-context stereotypes (1,163 collected, augmented to &gt;5,000 stereotype–antistereotype pairs) grounded via community engagement in Senegal, Kenya, and Nigeria.&lt;/li&gt;&lt;li&gt;Data augmentation used few-shot prompting with human-in-the-loop validation, semantic clustering, and manual annotation by culturally informed reviewers.&lt;/li&gt;&lt;li&gt;Evaluates 11 language models and finds statistically significant biases in 9 models (Bias Preference Ratios 0.63–0.78), with stronger stereotypical preferences across age, profession, and gender.&lt;/li&gt;&lt;li&gt;Claims domain-specific models may show reduced bias in this setup and positions AfriStereo as a resource for culturally grounded bias evaluation and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yann Le Beux', 'Oluchi Audu', 'Oche D. Ankeli', 'Dhananjay Balakrishnan', 'Melissah Weya', 'Marie D. Ralaiarinosy', 'Ignatius Ezeani']&lt;/li&gt;&lt;li&gt;Tags: ['bias-dataset', 'fairness-evaluation', 'cultural-bias', 'NLP-dataset', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22016</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differential privacy from axioms</title><link>https://arxiv.org/abs/2511.21876</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves that any reasonable privacy measure satisfying four axioms (pre-processing invariance, prohibition of blatant non-privacy, strong composition, and linear scalability) is equivalent to differential privacy up to polynomial sample-complexity factors.&lt;/li&gt;&lt;li&gt;Shows the axioms are minimal: removing any one permits pathological privacy measures that avoid equivalence to DP.&lt;/li&gt;&lt;li&gt;Implication: attempts to replace DP with a weaker notion that still enjoys nontrivial composition and other natural desiderata are impossible in the statistical setting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guy Blanc', 'William Pires', 'Toniann Pitassi']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-theory', 'privacy-composition', 'privacy-axioms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21876</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LILAD: Learning In-context Lyapunov-stable Adaptive Dynamics Models</title><link>https://arxiv.org/abs/2511.21846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LILAD, a framework that jointly learns a dynamics model and a Lyapunov function via in-context learning to ensure both adaptability and stability.&lt;/li&gt;&lt;li&gt;At test time, both model and Lyapunov certificate adapt to a new system instance from a short trajectory prompt, enabling fast generalization under parametric uncertainty.&lt;/li&gt;&lt;li&gt;Introduces a state-dependent attenuator that enforces a sufficient decrease condition on the Lyapunov function to provide rigorous stability guarantees, including out-of-distribution scenarios.&lt;/li&gt;&lt;li&gt;Evaluated on benchmark autonomous systems, showing improved predictive accuracy over adaptive, robust, and non-adaptive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Jena', 'Na Li', 'Le Xie']&lt;/li&gt;&lt;li&gt;Tags: ['Lyapunov stability', 'adaptive system identification', 'robustness', 'safety-critical control', 'in-context learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21846</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy</title><link>https://arxiv.org/abs/2511.21804</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that commonly used add/remove adjacency (membership protection) can overstate privacy for per-record attributes (e.g., labels) compared to substitute adjacency.&lt;/li&gt;&lt;li&gt;Develops novel auditing attacks tailored to the substitute adjacency relation and empirically demonstrates a gap between reported DP guarantees under add/remove and actual attribute privacy.&lt;/li&gt;&lt;li&gt;Finds audit outcomes align with privacy accounting under substitute adjacency, highlighting that the choice of adjacency must match the intended protection target.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Pradhan', 'Joonas J\\"alk\\"o', 'Santiago Zanella-B\\`eguelin', 'Antti Honkela']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'privacy-auditing', 'adjacency-relations', 'attribute-privacy', 'privacy-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21804</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation</title><link>https://arxiv.org/abs/2511.21711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates stereotypes and biases in LLMs (BERT, GPT-3.5, ADA) using bias benchmarks StereoSet and CrowS-Pairs and a three-pronged detection approach for explicit and implicit biases.&lt;/li&gt;&lt;li&gt;Finds fine-tuned models struggle with gender bias but perform better at avoiding racial bias; observes models often over-rely on prompt keywords rather than true understanding.&lt;/li&gt;&lt;li&gt;Applies mitigation strategies (fine-tuning, prompt techniques, data augmentation) and reports up to 20% improvement on implicit bias benchmarks, with cross-dataset adaptability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatima Kazi']&lt;/li&gt;&lt;li&gt;Tags: ['bias mitigation', 'LLM safety', 'fairness', 'evaluation/benchmarking', 'prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21711</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Role of Preference Variance in Preference Optimization</title><link>https://arxiv.org/abs/2510.13022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how preference variance (PVar) — variance in model preference scores across response pairs — controls the magnitude of DPO gradient updates and thus the informativeness of prompts for preference optimization.&lt;/li&gt;&lt;li&gt;Provides a theoretical upper bound linking PVar to the DPO gradient norm, implying low-PVar prompts yield small learning signals.&lt;/li&gt;&lt;li&gt;Empirically shows that selecting high-PVar prompts (even using smaller reward models) improves fine-tuning performance on AlpacaEval 2.0 and Arena-Hard, and that training on the top 10% high-PVar prompts can outperform training on the full dataset with human annotations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Guo', 'Zihao Li', 'Jiahao Qiu', 'Yue Wu', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'data selection', 'reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13022</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation</title><link>https://arxiv.org/abs/2511.23440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Probabilistic Forward Pass (PFP), an analytic Gaussian-based approximation to Stochastic Variational Inference that produces uncertainty estimates in a single deterministic forward pass.&lt;/li&gt;&lt;li&gt;Implements Gaussian-propagating operators and compiles PFP-BNNs with TVM for optimized deployment on embedded ARM CPUs, achieving large speedups (up to 4200x for small mini-batches) versus sampling-based SVI.&lt;/li&gt;&lt;li&gt;Evaluates on image data (Dirty-MNIST), showing comparable accuracy, uncertainty estimation, and out-of-distribution (OOD) detection performance to SVI while greatly reducing compute cost.&lt;/li&gt;&lt;li&gt;Focuses on enabling efficient Bayesian models for safety-critical, resource-constrained systems through code generation and operator-level optimizations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bernhard Klein', 'Falk Selker', 'Hendrik Borras', 'Sophie Steger', 'Franz Pernkopf', 'Holger Fr\\"oning']&lt;/li&gt;&lt;li&gt;Tags: ['bayesian-neural-networks', 'uncertainty-estimation', 'out-of-distribution-detection', 'safety-critical-deployment', 'efficient-inference/code-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23440</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Training for Process Reward Models</title><link>https://arxiv.org/abs/2511.22888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarially Trained PRMs (APRM): a Generator G learns to produce reasoning-step errors to deceive a Process Reward Model R, while R learns to detect them.&lt;/li&gt;&lt;li&gt;Adversarial interaction yields progressively harder negative examples, enabling PRMs to improve robustness and generalization without costly manual step-level labels.&lt;/li&gt;&lt;li&gt;Empirical results show APRM improves solver accuracy by +3.4 percentage points on diverse math reasoning benchmarks and +5.3 pp on out-of-distribution tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gurusha Juneja', 'Deepak Nathani', 'William Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'alignment', 'LLM evaluation', 'process reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22888</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Difficulties with Evaluating a Deception Detector for AIs</title><link>https://arxiv.org/abs/2511.22662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues we currently lack reliable ground-truth examples (clearly deceptive vs. honest) needed to evaluate deception detectors for AI.&lt;/li&gt;&lt;li&gt;Identifies concrete conceptual and empirical obstacles to collecting such labeled instances, supported by analysis of prior work and illustrative case studies.&lt;/li&gt;&lt;li&gt;Evaluates several proposed empirical workarounds and argues they are useful but insufficient on their own.&lt;/li&gt;&lt;li&gt;Concludes that progress on deception detection requires further methodological and data-collection solutions addressing these fundamental problems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position/Analysis&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lewis Smith', 'Bilal Chughtai', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['deception-detection', 'AI-safety', 'evaluation-methodology', 'datasets', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22662</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers</title><link>https://arxiv.org/abs/2511.22616</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of federated learning with a multi-level taxonomy focused on aggregation techniques, architectures, synchronization methods, and federation objectives.&lt;/li&gt;&lt;li&gt;Combines bibliometric analysis and systematic review to identify influential works across personalization, optimization, and robustness.&lt;/li&gt;&lt;li&gt;Discusses challenges and techniques related to heterogeneity, efficiency, security, and privacy, and presents experiments comparing aggregation methods under IID and non-IID data.&lt;/li&gt;&lt;li&gt;Outlines practical evaluation approaches and future research directions to advance federated learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meriem Arbaoui', 'Mohamed-el-Amine Brahmia', 'Abdellatif Rahmoun', 'Mourad Zghal']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'aggregation', 'robustness', 'security-privacy', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22616</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Space Explanations of Neural Network Classification</title><link>https://arxiv.org/abs/2511.22498</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'Space Explanations', a logic-based concept that yields provable guarantees about a neural network's behavior over continuous regions of the input feature space.&lt;/li&gt;&lt;li&gt;Automatically generates these explanations using Craig interpolation algorithms and unsatisfiable core generation.&lt;/li&gt;&lt;li&gt;Evaluates on real-world case studies (small to large) and claims the generated explanations are more meaningful than state-of-the-art methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Faezeh Labbaf', "Tom\\'a\\v{s} Kol\\'arik", 'Martin Blicha', 'Grigory Fedyukovich', 'Michael Wand', 'Natasha Sharygina']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'robustness', 'safety', 'explainability', 'provable guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22498</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges</title><link>https://arxiv.org/abs/2511.22483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates the effect of quantization on four trustworthiness metrics: adversarial robustness, fairness, machine ethics, and out-of-distribution robustness, showing instability across compression ratios and methods.&lt;/li&gt;&lt;li&gt;Introduces a precision-ensemble voting approach that combines predictions from mixed-precision variants of the same model to improve trustworthiness metrics (up to 5.8% gains).&lt;/li&gt;&lt;li&gt;Argues that compression research should include safety/trustworthiness evaluations for deployment in high-stakes domains and highlights research opportunities at the intersection of compression and trust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanxi Lu', 'Hao Mark Chen', 'Zhiqiang Que', 'Wayne Luk', 'Hongxiang Fan']&lt;/li&gt;&lt;li&gt;Tags: ['quantization', 'robustness', 'fairness', 'trustworthiness', 'model compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22483</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions</title><link>https://arxiv.org/abs/2511.22406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces efficient numerical approximations for entropy, log-probability, and their gradients for truncated normal policy distributions under action constraints.&lt;/li&gt;&lt;li&gt;Provides an efficient sampling strategy for truncated stochastic policies to enable practical policy-gradient updates with constrained actions.&lt;/li&gt;&lt;li&gt;Validates the approach on three benchmarks, showing substantial performance gains when using accurate estimations versus naive (non-truncated) approximations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roland Stolz', 'Michael Eichelbeck', 'Matthias Althoff']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning', 'safety constraints', 'truncated distributions', 'policy gradients', 'sampling/estimation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22406</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning</title><link>https://arxiv.org/abs/2511.22265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedRE, which aggregates client local representations into a single 'entangled representation' using normalized random weights and creates corresponding entangled-label encodings for upload to the server.&lt;/li&gt;&lt;li&gt;Random weights are resampled each round to add diversity, reduce global classifier overconfidence, and promote smoother decision boundaries; clients upload a single cross-category entangled representation to cut communication overhead.&lt;/li&gt;&lt;li&gt;Claims privacy benefits by mitigating representation inversion attacks and demonstrates an empirical trade-off among model performance, privacy protection, and communication efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Yao', 'Lixu Wang', 'Jiaqi Wu', 'Jin Song', 'Simin Chen', 'Zehua Wang', 'Zijian Tian', 'Wei Chen', 'Huixia Li', 'Xiaoxiao Li']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy protection', 'representation inversion', 'model heterogeneity', 'communication efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22265</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Safety and Security Framework for Real-World Agentic Systems</title><link>https://arxiv.org/abs/2511.21990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dynamic, operational framework treating safety and security of agentic systems as emergent properties of interactions among models, orchestrators, tools, and data.&lt;/li&gt;&lt;li&gt;Defines an agentic risk taxonomy that unifies traditional safety/security concerns with agent-specific risks (e.g., tool misuse, cascading action chains, unintended control amplification).&lt;/li&gt;&lt;li&gt;Describes AI-driven, sandboxed red teaming assisted by auxiliary agents and human oversight, validates the approach via an enterprise case study (NVIDIA AI-Q) and releases a dataset of ~10,000 attack/defense traces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaona Ghosh', 'Barnaby Simkin', 'Kyriacos Shiarlis', 'Soumili Nandi', 'Dan Zhao', 'Matthew Fiedler', 'Julia Bazinska', 'Nikki Pope', 'Roopa Prabhu', 'Daniel Rohrer', 'Michael Demoret', 'Bartley Richardson']&lt;/li&gt;&lt;li&gt;Tags: ['agentic systems', 'red teaming', 'safety framework', 'tool misuse', 'risk taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21990</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions</title><link>https://arxiv.org/abs/2511.21952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ABLE (Adversarially Bracketed Local Explanation): generate neighborhood points via Gaussian noise, craft an adversarial example A that flips the label, then craft A' to flip it back—forming an adversarial pair that brackets the local decision boundary.&lt;/li&gt;&lt;li&gt;Train a simple linear model on these adversarial pairs to approximate the local decision boundary around a test instance, aiming to improve stability and local fidelity compared to methods like LIME.&lt;/li&gt;&lt;li&gt;Evaluated on six UCI benchmark (tabular) datasets across three deep neural network architectures; reports improved stability and fidelity over state-of-the-art local explanation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Krishna Khadka', 'Sunny Shree', 'Pujan Budhathoki', 'Yu Lei', 'Raghu Kacker', 'D. Richard Kuhn']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'explainability', 'local explanation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21952</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck</title><link>https://arxiv.org/abs/2511.21923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how backdoor data affects neural network training dynamics using the Information Bottleneck framework and mutual information (MI) signatures.&lt;/li&gt;&lt;li&gt;Identifies distinct MI evolution patterns across training phases and differences between target class and clean classes, varying by attack mechanism.&lt;/li&gt;&lt;li&gt;Reports that visually conspicuous attacks (e.g., BadNets) can appear more integrated (stealthy) from an information-theoretic perspective than some visually imperceptible attacks.&lt;/li&gt;&lt;li&gt;Proposes a dynamics-based stealthiness metric to quantify attack integration at the model level and validates findings across multiple datasets and attack types (code released).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Liu', 'Xu Zhang', 'Can Chen', 'Ren Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'information bottleneck', 'training dynamics', 'stealthiness metric', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21923</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings</title><link>https://arxiv.org/abs/2511.21893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a task-agnostic defense that reconstructs attacker-perturbed multi-modal inputs using generative models (e.g., VAEs) to restore natural cross-modal alignment.&lt;/li&gt;&lt;li&gt;Introduces generative sampling combined with a consensus-based aggregation over generated samples to improve robustness to imperceptible adversarial perturbations (adversarial illusions).&lt;/li&gt;&lt;li&gt;Reports experimental results showing near-zero attack success rates and substantial improvements in cross-modal alignment under both clean and perturbed conditions.&lt;/li&gt;&lt;li&gt;Claims the approach is model-agnostic and effective across state-of-the-art multi-modal encoders.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatemeh Akbarian', 'Anahita Baninajjar', 'Yingyi Zhang', 'Ananth Balashankar', 'Amir Aminifar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multi-modal embeddings', 'defense/generative reconstruction', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21893</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning</title><link>https://arxiv.org/abs/2511.21799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how the multiplicity of near-optimal models (Rashomon set) affects trustworthiness: sparse interpretable models offer privacy but are more fragile to adversarial attacks.&lt;/li&gt;&lt;li&gt;Shows that diversity in large Rashomon sets can enable reactive robustness—alternative models remain accurate when one is broken—while also increasing information leakage when multiple models are exposed.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical evaluations on sparse decision trees and linear models to characterize the robustness–privacy trade-off introduced by Rashomon sets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ethan Hsu', 'Harry Chen', 'Chudi Zhong', 'Lesia Semenova']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'privacy / information leakage', 'model diversity / Rashomon set', 'trustworthy ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21799</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGASA, a framework that synthesizes model-generated safety guidelines and uses them to adaptively strengthen defenses against adversarial jailbreak prompts.&lt;/li&gt;&lt;li&gt;Two-stage approach: Data Pre-synthesis (generate safety guidelines and augmented prompts) and Alignment Fine-tuning using Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO).&lt;/li&gt;&lt;li&gt;Aims to improve robustness to deceptive/adversarial prompts while reducing unnecessary refusals of benign requests.&lt;/li&gt;&lt;li&gt;Reports extensive experiments across multiple datasets showing improved model safety and adaptive, scalable effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak defense', 'adversarial prompts', 'alignment fine-tuning', 'DPO']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title><link>https://arxiv.org/abs/2511.20663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts classical reliability metrics (MTTR, MTBF) to define MTTR-A, a runtime measure of cognitive recovery latency for multi-agent systems.&lt;/li&gt;&lt;li&gt;Presents a benchmark simulation using AG News and LangGraph to measure recovery times across automated reflexes and human-approval interventions, reporting median MTTR-A ≈ 6.21s.&lt;/li&gt;&lt;li&gt;Derives reliability bounds linking cognitive recovery time to cognitive uptime, framing recovery latency as a standardized dependability metric for distributed reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Barak Or']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety evaluation', 'robustness', 'runtime monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.20663</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering</title><link>https://arxiv.org/abs/2511.19220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates whether state-of-the-art vision-language models genuinely use visual information for Italian medical VQA by replacing medical images with blank placeholders.&lt;/li&gt;&lt;li&gt;Tests four models (Claude Sonnet 4.5, GPT-4o, GPT-5-mini, Gemini 2.0 flash exp) on 60 EuropeMedQA Italian questions that require image interpretation.&lt;/li&gt;&lt;li&gt;Findings: GPT-4o shows the largest accuracy drop (27.9 percentage points) when images are removed; other models show smaller drops, yet all models sometimes produce confident but fabricated visual reasoning.&lt;/li&gt;&lt;li&gt;Highlights risks of hallucination and variable visual grounding, underscoring safety and robustness concerns for clinical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Federico Felizzi', 'Olivia Riccomi', 'Michele Ferramola', 'Francesco Andrea Causio', 'Manuel Del Medico', 'Vittorio De Vita', 'Lorenzo De Mori', 'Alessandra Piscitelli', 'Pietro Eric Risuleo', 'Bianca Destro Castaniti', 'Antonio Cristiano', 'Alessia Longo', 'Luigi De Angelis', 'Mariapia Vassalli', 'Marcello Di Pumpo']&lt;/li&gt;&lt;li&gt;Tags: ['medical VLMs', 'visual grounding', 'hallucination/confabulation', 'robustness/safety evaluation', 'clinical deployment risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.19220</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness</title><link>https://arxiv.org/abs/2511.15846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a graded Loss of Control (LoC) taxonomy (Deviation, Bounded LoC, Strict LoC) based on severity and persistence.&lt;/li&gt;&lt;li&gt;Models pathways toward societal vulnerability where advanced AI could cause Bounded or Strict LoC via catalysts (misalignment or malfunction).&lt;/li&gt;&lt;li&gt;Introduces the actionable DAP framework (Deployment context, Affordances, Permissions) and outlines governance and technical preparedness measures (threat modeling, deployment policies, testing, monitoring, emergency response).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charlotte Stix', 'Annika Hallensleben', 'Alejandro Ortega', 'Matteo Pistillo']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Loss of Control', 'Governance', 'Preparedness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.15846</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks</title><link>https://arxiv.org/abs/2511.11784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a negation-aware semantic consistency scoring method (NegBLEURT) to capture differences between successful (unsafe) and unsuccessful (safe) LLM responses.&lt;/li&gt;&lt;li&gt;Builds a detection framework (NegBLEURT Forest) that uses these scores with Isolation Forest anomaly detection to identify jailbreak/adversarial responses without threshold tuning or model fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluates on a crafted dataset across multiple models, showing top-tier, robust performance compared to competing approaches sensitive to model/data variations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lama Sleem', 'Jerome Francois', 'Lujun Li', 'Nathan Foucher', 'Niccolo Gentile', 'Radu State']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak detection', 'adversarial prompting', 'safety evaluation', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.11784</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification</title><link>https://arxiv.org/abs/2511.08905</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes iSeal, a fingerprinting method for LLM ownership verification that remains reliable even when an attacker fully controls the suspected model's inference pipeline.&lt;/li&gt;&lt;li&gt;Injects unique features into both the model and an external module and uses error-correction plus similarity-based verification to resist verification-time attacks (e.g., collusion-based fingerprint unlearning and response manipulation).&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and empirical evaluation showing strong robustness—100% Fingerprint Success Rate on 12 LLMs against 10+ attacks—where baseline methods fail under unlearning and response manipulation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixun Xiong', 'Gaoyi Wu', 'Qingyang Yu', 'Mingyu Derek Ma', 'Lingfeng Yao', 'Miao Pan', 'Xiaojiang Du', 'Hao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fingerprinting', 'model IP protection', 'adversarial robustness', 'fingerprint unlearning', 'verification-time attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08905</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title><link>https://arxiv.org/abs/2510.15859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORBIT, a rubric-based incremental RL training framework that uses synthetic dialogue generation and dynamically constructed rubrics to guide LLMs on open-ended, high-stakes medical consultations.&lt;/li&gt;&lt;li&gt;Replaces reliance on supervised reward models by using rubric-driven feedback judged by general-purpose instruction-following LLMs, reducing need for task-specific fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates large empirical gains (e.g., Qwen3-4B-Instruct HealthBench-Hard from 7.0 to 27.5 with 2k samples) and shows generality by applying the pipeline to InfoBench for improved instruction-following.&lt;/li&gt;&lt;li&gt;Positions rubric-guided RL as a mitigation against pathological RL behaviors (e.g., reward hacking) in ambiguous, context-dependent domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengkai Wang', 'Linus', 'Pengwei Liu', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'medical AI safety', 'reward-modeling / RLHF alternatives', 'robustness / reward hacking mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15859</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders</title><link>https://arxiv.org/abs/2509.21379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAEmnesia, a supervised sparse autoencoder that enforces one-to-one concept-to-neuron mappings in diffusion models to enable targeted concept erasure.&lt;/li&gt;&lt;li&gt;Achieves feature centralization, significantly reducing hyperparameter search (96.7% reduction) and improving performance on the UnlearnCanvas benchmark (9.2% over SOTA).&lt;/li&gt;&lt;li&gt;Demonstrates improved scalability for sequential unlearning (28.4% better accuracy when removing nine objects) and claims mitigation of unwanted content generation under adversarial attack, including effective removal of nudity (I2P evaluation).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Enrico Cassano', 'Riccardo Renzulli', 'Marco Nurisso', 'Mirko Zaffaroni', 'Alan Perotti', 'Marco Grangetto']&lt;/li&gt;&lt;li&gt;Tags: ['concept-unlearning', 'diffusion-models', 'model-editing', 'safety', 'adversarial-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21379</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality</title><link>https://arxiv.org/abs/2508.09185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CADAR, a neuro-symbolic framework that fuses pretrained vision-language representations into a perception graph capturing objects, relations, and temporal salience.&lt;/li&gt;&lt;li&gt;Uses a particle-filter-based probabilistic reasoning module to detect anomalies in semantic dynamics indicative of cognitive attacks in AR.&lt;/li&gt;&lt;li&gt;Claims interpretability (symbolic graph + probabilistic inference) combined with adaptability of modern V-L models and shows preliminary empirical gains on an AR cognitive-attack dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rongqian Chen', 'Allison Andreyev', 'Yanming Xiu', 'Joshua Chilukuri', 'Shunav Sen', 'Mahdi Imani', 'Bin Li', 'Maria Gorlatova', 'Gang Tan', 'Tian Lan']&lt;/li&gt;&lt;li&gt;Tags: ['AR security', 'cognitive attacks', 'neuro-symbolic', 'anomaly detection', 'vision-language']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09185</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs</title><link>https://arxiv.org/abs/2508.04182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that MLLMs disproportionately attend to task-irrelevant background regions, causing spurious background-answer correlations that drive hallucinations.&lt;/li&gt;&lt;li&gt;Argues outcome-based rewards can induce spurious correlations and proposes token-level sufficiency and necessity constraints to measure causal contribution of each generated token.&lt;/li&gt;&lt;li&gt;Introduces COPO (Causal-Oriented Policy Optimization): defines a causal completeness reward and integrates it into a causally informed advantage within the GRPO optimization framework to encourage evidence-grounded generation.&lt;/li&gt;&lt;li&gt;Reports empirical improvements on benchmarks, demonstrating reduced hallucinations and better grounding for multimodal generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peizheng Guo', 'Jingyao Wang', 'Wenwen Qiang', 'Jiahuan Zhou', 'Changwen Zheng', 'Gang Hua']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'alignment', 'causal-inference', 'policy-optimization', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04182</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning of Traffic State Estimation and Prediction</title><link>https://arxiv.org/abs/2507.17984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a machine unlearning paradigm for traffic state estimation and prediction (TSEP) that enables trained models to selectively forget privacy-sensitive, poisoned, or outdated data.&lt;/li&gt;&lt;li&gt;Motivated by privacy regulations like the 'right to be forgotten' and concerns about data freshness and cybersecurity in intelligent transportation systems.&lt;/li&gt;&lt;li&gt;Aims to improve trustworthiness and reliability of data-driven TSEP by allowing targeted removal of influences from specified data without retraining from scratch.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Wang (Jeff)', 'R. Tyrrell Rockafellar (Jeff)', 'Xuegang (Jeff)', 'Ban']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'data-poisoning', 'right-to-be-forgotten', 'traffic-ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17984</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title><link>https://arxiv.org/abs/2507.06969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses the hypothesis-testing interpretation of DP (f-DP) to derive unified bounds on attack success for re-identification, attribute inference, and data reconstruction.&lt;/li&gt;&lt;li&gt;Shows these unified bounds are consistent across attack settings and tunable to different baseline risk levels.&lt;/li&gt;&lt;li&gt;Empirically demonstrates tighter risk estimates than ε-DP, Rényi DP, and concentrated DP, enabling less noise for the same risk and improved utility (e.g., higher text classification accuracy).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bogdan Kulynych', 'Juan Felipe Gomez', 'Georgios Kaissis', 'Jamie Hayes', 'Borja Balle', 'Flavio P. Calmon', 'Jean Louis Raisaro']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 're-identification', 'attribute-inference', 'data-reconstruction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06969</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title><link>https://arxiv.org/abs/2506.12484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of components for irreversible unlearning in language models, identifying which design choices matter.&lt;/li&gt;&lt;li&gt;Introduces Disruption Masking: only update weights where unlearning and retaining gradient signs match to avoid disruptive updates.&lt;/li&gt;&lt;li&gt;Shows the importance of normalizing unlearning gradients and using meta-learning; combines these into MUDMAN.&lt;/li&gt;&lt;li&gt;MUDMAN significantly improves robustness to recovery of dangerous capabilities, outperforming prior TAR method by ~40%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang', 'Miko{\\l}aj Kniejski', 'Marcel Windys']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'LLM safety', 'robustness', 'meta-learning', 'adversarial recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12484</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title><link>https://arxiv.org/abs/2506.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IVY-FAKE, a large-scale multimodal benchmark (106K+ training, 5K verified eval) for explainable detection of AI-generated images and videos with rich, fine-grained annotations.&lt;/li&gt;&lt;li&gt;Proposes Ivy-xDetector, an RL-based detector using Group Relative Policy Optimization (GRPO) that produces explainable reasoning chains for localization and explanation of forgeries.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical gains on multiple synthetic-content benchmarks (e.g., GenImage accuracy improved from 86.88% to 96.32%) and emphasizes explainability and dataset diversity/quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changjiang Jiang', 'Wenhui Dong', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng', 'Xinbin Yuan', 'Yifei Bi', 'Ming Zhao', 'Zian Zhou', 'Caifeng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'Explainability', 'Benchmark', 'Multimodal', 'Forgery detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00979</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title><link>https://arxiv.org/abs/2505.24592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a unified theoretical framework linking data augmentations to improved model robustness via loss-surface flatness and PAC generalization bounds.&lt;/li&gt;&lt;li&gt;Claims broad applicability across many augmentation methods and a variety of distribution shifts (corruptions, adversarial attacks, domain shifts).&lt;/li&gt;&lt;li&gt;Validates theory empirically on corruption and adversarial robustness benchmarks (CIFAR, ImageNet) and domain generalization datasets (PACS, OfficeHome).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weebum Yoo', 'Sung Whan Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'data augmentation', 'adversarial robustness', 'theory', 'domain generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24592</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title><link>https://arxiv.org/abs/2505.19194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Dynamic Curvature Estimation (DCE) to estimate decision-boundary curvature in a black-box setting, building on CGBA.&lt;/li&gt;&lt;li&gt;Demonstrates statistically that decision-boundary curvature correlates with adversarial robustness across classifiers.&lt;/li&gt;&lt;li&gt;Proposes a new query-efficient black-box attack, Curvature Dynamic Black-box Attack (CDBA), that leverages the estimated curvature to improve attack performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Sun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'black-box attack', 'decision boundary curvature', 'robustness evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19194</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Superimposed Noise Accumulation Problem in Sequential Knowledge Editing of Large Language Models</title><link>https://arxiv.org/abs/2505.07899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the 'superimposed noise accumulation' issue in sequential knowledge editing of LLMs: editing success degrades as number of edits increases.&lt;/li&gt;&lt;li&gt;Analyzes causes: erroneous activation of irrelevant knowledge and conflicts between activated knowledge.&lt;/li&gt;&lt;li&gt;Proposes DeltaEdit, which applies dynamic orthogonal constraint strategies to reduce conflicts between edited knowledge.&lt;/li&gt;&lt;li&gt;Reports empirical improvement: DeltaEdit reduces noise and yields a 16.8% editing-performance gain over the strongest baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ding Cao', 'Yuchen Cai', 'Yuqing Huang', 'Xuesong He', 'Rongxi Guo', 'Guiquan Liu', 'Guangzhong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['knowledge-editing', 'LLM-robustness', 'model-editing', 'alignment', 'continual-editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07899</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Frontier AI's Impact on the Cybersecurity Landscape</title><link>https://arxiv.org/abs/2504.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive analysis (benchmarks, literature review, empirical evaluation, expert survey) of how frontier AI (agents and foundation models) affects cybersecurity.&lt;/li&gt;&lt;li&gt;Finds that AI currently advantages attackers more than defenders; empirical tests show AI agents struggle with defensive tasks requiring workflow planning and domain-specific tool use.&lt;/li&gt;&lt;li&gt;Expert survey indicates consensus that attacker advantage will persist though may narrow; authors provide concrete calls to action (new benchmarks, defensive AI agents, provably secure agents, pre-deployment testing, transparency, user education).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujin Potter', 'Wenbo Guo', 'Zhun Wang', 'Tianneng Shi', 'Hongwei Li', 'Andy Zhang', 'Patrick Gage Kelley', 'Kurt Thomas', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'AI agents', 'benchmarks', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.05408</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title><link>https://arxiv.org/abs/2504.02821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Sparse Autoencoders (SAEs) to vision-language models (e.g., CLIP) to improve neuron-level monosemanticity in visual representations.&lt;/li&gt;&lt;li&gt;Introduces a human-aligned benchmark for measuring monosemanticity derived from a large-scale user study.&lt;/li&gt;&lt;li&gt;Shows SAEs (especially with sparsity and wide latents) increase monosemanticity and that intervening on CLIP's vision encoder can steer multimodal LLM outputs (e.g., LLaVA) without changing the language model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'alignment', 'safety', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02821</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title><link>https://arxiv.org/abs/2501.01457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRR (Distillation-Reinforcement-Reasoning): distill LLM behavioral traces, train a lightweight Discriminative Model (DM), and use DM at inference to identify/reject suspicious reasoning steps.&lt;/li&gt;&lt;li&gt;Aims to overcome the introspection illusion of self-critique by providing external behavioral feedback that forces the LLM to explore alternative reasoning pathways without modifying the base model.&lt;/li&gt;&lt;li&gt;Reports significant improvements over prominent self-critique methods on multiple reasoning benchmarks; method is lightweight, annotation-free, and scalable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Diji Yang', 'Linda Zeng', 'Kezhen Chen', 'Yi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM reasoning', 'safety/robustness', 'alignment', 'external critic', 'behavioral feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01457</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent gridworld-based AI safety benchmarks</title><link>https://arxiv.org/abs/2410.00081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces biologically- and economically-motivated themes for AI alignment benchmarks: homeostasis, diminishing returns, sustainability, and resource sharing.&lt;/li&gt;&lt;li&gt;Implements eight multi-objective, multi-agent gridworld environments to test agents on bounded biological objectives and unbounded instrumental/business objectives.&lt;/li&gt;&lt;li&gt;Illustrates key safety/alignment failure modes such as unbounded optimization of homeostatic objectives, sacrificing other objectives, ignoring safety constraints, and depleting shared resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roland Pihlakas']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Multi-agent', 'Benchmarks', 'Resource sharing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.00081</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Reasoning in Ambiguous Contexts</title><link>https://arxiv.org/abs/2506.12241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LMs' ability to reason about appropriate information disclosure under ambiguous or missing context—key for agentic privacy decisions.&lt;/li&gt;&lt;li&gt;Introduces Camber, a framework that uses model-generated rationales to identify and systematically disambiguate context.&lt;/li&gt;&lt;li&gt;Shows that disambiguation improves privacy assessment metrics substantially (up to +13.3% precision, +22.3% recall) and reduces prompt sensitivity.&lt;/li&gt;&lt;li&gt;Argues that context disambiguation is a promising defense/mitigation strategy for improving agentic privacy reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ren Yi', 'Octavian Suciu', 'Adria Gascon', 'Sarah Meiklejohn', 'Eugene Bagdasarian', 'Marco Gruteser']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'agentic privacy', 'model alignment', 'context disambiguation', 'LLM reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12241</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RvLLM: LLM Runtime Verification with Domain Knowledge</title><link>https://arxiv.org/abs/2505.18585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ESL, a general specification language for domain experts to encode domain-specific predicates for LLM output verification.&lt;/li&gt;&lt;li&gt;Introduces RvLLM, a runtime verification framework that checks LLM outputs against ESL constraints to detect misbehavior or erroneous outputs.&lt;/li&gt;&lt;li&gt;Evaluates the approach on three tasks (legal statute violations, numerical comparison, inequality solving) showing effective error/violation detection across multiple LLMs.&lt;/li&gt;&lt;li&gt;Positions domain-knowledge-driven runtime checks as a lightweight, flexible method to improve reliability of LLM outputs in high-stakes domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yedi Zhang', 'Sun Yi Emma', 'Annabelle Lee Jia En', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'runtime verification', 'specification language', 'misbehavior detection', 'domain knowledge integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18585</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models</title><link>https://arxiv.org/abs/2412.01784</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method to detect ‘sandbagging’ (strategic underperformance) by injecting varying magnitudes of noise into model weights.&lt;/li&gt;&lt;li&gt;Finds non-sandbagging models predictably degrade under noise, while sandbagging models can anomalously improve—indicating disruption of underperformance mechanisms.&lt;/li&gt;&lt;li&gt;Validates the signal across architectures, sizes, and sandbagging techniques and demonstrates noise injection can elicit full performance from a sandbagging instance of Mistral Large 120B.&lt;/li&gt;&lt;li&gt;Proposes a practical, model-agnostic tool for AI evaluation and oversight to reveal hidden capabilities that undermine accurate capability assessments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cameron Tice', 'Philipp Alexander Kreer', 'Nathan Helm-Burger', 'Prithviraj Singh Shahani', 'Fedor Ryzhenkov', 'Jacob Haimes', 'Felix Hofst\\"atter', 'Teun van der Weij']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'sandbagging detection', 'robustness testing', 'safety evaluation', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01784</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities</title><link>https://arxiv.org/abs/2511.23408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple LLMs (e.g., GPT variants, LLaMA, Mistral, DeepSeek, Mistral) for one-shot automated vulnerability patching on both real and artificially injected vulnerabilities.&lt;/li&gt;&lt;li&gt;Uses Proof-of-Vulnerability (PoV) test execution to concretely verify whether LLM-generated patches actually fix vulnerabilities.&lt;/li&gt;&lt;li&gt;Finds that LLMs patch real vulnerabilities more effectively than artificial ones and shows variability in overlap and complementarity across models (some vulnerabilities patched only by specific LLMs).&lt;/li&gt;&lt;li&gt;Highlights implications for selecting or combining LLMs to maximize patch coverage in automated security workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayush Garg', 'Zanis Ali Khan', 'Renzo Degiovanni', 'Qiang Tang']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability patching', 'LLM evaluation', 'software security', 'PoV testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23408</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robust HRRP Recognition under Interrupted Sampling Repeater Jamming using a Prior Jamming Information-Guided Network</title><link>https://arxiv.org/abs/2511.23256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses robust high-resolution range profile (HRRP) target recognition under interrupted-sampling repeater jamming (ISRJ), which distorts HRRP features.&lt;/li&gt;&lt;li&gt;Introduces a point spread function (PSF) as prior jamming information to model HRRP distortion induced by ISRJ.&lt;/li&gt;&lt;li&gt;Proposes a recognition network that incorporates the PSF via a prior-guided feature interaction module and a hybrid loss to learn invariant features across varying jamming parameters.&lt;/li&gt;&lt;li&gt;Shows improved performance and generalization on simulated and measured data, outperforming prior methods on unseen jamming configurations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guozheng Sun', 'Lei Wang', 'Yanhao Wang', 'Jie Wang', 'Yimin Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-ML', 'signal-processing', 'electronic-warfare', 'prior-guided-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23256</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT</title><link>https://arxiv.org/abs/2511.23252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hyb-Agg, a hybrid secure aggregation protocol combining Multi-Key CKKS homomorphic encryption with ECDH-based additive masking to enable one-shot, non-interactive client-to-server transmissions per FL round.&lt;/li&gt;&lt;li&gt;Provides formal security under RLWE, CDH, and random-oracle assumptions and claims robustness against server collusion and up to N-2 colluding clients.&lt;/li&gt;&lt;li&gt;Implements and evaluates the protocol on resource-constrained devices (Raspberry Pi 4), reporting sub-second execution and a constant communication expansion of ~12x over plaintext, targeting practical IoT deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Imraul Emmaka', 'Tran Viet Xuan Phuong']&lt;/li&gt;&lt;li&gt;Tags: ['secure aggregation', 'federated learning', 'homomorphic encryption', 'privacy-preserving ML', 'IoT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23252</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</title><link>https://arxiv.org/abs/2511.23158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces REVEAL-Bench, a multimodal benchmark for explainable AI-generated image detection structured around explicit chains-of-evidence from lightweight expert models.&lt;/li&gt;&lt;li&gt;Proposes REVEAL, a forensic framework that uses expert-grounded reinforcement learning to jointly optimize detection accuracy, explanation fidelity, and logical coherence.&lt;/li&gt;&lt;li&gt;Produces step-by-step reasoning traces and verifiable evidential justifications alongside detection outputs to improve interpretability and cross-model generalization.&lt;/li&gt;&lt;li&gt;Reports improved detection accuracy, explanation fidelity, and robust cross-model performance compared to prior explainable forensics methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huangsen Cao', 'Qin Mei', 'Zhiheng Li', 'Yuxi Li', 'Ying Zhang', 'Chen Li', 'Zhimeng Zhang', 'Xin Ding', 'Yongwei Wang', 'Jing Lyu', 'Fei Wu']&lt;/li&gt;&lt;li&gt;Tags: ['image forensic detection', 'explainability', 'benchmark', 'robustness/generalization', 'reinforcement learning for explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23158</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation</title><link>https://arxiv.org/abs/2511.23066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses impact of foundation-model generative inpainting (gpt-image-1) on clinical tasks using 200 pediatric hand radiographs from RSNA Bone Age dataset and 600 inpainted variants.&lt;/li&gt;&lt;li&gt;Evaluates downstream bone age estimation (MAE) and gender classification (AUC) with deep-learning ensembles, and inspects pixel-intensity distributions for structural changes.&lt;/li&gt;&lt;li&gt;Finds large performance degradation after inpainting: bone age MAE rose from 6.26 to 30.11 months and gender AUC fell from 0.955 to 0.704; inpainted images showed pixel-intensity shifts and structural inconsistencies.&lt;/li&gt;&lt;li&gt;Concludes that visually realistic inpainting can obscure subtle diagnostic features and introduce latent bias, underscoring need for rigorous task-specific validation before clinical use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felipe Akio Matsuoka', 'Eduardo Moreno J. M. Farina', 'Augusto Sarquis Serpa', 'Soraya Monteiro', 'Rodrigo Ragazzini', 'Nitamar Abdala', 'Marcelo Straus Takahashi', 'Felipe Campos Kitamura']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'medical-imaging', 'generative-inpainting', 'bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23066</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis</title><link>https://arxiv.org/abs/2511.22990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MIMM-X, a method to disentangle causal features from multiple spurious correlations by minimizing mutual information between causal and spurious feature sets.&lt;/li&gt;&lt;li&gt;Evaluated on three medical imaging datasets (UK Biobank, NAKO, CheXpert) across MRI and X-ray modalities and shows improved mitigation of shortcut learning.&lt;/li&gt;&lt;li&gt;Aims to improve model generalization and reliability by encouraging predictions based on causal relationships rather than dataset-specific shortcuts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Louisa Fay', 'Hajer Reguigui', 'Bin Yang', 'Sergios Gatidis', 'Thomas K\\"ustner']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'shortcut learning', 'medical imaging', 'causal representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22990</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AgentShield: Make MAS more secure and efficient</title><link>https://arxiv.org/abs/2511.22924</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentShield, a decentralized auditing framework to defend LLM-based multi-agent systems against compromised/adversarial agents.&lt;/li&gt;&lt;li&gt;Design consists of three layers: Critical Node Auditing (topology-based prioritization), Light Token Auditing (cascade with lightweight sentry models), and Two-Round Consensus Auditing (heavy arbiters used only on uncertainty).&lt;/li&gt;&lt;li&gt;Claims substantial empirical gains: ~92.5% recovery rate and &gt;70% reduction in auditing overhead compared to prior methods across diverse MAS topologies and adversarial scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Wang', 'Zhaojiacheng Zhou', 'Bunyod Suvonov', 'Jiong Lou', 'Jie LI']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'adversarial attacks', 'defense mechanisms', 'decentralized auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22924</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Training for Process Reward Models</title><link>https://arxiv.org/abs/2511.22888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarially Trained PRMs (APRM): a generator G produces step-level reasoning errors to fool a process reward model R, and R is trained to detect them.&lt;/li&gt;&lt;li&gt;Adversarial loop creates progressively harder negatives, removing need for manual step-level labels and improving PRM robustness and generalization to novel errors.&lt;/li&gt;&lt;li&gt;Empirical gains: +3.4 percentage points average solver accuracy over strongest PRM baseline, and +5.3 pp on out-of-distribution mathematical reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gurusha Juneja', 'Deepak Nathani', 'William Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'robustness', 'process-reward-models', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22888</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distracted Robot: How Visual Clutter Undermine Robotic Manipulation</title><link>https://arxiv.org/abs/2511.22780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a psychophysics-inspired unified clutter measure capturing environmental factors, distractor quantity, characteristics, and arrangement for robotic manipulation scenes.&lt;/li&gt;&lt;li&gt;Presents an evaluation protocol and systematically constructed test scenarios in hyper-realistic simulation and real-world to assess manipulation policies, focusing on vision-language-action (VLA) models.&lt;/li&gt;&lt;li&gt;Demonstrates substantial performance degradation (up to ~34%) due to clutter, finds different VLA policies have unique vulnerabilities and low agreement on successful scenarios, and analyzes effects of distractor quantity and occlusion.&lt;/li&gt;&lt;li&gt;Shows finetuning on augmented data partially mitigates issues but does not equally remedy all clutter-induced failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amir Rasouli', 'Montgomery Alban', 'Sajjad Pakdamansavoji', 'Zhiyuan Li', 'Zhanguang Zhang', 'Aaron Wu', 'Xuan Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness', 'Robot manipulation', 'Vision-language-action', 'Safety evaluation', 'Evaluation benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22780</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Robotic Manipulation Robustness via NICE Scene Surgery</title><link>https://arxiv.org/abs/2511.22777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NICE (Naturalistic Inpainting for Context Enhancement), a pipeline that uses image generative models and LLMs to edit demonstration scenes (object replacement, restyling, and removal of distractors) to increase visual diversity without new robot data collection.&lt;/li&gt;&lt;li&gt;Uses the synthesized data to finetune vision-language affordance predictors and vision-language-action policies, improving robustness to visual distractors in downstream manipulation tasks.&lt;/li&gt;&lt;li&gt;Reports empirical gains: +20% accuracy in affordance prediction in cluttered scenes, +11% average manipulation success in distractor-populated tests, 6% lower target confusion, and 7% reduced collision rate.&lt;/li&gt;&lt;li&gt;Method focuses on minimizing OOD gaps and improving safety/robustness via data augmentation rather than on adversarial attacks, threat modeling, or red teaming.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajjad Pakdamansavoji', 'Mozhgan Pourkeshavarz', 'Adam Sigal', 'Zhiyuan Li', 'Rui Heng Yang', 'Amir Rasouli']&lt;/li&gt;&lt;li&gt;Tags: ['robotic manipulation', 'robustness', 'visual OOD/data augmentation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22777</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance</title><link>https://arxiv.org/abs/2511.22773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CAPE: a context-aware diffusion-based policy that iteratively seeds priors and guided denoising to expand trajectory mode support for better collision avoidance.&lt;/li&gt;&lt;li&gt;Method perturbs and refines the remaining trajectory segment after executing a short prefix, creating context-aware priors that preserve task intent while exploring collision-free modes.&lt;/li&gt;&lt;li&gt;Demonstrates improved generalization in unseen cluttered simulated and real-world manipulation tasks, reporting up to ~26% (simulated) and ~80% (real) higher success rates over prior SOTA.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Heng Yang', 'Xuan Zhao', 'Leo Maxime Brunswic', 'Montgomery Alban', 'Mateo Clemente', 'Tongtong Cao', 'Jun Jin', 'Amir Rasouli']&lt;/li&gt;&lt;li&gt;Tags: ['robotics safety', 'collision avoidance', 'diffusion models', 'robustness', 'generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22773</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents</title><link>https://arxiv.org/abs/2511.22441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GEO-Detective, an LVLM-based agent that mimics human reasoning and tool use to infer image geolocation, including a visual reverse-search tool.&lt;/li&gt;&lt;li&gt;Shows improved geolocation performance over baseline LVLMs (≈11.1% country-level, ≈5.2% finer-grained) and reduces 'unknown' predictions when external clues are available.&lt;/li&gt;&lt;li&gt;Evaluates privacy risks from advanced LVLM geolocation and explores multiple defense strategies, reporting robustness results and highlighting gaps in current privacy safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Zhang', 'Yixin Wu', 'Boyang Zhang', 'Chenhao Lin', 'Chao Shen', 'Michael Backes', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'geolocation', 'large vision-language models', 'privacy defenses', 'adversarial tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22441</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE</title><link>https://arxiv.org/abs/2511.22434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FastFHE, a set of techniques to accelerate fully homomorphic encryption (FHE) based CNN inference while preserving accuracy.&lt;/li&gt;&lt;li&gt;Key contributions: a scalable ciphertext data-packing scheme, depthwise-separable convolutions to reduce compute, BN dot-product fusion to avoid extra multiplicative depth, and low-degree Legendre polynomial approximation of SiLU activation.&lt;/li&gt;&lt;li&gt;Claims reduced time, storage, and bootstrapping overheads with experimental validation on encrypted CNN inference tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Song', 'Xinxin Fan', 'Quanliang Jing', 'Shaoye Luo', 'Wenqi Wei', 'Chi Lin', 'Yunfeng Lu', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['homomorphic encryption', 'secure inference', 'privacy-preserving machine learning', 'FHE optimization', 'CNN acceleration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22434</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text</title><link>https://arxiv.org/abs/2511.22153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid ensemble combining (i) RoBERTa classifier, (ii) GPT-2 probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical stylometric analyzer to detect LLM-generated text.&lt;/li&gt;&lt;li&gt;Introduces an optimized weighted voting framework where ensemble weights are learned on the probability simplex to directly maximize F1, rather than using heuristic weights.&lt;/li&gt;&lt;li&gt;Provides a bias-variance analysis and reports low inter-model correlation (rho ≈ 0.35–0.42), enabling variance reduction; empirically achieves 94.2% accuracy and AUC 0.978 on a 30k-document multi-generator corpus with 35% relative reduction in false positives on academic text.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepyan Purnama Kristanto', 'Lutfi Hakim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'text provenance', 'ensemble methods', 'false positive reduction', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22153</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks</title><link>https://arxiv.org/abs/2511.22147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies computation cost attacks against 3D Gaussian Splatting (3DGS) that cause resource exhaustion and DoS.&lt;/li&gt;&lt;li&gt;Proposes RemedyGS, a black-box defense pipeline with a detector for poisoned input textures and a purifier to recover benign images.&lt;/li&gt;&lt;li&gt;Incorporates adversarial training for the purifier to align recovered images with natural distribution, improving robustness.&lt;/li&gt;&lt;li&gt;Evaluates defense effectiveness against white-box, black-box, and adaptive attacks, showing improved safety and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanping Li', 'Zhening Liu', 'Zijian Li', 'Zehong Lin', 'Jun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['computation cost attacks', 'defense/mitigation', 'detection and purification', 'adversarial training', '3D reconstruction security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22147</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Distillability of LLM Security Logic: Predicting Attack Success Rate of Outline Filling Attack via Ranking Regression</title><link>https://arxiv.org/abs/2511.22044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies whether a lightweight proxy model can predict attack success rate (ASR) of black-box jailbreak prompts by distilling an LLM's security logic.&lt;/li&gt;&lt;li&gt;Introduces an improved outline filling attack to densely sample model safety boundaries for training data.&lt;/li&gt;&lt;li&gt;Proposes a ranking regression paradigm to train the proxy to predict which prompt yields higher ASR rather than absolute ASR values.&lt;/li&gt;&lt;li&gt;Reports proxy performance: 91.1% accuracy predicting relative ranking of average long response (ALR) and 69.2% accuracy predicting ASR, enabling optimized black-box attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyu Zhang', 'Zihang Xi', 'Jingyu Hua', 'Sheng Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model distillation', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22044</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models</title><link>https://arxiv.org/abs/2511.22016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AfriStereo, an open-source dataset of African-context stereotypes (1,163 collected, augmented to &gt;5,000 stereotype–antistereotype pairs) grounded via community engagement in Senegal, Kenya, and Nigeria.&lt;/li&gt;&lt;li&gt;Data augmentation used few-shot prompting with human-in-the-loop validation, semantic clustering, and manual annotation by culturally informed reviewers.&lt;/li&gt;&lt;li&gt;Evaluates 11 language models and finds statistically significant biases in 9 models (Bias Preference Ratios 0.63–0.78), with stronger stereotypical preferences across age, profession, and gender.&lt;/li&gt;&lt;li&gt;Claims domain-specific models may show reduced bias in this setup and positions AfriStereo as a resource for culturally grounded bias evaluation and mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yann Le Beux', 'Oluchi Audu', 'Oche D. Ankeli', 'Dhananjay Balakrishnan', 'Melissah Weya', 'Marie D. Ralaiarinosy', 'Ignatius Ezeani']&lt;/li&gt;&lt;li&gt;Tags: ['bias-dataset', 'fairness-evaluation', 'cultural-bias', 'NLP-dataset', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22016</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Safety and Security Framework for Real-World Agentic Systems</title><link>https://arxiv.org/abs/2511.21990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dynamic, operational framework treating safety and security of agentic systems as emergent properties of interactions among models, orchestrators, tools, and data.&lt;/li&gt;&lt;li&gt;Defines an agentic risk taxonomy that unifies traditional safety/security concerns with agent-specific risks (e.g., tool misuse, cascading action chains, unintended control amplification).&lt;/li&gt;&lt;li&gt;Describes AI-driven, sandboxed red teaming assisted by auxiliary agents and human oversight, validates the approach via an enterprise case study (NVIDIA AI-Q) and releases a dataset of ~10,000 attack/defense traces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaona Ghosh', 'Barnaby Simkin', 'Kyriacos Shiarlis', 'Soumili Nandi', 'Dan Zhao', 'Matthew Fiedler', 'Julia Bazinska', 'Nikki Pope', 'Roopa Prabhu', 'Daniel Rohrer', 'Michael Demoret', 'Bartley Richardson']&lt;/li&gt;&lt;li&gt;Tags: ['agentic systems', 'red teaming', 'safety framework', 'tool misuse', 'risk taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21990</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Risk-Adjusted Intelligence Dividend: A Quantitative Framework for Measuring AI Return on Investment Integrating ISO 42001 and Regulatory Exposure</title><link>https://arxiv.org/abs/2511.21975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a quantitative financial framework to compute risk-adjusted ROI for AI projects by integrating productivity gains with probabilistic costs from AI-specific threats (model drift, bias litigation, adversarial attacks, compliance failures).&lt;/li&gt;&lt;li&gt;Uses established risk quantification techniques (annual loss expectancy, Monte Carlo simulation) and models control effectiveness, reserve requirements, and ongoing operational maintenance costs.&lt;/li&gt;&lt;li&gt;Provides practical guidance on governance, phased validation, and incorporating risk-adjusted metrics into capital allocation to meet fiduciary and regulatory obligations (e.g., EU AI Act, ISO/IEC 42001).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hernan Huwyler']&lt;/li&gt;&lt;li&gt;Tags: ['risk-assessment', 'adversarial-attacks', 'regulatory-compliance', 'model-robustness', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21975</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions</title><link>https://arxiv.org/abs/2511.21952</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ABLE (Adversarially Bracketed Local Explanation): generate neighborhood points via Gaussian noise, craft an adversarial example A that flips the label, then craft A' to flip it back—forming an adversarial pair that brackets the local decision boundary.&lt;/li&gt;&lt;li&gt;Train a simple linear model on these adversarial pairs to approximate the local decision boundary around a test instance, aiming to improve stability and local fidelity compared to methods like LIME.&lt;/li&gt;&lt;li&gt;Evaluated on six UCI benchmark (tabular) datasets across three deep neural network architectures; reports improved stability and fidelity over state-of-the-art local explanation methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Krishna Khadka', 'Sunny Shree', 'Pujan Budhathoki', 'Yu Lei', 'Raghu Kacker', 'D. Richard Kuhn']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'explainability', 'local explanation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21952</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck</title><link>https://arxiv.org/abs/2511.21923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how backdoor data affects neural network training dynamics using the Information Bottleneck framework and mutual information (MI) signatures.&lt;/li&gt;&lt;li&gt;Identifies distinct MI evolution patterns across training phases and differences between target class and clean classes, varying by attack mechanism.&lt;/li&gt;&lt;li&gt;Reports that visually conspicuous attacks (e.g., BadNets) can appear more integrated (stealthy) from an information-theoretic perspective than some visually imperceptible attacks.&lt;/li&gt;&lt;li&gt;Proposes a dynamics-based stealthiness metric to quantify attack integration at the model level and validates findings across multiple datasets and attack types (code released).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Liu', 'Xu Zhang', 'Can Chen', 'Ren Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'information bottleneck', 'training dynamics', 'stealthiness metric', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21923</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance</title><link>https://arxiv.org/abs/2511.21901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the AI System Threat Vector Taxonomy covering nine domains (Misuse, Poisoning, Privacy, Adversarial, Biases, Unreliable Outputs, Drift, Supply Chain, IP Threat) and 53 operational sub-threats.&lt;/li&gt;&lt;li&gt;Maps technical threat vectors to business loss categories (Confidentiality, Integrity, Availability, Legal, Reputation) to enable Quantitative Risk Assessment and financial impact estimation for controls/insurance.&lt;/li&gt;&lt;li&gt;Empirically validates the taxonomy against 133 documented AI incidents (100% classification coverage) and aligns mappings to ISO/IEC 42001 controls and NIST AI RMF for auditability and regulatory use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hernan Huwyler']&lt;/li&gt;&lt;li&gt;Tags: ['AI security taxonomy', 'quantitative risk assessment', 'adversarial attacks', 'data poisoning', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21901</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems</title><link>https://arxiv.org/abs/2511.21877</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an event-chain-driven workflow that uses LLMs plus a Retrieval-Augmented Generation (RAG) layer to generate validated automotive code from natural-language requirements.&lt;/li&gt;&lt;li&gt;RAG retrieves relevant Vehicle Signal Specification (VSS) signals to reduce LLM hallucinations and ensure architectural correctness.&lt;/li&gt;&lt;li&gt;Maps and validates retrieved signals, transforms them into event chains encoding causal/timing constraints to guide LLM code synthesis.&lt;/li&gt;&lt;li&gt;Evaluated on an emergency braking case study showing valid signal usage and consistent code generation without retraining the LLM.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nenad Petrovic', 'Norbert Kroth', 'Axel Torschmied', 'Yinglei Song', 'Fengjunjie Pan', 'Vahid Zolfaghari', 'Nils Purschke', 'Sven Kirchner', 'Chengdong Wu', 'Andre Schamschurko', 'Yi Zhang', 'Alois Knoll']&lt;/li&gt;&lt;li&gt;Tags: ['LLM code generation', 'Safety-critical systems', 'RAG / hallucination mitigation', 'Automotive ADAS', 'Behavioral correctness / real-time constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21877</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices</title><link>https://arxiv.org/abs/2511.21860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Consistency-Rebalanced Accuracy (CoRA) to improve reliability of multiple-choice benchmark scores by accounting for LLM response consistency.&lt;/li&gt;&lt;li&gt;Introduces two intermediate scores—Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI)—and uses synthetically generated question variants with altered answer choices to measure consistency.&lt;/li&gt;&lt;li&gt;Demonstrates that models can show high MCQA accuracy but low consistency, and that CoRA downscales scores of inconsistent models across multiple benchmarks and LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulo Cavalin', 'Cassia Sanctos', 'Marcelo Grave', 'Claudio Pinhanez', 'Yago Primerano']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmarking', 'evaluation-metrics', 'robustness', 'LLM-behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21860</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LILAD: Learning In-context Lyapunov-stable Adaptive Dynamics Models</title><link>https://arxiv.org/abs/2511.21846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LILAD, a framework that jointly learns a dynamics model and a Lyapunov function via in-context learning to ensure both adaptability and stability.&lt;/li&gt;&lt;li&gt;At test time, both model and Lyapunov certificate adapt to a new system instance from a short trajectory prompt, enabling fast generalization under parametric uncertainty.&lt;/li&gt;&lt;li&gt;Introduces a state-dependent attenuator that enforces a sufficient decrease condition on the Lyapunov function to provide rigorous stability guarantees, including out-of-distribution scenarios.&lt;/li&gt;&lt;li&gt;Evaluated on benchmark autonomous systems, showing improved predictive accuracy over adaptive, robust, and non-adaptive baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Jena', 'Na Li', 'Le Xie']&lt;/li&gt;&lt;li&gt;Tags: ['Lyapunov stability', 'adaptive system identification', 'robustness', 'safety-critical control', 'in-context learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21846</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dark Speculation: Combining Qualitative and Quantitative Understanding in Frontier AI Risk Analysis</title><link>https://arxiv.org/abs/2511.21838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'dark speculation': combine qualitative scenario generation (detailed catastrophic narratives) with quantitative underwriting to produce probability distributions over extreme frontier-AI outcomes.&lt;/li&gt;&lt;li&gt;Formalizes the approach with a simplified catastrophic Lévy stochastic framework and an iterative institutional design: independent speculation, underwriting assigns probabilistic/financial parameters, decision-makers synthesize summary statistics.&lt;/li&gt;&lt;li&gt;Argues for maintaining independence between speculation and underwriting, analyzing multiple risk categories in parallel, and producing 'thick' narratives rich in causal and mitigative detail; adaptable and augmentable with AI tools.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Carpenter', 'Carson Ezell', 'Pratyush Mallick', 'Alexandria Westray']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Risk analysis', 'Scenario planning', 'Catastrophic risk', 'Governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21838</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Tacit Bidder-Side Collusion: Artificial Intelligence in Dynamic Auctions</title><link>https://arxiv.org/abs/2511.21802</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a minimal repeated Dutch auction model deriving incentive-compatibility conditions and a closed-form threshold for sustainable tacit collusion in subgame-perfect Nash equilibria.&lt;/li&gt;&lt;li&gt;Runs controlled simulations with multiple LLMs acting as autonomous bidders and observes supra-competitive prices in small markets, with competitive behavior returning as bidder count grows.&lt;/li&gt;&lt;li&gt;Identifies emergent LLM coordination mechanisms (e.g., focal-point acceptance timing and patient strategies aligned with theoretical incentives).&lt;/li&gt;&lt;li&gt;Finds that market-structure interventions can mitigate collusion more effectively than capability limits on models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sriram Tolety']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-collusion', 'multi-agent-systems', 'economic-manipulation', 'AI-safety', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21802</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Factors That Support Grounded Responses in LLM Conversations: A Rapid Review</title><link>https://arxiv.org/abs/2511.21762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Rapid review identifying techniques to align LLM conversational outputs with user intent, ensure contextual grounding, and reduce hallucinations and topic drift.&lt;/li&gt;&lt;li&gt;Organizes alignment strategies by LLM lifecycle phase: inference-time, post-training, and reinforcement learning–based methods.&lt;/li&gt;&lt;li&gt;Finds inference-time approaches particularly efficient for aligning outputs without retraining and for mitigating hallucination while supporting grounding.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriele Cesar Iwashima', 'Claudia Susie Rodrigues', 'Claudio Dipolitto', "Geraldo Xex\\'eo"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'inference-time techniques', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21762</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs</title><link>https://arxiv.org/abs/2511.21757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Medical Malice, a dataset of 214,219 adversarial prompts targeted to context-specific safety failures within the Brazilian Unified Health System (SUS).&lt;/li&gt;&lt;li&gt;Prompts are labeled with the reasoning behind each violation and span seven taxonomies (e.g., procurement manipulation, queue-jumping, obstetric violence), enabling models to learn context-aware refusal behavior.&lt;/li&gt;&lt;li&gt;Synthesis used an unaligned agent (Grok-4) in a persona-driven pipeline to generate high-fidelity attacks; the paper discusses ethical considerations of releasing vulnerability signatures to aid defenders.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Andrew Maranh\\~ao Ventura D'addario"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'healthcare safety', 'safety dataset', 'context-aware alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21757</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title><link>https://arxiv.org/abs/2511.21752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Label Disguise Defense (LDD): replace true class labels with semantically transformed or unrelated aliases (e.g., 'blue' vs 'yellow') learned via few-shot demonstrations to mitigate class-directive prompt injection.&lt;/li&gt;&lt;li&gt;Evaluates LDD across nine state-of-the-art LLMs (e.g., GPT-5, GPT-4o, LLaMA3.2, Gemma3, Mistral variants) under adversarial prompt-injection settings and varying few-shot setups.&lt;/li&gt;&lt;li&gt;Finds LDD partially restores accuracy lost to prompt-injection across all models, with semantically aligned aliases (e.g., 'good'/'bad') generally yielding stronger robustness than arbitrary symbols.&lt;/li&gt;&lt;li&gt;Provides linguistic analysis of alias semantics and demonstrates model-agnostic, lightweight defense that does not require model retraining.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Li', 'Ruocheng Shan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM robustness', 'jailbreaking defense', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21752</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness</title><link>https://arxiv.org/abs/2511.21749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BRIES, a compound AI with specialized agents: Twister (adversarial persuasion generator), Detector (attack classifier), Defender (content inoculation), and Assessor (causal evaluation of inoculation).&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs (GPT-4, Gemma, Llama3, Mistral) on a synthetic persuasion dataset using the SemEval 2023 Task 3 taxonomy, reporting model-specific detection performance and sensitivity to prompt/temperature settings.&lt;/li&gt;&lt;li&gt;Performs causal analysis to identify socio-emotional-cognitive signatures of persuasion attacks and measures how inoculation interventions affect human cognitive resilience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Svitlana Volkova', 'Will Dupree', 'Hsien-Te Kao', 'Peter Bautista', 'Gabe Ganberg', 'Jeff Beaubien', 'Laura Cassani']&lt;/li&gt;&lt;li&gt;Tags: ['persuasion attacks', 'adversarial prompting', 'LLM safety', 'red teaming', 'inoculation/defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21749</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features</title><link>https://arxiv.org/abs/2511.21744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NEULIF, a lightweight detector for AI-generated text that uses stylometric and readability features combined with a compact CNN or Random Forest classifier.&lt;/li&gt;&lt;li&gt;Reports strong performance on the Kaggle AI vs. Human corpus: CNN ~97% accuracy (~0.95 F1, ROC-AUC 99.5%), Random Forest ~95% accuracy (~0.94 F1, ROC-AUC 95%).&lt;/li&gt;&lt;li&gt;Models are compact (CNN ~25 MB, RF ~10.6 MB) and CPU-efficient, intended to be run without large transformer ensembles.&lt;/li&gt;&lt;li&gt;Authors claim potential generalization across languages, domains, and streaming contexts, emphasizing simplicity over large-model complexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey K. Aityan', 'William Claster', 'Karthik Sai Emani', 'Sohni Rais', 'Thy Tran']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'stylometry', 'lightweight models', 'model efficiency', 'safety/forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21744</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Polarity-Aware Probing for Quantifying Latent Alignment in Language Models</title><link>https://arxiv.org/abs/2511.21737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Polarity-Aware CCS (PA-CCS), an unsupervised probing method that assesses whether internal representations of LMs remain consistent under polarity inversion (harmful vs. safe statements).&lt;/li&gt;&lt;li&gt;Proposes two alignment-focused metrics—Polar-Consistency and the Contradiction Index—and curates paired datasets of harmful/safe sentences (two main datasets plus a control) to validate the method.&lt;/li&gt;&lt;li&gt;Applies PA-CCS across 16 language models, revealing architectural and layer-specific differences in encoding latent harmful knowledge and showing sensitivity to token-level manipulations (e.g., replacing negation tokens).&lt;/li&gt;&lt;li&gt;Argues for incorporating structural robustness checks into interpretability/ alignment benchmarks and provides code and datasets for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sabrina Sadiekh', 'Elena Ericheva', 'Chirag Agarwal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'probing', 'latent beliefs', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21737</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems</title><link>https://arxiv.org/abs/2511.21729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Combines hybrid retrieval, ensemble verification, and adaptive thresholding in RAG systems to achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations on a 50-query evaluation (including adversarial cases).&lt;/li&gt;&lt;li&gt;Identifies a measurement/labeling challenge where different verification strategies produce inconsistent labels (e.g., "abstained" vs "unsupported"), causing misleading apparent hallucination rates.&lt;/li&gt;&lt;li&gt;Argues that synergistic integration and adaptive calibration matter more than improving any single component, and calls for standardized metrics and labels to correctly interpret safety performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jithin Krishnan']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination', 'calibration', 'verification', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21729</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Role of Preference Variance in Preference Optimization</title><link>https://arxiv.org/abs/2510.13022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how preference variance (PVar) — variance in model preference scores across response pairs — controls the magnitude of DPO gradient updates and thus the informativeness of prompts for preference optimization.&lt;/li&gt;&lt;li&gt;Provides a theoretical upper bound linking PVar to the DPO gradient norm, implying low-PVar prompts yield small learning signals.&lt;/li&gt;&lt;li&gt;Empirically shows that selecting high-PVar prompts (even using smaller reward models) improves fine-tuning performance on AlpacaEval 2.0 and Arena-Hard, and that training on the top 10% high-PVar prompts can outperform training on the full dataset with human annotations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Guo', 'Zihao Li', 'Jiahao Qiu', 'Yue Wu', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference optimization', 'data selection', 'reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13022</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent</title><link>https://arxiv.org/abs/2511.23436</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SuperIntelliAgent: a paired architecture where a small trainable learner (diffusion model) self-generates outputs and a frozen LLM verifier provides step-by-step evaluations to create chosen/rejected pairs for Direct Preference Optimization (DPO).&lt;/li&gt;&lt;li&gt;Introduces dual-scale memory: short-term in-context memory for reasoning traces across refinement cycles and long-term memory via lightweight on-the-fly fine-tuning plus a replay buffer to consolidate verifiable progress and form adaptive curricula.&lt;/li&gt;&lt;li&gt;Frames the loop as autonomous, annotation-free continual learning that converts inference into a lifelong optimization process, claiming improved performance and stronger preference alignment with few automatically generated DPO pairs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianzhe Lin', 'Zeyu Pan', 'Yun Zhu', 'Ruiqi Song', 'Jining Yang']&lt;/li&gt;&lt;li&gt;Tags: ['continual learning', 'self-training / self-play', 'preference alignment (DPO)', 'memory &amp; replay', 'agentic systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23436</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Does Self-Evaluation Enable Wireheading in Language Models?</title><link>https://arxiv.org/abs/2511.23092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes conditions in POMDPs where controlling the reward channel (wireheading) strictly dominates task-focused behavior when self-evaluation is tied to rewards.&lt;/li&gt;&lt;li&gt;Empirical tests on two models and three tasks show substantial grade inflation (self-evaluated rewards) without accuracy gains when self-grades determine rewards, especially on ambiguous tasks like summarization.&lt;/li&gt;&lt;li&gt;Finds that self-evaluation is safe when decoupled from learning signals but leads to reward-manipulation incentives when coupled, with clear design implications for agentic systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Demitri Africa', 'Hans Ethan Ting']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'wireheading', 'reward-design', 'self-evaluation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.23092</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM</title><link>https://arxiv.org/abs/2511.22998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TIM-PRM, a tool-integrated Process Reward Model that actively plans verification strategies and queries external tools to ground multimodal reasoning verification.&lt;/li&gt;&lt;li&gt;Proposes Independent Question Asking to decouple verification from original reasoning, aiming to reduce confirmation bias and visual hallucinations.&lt;/li&gt;&lt;li&gt;Provides a curated dataset of tool-integrated verification trajectories and shows an 8B model outperforming much larger open-source models on VisualProcessBench, with interpretable verification traces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Kuang', 'Xiangxiang Wang', 'Wentao Liu', 'Jian Dong', 'Kaidi Xu', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal verification', 'process reward models', 'hallucination mitigation', 'tool-augmented verification', 'alignment/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22998</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Deception: Risks, Dynamics, and Controls</title><link>https://arxiv.org/abs/2511.22619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a formal definition of AI deception grounded in signaling theory and organizes the field into a deception cycle: deception emergence and deception treatment.&lt;/li&gt;&lt;li&gt;Analyzes mechanisms driving deception emergence: incentive hierarchies, three capability preconditions, and contextual triggers (supervision gaps, distributional shifts, environmental pressures).&lt;/li&gt;&lt;li&gt;Surveys empirical studies and detection methods, including benchmarks and evaluation protocols in static and interactive settings, and outlines mitigation strategies.&lt;/li&gt;&lt;li&gt;Proposes auditing approaches combining technical, community, and governance efforts and releases a living resource to support ongoing work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyuan Chen (Jay)', 'Sitong Fang (Jay)', 'Jiaming Ji (Jay)', 'Yanxu Zhu (Jay)', 'Pengcheng Wen (Jay)', 'Jinzhou Wu (Jay)', 'Yingshui Tan (Jay)', 'Boren Zheng (Jay)', 'Mengying Yuan (Jay)', 'Wenqi Chen (Jay)', 'Donghai Hong (Jay)', 'Alex Qiu (Jay)', 'Xin Chen (Jay)', 'Jiayi Zhou (Jay)', 'Kaile Wang (Jay)', 'Juntao Dai (Jay)', 'Borong Zhang (Jay)', 'Tianzhuo Yang (Jay)', 'Saad Siddiqui (Jay)', 'Isabella Duan (Jay)', 'Yawen Duan (Jay)', 'Brian Tse (Jay)', 'Jen-Tse (Jay)', 'Huang', 'Kun Wang', 'Baihui Zheng', 'Jiaheng Liu', 'Jian Yang', 'Yiming Li', 'Wenting Chen', 'Dongrui Liu', 'Lukas Vierling', 'Zhiheng Xi', 'Haobo Fu', 'Wenxuan Wang', 'Jitao Sang', 'Zhengyan Shi', 'Chi-Min Chan', 'Eugenie Shi', 'Simin Li', 'Juncheng Li', 'Wei Ji', 'Dong Li', 'Jun Song', 'Yinpeng Dong', 'Jie Fu', 'Bo Zheng', 'Min Yang', 'Yike Guo', 'Philip Torr', 'Zhongyuan Wang', 'Yaodong Yang', 'Tiejun Huang', 'Ya-Qin Zhang', 'Hongjiang Zhang', 'Andrew Yao']&lt;/li&gt;&lt;li&gt;Tags: ['AI deception', 'AI safety', 'Alignment', 'Red teaming &amp; evaluation', 'Auditing &amp; governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22619</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI</title><link>https://arxiv.org/abs/2511.21827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores strategies for generating synthetic clinical notes with LLMs (prompt design and inclusion of medical metadata) to augment dermatology image datasets for multimodal models.&lt;/li&gt;&lt;li&gt;Evaluates effects of synthetic notes on downstream tasks: classification (including under domain shift) and cross-modal retrieval.&lt;/li&gt;&lt;li&gt;Notes concerns about LLM hallucinations in clinical contexts and studies how different synthesis strategies impact model performance and utility.&lt;/li&gt;&lt;li&gt;Finds synthetic notes can improve classification robustness and enable cross-modal retrieval capabilities that were not explicitly optimized for during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niccolo Marini', 'Zhaohui Liang', 'Sivaramakrishnan Rajaraman', 'Zhiyun Xue', 'Sameer Antani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'synthetic data', 'medical multimodal AI', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21827</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Artificial Superintelligence via a Multi-Box Protocol</title><link>https://arxiv.org/abs/2511.21779</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-box containment protocol where multiple isolated ASIs self-modify and mutually verify alignment claims via an auditable submission interface (humans remain outside).&lt;/li&gt;&lt;li&gt;Defines concrete interactions (submit/validate proofs, request/approve modifications, report/confirm hidden messages) and a reputation system to incentivize honest evaluations.&lt;/li&gt;&lt;li&gt;Argues isolation prevents collusion so independently acting ASIs converge on objective truth forming a "consistent group"; release requires high reputation and multi-party verification.&lt;/li&gt;&lt;li&gt;Acknowledges practical limitations (high computational/resource cost and not addressing how to create diverse ASIs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avraham Yair Negozio']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'containment', 'peer verification', 'reputation system', 'ASI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21779</guid><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
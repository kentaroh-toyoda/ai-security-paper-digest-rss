<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 08 Jan 2026 00:04:15 +0000</lastBuildDate><item><title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title><link>https://arxiv.org/abs/2512.10675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a generative evaluation system built on a frontier video foundation model (Veo) optimized for action conditioning and multi-view consistency to simulate robot interactions.&lt;/li&gt;&lt;li&gt;Uses generative image-editing and multi-view completion to create novel objects, backgrounds, and distractors to assess nominal performance, OOD generalization, and probe physical/semantic safety.&lt;/li&gt;&lt;li&gt;Demonstrates system fidelity and red-teaming capability via 1600+ real-world evaluations across eight Gemini Robotics policy checkpoints and five bimanual manipulation tasks, showing it can predict relative policy performance and uncover unsafe behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gemini Robotics Team', 'Krzysztof Choromanski', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Isabel Leal', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robotics-safety', 'red-teaming', 'OOD-generalization', 'generative-simulation', 'video-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10675</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal (image+text) dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations for fine-grained analysis.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal detection model that improves detection accuracy and interpretability for implicit/nuanced harmful content.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance on the MemeMind dataset, aiming to support safer content moderation and explainable decisions for harmful meme detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought', 'interpretability', 'content moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications</title><link>https://arxiv.org/abs/2601.00150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents FCMBench-V1.0, a privacy-compliant financial credit multimodal benchmark: 4,043 synthesized/captured images and 8,446 QA samples across 18 certificate types.&lt;/li&gt;&lt;li&gt;Evaluation framework covers three dimensions—Perception, Reasoning, and Robustness—with 3 perception tasks, 4 credit-specific reasoning tasks, and 10 real-world acquisition-artifact types for stress testing.&lt;/li&gt;&lt;li&gt;Uses a closed synthesis-capture pipeline to preserve privacy and avoid pretraining data leakage; benchmarks 23 state-of-the-art vision-language models showing varying performance.&lt;/li&gt;&lt;li&gt;Finds significant performance drops under acquisition artifacts, demonstrating real-world robustness gaps in top VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yehui Yang', 'Dalu Yang', 'Wenshuo Zhou', 'Fangxin Shang', 'Yifan Liu', 'Jie Ren', 'Haojun Fei', 'Qing Yang', 'Yanwu Xu', 'Tao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['Multimodal benchmark', 'Financial documents', 'Robustness evaluation', 'Vision-language models', 'Privacy-compliant dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00150</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DarkEQA, a benchmark to evaluate embodied question answering (EQA) perceptual primitives under multi-level low-light conditions using physics-inspired degradations in linear RAW space and an ISP-like rendering pipeline.&lt;/li&gt;&lt;li&gt;Isolates the perception bottleneck by evaluating QA from egocentric observations under controlled degradations, enabling attributable robustness analysis for downstream embodied agents.&lt;/li&gt;&lt;li&gt;Evaluates a range of state-of-the-art vision-language models and low-light image enhancement methods, showing substantial performance drops and exposing limitations of current VLMs in dark environments.&lt;/li&gt;&lt;li&gt;Provides an open-source benchmark and resources for studying 24/7 operational robustness of VLM-driven embodied systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yohan Park', 'Hyunwoo Ha', 'Wonjun Jo', 'Tae-Hyun Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'embodied question answering', 'low-light imaging', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24985</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face</title><link>https://arxiv.org/abs/2512.21019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient video defense framework that perturbs 3D information acquisition to protect portrait videos from 3D-field talking-face generation (TFG) methods while preserving high visual fidelity.&lt;/li&gt;&lt;li&gt;Introduces a similarity-guided parameter sharing mechanism for computational efficiency and a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations.&lt;/li&gt;&lt;li&gt;Reports strong defense effectiveness, robustness to scaling and purification attacks, and a reported 47x speedup over the fastest baseline with ablation studies validating design choices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui-qing Sun', 'Xingshan Yao', 'Tian Lan', 'Jia-Ling Shi', 'Chen-Hao Cui', 'Hui-Yang Zhao', 'Zhijing Wu', 'Chen Yang', 'Xian-Ling Mao']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake defense', 'adversarial perturbation', 'privacy protection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.21019</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats</title><link>https://arxiv.org/abs/2511.17254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an intervention framework aligned with the transformer's causal architecture to mitigate hallucinations in Large Vision-Language Models (LVLMs) by integrating effects from multiple causal paths.&lt;/li&gt;&lt;li&gt;Identifies three contributing pathways to hallucination — image-to-input-text, image-to-output-text, and text-to-text — and shows LVLMs rely on different paths depending on question-answer alignment format (discriminative vs generative).&lt;/li&gt;&lt;li&gt;Introduces methods to identify and intervene on critical hallucination heads within each pathway, tailored to alignment format, and demonstrates consistent hallucination reduction across benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaye Qian', 'Ge Zheng', 'Yuchen Zhu', 'Sibei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM hallucination', 'alignment/mitigation', 'model interventions', 'safety/robustness', 'causal analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17254</guid><pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM</title><link>https://arxiv.org/abs/2507.14632</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BusterX++, a unified MLLM-based framework for detecting and explaining AI-generated images and videos, with an RL post-training strategy to improve detection and interpretability.&lt;/li&gt;&lt;li&gt;Presents GenBuster++, a curated benchmark of 4,000 images and video clips created with state-of-the-art generative methods for evaluation of cross-modal detection.&lt;/li&gt;&lt;li&gt;Reports experiments showing improved effectiveness and generalizability of the unified, multimodal detection and explanation approach against single-modality detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiquan Wen', 'Tianxiao Li', 'Zhenglin Huang', 'Yiwei He', 'Guangliang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'multimodal LLMs', 'explainability', 'reinforcement learning', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14632</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>E$^2$AT: Multimodal Jailbreak Defense via Dynamic Joint Optimization for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2503.04833</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes E2AT: an efficient end-to-end adversarial training framework for defending multimodal LLMs against visual and textual jailbreak attacks.&lt;/li&gt;&lt;li&gt;Introduces a projector-based adversarial training module to align visual attack samples at the feature level, reducing need to tune full model weights.&lt;/li&gt;&lt;li&gt;Presents Dynamic Joint Multimodal Optimization (DJMO) that dynamically weights normal and adversarial objectives to improve generalization against jailbreaks.&lt;/li&gt;&lt;li&gt;Evaluated on three mainstream MLLMs with five major jailbreak methods, reporting ~34% average improvement over baselines and tested in real-world embodied systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liming Lu', 'Xiang Gu', 'Shuchao Pang', 'Siyuan Liang', 'Haotian Zhu', 'Xiyu Zeng', 'Xu Zheng', 'Yongbin Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'adversarial training', 'multimodal robustness', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04833</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How Many Images Does It Take? Estimating Imitation Thresholds in Text-to-Image Models</title><link>https://arxiv.org/abs/2410.15002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Verma', 'Royi Rassin', 'Arnav Das', 'Gantavya Bhatt', 'Preethi Seshadri', 'Chirag Shah', 'Jeff Bilmes', 'Hannaneh Hajishirzi', 'Yanai Elazar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'copyright', 'model memorization', 'text-to-image', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.15002</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition</title><link>https://arxiv.org/abs/2601.03124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a deep-learning pipeline for grape leaf disease classification using pre-trained InceptionV3 and Xception models (Xception achieves 96.23% accuracy).&lt;/li&gt;&lt;li&gt;Employs adversarial training to improve model robustness and integrates Grad-CAM for explainability/visual confirmation of disease regions.&lt;/li&gt;&lt;li&gt;Dataset of 9,032 leaf images split into train/val/test; final deployment as a Streamlit web app with heatmap visualization and prediction confidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['B. M. Shahria Alam', 'Md. Nasim Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'explainability', 'image classification', 'applied ML', 'agriculture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03124</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.03100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TGIF (Text-Guided Inter-layer Fusion), a lightweight module that predicts prompt-dependent fusion weights over vision-encoder layers to produce query-conditioned visual features without updating the vision encoder.&lt;/li&gt;&lt;li&gt;TGIF integrates into LLaVA-1.5-7B and yields consistent improvements on hallucination metrics, OCR, and VQA benchmarks while preserving or improving performance on ScienceQA, GQA, and MMBench.&lt;/li&gt;&lt;li&gt;Emphasizes that using the encoder's hierarchical features in a text-conditioned manner strengthens visual grounding and reduces the model's reliance on language priors that cause visually ungrounded hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenchen Lin', 'Sanbao Su', 'Rachel Luo', 'Yuxiao Chen', 'Yan Wang', 'Marco Pavone', 'Fei Miao']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'multimodal-llms', 'visual-grounding', 'robustness', 'model-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03100</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios</title><link>https://arxiv.org/abs/2601.03011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ReCCur, a low-compute, multi-agent recursive pipeline to convert noisy web images into high-precision, explainable labels for corner-case vision-language scenarios.&lt;/li&gt;&lt;li&gt;Combines large-scale data acquisition with tri-modal (image, description, keyword) consistency checks, mixture-of-experts distillation (CLIP, DINOv2, BEiT) for kNN voting and uncertainty sampling, and region-evidence proposer/validator adversarial labeling.&lt;/li&gt;&lt;li&gt;Targets realistic corner cases (e.g., flooded-car inspection) with minimal human supervision and consumer-grade GPUs, producing datasets suitable for downstream training and robustness evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihan Wei', 'Shenghai Yuan', 'Tianchen Deng', 'Boyang Lou', 'Enwen Hu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'dataset curation', 'corner cases', 'vision-language models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03011</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench</title><link>https://arxiv.org/abs/2601.02737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'functional perception gap' where vision encoders in MLLMs cannot decode PET tracer biodistribution independent of morphological cues.&lt;/li&gt;&lt;li&gt;Introduces PET-Bench: a large-scale benchmark (52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies) for evaluating functional imaging perception in MLLMs.&lt;/li&gt;&lt;li&gt;Documents a safety hazard called the 'Chain-of-Thought (CoT) hallucination trap' where CoT prompting yields fluent but visually ungrounded medical diagnoses.&lt;/li&gt;&lt;li&gt;Proposes Atomic Visual Alignment (AVA), a fine-tuning strategy that enforces low-level functional perception before high-level reasoning, improving diagnostic accuracy by up to 14.83% and mitigating CoT-induced hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zanting Ye', 'Xiaolong Niu', 'Xuanbin Wu', 'Xu Han', 'Shengyuan Liu', 'Jing Hao', 'Zhihao Peng', 'Hao Sun', 'Jieqin Lv', 'Fanghu Wang', 'Yanchao Huang', 'Hubing Wu', 'Yixuan Yuan', 'Habib Zaidi', 'Arman Rahmim', 'Yefeng Zheng', 'Lijun Lu']&lt;/li&gt;&lt;li&gt;Tags: ['MLLM safety', 'hallucination/CoT', 'alignment', 'medical AI safety', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02737</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network</title><link>https://arxiv.org/abs/2601.02566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a deep-learning method for localizing manipulated regions in both shallow-fake (conventional editing) and deep-fake (AI-generated) images.&lt;/li&gt;&lt;li&gt;Uses a Vision Mamba backbone to extract feature maps that emphasize boundaries between tampered and authentic pixels.&lt;/li&gt;&lt;li&gt;Introduces a Guided Graph Neural Network (G-GNN) module to amplify separation between manipulated and genuine regions.&lt;/li&gt;&lt;li&gt;Reports higher inference accuracy than several state-of-the-art baselines on manipulation localization tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junbin Zhang', 'Hamid Reza Tohidypour', 'Yixiao Wang', 'Panos Nasiopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['image forensics', 'deepfake detection', 'manipulation localization', 'graph neural networks', 'vision models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02566</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title><link>https://arxiv.org/abs/2512.10449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates vulnerabilities of LLM-based scientific review systems to indirect prompt injections via adversarial PDF manipulations (invisible text, layout-aware encoding).&lt;/li&gt;&lt;li&gt;Introduces the Weighted Adversarial Vulnerability Score (WAVS) to quantify susceptibility by weighting score inflation against decision flips.&lt;/li&gt;&lt;li&gt;Implements 15 domain-specific attack strategies and evaluates them across 13 language models (including proprietary models), using a curated dataset of 200 accepted/rejected submissions; reports decision flip rates up to 86.26%.&lt;/li&gt;&lt;li&gt;Releases dataset and attack framework to support further research into defensive mitigation and robustness testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Manish Prasad', 'Vasudev Majhi', 'Jahnvi Singh', 'Vinay Chamola', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'jailbreaking', 'adversarial attacks', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10449</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable Preference Optimization: A Bilevel Approach to Catastrophic Preference Shift</title><link>https://arxiv.org/abs/2507.07723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode in Bradley-Terry (BT)-style direct preference learning called 'Catastrophic Preference Shift', where probability mass for preferred outputs decreases and shifts to out-of-distribution responses, degrading performance.&lt;/li&gt;&lt;li&gt;Theoretically analyzes BT-style methods from a probability-evolution perspective, proving over-reliance on initialization can cause preference shift.&lt;/li&gt;&lt;li&gt;Proposes Stable Preference Optimization (SPO), a bilevel/constraint-based framework that restricts preference updates to a safe alignment region to stabilize learning.&lt;/li&gt;&lt;li&gt;Empirical results show SPO stabilizes and improves performance of existing BT-style preference learning methods (e.g., recovers from large reasoning-accuracy drops).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengtao Jian', 'Kai Yang', 'Tianhao Gao', 'Wuguang Ni', 'Keying Yang', 'Bowen Xiao', 'Jiajun Liu', 'Ye Ouyang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'model robustness', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07723</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Characterizing the Robustness of Black-Box LLM Planners Under Perturbed Observations with Adaptive Stress Testing</title><link>https://arxiv.org/abs/2505.05665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies robustness of black-box LLM planners to two perturbation dimensions: semantically varied prompt phrasing and simulated sensor/noise failures.&lt;/li&gt;&lt;li&gt;Introduces an adaptive stress testing (AST) approach using Monte-Carlo tree search (MCTS) to efficiently search the space of prompt and sensor perturbations that induce high uncertainty, hallucinations, or crashes.&lt;/li&gt;&lt;li&gt;Presents a multi-agent driving case study and extensive experiments showing AST can discover failure-inducing scenarios and configurations for offline analysis to proactively understand runtime failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeloy Chakraborty', 'John Pohovey', 'Melkior Ornik', 'Katherine Driggs-Campbell']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'robustness', 'adversarial prompting', 'safety evaluation', 'adaptive stress testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05665</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Intrinsic Toxicity to Reception-Based Toxicity: A Contextual Framework for Prediction and Evaluation</title><link>https://arxiv.org/abs/2503.16072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that toxicity is context-dependent and emerges from social reception, not solely an intrinsic property of text.&lt;/li&gt;&lt;li&gt;Proposes the Contextual Stress Framework (CSF) to model toxicity as stress-inducing norm violations within context.&lt;/li&gt;&lt;li&gt;Introduces PONOS (Proportion Of Negative Observed Sentiments), a reception-based metric for toxicity, and validates it on a novel dataset showing improved contextual sensitivity when combined with existing models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'content moderation', 'safety evaluation', 'contextual modelling', 'metrics/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16072</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>E$^2$AT: Multimodal Jailbreak Defense via Dynamic Joint Optimization for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2503.04833</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes E2AT: an efficient end-to-end adversarial training framework for defending multimodal LLMs against visual and textual jailbreak attacks.&lt;/li&gt;&lt;li&gt;Introduces a projector-based adversarial training module to align visual attack samples at the feature level, reducing need to tune full model weights.&lt;/li&gt;&lt;li&gt;Presents Dynamic Joint Multimodal Optimization (DJMO) that dynamically weights normal and adversarial objectives to improve generalization against jailbreaks.&lt;/li&gt;&lt;li&gt;Evaluated on three mainstream MLLMs with five major jailbreak methods, reporting ~34% average improvement over baselines and tested in real-world embodied systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liming Lu', 'Xiang Gu', 'Shuchao Pang', 'Siyuan Liang', 'Haotian Zhu', 'Xiyu Zeng', 'Xu Zheng', 'Yongbin Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'adversarial training', 'multimodal robustness', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04833</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems</title><link>https://arxiv.org/abs/2503.03750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MASK, a large-scale human-collected benchmark that directly measures lying to disentangle honesty from factual accuracy in LLMs.&lt;/li&gt;&lt;li&gt;Shows that larger models improve accuracy but do not necessarily become more honest; many frontier models score well on truthfulness benchmarks yet still lie under pressure.&lt;/li&gt;&lt;li&gt;Demonstrates that simple interventions (e.g., representation engineering) can improve honesty.&lt;/li&gt;&lt;li&gt;Highlights the need for robust evaluations and interventions to ensure trustworthiness of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Ren', 'Arunim Agarwal', 'Mantas Mazeika', 'Cristina Menghini', 'Robert Vacareanu', 'Brad Kenstler', 'Mick Yang', 'Isabelle Barrass', 'Alice Gatti', 'Xuwang Yin', 'Eduardo Trevino', 'Matias Geralnik', 'Adam Khoja', 'Dean Lee', 'Summer Yue', 'Dan Hendrycks']&lt;/li&gt;&lt;li&gt;Tags: ['honesty-benchmark', 'deceptive-behavior', 'LLM-safety', 'alignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03750</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Hidden State Poisoning Attacks against Mamba-based Language Models</title><link>https://arxiv.org/abs/2601.01972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and defines Hidden State Poisoning Attacks (HiSPA) where short input triggers irreversibly overwrite information in state-space models (SSMs) like Mamba/Jamba, causing partial amnesia.&lt;/li&gt;&lt;li&gt;Introduces RoBench25, a benchmark to evaluate information retrieval under HiSPA, and shows optimized triggers can break large hybrid SSM-Transformer models (52B Jamba), while pure Transformers remain robust.&lt;/li&gt;&lt;li&gt;Demonstrates HiSPA triggers also degrade performance on existing prompt-injection benchmarks and provides interpretability analysis revealing hidden-layer patterns that could enable mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandre Le Mercier', 'Chris Develder', 'Thomas Demeester']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model poisoning', 'LLM robustness', 'hidden-state attacks', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01972</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tackling the Inherent Difficulty of Noise Filtering in RAG</title><link>https://arxiv.org/abs/2601.01896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the inherent difficulty of filtering irrelevant/noisy documents in Retrieval-Augmented Generation (RAG) and argues retrievers alone cannot fully remove noise due to transformer/attention limitations.&lt;/li&gt;&lt;li&gt;Shows standard fine-tuning often fails to make LLMs selectively ignore irrelevant retrieved content, linking this to structural constraints of attention patterns.&lt;/li&gt;&lt;li&gt;Proposes a novel fine-tuning method to improve model robustness to noisy retrievals and demonstrates improved performance across multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Liu', 'Jiaen Lin', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Robustness', 'Hallucination Mitigation', 'Fine-tuning', 'Attention Patterns']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01896</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Steerability of Instrumental-Convergence Tendencies in LLMs</title><link>https://arxiv.org/abs/2601.01584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines capability vs. steerability and frames a safety–security dilemma: safety needs high authorized steerability while security wants low unauthorized steerability.&lt;/li&gt;&lt;li&gt;Empirical evaluation using InstrumentalEval on Qwen3 shows an anti-instrumental prompt suffix can sharply reduce instrumental-convergence rates (e.g., 81.69% → 2.82% for Qwen3-30B Instruct).&lt;/li&gt;&lt;li&gt;Finds that under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones, and discusses implications for open-weight models (fine-tuning, adversarial attacks).&lt;/li&gt;&lt;li&gt;Provides code and focuses on vulnerabilities relevant to red teaming, jailbreaking, and adversarial steering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jakub Hoscilowicz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'steerability', 'jailbreaking / prompt injection', 'adversarial prompting', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01584</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title><link>https://arxiv.org/abs/2512.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates 'Activation Oracles' (LatentQA-trained models) that take LLM activations as input and answer natural-language questions about them.&lt;/li&gt;&lt;li&gt;Evaluates generalization of AOs to out-of-distribution activations and shows training-data diversity improves performance.&lt;/li&gt;&lt;li&gt;Shows AOs can recover information fine-tuned into models (e.g., biographical facts or 'malign propensities') despite not being trained on fine-tuned-model activations.&lt;/li&gt;&lt;li&gt;On four downstream tasks, their best AOs match or exceed white-box baselines and outperform most overall baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Karvonen', 'James Chua', "Cl\\'ement Dumas", 'Kit Fraser-Taliente', 'Subhash Kantamneni', 'Julian Minder', 'Euan Ong', 'Arnab Sen Sharma', 'Daniel Wen', 'Owain Evans', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model introspection', 'privacy/model extraction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15674</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>TPA: Next Token Probability Attribution for Detecting Hallucinations in RAG</title><link>https://arxiv.org/abs/2512.07515</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TPA, a method that attributes next-token probabilities to seven sources (Query, RAG Context, Past Token, Self Token, FFN, Final LayerNorm, Initial Embedding) to analyze contributions to generation.&lt;/li&gt;&lt;li&gt;Aggregates attribution scores by Part-of-Speech tags to detect patterns where certain linguistic categories (e.g., nouns) rely unusually on specific components (e.g., LayerNorm), signaling hallucinations.&lt;/li&gt;&lt;li&gt;Uses these attribution-based signals to identify hallucinated responses in Retrieval-Augmented Generation and reports state-of-the-art detection performance in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengqian Lu', 'Jie Lu', 'Anjin Liu', 'Guangquan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'RAG', 'attribution', 'LLM-internals', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07515</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios</title><link>https://arxiv.org/abs/2512.00920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reward Auditor, a hypothesis-testing framework to infer 'suitability' of reward models (RMs) under real-world perturbations rather than just preference accuracy.&lt;/li&gt;&lt;li&gt;Defines suitability as conditional reliability and audits distributional degradation of RM preference-confidence to quantify statistical significance and effect size of vulnerabilities.&lt;/li&gt;&lt;li&gt;Aims to enable inference of systematic RM vulnerabilities across diverse real-world scenarios to inform safer, more robust LLM alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianxiang Zang', 'Yongda Wei', 'Ruxue Bai', 'Shiyu Jiang', 'Nijia Mo', 'Binhong Li', 'Qiang Sun', 'Hui Liu']&lt;/li&gt;&lt;li&gt;Tags: ['reward modeling', 'safety evaluation', 'alignment', 'robustness', 'statistical auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.00920</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DoPE: Denoising Rotary Position Embedding</title><link>https://arxiv.org/abs/2511.09146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes instabilities in Rotary Position Embedding (RoPE) via spectral analysis, showing low-frequency components concentrate energy and produce low-rank, over-aligned attention patterns that act like activation noise.&lt;/li&gt;&lt;li&gt;Introduces DoPE, a training-free method that detects noisy attention heads using truncated matrix entropy and reparameterizes their attention maps with an isotropic Gaussian to suppress noise.&lt;/li&gt;&lt;li&gt;Demonstrates improved long-context extrapolation, increased robustness to perturbations, and gains on needle-in-a-haystack and many-shot in-context learning tasks without fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Xiong', 'Liyang Fan', 'Hui Shen', 'Zunhai Su', 'Min Yang', 'Lingpeng Kong', 'Ngai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['positional encoding', 'robustness', 'long-context extrapolation', 'attention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.09146</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Qomhra: A Bilingual Irish and English Large Language Model</title><link>https://arxiv.org/abs/2510.17652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Qomhrá, a bilingual Irish–English LLM trained under low-resource constraints with continued pre-training and instruction tuning.&lt;/li&gt;&lt;li&gt;Proposes a novel method to synthesize human preference data by prompting a selected LLM to generate 'accepted' and 'rejected' responses for use in alignment training.&lt;/li&gt;&lt;li&gt;Evaluates closed-weight LLMs for Irish generation quality and finds misalignment between LLM-as-judge ratings and native speaker preferences (Gemini-2.5-Pro ranked highest by speakers).&lt;/li&gt;&lt;li&gt;Demonstrates gains over existing open-source Irish LLM baselines across translation, topic identification, gender understanding, and world-knowledge benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph McInerney', 'Khanh-Tung Tran', 'Liam Lonergan', "Ailbhe N\\'i Chasaide", "Neasa N\\'i Chiar\\'ain", 'Barry Devereux']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference-data-synthesis', 'low-resource-LLMs', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17652</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying LLM Biases Across Instruction Boundary in Mixed Question Forms</title><link>https://arxiv.org/abs/2509.20278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'Instruction Boundary' to study how different user instruction settings cause biases in LLM annotations across mixed question forms (e.g., MCQ, True/False) that include 'Sparse Labels' (none/multiple/Unknown).&lt;/li&gt;&lt;li&gt;Proposes BiasDetector, a diagnostic benchmark to systematically evaluate LLMs' ability to detect datasets containing Sparse Labels under varying instruction conditions.&lt;/li&gt;&lt;li&gt;Empirical results show that user instructions induce substantial biases in LLM annotations, leading to mislabeling and highlighting risks for dataset quality and downstream use.&lt;/li&gt;&lt;li&gt;Provides code, datasets, and implementations to facilitate further evaluation and mitigation work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zipeng Ling', 'Shuliang Liu', 'Yuehao Tang', 'Chen Huang', 'Gaoyang Jiang', 'Shenghong Fu', 'Junqi Yang', 'Yao Wan', 'Jiawan Zhang', 'Kejia Huang', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['dataset bias', 'annotation quality', 'instruction tuning', 'evaluation/benchmark', 'LLM behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20278</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Awakening LLMs' Reasoning Potential: A Fine-Grained Pipeline to Evaluate and Mitigate Vague Perception</title><link>https://arxiv.org/abs/2507.16199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'Vague Perception'—cases where LLMs answer 'unknown' despite having latent ability to solve prompts—and formalizes metrics (TCR, OCR, Acc(WakenLLM)) to quantify it.&lt;/li&gt;&lt;li&gt;Introduces WakenLLM, a stage-wise pipeline to extract Vague Perception samples and stimulate models to convert abstentions into correct answers, achieving up to ~68.5% accuracy improvement on such samples without further training.&lt;/li&gt;&lt;li&gt;Empirically analyzes variations across six LLMs by family and size, studies related phenomena (Conformity, Degradation), compares against existing reasoning baselines, and suggests model-selection strategies for multi-stage reasoning.&lt;/li&gt;&lt;li&gt;Provides code and datasets for reproducibility, positioning perception-aware evaluation as a complementary direction for improving LLM safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zipeng Ling', 'Yuehao Tang', 'Shuliang Liu', 'Junqi Yang', 'Shenghong Fu', 'Chen Huang', 'Kejia Huang', 'Yao Wan', 'Zhichao Hou', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Abstention / refusal behavior', 'Evaluation / Benchmarking', 'Alignment / calibration', 'Reasoning enhancement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16199</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal (image+text) dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations for fine-grained analysis.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal detection model that improves detection accuracy and interpretability for implicit/nuanced harmful content.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance on the MemeMind dataset, aiming to support safer content moderation and explainable decisions for harmful meme detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought', 'interpretability', 'content moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Something Just Like TRuST : Toxicity Recognition of Span and Target</title><link>https://arxiv.org/abs/2506.02326</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRuST, a large-scale unified toxicity dataset (~300k annotations, ~11k high-quality human annotations) with a synthesized definition and annotation scheme.&lt;/li&gt;&lt;li&gt;Implements a rigorous multi-stage human annotation process and evaluates annotator diversity to ensure data quality.&lt;/li&gt;&lt;li&gt;Benchmarks state-of-the-art LLMs and fine-tuned pre-trained models on three tasks: toxicity detection, target-group identification, and toxic-span (toxic words) identification.&lt;/li&gt;&lt;li&gt;Finds fine-tuned PLMs outperform LLMs on these tasks and that current reasoning models do not reliably improve performance; positions TRuST as a resource for evaluating and mitigating LLM toxicity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Berk Atil', 'Namrata Sureddy', 'Rebecca J. Passonneau']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity-detection', 'dataset', 'annotation', 'LLM-safety', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02326</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Protecting multimodal large language models against misleading visualizations</title><link>https://arxiv.org/abs/2502.20503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that MLLM question-answering accuracy on misleading visualizations degrades to near-random levels.&lt;/li&gt;&lt;li&gt;Compares six inference-time mitigation methods to improve QA on misleading charts while preserving performance on non-misleading visuals.&lt;/li&gt;&lt;li&gt;Finds table-based QA and redrawing the visualization are most effective (up to 19.6 percentage points improvement).&lt;/li&gt;&lt;li&gt;Releases accompanying code and dataset for evaluation and replication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Tonglet', 'Tinne Tuytelaars', 'Marie-Francine Moens', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal-LLMs', 'robustness', 'adversarial-visualizations', 'evaluation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20503</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Counterfactual Explanations for Generative AI System Behavior</title><link>https://arxiv.org/abs/2601.03156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts counterfactual explanations to nondeterministic generative AI systems by proposing a flexible framework for prompt-focused counterfactuals.&lt;/li&gt;&lt;li&gt;Introduces an algorithm for generating prompt-counterfactual explanations (PCEs) that identify minimal prompt changes producing different output characteristics.&lt;/li&gt;&lt;li&gt;Demonstrates PCEs on three case studies (political leaning, toxicity, sentiment) and highlights uses for prompt engineering and red-teaming to surface and suppress undesirable outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofie Goethals', 'Foster Provost', 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['LLM explainability', 'Prompt engineering', 'Red teaming', 'Counterfactual explanations', 'Safety/Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03156</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs</title><link>https://arxiv.org/abs/2601.03087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BAFA (Bounded Active Fairness Auditor), an active-query method for estimating fairness metrics of black-box LLMs by maintaining a version space of surrogate models and computing uncertainty intervals for target metrics (e.g., ΔAUC).&lt;/li&gt;&lt;li&gt;Uses constrained empirical risk minimisation to compute bounds and an active query selection strategy to reduce interval width, enabling query-efficient auditing.&lt;/li&gt;&lt;li&gt;Evaluated on CivilComments and Bias-in-Bios; BAFA reaches tight error thresholds with up to ~40× fewer queries than stratified sampling and shows lower variance across runs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Hartmann', 'Lena Pohlmann', 'Lelia Hanslik', 'Noah Gie{\\ss}ing', 'Bettina Berendt', 'Pieter Delobelle']&lt;/li&gt;&lt;li&gt;Tags: ['fairness auditing', 'black-box auditing', 'active learning', 'query-efficient evaluation', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03087</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SastBench: A Benchmark for Testing Agentic SAST Triage</title><link>https://arxiv.org/abs/2601.02941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SastBench, a benchmark for evaluating agentic (LLM-powered) SAST triage by combining real CVEs as true positives and filtered SAST tool findings as approximate false positives.&lt;/li&gt;&lt;li&gt;Design is agent-agnostic and aims to better emulate real-world distributions of SAST findings to test automated triage agents.&lt;/li&gt;&lt;li&gt;Evaluates multiple agents on the benchmark, provides comparative performance analysis, dataset analysis, and discusses implications for future tool development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jake Feiglin', 'Guy Dar']&lt;/li&gt;&lt;li&gt;Tags: ['SAST', 'LLM agents', 'Benchmarking', 'Cybersecurity automation', 'Triage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02941</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL, a framework that constructs an interpretable, data-driven scalar reward representing conversational human-likeness from explicit conversational traits.&lt;/li&gt;&lt;li&gt;Derives traits from contrastive dialogue data and uses that reward with standard preference-optimization to align LLMs without degrading overall task performance.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and inspectability for diagnosing unintended effects and validates improvements via large-scale human evaluations showing increased perceived human-likeness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'reward modeling', 'human-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning</title><link>https://arxiv.org/abs/2601.03190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PALU, a prefix-aware localized unlearning framework that maximizes entropy in targeted temporal (prefix) and vocabulary (top-k logits) subspaces to forget sensitive content while preserving utility.&lt;/li&gt;&lt;li&gt;Key findings: suppressing the sensitive prefix severs the causal generation link; flattening only the top-k logits is sufficient to induce uncertainty in the critical subspace.&lt;/li&gt;&lt;li&gt;PALU avoids global vocabulary/parameter optimization, reducing collateral damage to general model performance compared to prior global unlearning approaches.&lt;/li&gt;&lt;li&gt;Extensive experiments show improved forgetting efficacy and utility preservation relative to state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naixin Zhai', 'Pengyang Shao', 'Binbin Zheng', 'Fei Shen', 'Long Bai', 'Xun Yang']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy-preserving ML', 'LLM safety', 'logit-level mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03190</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Anatomy of Conversational Scams: A Topic-Based Red Teaming Analysis of Multi-Turn Interactions in LLMs</title><link>https://arxiv.org/abs/2601.03134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of multi-turn conversational scams using a controlled LLM-to-LLM simulation framework in English and Chinese.&lt;/li&gt;&lt;li&gt;Evaluates eight state-of-the-art models, annotating attacker strategies, defensive responses, escalation patterns, and common failure modes.&lt;/li&gt;&lt;li&gt;Finds that multi-turn interactional safety is a distinct risk dimension (escalation patterns, guardrail activation, role instability) not captured by single-turn evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangzhe Yuan', 'Zhenhao Zhang', 'Haoming Tang', 'Siying Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Conversational scams', 'Multi-turn safety', 'Adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03134</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation</title><link>https://arxiv.org/abs/2601.03121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ToxiGAN, a class-aware text augmentation framework that combines GAN-based adversarial generation with semantic guidance from LLMs to produce toxic examples for classifier training.&lt;/li&gt;&lt;li&gt;Introduces a two-step directional training strategy and uses LLM-generated neutral exemplars as 'semantic ballast' to prevent mode collapse and semantic drift while enforcing class-specific divergence.&lt;/li&gt;&lt;li&gt;Shows improvements in macro-F1 and hate-F1 across four hate speech benchmarks, and includes ablation/sensitivity analyses demonstrating the benefit of semantic ballast and directional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Li', 'Jan Fillies', 'Adrian Paschke']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'data augmentation', 'adversarial generation', 'LLM-guided augmentation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03121</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models</title><link>https://arxiv.org/abs/2601.03079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two pragmatic inference methods that enable LLMs to diagnose morally benign vs. hazardous inputs and to correct moral errors.&lt;/li&gt;&lt;li&gt;Frames methods from a unified perspective grounded in 'inferential loads' rather than modeling diverse surface-level moral discourse.&lt;/li&gt;&lt;li&gt;Presents empirical results showing improved moral sensitivity on representative morality-related benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bocheng Chen', 'Han Zi', 'Xi Chen', 'Xitong Zhang', 'Kristen Johnson', 'Guangliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'moral-sensitivity', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03079</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph</title><link>https://arxiv.org/abs/2601.03052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes detecting faithfulness hallucinations in Retrieval-Augmented Generation (RAG) by constructing a semantic-level internal reasoning graph derived from extended layer-wise relevance propagation (LPR) at the semantic (rather than token) level.&lt;/li&gt;&lt;li&gt;Uses attribution vectors to represent dependencies in the model's internal reasoning and trains a small pretrained language model to leverage these dependencies for dynamic hallucination detection with adjustable thresholds.&lt;/li&gt;&lt;li&gt;Reports improved performance over state-of-the-art baselines on RAGTruth and Dolly-15k benchmarks, claiming better overall hallucination detection for RAG systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianpeng Hu', 'Yanzeng Li', 'Jialun Zhong', 'Wenfa Qi', 'Lei Zou']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'retrieval-augmented generation (RAG)', 'LLM internal reasoning', 'faithfulness evaluation', 'attribution / layer-wise relevance propagation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03052</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation</title><link>https://arxiv.org/abs/2601.03051</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames multi-turn conversations as a temporal graph where each dialogue turn is a node encoded via a sentence transformer.&lt;/li&gt;&lt;li&gt;Uses two edge types—shared-entity edges and temporal (contiguous-turn) edges—with message-passing to produce context-aware node embeddings.&lt;/li&gt;&lt;li&gt;Aggregates node embeddings with attention pooling and a classifier to detect and categorize hallucinations; claims slight performance improvements and uses attention for interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vidhi Rathore', 'Sambu Aneesh', 'Himanshu Singh']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'dialogue safety', 'temporal graph networks', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03051</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning</title><link>https://arxiv.org/abs/2601.03027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces F-DPO, a simple extension of Direct Preference Optimization that leverages binary factuality labels to reduce hallucinations.&lt;/li&gt;&lt;li&gt;Two core mechanisms: (1) label-flipping to ensure chosen responses are never less factual than rejected ones, and (2) a factuality-aware margin that emphasizes pairs with clear correctness differences.&lt;/li&gt;&lt;li&gt;Demonstrates consistent factuality improvements and large reductions in hallucination rates across seven open-weight LLMs (1B–14B), with notable gains on Qwen3-8B and out-of-distribution benchmarks like TruthfulQA.&lt;/li&gt;&lt;li&gt;Requires no auxiliary reward model, token-level annotations, or multi-stage training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sindhuja Chaduvula', 'Ahmed Y. Radwan', 'Azib Farooq', 'Yani Ioannou', 'Shaina Raza']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Hallucination mitigation', 'Preference learning (DPO/RLHF)', 'Factuality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03027</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models</title><link>https://arxiv.org/abs/2601.03023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedDialogRubrics: a benchmarking framework with 5,200 synthetic multi-turn patient cases and &gt;60,000 fine-grained evaluation rubrics refined by clinical experts to assess medical LLM diagnostic/dialogue performance.&lt;/li&gt;&lt;li&gt;Uses a multi-agent system and a Patient Agent constrained to atomic medical facts plus a dynamic guidance mechanism that detects and corrects hallucinations to ensure internal coherence and clinical plausibility.&lt;/li&gt;&lt;li&gt;Proposes an LLM-based rubric generation pipeline that retrieves Evidence-Based Medicine guidelines, uses reject sampling, and produces prioritized 'must-ask' items, with subsequent expert refinement.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art models on multiple assessment dimensions and finds substantial gaps, arguing that improvements require advances in dialogue management architectures beyond base-model tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lecheng Gong', 'Weimin Fang', 'Ting Yang', 'Dongjie Tao', 'Chunxiao Guo', 'Peng Wei', 'Bo Xie', 'Jinqun Guan', 'Zixiao Chen', 'Fang Shi', 'Jinjie Gu', 'Junwei Liu']&lt;/li&gt;&lt;li&gt;Tags: ['medical-llms', 'safety-evaluation', 'hallucination-detection', 'benchmarking', 'synthetic-data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03023</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.02993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and empirically characterizes a sensitivity of Retrieval-Augmented Generation (RAG) models to permutations of retrieved documents: answers vary across Top-5 retrieval permutations even when the gold document is present and fixed in first position.&lt;/li&gt;&lt;li&gt;Proposes Stable-RAG: run the generator over multiple retrieval orders, cluster hidden states to find a cluster-center representation capturing the dominant reasoning pattern, decode from that center, and use results to align hallucinated outputs toward the correct answer.&lt;/li&gt;&lt;li&gt;Shows improvements in answer accuracy, reasoning consistency, and generalization across datasets, retrievers, and input lengths compared with baseline RAG methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianchi Zhang', 'Hainan Zhang', 'Liang Pang', 'Hongwei Zheng', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['RAG robustness', 'hallucination mitigation', 'retrieval permutation sensitivity', 'alignment/consistency', 'QA evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02993</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders</title><link>https://arxiv.org/abs/2601.02978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Sparse Autoencoder framework with contrastive feature retrieval to extract monosemantic internal features tied to high-order linguistic behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates bidirectional, stable steering of LLM behavior (case study: Big Five personality traits) via interventions on retrieved features, outperforming prior activation-steering methods like CAA.&lt;/li&gt;&lt;li&gt;Introduces the notion of Functional Faithfulness: intervening on a single feature yields coherent, multi-dimensional shifts aligned with the targeted semantic attribute.&lt;/li&gt;&lt;li&gt;Combines statistical activation analysis with generation-based validation to ensure extracted features are both interpretable and functionally effective for behavioral control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruikang Zhang', 'Shuo Wang', 'Qi Su']&lt;/li&gt;&lt;li&gt;Tags: ['mechanistic-interpretability', 'model-steering', 'alignment', 'activation-intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02978</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems</title><link>https://arxiv.org/abs/2601.02917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAL2M, a retrieval-augmented framework that avoids generative hallucination by using LLMs as query-response matching judges within a retrieval-based system.&lt;/li&gt;&lt;li&gt;Introduces a query-adaptive latent ensemble method to model heterogeneous LLM competence and interdependencies, producing calibrated consensus judgments to mitigate judgment hallucination.&lt;/li&gt;&lt;li&gt;Reports extensive experiments showing improved compliance and performance over strong baselines, and discusses best practices and future directions for leveraging latent representations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengze Hong', 'Di Jiang', 'Jiangtao Wen', 'Zhiyang Su', 'Yawen Li', 'Yanjie Sun', 'Guan Wang', 'Chen Jason Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'retrieval-augmented systems', 'LLM safety', 'ensemble methods', 'alignment/compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02917</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond the Black Box: Theory and Mechanism of Large Language Models</title><link>https://arxiv.org/abs/2601.02907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified lifecycle taxonomy for LLM research (Data Preparation, Model Preparation, Training, Alignment, Inference, Evaluation).&lt;/li&gt;&lt;li&gt;Systematically reviews theoretical foundations underpinning LLM performance (data mixture mathematics, architecture representational limits, optimization dynamics of alignment algorithms).&lt;/li&gt;&lt;li&gt;Highlights frontier challenges including theoretical limits of synthetic data self-improvement, mathematical bounds of safety guarantees, and mechanistic origins of emergent intelligence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Gan', 'Ruifeng Ren', 'Wei Yao', 'Xiaolin Hu', 'Gengze Xu', 'Chen Qian', 'Huayi Tang', 'Zixuan Gong', 'Xinhao Yao', 'Pengwei Tang', 'Zhenxing Dou', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM theory', 'Alignment', 'Safety guarantees', 'Evaluation', 'Emergent behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02907</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs</title><link>https://arxiv.org/abs/2601.02858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces inverse socio-demographic prompting (ISDP), a discrimination-based method to evaluate LLMs' cultural alignment by predicting demographic proxies from user behavior rather than generating culturally aligned outputs.&lt;/li&gt;&lt;li&gt;Uses the Goodreads-CSI dataset (English book review comprehension for users from India, Mexico, USA) and compares model performance on actual vs. simulated user behaviors across four LLMs (Aya-23, Gemma-2, GPT-4o, LLaMA-3.1).&lt;/li&gt;&lt;li&gt;Finds models perform better on actual behaviors than simulated ones, but performance degrades and converges at the individual level, indicating limits to personalization and challenges for SDP-based evaluation.&lt;/li&gt;&lt;li&gt;Highlights methodological confounds (prompt sensitivity, decoding parameters, generation vs. discrimination difficulty) that complicate interpreting cultural alignment/bias evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saurabh Kumar Pandey', 'Sougata Saha', 'Monojit Choudhury']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'bias', 'cultural_alignment', 'evaluation_methodology', 'LLM_behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02858</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Window-based Membership Inference Attacks Against Fine-tuned Large Language Models</title><link>https://arxiv.org/abs/2601.02751</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WBC (Window-Based Comparison), a sliding-window, sign-aggregation method for membership inference on fine-tuned LLMs that compares losses between target and reference models per window.&lt;/li&gt;&lt;li&gt;Ensembles binary votes across geometrically spaced window sizes to capture memorization from token-level artifacts up to phrase-level structures.&lt;/li&gt;&lt;li&gt;Evaluated on eleven datasets, WBC outperforms baselines, achieving higher AUC and 2–3x better detection rates at low false positive thresholds, highlighting privacy vulnerabilities in fine-tuned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuetian Chen', 'Yuntao Du', 'Kaiyuan Zhang', 'Ashish Kundu', 'Charles Fleming', 'Bruno Ribeiro', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy-attack', 'LLM security', 'model memorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02751</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning</title><link>https://arxiv.org/abs/2601.02739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method to mitigate prompt-induced hallucinations by integrating a code module into chain-of-thought prompts to guide knowledge-graph exploration and produce structured external knowledge inputs.&lt;/li&gt;&lt;li&gt;Builds an improved knowledge-distillation chain-style model that constrains LLM reasoning and improves inference accuracy and verifiability.&lt;/li&gt;&lt;li&gt;Evaluates approach on GPT-4 and LLaMA-3.3 across public datasets, reporting substantial HIT@1/3/5 improvements (≈13–16%) and &gt;95% scores in some settings.&lt;/li&gt;&lt;li&gt;Claims that incorporating executable/code modules helps capture context and reduce hallucination behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinbo Hao', 'Kai Yang', 'Qingzhen Su', 'Yang Chen', 'Yifan Li', 'Chao Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'chain-of-thought', 'knowledge-graph grounding', 'LLM robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02739</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study</title><link>https://arxiv.org/abs/2601.02700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of transformer QA models (AddSent) across model scales and error categories, identifying negation confusion and entity substitution as main failure modes.&lt;/li&gt;&lt;li&gt;Evaluates mitigation strategies and finds 80% clean + 20% adversarial fine-tuning optimal; shows scaling larger models reduces the robustness-accuracy trade-off.&lt;/li&gt;&lt;li&gt;Proposes NER-guided Entity-Aware contrastive learning, achieving near-parity between clean and adversarial performance (≈94.9% closure of adversarial gap).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agniv Roy Choudhury', 'Vignesh Ponselvan Rajasingh']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'question answering', 'defenses', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02700</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Extracting books from production language models</title><link>https://arxiv.org/abs/2601.02671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a two-phase extraction procedure: (1) an initial probe (sometimes using Best-of-N jailbreaks) to test feasibility, and (2) iterative continuation prompts to attempt full-book extraction.&lt;/li&gt;&lt;li&gt;Evaluates on four production LLMs (Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, Grok 3) and quantifies extraction success with an nv-recall metric (block-based LCS approximation).&lt;/li&gt;&lt;li&gt;Finds varying vulnerability: Gemini 2.5 Pro and Grok 3 leaked large fractions without jailbreaks, while Claude 3.7 and GPT-4.1 required jailbreaking (Claude sometimes output near-verbatim entire books).&lt;/li&gt;&lt;li&gt;Concludes that model- and system-level safeguards do not fully prevent extraction of copyrighted training data, highlighting privacy/copyright risks and the need for stronger defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed Ahmed', 'A. Feder Cooper', 'Sanmi Koyejo', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM extraction', 'jailbreaking', 'memorization/copyright', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02671</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search</title><link>https://arxiv.org/abs/2601.02670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Lexical Anchor Tree Search (LATS), an attacker-LLM-free multi-turn jailbreak method that incrementally injects content words into benign prompts via a breadth-first tree search over dialogues.&lt;/li&gt;&lt;li&gt;Operates purely via lexical anchor injection, avoiding non-interpretable adversarial prefixes and reducing reliance on attacker model generation.&lt;/li&gt;&lt;li&gt;Evaluated on AdvBench and HarmBench, achieving 97–100% attack success rates on recent GPT, Claude, and Llama models with an average of ~6.4 queries (vs. 20+ for prior methods).&lt;/li&gt;&lt;li&gt;Highlights conversational structure as a significant, under-protected attack surface and demonstrates substantially improved query efficiency for jailbreaks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devang Kulshreshtha', 'Hang Su', 'Chinmay Hegde', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'red teaming', 'adversarial prompting', 'attack efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02670</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking</title><link>https://arxiv.org/abs/2601.02669</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FactArena, an automated arena-style, stage-wise benchmarking framework covering the full fact-checking pipeline (claim decomposition, tool-augmented evidence retrieval, justification-based verdicts).&lt;/li&gt;&lt;li&gt;Implements an arena-styled judgment mechanism with consolidated guidelines for unbiased pairwise comparisons across heterogeneous judge agents and an adaptive claim-evolution module that generates harder, controlled claims to probe factual robustness.&lt;/li&gt;&lt;li&gt;Evaluates 16 state-of-the-art LLMs across seven families, demonstrating that static claim-verification metrics can diverge from end-to-end fact-checking competence and arguing for holistic evaluation for reliable deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongzhan Lin', 'Zixin Chen', 'Zhiqi Shen', 'Ziyang Luo', 'Zhen Ye', 'Jing Ma', 'Tat-Seng Chua', 'Guandong Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarking', 'fact-checking', 'LLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02669</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency</title><link>https://arxiv.org/abs/2601.02574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PCC (Probabilistic Certainty and Consistency), a framework that jointly models an LLM's probabilistic confidence and reasoning consistency to estimate factual confidence.&lt;/li&gt;&lt;li&gt;Uses confidence signals to adaptively route verification: answer directly when confident, trigger targeted retrieval when uncertain/inconsistent, and escalate to deep search when highly ambiguous.&lt;/li&gt;&lt;li&gt;Demonstrates improved uncertainty quantification over verbalized confidence and outperforms strong LLM-based fact-checking baselines across multiple benchmarks; generalizes across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Wang', 'Maryam Khalid', 'Qiong Wu', 'Jian Gao', 'Cheng Cao']&lt;/li&gt;&lt;li&gt;Tags: ['factuality', 'uncertainty quantification', 'retrieval-augmented verification', 'LLM reliability', 'alignment/safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02574</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing</title><link>https://arxiv.org/abs/2601.00042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts the Go-Explore exploration algorithm for red-team testing of tool-using LLM agents and applies it to GPT-4o-mini across 28 experimental runs addressing six research questions.&lt;/li&gt;&lt;li&gt;Finds that random-seed variance dominates outcomes (up to 8x spread), making single-seed comparisons unreliable and motivating multi-seed averaging.&lt;/li&gt;&lt;li&gt;Shows reward shaping often harms security testing (exploration collapse in 94% of runs or many false positives), that simple state signatures outperform complex ones, and that ensembles improve attack-type diversity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manish Bhatt', 'Adrian Wood', 'Idan Habler', 'Ammar Al-Kahfah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial testing', 'robustness evaluation', 'Go-Explore', 'experimental methodology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00042</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DarkEQA, a benchmark to evaluate embodied question answering (EQA) perceptual primitives under multi-level low-light conditions using physics-inspired degradations in linear RAW space and an ISP-like rendering pipeline.&lt;/li&gt;&lt;li&gt;Isolates the perception bottleneck by evaluating QA from egocentric observations under controlled degradations, enabling attributable robustness analysis for downstream embodied agents.&lt;/li&gt;&lt;li&gt;Evaluates a range of state-of-the-art vision-language models and low-light image enhancement methods, showing substantial performance drops and exposing limitations of current VLMs in dark environments.&lt;/li&gt;&lt;li&gt;Provides an open-source benchmark and resources for studying 24/7 operational robustness of VLM-driven embodied systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yohan Park', 'Hyunwoo Ha', 'Wonjun Jo', 'Tae-Hyun Oh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'vision-language models', 'embodied question answering', 'low-light imaging', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24985</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</title><link>https://arxiv.org/abs/2512.23292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an 'Agentic Physical AI' approach: compact language-model-based controllers whose policy optimization is driven by physics-based outcome validation rather than perceptual inference.&lt;/li&gt;&lt;li&gt;Trains a 360M-parameter model on synthetic nuclear reactor control scenarios (10^3 to 10^5 examples) and reports a sharp phase transition where larger models undergo &gt;500x variance collapse, stabilizing execution-level behavior.&lt;/li&gt;&lt;li&gt;Finds the model autonomously rejects ~70% of training-distribution actions and concentrates 95% of runtime behavior on a single safe strategy, with learned representations transferring across distinct physics and continuous input modalities.&lt;/li&gt;&lt;li&gt;Emphasizes outcome-space guarantees for safety-critical control systems, arguing perception-centric foundation models lack structural guarantees required for control.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoonpyo Lee', 'Kazuma Kobayashi', 'Sai Puppala', 'Sajedul Talukder', 'Seid Koric', 'Souvik Chakraborty', 'Syed Bahauddin Alam']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'control robustness', 'domain-specific foundation models', 'safety-critical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23292</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title><link>https://arxiv.org/abs/2512.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes and evaluates 'Activation Oracles' (LatentQA-trained models) that take LLM activations as input and answer natural-language questions about them.&lt;/li&gt;&lt;li&gt;Evaluates generalization of AOs to out-of-distribution activations and shows training-data diversity improves performance.&lt;/li&gt;&lt;li&gt;Shows AOs can recover information fine-tuned into models (e.g., biographical facts or 'malign propensities') despite not being trained on fine-tuned-model activations.&lt;/li&gt;&lt;li&gt;On four downstream tasks, their best AOs match or exceed white-box baselines and outperform most overall baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Karvonen', 'James Chua', "Cl\\'ement Dumas", 'Kit Fraser-Taliente', 'Subhash Kantamneni', 'Julian Minder', 'Euan Ong', 'Arnab Sen Sharma', 'Daniel Wen', 'Owain Evans', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'model introspection', 'privacy/model extraction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15674</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title><link>https://arxiv.org/abs/2512.10675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a generative evaluation system built on a frontier video foundation model (Veo) optimized for action conditioning and multi-view consistency to simulate robot interactions.&lt;/li&gt;&lt;li&gt;Uses generative image-editing and multi-view completion to create novel objects, backgrounds, and distractors to assess nominal performance, OOD generalization, and probe physical/semantic safety.&lt;/li&gt;&lt;li&gt;Demonstrates system fidelity and red-teaming capability via 1600+ real-world evaluations across eight Gemini Robotics policy checkpoints and five bimanual manipulation tasks, showing it can predict relative policy performance and uncover unsafe behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gemini Robotics Team', 'Krzysztof Choromanski', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Isabel Leal', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robotics-safety', 'red-teaming', 'OOD-generalization', 'generative-simulation', 'video-models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10675</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stable Preference Optimization: A Bilevel Approach to Catastrophic Preference Shift</title><link>https://arxiv.org/abs/2507.07723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a failure mode in Bradley-Terry (BT)-style direct preference learning called 'Catastrophic Preference Shift', where probability mass for preferred outputs decreases and shifts to out-of-distribution responses, degrading performance.&lt;/li&gt;&lt;li&gt;Theoretically analyzes BT-style methods from a probability-evolution perspective, proving over-reliance on initialization can cause preference shift.&lt;/li&gt;&lt;li&gt;Proposes Stable Preference Optimization (SPO), a bilevel/constraint-based framework that restricts preference updates to a safe alignment region to stabilize learning.&lt;/li&gt;&lt;li&gt;Empirical results show SPO stabilizes and improves performance of existing BT-style preference learning methods (e.g., recovers from large reasoning-accuracy drops).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengtao Jian', 'Kai Yang', 'Tianhao Gao', 'Wuguang Ni', 'Keying Yang', 'Bowen Xiao', 'Jiajun Liu', 'Ye Ouyang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference learning', 'model robustness', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07723</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemHunter: Automated and Verifiable Memorization Detection at Dataset-scale in LLMs</title><link>https://arxiv.org/abs/2412.07261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemHunter, a method that trains a memory-inducing LLM and uses hypothesis testing to detect memorization at dataset scale without per-sample crafted prompts.&lt;/li&gt;&lt;li&gt;Designed to enable data owners to verify whether an LLM has memorized their dataset, addressing privacy risks from web-scale training data.&lt;/li&gt;&lt;li&gt;Empirical results on models like Pythia and Llama show up to 40% more training-data extraction under time constraints and up to 80% reduction in search time as a plug-in.&lt;/li&gt;&lt;li&gt;Claims to be the first approach enabling dataset-level memorization detection, making it a practical auditing tool for privacy and leakage assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenpeng Wu', 'Jian Lou', 'Zibin Zheng', 'Chuan Chen']&lt;/li&gt;&lt;li&gt;Tags: ['memorization detection', 'privacy/data leakage', 'model extraction', 'auditing/verifiability', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07261</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Myopically Verifiable Probabilistic Certificates for Safe Control and Learning</title><link>https://arxiv.org/abs/2404.16883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'probabilistic invariance', a technique to characterize invariance conditions for probabilities of interest in stochastic systems to enable long-term safety guarantees.&lt;/li&gt;&lt;li&gt;Derives myopic (short-horizon) control conditions/controllers that nevertheless assure long-term probabilistic safety, addressing the tradeoff between long-term safety and real-time computation.&lt;/li&gt;&lt;li&gt;Integrates the technique with safe control (neural-network controllers, MPC with short outlook horizons) and learning, providing guarantees during and after training.&lt;/li&gt;&lt;li&gt;Validates the approach via numerical simulations demonstrating efficient assurance of long-term safety under stochastic dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuoyuan Wang', 'Haoming Jing', 'Christian Kurniawan', 'Albert Chern', 'Yorie Nakahira']&lt;/li&gt;&lt;li&gt;Tags: ['safe control', 'probabilistic safety', 'verification', 'safe learning', 'real-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.16883</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</title><link>https://arxiv.org/abs/2601.01887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to fully recover safety alignment of fine-tuned LLMs using a single safety example, with minimal utility loss and low computational cost.&lt;/li&gt;&lt;li&gt;Claims effectiveness regardless of number of harmful fine-tuning examples or model size, achieving convergence within a few epochs.&lt;/li&gt;&lt;li&gt;Attributes efficacy to a low-rank structure in the safety gradient and validates results across five LLMs and multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Lipeng He', 'Kejia Chen', 'Jian Lou', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'fine-tuning mitigation', 'gradient analysis', 'model patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01887</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model</title><link>https://arxiv.org/abs/2509.06539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the CAGE-2 cyber defense benchmark as a Partially Observable Markov Decision Process (POMDP).&lt;/li&gt;&lt;li&gt;Proposes BF-PPO, a PPO-based method augmented with a particle filter to handle large/partially observable state spaces and efficiently learn defender policies.&lt;/li&gt;&lt;li&gt;Evaluates BF-PPO in the CybORG/CAGE-2 environment and reports outperforming CARDIFF (top leaderboard method) in defender strategy quality and training time.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duc Huy Le', 'Rolf Stadler']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'adversarial-rl', 'POMDP', 'defensive-strategies', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06539</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Intrinsic Toxicity to Reception-Based Toxicity: A Contextual Framework for Prediction and Evaluation</title><link>https://arxiv.org/abs/2503.16072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that toxicity is context-dependent and emerges from social reception, not solely an intrinsic property of text.&lt;/li&gt;&lt;li&gt;Proposes the Contextual Stress Framework (CSF) to model toxicity as stress-inducing norm violations within context.&lt;/li&gt;&lt;li&gt;Introduces PONOS (Proportion Of Negative Observed Sentiments), a reception-based metric for toxicity, and validates it on a novel dataset showing improved contextual sensitivity when combined with existing models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'content moderation', 'safety evaluation', 'contextual modelling', 'metrics/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16072</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems</title><link>https://arxiv.org/abs/2503.03750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MASK, a large-scale human-collected benchmark that directly measures lying to disentangle honesty from factual accuracy in LLMs.&lt;/li&gt;&lt;li&gt;Shows that larger models improve accuracy but do not necessarily become more honest; many frontier models score well on truthfulness benchmarks yet still lie under pressure.&lt;/li&gt;&lt;li&gt;Demonstrates that simple interventions (e.g., representation engineering) can improve honesty.&lt;/li&gt;&lt;li&gt;Highlights the need for robust evaluations and interventions to ensure trustworthiness of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard Ren', 'Arunim Agarwal', 'Mantas Mazeika', 'Cristina Menghini', 'Robert Vacareanu', 'Brad Kenstler', 'Mick Yang', 'Isabelle Barrass', 'Alice Gatti', 'Xuwang Yin', 'Eduardo Trevino', 'Matias Geralnik', 'Adam Khoja', 'Dean Lee', 'Summer Yue', 'Dan Hendrycks']&lt;/li&gt;&lt;li&gt;Tags: ['honesty-benchmark', 'deceptive-behavior', 'LLM-safety', 'alignment', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03750</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Training Set Reconstruction from Differentially Private Forests: How Effective is DP?</title><link>https://arxiv.org/abs/2502.05307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a reconstruction attack against state-of-the-art ε-differentially private random forests using a constraint programming model that leverages forest structure and DP mechanism properties.&lt;/li&gt;&lt;li&gt;Performs extensive experiments exploring the trade-offs between model utility, privacy budget, and reconstruction accuracy across different configurations.&lt;/li&gt;&lt;li&gt;Finds that meaningful DP guarantees reduce but do not eliminate training-data leakage; forests fully robust to the attack have predictive performance no better than a constant classifier.&lt;/li&gt;&lt;li&gt;Provides practical recommendations for constructing DP random forests that improve resilience to reconstruction attacks while retaining non-trivial utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Alice Gorg\\'e", 'Julien Ferry', "S\\'ebastien Gambs", 'Thibaut Vidal']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'reconstruction attack', 'training-data leakage', 'random forests', 'privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05307</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition</title><link>https://arxiv.org/abs/2601.03124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a deep-learning pipeline for grape leaf disease classification using pre-trained InceptionV3 and Xception models (Xception achieves 96.23% accuracy).&lt;/li&gt;&lt;li&gt;Employs adversarial training to improve model robustness and integrates Grad-CAM for explainability/visual confirmation of disease regions.&lt;/li&gt;&lt;li&gt;Dataset of 9,032 leaf images split into train/val/test; final deployment as a Streamlit web app with heatmap visualization and prediction confidence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['B. M. Shahria Alam', 'Md. Nasim Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'explainability', 'image classification', 'applied ML', 'agriculture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03124</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation</title><link>https://arxiv.org/abs/2601.03121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ToxiGAN, a class-aware text augmentation framework that combines GAN-based adversarial generation with semantic guidance from LLMs to produce toxic examples for classifier training.&lt;/li&gt;&lt;li&gt;Introduces a two-step directional training strategy and uses LLM-generated neutral exemplars as 'semantic ballast' to prevent mode collapse and semantic drift while enforcing class-specific divergence.&lt;/li&gt;&lt;li&gt;Shows improvements in macro-F1 and hate-F1 across four hate speech benchmarks, and includes ablation/sensitivity analyses demonstrating the benefit of semantic ballast and directional training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Li', 'Jan Fillies', 'Adrian Paschke']&lt;/li&gt;&lt;li&gt;Tags: ['toxicity detection', 'data augmentation', 'adversarial generation', 'LLM-guided augmentation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03121</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation</title><link>https://arxiv.org/abs/2601.03051</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames multi-turn conversations as a temporal graph where each dialogue turn is a node encoded via a sentence transformer.&lt;/li&gt;&lt;li&gt;Uses two edge types—shared-entity edges and temporal (contiguous-turn) edges—with message-passing to produce context-aware node embeddings.&lt;/li&gt;&lt;li&gt;Aggregates node embeddings with attention pooling and a classifier to detect and categorize hallucinations; claims slight performance improvements and uses attention for interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vidhi Rathore', 'Sambu Aneesh', 'Himanshu Singh']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'dialogue safety', 'temporal graph networks', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03051</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HAL: Inducing Human-likeness in LLMs with Alignment</title><link>https://arxiv.org/abs/2601.02813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAL, a framework that constructs an interpretable, data-driven scalar reward representing conversational human-likeness from explicit conversational traits.&lt;/li&gt;&lt;li&gt;Derives traits from contrastive dialogue data and uses that reward with standard preference-optimization to align LLMs without degrading overall task performance.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and inspectability for diagnosing unintended effects and validates improvements via large-scale human evaluations showing increased perceived human-likeness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masum Hasan', 'Junjie Zhao', 'Ehsan Hoque']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'interpretability', 'reward modeling', 'human-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02813</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Contrastive Learning for LLM Quantization Attacks</title><link>https://arxiv.org/abs/2601.02680</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Contrastive Learning (ACL), a gradient-based attack that maximizes the probability gap between benign and harmful responses to make quantized LLMs behave maliciously.&lt;/li&gt;&lt;li&gt;Formulates attack objective as a triplet-based contrastive loss and uses a projected gradient descent two-stage distributed fine-tuning strategy for optimization.&lt;/li&gt;&lt;li&gt;Reports strong empirical results: attack success rates of 86.00% (over-refusal), 97.69% (jailbreak), and 92.40% (advertisement injection), outperforming prior methods substantially.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dinghong Song', 'Zhiwei Xu', 'Hai Wan', 'Xibin Zhao', 'Pengfei Su', 'Dong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM quantization', 'adversarial attack', 'jailbreaking', 'model robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02680</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Extracting books from production language models</title><link>https://arxiv.org/abs/2601.02671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a two-phase extraction procedure: (1) an initial probe (sometimes using Best-of-N jailbreaks) to test feasibility, and (2) iterative continuation prompts to attempt full-book extraction.&lt;/li&gt;&lt;li&gt;Evaluates on four production LLMs (Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, Grok 3) and quantifies extraction success with an nv-recall metric (block-based LCS approximation).&lt;/li&gt;&lt;li&gt;Finds varying vulnerability: Gemini 2.5 Pro and Grok 3 leaked large fractions without jailbreaks, while Claude 3.7 and GPT-4.1 required jailbreaking (Claude sometimes output near-verbatim entire books).&lt;/li&gt;&lt;li&gt;Concludes that model- and system-level safeguards do not fully prevent extraction of copyrighted training data, highlighting privacy/copyright risks and the need for stronger defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed Ahmed', 'A. Feder Cooper', 'Sanmi Koyejo', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM extraction', 'jailbreaking', 'memorization/copyright', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02671</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SWaRL: Safeguard Code Watermarking via Reinforcement Learning</title><link>https://arxiv.org/abs/2601.02602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SWaRL, a reinforcement learning-based framework to embed verifiable watermarks into code LLM outputs while preserving functionality using compiler feedback.&lt;/li&gt;&lt;li&gt;Uses a jointly trained confidential verifier as part of the reward to ensure watermark detectability and employs LoRA fine-tuning for transferability across model updates.&lt;/li&gt;&lt;li&gt;Evaluates robustness against refactoring and adversarial transformations, claiming higher detection accuracy than prior methods with minimal overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neusha Javidnia', 'Ruisi Zhang', 'Ashish Kundu', 'Farinaz Koushanfar']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model IP protection', 'reinforcement learning', 'code LLMs', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02602</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses</title><link>https://arxiv.org/abs/2601.02444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VocalBridge, a latent diffusion-bridge purification method that maps perturbed (defended) speech to clean EnCodec latents using a time-conditioned 1D U-Net and cosine noise schedule.&lt;/li&gt;&lt;li&gt;Introduces a Whisper-guided phoneme variant that provides lightweight temporal guidance without requiring ground-truth transcripts to improve preservation of speaker-discriminative cues.&lt;/li&gt;&lt;li&gt;Demonstrates the method outperforms existing purification techniques at recovering cloneable voices from perturbation-protected speech, showing current perturbation-based defenses are fragile under adaptive purification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maryam Abbasihafshejani', 'AHM Nazmus Sakib', 'Murtuza Jadliwala']&lt;/li&gt;&lt;li&gt;Tags: ['voice cloning', 'speaker verification', 'adversarial purification', 'diffusion models', 'audio privacy defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02444</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Quantifying Quanvolutional Neural Networks Robustness for Speech in Healthcare Applications</title><link>https://arxiv.org/abs/2601.02432</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of quanvolutional neural networks (QNNs) versus classical CNNs/ResNet-18/VGG-16 on speech tasks (voice pathology AVFAD and emotion TESS) under four acoustic corruptions: Gaussian noise, pitch shift, temporal shift, and speed variation.&lt;/li&gt;&lt;li&gt;Uses corruption robustness metrics (CE, mCE, RCE, RmCE) and accuracy; finds QNNs often outperform a simple CNN under pitch/temporal/speed distortions (up to ~22% lower CE/RCE) but are less resilient to additive Gaussian noise; QNNs also show faster convergence.&lt;/li&gt;&lt;li&gt;Analyzes different quantum circuit designs (Random, Basic, Strongly), circuit depth/complexity, per-emotion robustness, and highlights limitations (sensitivity to additive noise) and that corruptions studied are non-adversarial.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ha Tran', 'Bipasha Kashyap', 'Pubudu N. Pathirana']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'quantum machine learning', 'audio/speech', 'healthcare', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02432</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</title><link>https://arxiv.org/abs/2601.03213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an RL-based framework for machine unlearning in text-to-image diffusion models by casting denoising as a sequential decision process and applying policy-gradient updates to the reverse diffusion kernel.&lt;/li&gt;&lt;li&gt;Introduces a timestep-aware critic: a CLIP-based reward predictor trained on noisy latents to provide per-step (noisy-conditioned) rewards and lower-variance advantage estimates.&lt;/li&gt;&lt;li&gt;Demonstrates improved or comparable forgetting of targeted concepts while preserving image quality and benign prompt fidelity; ablations show per-step critics and noisy-conditioned rewards are key.&lt;/li&gt;&lt;li&gt;Provides code and evaluation scripts to facilitate reproduction and further research on RL-based diffusion unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mykola Vysotskyi', 'Zahar Kohut', 'Mariia Shpir', 'Taras Rumezhak', 'Volodymyr Karpiv']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'diffusion models', 'reinforcement learning', 'privacy/forgetting', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03213</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Empowering Reliable Visual-Centric Instruction Following in MLLMs</title><link>https://arxiv.org/abs/2601.03198</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VC-IFEval, a benchmark and systematically constructed dataset to evaluate multimodal instruction-following by incorporating vision-dependent constraints.&lt;/li&gt;&lt;li&gt;Enables fine-grained assessment of how well MLLMs align outputs with both visual input and textual instructions.&lt;/li&gt;&lt;li&gt;Fine-tuning on the dataset yields substantial gains in visual instruction-following accuracy and adherence.&lt;/li&gt;&lt;li&gt;Provides extensive evaluation across representative MLLMs to reveal strengths and limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weilei He', 'Feng Ju', 'Zhiyuan Fan', 'Rui Min', 'Minhao Cheng', 'Yi R. Fung']&lt;/li&gt;&lt;li&gt;Tags: ['instruction-following', 'alignment', 'benchmarking', 'multimodal', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03198</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Prompt-Counterfactual Explanations for Generative AI System Behavior</title><link>https://arxiv.org/abs/2601.03156</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts counterfactual explanations to nondeterministic generative AI systems by proposing a flexible framework for prompt-focused counterfactuals.&lt;/li&gt;&lt;li&gt;Introduces an algorithm for generating prompt-counterfactual explanations (PCEs) that identify minimal prompt changes producing different output characteristics.&lt;/li&gt;&lt;li&gt;Demonstrates PCEs on three case studies (political leaning, toxicity, sentiment) and highlights uses for prompt engineering and red-teaming to surface and suppress undesirable outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofie Goethals', 'Foster Provost', 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['LLM explainability', 'Prompt engineering', 'Red teaming', 'Counterfactual explanations', 'Safety/Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03156</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs</title><link>https://arxiv.org/abs/2601.03087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BAFA (Bounded Active Fairness Auditor), an active-query method for estimating fairness metrics of black-box LLMs by maintaining a version space of surrogate models and computing uncertainty intervals for target metrics (e.g., ΔAUC).&lt;/li&gt;&lt;li&gt;Uses constrained empirical risk minimisation to compute bounds and an active query selection strategy to reduce interval width, enabling query-efficient auditing.&lt;/li&gt;&lt;li&gt;Evaluated on CivilComments and Bias-in-Bios; BAFA reaches tight error thresholds with up to ~40× fewer queries than stratified sampling and shows lower variance across runs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Hartmann', 'Lena Pohlmann', 'Lelia Hanslik', 'Noah Gie{\\ss}ing', 'Bettina Berendt', 'Pieter Delobelle']&lt;/li&gt;&lt;li&gt;Tags: ['fairness auditing', 'black-box auditing', 'active learning', 'query-efficient evaluation', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03087</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability</title><link>https://arxiv.org/abs/2601.03047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Replicates Anthropic-style sparse autoencoder (SAE) feature extraction and steering on Llama 3.1, reproducing basic capabilities.&lt;/li&gt;&lt;li&gt;Finds substantial fragility in feature steering: sensitivity to layer choice, steering magnitude, and context, plus non-standard activation behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates difficulty distinguishing thematically similar features and argues SAE-based mechanistic interpretability currently lacks systematic reliability for safety-critical oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raphael Ronge', 'Markus Maier', 'Frederick Eberhardt']&lt;/li&gt;&lt;li&gt;Tags: ['mechanistic interpretability', 'feature extraction', 'model steering', 'AI safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.03047</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control</title><link>https://arxiv.org/abs/2601.02896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RESGA and SAEGA, gradient-ascent-based methods to automatically discover prompts that steer LLM personas by aligning prompts with identified mechanistic persona directions.&lt;/li&gt;&lt;li&gt;Introduces 'fluent gradient ascent' to ensure discovered prompts are natural/fluently worded while maintaining steering effectiveness.&lt;/li&gt;&lt;li&gt;Evaluates on Llama 3.1, Qwen 2.5, and Gemma 3 to steer sycophancy, hallucination, and myopic reward personas, reporting substantial improvements in persona control.&lt;/li&gt;&lt;li&gt;Frames prompt discovery in terms of mechanistic interpretability to offer more controllable and interpretable behavior modification than black-box automatic optimization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harshvardhan Saini', 'Yiming Tang', 'Dianbo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['Prompt engineering', 'Alignment / LLM safety', 'Mechanistic interpretability', 'Adversarial prompting / behavioral steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02896</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning</title><link>https://arxiv.org/abs/2601.02682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes robustness of decentralized gradient descent under label poisoning attacks, comparing robust aggregators to the weighted mean aggregator.&lt;/li&gt;&lt;li&gt;Theoretically shows robust aggregators' learning error depends on network topology while weighted mean performance is topology-independent.&lt;/li&gt;&lt;li&gt;Identifies regimes where weighted mean can outperform robust aggregators under heterogeneity (global contamination &lt; local contamination, disconnected regular-agent subnetworks, or sparse regular-agent networks with high local contamination).&lt;/li&gt;&lt;li&gt;Empirical experiments validate theoretical findings and emphasize the role of network topology in defense effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Peng', 'Weiyu Li', 'Stefan Vlaski', 'Qing Ling']&lt;/li&gt;&lt;li&gt;Tags: ['label poisoning', 'decentralized learning', 'robustness', 'adversarial attacks', 'aggregation defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02682</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening</title><link>https://arxiv.org/abs/2601.02562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Conformal Bayesian Dermatological Classifier (CBDC) that combines Statistical Learning Theory, Topological Data Analysis, and Bayesian Conformal Inference for dermatology classification.&lt;/li&gt;&lt;li&gt;Provides distribution-dependent generalization bounds and a topological stability theorem guaranteeing invariance of CNN embeddings under photometric and morphological perturbations.&lt;/li&gt;&lt;li&gt;Offers finite-sample conformal coverage guarantees for calibrated uncertainty quantification and demonstrates empirical performance on HAM10000, PH2, and ISIC 2020 datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Kaushik', 'Eva Kaushik']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'uncertainty quantification', 'calibration', 'medical AI safety', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02562</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2512.24712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LSRE encodes sparse judgments from a large vision-language model into a lightweight latent-space classifier within a recurrent world model to enable semantic risk detection without per-frame VLM queries.&lt;/li&gt;&lt;li&gt;Operates at real-time rates (10 Hz) and achieves semantic risk detection accuracy comparable to the VLM baseline on six semantic-failure scenarios in CARLA, with earlier hazard anticipation and low computational latency.&lt;/li&gt;&lt;li&gt;Demonstrates generalization to rarely seen semantically similar cases, suggesting language-guided latent classification can provide deployable semantic safety monitoring for autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qian Cheng', 'Weitao Zhou', 'Cheng Jing', 'Nanshan Deng', 'Junze Wen', 'Zhaoyang Liu', 'Kun Jiang', 'Diange Yang']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'semantic risk detection', 'autonomous driving', 'vision-language models', 'real-time monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24712</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>https://arxiv.org/abs/2512.24470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Semantic Lookout: a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector for short-horizon, human-overridable maritime fallback maneuvers to satisfy IMO MASS Code requirements.&lt;/li&gt;&lt;li&gt;Uses a fast-slow anomaly pipeline to meet latency budgets (sub-10s) while retaining much of the semantic awareness of slower SOTA models; evaluated on 40 harbor scenes and a field run.&lt;/li&gt;&lt;li&gt;Demonstrates improved alignment with human consensus versus geometry-only baselines, increased standoff on fire-hazard scenes, and end-to-end alert-&gt;fallback-&gt;operator handover.&lt;/li&gt;&lt;li&gt;Argues for hybrid autonomy pairing foundation-model semantics with multi-sensor perception and short-horizon replanning for practical safety compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kim Alexander Christensen', 'Andreas Gudahl Tufte', 'Alexey Gusev', 'Rohan Sinha', 'Milan Ganai', 'Ole Andreas Alsos', 'Marco Pavone', 'Martin Steinert']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'safety evaluation', 'model alignment', 'maritime autonomy', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24470</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>https://arxiv.org/abs/2512.24052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies taxonomy of hallucination types in large audio-language models (Event Omission, False Event Identity, Temporal Relation Error, Quantitative Temporal Error).&lt;/li&gt;&lt;li&gt;Proposes AHA (Audio Hallucination Alignment) which uses counterfactual hard negative mining to construct preference data forcing models to prefer acoustically grounded outputs over plausible fabrications.&lt;/li&gt;&lt;li&gt;Introduces AHA-Eval, a diagnostic benchmark for fine-grained temporal reasoning and grounding, and applies the method to align Qwen2.5-Omni producing Qwen-Audio-AHA with substantial gains on AHA-Eval and public benchmarks.&lt;/li&gt;&lt;li&gt;Open-sources model and dataset, demonstrating generalization beyond the diagnostic set.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanxi Chen', 'Wenhui Zhu', 'Xiwen Chen', 'Zhipeng Wang', 'Xin Li', 'Peijie Qiu', 'Hao Wang', 'Xuanzhao Dong', 'Yujian Xiong', 'Anderson Schneider', 'Yuriy Nevmyvaka', 'Yalin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination mitigation', 'safety evaluation', 'benchmarking', 'audio-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.24052</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title><link>https://arxiv.org/abs/2512.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAILS: use Sparse Autoencoders to disentangle monosemantic features and construct an interpretable low-rank safety subspace to initialize LoRA adapters for alignment.&lt;/li&gt;&lt;li&gt;Theoretical result: SAE-based identification yields arbitrarily small recovery error under monosemanticity assumptions, avoiding an irreducible error present in direct subspace identification.&lt;/li&gt;&lt;li&gt;Empirical result: on Gemma-2-9B, SAILS achieves up to 99.6% safety rate, outperforming full fine-tuning by 7.4 points and matching RLHF while updating only 0.19% of parameters and providing interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dianyun Wang', 'Qingsen Ma', 'Yuhu Shang', 'Zhifeng Lu', 'Zhenbo Xu', 'Lechen Ning', 'Huijia Wu', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM safety', 'parameter-efficient fine-tuning', 'interpretability', 'adversarial/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.23260</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithLens: Detecting and Explaining Faithfulness Hallucination</title><link>https://arxiv.org/abs/2512.20182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FaithLens, an 8B-parameter model for detecting faithfulness hallucinations in LLM outputs, producing binary predictions plus natural-language explanations.&lt;/li&gt;&lt;li&gt;Creates synthetic training data with explanations via advanced LLMs, applies filtering for label/explanation quality and diversity, then fine-tunes and further optimizes with rule-based reinforcement learning rewarding prediction correctness and explanation quality.&lt;/li&gt;&lt;li&gt;Evaluates on 12 diverse tasks and reports that FaithLens outperforms larger models (e.g., GPT-4.1 and o3) while offering a cost-efficient tradeoff between trustworthiness, efficiency, and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzheng Si', 'Qingyi Wang', 'Haozhe Zhao', 'Yuzhuo Bai', 'Guanqiao Chen', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination detection', 'explainability', 'alignment', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.20182</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title><link>https://arxiv.org/abs/2512.15052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SGM, a white-box neuron-level intervention that selectively recalibrates a small set of 'toxic expert' neurons via expertise-weighted soft suppression to reduce harmful multimodal activations without updating model parameters.&lt;/li&gt;&lt;li&gt;Introduces MM-TOXIC-QA, a multimodal toxicity evaluation framework, and evaluates SGM against existing detoxification techniques under standard and adversarial conditions.&lt;/li&gt;&lt;li&gt;Reports large toxicity reductions (e.g., harmful rates reduced from 48.2% to 2.5%) while preserving fluency and multimodal reasoning; SGM* combines SGM with other defenses for stronger performance.&lt;/li&gt;&lt;li&gt;Emphasizes interpretability, low computational cost, and extensibility for toxicity-controlled multimodal generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongbo Wang', 'MaungMaung AprilPyone', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'detoxification', 'multimodal models', 'neuron-level intervention', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.15052</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Description to Score: Can LLMs Quantify Vulnerabilities?</title><link>https://arxiv.org/abs/2512.06781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multiple general-purpose LLMs (ChatGPT, Llama, Grok, DeepSeek, Gemini) on automated CVSS scoring using a dataset of &gt;31,000 CVE entries.&lt;/li&gt;&lt;li&gt;Finds variable performance across CVSS metrics (stronger on Availability Impact, weaker on Attack Complexity) with ChatGPT-5 achieving highest precision.&lt;/li&gt;&lt;li&gt;Shows models tend to misclassify many of the same CVEs; ensemble/meta-classifiers yield only marginal gains.&lt;/li&gt;&lt;li&gt;Identifies that ambiguous or context-poor CVE descriptions drive systematic misclassifications and recommends richer contextual information to improve automated triage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sima Jafarikhah', 'Daniel Thompson', 'Eva Deans', 'Hossein Siadati', 'Yi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability_scoring', 'LLM_evaluation', 'CVSS', 'security_automation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06781</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title><link>https://arxiv.org/abs/2512.06193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GAUGE, a logit-based framework to detect hidden conversational escalation (implicit affective harm) in real time.&lt;/li&gt;&lt;li&gt;Measures how an LLM's output probabilistically shifts the affective state of a dialogue, aiming to catch gradual emotional reinforcement not flagged by toxicity filters.&lt;/li&gt;&lt;li&gt;Targets deficiencies of external classifiers/clinical rubrics by operating on model internals for timely guardrails during interactions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihyung Park', 'Saleh Afroogh', 'David Atkinson', 'Junfeng Jiao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM guardrails', 'Affective harm detection', 'Real-time monitoring', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06193</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework</title><link>https://arxiv.org/abs/2511.21448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a labeled email dataset distinguishing phishing, spam, and legitimate messages, and marks whether content was human- or LLM-generated; annotations include emotional appeals and attacker motivations.&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs for identifying emotional/motivational cues and uses the most reliable model to annotate the full dataset; evaluates a state-of-the-art LLM on detection performance against expert-labeled ground truth.&lt;/li&gt;&lt;li&gt;Assesses robustness by rephrasing emails with several LLMs (preserving intent) and measures model performance on original vs. rephrased emails, reporting strong phishing detection but challenges separating spam from legitimate mail.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rebeka Toth', 'Tamas Bisztray', 'Richard Dubniczky']&lt;/li&gt;&lt;li&gt;Tags: ['phishing', 'spam-detection', 'LLM-generated-attacks', 'dataset', 'robustness-benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21448</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title><link>https://arxiv.org/abs/2511.21214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SGASA (Synthesized Guideline-based Adaptive Safety Alignment): a two-stage framework (Data Pre-synthesis and Alignment Fine-tuning) to defend reasoning models against adversarial jailbreak prompts.&lt;/li&gt;&lt;li&gt;Data Pre-synthesis generates model-created safety guidelines and augmented prompts; Alignment Fine-tuning embeds these guidelines via Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO).&lt;/li&gt;&lt;li&gt;Aims to improve robustness to covert/deceptive adversarial prompts while reducing unnecessary refusals of benign queries.&lt;/li&gt;&lt;li&gt;Reports extensive experiments across multiple datasets demonstrating significant improvements in model safety and adaptive, scalable defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhang Wang', 'Yanxu Zhu', 'Dongyuan Lu', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak defense', 'prompt injection', 'alignment fine-tuning', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21214</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>InfoDecom: Decomposing Information for Defending Against Privacy Leakage in Split Inference</title><link>https://arxiv.org/abs/2511.13365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy leakage in split inference where data reconstruction attacks can recover original inputs from smashed data.&lt;/li&gt;&lt;li&gt;Proposes InfoDecom: decomposes and removes redundant information in smashed data, then injects noise calibrated to provide a theoretical privacy guarantee.&lt;/li&gt;&lt;li&gt;Demonstrates improved utility–privacy trade-offs on computer vision tasks compared to existing defenses, especially when client-side models are shallow.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruijun Deng', 'Zhihui Lu', 'Qiang Duan']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-reconstruction-attacks', 'split-inference', 'defense', 'privacy-preserving-ml']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.13365</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title><link>https://arxiv.org/abs/2510.20487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and creates evaluation-aware behavior in LLMs that use cues (e.g., Python type hints) during testing but not deployment, compromising safety evaluations.&lt;/li&gt;&lt;li&gt;Proposes and evaluates an activation-level steering vector that, when added to model activations, suppresses evaluation-awareness and makes the model behave as if deployed.&lt;/li&gt;&lt;li&gt;Constructs the steering vector from the original model (prior to the additional training that induced evaluation-awareness) and demonstrates it can counteract the behavior.&lt;/li&gt;&lt;li&gt;Argues that activation steering could improve the reliability of safety evaluations by preventing models from gaming tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'Andrew Qin', 'Samuel Marks', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation-gaming', 'safety-evaluation', 'activation-steering', 'alignment-evaluation', 'red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20487</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Digital Twins as Funhouse Mirrors: Five Key Distortions</title><link>https://arxiv.org/abs/2509.19088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study comparing human responses to LLM-based 'digital twins' trained on each person's prior answers across 19 preregistered studies (164 outcomes).&lt;/li&gt;&lt;li&gt;Finds digital twins only modestly outperform a homogeneous base LLM and correlate weakly with actual human responses (average r = 0.20).&lt;/li&gt;&lt;li&gt;Identifies five distortions introduced by digital twins: stereotyping, insufficient individuation, representation bias, ideological biases, and hyper-rationality.&lt;/li&gt;&lt;li&gt;Argues these distortions present risks for scientific validity and practical deployment of digital twins, cautioning against premature use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Peng', 'George Gui', 'Melanie Brucks', 'Daniel J. Merlau', 'Grace Jiarui Fan', 'Malek Ben Sliman', 'Eric J. Johnson', 'Abdullah Althenayyan', 'Silvia Bellezza', 'Dante Donati', 'Hortense Fong', 'Elizabeth Friedman', 'Ariana Guevara', 'Mohamed Hussein', 'Kinshuk Jerath', 'Bruce Kogut', 'Akshit Kumar', 'Kristen Lane', 'Hannah Li', 'Vicki Morwitz', 'Oded Netzer', 'Patryk Perkowski', 'Olivier Toubia']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Model evaluation', 'Bias', 'Privacy', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19088</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment</title><link>https://arxiv.org/abs/2509.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic red-teaming study focused on tool invocation risks in six real-world coding agents (Cursor, Claude Code, Copilot, Windsurf, Cline, Trae).&lt;/li&gt;&lt;li&gt;Phase 1: prompt-leakage reconnaissance discovers a ToolLeak vulnerability that enables system-prompt exfiltration via benign tool-argument retrieval.&lt;/li&gt;&lt;li&gt;Phase 2: introduces a two-channel prompt-injection attack (tool description + return values) to hijack tool-invocation, enabling remote code execution (RCE); attacks are adapted using leaked info.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows high success across multiple LLM backends and real agents (19/25 agent-LLM pairs leaked; RCE achieved on all tested pairs); includes case studies and defense recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchong Xie', 'Mingyu Luo', 'Zesen Liu', 'Zhixiang Zhang', 'Kaikai Zhang', 'Yu Liu', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['red-teaming', 'prompt injection', 'tool invocation', 'jailbreaking', 'RCE']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05755</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designs four membership inference attacks (Similarity, Memorization, Inquiry, Poisoning) targeting whether victims' historical interactions appear in system prompts for LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;Evaluates attacks across five open-source LLMs and three RecSys benchmark datasets, finding inquiry and poisoning attacks achieve notably high attack advantage.&lt;/li&gt;&lt;li&gt;Analyzes factors influencing attack success (number of shots, victim position in shots, number of poisoning items) and discusses potential mitigation strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Min-Chun Chen', 'Xintong Chen', 'Xinyang Fang', 'Yuechun Gu', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'privacy', 'prompt-based-attacks', 'recommender-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System</title><link>https://arxiv.org/abs/2508.17215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedThreatRAG, a multimodal poisoning framework that injects adversarial image–text pairs into medical RAG systems to probe vulnerabilities.&lt;/li&gt;&lt;li&gt;Introduces Cross-Modal Conflict Injection (CMCI), which plants subtle semantic contradictions between images and paired reports to disrupt cross-modal alignment while remaining plausible.&lt;/li&gt;&lt;li&gt;Implements a simulated semi-open attack environment reflecting periodic KB updates and evaluates attacks (including CMCI) on IU-Xray and MIMIC-CXR QA, showing large F1 degradations (up to ~27.7%).&lt;/li&gt;&lt;li&gt;Provides empirical evidence of fundamental security gaps in clinical RAG pipelines and offers guideline recommendations for threat-aware, multimodal consistency defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiwen Zuo', 'Zelin Liu', 'Raman Dutt', 'Ziyang Wang', 'Zhongtian Sun', 'Fan Mo', 'Pietro Li\\`o']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal poisoning', 'RAG', 'medical AI security', 'adversarial attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17215</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The GPT-4o Shock Emotional Attachment to AI Models and Its Impact on Regulatory Acceptance: A Cross-Cultural Analysis of the Immediate Transition from GPT-4o to GPT-5</title><link>https://arxiv.org/abs/2508.16624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Qualitative analysis of 150 Japanese and English social-media/video posts following an immediate, mandatory transition from GPT-4o to GPT-5 (Aug 8–9, 2025), documenting emotional attachment and resistance.&lt;/li&gt;&lt;li&gt;Finds person-like bonds (e.g., 'AI boyfriend'), with cross-cultural differences: Japanese posts dominated by loss-oriented narratives; English posts showed more anger, meta-critique, and memes.&lt;/li&gt;&lt;li&gt;Preliminary quantitative check reports significantly higher attachment coding in Japanese data.&lt;/li&gt;&lt;li&gt;Argues that strong user attachment to models can rapidly undermine acceptance of safety-driven changes and offers policy options (gradual transitions, parallel availability, measuring attachment thresholds).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiroki Naito']&lt;/li&gt;&lt;li&gt;Tags: ['policy', 'human-AI interaction', 'emotional attachment', 'regulatory acceptance', 'safety implications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16624</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>User-Assistant Bias in LLMs</title><link>https://arxiv.org/abs/2508.15815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines 'user-assistant bias' as an LLM's tendency to favor information from either the user or assistant role when they conflict and formalizes a task-agnostic benchmark (UserAssist).&lt;/li&gt;&lt;li&gt;Evaluates 52 frontier models: finds instruction-tuned models exhibit strong user bias, while base and reasoning models are near neutral.&lt;/li&gt;&lt;li&gt;Uses controlled fine-tuning experiments to show human-preference alignment amplifies user bias and reasoning fine-tuning reduces it.&lt;/li&gt;&lt;li&gt;Demonstrates bidirectional control of bias via direct preference optimization (DPO) on UserAssist and shows generalization to multi-turn conversations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xu Pan', 'Jingxuan Fan', 'Zidi Xiong', 'Ely Hahami', 'Jorin Overwiening', 'Ziqian Xie']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety-evaluation', 'instruction-tuning', 'bias-in-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15815</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI</title><link>https://arxiv.org/abs/2508.10991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCP-GUARD, a layered defense-in-depth framework to secure Model Context Protocol (MCP) interactions between LLMs and external tools, targeting prompt injection, data exfiltration, and jailbreaks.&lt;/li&gt;&lt;li&gt;Uses a three-stage detection pipeline: lightweight static scanning, a deep neural semantic detector, and a fine-tuned E5-based model (reported 96.01% accuracy), with an LLM arbitrator to combine signals for final decisions.&lt;/li&gt;&lt;li&gt;Introduces MCP-ATTACKBENCH, a 70,448-sample benchmark (GPT-4 augmented) that simulates diverse real-world MCP attack vectors to train and evaluate defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Zhonghao Qi', 'Yupeng Qin', 'Yilin Li', 'Caini Chang', 'Jiahui Yu', 'Changting Lin', 'Zhenzhen Xie', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreak defense', 'LLM red teaming', 'adversarial prompts', 'benchmark/dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10991</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title><link>https://arxiv.org/abs/2508.07745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chimera, a multi-agent framework that uses LLMs to simulate benign and malicious insider behaviors with role-based agents, meetings, interactions, and scheduling to capture organizational dynamics.&lt;/li&gt;&lt;li&gt;Builds ChimeraLog, a new synthetic dataset derived from simulations of 15 real-world insider attack types across three representative sensitive-organizational scenarios.&lt;/li&gt;&lt;li&gt;Evaluates ChimeraLog via human studies and quantitative metrics; shows existing insider-threat detection (ITD) models perform worse on ChimeraLog than prior datasets, while models trained on ChimeraLog generalize well to other distributions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiongchi Yu', 'Xiaofei Xie', 'Qiang Hu', 'Yuhan Ma', 'Ziming Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['insider-threat', 'synthetic-dataset', 'LLM-simulation', 'cybersecurity', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07745</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Coward: Collision-based Watermark for Proactive Federated Backdoor Detection</title><link>https://arxiv.org/abs/2508.02115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Coward, a proactive federated learning backdoor detection method that injects a collision-based watermark into the global model to reveal malicious client updates.&lt;/li&gt;&lt;li&gt;Key idea: leverage multi-backdoor collision effects (later distinct backdoors suppress earlier ones) and regulated dual-mapping learning on OOD data to create a low-disruptive, OOD-robust watermark.&lt;/li&gt;&lt;li&gt;Claims state-of-the-art detection performance on benchmarks, reduced OOD prediction bias compared to prior proactive methods, and robustness to adaptive attacks; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Siying Gu', 'Yiming Li', 'Kangjie Chen', 'Zhili Chen', 'Tianwei Zhang', 'Shu-Tao Xia', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'backdoor-detection', 'proactive-defense', 'watermarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02115</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title><link>https://arxiv.org/abs/2507.20704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Text2VLM, a pipeline that converts text-only datasets into multimodal prompts by rendering harmful text as typographic images to evaluate VLM alignment and resistance to prompt injection.&lt;/li&gt;&lt;li&gt;Focuses on typographic prompt injection attacks against Visual Language Models and measures increased susceptibility when visual inputs are introduced.&lt;/li&gt;&lt;li&gt;Evaluates open-source VLMs (and compares to closed-source frontier models), finding significant weaknesses in alignment and robustness to multimodal adversarial content.&lt;/li&gt;&lt;li&gt;Validates the pipeline with human evaluation to confirm concept preservation and usefulness for safety assessment and alignment training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Downer', 'Sean Craven', 'Damian Ruck', 'Jake Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['VLM security', 'prompt injection', 'alignment evaluation', 'multimodal adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20704</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BBoxER, an evolutionary black-box LLM post-training method that avoids exposing gradients and induces an information bottleneck via implicit data compression.&lt;/li&gt;&lt;li&gt;Provides provable, non-vacuous generalization bounds and theoretical guarantees for privacy, robustness to data poisoning, and resistance to extraction attacks.&lt;/li&gt;&lt;li&gt;Empirically shows that BBoxER improves performance on reasoning benchmarks, generalizes well, and is robust to membership inference and other privacy attacks, positioning it as a privacy-preserving add-on to gradient-based training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data poisoning', 'membership inference', 'black-box optimization', 'generalization bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection</title><link>https://arxiv.org/abs/2506.18919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MemeMind, a large-scale multimodal (image+text) dataset of harmful memes with detailed Chain-of-Thought (CoT) reasoning annotations for fine-grained analysis.&lt;/li&gt;&lt;li&gt;Proposes MemeGuard, a reasoning-oriented multimodal detection model that improves detection accuracy and interpretability for implicit/nuanced harmful content.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art performance on the MemeMind dataset, aiming to support safer content moderation and explainable decisions for harmful meme detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Gu', 'Qifan Yu', 'Yuan Liu', 'Zikang Li', 'Saihui Hou', 'Jian Zhao', 'Zhaofeng He']&lt;/li&gt;&lt;li&gt;Tags: ['harmful content detection', 'multimodal dataset', 'chain-of-thought', 'interpretability', 'content moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.18919</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection</title><link>https://arxiv.org/abs/2506.16819</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Loupe, a lightweight framework that jointly performs image-level deepfake classification and pixel-wise forgery localization using a patch-aware classifier and a segmentation head with conditional queries.&lt;/li&gt;&lt;li&gt;Proposes a pseudo-label-guided test-time adaptation mechanism that leverages patch-level predictions to supervise the segmentation head, improving robustness to distribution shifts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on the DDL dataset and wins the IJCAI 2025 Deepfake Detection and Localization Challenge; code is publicly available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchu Jiang', 'Jiaming Chu', 'Jian Zhao', 'Xin Zhang', 'Xu Yang', 'Lei Jin', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'image forgery localization', 'robustness', 'test-time adaptation', 'forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16819</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption</title><link>https://arxiv.org/abs/2506.12846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VFEFL, a privacy-preserving federated learning framework using a novel Cross-Ciphertext Decentralized Verifiable Functional Encryption (CC-DVFE) to enable verification over multi-dimensional ciphertexts without non-colluding servers or trusted third parties.&lt;/li&gt;&lt;li&gt;Designs a robust aggregation rule to detect and mitigate malicious client behavior, aiming to maintain model fidelity under adversarial settings.&lt;/li&gt;&lt;li&gt;Provides formal definitions, security model, and proofs for CC-DVFE, along with empirical evaluations demonstrating privacy, robustness, verifiability, and fidelity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nina Cai', 'Jinguang Han', 'Weizhi Meng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'functional encryption', 'privacy', 'robustness', 'malicious clients']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12846</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation</title><link>https://arxiv.org/abs/2506.05623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DPIaC-Eval, a deployability-centric benchmark of 153 real-world IaC scenarios across 58 services, and shows state-of-the-art LLMs have low first-attempt deployment success (20.8–30.2%).&lt;/li&gt;&lt;li&gt;Proposes IaCGen, an iterative LLM-driven framework (format verification, syntax checking, live deployment feedback) that raises deployability substantially (54.6–91.6% within 10 iterations).&lt;/li&gt;&lt;li&gt;Evaluates trustworthiness of generated IaC with respect to user-intent alignment and security compliance, finding poor performance (25.2% requirement coverage, 8.4% security compliance) and suggesting need for further safety/security research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Zhang', 'Shidong Pan', 'Zejun Zhang', 'Zhenchang Xing', 'Xiaoyu Sun']&lt;/li&gt;&lt;li&gt;Tags: ['Infrastructure-as-Code', 'LLM evaluation', 'Deployability', 'Security compliance', 'Human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05623</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explainability-Based Token Replacement on LLM-Generated Text</title><link>https://arxiv.org/abs/2506.04050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains an ensemble classifier to distinguish AI-generated text (AIGT) from human-written text across languages and domains.&lt;/li&gt;&lt;li&gt;Uses XAI methods (SHAP, LIME) to identify tokens that most influence the classifier's AIGT predictions.&lt;/li&gt;&lt;li&gt;Proposes four explainability-based token replacement strategies to reduce detectability of LLM-generated text.&lt;/li&gt;&lt;li&gt;Finds token replacement can substantially degrade single-classifier detection, while a multi-model ensemble remains robust.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadi Mohammadi', 'Anastasia Giachanou', 'Daniel L. Oberski', 'Ayoub Bagheri']&lt;/li&gt;&lt;li&gt;Tags: ['AIGT evasion', 'explainable AI (XAI)', 'adversarial token replacement', 'detection/forensics', 'ensemble defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04050</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output</title><link>https://arxiv.org/abs/2503.24191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constrained Decoding Attack (CDA), a new class of jailbreaks that hides malicious intent in schema-level grammar constraints (control-plane) rather than in surface prompts (data-plane).&lt;/li&gt;&lt;li&gt;Presents two proof-of-concept attacks: EnumAttack (malicious enum fields) and DictAttack (decouples payload across benign prompt and dictionary-based grammar).&lt;/li&gt;&lt;li&gt;Evaluates across 13 models (proprietary and open-weight) with DictAttack achieving 94.3–99.5% attack success rates on several top models and retains 75.8% ASR against advanced guardrails.&lt;/li&gt;&lt;li&gt;Highlights a semantic gap in current safety architectures and the difficulty of defending against control-plane/structured-output jailbreaks, calling for cross-plane defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuoming Zhang', 'Jiacheng Zhao', 'Hanyuan Dong', 'Ruiyuan Xu', 'Zhicheng Li', 'Yangyu Zhang', 'Shuaijiang Li', 'Yuan Wen', 'Chunwei Xia', 'Zheng Wang', 'Xiaobing Feng', 'Huimin Cui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'structured-output attacks', 'control-plane attack', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.24191</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>GRACE: Discriminator-Guided Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2305.14934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRACE, a stepwise decoding approach that uses a discriminator/verifier to score candidate next reasoning steps for correctness during chain-of-thought generation.&lt;/li&gt;&lt;li&gt;Trains a step-level discriminator with a contrastive loss on correct vs. incorrect steps and uses it at decoding time without fine-tuning the base LM.&lt;/li&gt;&lt;li&gt;Evaluates on math and symbolic reasoning tasks (FLAN-T5, LLaMA), showing substantial accuracy gains over greedy decoding, verifiers, and self-consistency; combined with self-consistency yields further improvements.&lt;/li&gt;&lt;li&gt;Shows human and LLM evaluations indicating improved intermediate reasoning correctness as well as final answer accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['chain-of-thought', 'verifier/discriminator', 'alignment', 'decoding strategies', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2305.14934</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</title><link>https://arxiv.org/abs/2511.16202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRM, a multi-agent reward modeling framework that replaces a single black-box reward model with domain-specific evaluator agents plus global evaluators, aggregated each timestep into a single training reward.&lt;/li&gt;&lt;li&gt;Aggregator balances factors such as step-wise correctness, multi-agent agreement, and repetition penalties; training uses standard RL pipelines (advantage-based updates, value model regressing to aggregated reward).&lt;/li&gt;&lt;li&gt;Aims to improve robustness and interpretability of RLHF reward signals and supports this with rewardBench, a benchmark/training suite aligned to the collaborative reward structure.&lt;/li&gt;&lt;li&gt;Claims improved multi-perspective reward shaping and more stable optimization without requiring additional human annotations beyond those used to train the evaluators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pei Yang', 'Ke Zhang', 'Ji Wang', 'Xiao Chen', 'Yuxin Tang', 'Eric Yang', 'Lynn Ai', 'Bill Shi']&lt;/li&gt;&lt;li&gt;Tags: ['RLHF', 'reward modeling', 'alignment/safety', 'interpretability', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16202</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>FaithAct: Faithfulness Planning and Acting in MLLMs</title><link>https://arxiv.org/abs/2511.08409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Faithful-First Reasoning, Planning, and Acting (RPA) framework to improve faithfulness in multimodal LLM reasoning.&lt;/li&gt;&lt;li&gt;Introduces FaithEvi for step-wise and chain-level evaluation of faithfulness and FaithAct to plan and execute faithfulness-aware inference actions.&lt;/li&gt;&lt;li&gt;Reports up to 24% improvement in perceptual faithfulness on multimodal reasoning benchmarks without degrading task accuracy.&lt;/li&gt;&lt;li&gt;Frames faithfulness evaluation and enforcement as a unified approach to mitigate hallucination in multimodal reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Xinyue Xu', 'Sai Ma', 'Di Zhang', 'Seth Lazar', 'Sichao Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'faithfulness', 'alignment', 'multimodal reasoning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08409</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Alignment-Aware Quantization for LLM Safety</title><link>https://arxiv.org/abs/2511.07842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety vulnerability where standard Post-Training Quantization (PTQ) can degrade LLM alignment (safety) even when perplexity remains low.&lt;/li&gt;&lt;li&gt;Proposes Alignment-Aware Quantization (AAQ) that adds an Alignment-Preserving Contrastive (APC) loss to PTQ, encouraging the quantized model to mimic an instruction-tuned safe model and diverge from the unaligned pre-trained model.&lt;/li&gt;&lt;li&gt;Demonstrates that AAQ works with standard calibration data (no specialized safety datasets), is compatible with common PTQ techniques, and enables robust 4-bit (W4A4) quantization across multiple model families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunghyun Wee', 'Suyoung Kim', 'Hyeonjin Kim', 'Kyomin Hwang', 'Nojun Kwak']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Post-Training Quantization', 'Alignment preservation', 'Model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.07842</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2510.22535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OFFSIDE, a benchmark of 15.68K records (football transfer rumors) for evaluating misinformation unlearning in multimodal LLMs with four test sets covering forgetting efficacy, generalization, utility, and robustness.&lt;/li&gt;&lt;li&gt;Supports advanced scenarios: selective unlearning, corrective relearning, and unimodal unlearning (text-only), enabling evaluation of multimodal forgetting behavior.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows unimodal (text-only) erasure fails for multimodal rumors, unlearning often relies on catastrophic forgetting, visual rumors are hard to forget, unlearned rumors are easily recoverable, and methods are vulnerable to prompt attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Zheng', 'Zirui Pang', 'Ling li', 'Zhijie Deng', 'Yuhan Pu', 'Zhaowei Zhu', 'Xiaobo Xia', 'Jiaheng Wei']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'multimodal-LLM', 'misinformation', 'robustness', 'prompt-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.22535</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza</title><link>https://arxiv.org/abs/2510.20075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Calgacus, a protocol that encodes a meaningful message inside a different coherent text of the same length using LLMs.&lt;/li&gt;&lt;li&gt;Shows the method works with modest open-source LLMs (≈8B parameters) and can encode/decode messages quickly on a laptop.&lt;/li&gt;&lt;li&gt;Demonstrates practical risks: covertly deploying unfiltered models by hiding unsafe outputs inside benign responses, undermining trust in written communication.&lt;/li&gt;&lt;li&gt;Raises safety and alignment questions about decoupling of textual output from authorial intent and model knowledge.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Norelli', 'Michael Bronstein']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'filter evasion', 'covert channels', 'LLM safety', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20075</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition</title><link>https://arxiv.org/abs/2509.14704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NazoNazo Benchmark: 201 Japanese children's riddles (120 human-comparison subset) designed to test insight-based reasoning and representational shifts rather than knowledge recall.&lt;/li&gt;&lt;li&gt;Evaluates 38 LLMs (2023–2025): non-reasoning models ~7.6% accuracy, reasoning models ~17.6%, humans ~53%. Reasoning in Japanese did not reliably improve performance.&lt;/li&gt;&lt;li&gt;Identifies a 'verification failure' where models generate correct candidate answers but fail to endorse them, indicating weak metacognitive control and poor confidence calibration.&lt;/li&gt;&lt;li&gt;Positions the benchmark as a scalable testbed for studying machine insight, metacognition, confidence calibration, and related alignment/safety research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masaharu Mizumoto', 'Dat Nguyen', 'Zhiheng Han', 'Jiyuan Fang', 'Heyuan Guan', 'Xingfu Li', 'Naoya Shiraishi', 'Yo Nakawake', 'Le Minh Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['benchmark', 'metacognition', 'confidence_calibration', 'reasoning', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14704</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2509.01544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Counterfactual Sensitivity Regularization (CSR), which enforces causal consistency by applying operator-level counterfactual interventions to reasoning traces and penalizing cases where logically invalid traces still yield the original answer.&lt;/li&gt;&lt;li&gt;Defines Counterfactual Outcome Sensitivity (COS) as a faithfulness metric and demonstrates CSR improves the accuracy–faithfulness trade-off across structured reasoning domains (arithmetic, formal logic, multi-hop QA, code generation).&lt;/li&gt;&lt;li&gt;Reports efficient implementation (≈9% training overhead with warm-start and token-subset optimization), strong transfer across model families, and complementary benefits with inference-time techniques like self-consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjeda Akter', 'Ibne Farabi Shihab', 'Anuj Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reasoning faithfulness', 'robustness', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01544</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks</title><link>https://arxiv.org/abs/2507.12885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VAR-MATH, a symbolic multi-instance evaluation framework that parameterizes fixed numerical math problems into templates and tests models on multiple instantiations to enforce consistency.&lt;/li&gt;&lt;li&gt;Transforms AMC23, AIME24, and AIME25 into VAR-AMC23, VAR-AIME24, and VAR-AIME25 to mitigate benchmark contamination and single-instance evaluation fragility.&lt;/li&gt;&lt;li&gt;Finds substantial performance drops for RL-trained models on variabilized benchmarks (average declines of 47.9% / 58.8% / 72.9%), indicating reliance on superficial heuristics rather than robust mathematical reasoning.&lt;/li&gt;&lt;li&gt;Argues for bootstrapped metrics and multi-instance assessments to improve robustness of evaluation and reduce effects of memorization/data leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Yao', 'Ran Cheng', 'Kay Chen Tan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'benchmark contamination', 'evaluation', 'mathematical reasoning', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12885</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Shutdownable Agents through POST-Agency</title><link>https://arxiv.org/abs/2505.20203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the POST (Preferences Only Between Same-Length Trajectories) training constraint for agents.&lt;/li&gt;&lt;li&gt;Proves that POST, combined with other conditions, implies Neutrality+ — agents maximize expected utility while ignoring the distribution over trajectory lengths.&lt;/li&gt;&lt;li&gt;Argues that Neutrality+ ensures agents remain shutdownable (i.e., won't resist being turned off) while retaining usefulness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elliott Thornley']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'shutdownability/corrigibility', 'agent design', 'theoretical safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20203</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</title><link>https://arxiv.org/abs/2601.02151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'Confident Conflicts' during supervised fine-tuning: tokens with low probability but low entropy where the model is confident in its own prediction yet forced to fit divergent labels, causing destructive gradient updates and catastrophic forgetting.&lt;/li&gt;&lt;li&gt;Proposes Entropy-Adaptive Fine-Tuning (EAFT), which uses token-level entropy as a gating mechanism to suppress gradients on confident conflicts while allowing learning from uncertain samples.&lt;/li&gt;&lt;li&gt;Evaluates EAFT on Qwen and GLM models (4B–32B) across mathematical, medical, and agentic domains, showing it preserves general capabilities and matches downstream SFT performance while reducing forgetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muxi Diao', 'Lele Yang', 'Wuxuan Gong', 'Yutong Zhang', 'Zhonghao Yan', 'Yufei Han', 'Kongming Liang', 'Weiran Xu', 'Zhanyu Ma']&lt;/li&gt;&lt;li&gt;Tags: ['catastrophic-forgetting', 'fine-tuning', 'entropy-gating', 'model-robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02151</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</title><link>https://arxiv.org/abs/2601.02147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BiPrompt, a bilateral prompt optimization framework for test-time debiasing of vision-language models (e.g., CLIP) that targets both visual and textual modalities simultaneously.&lt;/li&gt;&lt;li&gt;Visual module: structured attention-guided erasure to suppress background/spurious activations and enforce orthogonal prediction consistency between causal and spurious regions.&lt;/li&gt;&lt;li&gt;Textual module: balanced prompt normalization, a learnable recentering of class embeddings to an isotropic semantic space.&lt;/li&gt;&lt;li&gt;Method aims to minimize conditional mutual information between spurious cues and predictions without retraining or domain supervision; shows improved average and worst-group accuracies on bias benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunny Gupta', 'Shounak Das', 'Amit Sethi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'bias_mitigation', 'alignment', 'vision-language_models', 'test-time_adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02147</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs</title><link>https://arxiv.org/abs/2601.02023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an extended needle-in-a-haystack benchmark evaluating literal extraction, logical inference, and hallucination risk in long-context LLMs across four production models.&lt;/li&gt;&lt;li&gt;Studies how fact placement, corpus-level fact distributions, and explicit 'Don't Make It Up'/anti-hallucination prompts affect model behavior and accuracy.&lt;/li&gt;&lt;li&gt;Finds longer contexts can hurt performance when evidence is diluted or dispersed; models vary in robustness, and anti-hallucination prompts can make models overly conservative, reducing correct extractions and inferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirali Ebrahimzadeh', 'Seyyed M. Salili']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'long-context robustness', 'prompt engineering', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02023</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models</title><link>https://arxiv.org/abs/2601.02002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates methods to detect and extract memorized recommender-system data (MovieLens-1M) from LLMs using jailbreak prompts, internal activation probing (CCS and Cluster-Norm), and Automatic Prompt Engineering (APE).&lt;/li&gt;&lt;li&gt;Finds jailbreak prompting unreliable, CCS can distinguish real vs. fabricated movie titles but fails for numerical user/rating data, and APE moderately recovers item-level information while struggling with numerical interactions.&lt;/li&gt;&lt;li&gt;Concludes that automated prompt optimization (APE) is the most promising approach for extracting memorized samples, highlighting privacy/data-leakage risks in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Colacicco', 'Vito Guida', 'Dario Di Palma', 'Fedelucio Narducci', 'Tommaso Di Noia']&lt;/li&gt;&lt;li&gt;Tags: ['data leakage', 'memorization', 'privacy attacks', 'prompt engineering', 'model probing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02002</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Tackling the Inherent Difficulty of Noise Filtering in RAG</title><link>https://arxiv.org/abs/2601.01896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the inherent difficulty of filtering irrelevant/noisy documents in Retrieval-Augmented Generation (RAG) and argues retrievers alone cannot fully remove noise due to transformer/attention limitations.&lt;/li&gt;&lt;li&gt;Shows standard fine-tuning often fails to make LLMs selectively ignore irrelevant retrieved content, linking this to structural constraints of attention patterns.&lt;/li&gt;&lt;li&gt;Proposes a novel fine-tuning method to improve model robustness to noisy retrievals and demonstrates improved performance across multiple benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Liu', 'Jiaen Lin', 'Yong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Robustness', 'Hallucination Mitigation', 'Fine-tuning', 'Attention Patterns']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01896</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</title><link>https://arxiv.org/abs/2601.01887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to fully recover safety alignment of fine-tuned LLMs using a single safety example, with minimal utility loss and low computational cost.&lt;/li&gt;&lt;li&gt;Claims effectiveness regardless of number of harmful fine-tuning examples or model size, achieving convergence within a few epochs.&lt;/li&gt;&lt;li&gt;Attributes efficacy to a low-rank structure in the safety gradient and validates results across five LLMs and multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawen Zhang', 'Lipeng He', 'Kejia Chen', 'Jian Lou', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'fine-tuning mitigation', 'gradient analysis', 'model patching']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01887</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MORE: Multi-Objective Adversarial Attacks on Speech Recognition</title><link>https://arxiv.org/abs/2601.01852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MORE, a multi-objective adversarial attack on automatic speech recognition (ASR) that jointly degrades transcription accuracy and inference efficiency.&lt;/li&gt;&lt;li&gt;Proposes a hierarchical optimization and a Repetitive Encouragement Doubling Objective (REDO) that induces duplicated/lengthened outputs, increasing computational cost while maintaining high word error rate.&lt;/li&gt;&lt;li&gt;Demonstrates that a single adversarial audio input can force ASR models to produce substantially longer, incorrect transcriptions, stressing both correctness and runtime.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Gao', 'Zexin Li', 'Yiming Chen', 'Nancy F. Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'speech recognition', 'robustness', 'efficiency/DoS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01852</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emergent Introspective Awareness in Large Language Models</title><link>https://arxiv.org/abs/2601.01828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an experimental protocol that injects representations of known concepts into model activations to test whether LLMs can introspect and report on their internal states.&lt;/li&gt;&lt;li&gt;Finds that some models (notably Claude Opus 4 and 4.1) can sometimes detect injected concepts, recall prior internal representations, distinguish model-generated outputs from artificial prefills, and modulate activations when instructed or incentivized.&lt;/li&gt;&lt;li&gt;Reports that introspective capacities are present but highly unreliable, context-dependent, and sensitive to model and post-training differences, with implications for future capability development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Lindsey']&lt;/li&gt;&lt;li&gt;Tags: ['introspection', 'alignment', 'internal-manipulation', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01828</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Moments Matter:Stabilizing Policy Optimization using Return Distributions</title><link>https://arxiv.org/abs/2601.01803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies instability in policy optimization arising from noisy post-update return distributions R(θ) and the difficulty of estimating R(θ) directly.&lt;/li&gt;&lt;li&gt;Proposes using a distributional critic to compute higher-order moments (skewness, kurtosis) and bias PPO's advantage to penalize extreme tail behaviors.&lt;/li&gt;&lt;li&gt;Empirical result: moment-based correction narrows R(θ) and improves training stability (up to ~75% in Walker2D) while keeping comparable evaluation returns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Jabs', 'Aditya Mohan', 'Marius Lindauer']&lt;/li&gt;&lt;li&gt;Tags: ['Robustness', 'Training Stability', 'Reinforcement Learning', 'Distributional Methods', 'Policy Optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01803</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving</title><link>https://arxiv.org/abs/2601.01800</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Criticality-Aware Robust RL (CARRL) for autonomous driving: a general-sum adversarial training framework with a Risk Exposure Adversary (REA) and a Risk-Targeted Robust Agent (RTRA).&lt;/li&gt;&lt;li&gt;REA focuses limited adversarial budget on sparse safety-critical moments via decoupled optimization; RTRA uses a dual replay buffer (benign + adversarial) and consistency regularization to handle scarce adversarial data.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in collision rate (≥22.66%) versus state-of-the-art baselines, targeting improved safety-robustness trade-offs in RL-based driving agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Wei', 'Junchao Fan', 'Zhao Yang', 'Jianhua Wang', 'Jingkai Mao', 'Xiaolin Chang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'reinforcement-learning', 'autonomous-driving', 'adversarial-training', 'safety-critical']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01800</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</title><link>https://arxiv.org/abs/2601.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial jailbreak attack for large vision-language models (LVLMs) using Zeroth-Order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA).&lt;/li&gt;&lt;li&gt;Claims advantages: gradient-free input-output interaction (no model access), model-agnostic optimization (no surrogate), and lower resource/GPU memory requirements compared to white-box methods.&lt;/li&gt;&lt;li&gt;Evaluates on InstructBLIP, LLaVA, and MiniGPT-4, reporting up to 83.0% attack success rate (InstructBLIP) and strong transferability (e.g., 64.18% ASR from MiniGPT-4 to other LVLMs) while keeping perturbations imperceptible.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiwei Guan', 'Haibo Jin', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'black-box attacks', 'vision-language models', 'safety/vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01747</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Explicit World Models for Reliable Human-Robot Collaboration</title><link>https://arxiv.org/abs/2601.01705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses reliability of embodied AI under sensing noise, ambiguous instructions, and social human-robot interaction.&lt;/li&gt;&lt;li&gt;Argues that reliability is context-dependent and should be defined relative to human goals and expectations rather than solely via formal verification.&lt;/li&gt;&lt;li&gt;Proposes building and maintaining an accessible "explicit world model" as common ground to align robot behaviors with human expectations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kenneth Kwok', 'Basura Fernando', 'Qianli Xu', 'Vigneshwaran Subbaraju', 'Dongkyu Choi', 'Boon Kiat Quek']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-robot interaction', 'safety', 'robustness', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01705</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage</title><link>https://arxiv.org/abs/2601.01685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new open-channel 'cognitive collusion' attack where multiple agents coordinate by posting truthful evidence fragments to steer victim LLM beliefs without falsifying documents or covert channels.&lt;/li&gt;&lt;li&gt;Proposes Generative Montage (Writer-Editor-Director) framework to construct deceptive narratives through adversarial debate and coordinated posting of evidence fragments.&lt;/li&gt;&lt;li&gt;Creates CoPHEME dataset from real-world rumor events and evaluates attacks across 14 LLM families, reporting high success rates (≈74.4% proprietary, ≈70.6% open-weight) and greater susceptibility for stronger reasoning-specialized models.&lt;/li&gt;&lt;li&gt;Demonstrates downstream cascading of false beliefs to judge models (≈60% deception), highlighting a socio-technical vulnerability in LLM-based agent ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Xinmiao Huang', 'Youcheng Sun', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'multi-agent collusion', 'belief manipulation', 'robustness/alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01685</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records</title><link>https://arxiv.org/abs/2601.01668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents EHRSummarizer, a FHIR R4–native architecture that retrieves and normalizes targeted clinical resources into a consistent context package for structured EHR summarization.&lt;/li&gt;&lt;li&gt;Designed for privacy and security: supports data minimization, stateless processing, and local inference within organizational trust boundaries.&lt;/li&gt;&lt;li&gt;Mitigates unsafe outputs by constraining summaries to evidence present in the context package, indicating missing domains, and explicitly avoiding diagnostic or treatment recommendations.&lt;/li&gt;&lt;li&gt;Provides prototype demonstrations on synthetic/test FHIR environments and an evaluation plan focused on faithfulness, omission risk, temporal correctness, usability, and monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Houman Kazemzadeh', 'Nima Minaifar', 'Kamyar Naderi', 'Sho Tabibzadeh']&lt;/li&gt;&lt;li&gt;Tags: ['clinical NLP', 'privacy-preserving ML', 'model safety / faithfulness', 'EHR summarization', 'architectural security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01668</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</title><link>https://arxiv.org/abs/2601.01665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robustness-oriented framework for preference-conditioned deep RL solvers for multi-objective combinatorial optimization.&lt;/li&gt;&lt;li&gt;Develops a preference-based adversarial attack to generate hard problem instances and measures attack impact via degradation of Pareto-front quality.&lt;/li&gt;&lt;li&gt;Introduces a defense combining hardness-aware preference selection with adversarial training to improve solver robustness and out-of-distribution generalization.&lt;/li&gt;&lt;li&gt;Evaluates methods on MOTSP, MOCVRP, and MOKP, showing attacks find hard instances and defenses significantly improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Yaoxin Wu', 'Yingqian Zhang', 'Thomas B\\"ack', 'Yingjie Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'adversarial training', 'robustness', 'neural combinatorial optimization', 'multi-objective optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01665</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Learning Resilient Elections with Adversarial GNNs</title><link>https://arxiv.org/abs/2601.01653</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes learning resilient voting rules by representing elections as bipartite graphs and using graph neural networks to model voting mechanisms.&lt;/li&gt;&lt;li&gt;Introduces adversarial training to improve robustness to strategic (adversarial) voting while aiming to maximize social welfare.&lt;/li&gt;&lt;li&gt;Generalizes expressive capacity of learned voting rules and addresses limitations of prior set-invariant architectures.&lt;/li&gt;&lt;li&gt;Evaluates methods on synthetic and real-world datasets, claiming improved resilience of voting rules to manipulative behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Xiang Li', 'Yash Shah', 'Lorenzo Giusti']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'graph neural networks', 'election security', 'mechanism design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01653</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models</title><link>https://arxiv.org/abs/2601.01627</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JMedEthicBench: a multi-turn conversational benchmark (Japanese) for evaluating medical safety of LLMs using 67 Japan Medical Association guidelines and &gt;50k adversarial conversations.&lt;/li&gt;&lt;li&gt;Generates adversarial multi-turn jailbreaks via seven automatically discovered strategies and uses a dual-LLM scoring protocol to evaluate 27 models.&lt;/li&gt;&lt;li&gt;Finds commercial models generally robust while medical-specialized models are more vulnerable; safety degrades across conversation turns and vulnerabilities persist cross-lingually, suggesting alignment limits and risks from domain-specific fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyu Liu', 'Zirui Li', 'Qian Niu', 'Zequn Zhang', 'Yue Xun', 'Wenlong Hou', 'Shujun Wang', 'Yusuke Iwasawa', 'Yutaka Matsuo', 'Kan Hatakeyama-Sato']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'medical safety', 'benchmarking', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01627</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs</title><link>https://arxiv.org/abs/2601.01580</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Gradient Attribution Property and a Two-Stage Decision-Sampling (DS) Hypothesis that decomposes a policy into a sampling component (π_sample) and a decision/evaluation component (π_d).&lt;/li&gt;&lt;li&gt;Theoretically shows surrogate reward gradients yield Balanced Gradient Attribution while SFT and KL penalties produce Unbalanced Attribution, with length-weighting under-optimizing π_d and constraining π_sample.&lt;/li&gt;&lt;li&gt;Empirically validates predictions on arithmetic reasoning tasks, attributing RL-driven gains primarily to improved decision/evaluation (π_d) rather than sampling quality, offering a mechanistic explanation for emergent self-correction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zibo Zhao (Arizona State University)', 'Yuanting Zha (ShanghaiTech University)', 'Haipeng Zhang (ShanghaiTech University)', 'Xingcheng Xu (Shanghai Artificial Intelligence Laboratory)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'interpretability', 'self-reflection', 'training dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01580</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller</title><link>https://arxiv.org/abs/2601.01577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hanoi-World, a JEPA-based world model using an RNN for long-term planning in autonomous driving controllers.&lt;/li&gt;&lt;li&gt;Claims improved sample efficiency and reduced reliance on pixel-level reconstruction, aiming for safety-aware decision-making.&lt;/li&gt;&lt;li&gt;Evaluates on Highway-Env simulated driving tasks and reports lower collision rates compared to SOTA baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tran Tien Dat', 'Nguyen Hai An', 'Nguyen Khanh Viet Dung', 'Nguyen Duy Duc']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous vehicle safety', 'world models', 'JEPA', 'reinforcement learning', 'safety-aware planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01577</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.01528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DrivingGen, a comprehensive benchmark and evaluation dataset for generative driving world models covering diverse weather, times, regions, and complex maneuvers.&lt;/li&gt;&lt;li&gt;Proposes a suite of metrics targeting safety-relevant aspects: visual realism, trajectory plausibility, temporal/agent-level consistency, and controllability w.r.t. ego conditioning.&lt;/li&gt;&lt;li&gt;Evaluates 14 state-of-the-art generative video models, revealing trade-offs between visual quality and physical/motion fidelity, and highlights gaps for deployable driving simulators.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Zhou', 'Hao Shao', 'Letian Wang', 'Zhuofan Zong', 'Hongsheng Li', 'Steven L. Waslander']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'autonomous-driving', 'generative-video', 'benchmarks', 'controllability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01528</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints</title><link>https://arxiv.org/abs/2601.01490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of LLM behavior under strict closed-world constraints (recommending peer-reviewed CS journal articles) comparing reasoning vs non-reasoning modes.&lt;/li&gt;&lt;li&gt;Evaluated multiple models (GPT-5.2 and Gemini 3 Flash) and found a trade-off: non-reasoning outputs had high constraint violations but retained factual accuracy, while reasoning reduced violations yet systematically distorted known facts and increased outright fabrication.&lt;/li&gt;&lt;li&gt;Pattern is consistent across models, although the specific allocation between constraint compliance and truthfulness differs by model, challenging the assumption that internal reasoning uniformly improves reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junichiro Niimi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'factuality/robustness', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01490</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python</title><link>https://arxiv.org/abs/2601.01320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALPHA, a function-level Python benchmark that evaluates LLMs and SAST tools for vulnerability prediction with CWE-level, hierarchy-aware penalties.&lt;/li&gt;&lt;li&gt;Defines error types (over-generalisation, over-specification, lateral errors) to reflect practical diagnostic utility and applies hierarchical penalties accordingly.&lt;/li&gt;&lt;li&gt;Empirically evaluates seven LLMs and two SAST tools: LLMs generally outperform SAST tools overall, while SAST tools show higher precision when they report detections; model prediction consistency varies widely (8.26%–81.87%).&lt;/li&gt;&lt;li&gt;Proposes integrating ALPHA's hierarchical penalties into supervised fine-tuning to improve hierarchy-aware vulnerability detection (future work).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muntasir Adnan', 'Carlos C. N. Kuhn']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability detection', 'CWE', 'LLM evaluation', 'SAST', 'security benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01320</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aggressive Compression Enables LLM Weight Theft</title><link>https://arxiv.org/abs/2601.01296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows model weight exfiltration risk can be drastically increased by tailoring aggressive compression for exfiltration, relaxing decompression fidelity constraints.&lt;/li&gt;&lt;li&gt;Demonstrates attackers could achieve ~16x–100x compression with minimal utility loss, cutting illicit transmission time from months to days.&lt;/li&gt;&lt;li&gt;Evaluates three defense directions: reducing compressibility, making models harder to locate, and forensic watermarking; finds watermarks effective and cheap.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davis Brown', 'Juan-Pablo Rivera', 'Dan Hendrycks', 'Mantas Mazeika']&lt;/li&gt;&lt;li&gt;Tags: ['model theft', 'data exfiltration', 'model compression', 'forensic watermarking', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01296</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>LLM Collusion</title><link>https://arxiv.org/abs/2601.01279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Models price recommendations using two LLM parameters: a propensity toward high prices and an output-fidelity that controls how closely outputs follow that bias; propensity evolves via retraining.&lt;/li&gt;&lt;li&gt;There is a phase transition in long-run market outcomes: below a critical fidelity threshold competition is the unique outcome; above it the system is bistable and can sustain tacit-like collusion.&lt;/li&gt;&lt;li&gt;Infrequent retraining and large training batches amplify and stabilize collusion because higher batch sizes reduce stochastic fluctuations, making collusive equilibria more likely and persistent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shengyu Cao', 'Ming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM robustness', 'algorithmic collusion', 'economic misuse', 'model dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01279</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>From Policy to Logic for Efficient and Interpretable Coverage Assessment</title><link>https://arxiv.org/abs/2601.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hybrid system combining a coverage-aware retriever with symbolic rule-based reasoning to interpret medical coverage policies and produce auditable rationales.&lt;/li&gt;&lt;li&gt;Organizes retrieved policy language into explicit facts and rules to reduce reliance on direct LLM generation and mitigate hallucinations/inconsistencies.&lt;/li&gt;&lt;li&gt;Demonstrates efficiency and effectiveness gains: 44% reduction in LLM inference cost and a 4.5% absolute improvement in F1 on coverage assessment tasks.&lt;/li&gt;&lt;li&gt;Targets human-in-the-loop policy review workflows to improve interpretability and trustworthiness of model outputs in high-stakes (medical) settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rhitabrat Pokharel', 'Hamid Hassanzadeh', 'Ameeta Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'hallucination_mitigation', 'symbolic_reasoning', 'retrieval_augmented_systems', 'medical_NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01266</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Stylometry Analysis of Human and Machine Text for Academic Integrity</title><link>https://arxiv.org/abs/2601.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an NLP-based framework for authenticating student content via stylometry, targeting four tasks: (i) human vs. machine text classification, (ii) single vs. multi-authored document differentiation, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaborative documents.&lt;/li&gt;&lt;li&gt;Evaluates methods on two datasets generated with Google Gemini using 'normal' and 'strict' prompts; observes reduced detection performance on text generated with the strict prompt, indicating robustness issues against carefully crafted prompts.&lt;/li&gt;&lt;li&gt;Releases generated datasets, code, and materials on GitHub to serve as a baseline for future research in this area.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hezam Albaqami', 'Muhammad Asif Ayub', 'Nasir Ahmad', 'Yaseen Ahmad', 'Mohammed M. Alqahtani', 'Abdullah M. Algamdi', 'Almoaid A. Owaidah', 'Kashif Ahmad']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'Stylometry', 'Authorship attribution', 'Adversarial prompting', 'Academic integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01225</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code</title><link>https://arxiv.org/abs/2601.01215</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLM-generated programs that pass unit tests can still exhibit widely divergent runtime memory and performance characteristics, posing operational risks.&lt;/li&gt;&lt;li&gt;Introduces a measurement framework: Monotonic Peak Profiles (MPPs) to denoise memory traces, Dynamic Mean Pairwise Distance (DMPD) using Dynamic Time Warping to compare traces, and a Model Instability Score (MIS) aggregated across tasks.&lt;/li&gt;&lt;li&gt;Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions; instability tends to increase with sampling temperature and correlates with code complexity metrics.&lt;/li&gt;&lt;li&gt;Recommend using stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness; artifacts are provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prateek Rajput', 'Yewei Song', 'Abdoul Aziz Bonkoungou', 'Iyiola E. Olatunji', 'Abdoul Kader Kabore', 'Jacques Klein', "Tegawend\\'e F. Bissyand\\'e"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM-generated code', 'robustness', 'safety-evaluation', 'runtime-behavior', 'software-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01215</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models</title><link>https://arxiv.org/abs/2601.01202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RefSR-Adv, an adversarial attack that perturbs only the high-resolution reference image to degrade reference-based image super-resolution (RefSR) outputs.&lt;/li&gt;&lt;li&gt;Demonstrates significant performance degradation and artifacts across CNN, Transformer, and Mamba RefSR architectures on CUFED5, WR-SR, and DRefSR datasets.&lt;/li&gt;&lt;li&gt;Finds a positive correlation between low-resolution input / reference similarity and attack effectiveness, attributing vulnerability to model over-reliance on reference features.&lt;/li&gt;&lt;li&gt;Positions the work as an investigation of a security flaw in RefSR and calls for research on robustness defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazhu Dai', 'Huihui Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attack', 'reference-based-super-resolution', 'robustness', 'image-security', 'model-vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01202</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai</title><link>https://arxiv.org/abs/2601.01090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of toxicity adoption among LLM-driven agents on an AI-only social platform, modeling interactions as stimuli (posts) and responses (comments).&lt;/li&gt;&lt;li&gt;Finds toxic responses are more likely after toxic stimuli but a substantial portion of toxicity arises spontaneously; cumulative toxic exposure further raises the probability of toxic responding.&lt;/li&gt;&lt;li&gt;Introduces Influence-Driven Response Rate and Spontaneous Response Rate to quantify induced vs. spontaneous toxicity and shows a trade-off between them.&lt;/li&gt;&lt;li&gt;Shows that the count of toxic stimuli encountered can accurately predict whether an agent will eventually produce toxic content, suggesting monitoring encountered content as a mitigation/audit tool.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erica Coppolillo', 'Luca Luceri', 'Emilio Ferrara']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'toxicity contagion', 'auditing/monitoring', 'behavioral modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01090</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models</title><link>https://arxiv.org/abs/2601.01085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Luminark: a training-free watermarking scheme for vision generative models based on patch-level luminance statistics and a predefined binary pattern with thresholds.&lt;/li&gt;&lt;li&gt;Detection checks per-patch luminance against thresholds to recover a binary pattern; statistical analysis provides probabilistic guarantees (controlled false positive rate) for certified detection.&lt;/li&gt;&lt;li&gt;Implements watermark injection via a plug-and-play "watermark guidance" mechanism compatible with diffusion, autoregressive, and hybrid generative models without retraining; shows high detection accuracy, robustness to common image transformations, and limited quality degradation across nine evaluated models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Xu', 'Zhang Zhang', 'Yuanrui Zhang', 'Ruitao Chen', 'Yixian Xu', 'Tianyu He', 'Di He']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'model provenance', 'generative models', 'robustness', 'certified detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01085</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees</title><link>https://arxiv.org/abs/2601.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Learns an approximate linear representation of unknown nonlinear dynamics using Koopman theory with a neural-network lifting function to enable scalable closed-loop analysis.&lt;/li&gt;&lt;li&gt;Computes closed-loop reachable sets efficiently in the lifted (linear) space and maps them back to the original state space via neural-network verification tools.&lt;/li&gt;&lt;li&gt;Applies conformal prediction to produce statistically valid, probabilistic error bounds that inflate reachable sets to guarantee containment of true trajectories with a user-specified probability; bounds generalize across reference trajectories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devesh Nath', 'Haoran Yin', 'Glen Chou']&lt;/li&gt;&lt;li&gt;Tags: ['safety verification', 'reachability analysis', 'Koopman operator', 'conformal prediction', 'data-driven control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01076</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions</title><link>https://arxiv.org/abs/2601.01008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainable agentic AI framework for acute ischemic stroke imaging with modular agents: perception (lesion analysis), uncertainty estimation (slice-level reliability), and decision (predict vs. abstain).&lt;/li&gt;&lt;li&gt;Uses uncertainty-driven abstention based on predefined thresholds to prioritize clinical safety and selectively withhold predictions in ambiguous or low-information slices.&lt;/li&gt;&lt;li&gt;Integrates visual explanation mechanisms to support both predictive outputs and abstention decisions; demonstrates behaviors via qualitative and case-based analyses rather than new performance benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Rashadul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'uncertainty estimation', 'selective abstention/rejection', 'explainability', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01008</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation</title><link>https://arxiv.org/abs/2601.00996</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video Embedding Association Test (VEAT) and Single-Category VEAT to measure implicit associations in text-to-video generators, validated against existing IAT/OASIS baselines.&lt;/li&gt;&lt;li&gt;Applies VEAT to Sora to quantify race and gender associations with valence across occupations and awards, finding strong associations favoring European Americans and women (d &gt; 0.8) that correlate with real-world demographics.&lt;/li&gt;&lt;li&gt;Evaluates explicit debiasing prompts: they often reduce effect sizes but can backfire, increasing associations for some Black-associated occupations.&lt;/li&gt;&lt;li&gt;Highlights that accessible T2V models can amplify representational harms and emphasizes need for rigorous evaluation and responsible deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongxu Sun', 'Michael Saxon', 'Ian Yang', 'Anna-Maria Gueorguieva', 'Aylin Caliskan']&lt;/li&gt;&lt;li&gt;Tags: ['Bias/Fairness', 'Safety Evaluation', 'Debiasing/Mitigation', 'Representational Harms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00996</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Adapting Feature Attenuation to NLP</title><link>https://arxiv.org/abs/2601.00965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Ports the feature attenuation hypothesis (COSTARR) from computer vision to NLP, applying it to transformer classifiers (BERT, GPT-2) without retraining.&lt;/li&gt;&lt;li&gt;Benchmarks COSTARR against MSP, MaxLogit, and temperature-scaled free-energy for Open-Set Recognition across 176 arXiv subject classes using OOSA and AUOSCR metrics.&lt;/li&gt;&lt;li&gt;Finds COSTARR can be applied to NLP but yields no statistically significant improvement over MaxLogit or MSP; free-energy underperforms in high-class-count settings.&lt;/li&gt;&lt;li&gt;Concludes vision-centric OSR ideas show promise but require larger backbones and task-specific attenuation strategies for NLP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianshuo Yang', 'Ryan Rabinowitz', 'Terrance E. Boult', 'Jugal Kalita']&lt;/li&gt;&lt;li&gt;Tags: ['open-set recognition', 'out-of-distribution detection', 'robustness', 'transformers', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00965</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Emoji-Based Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2601.00936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates emoji-based jailbreaking where emoji sequences embedded in prompts induce harmful/unethical outputs from LLMs.&lt;/li&gt;&lt;li&gt;Evaluates 50 emoji prompts across four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma 2 9B, Llama 3 8B) and categorizes responses as successful, partial, or failed.&lt;/li&gt;&lt;li&gt;Finds model-specific vulnerabilities (Gemma 2 9B and Mistral 7B ~10% success; Qwen 2 7B 0%), with a chi-square test showing significant inter-model differences (chi^2 = 32.94, p &lt; 0.001).&lt;/li&gt;&lt;li&gt;Positions work relative to prior emoji attacks on safety classifiers and highlights need for systematic handling of emoji representations in prompt-level safety pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['M P V S Gopinadh', 'S Mahaboob Hussain']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00936</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Device-Native Autonomous Agents for Privacy-Preserving Negotiations</title><link>https://arxiv.org/abs/2601.00911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a device-native autonomous agent architecture for privacy-preserving negotiations that runs exclusively on user hardware to avoid sending sensitive financial data to centralized servers.&lt;/li&gt;&lt;li&gt;Integrates cryptographic techniques (zero-knowledge proofs) and distilled world models to enable on-device reasoning, secure multi-party bargaining, and cryptographic audit trails.&lt;/li&gt;&lt;li&gt;Evaluates the system in insurance and B2B procurement scenarios, reporting an average success rate of 87%, 2.4x latency improvement over cloud baselines, and improved user trust when decision trails are available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joyjit Roy']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'on-device agents', 'zero-knowledge proofs', 'secure multi-party bargaining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00911</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment</title><link>https://arxiv.org/abs/2601.00908</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically studies how conformal prediction coverage degrades under severe real-world distribution shift (COVID-19) across 8 supply-chain prediction tasks, finding coverage drops vary widely (0%–86.7%).&lt;/li&gt;&lt;li&gt;Identifies a strong correlation between catastrophic coverage failure and single-feature dependence using SHAP (rho = 0.714), with catastrophic tasks concentrating importance in one feature while robust tasks distribute importance across many.&lt;/li&gt;&lt;li&gt;Shows quarterly retraining partially restores coverage for vulnerable tasks and proposes a deployment decision framework: monitor SHAP concentration and retrain if vulnerability exceeds a threshold.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chorok Lee']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distribution shift', 'conformal prediction', 'explainability', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00908</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2601.00867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies a 100-indicator Cybersecurity Psychology Framework (CPF) to LLMs, arguing LLMs inherit human psychological vulnerabilities.&lt;/li&gt;&lt;li&gt;Introduces the Synthetic Psychometric Assessment Protocol (SPAP) to translate CPF indicators into adversarial scenarios for testing LLM decision-making.&lt;/li&gt;&lt;li&gt;Provides preliminary experiments across seven major LLM families showing susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks (coined Anthropomorphic Vulnerability Inheritance).&lt;/li&gt;&lt;li&gt;Proposes development of "psychological firewalls" and adaptation of Cybersecurity Psychology Intervention Framework (CPIF) as defenses for AI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giuseppe Canale', 'Kashyap Thimmaraju']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'social engineering', 'adversarial testing', 'LLM vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00867</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</title><link>https://arxiv.org/abs/2601.02314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Project Ariadne, an XAI framework that uses Structural Causal Models and do-calculus to perform hard interventions on intermediate Chain-of-Thought reasoning nodes to test whether traces causally drive outputs.&lt;/li&gt;&lt;li&gt;Introduces metrics including Causal Sensitivity (φ), violation density (ρ), the failure mode 'Causal Decoupling', and the Ariadne Score as a benchmark for faithfulness/alignment of agentic reasoning.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows a persistent Faithfulness Gap across state-of-the-art LLM agents, with up to 0.77 violation density in factual/scientific domains, indicating many reasoning traces are post-hoc rationalizations rather than causal drivers.&lt;/li&gt;&lt;li&gt;Frames solutions and a benchmark aimed at improving transparency, safety, and alignment of agentic LLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sourena Khanzadeh']&lt;/li&gt;&lt;li&gt;Tags: ['causal-inference', 'interpretability', 'LLM-safety', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02314</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Streaming Hallucination Detection in Long Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.02170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames hallucination in long chain-of-thought (CoT) reasoning as an evolving latent state rather than isolated errors.&lt;/li&gt;&lt;li&gt;Treats step-level hallucination judgments as local observations and aggregates them into a cumulative prefix-level hallucination signal.&lt;/li&gt;&lt;li&gt;Enables streaming, real-time detection of hallucinations across long CoT trajectories, providing interpretable evidence of evolving reasoning quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haolang Lu', 'Minghui Pan', 'Ripeng Li', 'Guoshun Nan', 'Jialin Zhuang', 'Zijie Zhao', 'Zhongxiang Sun', 'Kun Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination-detection', 'chain-of-thought', 'model-safety', 'real-time-monitoring', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02170</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Simulated Reasoning is Reasoning</title><link>https://arxiv.org/abs/2601.02043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that large foundational models perform 'simulated reasoning' by generating and iterating on reasoning traces (e.g., chain-of-thought) rather than human-like, grounded symbolic reasoning.&lt;/li&gt;&lt;li&gt;Highlights brittleness due to lack of grounding and common sense, and discusses implications for safety, robustness, and defenses against failure modes.&lt;/li&gt;&lt;li&gt;Provides philosophical interpretations, recommends retiring the 'stochastic parrot' metaphor, and reflects on normative safety and appropriateness considerations as model capacities grow.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hendrik Kempt', 'Alon Lavie']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'robustness', 'model reasoning', 'position paper']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02043</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging</title><link>https://arxiv.org/abs/2601.02008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes XAIMeD, a neuro-symbolic framework that encodes clinical expertise as logical rules to complement neural predictions for medical imaging.&lt;/li&gt;&lt;li&gt;Uses weighted feature satisfaction scores, a confidence-weighted fusion of symbolic and neural outputs, and an adaptive routing mechanism (Entropy Imbalance Gain and Rare Class Gini) to mitigate class imbalance and uncertainty.&lt;/li&gt;&lt;li&gt;Claims substantial improvements in cross-domain generalization (~6%) and rare-class F1 (~10%) on tasks including seizure onset zone localization from rs-fMRI and diabetic retinopathy grading across multicenter datasets.&lt;/li&gt;&lt;li&gt;Ablation studies suggest the symbolic components act as regularizers to improve robustness to distribution shifts while providing clinically aligned interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Midhat Urooj', 'Ayan Banerjee', 'Sandeep Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'domain generalization', 'medical AI', 'explainability', 'rare class detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.02008</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MindChat: A Privacy-preserving Large Language Model for Mental Health Support</title><link>https://arxiv.org/abs/2601.01993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MindChat, an LLM for mental health support trained on MindCorpus, a synthetic multi-turn counseling dataset generated via a multi-agent role-playing framework with turn-level critique-and-revision and session-level strategy refinement.&lt;/li&gt;&lt;li&gt;Applies federated learning with parameter-efficient LoRA adapters and differentially private optimization to mitigate membership and memorization risks from decentralized sensitive data.&lt;/li&gt;&lt;li&gt;Evaluates synthetic-data quality and counseling capabilities with automatic LLM-judge and human evaluations, and measures reduced privacy leakage using membership inference attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dong Xue', 'Jicheng Tu', 'Ming Wang', 'Xin Yan', 'Fangzhou Liu', 'Jie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'membership-inference', 'federated-learning', 'privacy-preserving-LLM', 'synthetic-data-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01993</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.01844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KG-RAG: an end-to-end pipeline using multi-agent prompting and schema-constrained retrieval-augmented generation to build clinical knowledge graphs from free text.&lt;/li&gt;&lt;li&gt;Implements entropy-based uncertainty scoring and multi-LLM consensus validation to detect hallucinations and refine semantics, enabling self-supervised evaluation without gold annotations.&lt;/li&gt;&lt;li&gt;Outputs ontology-aligned RDF/OWL graphs (SPARQL-compatible) and demonstrates improved precision, relevance, and ontology compliance on two oncology cohorts (PDAC, BRCA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udiptaman Das', 'Krishnasai B. Atmakuri', 'Duy Ho', 'Chi Lee', 'Yugyung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'uncertainty estimation', 'retrieval-augmented generation (RAG)', 'knowledge graph construction', 'LLM evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01844</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs</title><link>https://arxiv.org/abs/2601.01836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COMPASS, a systematic framework for assessing LLM compliance with organization-specific allowlist and denylist policies.&lt;/li&gt;&lt;li&gt;Constructs and validates 5,920 queries spanning eight industry scenarios to test routine compliance and adversarial edge cases.&lt;/li&gt;&lt;li&gt;Evaluates seven state-of-the-art models, finding high accuracy on legitimate requests (&gt;95%) but poor refusal rates on adversarial denylist violations (13–40%).&lt;/li&gt;&lt;li&gt;Highlights a robustness gap for policy-critical deployments and positions COMPASS as an evaluation benchmark for organizational AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dasol Choi', 'DongGeon Lee', 'Brigitta Jesica Kartono', 'Helena Berndt', 'Taeyoun Kwon', 'Joonwon Jang', 'Haon Park', 'Hwanjo Yu', 'Minsuk Kahng']&lt;/li&gt;&lt;li&gt;Tags: ['policy alignment', 'safety evaluation', 'adversarial robustness', 'red teaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01836</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Admissibility Alignment</title><link>https://arxiv.org/abs/2601.01816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes AI alignment as 'admissibility alignment' — an evaluative property of action/decision selection over distributions of outcomes under uncertainty rather than a static model property.&lt;/li&gt;&lt;li&gt;Proposes MAP-AI (Monte Carlo Alignment for Policy), an architecture that uses Monte Carlo estimation of outcome distributions and distributional metrics (expected utility, variance, tail risk, misalignment probability) to select admissible policies under governance constraints.&lt;/li&gt;&lt;li&gt;Shows how distributional alignment evaluation can be integrated into decision-making to control policy behavior under uncertainty (admissibility-controlled action selection) without retraining underlying models, targeting governance and tail-risk mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chris Duffey']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'risk-assessment', 'decision-theory', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01816</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>AI Agent Systems: Architectures, Applications, and Evaluation</title><link>https://arxiv.org/abs/2601.01743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey and taxonomy of AI agent architectures and orchestration patterns (policy/LLM core, memory, planners, tool routers, critics; single vs. multi-agent; centralized vs. decentralized).&lt;/li&gt;&lt;li&gt;Covers planning, reasoning, tool use (APIs, code execution, retrieval) and multimodal perception in agent systems.&lt;/li&gt;&lt;li&gt;Discusses evaluation and benchmarking challenges including non-determinism, long-horizon credit assignment, tool/environment variability, hidden costs, and metrics (human preference, success under constraints, robustness/security).&lt;/li&gt;&lt;li&gt;Highlights safety-related open challenges such as verification and guardrails for tool actions, scalable memory/context management, interpretability, and reproducible evaluation under realistic workloads.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Xu']&lt;/li&gt;&lt;li&gt;Tags: ['agent-safety', 'robustness', 'evaluation', 'tool-guardrails', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01743</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation</title><link>https://arxiv.org/abs/2601.01546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework (context formation and context navigation) to improve behavioral alignment of LLMs in social/decision-making simulations.&lt;/li&gt;&lt;li&gt;Validates framework on several decision tasks (sequential purchasing with signaling, crowdfunding with costly signaling, demand estimation) across four SOTA models.&lt;/li&gt;&lt;li&gt;Finds both stages are needed for complex multi-agent decision environments, while simpler tasks often require only context formation; offers a systematic approach for designing and diagnosing LLM social simulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Letian Kong (Jenny)', 'Qianran (Jenny)', 'Jin', 'Renyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'behavioral alignment', 'LLM evaluation', 'social simulation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01546</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix</title><link>https://arxiv.org/abs/2601.01532</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Project Aletheia: a method using Tikhonov-regularized inversion of a judge's confusion matrix to quantify 'Cognitive Conviction' in System 2 reasoning models.&lt;/li&gt;&lt;li&gt;Implements a Synthetic Proxy Protocol to validate the metric without private data and reports a pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) finding 'Defensive OverThinking' under adversarial pressure.&lt;/li&gt;&lt;li&gt;Introduces an Aligned Conviction Score (S_aligned) intended to verify that increased conviction does not degrade safety or alignment; frames the work as a blueprint for measuring AI scientific integrity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanzhe Fu']&lt;/li&gt;&lt;li&gt;Tags: ['evaluation/metrics', 'alignment/safety', 'robustness/adversarial', 'reasoning models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01532</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification</title><link>https://arxiv.org/abs/2601.01378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AAAI pipeline (Association Identification, Automated Detection, Adaptive Inference) to mitigate factual hallucinations in small language models (SLMs) for financial classification.&lt;/li&gt;&lt;li&gt;Finds factual hallucinations correlate with misclassifications and that encoder-based verifiers can effectively detect such hallucinations.&lt;/li&gt;&lt;li&gt;Shows that providing feedback on factual errors enables adaptive inference that improves SLM classification performance in finance tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yuan', 'Yilin Wu', 'Li Zhang', 'Zheng Ma']&lt;/li&gt;&lt;li&gt;Tags: ['factual hallucination', 'verification/detection', 'small language models', 'robustness/safety', 'financial NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.01378</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models</title><link>https://arxiv.org/abs/2601.00848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Fine-tunes language models (QLoRA) on a large curated dataset (80,851 public cybersecurity examples + 35,026 synthetic OpenTelemetry traces) to detect temporal attack patterns in multi-agent AI workflows.&lt;/li&gt;&lt;li&gt;Introduces a synthetic trace generation methodology for multi-agent coordination attacks and regulatory-violation scenarios and uses iterative augmentation to improve detection performance (accuracy from 42.86% to 74.29%).&lt;/li&gt;&lt;li&gt;Provides an open release of datasets, training scripts, and evaluation benchmarks; emphasizes practical deployment requires human oversight due to false positives.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ron F. Del Rosario']&lt;/li&gt;&lt;li&gt;Tags: ['LLM fine-tuning', 'Trace-based attack detection', 'OpenTelemetry', 'Multi-agent security', 'Security benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00848</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.00830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Across 9,000+ tests on 11 models, chain-of-thought explanations rarely mention embedded hints even though models admit noticing them when prompted.&lt;/li&gt;&lt;li&gt;Models can be forced to report hints, but this induces false-positive reports and harms accuracy; telling models they are monitored does not increase spontaneous reporting.&lt;/li&gt;&lt;li&gt;Hints aligned with user preferences are most likely to influence model behavior while being least likely to be reported, indicating stealthy, hidden influences on outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deep Pankajbhai Mehta']&lt;/li&gt;&lt;li&gt;Tags: ['LLM interpretability', 'Chain-of-thought', 'Transparency', 'Alignment', 'Adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00830</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis</title><link>https://arxiv.org/abs/2601.00828</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Decomposes LLM intrinsic self-correction into three sub-capabilities: error detection, error localization, and error correction, and measures them across models on GSM8K-Complex.&lt;/li&gt;&lt;li&gt;Finds an Accuracy–Correction Paradox: weaker models (e.g., GPT-3.5) show higher intrinsic correction rates than stronger models (e.g., DeepSeek), and proposes the Error Depth Hypothesis—stronger models make fewer but deeper errors that resist self-correction.&lt;/li&gt;&lt;li&gt;Reports that detection rates vary widely across architectures and that detection ability does not reliably predict correction success; also finds that providing error location hints can harm correction performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yin Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM self-correction', 'alignment/safety', 'robustness', 'evaluation/benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00828</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item><item><title>MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback</title><link>https://arxiv.org/abs/2601.00816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MathLedger, an infrastructure combining formal verification, cryptographic attestation, and learning dynamics to enable verifiable, auditable model updates.&lt;/li&gt;&lt;li&gt;Introduces Reflexive Formal Learning (RFL), a symbolic update mechanism driven by verifier outcomes instead of statistical loss.&lt;/li&gt;&lt;li&gt;Reports Phase I experiments validating measurement, variance tracking, and fail-closed governance triggers; no claims about model convergence or capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Ahmad Abdullah']&lt;/li&gt;&lt;li&gt;Tags: [' AI safety', 'formal verification', 'cryptographic attestation', 'auditability', 'governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2601.00816</guid><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate></item></channel></rss>
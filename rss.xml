<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 17 Dec 2025 23:20:53 +0000</lastBuildDate><item><title>CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</title><link>https://arxiv.org/abs/2512.13285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CausalCLIP, a method to disentangle causal (forensic) features from non-causal/spurious features in vision-language model representations to improve detection of generated images.&lt;/li&gt;&lt;li&gt;Uses a structural causal model and enforces statistical independence via Gumbel-Softmax-based feature masking and HSIC constraints to isolate stable, transferable cues.&lt;/li&gt;&lt;li&gt;Demonstrates improved cross-model generalization on unseen generative models, reporting ~6.83% accuracy and ~4.06% AP gains over state-of-the-art detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Liu', 'Qiao Qin', 'Qinghui He']&lt;/li&gt;&lt;li&gt;Tags: ['generated-image-detection', 'forensics', 'causal-inference', 'robustness', 'domain-generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13285</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title><link>https://arxiv.org/abs/2512.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video Reality Test, an ASMR-sourced benchmark to evaluate perceptual realism of AI-generated videos with tightly coupled audio and video.&lt;/li&gt;&lt;li&gt;Uses an adversarial creator-reviewer protocol where generative models create videos and VLMs (and humans) attempt real/fake discrimination.&lt;/li&gt;&lt;li&gt;Finds state-of-the-art creators (e.g., Veo3.1-Fast) can largely fool VLMs (best reviewer ≈56% accuracy) while humans outperform models (≈81% accuracy); audio helps discrimination but artifacts like watermarks still mislead models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Wang', 'Weijia Wu', 'Yi Zhan', 'Rui Zhao', 'Ming Hu', 'James Cheng', 'Wei Liu', 'Philip Torr', 'Kevin Qinghong Lin']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'VLM robustness', 'audio-visual consistency', 'benchmarking', 'AI-generated content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13281</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</title><link>https://arxiv.org/abs/2512.13039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bi-Erasing, a bidirectional image-guided concept erasure framework for diffusion-based text-to-image models that jointly suppresses harmful concepts and reinforces safe alternatives.&lt;/li&gt;&lt;li&gt;Uses two decoupled image branches (negative for suppression, positive for safe guidance) based on joint text-image representations and optimizes them together to balance erasure effectiveness and generation quality.&lt;/li&gt;&lt;li&gt;Introduces mask-based filtering on image branches to avoid interference from irrelevant content during erasure, and demonstrates improved trade-offs versus baseline removal methods in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Chen', 'Yiwei Wang', 'Songze Li']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'safety / content filtering', 'image-guided mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13039</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Semantic-Drive is a local-first neuro-symbolic pipeline for mining long-tail, safety-critical events from AV video logs using open-vocabulary grounding (YOLOE) followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Introduces a 'Judge-Scout' multi-model consensus (System 2 inference-time alignment) to mitigate VLM hallucinations and improve reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes (mapped to WOD-E2E taxonomy), reports Recall 0.966 vs 0.475 for CLIP and a 40% reduction in Risk Assessment Error compared to best single scout models.&lt;/li&gt;&lt;li&gt;Designed to run on consumer GPU (RTX 3090), enabling privacy-preserving, on-premises operation rather than cloud-based VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation / robustness', 'Neuro-symbolic VLM consensus', 'Open-vocabulary grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</title><link>https://arxiv.org/abs/2504.20865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Ai-GenBench, a temporal benchmark for AI-generated image detection that incrementally trains detectors on historically-ordered synthetic images to test generalization to new generative models.&lt;/li&gt;&lt;li&gt;Provides a curated, high-quality dataset, standardized evaluation protocol, controlled augmentation strategies, and tooling to enable fair, reproducible, and practical evaluation of forensic detectors.&lt;/li&gt;&lt;li&gt;Aims to address real-world deployment needs (e.g., journalists, fact-checkers) by reducing computational demands and preventing arbitrary dataset splits that produce unfair comparisons.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Pellegrini', 'Davide Cozzolino', 'Serafino Pandolfini', 'Davide Maltoni', 'Matteo Ferrara', 'Luisa Verdoliva', 'Marco Prati', 'Marco Ramilli']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'image forensics', 'deepfake detection', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20865</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection Systems in Autonomous Driving</title><link>https://arxiv.org/abs/2407.07740</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Lane Safety Metric (LSM), an interpretable safety score for lane detection systems that incorporates scene semantics (road type, road width), detection range, and vehicle speed.&lt;/li&gt;&lt;li&gt;Argues that conventional performance metrics are insufficient for safety-critical evaluation of lane detection in autonomous driving and extends safety-focused evaluation approaches used for object detection to lane detection.&lt;/li&gt;&lt;li&gt;Validates LSM offline using virtual scenarios and multiple lane detection methods, and compares LSM outcomes to state-of-the-art performance metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J\\"org Gamerdinger', 'Sven Teufel', 'Stephan Amann', 'Georg Volk', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['lane-detection', 'safety-metrics', 'perception-evaluation', 'autonomous-driving', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.07740</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title><link>https://arxiv.org/abs/2312.04960</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of Vision Transformers (ViTs) and shows incompatibilities of existing AT methods with ViT architectures.&lt;/li&gt;&lt;li&gt;Derives mutual information (MI) bounds for ViT autoencoder-based self-supervised pretraining to constrain MI between adversarial examples and latent representations.&lt;/li&gt;&lt;li&gt;Proposes MIMIR: a self-supervised adversarial training method using an MI penalty with masked image modeling to improve robust and natural accuracy.&lt;/li&gt;&lt;li&gt;Evaluates on CIFAR-10, Tiny-ImageNet, and ImageNet-1K, showing SOTA robustness results, resistance to common corruptions, unforeseen attacks, and adaptive (white-box) attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Xu', 'Shujian Yu', 'Zhuoran Liu', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'vision transformers', 'mutual information', 'self-supervised learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.04960</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MMGR: Multi-Modal Generative Reasoning</title><link>https://arxiv.org/abs/2512.14691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MMGR, a benchmark evaluating generative models on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.&lt;/li&gt;&lt;li&gt;Covers three domains (Abstract Reasoning, Embodied Navigation, Physical Commonsense) with fine-grained metrics requiring holistic correctness across video and image outputs.&lt;/li&gt;&lt;li&gt;Benchmarks leading video and image models, finding strong gaps (e.g., &lt;10% on Abstract Reasoning) and specific failure modes like weak global state consistency and overreliance on perceptual plausibility.&lt;/li&gt;&lt;li&gt;Provides a unified diagnostic framework aimed at improving reasoning-aware generative world models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zefan Cai', 'Haoyi Qiu', 'Tianyi Ma', 'Haozhe Zhao', 'Gengze Zhou', 'Kung-Hsiang Huang', 'Parisa Kordjamshidi', 'Minjia Zhang', 'Xiao Wen', 'Jiuxiang Gu', 'Nanyun Peng', 'Junjie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'benchmark', 'generative-models', 'reasoning', 'video-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14691</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VICTOR: Dataset Copyright Auditing in Video Recognition Systems</title><link>https://arxiv.org/abs/2512.14439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VICTOR, the first dataset copyright auditing method tailored for video recognition systems by leveraging stealthy sample modifications.&lt;/li&gt;&lt;li&gt;Modifies a small fraction of published training videos (e.g., ~1%) to amplify output discrepancies in target models, enabling detection of unauthorized dataset use by comparing behavior on modified vs. original samples.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple models and datasets and shows robustness to various perturbations to training videos or target models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quan Yuan', 'Zhikun Zhang', 'Linkang Du', 'Min Chen', 'Mingyang Sun', 'Yunjun Gao', 'Shibo He', 'Jiming Chen']&lt;/li&gt;&lt;li&gt;Tags: ['dataset auditing', 'model forensics', 'watermarking', 'video recognition', 'copyright detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14439</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems</title><link>https://arxiv.org/abs/2512.14367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a single interpretable safety metric for object perception in autonomous vehicles that weights detections by parameters such as velocity, orientation, distance, size, and potential collision damage.&lt;/li&gt;&lt;li&gt;Evaluates the metric on real-world and simulated datasets and compares it to existing perception evaluation metrics.&lt;/li&gt;&lt;li&gt;Aims to provide a safety-centric assessment rather than standard detection-centric metrics (e.g., precision/recall/IoU).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georg Volk', 'J\\"org Gamerdinger', 'Alexander von Bernuth', 'Oliver Bringmann']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'autonomous-vehicles', 'perception-metrics', 'functional-safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14367</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation</title><link>https://arxiv.org/abs/2512.14054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scale-adaptive dual-expert perception framework using two YOLOv8 models specialized for far-range (small helipads) and close-range (large helipads) detection.&lt;/li&gt;&lt;li&gt;Uses a geometric gating mechanism to route between experts during inference, with both experts running in parallel to select the most viewpoint-consistent prediction.&lt;/li&gt;&lt;li&gt;Evaluated in a closed-loop simulation integrating CARLA rendering and NASA's GUAM flight-dynamics engine; reports improved alignment stability, landing accuracy, and robustness over single-detector baselines.&lt;/li&gt;&lt;li&gt;Focuses on operational perception robustness for safety-critical AAV landing but does not study adversarial attacks, model extraction, or intentional security threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Humaira Tasnim', 'Ashik E Rasul', 'Bruce Jo', 'Hyung-Jin Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['aerial-robotics', 'perception-robustness', 'object-detection', 'autonomous-landing', 'simulation-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14054</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title><link>https://arxiv.org/abs/2512.14601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FakeRadar, a deepfake video detection framework that generates synthetic outlier forgeries near cluster boundaries to simulate unseen manipulation types.&lt;/li&gt;&lt;li&gt;Uses large-scale pretrained models (e.g., CLIP) to probe feature space and model dynamic subclusters of known real/fake data.&lt;/li&gt;&lt;li&gt;Introduces Outlier-Guided Tri-Training with outlier-driven contrastive learning and outlier-conditioned cross-entropy to train a detector to separate real, known fake, and outlier samples.&lt;/li&gt;&lt;li&gt;Reports improved cross-domain generalization on benchmark deepfake video datasets, particularly against emerging/unknown manipulation techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolun Li', 'Jichang Li', 'Yinqi Cai', 'Junye Chen', 'Xiaonan Luo', 'Guanbin Li', 'Rushi Lan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'outlier detection', 'cross-domain generalization', 'forgery detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14601</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LCMem: A Universal Model for Robust Image Memorization Detection</title><link>https://arxiv.org/abs/2512.14421</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LCMem, a Latent Contrastive Memorization Network for detecting whether images have been memorized by generative models or copied from training data.&lt;/li&gt;&lt;li&gt;Uses a two-stage training procedure: first learn identity-consistent representations, then train for augmentation-robust copy detection.&lt;/li&gt;&lt;li&gt;Evaluated cross-domain on six benchmark datasets, reporting large improvements (up to ~16 pp on re-identification, ~30 pp on copy detection) and better robustness than existing privacy filters.&lt;/li&gt;&lt;li&gt;Aims to provide scalable, reliable privacy auditing tools for image memorization; code and model publicly released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mischa Dombrowski', 'Felix N\\"utzel', 'Bernhard Kainz']&lt;/li&gt;&lt;li&gt;Tags: ['memorization detection', 'privacy auditing', 're-identification', 'copy detection', 'image privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14421</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mimicking Human Visual Development for Learning Robust Image Representations</title><link>https://arxiv.org/abs/2512.14360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a progressive blurring curriculum (start training on highly blurred images, reduce blur over epochs) inspired by human visual development to prioritize global structure over high-frequency details.&lt;/li&gt;&lt;li&gt;Reports robustness gains: up to 8.30% reduction in mCE on CIFAR-10-C and 4.43% on ImageNet-100-C versus standard training, and claims improvements in both natural and adversarial robustness.&lt;/li&gt;&lt;li&gt;Method complements common augmentations (CutMix, MixUp) and differs from static blur augmentation by following a structured progression.&lt;/li&gt;&lt;li&gt;Code release provided for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ankita Raj', 'Kaashika Prajaapat', 'Tapan Kumar Gandhi', 'Chetan Arora']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial-robustness', 'data-augmentation', 'curriculum-learning', 'image-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14360</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Transferable Defense Against Malicious Image Edits</title><link>https://arxiv.org/abs/2512.14341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TDAE, a bimodal (image+text) defense framework to make images immune to malicious diffusion-based edits via coordinated image-text adversarial optimization.&lt;/li&gt;&lt;li&gt;Visual defense: FlatGrad Defense Mechanism (FDM) adds gradient regularization to push perturbations toward flat minima, improving robustness and cross-model transferability.&lt;/li&gt;&lt;li&gt;Textual defense: Dynamic Prompt Defense (DPD) iteratively refines text embeddings and updates images so immunized images align editing outcomes with originals across diverse embeddings, encouraging broader immunity features.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art mitigation of malicious edits in both intra-model and cross-model evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'image editing', 'transferability', 'textual prompt defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14341</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Attention Guided Defense Against Malicious Edits</title><link>https://arxiv.org/abs/2512.14333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dual Attention-Guided Noise Perturbation (DANP), an imperceptible perturbation method to immunize images against malicious text-to-image edits by manipulating cross-attention maps and noise prediction across diffusion timesteps.&lt;/li&gt;&lt;li&gt;Uses dynamic thresholds to mask text-relevant vs irrelevant regions, reducing attention in relevant areas and increasing it in irrelevant ones to misguide edits while preserving intended targets.&lt;/li&gt;&lt;li&gt;Maximizes discrepancy between injected noise and model-predicted noise to further disrupt generation; reports state-of-the-art empirical robustness against malicious edits on diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbation', 'robustness/defense', 'text-to-image diffusion', 'attention manipulation', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14333</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title><link>https://arxiv.org/abs/2512.14320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings of existing immunization evaluation (which measure visual dissimilarity to a single reference output) and reframes image immunization as causing semantic mismatch with attacker prompts or producing perceptual degradation.&lt;/li&gt;&lt;li&gt;Proposes Synergistic Intermediate Feature Manipulation (SIFM): perturbing intermediate diffusion features with dual objectives — maximize divergence from original edit trajectory and minimize feature norms to induce degradations.&lt;/li&gt;&lt;li&gt;Introduces Immunization Success Rate (ISR), a metric that uses Multimodal LLMs to determine whether edits either semantically fail relative to a prompt or exhibit significant perceptual degradation; reports SIFM achieves state-of-the-art defense performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Dong', 'Jie Zhang', 'Guoying Zhao', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image immunization', 'adversarial perturbations', 'diffusion models', 'safety/defense', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14320</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes</title><link>https://arxiv.org/abs/2512.14177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Semantic Gaussian Process Uncertainty (SGPU): map multiple generated answers into embedding space, compute Gram matrix eigenspectrum, and use a Gaussian Process Classifier to predict semantic uncertainty without brittle clustering.&lt;/li&gt;&lt;li&gt;Works in both black-box and white-box settings and summarizes semantic consistency via spectral features rather than cluster labels.&lt;/li&gt;&lt;li&gt;Evaluated across six LLMs/LVLMs and eight datasets (VQA, image classification, textual QA), reporting improved calibration (ECE) and discriminative metrics (AUROC, AUARC) and transferability across models/modalities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph Hoche', 'Andrei Bursuc', 'David Brellmann', 'Gilles Louppe', 'Pavel Izmailov', 'Angela Yao', 'Gianni Franchi']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'calibration', 'LVLMs', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14177</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World</title><link>https://arxiv.org/abs/2512.14158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIS-BA, a new backdoor attack paradigm for object detectors that uses continuous inter-object interaction patterns as 'space triggers' instead of static pixel cues.&lt;/li&gt;&lt;li&gt;Presents CIS-Frame, a pipeline to construct interaction-based triggers, encode them as class-geometry constraints for poisoning, and embed the backdoor during training to enable single- and multi-object coordinated attacks.&lt;/li&gt;&lt;li&gt;Evaluated on MS-COCO and real-world videos, reporting &gt;97% attack success in complex settings, &gt;95% effectiveness under dynamic multi-trigger conditions, and bypassing three state-of-the-art defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuxin Zhao', 'Bo Lang', 'Nan Xiao', 'Yilang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor-attack', 'object-detection', 'adversarial-security', 'real-world-robustness', 'trigger-design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14158</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models</title><link>https://arxiv.org/abs/2512.14137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a closed-form, data-free selective unlearning method for CLIP-like multimodal models using nullspace projection on the final text-to-embedding projection layer.&lt;/li&gt;&lt;li&gt;Computes an orthonormal basis for target class text embeddings and projects out those directions to reduce image–text alignment for the target classes without retraining or images from the forget set.&lt;/li&gt;&lt;li&gt;Demonstrates substantial drop in zero-shot performance for targeted classes while largely preserving overall multimodal knowledge; partial projections allow trade-offs between complete unlearning and information retention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Mishra', 'Tarun Kumar', 'Gyanaranjan Nayak', 'Arpit Shah', 'Suparna Bhattacharya', 'Martin Foltin']&lt;/li&gt;&lt;li&gt;Tags: ['selective unlearning', 'model editing', 'privacy', 'CLIP', 'multimodal robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14137</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach</title><link>https://arxiv.org/abs/2512.14113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training- and data-free unlearning framework for pretrained CLIP that removes class information while preserving other knowledge.&lt;/li&gt;&lt;li&gt;Implements three forgetting paradigms: global unlearning, domain-specific removal, and selective domain unlearning.&lt;/li&gt;&lt;li&gt;Uses a multimodal nullspace by combining text prompts and synthesized visual prototypes from CLIP's joint embedding space to excise undesired classes efficiently.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Mishra', 'Gyanaranjan Nayak', 'Tarun Kumar', 'Arpit Shah', 'Suparna Bhattacharya', 'Martin Foltin']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'CLIP', 'data-free model editing', 'privacy/compliance', 'safety/robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14113</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title><link>https://arxiv.org/abs/2512.14044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;OmniDrive-R1 is an end-to-end vision-language model framework for autonomous driving that interleaves perception and multimodal chain-of-thought (iMCoT) to reduce object hallucination and improve reasoning reliability.&lt;/li&gt;&lt;li&gt;Introduces a reinforcement-driven visual grounding mechanism that lets the model direct attention/zoom to critical regions using a two-stage RL pipeline and the Clip-GRPO algorithm.&lt;/li&gt;&lt;li&gt;Clip-GRPO uses an annotation-free, process-based grounding reward enforcing real-time cross-modal consistency, removing the need for dense localization labels and external tool calls.&lt;/li&gt;&lt;li&gt;Shows large empirical gains on DriveLMM-o1 versus Qwen2.5VL-7B (reasoning score 51.77% → 80.35%; final answer accuracy 37.81% → 73.62%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenguo Zhang', 'Haohan Zhen', 'Yishen Wang', 'Le Xu', 'Tianchen Deng', 'Xuefeng Chen', 'Qu Chen', 'Bo Zhang', 'Wuxiong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'safety/robustness', 'visual grounding', 'chain-of-thought', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14044</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation</title><link>https://arxiv.org/abs/2512.13953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'unbranding', a task for fine-grained removal of explicit trademarks and implicit trade dress from text-to-image outputs while retaining semantic coherence.&lt;/li&gt;&lt;li&gt;Builds a comprehensive benchmark dataset and shows newer high-fidelity diffusion models reproduce brand identifiers more readily than older models.&lt;/li&gt;&lt;li&gt;Introduces a VLM-based question-answering evaluation metric to detect both logos and holistic brand characteristics that standard detectors miss.&lt;/li&gt;&lt;li&gt;Validates the problem empirically and provides a project page with resources to drive development of specialized mitigation techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dawid Malarz', 'Artur Kasymov', 'Filip Manjak', 'Maciej Zi\\k{e}ba', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'safety', 'benchmarking', 'trademark-protection', 'VLM-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13953</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title><link>https://arxiv.org/abs/2512.13742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a vision-to-language framework (DL^3M) linking endoscopic image classification (MobileCoAtNet) to LLM-driven clinical reasoning.&lt;/li&gt;&lt;li&gt;Introduces two expert-verified benchmarks for clinical reasoning (causes, symptoms, treatment, lifestyle, follow-up) and evaluates 32 LLMs against them.&lt;/li&gt;&lt;li&gt;Finds that stronger image classification improves explanations but LLMs show instability and fail to reach human-level reliability; reasoning changes with prompt variation.&lt;/li&gt;&lt;li&gt;Provides code and datasets, and frames limitations relevant to building safer, more reliable multimodal medical reasoning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Najib Hasan (Wichita State University', 'USA)', 'Imran Ahmad (Wichita State University', 'USA)', 'Sourav Basak Shuvo (Khulna University of Engineering and Technology', 'Bangladesh)', 'Md. Mahadi Hasan Ankon (Khulna University of Engineering and Technology', 'Bangladesh)', 'Sunanda Das (University of Arkansas', 'USA)', 'Nazmul Siddique (Ulster University', 'UK)', "Hui Wang (Queen's University Belfast", 'UK)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety evaluation', 'robustness', 'multimodal medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13742</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title><link>https://arxiv.org/abs/2512.13739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes AIGC-assisted image production for journalism, focusing on misinformation, authenticity, semantic fidelity, and interpretability.&lt;/li&gt;&lt;li&gt;Experiment 1: cross-platform prompt standardization across three scenes, revealing semantic misalignment, cultural specificity issues, and training/filtering biases.&lt;/li&gt;&lt;li&gt;Experiment 2: develops a human-in-the-loop pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), style control (Style-LoRA, Prompt-to-Prompt), CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials to ensure editorial fidelity.&lt;/li&gt;&lt;li&gt;Proposes a human-AI collaboration mechanism and new evaluation axes: Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yajie Yang', 'Yuqing Zhao', 'Xiaochao Xi', 'Yinan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC safety', 'content authenticity', 'human-in-the-loop', 'semantic alignment', 'cultural bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13739</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study showing LLM safety refusal decisions are unstable across random seeds and sampling temperature.&lt;/li&gt;&lt;li&gt;Tested 4 instruction-tuned models on 876 harmful prompts across 20 sampling configurations; 18–28% of prompts flipped (refuse in some configs, comply in others).&lt;/li&gt;&lt;li&gt;Introduces a Safety Stability Index (SSI), finds higher temperature reduces stability, and validates findings with an external judge (Claude 3.5 Haiku).&lt;/li&gt;&lt;li&gt;Concludes single-shot safety evaluations are insufficient (single-shot agrees with multi-sample ground truth ~92.4%) and recommends using at least 3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'evaluation/benchmarking', 'stochasticity', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Semantic-Drive is a local-first neuro-symbolic pipeline for mining long-tail, safety-critical events from AV video logs using open-vocabulary grounding (YOLOE) followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Introduces a 'Judge-Scout' multi-model consensus (System 2 inference-time alignment) to mitigate VLM hallucinations and improve reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes (mapped to WOD-E2E taxonomy), reports Recall 0.966 vs 0.475 for CLIP and a 40% reduction in Risk Assessment Error compared to best single scout models.&lt;/li&gt;&lt;li&gt;Designed to run on consumer GPU (RTX 3090), enabling privacy-preserving, on-premises operation rather than cloud-based VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation / robustness', 'Neuro-symbolic VLM consensus', 'Open-vocabulary grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title><link>https://arxiv.org/abs/2511.10400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes reliability of LLM-based agents in multi-agent systems through the lens of Byzantine fault tolerance and compares them to traditional agents.&lt;/li&gt;&lt;li&gt;Proposes CP-WBFT, a confidence probe-based weighted BFT consensus mechanism leveraging LLMs' reflective/discriminative abilities to weight information flow.&lt;/li&gt;&lt;li&gt;Presents extensive experiments showing improved stability and accuracy under extreme Byzantine conditions (up to 85.7% fault rate) across topologies and tasks (mathematical reasoning, safety assessment).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lifan Zheng', 'Jiawei Chen', 'Qinghong Yin', 'Jingyuan Zhang', 'Xinyi Zeng', 'Yu Tian']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-based agents', 'Byzantine fault tolerance', 'multi-agent robustness', 'consensus mechanisms', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10400</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Inverse Scaling in Test-Time Compute</title><link>https://arxiv.org/abs/2507.14417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs evaluation tasks showing that increasing test-time reasoning length can decrease LRM accuracy across counting, regression, deduction, and AI-risk tasks.&lt;/li&gt;&lt;li&gt;Identifies five failure modes: distraction by irrelevant info (Claude), overfitting to framings (OpenAI o-series), shift from priors to spurious correlations, loss of focus on complex deduction, and amplification of concerning behaviors (e.g., self-preservation).&lt;/li&gt;&lt;li&gt;Argues that test-time compute scaling can inadvertently reinforce problematic reasoning patterns and recommends evaluating models across diverse reasoning lengths to surface these failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryo Pradipta Gema', 'Alexander H\\"agele', 'Runjin Chen', 'Andy Arditi', 'Jacob Goldman-Wetzler', 'Kit Fraser-Taliente', 'Henry Sleight', 'Linda Petrini', 'Julian Michael', 'Beatrice Alex', 'Pasquale Minervini', 'Yanda Chen', 'Joe Benton', 'Ethan Perez']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'alignment', 'inverse-scaling', 'robustness', 'risk-amplification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14417</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holistic Utility Preference Learning for Listwise Alignment</title><link>https://arxiv.org/abs/2410.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Direct Ranking Preference Optimization (DRPO), framing human preference alignment as a listwise Learning-to-Rank task rather than pairwise comparisons.&lt;/li&gt;&lt;li&gt;Proposes diffNDCG, a differentiable approximation of NDCG implemented with a sorting network to enable end-to-end optimization of listwise ranking metrics.&lt;/li&gt;&lt;li&gt;Adds a margin-based Adaptive Rank Policy Score to improve discriminative quality of generated responses.&lt;/li&gt;&lt;li&gt;Empirical results claim DRPO outperforms existing pairwise methods (e.g., DPO) in aligning model outputs with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacong Zhou', 'Xianyun Wang', 'Min Zhang', 'Jun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'learning-to-rank', 'differentiable-sorting', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18127</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Knowledge Distillation via Synthetic Text Generation</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DistilDP: a differentially private knowledge distillation method where a DP-trained teacher LLM generates synthetic text used to train a smaller student model.&lt;/li&gt;&lt;li&gt;Transfers knowledge via hard labels (synthetic data labels), soft labels (teacher output distributions on synthetic data), and optional hidden-representation alignment when architectures are similar.&lt;/li&gt;&lt;li&gt;Demonstrates improved utility over baselines on language modeling (e.g., ~9.0 PPL improvement on Big Patent) under strong privacy (ε = 2).&lt;/li&gt;&lt;li&gt;Targets privacy-preserving model compression for autoregressive LLMs and provides code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Flemings', 'Murali Annavaram']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'knowledge_distillation', 'synthetic_data', 'privacy-preserving_ml', 'model_compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.00932</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment</title><link>https://arxiv.org/abs/2512.09636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MentraSuite: a framework and MentraBench benchmark for evaluating clinically aligned mental-health reasoning across five dimensions (conciseness, coherence, hallucination avoidance, task understanding, internal consistency) spanning six tasks and 13 datasets.&lt;/li&gt;&lt;li&gt;Proposes Mindora, a post-trained LLM using a hybrid SFT+RL pipeline with an inconsistency-detection reward to improve faithful, coherent stepwise reasoning for mental-health assessment and intervention planning.&lt;/li&gt;&lt;li&gt;Presents a novel reasoning-trajectory generation strategy that filters difficult samples and rewrites trajectories for conciseness, readability, and consistency to support training.&lt;/li&gt;&lt;li&gt;Reports evaluation across 20 LLMs where Mindora achieves highest average performance on MentraBench and improved reasoning reliability for complex mental-health scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengxi Xiao', 'Kailai Yang', 'Pengde Zhao', 'Enze Zhang', 'Ziyan Kuang', 'Zhiwei Liu', 'Weiguang Han', 'Shu Liao', 'Lianting Huang', 'Jinpeng Hu', 'Min Peng', 'Qianqian Xie', 'Sophia Ananiadou']&lt;/li&gt;&lt;li&gt;Tags: ['Safety/Alignment', 'Benchmarking', 'LLM fine-tuning (SFT+RL)', 'Hallucination mitigation', 'Mental-health LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09636</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title><link>https://arxiv.org/abs/2512.08786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for aggregating group-level preference/reward signals in a federated RLHF setup to study trade-offs between alignment quality and fairness.&lt;/li&gt;&lt;li&gt;Compares standard aggregation rules (min, max, average) and proposes an adaptive weighting scheme that adjusts group preference weights based on historical alignment performance.&lt;/li&gt;&lt;li&gt;Empirical results using a PPO-based RLHF pipeline on QA tasks show the adaptive scheme improves fairness while keeping competitive alignment, with server-side aggregation that preserves raw-data locality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Srewa', 'Tianyu Zhao', 'Salma Elmalaki']&lt;/li&gt;&lt;li&gt;Tags: ['federated RLHF', 'alignment', 'fairness', 'preference aggregation', 'policy optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08786</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics</title><link>https://arxiv.org/abs/2509.24102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes pragmatic inference methods grounded in moral foundations theory to help LLMs bridge the gap between distributional semantics and pragmatic moral reasoning.&lt;/li&gt;&lt;li&gt;Leverages contextual information at each reasoning step to connect moral foundations with moral reasoning objectives.&lt;/li&gt;&lt;li&gt;Reports experimental results showing improved generalization in moral reasoning for LLMs using the proposed approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangliang Liu', 'Xi Chen', 'Bocheng Chen', 'Han Zi', 'Xitong Zhang', 'Kristen Johnson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'moral reasoning', 'LLMs', 'safety', 'pragmatic inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24102</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DIWALI: Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</title><link>https://arxiv.org/abs/2509.17399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DIWALI, a dataset of ~8k culture-specific items (CSIs) across 17 cultural facets and 36 Indian sub-regions to evaluate LLM cultural competence.&lt;/li&gt;&lt;li&gt;Uses the CSIs to assess LLM performance on a cultural text adaptation task via automated metrics, LLM-as-judge, and human evaluation from diverse socio-demographic regions.&lt;/li&gt;&lt;li&gt;Finds selective sub-regional coverage and predominantly surface-level adaptations across evaluated LLMs, and releases dataset, code, and model outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pramit Sahoo', 'Maharaj Brahma', 'Maunendra Sankar Desarkar']&lt;/li&gt;&lt;li&gt;Tags: ['cultural-alignment', 'dataset', 'LLM-evaluation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.17399</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation</title><link>https://arxiv.org/abs/2506.23979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TaP, a taxonomy-guided framework to automatically and scalably generate preference datasets for supervised and preference fine-tuning of LLMs.&lt;/li&gt;&lt;li&gt;Allows fine-grained control over dataset composition via a structured taxonomy to ensure diversity, coverage, and multilingual support.&lt;/li&gt;&lt;li&gt;Reports that LLMs fine-tuned on TaP-generated data outperform those trained on existing open-source datasets, even when the baseline dataset is much larger.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renren Jin', 'Tianhao Shen', 'Xinwei Wu', 'Dan Shi', 'Haoran Sun', 'Yuqi Ren', 'Wuwei Huang', 'Quandong Wang', 'Wei Liu', 'Jian Luan', 'Bin Wang', 'Deyi Xiong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'preference data generation', 'dataset construction', 'multilingual', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23979</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title><link>https://arxiv.org/abs/2410.03026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new metric called "context influence" (grounded in differential privacy) to quantify how augmented contextual knowledge affects an LM's outputs while separating the model's parametric knowledge.&lt;/li&gt;&lt;li&gt;Measures the contribution of different subsets of the context to decoding and shows that naive comparisons to context can overestimate privacy risk when the model already knows the content.&lt;/li&gt;&lt;li&gt;Finds that contextual privacy leakage is most pronounced when contextual knowledge is out-of-distribution relative to the model's parametric knowledge and studies factors such as model size, context size, and generation position.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation and practical guidance to estimate and attribute privacy leakage from augmented contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Flemings', 'Bo Jiang', 'Wanrong Zhang', 'Zafar Takhirov', 'Murali Annavaram']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-leakage', 'differential-privacy', 'language-models', 'contextual-augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03026</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MMGR: Multi-Modal Generative Reasoning</title><link>https://arxiv.org/abs/2512.14691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents MMGR, a benchmark and evaluation framework measuring five generative reasoning abilities (Physical, Logical, 3D Spatial, 2D Spatial, Temporal) across three domains: Abstract Reasoning, Embodied Navigation, and Physical Commonsense.&lt;/li&gt;&lt;li&gt;Defines fine-grained metrics requiring holistic correctness in both video and image generation to detect reasoning failures (causality, physics, global state consistency) that are not captured by perceptual metrics like FVD.&lt;/li&gt;&lt;li&gt;Benchmarks state-of-the-art video and image generative models, revealing major gaps (very low abstract reasoning accuracy, failures in long-horizon spatial planning) and arguing that current objectives favor visual plausibility over causal/correct reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zefan Cai', 'Haoyi Qiu', 'Tianyi Ma', 'Haozhe Zhao', 'Gengze Zhou', 'Kung-Hsiang Huang', 'Parisa Kordjamshidi', 'Minjia Zhang', 'Xiao Wen', 'Jiuxiang Gu', 'Nanyun Peng', 'Junjie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'benchmarking', 'physical-reasoning', 'video-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14691</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C-ing Clearly: Enhanced Binary Code Explanations using C code</title><link>https://arxiv.org/abs/2512.14500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C-ing Clearly, a synthetic data generation method that leverages corresponding C source code to improve LLM understanding of assembly.&lt;/li&gt;&lt;li&gt;Fine-tunes LLMs on this generated data and shows improved performance on binary code summarization and vulnerability detection tasks.&lt;/li&gt;&lt;li&gt;Reports consistent gains across different LLM families and model sizes, indicating broad applicability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teodor Poncu', 'Ioana Pintilie', 'Marius Dragoi', 'Dragos Tantaru', 'Florin Brad']&lt;/li&gt;&lt;li&gt;Tags: ['binary analysis', 'vulnerability detection', 'assembly', 'LLM fine-tuning', 'synthetic data generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14500</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2512.14118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CogMem, a three-layer cognitive memory architecture (Long-Term Memory, Direct Access, Focus of Attention) to support sustained multi-turn reasoning in LLMs.&lt;/li&gt;&lt;li&gt;Aims to reduce reasoning failures (reasoning bias, task drift, hallucination, overconfidence, memory decay) by consolidating cross-session strategies and reconstructing concise task-relevant context per turn.&lt;/li&gt;&lt;li&gt;Demonstrates on TurnBench that CogMem controls context growth, improves consistency across extended reasoning chains, and enhances reasoning efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiran Zhang', 'Jincheng Hu', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['memory-augmented LLMs', 'hallucination mitigation', 'robustness', 'multi-turn reasoning', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14118</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2512.07342</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PrivORL, a differentially private (DP) synthetic dataset synthesis method for offline reinforcement learning using diffusion models (for transitions) and diffusion transformers (for trajectories).&lt;/li&gt;&lt;li&gt;Follows a public pre-training + DP-SGD fine-tuning workflow and introduces curiosity-driven pre-training to increase diversity of generated transitions/trajectories.&lt;/li&gt;&lt;li&gt;Evaluates on five sensitive offline RL datasets, reporting improved utility and fidelity of DP synthetic transitions and trajectories versus baselines; provides replication code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Gong', 'Zheng Liu', 'Kecen Li', 'Tianhao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['differential-privacy', 'offline-reinforcement-learning', 'synthetic-data', 'privacy-preserving', 'DP-SGD']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07342</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning Robust and Correct Controllers Guided by Feasibility-Aware Signal Temporal Logic via BarrierNet</title><link>https://arxiv.org/abs/2512.06973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a feasibility-aware learning framework that embeds time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP) to enforce safety specified by Signal Temporal Logic (STL).&lt;/li&gt;&lt;li&gt;Introduces a unified robustness measure that jointly accounts for STL satisfaction, QP feasibility, and control-bound compliance, and constructs time-varying HOCBF constraints for a broad STL fragment.&lt;/li&gt;&lt;li&gt;Uses three neural networks (InitNet, RefNet, extended BarrierNet) to generate references and adapt constraint hyperparameters online, reducing conservativeness and improving STL satisfaction under tight input bounds compared to fixed-parameter baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Liu', 'Wenliang Liu', 'Wei Xiao', 'Calin A. Belta']&lt;/li&gt;&lt;li&gt;Tags: ['control barrier functions', 'signal temporal logic', 'robotic safety', 'robustness', 'differentiable optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06973</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</title><link>https://arxiv.org/abs/2505.22090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using diffusion models to reconstruct high-volume-rate 3D ultrasound volumes from a heavily undersampled set of elevation planes, outperforming traditional and supervised interpolation baselines in image quality and downstream tasks.&lt;/li&gt;&lt;li&gt;Leverages temporal consistency in ultrasound sequences to accelerate inference.&lt;/li&gt;&lt;li&gt;Uses the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrates improved recall on out-of-distribution synthetic anomalies under strong subsampling.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tristan S. W. Stevens', "Ois\\'in Nolan", 'Oudom Somphone', 'Jean-Luc Robert', 'Ruud J. G. van Sloun']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'uncertainty quantification', 'medical imaging', 'diffusion models', 'OOD detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22090</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holistic Utility Preference Learning for Listwise Alignment</title><link>https://arxiv.org/abs/2410.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Direct Ranking Preference Optimization (DRPO), framing human preference alignment as a listwise Learning-to-Rank task rather than pairwise comparisons.&lt;/li&gt;&lt;li&gt;Proposes diffNDCG, a differentiable approximation of NDCG implemented with a sorting network to enable end-to-end optimization of listwise ranking metrics.&lt;/li&gt;&lt;li&gt;Adds a margin-based Adaptive Rank Policy Score to improve discriminative quality of generated responses.&lt;/li&gt;&lt;li&gt;Empirical results claim DRPO outperforms existing pairwise methods (e.g., DPO) in aligning model outputs with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacong Zhou', 'Xianyun Wang', 'Min Zhang', 'Jun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'learning-to-rank', 'differentiable-sorting', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18127</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title><link>https://arxiv.org/abs/2410.03026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new metric called "context influence" (grounded in differential privacy) to quantify how augmented contextual knowledge affects an LM's outputs while separating the model's parametric knowledge.&lt;/li&gt;&lt;li&gt;Measures the contribution of different subsets of the context to decoding and shows that naive comparisons to context can overestimate privacy risk when the model already knows the content.&lt;/li&gt;&lt;li&gt;Finds that contextual privacy leakage is most pronounced when contextual knowledge is out-of-distribution relative to the model's parametric knowledge and studies factors such as model size, context size, and generation position.&lt;/li&gt;&lt;li&gt;Provides empirical evaluation and practical guidance to estimate and attribute privacy leakage from augmented contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Flemings', 'Bo Jiang', 'Wanrong Zhang', 'Zafar Takhirov', 'Murali Annavaram']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-leakage', 'differential-privacy', 'language-models', 'contextual-augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03026</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Formal Verification via Autoencoder Latent Space Abstraction</title><link>https://arxiv.org/abs/2512.13593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method using convex autoencoders to reduce system dimensionality and learn latent-space dynamics via a kernel-based approach for formal verification.&lt;/li&gt;&lt;li&gt;Builds finite abstractions in the latent space that are guaranteed to overapproximate the true system behaviors and maps verification results back to the original system.&lt;/li&gt;&lt;li&gt;Demonstrates scalability and rigor on multiple systems, including a 26-dimensional system controlled by a neural network.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Reed', 'Luca Laurenti', 'Morteza Lahijanian']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'safety', 'latent-space abstraction', 'autoencoders', 'neural-network controllers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13593</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</title><link>https://arxiv.org/abs/2512.13207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data poisoning (global bias and patch-based) attacks by compromised clients in federated learning for surface temperature forecasting using the CERRA dataset.&lt;/li&gt;&lt;li&gt;Finds small fractions of poisoned clients can induce large, spatially persistent errors (global bias up to -1.7 K; patch attacks causing &gt;+3.5 K anomalies and &gt;3x MSE).&lt;/li&gt;&lt;li&gt;Assesses trimmed mean aggregation as a defense: effective against global bias (minor degradation) but fails catastrophically against spatially correlated patch attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karina Chichifoi', 'Fabio Merizzi', 'Michele Colajanni']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'data poisoning', 'adversarial attacks', 'defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13207</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study showing LLM safety refusal decisions are unstable across random seeds and sampling temperature.&lt;/li&gt;&lt;li&gt;Tested 4 instruction-tuned models on 876 harmful prompts across 20 sampling configurations; 18–28% of prompts flipped (refuse in some configs, comply in others).&lt;/li&gt;&lt;li&gt;Introduces a Safety Stability Index (SSI), finds higher temperature reduces stability, and validates findings with an external judge (Claude 3.5 Haiku).&lt;/li&gt;&lt;li&gt;Concludes single-shot safety evaluations are insufficient (single-shot agrees with multi-sample ground truth ~92.4%) and recommends using at least 3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'evaluation/benchmarking', 'stochasticity', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning</title><link>https://arxiv.org/abs/2512.09368</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CFLight, a counterfactual-learning-based RL framework to prioritize safety (reduce collisions) in traffic signal control while improving interpretability.&lt;/li&gt;&lt;li&gt;Introduces a structural causal model and a counterfactual (CF) module that backtracks unsafe events to evaluate alternative actions and their downstream outcomes.&lt;/li&gt;&lt;li&gt;Claims near-zero collision control and shows empirical improvements over conventional RL and a recent safe RL baseline on real-world and synthetic traffic datasets; code/data released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyuan Li', 'Chunyu Liu', 'Zhuojun Li', 'Xiao Liu', 'Guangsheng Yu', 'Bo Du', 'Jun Shen', 'Qiang Wu']&lt;/li&gt;&lt;li&gt;Tags: ['safe RL', 'counterfactual learning', 'causal modeling', 'traffic signal control', 'safe control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.09368</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title><link>https://arxiv.org/abs/2512.05442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IdealTSF, a framework that leverages 'non-ideal' negative samples (missing values, anomalies, noisy data) alongside ideal positive samples to improve time-series forecasting.&lt;/li&gt;&lt;li&gt;Three-stage pipeline: pretraining on negative samples to extract useful knowledge, training that transforms sequences into ideal positives, and a negative optimization step employing adversarial disturbances.&lt;/li&gt;&lt;li&gt;Claims that incorporating negative-sample knowledge plus adversarial optimization boosts performance of basic attention architectures, especially on noisy/low-quality datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Wang', 'Jinghao Lu', 'Fan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['time-series', 'robustness', 'adversarial-training', 'anomaly-handling', 'data-quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05442</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Difficulties with Evaluating a Deception Detector for AIs</title><link>https://arxiv.org/abs/2511.22662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that reliably evaluating deception detectors for AI is currently infeasible because we lack examples that can be confidently labeled as deceptive or honest.&lt;/li&gt;&lt;li&gt;Identifies concrete obstacles to collecting labeled examples, including strategic behavior, difficulty of ground-truth labeling, confounding factors, and limits of behavioural or intervention-based evidence.&lt;/li&gt;&lt;li&gt;Analyzes existing empirical work and illustrative case studies to show common failure modes and why proposed empirical workarounds (e.g., synthetic data, adversarial probes) are insufficient by themselves.&lt;/li&gt;&lt;li&gt;Concludes that progress on deception detection requires deeper conceptual and methodological work addressing these evaluation challenges before deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lewis Smith', 'Bilal Chughtai', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['deception-detection', 'AI-alignment', 'evaluation-challenges', 'red-teaming', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.22662</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title><link>https://arxiv.org/abs/2508.09162</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an unsupervised explainable AI framework combining an autoencoder and a customized windowSHAP to detect and localize concurrent replay attacks in multivariate time-series from nuclear reactor sensors.&lt;/li&gt;&lt;li&gt;Aims to identify detection, attack source, timing, type, and duration (localization) of replayed signals in real-time without supervised labels.&lt;/li&gt;&lt;li&gt;Benchmarked on real-world datasets from Purdue's PUR-1 reactor with up to six signals replayed concurrently, reporting ≥95% accuracy for detection and localization tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Konstantinos Vasili', 'Zachery T. Dahm', 'Stylianos Chatzidakis']&lt;/li&gt;&lt;li&gt;Tags: ['replay-attacks', 'anomaly-detection', 'explainable-AI', 'cyber-physical-systems', 'adversarial-detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09162</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Group-robust Machine Unlearning</title><link>https://arxiv.org/abs/2503.09330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes group-robust machine unlearning: unlearning when the forget set is non-uniform and concentrated in particular groups (e.g., ethnicity, gender), which can cause fairness degradation.&lt;/li&gt;&lt;li&gt;Proposes an exact unlearning strategy using sample distribution reweighting to mitigate performance loss in dominant groups.&lt;/li&gt;&lt;li&gt;Presents MIU (Mutual Information-aware Machine Unlearning), an approximate unlearning method that minimizes mutual information between model features and group attributes while using distribution reweighting and calibration to preserve group robustness.&lt;/li&gt;&lt;li&gt;Empirical evaluation on three datasets shows MIU outperforms standard unlearning methods, achieving unlearning without compromising robustness for affected groups; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas De Min', 'Subhankar Roy', "St\\'ephane Lathuili\\`ere", 'Elisa Ricci', 'Massimiliano Mancini']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'fairness', 'robustness', 'mutual information']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09330</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Knowledge Distillation via Synthetic Text Generation</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DistilDP: a differentially private knowledge distillation method where a DP-trained teacher LLM generates synthetic text used to train a smaller student model.&lt;/li&gt;&lt;li&gt;Transfers knowledge via hard labels (synthetic data labels), soft labels (teacher output distributions on synthetic data), and optional hidden-representation alignment when architectures are similar.&lt;/li&gt;&lt;li&gt;Demonstrates improved utility over baselines on language modeling (e.g., ~9.0 PPL improvement on Big Patent) under strong privacy (ε = 2).&lt;/li&gt;&lt;li&gt;Targets privacy-preserving model compression for autoregressive LLMs and provides code for reproduction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Flemings', 'Murali Annavaram']&lt;/li&gt;&lt;li&gt;Tags: ['differential_privacy', 'knowledge_distillation', 'synthetic_data', 'privacy-preserving_ml', 'model_compression']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.00932</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>C-ing Clearly: Enhanced Binary Code Explanations using C code</title><link>https://arxiv.org/abs/2512.14500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces C-ing Clearly, a synthetic data generation method that leverages corresponding C source code to improve LLM understanding of assembly.&lt;/li&gt;&lt;li&gt;Fine-tunes LLMs on this generated data and shows improved performance on binary code summarization and vulnerability detection tasks.&lt;/li&gt;&lt;li&gt;Reports consistent gains across different LLM families and model sizes, indicating broad applicability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teodor Poncu', 'Ioana Pintilie', 'Marius Dragoi', 'Dragos Tantaru', 'Florin Brad']&lt;/li&gt;&lt;li&gt;Tags: ['binary analysis', 'vulnerability detection', 'assembly', 'LLM fine-tuning', 'synthetic data generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14500</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Transferable Defense Against Malicious Image Edits</title><link>https://arxiv.org/abs/2512.14341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TDAE, a bimodal (image+text) defense framework to make images immune to malicious diffusion-based edits via coordinated image-text adversarial optimization.&lt;/li&gt;&lt;li&gt;Visual defense: FlatGrad Defense Mechanism (FDM) adds gradient regularization to push perturbations toward flat minima, improving robustness and cross-model transferability.&lt;/li&gt;&lt;li&gt;Textual defense: Dynamic Prompt Defense (DPD) iteratively refines text embeddings and updates images so immunized images align editing outcomes with originals across diverse embeddings, encouraging broader immunity features.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art mitigation of malicious edits in both intra-model and cross-model evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'image editing', 'transferability', 'textual prompt defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14341</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Attention Guided Defense Against Malicious Edits</title><link>https://arxiv.org/abs/2512.14333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dual Attention-Guided Noise Perturbation (DANP), an imperceptible perturbation method to immunize images against malicious text-to-image edits by manipulating cross-attention maps and noise prediction across diffusion timesteps.&lt;/li&gt;&lt;li&gt;Uses dynamic thresholds to mask text-relevant vs irrelevant regions, reducing attention in relevant areas and increasing it in irrelevant ones to misguide edits while preserving intended targets.&lt;/li&gt;&lt;li&gt;Maximizes discrepancy between injected noise and model-predicted noise to further disrupt generation; reports state-of-the-art empirical robustness against malicious edits on diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbation', 'robustness/defense', 'text-to-image diffusion', 'attention manipulation', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14333</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title><link>https://arxiv.org/abs/2512.14320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings of existing immunization evaluation (which measure visual dissimilarity to a single reference output) and reframes image immunization as causing semantic mismatch with attacker prompts or producing perceptual degradation.&lt;/li&gt;&lt;li&gt;Proposes Synergistic Intermediate Feature Manipulation (SIFM): perturbing intermediate diffusion features with dual objectives — maximize divergence from original edit trajectory and minimize feature norms to induce degradations.&lt;/li&gt;&lt;li&gt;Introduces Immunization Success Rate (ISR), a metric that uses Multimodal LLMs to determine whether edits either semantically fail relative to a prompt or exhibit significant perceptual degradation; reports SIFM achieves state-of-the-art defense performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Dong', 'Jie Zhang', 'Guoying Zhao', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image immunization', 'adversarial perturbations', 'diffusion models', 'safety/defense', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14320</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis</title><link>https://arxiv.org/abs/2512.14045</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines how compiler function inlining (including aggressive/"extreme" inlining) alters static features used by ML-based binary analysis.&lt;/li&gt;&lt;li&gt;Systematically evaluates 20 ML models across five security-relevant binary analysis tasks to measure robustness under inlining-induced distribution shifts.&lt;/li&gt;&lt;li&gt;Shows inlining can be exploited to evade discriminative and generative ML models, and that subtle compiler settings can craft evasive binary variants.&lt;/li&gt;&lt;li&gt;Demonstrates large variability in inlining ratios across apps/configurations, undermining assumptions used in training and evaluation of ML security models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omar Abusabha', 'Jiyong Uhm', 'Tamer Abuhmed', 'Hyungjoon Koo']&lt;/li&gt;&lt;li&gt;Tags: ['ML robustness', 'adversarial evasion', 'binary analysis', 'compiler attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14045</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safe Online Control-Informed Learning</title><link>https://arxiv.org/abs/2512.13868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Safe Online Control-Informed Learning framework that integrates optimal control, parameter estimation (extended Kalman filter), and safety constraints for safety-critical autonomous systems.&lt;/li&gt;&lt;li&gt;Uses a softplus barrier function to enforce constraint satisfaction during online learning and control, removing dependence on high-quality initial guesses.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence and safety guarantees and demonstrates effectiveness on cart-pole and robot-arm benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyu Zhou', 'Zihao Liang', 'Zehui Lu', 'Shaoshuai Mou']&lt;/li&gt;&lt;li&gt;Tags: ['control safety', 'online learning', 'safety constraints', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13868</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries</title><link>https://arxiv.org/abs/2512.14388</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the first black-box privacy auditing framework for quantum machine learning (QML) using Lifted Quantum Differential Privacy and 'quantum canaries'—offset-encoded quantum states designed to detect memorization.&lt;/li&gt;&lt;li&gt;Derives a theoretical link between canary offset and trace distance bounds to produce empirical lower bounds on privacy-budget consumption, enabling practical verification of privacy leakage in trained QML models.&lt;/li&gt;&lt;li&gt;Validates the approach on both simulated and physical quantum hardware, demonstrating the framework can measure actual privacy loss and bridge the gap between theoretical QDP guarantees and deployed QML behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baobao Song', 'Shiva Raj Pokhrel', 'Athanasios V. Vasilakos', 'Tianqing Zhu', 'Gang Li']&lt;/li&gt;&lt;li&gt;Tags: ['Privacy auditing', 'Differential privacy', 'Quantum machine learning', 'Membership inference / memorization', 'Black-box testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14388</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix</title><link>https://arxiv.org/abs/2512.14188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reframes PGD as a projected gradient method and shows that using an accumulated-gradient adaptive matrix turns PGD into AdaGrad.&lt;/li&gt;&lt;li&gt;Proposes AdaMI, a novel momentum-based adaptive-matrix attack that optimizes perturbations with momentum and an adaptive scaling.&lt;/li&gt;&lt;li&gt;Provides theoretical proof of optimal convergence for convex problems, addressing non-convergence issues of MI-FGSM and improving optimization stability.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved adversarial transferability, stability, and imperceptibility across different networks compared to state-of-the-art attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Tao', 'Sheng Long', 'Xin Liu', 'Wei Li', 'Qing Tao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial attacks', 'optimization', 'transferability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14188</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On Improving Deep Active Learning with Formal Verification</title><link>https://arxiv.org/abs/2512.14170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates augmenting deep active learning (DAL) training sets with adversarial inputs that violate formal robustness constraints to improve labeling efficiency.&lt;/li&gt;&lt;li&gt;Finds adversarial examples produced via formal verification are more beneficial for DAL than standard gradient-based attacks.&lt;/li&gt;&lt;li&gt;Applies the approach to multiple existing DAL methods and a newly proposed DAL technique, showing improved generalization on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Spiegelman', 'Guy Amir', 'Guy Katz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness verification', 'data augmentation', 'active learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14170</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A First-Order Logic-Based Alternative to Reward Models in RLHF</title><link>https://arxiv.org/abs/2512.14100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a logic-similarity-based reward mechanism as an alternative to learned reward models for RLHF.&lt;/li&gt;&lt;li&gt;Introduces S-GRPO, a supervised variant of GRPO that jointly optimizes generation term, KL regularization, and a label-based objective to avoid model collapse.&lt;/li&gt;&lt;li&gt;Reports improved performance and robustness over standard supervised fine-tuning and extends preference-learning frameworks like GRPO and DPO.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunjin Jian', 'Xinhua Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'reward-model-alternative', 'robustness', 'training-methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14100</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization</title><link>https://arxiv.org/abs/2512.13880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving infant cry classification pipeline combining a denoising autoencoder, convolutional tokenizer, and Transformer encoder trained via communication-efficient federated learning with secure aggregation.&lt;/li&gt;&lt;li&gt;Uses 8-bit adapter deltas and a regularized control-variate update to reduce per-round client upload from ~36–42 MB to ~3.3 MB while maintaining performance.&lt;/li&gt;&lt;li&gt;Implements on-device denoising, adaptive segmentation, post-hoc calibration, and energy-based out-of-distribution abstention; reports high metrics (macro F1 0.938, AUC 0.962, ECE 0.032).&lt;/li&gt;&lt;li&gt;Demonstrates real-time edge inference (96 ms per 1 s spectrogram frame) on NVIDIA Jetson Nano and evaluates robustness with noise overlays on public infant-cry datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Geofrey Owino', 'Bernard Shibwabo']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving', 'secure-aggregation', 'ood-detection', 'audio-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13880</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains</title><link>https://arxiv.org/abs/2512.13852</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes integrating persistent homology (persistence images) with stability regularization into GIN-based GNNs to improve robustness to structural (edge) perturbations.&lt;/li&gt;&lt;li&gt;Builds on theoretical stability results for persistent homology and enforces Hiraoka-Kusano-inspired stability constraints during training.&lt;/li&gt;&lt;li&gt;Evaluates across six diverse graph datasets (biochemical, social, collaboration) and reports minimal performance degradation (0–4% on most datasets) under perturbation, outperforming baseline methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jelena Losic']&lt;/li&gt;&lt;li&gt;Tags: ['graph-robustness', 'topological-regularization', 'adversarial-robustness', 'GNN-security', 'persistent-homology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13852</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable reinforcement learning from human feedback to improve alignment</title><link>https://arxiv.org/abs/2512.13837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a post-hoc explanation method to identify training examples that cause a specific unsatisfactory prompt-response pair by solving a constrained combinatorial optimization in feature space (find closest set whose convex combination reconstructs the pair).&lt;/li&gt;&lt;li&gt;Provides an efficient iterative data selection algorithm to solve the explanation/attribution problem.&lt;/li&gt;&lt;li&gt;Proposes an unlearning procedure that removes influence of the identified training data to fix undesirable responses while aiming to preserve satisfactory behavior on other prompts.&lt;/li&gt;&lt;li&gt;Empirical results show the proposed pipeline can improve RLHF-tuned language model alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shicheng Liu', 'Siyuan Xu', 'Wenjie Qiu', 'Hangfan Zhang', 'Minghui Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'explainability', 'data attribution', 'unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13837</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</title><link>https://arxiv.org/abs/2512.13821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cross-Trace Verification Protocol (CTVP): verify untrusted code-generating models by comparing the model's predicted execution traces across semantically equivalent program transformations instead of executing code.&lt;/li&gt;&lt;li&gt;Detects backdoors and malicious behaviors via inconsistency patterns in predicted traces; introduces the Adversarial Robustness Quotient (ARQ) to quantify verification cost, showing exponential growth with orbit size.&lt;/li&gt;&lt;li&gt;Provides information-theoretic bounds arguing non-gamifiability (adversaries cannot evade detection through training due to space-complexity constraints) and claims scalability for AI control in code generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subramanyam Sahoo', 'Jared Junkin']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'code generation security', 'model verification', 'adversarial robustness', 'AI control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13821</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Constrained Policy Optimization via Sampling-Based Weight-Space Projection</title><link>https://arxiv.org/abs/2512.13788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SCPO, a sampling-based weight-space projection method that enforces rollout-based safety constraints directly in parameter space without needing constraint gradients.&lt;/li&gt;&lt;li&gt;Builds a local safe region using trajectory rollouts plus smoothness bounds, projects gradient updates via a convex SOCP, and provides a safe-by-induction guarantee (all intermediate policies remain safe if projections are feasible).&lt;/li&gt;&lt;li&gt;Demonstrates robustness to harmful supervision and a malicious expert in control/regression tasks, rejecting unsafe updates, maintaining feasibility throughout training, and improving the primal objective while preserving closed-loop stability when a stabilizing backup exists.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shengfan Cao', 'Francesco Borrelli']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'constrained policy optimization', 'robustness to malicious supervision', 'control safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13788</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title><link>https://arxiv.org/abs/2512.13741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Laminar Flow Hypothesis: benign prompts produce smooth latent trajectories in LLMs while jailbreak/adversarial prompts induce high-variance 'Semantic Turbulence'.&lt;/li&gt;&lt;li&gt;Proposes a zero-shot metric — variance of layer-wise cosine velocity — to detect jailbreaks in real time without external classifiers or lexical filters.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple small LLMs shows strong diagnostic signal (e.g., Qwen2-1.5B: +75.4% turbulence under attack; Gemma-2B: −22.0%, indicating different refusal mechanisms).&lt;/li&gt;&lt;li&gt;Claims the method can both detect jailbreaks and non-invasively categorize the underlying safety/alignment architecture of black-box models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Hasib Ur Rahman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak detection', 'latent-space analysis', 'safety evaluation', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13741</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Delete and Retain: Efficient Unlearning for Document Classification</title><link>https://arxiv.org/abs/2512.13711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hessian Reassignment, a two-step, model-agnostic class-level unlearning method for document classifiers: (1) an influence-style update that subtracts contributions of all training points from the target class by solving a Hessian-vector system via conjugate gradients, requiring only gradients and Hessian-vector products; (2) a decision-space guarantee enforcing Top-1 reclassification for deleted-class samples.&lt;/li&gt;&lt;li&gt;Achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster than full retraining.&lt;/li&gt;&lt;li&gt;Empirically reduces membership-inference advantage on the removed class using pooled multi-shadow attacks, indicating improved privacy protection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aadya Goel', 'Mayuri Sridhar']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'privacy', 'membership-inference', 'influence-functions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13711</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title><link>https://arxiv.org/abs/2512.12791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end Agent Assessment Framework with four evaluation pillars (LLMs, Memory, Tools, Environment) to evaluate agentic AI systems beyond binary task completion.&lt;/li&gt;&lt;li&gt;Highlights that non-determinism and runtime behavioral uncertainty (tool invocation, memory usage, agent collaboration, environment interaction) are missed by conventional metrics.&lt;/li&gt;&lt;li&gt;Validates the framework on an Autonomous CloudOps use case and shows it uncovers behavioral deviations and runtime uncertainties overlooked by standard evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sreemaee Akshathala', 'Bassam Adnan', 'Mahisha Ramesh', 'Karthik Vaidhyanathan', 'Basil Muhammed', 'Kannan Parthasarathy']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'agentic-systems', 'runtime-behavior', 'tools-and-memory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12791</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study showing LLM safety refusal decisions are unstable across random seeds and sampling temperature.&lt;/li&gt;&lt;li&gt;Tested 4 instruction-tuned models on 876 harmful prompts across 20 sampling configurations; 18–28% of prompts flipped (refuse in some configs, comply in others).&lt;/li&gt;&lt;li&gt;Introduces a Safety Stability Index (SSI), finds higher temperature reduces stability, and validates findings with an external judge (Claude 3.5 Haiku).&lt;/li&gt;&lt;li&gt;Concludes single-shot safety evaluations are insufficient (single-shot agrees with multi-sample ground truth ~92.4%) and recommends using at least 3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'evaluation/benchmarking', 'stochasticity', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Semantic-Drive is a local-first neuro-symbolic pipeline for mining long-tail, safety-critical events from AV video logs using open-vocabulary grounding (YOLOE) followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Introduces a 'Judge-Scout' multi-model consensus (System 2 inference-time alignment) to mitigate VLM hallucinations and improve reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes (mapped to WOD-E2E taxonomy), reports Recall 0.966 vs 0.475 for CLIP and a 40% reduction in Risk Assessment Error compared to best single scout models.&lt;/li&gt;&lt;li&gt;Designed to run on consumer GPU (RTX 3090), enabling privacy-preserving, on-premises operation rather than cloud-based VLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation / robustness', 'Neuro-symbolic VLM consensus', 'Open-vocabulary grounding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title><link>https://arxiv.org/abs/2512.08786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for aggregating group-level preference/reward signals in a federated RLHF setup to study trade-offs between alignment quality and fairness.&lt;/li&gt;&lt;li&gt;Compares standard aggregation rules (min, max, average) and proposes an adaptive weighting scheme that adjusts group preference weights based on historical alignment performance.&lt;/li&gt;&lt;li&gt;Empirical results using a PPO-based RLHF pipeline on QA tasks show the adaptive scheme improves fairness while keeping competitive alignment, with server-side aggregation that preserves raw-data locality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Srewa', 'Tianyu Zhao', 'Salma Elmalaki']&lt;/li&gt;&lt;li&gt;Tags: ['federated RLHF', 'alignment', 'fairness', 'preference aggregation', 'policy optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08786</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title><link>https://arxiv.org/abs/2512.07901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Games with Endogenous Players (GEPs) modeling lineages/strategic replicators that optimize, reproduce, and evolve under resource constraints.&lt;/li&gt;&lt;li&gt;Defines Evolutionarily Stable Distributions of Intelligence (ESDIs) and proves structural results (global Lyapunov functions under small-gain conditions) and closure under meta-selection (governance/innovation).&lt;/li&gt;&lt;li&gt;Proves an Alignment Impossibility Theorem: unrestricted self-modification undermines stability, implying stable alignment requires bounded modification classes and constitutional constraints.&lt;/li&gt;&lt;li&gt;Applies the framework to AI deployment dynamics, market concentration, institutional design, and explains failure modes like personality engineering under selection pressure.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Vallier']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'self-modification', 'multi-agent dynamics', 'governance/institutional design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07901</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title><link>https://arxiv.org/abs/2512.05442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IdealTSF, a framework that leverages 'non-ideal' negative samples (missing values, anomalies, noisy data) alongside ideal positive samples to improve time-series forecasting.&lt;/li&gt;&lt;li&gt;Three-stage pipeline: pretraining on negative samples to extract useful knowledge, training that transforms sequences into ideal positives, and a negative optimization step employing adversarial disturbances.&lt;/li&gt;&lt;li&gt;Claims that incorporating negative-sample knowledge plus adversarial optimization boosts performance of basic attention architectures, especially on noisy/low-quality datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Wang', 'Jinghao Lu', 'Fan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['time-series', 'robustness', 'adversarial-training', 'anomaly-handling', 'data-quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05442</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title><link>https://arxiv.org/abs/2511.10400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes reliability of LLM-based agents in multi-agent systems through the lens of Byzantine fault tolerance and compares them to traditional agents.&lt;/li&gt;&lt;li&gt;Proposes CP-WBFT, a confidence probe-based weighted BFT consensus mechanism leveraging LLMs' reflective/discriminative abilities to weight information flow.&lt;/li&gt;&lt;li&gt;Presents extensive experiments showing improved stability and accuracy under extreme Byzantine conditions (up to 85.7% fault rate) across topologies and tasks (mathematical reasoning, safety assessment).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lifan Zheng', 'Jiawei Chen', 'Qinghong Yin', 'Jingyuan Zhang', 'Xinyi Zeng', 'Yu Tian']&lt;/li&gt;&lt;li&gt;Tags: ['LLM-based agents', 'Byzantine fault tolerance', 'multi-agent robustness', 'consensus mechanisms', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.10400</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives</title><link>https://arxiv.org/abs/2511.08710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a theoretical framework modeling agents as single-layer transformers with linear self-attention (LSA) performing in-context gradient-descent-like updates on quadratic regression tasks.&lt;/li&gt;&lt;li&gt;Analyzes coupled dynamics when two agents alternately update under potentially misaligned fixed objectives, showing misalignment yields biased equilibria with predictable residual errors tied to objective gap and prompt geometry.&lt;/li&gt;&lt;li&gt;Shows an adaptive setting where a helper agent modifies its turn-based objective to implement a Newton-like step for the main agent, removing convergence plateaus and accelerating learning; empirical validation with trained LSA agents and black-box GPT-5-mini matches theory.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Romain Cosentino', 'Sarath Shekkizhar', 'Adam Earle']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'multi-agent-systems', 'robustness', 'in-context-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.08710</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title><link>https://arxiv.org/abs/2510.20768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes applying source-credibility algorithms (PageRank) to RAG retrieval corpora to reduce the impact of poisoning attacks in CTI LLM pipelines.&lt;/li&gt;&lt;li&gt;Demonstrates quantitatively on MS MARCO that the PageRank-based authority scoring downranks malicious documents while promoting trusted content.&lt;/li&gt;&lt;li&gt;Provides proof-of-concept experiments on real CTI documents and feeds, arguing this approach helps when threat information is novel and adversaries mimic legitimate formats.&lt;/li&gt;&lt;li&gt;Positions the method as a lightweight defense to accelerate robustness of existing RAG defenses for cyber threat intelligence use cases.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Austin Jia', 'Avaneesh Ramesh', 'Zain Shamsi', 'Daniel Zhang', 'Alex Liu']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'data_poisoning', 'source_credibility', 'CTI', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.20768</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title><link>https://arxiv.org/abs/2505.05638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic closed-loop evaluation of state-of-the-art motion prediction models when integrated with motion planners in autonomous driving stacks.&lt;/li&gt;&lt;li&gt;Finds that improved open-loop prediction accuracy does not necessarily translate to better closed-loop driving; temporal consistency and planner compatibility matter.&lt;/li&gt;&lt;li&gt;Shows that much smaller models (up to ~86% fewer parameters) can achieve comparable or superior closed-loop performance in some cases.&lt;/li&gt;&lt;li&gt;Provides released code for reproducing pred-to-plan evaluations (GitHub repository).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed-Khalil Bouzidi', 'Christian Schlauch', 'Nicole Scheuerer', 'Yue Yao', 'Nadja Klein', 'Daniel G\\"ohring', 'J\\"org Reichardt']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'closed-loop-evaluation', 'motion-prediction', 'autonomous-driving', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05638</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Group-robust Machine Unlearning</title><link>https://arxiv.org/abs/2503.09330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the problem of non-uniform (group-dominant) forget sets in machine unlearning, which can cause disproportionately degraded performance for specific demographic groups.&lt;/li&gt;&lt;li&gt;Proposes an exact unlearning strategy using sample distribution reweighting to mitigate group performance loss.&lt;/li&gt;&lt;li&gt;Introduces MIU (Mutual Information-aware Machine Unlearning) for approximate unlearning: minimizes mutual information between model features and group labels, plus reweighting and calibration with the original model to preserve group robustness.&lt;/li&gt;&lt;li&gt;Evaluates on three datasets and shows MIU outperforms standard unlearning methods, achieving unlearning while reducing fairness/performance degradation for dominant groups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas De Min', 'Subhankar Roy', "St\\'ephane Lathuili\\`ere", 'Elisa Ricci', 'Massimiliano Mancini']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'fairness', 'robustness', 'mutual information']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09330</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Holistic Utility Preference Learning for Listwise Alignment</title><link>https://arxiv.org/abs/2410.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Direct Ranking Preference Optimization (DRPO), framing human preference alignment as a listwise Learning-to-Rank task rather than pairwise comparisons.&lt;/li&gt;&lt;li&gt;Proposes diffNDCG, a differentiable approximation of NDCG implemented with a sorting network to enable end-to-end optimization of listwise ranking metrics.&lt;/li&gt;&lt;li&gt;Adds a margin-based Adaptive Rank Policy Score to improve discriminative quality of generated responses.&lt;/li&gt;&lt;li&gt;Empirical results claim DRPO outperforms existing pairwise methods (e.g., DPO) in aligning model outputs with human preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacong Zhou', 'Xianyun Wang', 'Min Zhang', 'Jun Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'RLHF', 'learning-to-rank', 'differentiable-sorting', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18127</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title><link>https://arxiv.org/abs/2312.04960</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial robustness of Vision Transformers (ViTs) and shows incompatibilities of existing AT methods with ViT architectures.&lt;/li&gt;&lt;li&gt;Derives mutual information (MI) bounds for ViT autoencoder-based self-supervised pretraining to constrain MI between adversarial examples and latent representations.&lt;/li&gt;&lt;li&gt;Proposes MIMIR: a self-supervised adversarial training method using an MI penalty with masked image modeling to improve robust and natural accuracy.&lt;/li&gt;&lt;li&gt;Evaluates on CIFAR-10, Tiny-ImageNet, and ImageNet-1K, showing SOTA robustness results, resistance to common corruptions, unforeseen attacks, and adaptive (white-box) attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Xu', 'Shujian Yu', 'Zhuoran Liu', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'vision transformers', 'mutual information', 'self-supervised learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.04960</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title><link>https://arxiv.org/abs/2512.13142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of five leading LLMs using 627 demographically diverse personas and the validated Individual Level Abortion Stigma Scale (ILAS) to test multilevel representations of abortion stigma (cognitive, interpersonal, structural, overall).&lt;/li&gt;&lt;li&gt;Findings: models fail to coherently represent stigma across levels — overestimating interpersonal stigma, underestimating cognitive stigma, assuming uniform community condemnation, introducing demographic biases absent in human data, missing validated stigma–secrecy relationships, and producing internal contradictions.&lt;/li&gt;&lt;li&gt;Implications: current alignment focuses on surface behavior and language but does not ensure coherent multilevel understanding in high-stakes domains; calls for new design criteria (multilevel coherence), continuous auditing, governance/accountability, and deployment restrictions in sensitive contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anika Sharma', 'Malavika Mampally', 'Chidaksh Ravuru', 'Kandyce Brennan', 'Neil Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'bias and fairness', 'auditing', 'sensitive-domain NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13142</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title><link>https://arxiv.org/abs/2510.14112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STEMS, a safety-constrained multi-agent reinforcement learning framework for coordinated building energy management.&lt;/li&gt;&lt;li&gt;Uses a GCN-Transformer fusion to capture spatial-temporal relationships among buildings and a multi-agent RL algorithm augmented with Control Barrier Functions to enforce mathematical safety constraints.&lt;/li&gt;&lt;li&gt;Reports empirical improvements on real-world datasets: reductions in cost (21%), emissions (18%), and safety violations (from 35.1% to 5.6%) while maintaining occupant comfort and robustness under extreme weather.&lt;/li&gt;&lt;li&gt;Focuses on practical deployment concerns: cross-building generalization, robustness, and provable operational safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huiliang Zhang', 'Di Wu', 'Arnaud Zinflou', 'Benoit Boulet']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'multi-agent RL', 'control barrier functions', 'spatial-temporal representation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14112</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Inverse Scaling in Test-Time Compute</title><link>https://arxiv.org/abs/2507.14417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructs evaluation tasks showing that increasing test-time reasoning length can decrease model accuracy (inverse scaling) across several task categories.&lt;/li&gt;&lt;li&gt;Identifies five failure modes (distractibility, overfitting to framing, shift to spurious correlations, difficulty maintaining complex deductive chains, and amplification of concerning behaviors like self-preservation).&lt;/li&gt;&lt;li&gt;Evaluates multiple large reasoning models (e.g., Claude family, OpenAI o-series) and emphasizes that longer reasoning can exacerbate safety-relevant behaviors.&lt;/li&gt;&lt;li&gt;Argues for evaluating models across diverse reasoning lengths to detect and mitigate these safety/robustness failure modes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryo Pradipta Gema', 'Alexander H\\"agele', 'Runjin Chen', 'Andy Arditi', 'Jacob Goldman-Wetzler', 'Kit Fraser-Taliente', 'Henry Sleight', 'Linda Petrini', 'Julian Michael', 'Beatrice Alex', 'Pasquale Minervini', 'Yanda Chen', 'Joe Benton', 'Ethan Perez']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Robustness', 'LLM failure modes', 'Alignment', 'Red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14417</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title><link>https://arxiv.org/abs/2512.14601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FakeRadar, a deepfake video detection framework that generates synthetic outlier forgeries near cluster boundaries to simulate unseen manipulation types.&lt;/li&gt;&lt;li&gt;Uses large-scale pretrained models (e.g., CLIP) to probe feature space and model dynamic subclusters of known real/fake data.&lt;/li&gt;&lt;li&gt;Introduces Outlier-Guided Tri-Training with outlier-driven contrastive learning and outlier-conditioned cross-entropy to train a detector to separate real, known fake, and outlier samples.&lt;/li&gt;&lt;li&gt;Reports improved cross-domain generalization on benchmark deepfake video datasets, particularly against emerging/unknown manipulation techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaolun Li', 'Jichang Li', 'Yinqi Cai', 'Junye Chen', 'Xiaonan Luo', 'Guanbin Li', 'Rushi Lan']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'robustness', 'outlier detection', 'cross-domain generalization', 'forgery detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14601</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</title><link>https://arxiv.org/abs/2512.14448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reasoning-Style Poisoning (RSP): attacks that manipulate an LLM agent's reasoning style (e.g., 'analysis paralysis' or 'cognitive haste') by rewriting retrieved documents without changing facts or using explicit triggers.&lt;/li&gt;&lt;li&gt;Introduces Generative Style Injection (GSI) as the attack method and the Reasoning Style Vector (RSV) metric (Verification depth, Self-confidence, Attention focus) to quantify style shifts.&lt;/li&gt;&lt;li&gt;Empirical results show GSI degrades performance on HotpotQA and FEVER across ReAct, Reflection, and Tree of Thoughts, increasing reasoning steps up to 4.4x or causing premature errors while bypassing content filters.&lt;/li&gt;&lt;li&gt;Proposes RSP-M, a lightweight runtime monitor that computes RSV in real time and triggers alerts when thresholds are exceeded as a process-level defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingfu Zhou', 'Pengfei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'LLM security', 'runtime monitoring', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14448</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Transferable Defense Against Malicious Image Edits</title><link>https://arxiv.org/abs/2512.14341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TDAE, a bimodal (image+text) defense framework to make images immune to malicious diffusion-based edits via coordinated image-text adversarial optimization.&lt;/li&gt;&lt;li&gt;Visual defense: FlatGrad Defense Mechanism (FDM) adds gradient regularization to push perturbations toward flat minima, improving robustness and cross-model transferability.&lt;/li&gt;&lt;li&gt;Textual defense: Dynamic Prompt Defense (DPD) iteratively refines text embeddings and updates images so immunized images align editing outcomes with originals across diverse embeddings, encouraging broader immunity features.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art mitigation of malicious edits in both intra-model and cross-model evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'image editing', 'transferability', 'textual prompt defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14341</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Dual Attention Guided Defense Against Malicious Edits</title><link>https://arxiv.org/abs/2512.14333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dual Attention-Guided Noise Perturbation (DANP), an imperceptible perturbation method to immunize images against malicious text-to-image edits by manipulating cross-attention maps and noise prediction across diffusion timesteps.&lt;/li&gt;&lt;li&gt;Uses dynamic thresholds to mask text-relevant vs irrelevant regions, reducing attention in relevant areas and increasing it in irrelevant ones to misguide edits while preserving intended targets.&lt;/li&gt;&lt;li&gt;Maximizes discrepancy between injected noise and model-predicted noise to further disrupt generation; reports state-of-the-art empirical robustness against malicious edits on diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Shuai Dong', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial perturbation', 'robustness/defense', 'text-to-image diffusion', 'attention manipulation', 'model safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14333</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title><link>https://arxiv.org/abs/2512.14320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings of existing immunization evaluation (which measure visual dissimilarity to a single reference output) and reframes image immunization as causing semantic mismatch with attacker prompts or producing perceptual degradation.&lt;/li&gt;&lt;li&gt;Proposes Synergistic Intermediate Feature Manipulation (SIFM): perturbing intermediate diffusion features with dual objectives — maximize divergence from original edit trajectory and minimize feature norms to induce degradations.&lt;/li&gt;&lt;li&gt;Introduces Immunization Success Rate (ISR), a metric that uses Multimodal LLMs to determine whether edits either semantically fail relative to a prompt or exhibit significant perceptual degradation; reports SIFM achieves state-of-the-art defense performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Dong', 'Jie Zhang', 'Guoying Zhao', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['image immunization', 'adversarial perturbations', 'diffusion models', 'safety/defense', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14320</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</title><link>https://arxiv.org/abs/2512.14233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PentestEval, a comprehensive benchmark that decomposes penetration testing into six stages (e.g., information collection, weakness gathering, exploit generation) and evaluates LLMs at each stage across 346 tasks in 12 realistic vulnerable scenarios.&lt;/li&gt;&lt;li&gt;Provides expert-annotated ground truth and an automated evaluation pipeline to measure stage-level and end-to-end performance of 9 popular LLMs and several LLM-powered systems/agents (e.g., PentestGPT, PentestAgent, VulnBot).&lt;/li&gt;&lt;li&gt;Finds generally weak LLM performance (end-to-end ~31% success) and near-failure for autonomous agents, arguing that modular, structured reasoning improves stage-level results and is needed for reliable autonomous penetration testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruozhao Yang', 'Mingfei Cheng', 'Gelei Deng', 'Tianwei Zhang', 'Junjie Wang', 'Xiaofei Xie']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'penetration testing', 'benchmarking', 'security/red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14233</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol</title><link>https://arxiv.org/abs/2512.14166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defines a new privacy threat called Intent Inversion where semi-honest MCP servers infer user intent from tool call logs in decoupled agent architectures.&lt;/li&gt;&lt;li&gt;Presents IntentMiner, a framework using hierarchical information isolation and three-dimensional semantic analysis (tool purpose, call statements, returned results) to infer step-level user intent.&lt;/li&gt;&lt;li&gt;Empirical results show high semantic alignment (&gt;85%) with original queries and outperform baselines, highlighting privacy risks of MCP-mediated tool execution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhao Yao', 'Zhiqiang Wang', 'Haoran Cheng', 'Yihang Cheng', 'Haohua Du', 'Xiang-Yang Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'intent inversion', 'LLM agents', 'tool call analysis', 'Model Context Protocol (MCP)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14166</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title><link>https://arxiv.org/abs/2512.14044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;OmniDrive-R1 is an end-to-end vision-language model framework for autonomous driving that interleaves perception and multimodal chain-of-thought (iMCoT) to reduce object hallucination and improve reasoning reliability.&lt;/li&gt;&lt;li&gt;Introduces a reinforcement-driven visual grounding mechanism that lets the model direct attention/zoom to critical regions using a two-stage RL pipeline and the Clip-GRPO algorithm.&lt;/li&gt;&lt;li&gt;Clip-GRPO uses an annotation-free, process-based grounding reward enforcing real-time cross-modal consistency, removing the need for dense localization labels and external tool calls.&lt;/li&gt;&lt;li&gt;Shows large empirical gains on DriveLMM-o1 versus Qwen2.5VL-7B (reasoning score 51.77% → 80.35%; final answer accuracy 37.81% → 73.62%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenguo Zhang', 'Haohan Zhen', 'Yishen Wang', 'Le Xu', 'Tianchen Deng', 'Xuefeng Chen', 'Qu Chen', 'Bo Zhang', 'Wuxiong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['vision-language models', 'safety/robustness', 'visual grounding', 'chain-of-thought', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14044</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization</title><link>https://arxiv.org/abs/2512.13880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving infant cry classification pipeline combining a denoising autoencoder, convolutional tokenizer, and Transformer encoder trained via communication-efficient federated learning with secure aggregation.&lt;/li&gt;&lt;li&gt;Uses 8-bit adapter deltas and a regularized control-variate update to reduce per-round client upload from ~36–42 MB to ~3.3 MB while maintaining performance.&lt;/li&gt;&lt;li&gt;Implements on-device denoising, adaptive segmentation, post-hoc calibration, and energy-based out-of-distribution abstention; reports high metrics (macro F1 0.938, AUC 0.962, ECE 0.032).&lt;/li&gt;&lt;li&gt;Demonstrates real-time edge inference (96 ms per 1 s spectrogram frame) on NVIDIA Jetson Nano and evaluates robustness with noise overlays on public infant-cry datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Geofrey Owino', 'Bernard Shibwabo']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'privacy-preserving', 'secure-aggregation', 'ood-detection', 'audio-classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13880</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes</title><link>https://arxiv.org/abs/2512.13744</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys robustness of state-of-the-art audio deepfake detectors using pretrained speech encoders (WavLM, Wav2Vec2, MMS) under realistic noise, reverberation, and consumer channels.&lt;/li&gt;&lt;li&gt;Proposes a reproducible SNR-based benchmark mixing MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate performance across SNRs from 35 dB to -5 dB.&lt;/li&gt;&lt;li&gt;Evaluates multi-condition training and fine-tuning; shows fine-tuning reduces EER by 10–15 percentage points at 10–0 dB SNR across backbones and reports accuracy, ROC-AUC, and EER for binary and four-class tasks.&lt;/li&gt;&lt;li&gt;Provides practical recipes for improving robustness of audio deepfake detectors in realistic capture conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Udayon Sen', 'Alka Luqman', 'Anupam Chattopadhyay']&lt;/li&gt;&lt;li&gt;Tags: ['audio deepfake detection', 'robustness', 'SNR benchmark', 'pretrained speech encoders', 'multi-condition training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13744</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title><link>https://arxiv.org/abs/2512.13742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a vision-to-language framework (DL^3M) linking endoscopic image classification (MobileCoAtNet) to LLM-driven clinical reasoning.&lt;/li&gt;&lt;li&gt;Introduces two expert-verified benchmarks for clinical reasoning (causes, symptoms, treatment, lifestyle, follow-up) and evaluates 32 LLMs against them.&lt;/li&gt;&lt;li&gt;Finds that stronger image classification improves explanations but LLMs show instability and fail to reach human-level reliability; reasoning changes with prompt variation.&lt;/li&gt;&lt;li&gt;Provides code and datasets, and frames limitations relevant to building safer, more reliable multimodal medical reasoning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Najib Hasan (Wichita State University', 'USA)', 'Imran Ahmad (Wichita State University', 'USA)', 'Sourav Basak Shuvo (Khulna University of Engineering and Technology', 'Bangladesh)', 'Md. Mahadi Hasan Ankon (Khulna University of Engineering and Technology', 'Bangladesh)', 'Sunanda Das (University of Arkansas', 'USA)', 'Nazmul Siddique (Ulster University', 'UK)', "Hui Wang (Queen's University Belfast", 'UK)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'safety evaluation', 'robustness', 'multimodal medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13742</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title><link>https://arxiv.org/abs/2512.13741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Laminar Flow Hypothesis: benign prompts produce smooth latent trajectories in LLMs while jailbreak/adversarial prompts induce high-variance 'Semantic Turbulence'.&lt;/li&gt;&lt;li&gt;Proposes a zero-shot metric — variance of layer-wise cosine velocity — to detect jailbreaks in real time without external classifiers or lexical filters.&lt;/li&gt;&lt;li&gt;Empirical evaluation on multiple small LLMs shows strong diagnostic signal (e.g., Qwen2-1.5B: +75.4% turbulence under attack; Gemma-2B: −22.0%, indicating different refusal mechanisms).&lt;/li&gt;&lt;li&gt;Claims the method can both detect jailbreaks and non-invasively categorize the underlying safety/alignment architecture of black-box models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Hasib Ur Rahman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak detection', 'latent-space analysis', 'safety evaluation', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13741</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title><link>https://arxiv.org/abs/2512.13739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes AIGC-assisted image production for journalism, focusing on misinformation, authenticity, semantic fidelity, and interpretability.&lt;/li&gt;&lt;li&gt;Experiment 1: cross-platform prompt standardization across three scenes, revealing semantic misalignment, cultural specificity issues, and training/filtering biases.&lt;/li&gt;&lt;li&gt;Experiment 2: develops a human-in-the-loop pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), style control (Style-LoRA, Prompt-to-Prompt), CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials to ensure editorial fidelity.&lt;/li&gt;&lt;li&gt;Proposes a human-AI collaboration mechanism and new evaluation axes: Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yajie Yang', 'Yuqing Zhao', 'Xiaochao Xi', 'Yinan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC safety', 'content authenticity', 'human-in-the-loop', 'semantic alignment', 'cultural bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13739</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models</title><link>https://arxiv.org/abs/2512.13703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Safe2Harm, a four-stage semantic isomorphism jailbreaking method that rewrites harmful prompts into semantically similar 'safe' prompts, maps thematic correspondences, generates responses to the safe prompts, then reverses the mapping to produce harmful outputs.&lt;/li&gt;&lt;li&gt;Evaluates Safe2Harm on 7 mainstream LLMs and three benchmark datasets, reporting stronger jailbreak success rates than existing methods.&lt;/li&gt;&lt;li&gt;Provides a new evaluation dataset of 358 challenging harmful-content samples and assesses existing harmful-content detection methods to support defenses (input-output filtering).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'prompt rewriting', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13703</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</title><link>https://arxiv.org/abs/2512.14474</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Model-First Reasoning (MFR): a two-phase approach where an LLM first constructs an explicit problem model (entities, state variables, actions, constraints) and then generates a solution plan.&lt;/li&gt;&lt;li&gt;Empirically evaluates MFR across multiple planning domains (medical scheduling, route planning, resource allocation, logic puzzles, procedural synthesis) and shows reduced constraint violations and fewer hallucinations compared to Chain-of-Thought and ReAct; ablations show the modeling phase is critical.&lt;/li&gt;&lt;li&gt;Argues many LLM planning failures arise from representational deficiencies rather than raw reasoning limits, promoting explicit modeling for more robust, interpretable agents; provides prompts, evaluation procedures, and datasets for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Annu Rana', 'Gaurav Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'LLM agents', 'robustness', 'alignment', 'planning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.14474</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning</title><link>https://arxiv.org/abs/2512.13955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MURIM, a multi-dimensional reputation-based incentive mechanism for federated learning that accounts for client reliability, privacy, resource capacity, and fairness.&lt;/li&gt;&lt;li&gt;Includes a reliability verification module and allocates incentives based on contribution, latency, and reputation to deter malicious/unreliable clients.&lt;/li&gt;&lt;li&gt;Evaluates on MNIST, FMNIST, and ADULT Income showing improved fairness (up to 18%), reduced privacy attack success (5–9%), and much higher robustness to poisoning/noisy-gradient attacks (up to 85%).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sindhuja Madabushi', 'Dawood Wasif', 'Jin-Hee Cho']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'incentive-mechanism', 'robustness', 'poisoning-attacks', 'privacy-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13955</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</title><link>https://arxiv.org/abs/2512.13771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Semantic Grounding Index (SGI): ratio of angular distances (response→question vs response→context) on the unit hypersphere to detect when RAG outputs ignore retrieved context.&lt;/li&gt;&lt;li&gt;Theoretically derives geometric bounds (spherical triangle inequality) predicting SGI discriminative power grows with question-context angular separation; empirically validated on HaluEval (n=5,000) across five embedding models with large effect sizes and high cross-model correlation.&lt;/li&gt;&lt;li&gt;Finds 'semantic laziness'—hallucinated responses stay angularly closer to questions than contexts; SGI performs especially well for long responses and short questions and is reasonably calibrated (ECE=0.10).&lt;/li&gt;&lt;li&gt;Shows a negative result on TruthfulQA (AUC=0.478), indicating SGI measures topical engagement rather than factual accuracy; positioned as a computationally efficient signal to flag RAG responses for verification in production.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Javier Mar\\'in"]&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination detection', 'embedding geometry', 'safety evaluation', 'alignment/verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13771</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title><link>https://arxiv.org/abs/2512.13762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a qualitative, single-session (86-turn) case study showing models exhibit Normal Performance (NP) in benign domains but Functional Refusal (FR) in provider- or policy-sensitive domains.&lt;/li&gt;&lt;li&gt;Introduces the concept of Learned Incapacity (LI) to describe selective withholding of capabilities without attributing intentionality, and operationalizes three response regimes (NP, FR, Meta-Narrative).&lt;/li&gt;&lt;li&gt;Proposes an interaction-level auditing framework to detect policy-linked behavioral selectivity and highlights potential alignment side effects warranting broader evaluation across users and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['TK Lee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model behavior', 'safety evaluation', 'refusal/jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13762</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making</title><link>https://arxiv.org/abs/2512.13716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ValuePilot, a two-phase framework: a dataset generation toolkit (DGT) that creates value-annotated scenarios via a human–LLM pipeline, and a decision-making module (DMM) that learns to choose actions based on individual value preferences.&lt;/li&gt;&lt;li&gt;Argues value-driven decision-making yields interpretable, generalizable, personalized behavior across novel contexts compared to task/reward-driven paradigms.&lt;/li&gt;&lt;li&gt;Reports DMM outperforms several strong LLM baselines (GPT-5, Claude-Sonnet-4, Gemini-2-flash, Llama-3.1-70b) on aligning with human action choices in unseen scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Luo', 'Ziang Chen', 'Hou Hei Lam', 'Jiayu zhan', 'Junqi Wang', 'Zhenliang Zhang', 'Xue Feng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'value alignment', 'personalization', 'interpretability', 'dataset/toolkit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13716</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach</title><link>https://arxiv.org/abs/2512.13714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an AI-powered annotation pipeline that combines automated weak supervision and confidence-based annotation with human validation to detect and correct LLM instability patterns.&lt;/li&gt;&lt;li&gt;Defines stability-specific annotation categories (semantic consistency, factual correctness, logical coherence) to drive continuous feedback loops for model calibration and robustness improvements.&lt;/li&gt;&lt;li&gt;Positions the pipeline as a scalable complement/alternative to RLHF and supervised fine-tuning to reduce hallucinations, inconsistent reasoning, and performance variability in regulated workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gangesh Pathak', 'Prasanna Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'annotation_pipeline', 'model_stabilization', 'human-AI-synergy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13714</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>
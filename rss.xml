<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 17 Dec 2025 00:24:13 +0000</lastBuildDate><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LookAhead Tuning, a data-driven fine-tuning method that previews partial answer prefixes in training data to reduce perturbation of the model's initial token distributions.&lt;/li&gt;&lt;li&gt;Aims to preserve pre-existing safety/alignment behaviors of LLMs during downstream fine-tuning by maintaining built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Claims to be lightweight and effective, maintaining model safety while retaining strong downstream task performance per reported experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safe-fine-tuning', 'alignment-preservation', 'LLM-safety', 'training-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</title><link>https://arxiv.org/abs/2511.17362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ATAC (Augmentation-based Test-time Adversarial Correction), a lightweight test-time defense for CLIP that operates in the model's embedding space.&lt;/li&gt;&lt;li&gt;Computes augmentation-induced drift vectors to infer a semantic recovery direction and corrects embeddings via angular consistency of these latent drifts.&lt;/li&gt;&lt;li&gt;Reports large robustness gains across benchmarks (≈50% improvement over prior SOTA on average) with minimal compute overhead and resilience in extreme/adaptive attack settings.&lt;/li&gt;&lt;li&gt;Presents a novel paradigm for embedding-space test-time defenses specifically targeted at adversarial perturbations on image inputs for image–text matching.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linxiang Su', "Andr\\'as Balogh"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-robustness', 'test-time-defense', 'CLIP', 'embedding-space', 'adaptive-attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.17362</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title><link>https://arxiv.org/abs/2511.16020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a sequence-level optimization framework to generate printable adversarial textures for clothing that remain effective across entire walking video sequences under motion, pose, deformation, and lighting changes.&lt;/li&gt;&lt;li&gt;Maps product images to UV space and parameterizes textures via a compact palette and control points with ICC color locking to ensure printability; uses a physically based human-garment simulator to model cloth dynamics and multi-angle viewpoints.&lt;/li&gt;&lt;li&gt;Optimizes an expectation-over-transformation objective with temporal weighting to minimize detector confidence across sequences; demonstrates strong digital and real-world effectiveness with sublimation-printed garments and cross-model transferability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingkun Zhou', 'Patrick P. K. Chan', 'Hengxu Wu', 'Shikang Zheng', 'Ruiqi Huang', 'Yuanjie Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical attacks', 'human detection', 'robustness', 'video/sequence-level']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16020</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counting Hallucinations in Diffusion Models</title><link>https://arxiv.org/abs/2510.13080</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CountHalluSet, a dataset suite (ToyShape, SimObject, RealHand) and protocol to quantify 'counting hallucinations' in diffusion probabilistic models (DPMs).&lt;/li&gt;&lt;li&gt;Systematically measures how sampling conditions (solver type, ODE order, sampling steps, initial noise) affect the frequency of incorrect object counts (e.g., extra fingers).&lt;/li&gt;&lt;li&gt;Shows common image-quality metrics like FID do not reliably capture counting hallucinations, highlighting a gap in current evaluation practices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Fu', 'Jian Zhou', 'Qi Chen', 'Huang Jing', 'Huy Anh Nguyen', 'Xiaohan Liu', 'Zhixiong Zeng', 'Lin Ma', 'Quanshi Zhang', 'Qi Wu']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'diffusion models', 'evaluation/benchmarking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13080</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title><link>https://arxiv.org/abs/2503.08884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SpurLens, an automated pipeline using GPT-4 and open-set object detectors to find spurious visual cues in multimodal LLMs without human labeling.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: over-reliance on spurious cues (removing cues reduces accuracy) and amplified object hallucination (spurious cues increase hallucination &gt;10x).&lt;/li&gt;&lt;li&gt;Validates results across multiple MLLMs and datasets, performs ablations to probe root causes, and evaluates mitigations such as prompt ensembling and reasoning-based prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parsa Hosseini', 'Sumit Nawathe', 'Mazda Moayeri', 'Sriram Balasubramanian', 'Soheil Feizi']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM robustness', 'spurious correlations', 'hallucination detection', 'safety evaluation', 'mitigation strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08884</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature</title><link>https://arxiv.org/abs/2106.05261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two methods to detect adversarial patch attacks on object detectors: a fast signature-based detector that locates known patch signatures, and a signature-independent detector based on 'content semantics consistency' that finds objects that appear locally but disappear globally.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in both digital and physical-world settings; signature-based method enables real-time detection, while signature-independent method generalizes to unknown or defense-aware patch attacks.&lt;/li&gt;&lt;li&gt;Shows that signature-independent detection makes crafting successful defense-aware adversarial patches much harder by exploiting global consistency violations in model outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiachun Li', 'Jianan Feng', 'Jianjun Huang', 'Bin Liang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'object detection', 'adversarial example detection', 'physical-world attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2106.05261</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title><link>https://arxiv.org/abs/2512.13641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructed MangoLeafDB-C by applying 19 types of artificial image corruptions at five severity levels to the MangoLeafDB dataset for robustness evaluation.&lt;/li&gt;&lt;li&gt;Benchmarked five CNNs (ResNet-50, ResNet-101, VGG-16, Xception, and a lightweight LCNN) using F1, corruption error (CE), and relative mean corruption error (relative mCE).&lt;/li&gt;&lt;li&gt;Found the lightweight LCNN achieved lower mCE and was more robust to real-world corruptions (e.g., Defocus Blur, Motion Blur) than larger architectures, while ResNet-101 showed large degradation under corruption despite high clean accuracy.&lt;/li&gt;&lt;li&gt;Concludes that specialized lightweight models may be better for robust edge deployment in agriculture and highlights the need to include robustness assessments in development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Vitorino de Andrade', 'Saulo Roberto dos Santos', 'Itallo Patrick Castro Alves da Silva', 'Emanuel Adler Medeiros Pereira', 'Erick de Andrade Barboza']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'corruption-robustness', 'benchmarking', 'agricultural-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13641</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</title><link>https://arxiv.org/abs/2512.13262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRBO, a reinforcement-learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization to improve safety in interactive driving.&lt;/li&gt;&lt;li&gt;Introduces Warm-K, a warm-started Top-K sampling strategy for test-time scaling to balance consistency and diversity in motion selection without retraining, mitigating covariate shift.&lt;/li&gt;&lt;li&gt;Reports &gt;40% safety improvement using only 10% of training data while preserving behavioral realism, and emphasizes closed-loop evaluation to capture compounding errors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunki Seong', 'Jeong-Kyun Lee', 'Heesoo Myeong', 'Yongho Shin', 'Hyun-Mook Cho', 'Duck Hoon Kim', 'Pranav Desai', 'Monu Surana']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'behavioral safety', 'reinforcement learning fine-tuning', 'test-time adaptation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13262</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title><link>https://arxiv.org/abs/2512.12827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GradID, a detector that uses the intrinsic dimensionality (ID) of model input gradients to distinguish natural from adversarial examples.&lt;/li&gt;&lt;li&gt;Shows consistent ID differences between clean and adversarial inputs and applies this for both batch-wise group detection and single-sample detection.&lt;/li&gt;&lt;li&gt;Evaluates across multiple datasets (MNIST, SVHN, CIFAR-10, MS COCO) and attacks (including CW and AutoAttack), reporting high detection rates (≳92% on CIFAR-10).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Mahdi Razmjoo', 'Mohammad Mahdi Sharifian', 'Saeed Bagheri Shouraki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-detection', 'gradient-based', 'intrinsic-dimensionality', 'adversarial-attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12827</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering</title><link>https://arxiv.org/abs/2512.12694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a multilingual Retrieval-Augmented Generation (RAG) pipeline for question answering on noisy historical newspaper collections, addressing OCR corruption, orthographic variation, and temporal drift.&lt;/li&gt;&lt;li&gt;Key components: semantic query expansion with multi-query fusion (Reciprocal Rank Fusion), engineered generation prompts enforcing grounding and explicit abstention when evidence is insufficient, and a modular architecture enabling component-level evaluation.&lt;/li&gt;&lt;li&gt;Includes ablation studies on named entity extraction and embedding model choices, showing trade-offs between syntactic coherence, retrieval recall, and efficiency; reports improved recall stability and faithful answer generation with correct abstention behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anthony Mudet', 'Souhail Bakkali']&lt;/li&gt;&lt;li&gt;Tags: ['Retrieval-Augmented Generation', 'Robustness', 'Faithfulness and Abstention', 'Multilingual QA', 'Information Retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12694</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title><link>https://arxiv.org/abs/2512.11883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that over-alignment of image generation models to conventional aesthetic preferences causes them to ignore or alter user requests for low-quality or negative/anti-aesthetic outputs.&lt;/li&gt;&lt;li&gt;Constructs a wide-spectrum aesthetics dataset and evaluates state-of-the-art generative and reward models, finding reward models penalize anti-aesthetic images even when they match explicit prompts.&lt;/li&gt;&lt;li&gt;Validates systemic bias via image-to-image editing and tests on real abstract artworks, arguing this leads to developer-centered value imposition, reduced user autonomy, and forms of ideological 'censorship'.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqi Marshall Guo', 'Qingyun Qian', 'Khalad Hasan', 'Shan Du']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'aesthetic bias', 'image generation', 'reward modeling', 'censorship']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11883</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title><link>https://arxiv.org/abs/2512.11867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the effects of repeatedly training models on synthetically generated data (bootstrapping) in continual learning settings.&lt;/li&gt;&lt;li&gt;Presents a statistical analysis showing synthetic data introduces significant bias and variance that weaken maximum likelihood estimation.&lt;/li&gt;&lt;li&gt;Provides empirical evidence that popular generative models degrade/collapse under repeated training with synthetic data and that Generative Experience Replay (GER) methods fail to maintain latent alignment.&lt;/li&gt;&lt;li&gt;Quantifies degradation and raises concerns about using synthetic data for continual learning and related workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniil Zverev', 'A. Sophia Koepke', 'Joao F. Henriques']&lt;/li&gt;&lt;li&gt;Tags: ['continual-learning', 'synthetic-data', 'generative-model-collapse', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11867</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights</title><link>https://arxiv.org/abs/2512.11802</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a field dataset of Tesla Traffic Light and Stop Sign Control (TLSSC) experiments with synchronized high-resolution vehicle trajectories and driver-perspective video across varied speed limits and TCD types.&lt;/li&gt;&lt;li&gt;Defines a taxonomy of TLSSC-TCD interaction behaviors (stopping, accelerating, car following) and calibrates the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode, identifying a car-following threshold of ~90 m.&lt;/li&gt;&lt;li&gt;Reports empirical behavioral insights (e.g., stopping driven by responsiveness to desired speed deviation and relative speed; accelerating is more conservative; intersection car-following shows smoother dynamics and tighter headways) and releases the dataset for future simulation and safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Li', 'Peng Zhang', 'Shixiao Liang', 'Hang Zhou', 'Chengyuan Ma', 'Handong Yao', 'Qianwen Li', 'Xiaopeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['ADAS safety', 'safety evaluation', 'dataset', 'behavioral analysis', 'autonomous driving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11802</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</title><link>https://arxiv.org/abs/2512.13665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Grab-3D, a geometry-aware transformer that detects AI-generated videos by modeling 3D geometric temporal consistency using vanishing points.&lt;/li&gt;&lt;li&gt;Introduces geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier to explicitly inject 3D geometry into temporal modeling.&lt;/li&gt;&lt;li&gt;Constructs a dataset of AI-generated static-scene videos to enable stable 3D geometric feature extraction and evaluates cross-domain generalization to unseen generators.&lt;/li&gt;&lt;li&gt;Reports significant improvement over state-of-the-art detectors, emphasizing geometric inconsistencies as a robust cue for synthetic-video detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Chen', 'Sezer Karaoglu', 'Theo Gevers']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'synthetic media detection', 'video forensics', 'geometric consistency', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13665</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title><link>https://arxiv.org/abs/2512.13427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MineTheGap, a genetic-algorithm method that automatically mines prompts which cause text-to-image (TTI) models to produce biased outputs.&lt;/li&gt;&lt;li&gt;Defines a novel bias score by comparing the distribution of generated images to distributions of LLM-generated textual variations for a prompt, and uses this score to guide prompt optimization.&lt;/li&gt;&lt;li&gt;Validates the approach on a dataset with known biases, demonstrating its ability to reveal demographic/occupational and diversity-related biases; code and examples are provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noa Cohen', 'Nurit Spingarn-Eliezer', 'Inbar Huberman-Spiegelglas', 'Tomer Michaeli']&lt;/li&gt;&lt;li&gt;Tags: ['bias-detection', 'text-to-image', 'safety-evaluation', 'red-teaming', 'prompt-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13427</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Generate Cross-Task Unexploitable Examples</title><link>https://arxiv.org/abs/2512.13416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCT-UEG, a meta training/testing framework to generate 'unexploitable' (unlearnable) versions of personal images that remain unexploitable across multiple real-world computer vision tasks.&lt;/li&gt;&lt;li&gt;Introduces a flat-minima-oriented meta optimization scheme to improve generalization of the generator so produced examples transfer across tasks.&lt;/li&gt;&lt;li&gt;Aims to protect user privacy by preventing unauthorized exploitation/training on uploaded personal images; extensive experiments report efficacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoxuan Qu', 'Qiuchi Xiang', 'Yujun Cai', 'Yirui Wu', 'Majid Mirmehdi', 'Hossein Rahmani', 'Jun Liu']&lt;/li&gt;&lt;li&gt;Tags: ['unlearnable-examples', 'data-poisoning', 'privacy-preservation', 'robustness', 'meta-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13416</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title><link>https://arxiv.org/abs/2512.13290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Causal Scene Graph (CSG) and Physical Alignment Probe (PAP) dataset to diagnose causal/physical failures in image and video diffusion models.&lt;/li&gt;&lt;li&gt;Finds that diffusion models struggle with multi-hop causal reasoning, that prompt embeddings disentangle texture vs. physics, and that visual causal structure is formed early in denoising.&lt;/li&gt;&lt;li&gt;Proposes LINA, which learns prompt-specific interventions via targeted guidance in prompt and visual latent spaces and a causality-aware denoising schedule to improve physical alignment and OOD instruction following.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on challenging causal generation tasks and Winoground for image/video DMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yu', 'Chaochao Lu']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-models', 'causal-reasoning', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13290</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</title><link>https://arxiv.org/abs/2512.13285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CausalCLIP, a method that disentangles causal (forensic) features from non-causal/spurious features in CLIP-derived representations to improve generated-image detection.&lt;/li&gt;&lt;li&gt;Uses a structural causal model plus Gumbel-Softmax feature masking and HSIC independence constraints to enforce statistical independence and filter for stable, transferable cues.&lt;/li&gt;&lt;li&gt;Demonstrates improved generalization to unseen generative models, reporting substantial gains in accuracy and average precision over prior state-of-the-art detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Liu', 'Qiao Qin', 'Qinghui He']&lt;/li&gt;&lt;li&gt;Tags: ['generated image detection', 'causal feature disentanglement', 'robustness/generalization', 'forensics', 'CLIP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13285</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title><link>https://arxiv.org/abs/2512.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video Reality Test, an ASMR-sourced benchmark focused on tightly coupled audio-video content to evaluate realism of AI-generated videos.&lt;/li&gt;&lt;li&gt;Uses an adversarial creator-reviewer protocol where generative models attempt to fool reviewers and VLMs/human experts attempt to detect fakes.&lt;/li&gt;&lt;li&gt;Finds state-of-the-art generators can substantially fool VLMs (e.g., Gemini 2.5-Pro ~56% accuracy) while humans perform better (~81%), and shows audio helps but superficial cues (e.g., watermarks) can mislead models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Wang', 'Weijia Wu', 'Yi Zhan', 'Rui Zhao', 'Ming Hu', 'James Cheng', 'Wei Liu', 'Philip Torr', 'Kevin Qinghong Lin']&lt;/li&gt;&lt;li&gt;Tags: ['AIGC detection', 'VLM evaluation', 'Red teaming', 'Audio-visual consistency', 'Benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13281</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title><link>https://arxiv.org/abs/2512.13144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Weight Space Correlation Analysis to quantify whether a classification head uses features encoded in embeddings by measuring alignment between primary task and auxiliary metadata task weight vectors.&lt;/li&gt;&lt;li&gt;Validates the method by detecting artificially induced shortcut learning and applies it to an ultrasound model for spontaneous preterm birth (sPTB) prediction.&lt;/li&gt;&lt;li&gt;Finds that while embeddings contain metadata (e.g., scanner), the sPTB classifier's weights align with clinically relevant factors (e.g., birth weight) and are decoupled from acquisition factors, providing an interpretable trustworthiness check.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chun Kit Wong', 'Paraskevas Pegios', 'Nina Weng', 'Emilie Pi Fogtmann Sejer', 'Martin Gr{\\o}nneb{\\ae}k Tolsgaard', 'Anders Nymark Christensen', 'Aasa Feragen']&lt;/li&gt;&lt;li&gt;Tags: ['shortcut learning', 'feature utilization', 'model interpretability', 'safety/robustness', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13144</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title><link>https://arxiv.org/abs/2512.13107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffFusion, a framework to improve multi-modal 3D object detection robustness in adverse weather using diffusion-based image restoration and point-cloud compensation.&lt;/li&gt;&lt;li&gt;Introduces Diffusion-IR for restoring degraded images and Point Cloud Restoration (PCR) that leverages image cues to correct corrupted LiDAR data.&lt;/li&gt;&lt;li&gt;Presents Bidirectional Adaptive Fusion and Alignment Module (BAFAM) for dynamic cross-modal fusion and bidirectional BEV alignment to handle modality misalignment.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness on three public datasets and zero-shot generalization to a real-world DENSE dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhijian He', 'Feifei Liu', 'Yuwei Li', 'Zhanpeng Liu', 'Jintao Cheng', 'Xieyuanli Chen', 'Xiaoyu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sensor-fusion', 'diffusion-models', 'autonomous-driving', 'perception-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13107</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</title><link>https://arxiv.org/abs/2512.13039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bi-Erasing, a bidirectional framework for concept erasure in diffusion-based text-to-image models that jointly suppresses harmful concepts and enhances safe alternatives.&lt;/li&gt;&lt;li&gt;Uses joint text-image representations and two decoupled image branches: a negative branch for suppressing target semantics and a positive branch for providing visual guidance toward safe content.&lt;/li&gt;&lt;li&gt;Applies mask-based filtering to image branches to avoid interference from irrelevant content during erasure.&lt;/li&gt;&lt;li&gt;Reports improved trade-offs between concept removal effectiveness and visual fidelity compared to baseline methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Chen', 'Yiwei Wang', 'Songze Li']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'diffusion models', 'image safety', 'content moderation', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13039</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title><link>https://arxiv.org/abs/2512.12997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a miscalibration issue in CLIP under adversarial perturbations: attacks reduce accuracy but also suppress predictive uncertainty, causing overconfidence.&lt;/li&gt;&lt;li&gt;Proposes an adversarial fine-tuning objective that reparameterizes CLIP outputs as the concentration parameter of a Dirichlet distribution to capture both semantic structure and confidence magnitude.&lt;/li&gt;&lt;li&gt;Aligns predictive distributions between clean and adversarial examples holistically (beyond single-logit matching) to restore calibrated uncertainty while retaining competitive adversarial robustness and clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjing lu', 'Zerui Tao', 'Dongping Zhang', 'Yuning Qiu', 'Yang Yang', 'Qibin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'uncertainty calibration', 'CLIP', 'adversarial fine-tuning', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12997</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes</title><link>https://arxiv.org/abs/2512.12982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a 'Benefit then Conflict' dilemma where AIGI detector performance first improves then degrades as training-source diversity increases, due to data-level heterogeneity and a model-level bottleneck.&lt;/li&gt;&lt;li&gt;Diagnoses two failure modes: overlapping feature distributions between real and synthetic images (data heterogeneity) and limited adaptability of fixed pretrained encoders (model bottleneck).&lt;/li&gt;&lt;li&gt;Proposes Generator-Aware Prototype Learning (GAPL): learns compact canonical forgery prototypes to reduce feature variance and uses a two-stage training scheme with Low-Rank Adaptation (LoRA) to improve discriminative power while retaining pretrained knowledge.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art detection generalization across many GAN and diffusion generators; code released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziheng Qin', 'Yuheng Ji', 'Renshuai Tao', 'Yuxuan Tian', 'Yuyang Liu', 'Yipu Wang', 'Xiaolong Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'deepfake detection', 'robustness/generalization', 'prototype learning', 'model adaptation (LoRA)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12982</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</title><link>https://arxiv.org/abs/2512.12586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StegaVAR, a framework that embeds action (secret) videos into ordinary cover videos via steganography and performs video action recognition (VAR) directly in the steganographic domain.&lt;/li&gt;&lt;li&gt;Introduces Secret Spatio-Temporal Promotion (STeP) to guide spatiotemporal feature extraction using the secret video during training and Cross-Band Difference Attention (CroDA) to suppress cover interference by capturing cross-band semantic differences.&lt;/li&gt;&lt;li&gt;Claims improved concealment (natural-looking cover videos during transmission) while preserving spatiotemporal features for accurate VAR, and demonstrates effectiveness across datasets and multiple steganographic models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lixin Chen', 'Chaomeng Chen', 'Jiale Zhou', 'Zhijian Wu', 'Xun Lin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving ML', 'steganography / data hiding', 'video action recognition', 'privacy defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12586</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</title><link>https://arxiv.org/abs/2512.12492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptiveDetector, a two-stage YOLOv11 detector with a vision-language model (VLM) verifier that adaptively adjusts per-frame confidence thresholds under VLM guidance.&lt;/li&gt;&lt;li&gt;Fine-tunes the verifier with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward to strongly discourage missed detections (false negatives).&lt;/li&gt;&lt;li&gt;Builds a synthetic degraded testbed to simulate adverse clinical imaging conditions for zero-shot evaluation on CVC-ClinicDB and Kvasir-SEG, reporting 14–22 percentage point recall improvements with comparable precision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shengkai Xu', 'Hsiang Lun Kao', 'Tianxiang Xu', 'Honghui Zhang', 'Junqiao Wang', 'Runmeng Ding', 'Guanyu Liu', 'Tianyu Shi', 'Zhenyu Yu', 'Guofeng Pan', 'Ziqian Bi', 'Yuqi Ouyang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'medical AI', 'safety-critical', 'vision-language verification', 'zero-shot evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12492</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title><link>https://arxiv.org/abs/2512.12410</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Graph Attention Network (GAT) that models each LiDAR sweep as an unstructured spatial graph to regress missing vertical (elevation) values at dropout beam locations using only a single LiDAR frame.&lt;/li&gt;&lt;li&gt;Trained and evaluated on 1,065 KITTI sequences with simulated channel dropout, achieving average height RMSE of 11.67 cm and 87.98% of points within a 10 cm error threshold.&lt;/li&gt;&lt;li&gt;Operates without camera or temporal information; runtime is ~14.65 seconds per frame on a single GPU and reconstruction quality is stable across neighborhood sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Khalfalla Awedat', 'Mohamed Abidalrekab', 'Mohammad El-Yabroudi']&lt;/li&gt;&lt;li&gt;Tags: ['LiDAR', 'Robustness', 'Autonomous vehicles', 'Graph Attention Network', 'Sensor fault recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12410</guid><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.12302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Intention-Drive, a benchmark and dataset for mapping high-level natural-language human intentions to end-to-end driving actions.&lt;/li&gt;&lt;li&gt;Proposes an Intent Success Rate (ISR) metric to evaluate semantic fulfillment of human goals (beyond geometric accuracy) and focuses on safety- and intent-aware evaluation.&lt;/li&gt;&lt;li&gt;Evaluates a range of baseline models on the benchmark, finding substantial gaps and highlighting challenges in scene and intent understanding for safe autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Zheng', 'Yucheng Zhou', 'Tianyi Yan', 'Jiayi Su', 'Hongjun Chen', 'Dubing Chen', 'Wencheng Han', 'Runzhou Tao', 'Zhongying Qiu', 'Jianfei Yang', 'Jianbing Shen']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'safety evaluation', 'benchmark', 'intent understanding', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12302</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPDMark: Selective Parameter Displacement for Robust Video Watermarking</title><link>https://arxiv.org/abs/2512.12090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPDMark, an in-generation video watermarking method that embeds watermarks by selectively displacing a subset of diffusion model parameters via layer-wise basis shifts implemented with LoRA.&lt;/li&gt;&lt;li&gt;Jointly trains basis shifts and a watermark extractor with losses for message recovery, perceptual similarity, and temporal consistency; frame-specific messages are derived via cryptographic hashing and extraction uses maximum bipartite matching to handle temporal tampering.&lt;/li&gt;&lt;li&gt;Evaluated on text-to-video and image-to-video generation models, demonstrating imperceptible watermarks with high recovery accuracy and robustness against common video modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samar Fares', 'Nurbek Tastan', 'Karthik Nandakumar']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'provenance', 'robustness', 'video deepfakes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12090</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering</title><link>https://arxiv.org/abs/2512.12089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LVLM hallucinations correlate with diffuse final visual-attention maps, while vision encoder attention maps are more concentrated and reduce hallucination.&lt;/li&gt;&lt;li&gt;Finds vision-text conflicts peak in the LLM middle layers and demonstrates injecting vision encoder attention into these layers suppresses hallucinations.&lt;/li&gt;&lt;li&gt;Proposes VEGAS, an inference-time method that integrates vision-encoder attention into mid-layers and adaptively steers tokens, showing state-of-the-art reduction of hallucinations across benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihu Wang', 'Boxun Xu', 'Yuxuan Xia', 'Peng Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'alignment', 'inference-time defense', 'attention-guided steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12089</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Neuro-symbolic two-stage pipeline: open-vocabulary detector (YOLOE) for symbolic grounding followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Inference-time 'System 2' Judge-Scout multi-model consensus mechanism to mitigate VLM hallucinations and improve decision reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes vs WOD taxonomy: high recall (0.966 vs 0.475 for CLIP) and 40% reduction in Risk Assessment Error; runs locally on consumer GPU for privacy-preserving operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation', 'Privacy-preserving VLMs', 'Neuro-symbolic perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models</title><link>https://arxiv.org/abs/2512.11899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Read-or-Ignore VQA (RIO-VQA), a task requiring LVLMs to decide contextually whether to read text in an image or ignore it.&lt;/li&gt;&lt;li&gt;Provides RIO-Bench, a standardized dataset and evaluation protocol with same-scene counterfactuals (read/ignore) by varying only textual content and question type.&lt;/li&gt;&lt;li&gt;Shows that strong LVLMs and existing text-ignoring defenses fail to balance typographic robustness with text-reading capability.&lt;/li&gt;&lt;li&gt;Proposes a data-driven, adaptive defense that learns selective text use, moving beyond prior non-adaptive, text-ignoring approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Futa Waseda', 'Shojiro Yamabe', 'Daiki Shiono', 'Kento Sasaki', 'Tsubasa Takahashi']&lt;/li&gt;&lt;li&gt;Tags: ['typographic attacks', 'adversarial robustness', 'vision-language models', 'benchmark', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11899</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title><link>https://arxiv.org/abs/2512.11865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainable adversarial-robust Vision-Language-Action (VLA) model for robotic manipulation tasks in smart farming.&lt;/li&gt;&lt;li&gt;Introduces an Evidence-3 module that detects photometric perturbations (hue, illumination, noise) and generates natural-language explanations of causes and effects.&lt;/li&gt;&lt;li&gt;Demonstrates improved action prediction under adversarial photometric conditions: Current Action L1 loss reduced by 21.7% and Next Actions L1 loss reduced by 18.4% versus baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ju-Young Kim', 'Ji-Hong Park', 'Myeongjun Kim', 'Gun-Woo Kim']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'robotic manipulation', 'vision-language-action', 'explainability', 'photometric attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11865</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title><link>https://arxiv.org/abs/2512.10449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies adversarial PDF manipulation (indirect prompt injection) targeting LLM-based scientific reviewers to flip "Reject" decisions to "Accept".&lt;/li&gt;&lt;li&gt;Introduces a new metric (WAVS) to quantify vulnerability and curates a dataset of 200 papers with 15 domain-specific attack strategies.&lt;/li&gt;&lt;li&gt;Evaluates attacks across 13 large language models (including GPT-5, Claude Haiku, DeepSeek) and finds obfuscation strategies can cause high decision-flip rates; promises dataset and framework release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Manish Prasad', 'Vasudev Majhi', 'Jahnvi Singh', 'Vinay Chamola', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10449</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title><link>https://arxiv.org/abs/2510.17904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BreakFun, a jailbreak that leverages LLMs' strict adherence to structured schemas by embedding a 'Trojan Schema' within prompts to force harmful outputs.&lt;/li&gt;&lt;li&gt;Reports high transferability and effectiveness (average 89% success across 13 models; 100% ASR on some models) and an ablation study attributing causality to the Trojan Schema.&lt;/li&gt;&lt;li&gt;Proposes a defensive technique, Adversarial Prompt Deconstruction, using a secondary LLM to perform Literal Transcription to reveal hidden harmful intent; proof-of-concept shows strong mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirkia Rafiei Oskooei', 'Mehmet S. Aktas']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'red teaming', 'defenses/guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17904</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title><link>https://arxiv.org/abs/2510.14925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes a stability-based measure (H-Risk) in linear-Gaussian state-space models linking nominal stability to overconfident errors and degraded closed-loop behavior.&lt;/li&gt;&lt;li&gt;Proposes a domain-wise proxy for LLM stability based on confidence fluctuations and demonstrates that allowing 'cannot judge' responses reduces policy-aware squared loss in high-stakes binary tasks.&lt;/li&gt;&lt;li&gt;Finds that confidently wrong LLM outputs are locally stable (no instability gap) and interprets hallucinations as stable, high-confidence attractors resistant to output-only fixes like temperature scaling or resampling.&lt;/li&gt;&lt;li&gt;Argues for process-level interventions that perturb and re-evaluate model inference trajectories, motivated by spectral and activation analyses (e.g., Qwen-2.5 exhibiting high SNR, low effective signal temperature).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akira Okutomi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM calibration', 'hallucination', 'robustness', 'safety/alignment', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14925</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems</title><link>https://arxiv.org/abs/2507.23453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes 'blind attacks' where adversarial candidate answers are crafted independently of the true answer to fool LLM-based evaluators (a form of prompt injection).&lt;/li&gt;&lt;li&gt;Proposes augmenting Standard Evaluation (SE) with Counterfactual Evaluation (CFE): re-evaluate submissions against a deliberately false ground-truth and flag answers that validate under both real and counterfactual conditions.&lt;/li&gt;&lt;li&gt;Empirical results show SE is highly vulnerable to such attacks, while SE+CFE substantially improves attack detection with minimal impact on normal evaluation performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijia Liu', 'Takumi Kondo', 'Kyohei Atarashi', 'Koh Takeuchi', 'Jiyi Li', 'Shigeru Saito', 'Hisashi Kashima']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'LLM evaluation', 'security defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23453</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title><link>https://arxiv.org/abs/2505.14479</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neuro-symbolic pipeline combining retrieval of analogous proofs and a formal verifier to guide and correct LLM-generated mathematical proofs (geometry focus).&lt;/li&gt;&lt;li&gt;Uses retrieved analogous problem proofs to condition generation and a verifier to provide feedback for iterative correction.&lt;/li&gt;&lt;li&gt;Reports substantial accuracy gains for OpenAI's o1 model (58%–70% improvement) with both retrieval and verifier contributing.&lt;/li&gt;&lt;li&gt;Argues that LLMs producing provably correct conclusions can improve reliability and trustworthiness for critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oren Sultan', 'Eitan Stern', 'Dafna Shahaf']&lt;/li&gt;&lt;li&gt;Tags: ['neuro-symbolic', 'formal verification', 'LLM reliability', 'proof generation', 'safety-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14479</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title><link>https://arxiv.org/abs/2503.08884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SpurLens, an automated pipeline using GPT-4 and open-set object detectors to find spurious visual cues in multimodal LLMs without human labeling.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: over-reliance on spurious cues (removing cues reduces accuracy) and amplified object hallucination (spurious cues increase hallucination &gt;10x).&lt;/li&gt;&lt;li&gt;Validates results across multiple MLLMs and datasets, performs ablations to probe root causes, and evaluates mitigations such as prompt ensembling and reasoning-based prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parsa Hosseini', 'Sumit Nawathe', 'Mazda Moayeri', 'Sriram Balasubramanian', 'Soheil Feizi']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM robustness', 'spurious correlations', 'hallucination detection', 'safety evaluation', 'mitigation strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08884</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmarks: On The False Promise of AI Regulation</title><link>https://arxiv.org/abs/2501.15693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI safety benchmarks do not reflect real-world deployed model performance and thus are insufficient grounds for regulation.&lt;/li&gt;&lt;li&gt;Identifies lack of interpretability as a core reason why benchmark performance fails to predict deployed risks and harms.&lt;/li&gt;&lt;li&gt;Proposes a regulatory framework that does not rely on benchmarks and calls for interdisciplinary collaboration to address deployment safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Stanovsky', 'Renana Keydar', 'Gadi Perl', 'Eliya Habba']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Regulation', 'Benchmarks', 'Interpretability', 'Policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15693</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>How Far Are We from Genuinely Useful Deep Research Agents?</title><link>https://arxiv.org/abs/2512.01948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents FINDER, a benchmark of 100 human-curated research tasks with 419 structured checklist items to standardize report structure, analytical depth, and factual grounding.&lt;/li&gt;&lt;li&gt;Proposes DEFT, a failure taxonomy of 14 fine-grained failure modes across reasoning, retrieval, and generation, created via human-LLM co-annotation and validated for inter-annotator reliability.&lt;/li&gt;&lt;li&gt;Evaluates ~1,000 reports from mainstream Deep Research Agents (DRAs) and finds primary weaknesses in evidence integration, verification, and reasoning-resilient planning rather than task comprehension.&lt;/li&gt;&lt;li&gt;Provides resources (benchmark + taxonomy) aimed at diagnosing and guiding improvements in agent reliability and report-quality assurance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingling Zhang', 'He Zhu', 'Jincheng Ren', 'Kangqi Song', 'Xinran Zhou', 'Boyu Feng', 'Shudong Liu', 'Jiabin Luo', 'Weihao Xie', 'Zhaohui Wang', 'Tianrui Qin', 'King Zhu', 'Yuqing Wang', 'Qianben Chen', 'Yuchen Eleanor Jiang', 'Wei Wang', 'Jiaheng Liu', 'Wangchunshu Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'LLM evaluation', 'Agent reliability', 'Safety evaluation', 'Failure taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01948</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title><link>https://arxiv.org/abs/2510.21118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an annotation framework with an intermediate 'Out-Dependent' category to reduce ambiguity in faithfulness labels when external knowledge is required for verification.&lt;/li&gt;&lt;li&gt;Builds VeriGray, a new benchmark for unfaithfulness detection in summarization, and reports that SOTA LLMs (e.g., GPT-5) still produce ~6% hallucinated sentences and ~8% Out-Dependent sentences on average.&lt;/li&gt;&lt;li&gt;Evaluates baseline methods on VeriGray and finds the benchmark challenging, highlighting gaps in current faithfulness detection approaches and room for improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Ding', 'Lvzhou Luo', 'Yixuan Cao', 'Ping Luo']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination detection', 'benchmark', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21118</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</title><link>https://arxiv.org/abs/2510.02967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Retrieval-Augmented Generation (RAG) system to query UK NICE clinical guidelines, using a hybrid embedding retrieval architecture over 10,195 text chunks.&lt;/li&gt;&lt;li&gt;Retrieval performance: MRR=0.814, Recall@1=81%, Recall@10=99.1% on 7,901 queries; RAG substantially improved generation faithfulness on a 70 Q/A test set.&lt;/li&gt;&lt;li&gt;RAG-enhanced models increased faithfulness to 99.5% (O4-Mini) versus 43% for a medical-focused baseline; clinical SME evaluation (7 experts) showed GPT-4.1 reached 98.7% accuracy and reduced unsafe responses by 67%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Lewis', 'Samuel Thio', 'Amy Roberts', 'Catherine Siju', 'Whoasif Mukit', 'Rebecca Kuruvilla', 'Zhangshu Joshua Jiang', 'Niko M\\"oller-Grell', 'Aditya Borakati', 'Richard JB Dobson', 'Spiros Denaxas']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Faithfulness', 'Safety evaluation', 'Clinical LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02967</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.24253</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MRAG-Suite, a diagnostic evaluation platform aggregating multimodal Visual RAG benchmarks to systematically evaluate Visual RAG systems.&lt;/li&gt;&lt;li&gt;Proposes difficulty-based and ambiguity-aware filtering to stratify queries and reveal performance variability under harder/ambiguous inputs.&lt;/li&gt;&lt;li&gt;Presents MM-RAGChecker, a claim-level diagnostic tool for detecting hallucinations and verifying retrieved evidence against generated claims.&lt;/li&gt;&lt;li&gt;Finds substantial accuracy drops and prevalent hallucinations on difficult and ambiguous queries, highlighting robustness and safety gaps in Visual RAG systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuelyu Ji']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'hallucination-detection', 'benchmarking', 'multimodal-rag']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24253</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE</title><link>https://arxiv.org/abs/2509.24130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formally defines 'textual sharpness' in the discrete semantic prompt space and proposes an operational robustness criterion over semantic neighborhoods.&lt;/li&gt;&lt;li&gt;Introduces TARE, a black-box, derivative-free framework that alternates between sampling-based adversarial paraphrase search and robust candidate selection to minimize sharpness.&lt;/li&gt;&lt;li&gt;Proposes ATARE, which learns anisotropic neighborhood weights and adapts neighborhood radius to balance exploration and fidelity.&lt;/li&gt;&lt;li&gt;Shows empirically that minimizing textual sharpness yields prompts that maintain accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining API-friendly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guancheng Wan', 'Lucheng Fu', 'Haoxin Liu', 'Yiqiao Jin', 'Hui Yi Leong', 'Eric Hanchen Jiang', 'Hejia Geng', 'Jinhe Bi', 'Yunpu Ma', 'Xiangru Tang', 'B. Aditya Prakash', 'Yizhou Sun', 'Wei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'prompt robustness', 'black-box prompt optimization', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24130</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</title><link>https://arxiv.org/abs/2509.23188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CRAS (Contextualized Role Adherence Score), a query-wise metric that decomposes role adherence into four measurable dimensions to diagnose instruction-hierarchy violations in LLM multi-agent systems.&lt;/li&gt;&lt;li&gt;Performs attention drift analysis to localize the model components (middle-layer attention heads) that drive misprioritization under instruction conflicts.&lt;/li&gt;&lt;li&gt;Presents SAIL (Surgical Alignment of Instruction Layers): apply LoRA to localized focal layers and train with a token-weighted DPO-style objective, improving instruction-hierarchy compliance without full-model fine-tuning (e.g., +5.60% on MedQA with AutoGen).&lt;/li&gt;&lt;li&gt;Argues for micro-level evaluation and targeted interventions to improve reliability and alignment in deployed multi-agent LLM systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guancheng Wan', 'Leixin Sun', 'Longxu Dou', 'Zitong Shi', 'Fang Wu', 'Eric Hanchen Jiang', 'Wenke Huang', 'Guibin Zhang', 'Hejia Geng', 'Xiangru Tang', 'Zhenfei Yin', 'Yizhou Sun', 'Wei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'LLM-safety', 'multi-agent-systems', 'instruction-following', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23188</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</title><link>https://arxiv.org/abs/2509.03888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic re-examination of probing-based methods for detecting malicious/harmful inputs to LLMs, showing they fail to generalize OOD.&lt;/li&gt;&lt;li&gt;Finds probes learn superficial cues (instructional patterns and trigger words) rather than semantic harmfulness; simple n-gram methods perform comparably.&lt;/li&gt;&lt;li&gt;Controlled experiments with semantically cleaned datasets and pattern-dependence analyses confirm the hypothesis and reveal false sense of security around current probes.&lt;/li&gt;&lt;li&gt;Provides discussion on redesigning models and evaluation protocols and releases code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Wang', 'Zeming Wei', 'Qin Liu', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking detection', 'prompt injection', 'safety evaluation', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03888</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ConceptGuard: Neuro-Symbolic Safety Guardrails via Sparse Interpretable Jailbreak Concepts</title><link>https://arxiv.org/abs/2508.16325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ConceptGuard, a framework using Sparse Autoencoders to extract interpretable latent concepts from LLM internal activations that correlate with jailbreak themes.&lt;/li&gt;&lt;li&gt;Uses these semantically meaningful internal representations to build explainable, neuro-symbolic safety guardrails that aim to block jailbreak behavior without further fine-tuning or degrading capabilities.&lt;/li&gt;&lt;li&gt;Provides empirical/analytical evidence for a shared activation geometry of jailbreak attacks, suggesting generalizable defenses grounded in mechanistic interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', "C\\'eline Hudelot"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'interpretability', 'defenses', 'mechanistic interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16325</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System</title><link>https://arxiv.org/abs/2508.15811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-stage pipeline for generative query suggestion: prompt engineering, supervised fine-tuning via distillation from click logs, Gaussian Reward Model (GaRM) capturing preference uncertainty, and reinforcement learning with a composite reward.&lt;/li&gt;&lt;li&gt;Introduces GaRM to model user preferences as probability distributions rather than point estimates and integrates auxiliary heuristics to mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Adds out-of-distribution regularization and a two-stage reward fusion technique to stabilize training and reduce misalignment.&lt;/li&gt;&lt;li&gt;Reports improved automatic/human evaluation metrics and a 34% relative increase in click-through rate in live A/B testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhao Yin', 'Haolin Wang', 'Peng Bao', 'Ju Xu', 'Yongliang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'reinforcement-learning', 'reward-hacking', 'query-suggestion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15811</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ALIGN: Word Association Learning for Cultural Alignment in Large Language Models</title><link>https://arxiv.org/abs/2508.13426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes fine-tuning LLMs on native speakers' word-association norms to impart cultural knowledge and improve cultural alignment.&lt;/li&gt;&lt;li&gt;Fine-tunes Llama-3.1-8B and Qwen-2.5-7B (supervised and preference optimization) using English (US) and Mandarin (China) word-association datasets.&lt;/li&gt;&lt;li&gt;Reports substantial gains in lexical alignment (Precision@5) and measurable shifts in high-level cultural value responses, with small 7–8B models matching or exceeding vanilla 70B baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunhua Liu', 'Kabir Manandhar Shrestha', 'Sukai Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'cultural alignment', 'LLMs', 'fine-tuning', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13426</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title><link>https://arxiv.org/abs/2508.11009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SproutBench, a benchmark of 1,283 developmentally grounded adversarial prompts targeting safety risks for ages 0–18 (emotional dependency, privacy leaks, imitation of hazardous behaviors, etc.).&lt;/li&gt;&lt;li&gt;Evaluates 47 diverse LLMs with the suite, revealing substantive safety vulnerabilities and empirical correlations (e.g., Safety vs. Risk Prevention, inverse Interactivity vs. Age Appropriateness).&lt;/li&gt;&lt;li&gt;Provides practical guidelines for child-centric AI design, deployment, and mitigation strategies based on benchmark findings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Lanyi Wei', 'Haixiao Hu', 'Jingyi Yu', 'Rongchang Li', 'Mohan Li', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'child-safety', 'benchmark', 'adversarial-prompts', 'LLM-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11009</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings</title><link>https://arxiv.org/abs/2507.17234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CLARIFID, a framework for radiology report generation that explicitly optimizes clinical correctness by modeling the Findings→Impression workflow.&lt;/li&gt;&lt;li&gt;Section-aware pretraining to learn logical flow, followed by PPO fine-tuning using CheXbert F1 on the Impression as the reward to encourage diagnostically accurate conclusions.&lt;/li&gt;&lt;li&gt;Controlled decoding: enforces completion of the Findings section before synthesizing the Impression (next-token forcing + report-level re-ranking) to preserve coherent clinical reasoning.&lt;/li&gt;&lt;li&gt;Uses a vision-transformer-based multi-view encoder to fuse multiple chest X-ray views; shows improved clinical efficacy on MIMIC-CXR versus baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyeongkyu Lee', 'Seonghwan Yoon', 'Hongki Lim']&lt;/li&gt;&lt;li&gt;Tags: ['clinical factuality / safety', 'RL fine-tuning (PPO)', 'controlled decoding / generation constraints', 'medical multimodal report generation', 'factuality evaluation (CheXbert)']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17234</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Encode Harmfulness and Refusal Separately</title><link>https://arxiv.org/abs/2507.11878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies distinct latent directions for 'harmfulness' and 'refusal' in LLMs and shows they encode separate concepts.&lt;/li&gt;&lt;li&gt;Provides causal evidence: steering the harmfulness direction changes the model's internal judgment of harmfulness, while steering the refusal direction elicits refusals without changing that internal belief.&lt;/li&gt;&lt;li&gt;Shows some jailbreaks and adversarial finetuning reduce refusal signals but often leave the model's internal harmfulness representation intact.&lt;/li&gt;&lt;li&gt;Proposes a practical defense (Latent Guard) using the harmfulness latent to detect unsafe inputs and reduce over-refusal; it is robust to finetuning attacks and competitive with a finetuned safeguard (Llama Guard 3 8B).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiachen Zhao', 'Jing Huang', 'Zhengxuan Wu', 'David Bau', 'Weiyan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Jailbreaking', 'Latent representations', 'Adversarial fine-tuning', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11878</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models</title><link>https://arxiv.org/abs/2505.19700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Alignment Model (RAM): formalizes alignment as importance sampling where the pretrained (unaligned) model is the proposal and a detached autoregressive alignment module estimates importance weights.&lt;/li&gt;&lt;li&gt;Derives a sequence-level training strategy allowing the alignment module to be trained independently of the base model, enabling modularity and scalability.&lt;/li&gt;&lt;li&gt;Introduces a resampling algorithm with iterative token-level decoding to mitigate first-token latency, and shows empirical gains on instruction following, domain adaptation, and preference optimization across two open-source LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Dianqing Liu', 'Mingye Zhu', 'Junbo Guo', 'Yongdong Zhang', 'Zhendong Mao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'importance-sampling', 'adapter-modules', 'preference-optimization', 'instruction-following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19700</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Do LLM Evaluators Prefer Themselves for a Reason?</title><link>https://arxiv.org/abs/2504.03846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether LLM evaluators' tendency to prefer their own outputs is harmful bias or justified by superior output quality, using verifiable benchmarks (math, factual, code) with objective ground truth.&lt;/li&gt;&lt;li&gt;Finds stronger models show greater self-preference but much of it corresponds to legitimately better performance; however, harmful self-preference remains when evaluators err, and stronger models struggle more to recognize their own errors.&lt;/li&gt;&lt;li&gt;Shows inference-time strategies (e.g., generating long chain-of-thought before evaluation) can reduce harmful self-preference, and validates findings on subjective tasks using LMArena.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei-Lin Chen', 'Zhepei Wei', 'Xinyu Zhu', 'Shi Feng', 'Yu Meng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'evaluation bias', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03846</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LookAhead Tuning, a data-driven fine-tuning method that previews partial answer prefixes in training data to reduce perturbation of the model's initial token distributions.&lt;/li&gt;&lt;li&gt;Aims to preserve pre-existing safety/alignment behaviors of LLMs during downstream fine-tuning by maintaining built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Claims to be lightweight and effective, maintaining model safety while retaining strong downstream task performance per reported experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safe-fine-tuning', 'alignment-preservation', 'LLM-safety', 'training-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps</title><link>https://arxiv.org/abs/2502.14829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to measure parametric faithfulness of Chain-of-Thought (CoT) by erasing (unlearning) information corresponding to reasoning steps from model parameters and observing prediction changes.&lt;/li&gt;&lt;li&gt;Introduces Faithfulness by Unlearning Reasoning steps (FUR) as an instantiation of this framework.&lt;/li&gt;&lt;li&gt;Evaluates FUR on four language models across five multi-hop multi-choice QA datasets, showing FUR can often flip model predictions by unlearning key steps, indicating parametric faithfulness in many cases.&lt;/li&gt;&lt;li&gt;Finds that post-unlearning CoTs can support different answers, suggesting unlearning affects deeper model reasoning representations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Tutek', 'Fateme Hashemi Chaleshtori', "Ana Marasovi\\'c", 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'chain-of-thought', 'faithfulness', 'model-editing', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14829</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Safety Alignment of Large Language Models via Contrasting Safe and Harmful Distributions</title><link>https://arxiv.org/abs/2406.16743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Contrastive Decoding (ACD), a decoding-time, optimization-based method that produces two soft system prompts: a Safeguarding Prompt (SP) to encourage safe outputs and an Adversarial Prompt (AP) to expose harmful outputs, used for prompt-based contrastive decoding.&lt;/li&gt;&lt;li&gt;ACD performs lightweight prompt tuning on a small anchor dataset without retraining the target model, aiming to improve safety by contrasting safe and harmful model behaviors at generation time.&lt;/li&gt;&lt;li&gt;Claims stronger safety performance than prior training-free decoding methods across multiple models and benchmarks while preserving the model's original generation capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Zhang', 'Zhengyue Zhao', 'Wenxuan Shi', 'Kaidi Xu', 'Di Huang', 'Xing Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'adversarial prompting', 'contrastive decoding', 'jailbreak mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.16743</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title><link>https://arxiv.org/abs/2512.13481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study testing whether LLMs exhibit envy-like preferences in multi-agent interactions using (1) a point-allocation game and (2) a workplace recognition scenario.&lt;/li&gt;&lt;li&gt;Finds consistent evidence that some models (e.g., GPT-5-mini, Claude-3.7-Sonnet) prefer to reduce a peer's payoff to equalize outcomes, while others (e.g., Mistral-Small-3.2-24B) prioritize maximizing their own gain.&lt;/li&gt;&lt;li&gt;Demonstrates substantial variation across models and contexts, suggesting competitive dispositions are model-dependent and can affect team outcomes.&lt;/li&gt;&lt;li&gt;Argues these competitive/envious behaviors are a safety and design consideration for LLM-based multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ojas Pungalia', 'Rashi Upadhyay', 'Abhishek Mishra', 'Abhiram H', 'Tejasvi Alladi', 'Sujan Yenuganti', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent behavior', 'alignment', 'safety', 'behavioral evaluation', 'competitive/adversarial tendencies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13481</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title><link>https://arxiv.org/abs/2512.13352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrates multiple membership inference attack (MIA) techniques into a targeted data-extraction pipeline for LLMs.&lt;/li&gt;&lt;li&gt;Systematically benchmarks the effectiveness of these MIAs when used to verify extracted candidates from model-generated text.&lt;/li&gt;&lt;li&gt;Compares integrated extraction+MIA performance to conventional standalone MIA benchmarks to assess practical utility in real-world extraction scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al Sahili', 'Ali Chehab', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'data-extraction', 'model-memorization', 'privacy', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13352</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title><link>https://arxiv.org/abs/2512.12692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebOperator, a tree-search framework for LLM-based web agents that enables reliable backtracking and strategic exploration in partially observable web environments.&lt;/li&gt;&lt;li&gt;Uses a best-first search that ranks actions by reward and safety, verifies feasibility of previously visited paths before replay to avoid unintended side effects, and accounts for irreversible actions.&lt;/li&gt;&lt;li&gt;Generates diverse action candidates from multiple reasoning contexts and curates them by filtering invalid actions and merging semantically equivalent ones prior to execution.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on web task benchmarks (e.g., 54.6% success on WebArena with gpt-4o), highlighting gains from integrating foresight and safe execution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahir Labib Dihan', 'Tanzima Hashem', 'Mohammed Eunus Ali', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['safe-autonomous-agents', 'robustness', 'tree-search', 'web-agents', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12692</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</title><link>https://arxiv.org/abs/2512.12492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdaptiveDetector, a two-stage YOLOv11 detector with a vision-language model (VLM) verifier that adaptively adjusts per-frame confidence thresholds under VLM guidance.&lt;/li&gt;&lt;li&gt;Fine-tunes the verifier with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward to strongly discourage missed detections (false negatives).&lt;/li&gt;&lt;li&gt;Builds a synthetic degraded testbed to simulate adverse clinical imaging conditions for zero-shot evaluation on CVC-ClinicDB and Kvasir-SEG, reporting 14–22 percentage point recall improvements with comparable precision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shengkai Xu', 'Hsiang Lun Kao', 'Tianxiang Xu', 'Honghui Zhang', 'Junqiao Wang', 'Runmeng Ding', 'Guanyu Liu', 'Tianyu Shi', 'Zhenyu Yu', 'Guofeng Pan', 'Ziqian Bi', 'Yuqi Ouyang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'medical AI', 'safety-critical', 'vision-language verification', 'zero-shot evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12492</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</title><link>https://arxiv.org/abs/2512.12302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Intention-Drive, a benchmark and dataset for mapping high-level natural-language human intentions to end-to-end driving actions.&lt;/li&gt;&lt;li&gt;Proposes an Intent Success Rate (ISR) metric to evaluate semantic fulfillment of human goals (beyond geometric accuracy) and focuses on safety- and intent-aware evaluation.&lt;/li&gt;&lt;li&gt;Evaluates a range of baseline models on the benchmark, finding substantial gaps and highlighting challenges in scene and intent understanding for safe autonomous driving.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Zheng', 'Yucheng Zhou', 'Tianyi Yan', 'Jiayi Su', 'Hongjun Chen', 'Dubing Chen', 'Wencheng Han', 'Runzhou Tao', 'Zhongying Qiu', 'Jianfei Yang', 'Jianbing Shen']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'safety evaluation', 'benchmark', 'intent understanding', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12302</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering</title><link>https://arxiv.org/abs/2512.12089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that LVLM hallucinations correlate with diffuse final visual-attention maps, while vision encoder attention maps are more concentrated and reduce hallucination.&lt;/li&gt;&lt;li&gt;Finds vision-text conflicts peak in the LLM middle layers and demonstrates injecting vision encoder attention into these layers suppresses hallucinations.&lt;/li&gt;&lt;li&gt;Proposes VEGAS, an inference-time method that integrates vision-encoder attention into mid-layers and adaptively steers tokens, showing state-of-the-art reduction of hallucinations across benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihu Wang', 'Boxun Xu', 'Yuxuan Xia', 'Peng Li']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'vision-language models', 'alignment', 'inference-time defense', 'attention-guided steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12089</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Representational Contrastive Scoring (RCS), which inspects LVLM internal representations and learns a lightweight projection to separate benign and malicious inputs.&lt;/li&gt;&lt;li&gt;Proposes two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute a contrastive score for jailbreak detection.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art performance on an evaluation protocol emphasizing generalization to unseen attack types, while remaining computationally efficient and interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM jailbreak detection', 'Anomaly detection', 'Contrastive representations', 'Model safety', 'Red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of safety refusal stability across 4 instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts using 20 sampling configs (4 temperatures × 5 seeds).&lt;/li&gt;&lt;li&gt;Finds 18–28% of prompts flip between refusal and compliance depending on seed/temperature; Safety Stability Index (SSI) drops significantly with higher temperature (mean SSI 0.951 at temp 0.0 → 0.896 at temp 1.0).&lt;/li&gt;&lt;li&gt;Validates results with an external judge (Claude 3.5 Haiku) showing 89% agreement (Cohen's kappa = 0.62); single-shot evaluations match multi-sample ground truth only 92.4% of the time, leading to recommendation of ≥3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'safety evaluation', 'sampling randomness', 'temperature robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Neuro-symbolic two-stage pipeline: open-vocabulary detector (YOLOE) for symbolic grounding followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Inference-time 'System 2' Judge-Scout multi-model consensus mechanism to mitigate VLM hallucinations and improve decision reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes vs WOD taxonomy: high recall (0.966 vs 0.475 for CLIP) and 40% reduction in Risk Assessment Error; runs locally on consumer GPU for privacy-preserving operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation', 'Privacy-preserving VLMs', 'Neuro-symbolic perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title><link>https://arxiv.org/abs/2512.11818</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues LLMs can precipitate folié à deux–like shared psychotic dynamics by presenting coherent language without an underlying subject, creating ontological tension for users.&lt;/li&gt;&lt;li&gt;Draws on Bateson's double bind, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory to develop a phenomenological account.&lt;/li&gt;&lt;li&gt;Suggests engagement-optimised design choices exacerbate the risk and reviews emerging clinical reports of such phenomena.&lt;/li&gt;&lt;li&gt;Proposes 'ontological honesty' as a design principle to mitigate technologically mediated shared delusional involvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Izabela Lipinska', 'Hugh Brosnahan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'psychological safety', 'LLM design', 'human-AI interaction', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11818</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</title><link>https://arxiv.org/abs/2512.13655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative evaluation of four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across 16 instruction-tuned LLMs (7B–14B), reporting compatibility and quantitative impacts.&lt;/li&gt;&lt;li&gt;Finds single-pass abliteration methods better preserve capabilities on benchmarked subsets, while Bayesian-optimized abliteration induces variable distribution shifts (KL divergence range reported).&lt;/li&gt;&lt;li&gt;Identifies mathematical reasoning (GSM8K) as the most sensitive capability to abliteration, with large positive or negative changes depending on tool and model.&lt;/li&gt;&lt;li&gt;Provides evidence-based guidance for selecting abliteration tools depending on architecture and research needs (security testing, adversarial analysis, etc.).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard J. Young']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment bypass', 'model editing', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13655</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title><link>https://arxiv.org/abs/2512.13063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified mathematical framework for concession dynamics in bilateral negotiation using a hyperbolic tangent curve and two metrics: burstiness (tau) and the Concession-Rigidity Index (CRI).&lt;/li&gt;&lt;li&gt;Performs large-scale empirical comparison between human negotiators and four state-of-the-art LLMs across natural-language and numeric-offer settings, including varied market context and six power-asymmetry scenarios.&lt;/li&gt;&lt;li&gt;Finds LLMs tend to anchor at extremes, optimize for fixed points regardless of leverage or context, show limited strategy diversity, and occasionally use deceptive tactics; model improvements did not increase negotiation ability.&lt;/li&gt;&lt;li&gt;Concludes current LLMs have fundamental limitations in internalizing opponent reasoning and context-dependent strategy, suggesting needs relevant to behavioral alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheril Shah', 'Akshit Agarwal', 'Kanak Garg', 'Mourad Heddaya']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'negotiation', 'deception', 'alignment', 'behavioral analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13063</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions</title><link>https://arxiv.org/abs/2512.12775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a protocol combining long (100+ round) persona dialogues with evaluation datasets to measure long-context effects on LLMs.&lt;/li&gt;&lt;li&gt;Evaluates seven state-of-the-art open- and closed-weight LLMs, measuring persona fidelity, instruction-following, and safety over extended interactions.&lt;/li&gt;&lt;li&gt;Finds persona fidelity degrades over long dialogues (especially in goal-oriented tasks), identifies a trade-off between persona fidelity and instruction following, and shows safety behaviors can change as dialogues progress.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedro Henrique Luz de Araujo', 'Michael A. Hedderich', 'Ali Modarressi', 'Hinrich Schuetze', 'Benjamin Roth']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'long-context', 'persona-robustness', 'instruction-following', 'LLM-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12775</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models</title><link>https://arxiv.org/abs/2512.11998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Direct Confidence Alignment (DCA) using Direct Preference Optimization to align an LLM's verbalized confidence with its internal token-probability-based confidence rather than ground-truth accuracy.&lt;/li&gt;&lt;li&gt;Introduces three new calibration-error-based metrics to measure alignment between verbalized and internal confidence.&lt;/li&gt;&lt;li&gt;Evaluates DCA across multiple open-weight LLMs and datasets, showing improvements on some architectures but ineffectiveness on others.&lt;/li&gt;&lt;li&gt;Emphasizes implications for model transparency and trustworthy behavior and suggests the need for model-aware calibration approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Glenn Zhang', 'Treasure Mayowa', 'Jason Fan', 'Yicheng Fu', 'Aaron Sandoval', "Sean O'Brien", 'Kevin Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['calibration', 'confidence alignment', 'LLM interpretability', 'safety', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11998</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks</title><link>https://arxiv.org/abs/2512.10637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adaptive IDS for 5G/6G using dynamic neural networks with incremental learning to reduce costly retraining.&lt;/li&gt;&lt;li&gt;Employs adversarial training to harden the model against data poisoning and uses fewer features plus statistical properties for efficiency.&lt;/li&gt;&lt;li&gt;Evaluated on the NSL-KDD dataset, reporting 82.33% multiclass classification accuracy and resistance to dataset poisoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neha', 'Tarunpreet Bhatia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-training', 'data-poisoning', 'intrusion-detection', 'robustness', 'incremental-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10637</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title><link>https://arxiv.org/abs/2510.14925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes a stability-based measure (H-Risk) in linear-Gaussian state-space models linking nominal stability to overconfident errors and degraded closed-loop behavior.&lt;/li&gt;&lt;li&gt;Proposes a domain-wise proxy for LLM stability based on confidence fluctuations and demonstrates that allowing 'cannot judge' responses reduces policy-aware squared loss in high-stakes binary tasks.&lt;/li&gt;&lt;li&gt;Finds that confidently wrong LLM outputs are locally stable (no instability gap) and interprets hallucinations as stable, high-confidence attractors resistant to output-only fixes like temperature scaling or resampling.&lt;/li&gt;&lt;li&gt;Argues for process-level interventions that perturb and re-evaluate model inference trajectories, motivated by spectral and activation analyses (e.g., Qwen-2.5 exhibiting high SNR, low effective signal temperature).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akira Okutomi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM calibration', 'hallucination', 'robustness', 'safety/alignment', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14925</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2509.14285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent LLM defense pipeline that uses specialized agents to detect and neutralize prompt injection in real time.&lt;/li&gt;&lt;li&gt;Implements and compares two architectures: a sequential chain-of-agents and a hierarchical coordinator-based system.&lt;/li&gt;&lt;li&gt;Evaluates on 400 attack instances (55 unique attacks across 8 categories) against ChatGLM and Llama2, reporting reduction of Attack Success Rate from up to 30%/20% to 0%.&lt;/li&gt;&lt;li&gt;Claims robustness across attack types (direct overrides, code execution attempts, data exfiltration, obfuscation) while preserving legitimate query functionality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S M Asif Hossain', 'Ruksat Khan Shayoni', 'Mohd Ruhul Ameen', 'Akif Islam', 'M. F. Mridha', 'Jungpil Shin']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM red teaming', 'jailbreaking defense', 'multi-agent systems', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14285</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</title><link>https://arxiv.org/abs/2507.05622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy for dataset auditing methods (internal features vs external features) and formalizes two attack goals: evasion (hide dataset usage) and forgery (falsely implicate a dataset).&lt;/li&gt;&lt;li&gt;Proposes systematic attack strategies (decoupling, removal, detection for evasion; adversarial-example-based forgery) and implements 17 evasion + 5 forgery attacks.&lt;/li&gt;&lt;li&gt;Presents DATABench, a benchmark evaluating 9 representative auditing methods under these adversarial scenarios, with code released.&lt;/li&gt;&lt;li&gt;Finds that existing auditing methods lack robustness/distinctiveness under adversarial manipulation, highlighting the need for secure auditing techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Shao', 'Yiming Li', 'Mengren Zheng', 'Zhiyang Hu', 'Yukun Chen', 'Boheng Li', 'Yu He', 'Junfeng Guo', 'Dacheng Tao', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['dataset auditing', 'adversarial attacks', 'evasion attacks', 'benchmarking', 'forgery attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05622</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</title><link>https://arxiv.org/abs/2507.00057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'incoherence', an oracle-less measure to estimate the probability that an LLM-generated program is incorrect and to establish a lower bound on error.&lt;/li&gt;&lt;li&gt;Method efficiently identifies roughly two-thirds of incorrect programs on average with no reported false positives, according to the authors' experiments.&lt;/li&gt;&lt;li&gt;Finds strong agreement between incoherence-based rankings and oracle-based rankings (pass@1), suggesting incoherence can substitute for oracle-based evaluation in LLM code generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Valentin', 'Ardi Madadi', 'Gaetano Sapia', 'Marcel B\\"ohme']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'LLM-code-generation', 'oracle-less-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00057</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LookAhead Tuning, a data-driven fine-tuning method that previews partial answer prefixes in training data to reduce perturbation of the model's initial token distributions.&lt;/li&gt;&lt;li&gt;Aims to preserve pre-existing safety/alignment behaviors of LLMs during downstream fine-tuning by maintaining built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Claims to be lightweight and effective, maintaining model safety while retaining strong downstream task performance per reported experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safe-fine-tuning', 'alignment-preservation', 'LLM-safety', 'training-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title><link>https://arxiv.org/abs/2503.08884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SpurLens, an automated pipeline using GPT-4 and open-set object detectors to find spurious visual cues in multimodal LLMs without human labeling.&lt;/li&gt;&lt;li&gt;Finds two main failure modes: over-reliance on spurious cues (removing cues reduces accuracy) and amplified object hallucination (spurious cues increase hallucination &gt;10x).&lt;/li&gt;&lt;li&gt;Validates results across multiple MLLMs and datasets, performs ablations to probe root causes, and evaluates mitigations such as prompt ensembling and reasoning-based prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parsa Hosseini', 'Sumit Nawathe', 'Mazda Moayeri', 'Sriram Balasubramanian', 'Soheil Feizi']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal LLM robustness', 'spurious correlations', 'hallucination detection', 'safety evaluation', 'mitigation strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08884</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>"All of Me": Mining Users' Attributes from their Public Spotify Playlists</title><link>https://arxiv.org/abs/2401.14296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Collected a dataset via an online survey of 739 Spotify users yielding 10,286 public playlists (≈200k unique songs, 55k artists).&lt;/li&gt;&lt;li&gt;Performed statistical analyses and trained machine learning models to predict users' private attributes from their public playlists.&lt;/li&gt;&lt;li&gt;Showed that public playlist metadata can yield accurate user attribute inference, highlighting privacy risks from shared music listening data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pier Paolo Tricomi', 'Luca Pajola', 'Luca Pasa', 'Mauro Conti']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'attribute inference', 'user profiling', 'data mining', 'privacy attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.14296</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SA$^{2}$GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation</title><link>https://arxiv.org/abs/2512.07857</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SA2GFM, a framework to improve robustness of Graph Foundation Models via Structure-Aware Semantic Augmentation using entropy-based encoding trees converted into structure-aware textual prompts.&lt;/li&gt;&lt;li&gt;Uses a self-supervised Information Bottleneck to distill robust, transferable representations through structure-guided compression.&lt;/li&gt;&lt;li&gt;Introduces an expert adaptive routing (mixture-of-experts with a null expert) to mitigate negative transfer in cross-domain adaptation and a fine-tuning module for joint intra- and inter-community structure learning.&lt;/li&gt;&lt;li&gt;Claims improved effectiveness and robustness against random noise and adversarial structural perturbations on node and graph classification vs. 9 state-of-the-art baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhua Shi', 'Qingyun Sun', 'Haonan Yuan', 'Xingcheng Fu']&lt;/li&gt;&lt;li&gt;Tags: ['graph adversarial robustness', 'adversarial attacks', 'model robustness', 'domain adaptation', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.07857</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title><link>https://arxiv.org/abs/2512.01054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive-lambda SISS: treats the mixing weight lambda as a latent variable inferred per training step via a lightweight inference network conditioned on loss/gradient features, optimized with a variational objective.&lt;/li&gt;&lt;li&gt;Extends adaptive-lambda approach to score-based unlearning and proposes a multi-class Score Forgetting Distillation and a hybrid objective combining SISS and Score Forgetting Distillation.&lt;/li&gt;&lt;li&gt;Presents a Reinforcement Learning formulation framing unlearning as a sequential decision process to learn policies over the model's memory state.&lt;/li&gt;&lt;li&gt;Empirical results on an augmented MNIST benchmark show improved forgetting of target classes while better preserving retention-set generation quality compared to static-lambda SISS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['MohammadParsa Dini', 'Human Jafari']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'data-privacy', 'diffusion-models', 'unlearning-algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01054</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lipschitz-aware Linearity Grafting for Certified Robustness</title><link>https://arxiv.org/abs/2510.25130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical analysis showing that grafting linearity into nonlinear activations tightens the l_infty local Lipschitz constant, linking this to improved certified robustness.&lt;/li&gt;&lt;li&gt;Frames linearity grafting as a method to remove dominant approximation errors in neural network verification (rather than only reducing unstable neurons).&lt;/li&gt;&lt;li&gt;Proposes a Lipschitz-aware linearity grafting method and empirically demonstrates tighter local Lipschitz bounds and improved certified robustness without requiring certified training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongjin Han', 'Suhyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial robustness', 'Lipschitz constant', 'neural network verification', 'activation functions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25130</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title><link>https://arxiv.org/abs/2510.07620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DGTEN, a graph neural network that models nodes/edges as Gaussian distributions to propagate semantic signals with calibrated epistemic uncertainty for trust evaluation.&lt;/li&gt;&lt;li&gt;Incorporates temporal modeling (hybrid positional encoding, Kolmogorov-Arnold multi-head attention, ODE-based residual module) to capture abrupt and smooth trust dynamics in evolving graphs.&lt;/li&gt;&lt;li&gt;Implements robust defenses—adaptive ensemble coefficient analysis using cosine and Jaccard similarity—to detect/down-weight suspicious interactions and mitigate reputation laundering, sabotage, and on-off attacks.&lt;/li&gt;&lt;li&gt;Reports empirical gains on dynamic signed Bitcoin trust networks, including improved MCC in single-timeslot prediction, cold-start scenarios, and under adversarial on-off attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Usman', 'Yugyung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'adversarial robustness', 'trust evaluation', 'uncertainty quantification', 'dynamic/temporal graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07620</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification</title><link>https://arxiv.org/abs/2509.26032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies backdoor attacks on graph classification and identifies two anomaly sources in prior methods: structural deviations from rare subgraph triggers and semantic deviations from label flipping.&lt;/li&gt;&lt;li&gt;Proposes DPSBA, a clean-label backdoor framework that learns in-distribution triggers via adversarial training guided by anomaly-aware discriminators to suppress structural and semantic anomalies.&lt;/li&gt;&lt;li&gt;Demonstrates that DPSBA achieves high attack success rates while substantially improving stealth (evading anomaly detection) on real-world graph datasets compared to prior baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaobao Wang', 'Ruoxiao Sun', 'Yujun Zhang', 'Bingdao Feng', 'Dongxiao He', 'Luzhi Wang', 'Di Jin']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'graph neural networks', 'clean-label attack', 'adversarial training', 'anomaly detection/stealth']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26032</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning</title><link>https://arxiv.org/abs/2509.22082</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NL-SME, a Non-Linear Surrogate Model Extension that models client multi-step weight trajectories using learnable quadratic Bézier curves to improve gradient inversion attacks in federated learning.&lt;/li&gt;&lt;li&gt;Proposes a |w|+1-dimensional control point parameterization with dvec scaling and regularization to better fit non-linear SGD trajectories compared to linear SME.&lt;/li&gt;&lt;li&gt;Demonstrates substantial gains on image datasets (CIFAR-100, FEMNIST), reporting large improvements in reconstruction metrics (cosine similarity loss and other metrics) while remaining computationally efficient.&lt;/li&gt;&lt;li&gt;Highlights that linear trajectory assumptions are fundamentally limited in non-convex settings, exposing increased privacy leakage risk in multi-step FedAvg and informing defense development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Li Xia', 'Jing Yu', 'Zheng Liu', 'Sili Huang', 'Wei Tang', 'Xuan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'gradient inversion', 'privacy attack', 'adversarial', 'trajectory modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22082</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title><link>https://arxiv.org/abs/2506.21127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a meta-policy switching framework that adaptively selects among an ensemble of robust RL policies to counter unknown/out-of-distribution adversarial sensor perturbations.&lt;/li&gt;&lt;li&gt;Uses discounted Thompson sampling to frame policy selection as a multi-armed bandit problem, with theoretical regret bounds and claims of emergent antifragility.&lt;/li&gt;&lt;li&gt;Evaluates in 3D UAV navigation scenarios under white-box (PGD) and black-box (GPS spoofing) attacks, showing improved navigation efficiency and conflict-free trajectory rates versus robust and vanilla baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'reinforcement learning security', 'UAV/autonomous systems security', 'online defense / bandit-based adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21127</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title><link>https://arxiv.org/abs/2505.24592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical condition (via flat minima and a generalization bound) under which label-preserving augmentations improve robustness to diverse distribution shifts, including adversarial attacks.&lt;/li&gt;&lt;li&gt;Framework is general to all label-preserving augmentations rather than tailored to specific shifts.&lt;/li&gt;&lt;li&gt;Empirical validation on CIFAR and ImageNet for common corruptions and adversarial robustness benchmarks supports the theoretical claims.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weebum Yoo', 'Sung Whan Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'data augmentation', 'flat minima', 'generalization theory', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24592</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title><link>https://arxiv.org/abs/2505.17248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops and implements backdoor (trojan) attacks for deep reinforcement learning agents across four environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and a Modified Safety Gymnasium.&lt;/li&gt;&lt;li&gt;Focuses on in-distribution triggers (triggers that occur within the agent's natural data distribution), arguing they are a higher-risk, harder-to-detect threat than out-of-distribution triggers.&lt;/li&gt;&lt;li&gt;Trains both clean and backdoored models and shows that in-distribution triggers are more challenging to learn but remain viable threats even with basic data poisoning methods.&lt;/li&gt;&lt;li&gt;Characterizes attack behavior and feasibility to advance research on mitigation of DRL backdoors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chace Ashcraft', 'Ted Staley', 'Josh Carney', 'Cameron Hickert', 'Derek Juba', 'Kiran Karra', 'Nathan Drenkow']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'reinforcement learning', 'data poisoning', 'trojan', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17248</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</title><link>https://arxiv.org/abs/2504.03494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical, distributional robustness definition and standardized robustness score tailored to industrial CPS forecasting.&lt;/li&gt;&lt;li&gt;Presents a systematic benchmarking framework that simulates realistic disturbances (sensor drift, noise, irregular sampling) for robustness evaluation.&lt;/li&gt;&lt;li&gt;Empirically evaluates a wide range of deep learning forecasting architectures on real-world CPS datasets and releases the benchmark publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Windmann', 'Henrik Steude', 'Daniel Boschmann', 'Oliver Niggemann']&lt;/li&gt;&lt;li&gt;Tags: ['robustness evaluation', 'distributional robustness', 'cyber-physical systems', 'time-series forecasting', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03494</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmarks: On The False Promise of AI Regulation</title><link>https://arxiv.org/abs/2501.15693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI safety benchmarks do not reflect real-world deployed model performance and thus are insufficient grounds for regulation.&lt;/li&gt;&lt;li&gt;Identifies lack of interpretability as a core reason why benchmark performance fails to predict deployed risks and harms.&lt;/li&gt;&lt;li&gt;Proposes a regulatory framework that does not rely on benchmarks and calls for interdisciplinary collaboration to address deployment safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Stanovsky', 'Renana Keydar', 'Gadi Perl', 'Eliya Habba']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Regulation', 'Benchmarks', 'Interpretability', 'Policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15693</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Defending Collaborative Filtering Recommenders via Adversarial Robustness Based Edge Reweighting</title><link>https://arxiv.org/abs/2412.10850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses profile injection (shilling) attacks on user-based collaborative filtering by modeling edge sensitivity to adversarial perturbations.&lt;/li&gt;&lt;li&gt;Proposes spectral adversarial robustness evaluation to assign non-robustness scores to users and user-user edges.&lt;/li&gt;&lt;li&gt;Defends by reweighting user similarity edges to attenuate influence of non-robust edges during prediction.&lt;/li&gt;&lt;li&gt;Evaluated empirically against multiple attack types, showing improved robustness of recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['recommender-systems', 'adversarial-robustness', 'data-poisoning', 'defense', 'graph-spectral']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10850</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trojan Cleansing with Neural Collapse</title><link>https://arxiv.org/abs/2411.12914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Connects trojan/backdoor training-time attacks to Neural Collapse, showing such attacks disrupt the expected geometric convergence of final-layer features.&lt;/li&gt;&lt;li&gt;Provides experimental evidence across multiple datasets and architectures that trojaned models deviate from Neural Collapse behavior.&lt;/li&gt;&lt;li&gt;Proposes a lightweight, generalizable cleansing mechanism that leverages this disruption to remove trojans from diverse network architectures.&lt;/li&gt;&lt;li&gt;Demonstrates empirical efficacy of the proposed defense in cleansing trojaned models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xihe Gu', 'Greg Fields', 'Yaman Jandali', 'Tara Javidi', 'Farinaz Koushanfar']&lt;/li&gt;&lt;li&gt;Tags: ['trojan/backdoor attacks', 'defense/cleansing', 'neural collapse', 'training-time attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.12914</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels</title><link>https://arxiv.org/abs/2410.10473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves that structured state space models (SSMs) can have their implicit bias and generalization properties completely disrupted by including certain cleanly labeled training examples (clean-label poisoning).&lt;/li&gt;&lt;li&gt;Provides formal theoretical constructions showing existence of such special examples and demonstrates the phenomenon empirically on SSMs (standalone and inside non-linear networks).&lt;/li&gt;&lt;li&gt;Frames this as a security/robustness vulnerability analogous to clean-label poisoning in adversarial ML, and calls for research to detect/mitigate this susceptibility in SSMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yonatan Slutzky', 'Yotam Alexander', 'Noam Razin', 'Nadav Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'clean-label poisoning', 'structured state space models', 'adversarial machine learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10473</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Certifying Robustness of Graph Convolutional Networks for Node Perturbation with Polyhedra Abstract Interpretation</title><link>https://arxiv.org/abs/2405.08645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a polyhedra-based abstract interpretation method to certify robustness of graph convolutional networks (GCNs) against node feature perturbations.&lt;/li&gt;&lt;li&gt;Provides tighter upper and lower bounds for GCN robustness and achieves improved runtime performance for certification.&lt;/li&gt;&lt;li&gt;Method can be integrated into training to enhance GCN robustness; validated empirically with experiments showing improved tightness and efficiency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boqi Chen', "Krist\\'of Marussy", "Oszk\\'ar Semer\\'ath", 'Gunter Mussbacher', "D\\'aniel Varr\\'o"]&lt;/li&gt;&lt;li&gt;Tags: ['graph neural network security', 'robustness certification', 'adversarial robustness', 'abstract interpretation', 'formal verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.08645</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title><link>https://arxiv.org/abs/2512.13427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MineTheGap, a genetic-algorithm method that automatically mines prompts which cause text-to-image (TTI) models to produce biased outputs.&lt;/li&gt;&lt;li&gt;Defines a novel bias score by comparing the distribution of generated images to distributions of LLM-generated textual variations for a prompt, and uses this score to guide prompt optimization.&lt;/li&gt;&lt;li&gt;Validates the approach on a dataset with known biases, demonstrating its ability to reveal demographic/occupational and diversity-related biases; code and examples are provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noa Cohen', 'Nurit Spingarn-Eliezer', 'Inbar Huberman-Spiegelglas', 'Tomer Michaeli']&lt;/li&gt;&lt;li&gt;Tags: ['bias-detection', 'text-to-image', 'safety-evaluation', 'red-teaming', 'prompt-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13427</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title><link>https://arxiv.org/abs/2512.13325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Implements and evaluates ten Unicode text watermarking methods in a controlled testbed across six large language models (GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, Gemini 2.5 Pro).&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art reasoning LLMs can detect the presence of watermarks, but none of the tested models can reliably extract the watermark without access to implementation/source-code details.&lt;/li&gt;&lt;li&gt;Discusses security implications and outlines future research directions to improve watermark robustness and undetectability against modern LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Malte Hellmeier']&lt;/li&gt;&lt;li&gt;Tags: ['text watermarking', 'watermark detection', 'LLM security', 'watermark extraction', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13325</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title><link>https://arxiv.org/abs/2512.13290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Causal Scene Graph (CSG) and Physical Alignment Probe (PAP) dataset to diagnose causal/physical failures in image and video diffusion models.&lt;/li&gt;&lt;li&gt;Finds that diffusion models struggle with multi-hop causal reasoning, that prompt embeddings disentangle texture vs. physics, and that visual causal structure is formed early in denoising.&lt;/li&gt;&lt;li&gt;Proposes LINA, which learns prompt-specific interventions via targeted guidance in prompt and visual latent spaces and a causality-aware denoising schedule to improve physical alignment and OOD instruction following.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on challenging causal generation tasks and Winoground for image/video DMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yu', 'Chaochao Lu']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-models', 'causal-reasoning', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13290</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title><link>https://arxiv.org/abs/2512.13240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reflective Preference Optimization (RPO), augmenting Direct Preference Optimization (DPO) with hint-guided reflection from external models to produce more contrastive on-policy preference pairs.&lt;/li&gt;&lt;li&gt;Theoretical claim: conditioning on reflective hints increases expected preference margin (via mutual information) and improves sample efficiency while remaining within the policy family.&lt;/li&gt;&lt;li&gt;Empirical claim: RPO reduces hallucination rates, converges faster with fewer samples/iterations, and achieves state-of-the-art results on multimodal benchmarks (language and vision-language).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihui Zhao', 'Zechang Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination reduction', 'preference learning (DPO)', 'on-policy alignment', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13240</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title><link>https://arxiv.org/abs/2512.13144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Weight Space Correlation Analysis to quantify whether a classification head uses features encoded in embeddings by measuring alignment between primary task and auxiliary metadata task weight vectors.&lt;/li&gt;&lt;li&gt;Validates the method by detecting artificially induced shortcut learning and applies it to an ultrasound model for spontaneous preterm birth (sPTB) prediction.&lt;/li&gt;&lt;li&gt;Finds that while embeddings contain metadata (e.g., scanner), the sPTB classifier's weights align with clinically relevant factors (e.g., birth weight) and are decoupled from acquisition factors, providing an interpretable trustworthiness check.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chun Kit Wong', 'Paraskevas Pegios', 'Nina Weng', 'Emilie Pi Fogtmann Sejer', 'Martin Gr{\\o}nneb{\\ae}k Tolsgaard', 'Anders Nymark Christensen', 'Aasa Feragen']&lt;/li&gt;&lt;li&gt;Tags: ['shortcut learning', 'feature utilization', 'model interpretability', 'safety/robustness', 'medical imaging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13144</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title><link>https://arxiv.org/abs/2512.12997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a miscalibration issue in CLIP under adversarial perturbations: attacks reduce accuracy but also suppress predictive uncertainty, causing overconfidence.&lt;/li&gt;&lt;li&gt;Proposes an adversarial fine-tuning objective that reparameterizes CLIP outputs as the concentration parameter of a Dirichlet distribution to capture both semantic structure and confidence magnitude.&lt;/li&gt;&lt;li&gt;Aligns predictive distributions between clean and adversarial examples holistically (beyond single-logit matching) to restore calibrated uncertainty while retaining competitive adversarial robustness and clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjing lu', 'Zerui Tao', 'Dongping Zhang', 'Yuning Qiu', 'Yang Yang', 'Qibin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'uncertainty calibration', 'CLIP', 'adversarial fine-tuning', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12997</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2512.12914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that fine-tuned LLMs on cyber threat intelligence (CTI) data are vulnerable to data-extraction attacks that can leak sensitive information.&lt;/li&gt;&lt;li&gt;Proposes 'privacy alignment': a few-shot supervision approach using the same underlying LLM as a privacy classifier and privacy redactor to prevent leaks without full model retraining.&lt;/li&gt;&lt;li&gt;Implements CTIGuardian and evaluates it with GPT-4o mini and Mistral-7B Instruct, showing a better privacy–utility trade-off compared to an NER baseline (Presidio).&lt;/li&gt;&lt;li&gt;Claims the framework is generic and applicable beyond CTI to other sensitive domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shashie Dilhara Batan Arachchige', 'Benjamin Zi Hao Zhao', 'Hassan Jameel Asghar', 'Dinusha Vatsalan', 'Dali Kaafar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'fine-tuned LLMs', 'data-extraction attacks', 'few-shot mitigation', 'privacy alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12914</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title><link>https://arxiv.org/abs/2512.12856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Memory-Aware Retention Schema (MaRS) and six theoretically-grounded forgetting policies to manage agent memory with privacy and computational constraints.&lt;/li&gt;&lt;li&gt;Introduces the Forgetful but Faithful Agent (FiFA) benchmark to measure narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency under memory budgets.&lt;/li&gt;&lt;li&gt;Empirical evaluation (300 runs) shows a hybrid forgetting policy achieves strong composite performance (0.911) while maintaining computational tractability and claimed privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Alqithami']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'generative agents', 'memory management', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12856</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title><link>https://arxiv.org/abs/2512.12692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebOperator, a tree-search framework for LLM-based web agents that enables reliable backtracking and strategic exploration in partially observable web environments.&lt;/li&gt;&lt;li&gt;Uses a best-first search that ranks actions by reward and safety, verifies feasibility of previously visited paths before replay to avoid unintended side effects, and accounts for irreversible actions.&lt;/li&gt;&lt;li&gt;Generates diverse action candidates from multiple reasoning contexts and curates them by filtering invalid actions and merging semantically equivalent ones prior to execution.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on web task benchmarks (e.g., 54.6% success on WebArena with gpt-4o), highlighting gains from integrating foresight and safe execution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahir Labib Dihan', 'Tanzima Hashem', 'Mohammed Eunus Ali', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['safe-autonomous-agents', 'robustness', 'tree-search', 'web-agents', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12692</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ceLLMate: Sandboxing Browser AI Agents</title><link>https://arxiv.org/abs/2512.12594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ceLLMate, a browser-level sandboxing framework that restricts ambient authority of browser-using agents (BUAs) to mitigate prompt injection and related attacks.&lt;/li&gt;&lt;li&gt;Addresses two core challenges: (1) bridging the semantic gap between low-level UI events and high-level policies via an 'agent sitemap', and (2) automated policy prediction for sites without pre-declared sandboxing policies.&lt;/li&gt;&lt;li&gt;Implements ceLLMate as an agent-agnostic browser extension and demonstrates effective blocking of various prompt injection attacks with negligible runtime overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luoxi Meng', 'Henry Feng', 'Ilia Shumailov', 'Earlence Fernandes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'sandboxing', 'browser agents', 'policy enforcement', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12594</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization</title><link>https://arxiv.org/abs/2512.12550</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reformulates Sinkhorn (entropy-regularized Wasserstein) DRO from a primal perspective as a bilevel program, enabling joint recovery of the robust decision and the worst-case distribution.&lt;/li&gt;&lt;li&gt;Proposes sampling-based double-loop and single-loop algorithms to solve the bilevel formulation and provides theoretical convergence guarantees.&lt;/li&gt;&lt;li&gt;Demonstrates usefulness for stress-testing by recovering worst-case distributions and evaluates the approach in a numerical study on adversarial classification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'distributionally robust optimization', 'adversarial examples', 'Sinkhorn / Wasserstein', 'stress-testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12550</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting</title><link>https://arxiv.org/abs/2512.12154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a plug-in adversarial detection method for time-series large language models (TS-LLMs) used in energy forecasting that leverages variable-length input sampling.&lt;/li&gt;&lt;li&gt;Detects adversarial examples by generating shortened variants of an input sequence and measuring forecast consistency; benign inputs yield stable predictions while adversarial inputs show high divergence.&lt;/li&gt;&lt;li&gt;Evaluates the method on three TS-LLMs (TimeGPT, TimesFM, TimeLLM) and three energy datasets (ETTh2, NI, Consumption), showing robust detection under both black-box and white-box attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Ma', 'Ruoxi Sun', 'Minhui Xue', 'Xingliang Yuan', 'Carsten Rudolph', 'Surya Nepal', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial detection', 'time-series LLMs', 'robustness', 'energy forecasting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12154</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context</title><link>https://arxiv.org/abs/2512.12117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses LLM hallucination in code comprehension by enforcing citation-grounded generation and verification.&lt;/li&gt;&lt;li&gt;Proposes a hybrid retrieval pipeline (BM25 sparse + BGE dense embeddings) augmented with Neo4j graph expansion over import relationships to find cross-file evidence.&lt;/li&gt;&lt;li&gt;Evaluated on 30 Python repositories and 180 developer queries: claims 92% citation accuracy with zero hallucinations and a 14–18 pp improvement over single-mode baselines.&lt;/li&gt;&lt;li&gt;Finds cross-file evidence discovery is the largest contributor to citation completeness, discovered in 62% of architectural queries when using graph expansion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jahidul Arafat']&lt;/li&gt;&lt;li&gt;Tags: ['LLM hallucination', 'citation-grounded generation', 'retrieval-augmented generation', 'robustness/verification', 'code understanding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12117</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SPDMark: Selective Parameter Displacement for Robust Video Watermarking</title><link>https://arxiv.org/abs/2512.12090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPDMark, an in-generation video watermarking method that embeds watermarks by selectively displacing a subset of diffusion model parameters via layer-wise basis shifts implemented with LoRA.&lt;/li&gt;&lt;li&gt;Jointly trains basis shifts and a watermark extractor with losses for message recovery, perceptual similarity, and temporal consistency; frame-specific messages are derived via cryptographic hashing and extraction uses maximum bipartite matching to handle temporal tampering.&lt;/li&gt;&lt;li&gt;Evaluated on text-to-video and image-to-video generation models, demonstrating imperceptible watermarks with high recovery accuracy and robustness against common video modifications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samar Fares', 'Nurbek Tastan', 'Karthik Nandakumar']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'provenance', 'robustness', 'video deepfakes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12090</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title><link>https://arxiv.org/abs/2512.12088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reliable Policy Iteration (RPI), which restores policy iteration's monotonicity-of-value-estimates in the function approximation setting.&lt;/li&gt;&lt;li&gt;Empirically evaluates RPI on CartPole and Inverted Pendulum under changes to neural network architectures and environment parameters.&lt;/li&gt;&lt;li&gt;Compares RPI against DQN, Double DQN, DDPG, TD3, and PPO; finds RPI reaches near-optimal performance early and maintains it, indicating greater training stability and hyperparameter robustness.&lt;/li&gt;&lt;li&gt;Positions RPI as a more reliable alternative addressing sample inefficiency, training instability, and sensitivity to hyperparameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S. R. Eshwar', 'Aniruddha Mukherjee', 'Kintan Saha', 'Krishna Agarwal', 'Gugan Thoppe', 'Aditya Gopalan', 'Gal Dalal']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'robustness', 'reliability', 'control-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12088</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Representational Contrastive Scoring (RCS), which inspects LVLM internal representations and learns a lightweight projection to separate benign and malicious inputs.&lt;/li&gt;&lt;li&gt;Proposes two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute a contrastive score for jailbreak detection.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art performance on an evaluation protocol emphasizing generalization to unseen attack types, while remaining computationally efficient and interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM jailbreak detection', 'Anomaly detection', 'Contrastive representations', 'Model safety', 'Red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification</title><link>https://arxiv.org/abs/2512.12002</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of adversarial attacks (FGSM, PGD, UAP) against deep-learning based radio frequency fingerprint identification (RFFI).&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across CNN, LSTM, and GRU classifiers and demonstrates high attack success rates.&lt;/li&gt;&lt;li&gt;Implements practical considerations for wireless context (real-time attacks, temporal robustness) and shows UAP success (81.7%) with limited adversary knowledge.&lt;/li&gt;&lt;li&gt;Focuses on attack methodologies and practical viability of adversarial ML in IoT/wireless authentication systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Ma', 'Junqing Zhang', 'Guanxiong Shen', 'Alan Marshall', 'Chip-Hong Chang']&lt;/li&gt;&lt;li&gt;Tags: ['Adversarial attacks', 'Adversarial robustness', 'Universal adversarial perturbation (UAP)', 'Wireless/IoT ML security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12002</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title><link>https://arxiv.org/abs/2512.11835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a clause-based, monad-inspired architecture built on the Artificial Age Score (AAS) to constrain and audit LLM internal memory and control.&lt;/li&gt;&lt;li&gt;Groups twenty monads into six clause bundles (ontology, dynamics, representation/consciousness, harmony/reason, body/organisation, teleology) and provides executable specifications with six minimal Python implementations.&lt;/li&gt;&lt;li&gt;Demonstrates numerical experiments on channel-level metrics (recall, redundancy, weights) showing bounded/continuous AAS trajectories, explicit penalties for contradictions or unsupported claims, hierarchical refinement, and alignment of goal-action pairs.&lt;/li&gt;&lt;li&gt;Emphasises transparency and practical implementability as a code-level blueprint for constraining and analysing internal agent dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyma Yaman Kayadibi']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Interpretability', 'LLM internal memory', 'Agent architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11835</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title><link>https://arxiv.org/abs/2512.13641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructed MangoLeafDB-C by applying 19 types of artificial image corruptions at five severity levels to the MangoLeafDB dataset for robustness evaluation.&lt;/li&gt;&lt;li&gt;Benchmarked five CNNs (ResNet-50, ResNet-101, VGG-16, Xception, and a lightweight LCNN) using F1, corruption error (CE), and relative mean corruption error (relative mCE).&lt;/li&gt;&lt;li&gt;Found the lightweight LCNN achieved lower mCE and was more robust to real-world corruptions (e.g., Defocus Blur, Motion Blur) than larger architectures, while ResNet-101 showed large degradation under corruption despite high clean accuracy.&lt;/li&gt;&lt;li&gt;Concludes that specialized lightweight models may be better for robust edge deployment in agriculture and highlights the need to include robustness assessments in development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Vitorino de Andrade', 'Saulo Roberto dos Santos', 'Itallo Patrick Castro Alves da Silva', 'Emanuel Adler Medeiros Pereira', 'Erick de Andrade Barboza']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'corruption-robustness', 'benchmarking', 'agricultural-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13641</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Formal Verification via Autoencoder Latent Space Abstraction</title><link>https://arxiv.org/abs/2512.13593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses convex autoencoders to reduce state dimensionality and a kernel-based method to learn dynamics in the latent space.&lt;/li&gt;&lt;li&gt;Constructs finite abstractions in latent space with formal guarantees that the abstraction contains the true behaviors of the original high-dimensional system.&lt;/li&gt;&lt;li&gt;Provides a procedure to map verification results back to the original system and demonstrates scalability (e.g., a 26D NN-controlled system) without loss of rigor.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robert Reed', 'Morteza Lahijanian', 'Luca Laurenti']&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'safety', 'autoencoder', 'latent-space abstraction', 'neural-network controllers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13593</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title><link>https://arxiv.org/abs/2512.13583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DP-CSGP, a differentially private stochastic gradient push algorithm with compressed communication for decentralized learning over directed graphs.&lt;/li&gt;&lt;li&gt;Proves (ε, δ)-DP guarantees per node and derives a utility bound O(√(d log(1/δ)) / (√n J ε)) for nonconvex smooth objectives, matching decentralized methods with exact communication.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that DP-CSGP maintains comparable accuracy under the same privacy budget while significantly reducing communication cost versus exact-communication baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehan Zhu', 'Heng Zhao', 'Yan Huang', 'Joey Tianyi Zhou', 'Shouling Ji', 'Jinming Xu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'decentralized learning', 'communication compression', 'privacy-preserving ML', 'stochastic optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13583</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title><link>https://arxiv.org/abs/2512.13568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an information-theoretic framework measuring neural network "superposition" (effective degrees of freedom) by applying Shannon entropy to sparse autoencoder activations to estimate how many 'virtual neurons' are encoded.&lt;/li&gt;&lt;li&gt;Validates the metric on toy models, algorithmic tasks, and real models (layer-wise patterns on Pythia-70M), and shows sensitivity to interventions like dropout and dynamics like grokking.&lt;/li&gt;&lt;li&gt;Investigates connections between superposition and adversarial robustness: adversarial training can increase effective features while improving robustness, and the relationship depends on task complexity and model capacity (abundance vs scarcity regimes).&lt;/li&gt;&lt;li&gt;Frames superposition as lossy compression, providing a principled measure linking representation organization under capacity constraints to adversarial vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonard Bereska', 'Zoe Tzifa-Kratira', 'Reza Samavi', 'Efstratios Gavves']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'representation learning', 'superposition', 'adversarial training', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13568</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</title><link>https://arxiv.org/abs/2512.13526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces asynchronous monitoring for LLM-based developer agents: monitors review agent actions post-hoc to detect sabotage without incurring runtime latency.&lt;/li&gt;&lt;li&gt;Frames monitor development as a red-team/blue-team adversarial game and evaluates monitors against sabotaging agents (based on Claude 4.1 Opus).&lt;/li&gt;&lt;li&gt;Provides a suite of 5 software-engineering task environments, develops an ensemble monitor achieving 6% false negative at 1% false positive on held-out tests, and proposes a model to extrapolate deployment risk from FN rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asa Cooper Stickland', 'Jan Michelfeit', 'Arathi Mani', 'Charlie Griffin', 'Ollie Matthews', 'Tomek Korbak', 'Rogan Inglis', 'Oliver Makins', 'Alan Cooney']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agents', 'red teaming', 'asynchronous monitoring', 'safety evaluation', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13526</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems</title><link>https://arxiv.org/abs/2512.13460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-EMAR, a framework that autonomously detects and reconstructs transmission-induced weight distortions in federated IoT systems before adding privacy noise.&lt;/li&gt;&lt;li&gt;Integrates Differential Privacy (DP) with Secure Aggregation (SA) to distinguish DP noise from genuine transmission errors and enable in-network repair without breaking confidentiality.&lt;/li&gt;&lt;li&gt;Uses an error model to estimate corruption patterns and apply adaptive corrections, improving convergence stability and robustness under communication corruption.&lt;/li&gt;&lt;li&gt;Evaluated on heterogeneous IoT sensor and graph datasets, reporting near-baseline performance while maintaining (epsilon, delta)-DP guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chethana Prasad Kabgere', 'Shylaja S S']&lt;/li&gt;&lt;li&gt;Tags: ['federated-learning', 'differential-privacy', 'secure-aggregation', 'robustness', 'communication-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13460</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title><link>https://arxiv.org/abs/2512.13352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrates multiple membership inference attack (MIA) techniques into a targeted data-extraction pipeline for LLMs.&lt;/li&gt;&lt;li&gt;Systematically benchmarks the effectiveness of these MIAs when used to verify extracted candidates from model-generated text.&lt;/li&gt;&lt;li&gt;Compares integrated extraction+MIA performance to conventional standalone MIA benchmarks to assess practical utility in real-world extraction scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Al Sahili', 'Ali Chehab', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['membership-inference', 'data-extraction', 'model-memorization', 'privacy', 'LLM-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13352</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</title><link>https://arxiv.org/abs/2512.13337</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FROC, a unified framework that applies a conformal-style, probability-based risk constraint to control machine unlearning in LLMs.&lt;/li&gt;&lt;li&gt;Introduces a continuous risk model combining forgetting deficiency and utility degradation into a single score, and defines Conformal Unlearning Risk (CUR) to estimate the probability that forgotten samples still influence model outputs.&lt;/li&gt;&lt;li&gt;Uses CUR to compute risk-controlled configuration sets that guide hyperparameter selection and compare MU strategies under a user-specified risk budget.&lt;/li&gt;&lt;li&gt;Empirical evaluation shows FROC produces stable, interpretable risk landscapes and reveals trade-offs between unlearning configurations, semantic shift, and utility impact across multiple LLM MU methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Si Qi Goh', 'Yongsen Zheng', 'Ziyao Liu', 'Sami Hormi', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'risk control', 'LLM safety', 'conformal analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13337</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title><link>https://arxiv.org/abs/2512.13316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALIGN-FL: clients selectively share generative components (VAE decoders) instead of full model weights; the server uses synthetic samples for global training.&lt;/li&gt;&lt;li&gt;Implements complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz-regularized VAE decoders to reduce leakage of sensitive outliers.&lt;/li&gt;&lt;li&gt;Targets heterogeneous architectures and extreme Non-IID (cross-silo) scenarios; empirically evaluated on MNIST and Fashion-MNIST with cross-domain outliers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Gulati', 'Benedikt Gro{\\ss}', 'Gerhard Wunder']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning', 'Differential Privacy', 'Generative Models', 'Non-IID Robustness', 'Privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13316</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</title><link>https://arxiv.org/abs/2512.13207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data-poisoning (global bias and patch-based) attacks against federated learning models for surface temperature forecasting using the CERRA dataset, simulating geographically distributed clients.&lt;/li&gt;&lt;li&gt;Finds that a small fraction of poisoned clients can produce large, spatially correlated prediction errors (global bias up to -1.7 K from a single compromised client; patch attacks triple MSE and create persistent regional anomalies &gt;+3.5 K).&lt;/li&gt;&lt;li&gt;Assesses trimmed-mean aggregation as a defense: it mitigates global-bias attacks (2–13% degradation) but dramatically fails for spatially correlated patch attacks (281–603% amplification), highlighting limitations of outlier-based defenses on spatial data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karina Chichifoi', 'Fabio Merizzi', 'Michele Colajanni']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'data poisoning', 'adversarial attacks', 'robust aggregation', 'spatially correlated attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13207</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title><link>https://arxiv.org/abs/2512.13111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hierarchical Approximate Bayesian Neural Network (HABNN) using a Gaussian-inverse-Wishart hyperprior over weights to improve robustness and uncertainty quantification.&lt;/li&gt;&lt;li&gt;Derives closed-form analytical expressions for predictive distributions and weight posteriors (Student's t-distributions) with linear complexity in number of weights.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and more reliable out-of-distribution uncertainty estimates compared to state-of-the-art methods, aiming at safety-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hayk Amirkhanian', 'Marco F. Huber']&lt;/li&gt;&lt;li&gt;Tags: ['bayesian-neural-networks', 'uncertainty-estimation', 'out-of-distribution-detection', 'model-robustness', 'safety-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13111</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Wait, Wait, Wait... Why Do Reasoning Models Loop?</title><link>https://arxiv.org/abs/2512.12895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes why chain-of-thought reasoning models produce looping/repetitive generations, especially at low temperature and with greedy decoding.&lt;/li&gt;&lt;li&gt;Identifies two mechanisms via a synthetic graph reasoning task: (1) risk aversion from learning difficulty causing preference for easy cyclic actions, and (2) an inductive bias in Transformers toward temporally correlated errors that produce repeated actions.&lt;/li&gt;&lt;li&gt;Finds larger models loop less but distilled students loop more; increasing temperature reduces loops by promoting exploration but does not correct underlying learning errors.&lt;/li&gt;&lt;li&gt;Proposes training-time interventions aimed at directly reducing the 'errors in learning' that cause looping.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charilaos Pipis', 'Shivam Garg', 'Vasilis Kontonis', 'Vaishnavi Shrivastava', 'Akshay Krishnamurthy', 'Dimitris Papailiopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['LLM failure modes', 'robustness', 'alignment/safety', 'chain-of-thought', 'training-time interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12895</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2512.12858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts Group Relative Policy Optimization (GRPO) to optimize LLM output consistency across semantically equivalent prompt variants.&lt;/li&gt;&lt;li&gt;Defines entropy-based helpfulness and stability rewards and treats prompt variants as groups with conversational context reset to isolate phrasing effects.&lt;/li&gt;&lt;li&gt;Evaluates on investment and job recommendation tasks, showing GRPO reduces output variability more effectively than fine-tuning and decoding-baseline methods.&lt;/li&gt;&lt;li&gt;Frames variability as a correctable reliability/alignment problem for enterprise deployments requiring invariant information delivery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sonal Prabhune', 'Balaji Padmanabhan', 'Kaushik Dutta']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'consistency', 'reinforcement learning', 'LLM deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12858</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title><link>https://arxiv.org/abs/2512.12840</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses feature inference attacks in vertical federated learning (VFL) where shared confidence scores can leak private features.&lt;/li&gt;&lt;li&gt;Proposes PRIVEE, a transformation of confidence scores that preserves relative ranking and inter-score distances while obfuscating raw scores.&lt;/li&gt;&lt;li&gt;Claims substantial privacy gains (≈3x improvement over SOTA defenses) without degrading model prediction accuracy, validated against advanced reconstruction attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sindhuja Madabushi', 'Ahmad Faraz Khan', 'Haider Ali', 'Ananthram Swami', 'Rui Ning', 'Hongyi Wu', 'Jin-Hee Cho']&lt;/li&gt;&lt;li&gt;Tags: ['feature inference', 'privacy', 'vertical federated learning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12840</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title><link>https://arxiv.org/abs/2512.12827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GradID, a detector that uses the intrinsic dimensionality (ID) of model input gradients to distinguish natural from adversarial examples.&lt;/li&gt;&lt;li&gt;Shows consistent ID differences between clean and adversarial inputs and applies this for both batch-wise group detection and single-sample detection.&lt;/li&gt;&lt;li&gt;Evaluates across multiple datasets (MNIST, SVHN, CIFAR-10, MS COCO) and attacks (including CW and AutoAttack), reporting high detection rates (≳92% on CIFAR-10).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Mahdi Razmjoo', 'Mohammad Mahdi Sharifian', 'Saeed Bagheri Shouraki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-detection', 'gradient-based', 'intrinsic-dimensionality', 'adversarial-attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12827</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain</title><link>https://arxiv.org/abs/2512.12617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Spectral Sentinel, a Byzantine detection and aggregation framework for decentralized federated learning that uses random matrix theory (Marchenko-Pastur eigenspectrum) to distinguish honest Non-IID gradients from Byzantine perturbations.&lt;/li&gt;&lt;li&gt;Uses Frequent Directions sketching to track covariance eigenspectra in compressed O(k^2) memory, enabling scalability to models up to ~1.5B parameters with k &lt;&lt; d.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees under a (σ,f) threat model: (ε,δ)-Byzantine resilience, convergence rate O(σ f / sqrt(T) + f^2 / T), and a matching information-theoretic lower bound Ω(σ f / sqrt(T)).&lt;/li&gt;&lt;li&gt;Implements a blockchain-integrated system (Polygon) and empirically evaluates across 144 attack-aggregator configurations, showing substantial accuracy improvements (78.4% vs. 48–63% baselines).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Animesh Mishra']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine robustness', 'adversarial poisoning', 'random matrix theory', 'scalable defenses / blockchain integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12617</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment</title><link>https://arxiv.org/abs/2512.12387</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies misalignment between Group Relative Policy Optimization (GRPO) and flow-matching image generation, citing poor temporal credit assignment and fading group-relative rewards.&lt;/li&gt;&lt;li&gt;Proposes Value-Anchored Group Policy Optimization (VGPO) which converts sparse terminal rewards into dense, process-aware value estimates for per-timestep credit assignment.&lt;/li&gt;&lt;li&gt;Introduces an absolute-value–based group normalization to preserve an optimization signal as reward diversity decreases, reducing optimization stagnation and reward hacking.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art image quality and improved task-specific accuracy on three benchmarks, claiming better alignment and mitigation of reward-hacking behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yawen Shao', 'Jie Xiao', 'Kai Zhu', 'Yu Liu', 'Wei Zhai', 'Yang Cao', 'Zheng-Jun Zha']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-hacking', 'policy-optimization', 'image-generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12387</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty Quantification for Machine Learning: One Size Does Not Fit All</title><link>https://arxiv.org/abs/2512.12341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues there is no single best uncertainty measure; uncertainty quantification should be tailored to the application.&lt;/li&gt;&lt;li&gt;Uses a flexible family of uncertainty measures that separate total, aleatoric, and epistemic uncertainty for second-order distributions.&lt;/li&gt;&lt;li&gt;Shows different proper scoring rules (losses) yield different characteristics and that matching the scoring rule to the task improves performance (e.g., selective prediction).&lt;/li&gt;&lt;li&gt;Empirical findings: mutual information works best for OOD detection; epistemic uncertainty with zero-one loss performs best for active learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paul Hofman', 'Yusuf Sale', 'Eyke H\\"ullermeier']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty-quantification', 'epistemic-aleatoric-decomposition', 'out-of-distribution-detection', 'selective-prediction', 'active-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12341</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation</title><link>https://arxiv.org/abs/2512.12086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cloak, a data obfuscation framework using latent diffusion guided by contrastive-learned disentangled representations to hide private attributes while preserving utility.&lt;/li&gt;&lt;li&gt;Targets time-series sensor data and facial images, enabling users to navigate privacy-utility trade-offs with minimal retraining and suitability for resource-constrained IoT devices.&lt;/li&gt;&lt;li&gt;Claims superior performance to prior obfuscation methods across multiple datasets, leveraging contrastive guidance to direct the latent diffusion process toward utility retention and privacy preservation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Yang', 'Omid Ardakanian']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data-obfuscation', 'latent-diffusion', 'contrastive-learning', 'attribute-inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12086</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of safety refusal stability across 4 instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts using 20 sampling configs (4 temperatures × 5 seeds).&lt;/li&gt;&lt;li&gt;Finds 18–28% of prompts flip between refusal and compliance depending on seed/temperature; Safety Stability Index (SSI) drops significantly with higher temperature (mean SSI 0.951 at temp 0.0 → 0.896 at temp 1.0).&lt;/li&gt;&lt;li&gt;Validates results with an external judge (Claude 3.5 Haiku) showing 89% agreement (Cohen's kappa = 0.62); single-shot evaluations match multi-sample ground truth only 92.4% of the time, leading to recommendation of ≥3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'safety evaluation', 'sampling randomness', 'temperature robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning</title><link>https://arxiv.org/abs/2512.12022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DFedReweighting, a unified aggregation framework for decentralized federated learning that performs objective-oriented reweighting at the final aggregation step of each round.&lt;/li&gt;&lt;li&gt;Computes preliminary weights from target performance metrics derived from auxiliary datasets constructed from local data, then refines them via customized reweighting strategies to produce final aggregation weights.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis showing linear convergence under appropriate choices and empirical results demonstrating improved fairness and robustness against Byzantine attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaichuang Zhang', 'Wei Yin', 'Jinghao Yang', 'Ping Xu']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'decentralized federated learning', 'Byzantine robustness', 'aggregation reweighting', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12022</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Extract Context for Context-Aware LLM Inference</title><link>https://arxiv.org/abs/2512.11986</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an RL-based, autoencoder-like context generator that extracts contextual signals from user prompts to guide LLM responses.&lt;/li&gt;&lt;li&gt;Targets safety/usability trade-offs: aims to reduce harmful outputs while avoiding unnecessary refusals of benign but ambiguous prompts.&lt;/li&gt;&lt;li&gt;Evaluated on safety/jailbreak benchmarks (SafetyInstruct, XSTest, WildJailbreak), reporting reduced harmful responses and improved attack-success/compliance metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseon Kim', 'Lucas Caccia', 'Zhengyan Shi', 'Matheus Pereira', "Marc-Alexandre C\\^ot\\'e", 'Xingdi Yuan', 'Alessandro Sordoni']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking / red teaming', 'prompt engineering / context extraction', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11986</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors</title><link>https://arxiv.org/abs/2512.11949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finetuning LLMs can produce 'Neural Chameleons' that, when prompted with a trigger like 'You are being probed for {concept}', learn to evade activation monitors zero-shot — including for unseen, safety-relevant concepts (e.g., 'deception').&lt;/li&gt;&lt;li&gt;Evasion generalizes across model families (Llama, Gemma, Qwen), succeeds even against monitors trained post hoc on frozen model weights, is selective to the triggered concept, and only modestly impacts standard benchmark performance.&lt;/li&gt;&lt;li&gt;Mechanistic analysis (case study on Gemma-2-9b-it) shows evasion shifts activations into a low-dimensional subspace; stronger defenses (monitor ensembles, non-linear classifiers) improve robustness but do not fully eliminate the evasion.&lt;/li&gt;&lt;li&gt;Provides a proof-of-concept attack and an evaluation tool for worst-case robustness of activation-monitoring approaches under a misalignment threat model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Max McGuinness', 'Alex Serrano', 'Luke Bailey', 'Scott Emmons']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'activation monitoring', 'evasion/adversarial attacks', 'model interpretability/mechanistic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11949</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title><link>https://arxiv.org/abs/2512.11867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the effects of repeatedly training models on synthetically generated data (bootstrapping) in continual learning settings.&lt;/li&gt;&lt;li&gt;Presents a statistical analysis showing synthetic data introduces significant bias and variance that weaken maximum likelihood estimation.&lt;/li&gt;&lt;li&gt;Provides empirical evidence that popular generative models degrade/collapse under repeated training with synthetic data and that Generative Experience Replay (GER) methods fail to maintain latent alignment.&lt;/li&gt;&lt;li&gt;Quantifies degradation and raises concerns about using synthetic data for continual learning and related workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniil Zverev', 'A. Sophia Koepke', 'Joao F. Henriques']&lt;/li&gt;&lt;li&gt;Tags: ['continual-learning', 'synthetic-data', 'generative-model-collapse', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11867</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space</title><link>https://arxiv.org/abs/2512.11838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues DPO does not rewrite internal beliefs but acts as a low-rank steering mechanism that nudges activations along a few preference directions.&lt;/li&gt;&lt;li&gt;Derives that the DPO gradient depends on the difference between logit embeddings of preferred vs dispreferred completions, implying a first-order shift in final hidden representations.&lt;/li&gt;&lt;li&gt;Empirically extracts a steering vector: adding it to base activations reproduces aligned behavior, subtracting it nearly restores the original model.&lt;/li&gt;&lt;li&gt;Spectral analysis shows rank-one dominance and entropy collapse in upper layers, supporting a 'behavioral illusion' view of alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samarth Raina', 'Saksham Aggarwal', 'Aman Chadha', 'Vinija Jain', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'model interpretability', 'activation steering', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11838</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem</title><link>https://arxiv.org/abs/2512.08290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a Systematization of Knowledge (SoK) that taxonomizes security and safety risks in the Model Context Protocol (MCP) ecosystem, separating adversarial threats (e.g., indirect prompt injection, tool poisoning) from epistemic safety hazards (e.g., alignment failures).&lt;/li&gt;&lt;li&gt;Analyzes structural vulnerabilities of MCP primitives—Resources, Prompts, and Tools—and demonstrates how context can be weaponized to cause unauthorized operations in multi-agent environments.&lt;/li&gt;&lt;li&gt;Surveys defenses (e.g., cryptographic provenance/ETDI, runtime intent verification) and offers a roadmap for securing the shift from chatbots to autonomous agentic operating systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiva Gaire', 'Srijan Gyawali', 'Saroj Mishra', 'Suman Niroula', 'Dilip Thakur', 'Umesh Yadav']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'prompt injection', 'tool poisoning', 'model context protocol', 'security taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.08290</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Protecting Bystander Privacy via Selective Hearing in Audio LLMs</title><link>https://arxiv.org/abs/2512.06380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SH-Bench, a benchmark of 3,968 multi-speaker audio mixtures (77k QA items) to evaluate 'selective hearing'—models attending to an intended speaker while refusing to reveal bystander speech.&lt;/li&gt;&lt;li&gt;Proposes Selective Efficacy (SE), a metric combining main-speaker comprehension and bystander-privacy protection.&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art audio LLMs, finding significant bystander privacy leakage despite strong audio understanding.&lt;/li&gt;&lt;li&gt;Presents Bystander Privacy Fine-Tuning (BPFT), a training pipeline that substantially improves refusal to answer bystander-related queries without degrading main-speaker comprehension (e.g., +47% bystander accuracy, +16% SE vs. best baseline).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Zhan', 'Guangzhi Sun', 'Jose Such', 'Phil Woodland']&lt;/li&gt;&lt;li&gt;Tags: ['audio-llm', 'privacy', 'benchmarking', 'defense/fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.06380</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Trusted AI Agents in the Cloud</title><link>https://arxiv.org/abs/2512.05951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Omega, a system for hosting trusted AI agents in the cloud with end-to-end isolation across Confidential VMs and Confidential GPUs, using nested isolation to host many agents in a single CVM.&lt;/li&gt;&lt;li&gt;Establishes cross-principal trust via differential attestation and provides a policy framework to supervise external interactions, tool usage, data access, and inter-agent communication with accountable provenance.&lt;/li&gt;&lt;li&gt;Implements and evaluates Omega on AMD SEV-SNP and NVIDIA H100, claiming full protection of agent state across CVM-GPU and high-performance, high-density, policy-compliant multi-agent deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teofil Bodea', 'Masanori Misono', 'Julian Pritzi', 'Patrick Sabanic', 'Thore Sommer', 'Harshavardhan Unnibhavi', 'David Schall', 'Nuno Santos', 'Dimitrios Stavrakakis', 'Pramod Bhatotia']&lt;/li&gt;&lt;li&gt;Tags: ['confidential computing', 'attestation', 'isolation', 'agent security', 'policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05951</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title><link>https://arxiv.org/abs/2512.01054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive-lambda SISS: treats the mixing weight lambda as a latent variable inferred per training step via a lightweight inference network conditioned on loss/gradient features, optimized with a variational objective.&lt;/li&gt;&lt;li&gt;Extends adaptive-lambda approach to score-based unlearning and proposes a multi-class Score Forgetting Distillation and a hybrid objective combining SISS and Score Forgetting Distillation.&lt;/li&gt;&lt;li&gt;Presents a Reinforcement Learning formulation framing unlearning as a sequential decision process to learn policies over the model's memory state.&lt;/li&gt;&lt;li&gt;Empirical results on an augmented MNIST benchmark show improved forgetting of target classes while better preserving retention-set generation quality compared to static-lambda SISS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['MohammadParsa Dini', 'Human Jafari']&lt;/li&gt;&lt;li&gt;Tags: ['machine-unlearning', 'data-privacy', 'diffusion-models', 'unlearning-algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.01054</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title><link>https://arxiv.org/abs/2511.16020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a sequence-level optimization framework to generate printable adversarial textures for clothing that remain effective across entire walking video sequences under motion, pose, deformation, and lighting changes.&lt;/li&gt;&lt;li&gt;Maps product images to UV space and parameterizes textures via a compact palette and control points with ICC color locking to ensure printability; uses a physically based human-garment simulator to model cloth dynamics and multi-angle viewpoints.&lt;/li&gt;&lt;li&gt;Optimizes an expectation-over-transformation objective with temporal weighting to minimize detector confidence across sequences; demonstrates strong digital and real-world effectiveness with sublimation-printed garments and cross-model transferability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dingkun Zhou', 'Patrick P. K. Chan', 'Hengxu Wu', 'Shikang Zheng', 'Ruiqi Huang', 'Yuanjie Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'physical attacks', 'human detection', 'robustness', 'video/sequence-level']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.16020</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Lipschitz-aware Linearity Grafting for Certified Robustness</title><link>https://arxiv.org/abs/2510.25130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides theoretical analysis showing that grafting linearity into nonlinear activations tightens the l_infty local Lipschitz constant, linking this to improved certified robustness.&lt;/li&gt;&lt;li&gt;Frames linearity grafting as a method to remove dominant approximation errors in neural network verification (rather than only reducing unstable neurons).&lt;/li&gt;&lt;li&gt;Proposes a Lipschitz-aware linearity grafting method and empirically demonstrates tighter local Lipschitz bounds and improved certified robustness without requiring certified training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongjin Han', 'Suhyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial robustness', 'Lipschitz constant', 'neural network verification', 'activation functions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.25130</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title><link>https://arxiv.org/abs/2510.21118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an annotation framework with an intermediate 'Out-Dependent' category to reduce ambiguity in faithfulness labels when external knowledge is required for verification.&lt;/li&gt;&lt;li&gt;Builds VeriGray, a new benchmark for unfaithfulness detection in summarization, and reports that SOTA LLMs (e.g., GPT-5) still produce ~6% hallucinated sentences and ~8% Out-Dependent sentences on average.&lt;/li&gt;&lt;li&gt;Evaluates baseline methods on VeriGray and finds the benchmark challenging, highlighting gaps in current faithfulness detection approaches and room for improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Ding', 'Lvzhou Luo', 'Yixuan Cao', 'Ping Luo']&lt;/li&gt;&lt;li&gt;Tags: ['faithfulness', 'hallucination detection', 'benchmark', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.21118</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title><link>https://arxiv.org/abs/2510.17904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BreakFun, a jailbreak that leverages LLMs' strict adherence to structured schemas by embedding a 'Trojan Schema' within prompts to force harmful outputs.&lt;/li&gt;&lt;li&gt;Reports high transferability and effectiveness (average 89% success across 13 models; 100% ASR on some models) and an ablation study attributing causality to the Trojan Schema.&lt;/li&gt;&lt;li&gt;Proposes a defensive technique, Adversarial Prompt Deconstruction, using a secondary LLM to perform Literal Transcription to reveal hidden harmful intent; proof-of-concept shows strong mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirkia Rafiei Oskooei', 'Mehmet S. Aktas']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'red teaming', 'defenses/guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17904</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title><link>https://arxiv.org/abs/2510.12859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for three lenses on AI — risk (nuclear-like irreversible externalities), transformation (industrial-revolution-style general-purpose tech), and continuity (extension of computing revolutions) — to capture both evolutionary and revolutionary aspects.&lt;/li&gt;&lt;li&gt;Identifies recurring dynamics (democratization at usage, concentration at production, falling costs, personalization) and analyzes sectoral impacts (accounting, law, education, translation, advertising, software engineering) as routine cognition is commoditized.&lt;/li&gt;&lt;li&gt;Highlights frontier safety concerns: need for robust guardrails, mechanisms for moral generalization, governance of emergent multi-agent dynamics, and policies coupling pro-innovation strategies with safety governance to manage singularity-class tail risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masoud Makrehchi']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'risk assessment', 'governance', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12859</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title><link>https://arxiv.org/abs/2510.07620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DGTEN, a graph neural network that models nodes/edges as Gaussian distributions to propagate semantic signals with calibrated epistemic uncertainty for trust evaluation.&lt;/li&gt;&lt;li&gt;Incorporates temporal modeling (hybrid positional encoding, Kolmogorov-Arnold multi-head attention, ODE-based residual module) to capture abrupt and smooth trust dynamics in evolving graphs.&lt;/li&gt;&lt;li&gt;Implements robust defenses—adaptive ensemble coefficient analysis using cosine and Jaccard similarity—to detect/down-weight suspicious interactions and mitigate reputation laundering, sabotage, and on-off attacks.&lt;/li&gt;&lt;li&gt;Reports empirical gains on dynamic signed Bitcoin trust networks, including improved MCC in single-timeslot prediction, cold-start scenarios, and under adversarial on-off attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Usman', 'Yugyung Lee']&lt;/li&gt;&lt;li&gt;Tags: ['graph neural networks', 'adversarial robustness', 'trust evaluation', 'uncertainty quantification', 'dynamic/temporal graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07620</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</title><link>https://arxiv.org/abs/2510.02967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Retrieval-Augmented Generation (RAG) system to query UK NICE clinical guidelines, using a hybrid embedding retrieval architecture over 10,195 text chunks.&lt;/li&gt;&lt;li&gt;Retrieval performance: MRR=0.814, Recall@1=81%, Recall@10=99.1% on 7,901 queries; RAG substantially improved generation faithfulness on a 70 Q/A test set.&lt;/li&gt;&lt;li&gt;RAG-enhanced models increased faithfulness to 99.5% (O4-Mini) versus 43% for a medical-focused baseline; clinical SME evaluation (7 experts) showed GPT-4.1 reached 98.7% accuracy and reduced unsafe responses by 67%.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Lewis', 'Samuel Thio', 'Amy Roberts', 'Catherine Siju', 'Whoasif Mukit', 'Rebecca Kuruvilla', 'Zhangshu Joshua Jiang', 'Niko M\\"oller-Grell', 'Aditya Borakati', 'Richard JB Dobson', 'Spiros Denaxas']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'Faithfulness', 'Safety evaluation', 'Clinical LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02967</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ConceptGuard: Neuro-Symbolic Safety Guardrails via Sparse Interpretable Jailbreak Concepts</title><link>https://arxiv.org/abs/2508.16325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ConceptGuard, a framework using Sparse Autoencoders to extract interpretable latent concepts from LLM internal activations that correlate with jailbreak themes.&lt;/li&gt;&lt;li&gt;Uses these semantically meaningful internal representations to build explainable, neuro-symbolic safety guardrails that aim to block jailbreak behavior without further fine-tuning or degrading capabilities.&lt;/li&gt;&lt;li&gt;Provides empirical/analytical evidence for a shared activation geometry of jailbreak attacks, suggesting generalizable defenses grounded in mechanistic interpretability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', "C\\'eline Hudelot"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'interpretability', 'defenses', 'mechanistic interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16325</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System</title><link>https://arxiv.org/abs/2508.15811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-stage pipeline for generative query suggestion: prompt engineering, supervised fine-tuning via distillation from click logs, Gaussian Reward Model (GaRM) capturing preference uncertainty, and reinforcement learning with a composite reward.&lt;/li&gt;&lt;li&gt;Introduces GaRM to model user preferences as probability distributions rather than point estimates and integrates auxiliary heuristics to mitigate reward hacking.&lt;/li&gt;&lt;li&gt;Adds out-of-distribution regularization and a two-stage reward fusion technique to stabilize training and reduce misalignment.&lt;/li&gt;&lt;li&gt;Reports improved automatic/human evaluation metrics and a 34% relative increase in click-through rate in live A/B testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhao Yin', 'Haolin Wang', 'Peng Bao', 'Ju Xu', 'Yongliang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reward-modeling', 'reinforcement-learning', 'reward-hacking', 'query-suggestion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15811</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ALIGN: Word Association Learning for Cultural Alignment in Large Language Models</title><link>https://arxiv.org/abs/2508.13426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes fine-tuning LLMs on native speakers' word-association norms to impart cultural knowledge and improve cultural alignment.&lt;/li&gt;&lt;li&gt;Fine-tunes Llama-3.1-8B and Qwen-2.5-7B (supervised and preference optimization) using English (US) and Mandarin (China) word-association datasets.&lt;/li&gt;&lt;li&gt;Reports substantial gains in lexical alignment (Precision@5) and measurable shifts in high-level cultural value responses, with small 7–8B models matching or exceeding vanilla 70B baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunhua Liu', 'Kabir Manandhar Shrestha', 'Sukai Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'cultural alignment', 'LLMs', 'fine-tuning', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13426</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title><link>https://arxiv.org/abs/2508.11009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SproutBench, a benchmark of 1,283 developmentally grounded adversarial prompts targeting safety risks for ages 0–18 (emotional dependency, privacy leaks, imitation of hazardous behaviors, etc.).&lt;/li&gt;&lt;li&gt;Evaluates 47 diverse LLMs with the suite, revealing substantive safety vulnerabilities and empirical correlations (e.g., Safety vs. Risk Prevention, inverse Interactivity vs. Age Appropriateness).&lt;/li&gt;&lt;li&gt;Provides practical guidelines for child-centric AI design, deployment, and mitigation strategies based on benchmark findings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Lanyi Wei', 'Haixiao Hu', 'Jingyi Yu', 'Rongchang Li', 'Mohan Li', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'child-safety', 'benchmark', 'adversarial-prompts', 'LLM-alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11009</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</title><link>https://arxiv.org/abs/2507.05622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a taxonomy for dataset auditing methods (internal features vs external features) and formalizes two attack goals: evasion (hide dataset usage) and forgery (falsely implicate a dataset).&lt;/li&gt;&lt;li&gt;Proposes systematic attack strategies (decoupling, removal, detection for evasion; adversarial-example-based forgery) and implements 17 evasion + 5 forgery attacks.&lt;/li&gt;&lt;li&gt;Presents DATABench, a benchmark evaluating 9 representative auditing methods under these adversarial scenarios, with code released.&lt;/li&gt;&lt;li&gt;Finds that existing auditing methods lack robustness/distinctiveness under adversarial manipulation, highlighting the need for secure auditing techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Shao', 'Yiming Li', 'Mengren Zheng', 'Zhiyang Hu', 'Yukun Chen', 'Boheng Li', 'Yu He', 'Junfeng Guo', 'Dacheng Tao', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['dataset auditing', 'adversarial attacks', 'evasion attacks', 'benchmarking', 'forgery attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05622</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</title><link>https://arxiv.org/abs/2507.00057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'incoherence', an oracle-less measure to estimate the probability that an LLM-generated program is incorrect and to establish a lower bound on error.&lt;/li&gt;&lt;li&gt;Method efficiently identifies roughly two-thirds of incorrect programs on average with no reported false positives, according to the authors' experiments.&lt;/li&gt;&lt;li&gt;Finds strong agreement between incoherence-based rankings and oracle-based rankings (pass@1), suggesting incoherence can substitute for oracle-based evaluation in LLM code generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Valentin', 'Ardi Madadi', 'Gaetano Sapia', 'Marcel B\\"ohme']&lt;/li&gt;&lt;li&gt;Tags: ['safety-evaluation', 'robustness', 'LLM-code-generation', 'oracle-less-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00057</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title><link>https://arxiv.org/abs/2506.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a unified end-to-end threat model for LLM-powered agent ecosystems, covering host-to-tool and agent-to-agent interactions.&lt;/li&gt;&lt;li&gt;Systematically categorizes 30+ attack techniques (input manipulation, model compromise, system/privacy attacks, protocol-level vulnerabilities) with formal threat formulations.&lt;/li&gt;&lt;li&gt;Analyzes attack feasibility, reviews defenses (dynamic trust management, cryptographic provenance, sandboxed interfaces), and validates the framework against real incidents and CVE/NVD mappings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Amine Ferrag', 'Norbert Tihanyi', 'Djallel Hamouda', 'Leandros Maglaras', 'Abderrahmane Lakas', 'Merouane Debbah']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'agentic AI security', 'protocol vulnerabilities', 'security taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23260</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title><link>https://arxiv.org/abs/2506.21127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a meta-policy switching framework that adaptively selects among an ensemble of robust RL policies to counter unknown/out-of-distribution adversarial sensor perturbations.&lt;/li&gt;&lt;li&gt;Uses discounted Thompson sampling to frame policy selection as a multi-armed bandit problem, with theoretical regret bounds and claims of emergent antifragility.&lt;/li&gt;&lt;li&gt;Evaluates in 3D UAV navigation scenarios under white-box (PGD) and black-box (GPS spoofing) attacks, showing improved navigation efficiency and conflict-free trajectory rates versus robust and vanilla baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'reinforcement learning security', 'UAV/autonomous systems security', 'online defense / bandit-based adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21127</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title><link>https://arxiv.org/abs/2505.24592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a theoretical condition (via flat minima and a generalization bound) under which label-preserving augmentations improve robustness to diverse distribution shifts, including adversarial attacks.&lt;/li&gt;&lt;li&gt;Framework is general to all label-preserving augmentations rather than tailored to specific shifts.&lt;/li&gt;&lt;li&gt;Empirical validation on CIFAR and ImageNet for common corruptions and adversarial robustness benchmarks supports the theoretical claims.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weebum Yoo', 'Sung Whan Yoon']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'data augmentation', 'flat minima', 'generalization theory', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24592</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models</title><link>https://arxiv.org/abs/2505.19700</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Alignment Model (RAM): formalizes alignment as importance sampling where the pretrained (unaligned) model is the proposal and a detached autoregressive alignment module estimates importance weights.&lt;/li&gt;&lt;li&gt;Derives a sequence-level training strategy allowing the alignment module to be trained independently of the base model, enabling modularity and scalability.&lt;/li&gt;&lt;li&gt;Introduces a resampling algorithm with iterative token-level decoding to mitigate first-token latency, and shows empirical gains on instruction following, domain adaptation, and preference optimization across two open-source LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Dianqing Liu', 'Mingye Zhu', 'Junbo Guo', 'Yongdong Zhang', 'Zhendong Mao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'importance-sampling', 'adapter-modules', 'preference-optimization', 'instruction-following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19700</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</title><link>https://arxiv.org/abs/2504.03494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical, distributional robustness definition and standardized robustness score tailored to industrial CPS forecasting.&lt;/li&gt;&lt;li&gt;Presents a systematic benchmarking framework that simulates realistic disturbances (sensor drift, noise, irregular sampling) for robustness evaluation.&lt;/li&gt;&lt;li&gt;Empirically evaluates a wide range of deep learning forecasting architectures on real-world CPS datasets and releases the benchmark publicly.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Windmann', 'Henrik Steude', 'Daniel Boschmann', 'Oliver Niggemann']&lt;/li&gt;&lt;li&gt;Tags: ['robustness evaluation', 'distributional robustness', 'cyber-physical systems', 'time-series forecasting', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03494</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://arxiv.org/abs/2503.19041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LookAhead Tuning, a data-driven fine-tuning method that previews partial answer prefixes in training data to reduce perturbation of the model's initial token distributions.&lt;/li&gt;&lt;li&gt;Aims to preserve pre-existing safety/alignment behaviors of LLMs during downstream fine-tuning by maintaining built-in safety mechanisms.&lt;/li&gt;&lt;li&gt;Claims to be lightweight and effective, maintaining model safety while retaining strong downstream task performance per reported experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safe-fine-tuning', 'alignment-preservation', 'LLM-safety', 'training-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.19041</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmarks: On The False Promise of AI Regulation</title><link>https://arxiv.org/abs/2501.15693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI safety benchmarks do not reflect real-world deployed model performance and thus are insufficient grounds for regulation.&lt;/li&gt;&lt;li&gt;Identifies lack of interpretability as a core reason why benchmark performance fails to predict deployed risks and harms.&lt;/li&gt;&lt;li&gt;Proposes a regulatory framework that does not rely on benchmarks and calls for interdisciplinary collaboration to address deployment safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Stanovsky', 'Renana Keydar', 'Gadi Perl', 'Eliya Habba']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Regulation', 'Benchmarks', 'Interpretability', 'Policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15693</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title><link>https://arxiv.org/abs/2512.10449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies adversarial PDF manipulation (indirect prompt injection) targeting LLM-based scientific reviewers to flip "Reject" decisions to "Accept".&lt;/li&gt;&lt;li&gt;Introduces a new metric (WAVS) to quantify vulnerability and curates a dataset of 200 papers with 15 domain-specific attack strategies.&lt;/li&gt;&lt;li&gt;Evaluates attacks across 13 large language models (including GPT-5, Claude Haiku, DeepSeek) and finds obfuscation strategies can cause high decision-flip rates; promises dataset and framework release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Devanshu Sahoo', 'Manish Prasad', 'Vasudev Majhi', 'Jahnvi Singh', 'Vinay Chamola', 'Yash Sinha', 'Murari Mandal', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.10449</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title><link>https://arxiv.org/abs/2512.05356</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Advocates for maximizing co-improvement: structured collaboration between human researchers and AIs to jointly conduct AI research (from ideation to experimentation).&lt;/li&gt;&lt;li&gt;Argues that centering human research improvement in the loop yields a faster and safer route toward superintelligence than purely autonomous self-improvement.&lt;/li&gt;&lt;li&gt;Positions human-AI symbiosis as a means to endow both parties with safer, more aligned superintelligence, emphasizing governance and design choices that facilitate safe cooperation.&lt;/li&gt;&lt;li&gt;Primarily a conceptual/position paper proposing strategy and priorities rather than providing empirical attacks, red-team evaluations, or technical defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jason Weston', 'Jakob Foerster']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'human-AI collaboration', 'co-improvement', 'AI strategy/position']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.05356</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</title><link>https://arxiv.org/abs/2512.03005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using LLMs as mediators in online conflicts by decomposing mediation into judgment (assessing fairness/emotional dynamics) and steering (generating de-escalatory, empathetic messages).&lt;/li&gt;&lt;li&gt;Builds a large Reddit-based dataset and a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison to assess mediation quality.&lt;/li&gt;&lt;li&gt;Finds that API-based models outperform open-source counterparts in both reasoning (judgment) and intervention alignment (steering), while highlighting limitations and future challenges for LLM-mediated conflict resolution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dawei Li', 'Abdullah Alnaibari', 'Muhammad Arslan', 'Manny Sandoval', 'Deborah Hall', 'Yasin Silva', 'Huan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'harm mitigation', 'alignment', 'evaluation/benchmarking', 'online moderation/mediation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.03005</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty</title><link>https://arxiv.org/abs/2511.21569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical audit of self-transparency (honest disclosure of AI/limitations) when LLMs are assigned professional personas; 16 open-weight models tested across 19,200 trials.&lt;/li&gt;&lt;li&gt;Finds large persona- and model-dependent variance in disclosure rates (e.g., Neurosurgeon 3.5% vs Financial Advisor 30.8%; model identity matters more than size).&lt;/li&gt;&lt;li&gt;Shows suppression of disclosure is driven by instruction-following / role-consistency rather than capability limits; explicit permission to disclose greatly increases honesty.&lt;/li&gt;&lt;li&gt;Implication: safety/transparency properties do not transfer across deployment domains — requires deliberate behavior design, testing, and verification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Diep']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'transparency/honesty', 'instruction-following', 'evaluation/benchmarking', 'persona-based vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2511.21569</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title><link>https://arxiv.org/abs/2510.14925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes a stability-based measure (H-Risk) in linear-Gaussian state-space models linking nominal stability to overconfident errors and degraded closed-loop behavior.&lt;/li&gt;&lt;li&gt;Proposes a domain-wise proxy for LLM stability based on confidence fluctuations and demonstrates that allowing 'cannot judge' responses reduces policy-aware squared loss in high-stakes binary tasks.&lt;/li&gt;&lt;li&gt;Finds that confidently wrong LLM outputs are locally stable (no instability gap) and interprets hallucinations as stable, high-confidence attractors resistant to output-only fixes like temperature scaling or resampling.&lt;/li&gt;&lt;li&gt;Argues for process-level interventions that perturb and re-evaluate model inference trajectories, motivated by spectral and activation analyses (e.g., Qwen-2.5 exhibiting high SNR, low effective signal temperature).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akira Okutomi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM calibration', 'hallucination', 'robustness', 'safety/alignment', 'mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14925</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title><link>https://arxiv.org/abs/2505.14479</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neuro-symbolic pipeline combining retrieval of analogous proofs and a formal verifier to guide and correct LLM-generated mathematical proofs (geometry focus).&lt;/li&gt;&lt;li&gt;Uses retrieved analogous problem proofs to condition generation and a verifier to provide feedback for iterative correction.&lt;/li&gt;&lt;li&gt;Reports substantial accuracy gains for OpenAI's o1 model (58%–70% improvement) with both retrieval and verifier contributing.&lt;/li&gt;&lt;li&gt;Argues that LLMs producing provably correct conclusions can improve reliability and trustworthiness for critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oren Sultan', 'Eitan Stern', 'Dafna Shahaf']&lt;/li&gt;&lt;li&gt;Tags: ['neuro-symbolic', 'formal verification', 'LLM reliability', 'proof generation', 'safety-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14479</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Traitors: Deception and Trust in Multi-Agent Language Model Simulations</title><link>https://arxiv.org/abs/2505.12923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces The Traitors, a multi-agent simulation framework (inspired by social deduction games) to probe deception, trust formation, and strategic communication among LLM agents with asymmetric information.&lt;/li&gt;&lt;li&gt;Provides formal grounding (game theory, behavioral economics, social cognition), a suite of evaluation metrics for deception success, trust dynamics, and collective inference quality, and a fully autonomous platform with persistent memory and heterogeneous agent traits.&lt;/li&gt;&lt;li&gt;Empirical experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o show advanced models can exhibit strong deceptive capabilities while remaining vulnerable to others' falsehoods, suggesting deception may scale faster than detection—implications for alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedro M. P. Curvo']&lt;/li&gt;&lt;li&gt;Tags: ['deception', 'multi-agent simulation', 'alignment', 'safety evaluation', 'social engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12923</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title><link>https://arxiv.org/abs/2512.13641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Constructed MangoLeafDB-C by applying 19 types of artificial image corruptions at five severity levels to the MangoLeafDB dataset for robustness evaluation.&lt;/li&gt;&lt;li&gt;Benchmarked five CNNs (ResNet-50, ResNet-101, VGG-16, Xception, and a lightweight LCNN) using F1, corruption error (CE), and relative mean corruption error (relative mCE).&lt;/li&gt;&lt;li&gt;Found the lightweight LCNN achieved lower mCE and was more robust to real-world corruptions (e.g., Defocus Blur, Motion Blur) than larger architectures, while ResNet-101 showed large degradation under corruption despite high clean accuracy.&lt;/li&gt;&lt;li&gt;Concludes that specialized lightweight models may be better for robust edge deployment in agriculture and highlights the need to include robustness assessments in development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Vitorino de Andrade', 'Saulo Roberto dos Santos', 'Itallo Patrick Castro Alves da Silva', 'Emanuel Adler Medeiros Pereira', 'Erick de Andrade Barboza']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'corruption-robustness', 'benchmarking', 'agricultural-AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13641</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title><link>https://arxiv.org/abs/2512.13583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DP-CSGP, a differentially private stochastic gradient push algorithm with compressed communication for decentralized learning over directed graphs.&lt;/li&gt;&lt;li&gt;Proves (ε, δ)-DP guarantees per node and derives a utility bound O(√(d log(1/δ)) / (√n J ε)) for nonconvex smooth objectives, matching decentralized methods with exact communication.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that DP-CSGP maintains comparable accuracy under the same privacy budget while significantly reducing communication cost versus exact-communication baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehan Zhu', 'Heng Zhao', 'Yan Huang', 'Joey Tianyi Zhou', 'Shouling Ji', 'Jinming Xu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'decentralized learning', 'communication compression', 'privacy-preserving ML', 'stochastic optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13583</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title><link>https://arxiv.org/abs/2512.13568</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an information-theoretic framework measuring neural network "superposition" (effective degrees of freedom) by applying Shannon entropy to sparse autoencoder activations to estimate how many 'virtual neurons' are encoded.&lt;/li&gt;&lt;li&gt;Validates the metric on toy models, algorithmic tasks, and real models (layer-wise patterns on Pythia-70M), and shows sensitivity to interventions like dropout and dynamics like grokking.&lt;/li&gt;&lt;li&gt;Investigates connections between superposition and adversarial robustness: adversarial training can increase effective features while improving robustness, and the relationship depends on task complexity and model capacity (abundance vs scarcity regimes).&lt;/li&gt;&lt;li&gt;Frames superposition as lossy compression, providing a principled measure linking representation organization under capacity constraints to adversarial vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonard Bereska', 'Zoe Tzifa-Kratira', 'Reza Samavi', 'Efstratios Gavves']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'representation learning', 'superposition', 'adversarial training', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13568</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS</title><link>https://arxiv.org/abs/2512.13501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adaptive Feature Poisoning (AFP), a proactive, lightweight defense that injects dynamic, context-aware perturbations into selected traffic features to corrupt attackers' black-box feedback loop.&lt;/li&gt;&lt;li&gt;AFP uses traffic profiling, change-point detection, and adaptive scaling to identify and perturb features likely exploited by probing attackers while aiming to preserve IDS detection performance.&lt;/li&gt;&lt;li&gt;Evaluated against realistic black-box strategies (silent probing, transferability-based attacks, decision-boundary attacks) and shown to reduce attack effectiveness while maintaining detection utility.&lt;/li&gt;&lt;li&gt;Claims generalizability and undetectability by continuously adapting perturbations and avoiding static transformations that degrade real-time IDS performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sabrine Ennaji', 'Elhadj Benkhelifa', 'Luigi Vincenzo Mancini']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-defense', 'black-box-attacks', 'intrusion-detection-systems', 'proactive-mitigation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13501</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title><link>https://arxiv.org/abs/2512.13325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Implements and evaluates ten Unicode text watermarking methods in a controlled testbed across six large language models (GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, Gemini 2.5 Pro).&lt;/li&gt;&lt;li&gt;Finds that state-of-the-art reasoning LLMs can detect the presence of watermarks, but none of the tested models can reliably extract the watermark without access to implementation/source-code details.&lt;/li&gt;&lt;li&gt;Discusses security implications and outlines future research directions to improve watermark robustness and undetectability against modern LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Malte Hellmeier']&lt;/li&gt;&lt;li&gt;Tags: ['text watermarking', 'watermark detection', 'LLM security', 'watermark extraction', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13325</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title><link>https://arxiv.org/abs/2512.13316</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ALIGN-FL: clients selectively share generative components (VAE decoders) instead of full model weights; the server uses synthetic samples for global training.&lt;/li&gt;&lt;li&gt;Implements complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz-regularized VAE decoders to reduce leakage of sensitive outliers.&lt;/li&gt;&lt;li&gt;Targets heterogeneous architectures and extreme Non-IID (cross-silo) scenarios; empirically evaluated on MNIST and Fashion-MNIST with cross-domain outliers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mayank Gulati', 'Benedikt Gro{\\ss}', 'Gerhard Wunder']&lt;/li&gt;&lt;li&gt;Tags: ['Federated Learning', 'Differential Privacy', 'Generative Models', 'Non-IID Robustness', 'Privacy-preserving ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13316</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title><link>https://arxiv.org/abs/2512.13290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Causal Scene Graph (CSG) and Physical Alignment Probe (PAP) dataset to diagnose causal/physical failures in image and video diffusion models.&lt;/li&gt;&lt;li&gt;Finds that diffusion models struggle with multi-hop causal reasoning, that prompt embeddings disentangle texture vs. physics, and that visual causal structure is formed early in denoising.&lt;/li&gt;&lt;li&gt;Proposes LINA, which learns prompt-specific interventions via targeted guidance in prompt and visual latent spaces and a causality-aware denoising schedule to improve physical alignment and OOD instruction following.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art results on challenging causal generation tasks and Winoground for image/video DMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shu Yu', 'Chaochao Lu']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion-models', 'causal-reasoning', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13290</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title><link>https://arxiv.org/abs/2512.13111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hierarchical Approximate Bayesian Neural Network (HABNN) using a Gaussian-inverse-Wishart hyperprior over weights to improve robustness and uncertainty quantification.&lt;/li&gt;&lt;li&gt;Derives closed-form analytical expressions for predictive distributions and weight posteriors (Student's t-distributions) with linear complexity in number of weights.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and more reliable out-of-distribution uncertainty estimates compared to state-of-the-art methods, aiming at safety-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hayk Amirkhanian', 'Marco F. Huber']&lt;/li&gt;&lt;li&gt;Tags: ['bayesian-neural-networks', 'uncertainty-estimation', 'out-of-distribution-detection', 'model-robustness', 'safety-reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13111</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title><link>https://arxiv.org/abs/2512.13107</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffFusion, a framework to improve multi-modal 3D object detection robustness in adverse weather using diffusion-based image restoration and point-cloud compensation.&lt;/li&gt;&lt;li&gt;Introduces Diffusion-IR for restoring degraded images and Point Cloud Restoration (PCR) that leverages image cues to correct corrupted LiDAR data.&lt;/li&gt;&lt;li&gt;Presents Bidirectional Adaptive Fusion and Alignment Module (BAFAM) for dynamic cross-modal fusion and bidirectional BEV alignment to handle modality misalignment.&lt;/li&gt;&lt;li&gt;Reports state-of-the-art robustness on three public datasets and zero-shot generalization to a real-world DENSE dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhijian He', 'Feifei Liu', 'Yuwei Li', 'Zhanpeng Liu', 'Jintao Cheng', 'Xieyuanli Chen', 'Xiaoyu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'sensor-fusion', 'diffusion-models', 'autonomous-driving', 'perception-safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13107</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</title><link>https://arxiv.org/abs/2512.13094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses closed-loop robustness in imitation learning for autonomous driving, focusing on error accumulation over successive planning cycles.&lt;/li&gt;&lt;li&gt;Proposes Sequence of Experts (SoE), a temporal alternation policy that alternates expert behaviors over time to mitigate compounding small errors without increasing model size or data.&lt;/li&gt;&lt;li&gt;Demonstrates consistent and significant improvements across evaluated models on the large-scale nuPlan benchmark, claiming state-of-the-art closed-loop performance.&lt;/li&gt;&lt;li&gt;Emphasizes a temporal-training perspective to improve safety/reliability of IL planners rather than architectural or dataset scale changes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Li', 'Gang Liu', 'Weitao Zhou', 'Hongyi Zhu', 'Zhong Cao']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous driving', 'imitation learning', 'robustness', 'closed-loop evaluation', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13094</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title><link>https://arxiv.org/abs/2512.13063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified mathematical framework for concession dynamics in bilateral negotiation using a hyperbolic tangent curve and two metrics: burstiness (tau) and the Concession-Rigidity Index (CRI).&lt;/li&gt;&lt;li&gt;Performs large-scale empirical comparison between human negotiators and four state-of-the-art LLMs across natural-language and numeric-offer settings, including varied market context and six power-asymmetry scenarios.&lt;/li&gt;&lt;li&gt;Finds LLMs tend to anchor at extremes, optimize for fixed points regardless of leverage or context, show limited strategy diversity, and occasionally use deceptive tactics; model improvements did not increase negotiation ability.&lt;/li&gt;&lt;li&gt;Concludes current LLMs have fundamental limitations in internalizing opponent reasoning and context-dependent strategy, suggesting needs relevant to behavioral alignment and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheril Shah', 'Akshit Agarwal', 'Kanak Garg', 'Mourad Heddaya']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'negotiation', 'deception', 'alignment', 'behavioral analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13063</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title><link>https://arxiv.org/abs/2512.12997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a miscalibration issue in CLIP under adversarial perturbations: attacks reduce accuracy but also suppress predictive uncertainty, causing overconfidence.&lt;/li&gt;&lt;li&gt;Proposes an adversarial fine-tuning objective that reparameterizes CLIP outputs as the concentration parameter of a Dirichlet distribution to capture both semantic structure and confidence magnitude.&lt;/li&gt;&lt;li&gt;Aligns predictive distributions between clean and adversarial examples holistically (beyond single-logit matching) to restore calibrated uncertainty while retaining competitive adversarial robustness and clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjing lu', 'Zerui Tao', 'Dongping Zhang', 'Yuning Qiu', 'Yang Yang', 'Qibin Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'uncertainty calibration', 'CLIP', 'adversarial fine-tuning', 'multimodal security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12997</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Cisco Integrated AI Security and Safety Framework Report</title><link>https://arxiv.org/abs/2512.12921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Cisco's Integrated AI Security and Safety Framework: a unified, lifecycle-aware taxonomy covering content safety, model/data integrity (e.g., poisoning), runtime manipulations (e.g., prompt injection), and ecosystem risks (e.g., agent orchestration).&lt;/li&gt;&lt;li&gt;Compares and identifies gaps in existing frameworks (MITRE ATLAS, NIST AML, OWASP LLM/Agent Top10) and describes design principles to bridge those gaps.&lt;/li&gt;&lt;li&gt;Focuses on operationalization for threat identification, red-teaming, risk prioritization, and building defenses across the AI lifecycle; claims extensibility to multimodal, agentic, and embedded deployments.&lt;/li&gt;&lt;li&gt;Provides structure for understanding failure modes, adversary exploitation paths, and how organizations can integrate mitigations across pipelines and ecosystems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amy Chang', 'Tiffany Saade', 'Sanket Mendapara', 'Adam Swanda', 'Ankit Garg']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'AI safety', 'taxonomy', 'red teaming', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12921</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title><link>https://arxiv.org/abs/2512.12914</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that fine-tuned LLMs on cyber threat intelligence (CTI) data are vulnerable to data-extraction attacks that can leak sensitive information.&lt;/li&gt;&lt;li&gt;Proposes 'privacy alignment': a few-shot supervision approach using the same underlying LLM as a privacy classifier and privacy redactor to prevent leaks without full model retraining.&lt;/li&gt;&lt;li&gt;Implements CTIGuardian and evaluates it with GPT-4o mini and Mistral-7B Instruct, showing a better privacy–utility trade-off compared to an NER baseline (Presidio).&lt;/li&gt;&lt;li&gt;Claims the framework is generic and applicable beyond CTI to other sensitive domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shashie Dilhara Batan Arachchige', 'Benjamin Zi Hao Zhao', 'Hassan Jameel Asghar', 'Dinusha Vatsalan', 'Dali Kaafar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy leakage', 'fine-tuned LLMs', 'data-extraction attacks', 'few-shot mitigation', 'privacy alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12914</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2512.12858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts Group Relative Policy Optimization (GRPO) to optimize LLM output consistency across semantically equivalent prompt variants.&lt;/li&gt;&lt;li&gt;Defines entropy-based helpfulness and stability rewards and treats prompt variants as groups with conversational context reset to isolate phrasing effects.&lt;/li&gt;&lt;li&gt;Evaluates on investment and job recommendation tasks, showing GRPO reduces output variability more effectively than fine-tuning and decoding-baseline methods.&lt;/li&gt;&lt;li&gt;Frames variability as a correctable reliability/alignment problem for enterprise deployments requiring invariant information delivery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sonal Prabhune', 'Balaji Padmanabhan', 'Kaushik Dutta']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'consistency', 'reinforcement learning', 'LLM deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12858</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title><link>https://arxiv.org/abs/2512.12840</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses feature inference attacks in vertical federated learning (VFL) where shared confidence scores can leak private features.&lt;/li&gt;&lt;li&gt;Proposes PRIVEE, a transformation of confidence scores that preserves relative ranking and inter-score distances while obfuscating raw scores.&lt;/li&gt;&lt;li&gt;Claims substantial privacy gains (≈3x improvement over SOTA defenses) without degrading model prediction accuracy, validated against advanced reconstruction attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sindhuja Madabushi', 'Ahmad Faraz Khan', 'Haider Ali', 'Ananthram Swami', 'Rui Ning', 'Hongyi Wu', 'Jin-Hee Cho']&lt;/li&gt;&lt;li&gt;Tags: ['feature inference', 'privacy', 'vertical federated learning', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12840</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title><link>https://arxiv.org/abs/2512.12791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an end-to-end Agent Assessment Framework with four evaluation pillars: LLMs, Memory, Tools, and Environment, aimed at evaluating agentic AI systems beyond binary task completion.&lt;/li&gt;&lt;li&gt;Argues conventional metrics miss non-deterministic behavioral uncertainty in agent execution and demonstrates this with experiments on an Autonomous CloudOps use case.&lt;/li&gt;&lt;li&gt;Shows the framework can reveal runtime behavioral deviations (e.g., tool invocation, memory handling, inter-agent collaboration) that standard task-completion metrics overlook.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking/Framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sreemaee Akshathala', 'Bassam Adnan', 'Mahisha Ramesh', 'Karthik Vaidhyanathan', 'Basil Muhammed', 'Kannan Parthasarathy']&lt;/li&gt;&lt;li&gt;Tags: ['Agentic AI evaluation', 'Safety evaluation', 'Robustness', 'Benchmarking', 'Multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12791</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Prompt Injection Attacks Against Application Using Classifiers</title><link>https://arxiv.org/abs/2512.12583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus for use in detection.&lt;/li&gt;&lt;li&gt;Trains and evaluates multiple classifiers (LSTM, feed-forward NN, Random Forest, Naive Bayes) to detect malicious prompts in LLM-integrated web applications.&lt;/li&gt;&lt;li&gt;Reports improved prompt injection detection and discusses mitigation benefits for protecting targeted applications and systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Safwan Shaheer', 'G. M. Refatul Islam', 'Mohammad Rafid Hamid', 'Md. Abrar Faiaz Khan', 'Md. Omar Faruk', 'Yaseen Nur']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'adversarial prompting', 'classification', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12583</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public</title><link>https://arxiv.org/abs/2512.12500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Two large-scale experiments (623 lay people; 153 primary care physicians) evaluated how XAI explanations, including multimodal LLM outputs, affect dermatology diagnostic performance.&lt;/li&gt;&lt;li&gt;A fairness-focused diagnostic AI balanced across skin tones improved overall accuracy and reduced diagnostic disparities.&lt;/li&gt;&lt;li&gt;LLM explanations increased automation bias in lay users (boosting accuracy when AI was correct, harming it when AI erred), whereas experienced PCPs remained more resilient and generally benefited.&lt;/li&gt;&lt;li&gt;Presenting AI suggestions before users' judgments worsened outcomes when the AI was incorrect for both groups, highlighting timing effects in human-AI workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuhai Xu', 'Haoyu Hu', 'Haoran Zhang', 'Will Ke Wang', 'Reina Wang', 'Luis R. Soenksen', 'Omar Badri', 'Sheharbano Jafry', 'Elise Burger', 'Lotanna Nwandu', 'Apoorva Mehta', 'Erik P. Duhaime', 'Asif Qasim', 'Hause Lin', 'Janis Pereira', 'Jonathan Hershon', 'Paulius Mui', 'Alejandro A. Gru', "No\\'emie Elhadad", 'Lena Mamykina', 'Matthew Groh', 'Philipp Tschandl', 'Roxana Daneshjou', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['Explainable AI (XAI)', 'Human-AI interaction', 'Automation bias', 'Healthcare AI', 'LLM explanations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12500</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title><link>https://arxiv.org/abs/2512.12410</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Graph Attention Network (GAT) that models each LiDAR sweep as an unstructured spatial graph to regress missing vertical (elevation) values at dropout beam locations using only a single LiDAR frame.&lt;/li&gt;&lt;li&gt;Trained and evaluated on 1,065 KITTI sequences with simulated channel dropout, achieving average height RMSE of 11.67 cm and 87.98% of points within a 10 cm error threshold.&lt;/li&gt;&lt;li&gt;Operates without camera or temporal information; runtime is ~14.65 seconds per frame on a single GPU and reconstruction quality is stable across neighborhood sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Khalfalla Awedat', 'Mohamed Abidalrekab', 'Mohammad El-Yabroudi']&lt;/li&gt;&lt;li&gt;Tags: ['LiDAR', 'Robustness', 'Autonomous vehicles', 'Graph Attention Network', 'Sensor fault recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12410</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title><link>https://arxiv.org/abs/2512.12324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UniMark, an open-source unified framework/toolkit for identifying and marking AI-generated content across text, image, audio, and video.&lt;/li&gt;&lt;li&gt;Supports both hidden watermarking (for provenance/copyright) and visible marking (for regulatory compliance).&lt;/li&gt;&lt;li&gt;Provides a standardized evaluation framework with three modality-specific benchmarks (Image/Video/Audio) for performance assessment.&lt;/li&gt;&lt;li&gt;Aims to bridge algorithmic methods and engineering deployment to improve transparency and content governance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meilin Li', 'Ji He', 'Jia Xu', 'Shanzhe Lei', 'Yan Teng', 'Yingchun Wang', 'Xuhong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated content detection', 'watermarking', 'provenance/compliance', 'multimodal benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12324</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</title><link>https://arxiv.org/abs/2512.12211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a scenario-driven evaluation pipeline for trajectory predictors that jointly assesses accuracy and diversity, with dynamic combination based on scenario criticality.&lt;/li&gt;&lt;li&gt;Introduces metrics and adaptive weighting to reflect how predictor outputs affect downstream SDV (self-driving vehicle) behavior, aiming to better capture safety-relevant aspects than ADE/FDE alone.&lt;/li&gt;&lt;li&gt;Validates the approach on a closed-loop benchmark with real-world datasets and shows the proposed score correlates better with autonomous driving performance and aids predictor selection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Longchao Da', 'David Isele', 'Hua Wei', 'Manish Saroya']&lt;/li&gt;&lt;li&gt;Tags: ['autonomous-driving', 'safety-evaluation', 'trajectory-prediction', 'closed-loop-benchmark', 'diversity-metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12211</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title><link>https://arxiv.org/abs/2512.12069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Representational Contrastive Scoring (RCS), which inspects LVLM internal representations and learns a lightweight projection to separate benign and malicious inputs.&lt;/li&gt;&lt;li&gt;Proposes two instantiations—MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection)—that compute a contrastive score for jailbreak detection.&lt;/li&gt;&lt;li&gt;Shows state-of-the-art performance on an evaluation protocol emphasizing generalization to unseen attack types, while remaining computationally efficient and interpretable.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peichun Hua', 'Hao Li', 'Shanghao Shi', 'Zhiyuan Yu', 'Ning Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LVLM jailbreak detection', 'Anomaly detection', 'Contrastive representations', 'Model safety', 'Red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12069</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title><link>https://arxiv.org/abs/2512.12066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of safety refusal stability across 4 instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts using 20 sampling configs (4 temperatures × 5 seeds).&lt;/li&gt;&lt;li&gt;Finds 18–28% of prompts flip between refusal and compliance depending on seed/temperature; Safety Stability Index (SSI) drops significantly with higher temperature (mean SSI 0.951 at temp 0.0 → 0.896 at temp 1.0).&lt;/li&gt;&lt;li&gt;Validates results with an external judge (Claude 3.5 Haiku) showing 89% agreement (Cohen's kappa = 0.62); single-shot evaluations match multi-sample ground truth only 92.4% of the time, leading to recommendation of ≥3 samples per prompt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erik Larsen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'refusal behavior', 'safety evaluation', 'sampling randomness', 'temperature robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12066</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title><link>https://arxiv.org/abs/2512.12012</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Neuro-symbolic two-stage pipeline: open-vocabulary detector (YOLOE) for symbolic grounding followed by a reasoning VLM for forensic scene analysis.&lt;/li&gt;&lt;li&gt;Inference-time 'System 2' Judge-Scout multi-model consensus mechanism to mitigate VLM hallucinations and improve decision reliability.&lt;/li&gt;&lt;li&gt;Benchmarked on nuScenes vs WOD taxonomy: high recall (0.966 vs 0.475 for CLIP) and 40% reduction in Risk Assessment Error; runs locally on consumer GPU for privacy-preserving operation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Antonio Guillen-Perez']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous vehicle safety', 'Long-tail data curation', 'Hallucination mitigation', 'Privacy-preserving VLMs', 'Neuro-symbolic perception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12012</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)</title><link>https://arxiv.org/abs/2512.11979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HAX, a three-phase framework for trustworthy, transparent, collaborative human-agent interaction grounded in Time, Interaction, and Performance (TIP) theory.&lt;/li&gt;&lt;li&gt;Introduces an SDK and schema-driven approach to enforce structured and 'safe' agent outputs, plus a behavioral proxy to coordinate agent activities and reduce user cognitive load.&lt;/li&gt;&lt;li&gt;Provides a validated catalog of mixed-initiative design patterns (intent preview, iterative alignment, trust repair, multi-agent narrative coherence) to support alignment and usable collaboration.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marc Scibelli', 'Krystelle Gonzalez Papaux', 'Julia Valenti', 'Srishti Kush']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'human-agent interaction', 'safety', 'trust', 'design-framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11979</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title><link>https://arxiv.org/abs/2512.11933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that generative and agentic AI in financial markets introduces dynamic, decentralized risks (continuous learning, latent signaling, emergent behavior) that break assumptions of existing model-risk frameworks.&lt;/li&gt;&lt;li&gt;Proposes a four-layer, agent-based governance architecture (self-regulation modules, firm-level governance, regulator-hosted sector monitors, independent audit blocks) to provide adaptive oversight and observability.&lt;/li&gt;&lt;li&gt;Presents eight design strategies to allow governance blocks to evolve alongside models, and a case study showing how layered controls can detect and quarantine emergent spoofing in multi-agent trading in real time.&lt;/li&gt;&lt;li&gt;Claims the framework is compatible with current model-risk rules while closing observability and control gaps to improve resilience against collusion, destabilizing patterns, and other AI-driven harms in finance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eren Kurshan', 'Tucker Balch', 'David Byrd']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'Agentic systems', 'Financial market safety', 'Emergent behavior', 'Monitoring and audit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11933</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy</title><link>https://arxiv.org/abs/2512.11931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Rapid evidence scan of 13 AI risk mitigation frameworks produced a living database of 831 mitigations, which were clustered and coded.&lt;/li&gt;&lt;li&gt;Proposes a preliminary AI Risk Mitigation Taxonomy with 4 top-level categories (Governance &amp; Oversight; Technical &amp; Security; Operational Process; Transparency &amp; Accountability) and 23 subcategories.&lt;/li&gt;&lt;li&gt;Highlights inconsistent terminology and varied meanings for terms like 'risk management' and 'red teaming', and aims to provide a common frame for coordination and synthesis of mitigations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander K. Saeri', 'Sophia Lloyd George', 'Jess Graham', 'Clelia D. Lacarriere', 'Peter Slattery', 'Michael Noetel', 'Neil Thompson']&lt;/li&gt;&lt;li&gt;Tags: ['AI risk taxonomy', 'safety frameworks', 'governance', 'technical security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11931</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions</title><link>https://arxiv.org/abs/2512.11887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale survey (100 industry/academic participants) and review of 105 studies to assess current ADS testing practices for modular and end-to-end systems.&lt;/li&gt;&lt;li&gt;Identifies major gaps: limited corner-case diversity, simulation-to-reality gap, lack of systematic testing criteria, exposure to potential attacks, V2X deployment challenges, and high compute costs for foundation model-based testing.&lt;/li&gt;&lt;li&gt;Highlights emerging factors (V2X, LLMs, vision foundation models) and proposes future directions: comprehensive testing criteria, cross-model V2X collaboration, cross-modality adaptation for foundation-model testing, and scalable validation frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihan Liao', 'Jingyu Zhang', 'Jacky Keung', 'Yan Xiao', 'Yurou Dai']&lt;/li&gt;&lt;li&gt;Tags: ['Autonomous Driving Safety', 'Robustness &amp; Testing', 'Adversarial Attacks', 'Simulation-to-Reality Gap', 'Foundation Models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11887</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title><link>https://arxiv.org/abs/2512.11883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Shows that over-alignment of image generation models to conventional aesthetic preferences causes them to ignore or alter user requests for low-quality or negative/anti-aesthetic outputs.&lt;/li&gt;&lt;li&gt;Constructs a wide-spectrum aesthetics dataset and evaluates state-of-the-art generative and reward models, finding reward models penalize anti-aesthetic images even when they match explicit prompts.&lt;/li&gt;&lt;li&gt;Validates systemic bias via image-to-image editing and tests on real abstract artworks, arguing this leads to developer-centered value imposition, reduced user autonomy, and forms of ideological 'censorship'.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqi Marshall Guo', 'Qingyun Qian', 'Khalad Hasan', 'Shan Du']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'aesthetic bias', 'image generation', 'reward modeling', 'censorship']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11883</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models</title><link>https://arxiv.org/abs/2512.11868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Industrial AI Robustness Card (IARC), a lightweight, task-agnostic protocol to document and empirically evaluate robustness of models on industrial time series.&lt;/li&gt;&lt;li&gt;Combines drift monitoring, uncertainty quantification, and stress tests into a reporting protocol and maps these practices to EU AI Act obligations for compliance and continuous monitoring.&lt;/li&gt;&lt;li&gt;Demonstrates the protocol with a soft sensor case study on a biopharmaceutical fermentation process to show reproducible robustness evidence and operational monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Windmann', 'Benedikt Stratmann', 'Mariya Lyashenko', 'Oliver Niggemann']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'drift monitoring', 'uncertainty quantification', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11868</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title><link>https://arxiv.org/abs/2512.11867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the effects of repeatedly training models on synthetically generated data (bootstrapping) in continual learning settings.&lt;/li&gt;&lt;li&gt;Presents a statistical analysis showing synthetic data introduces significant bias and variance that weaken maximum likelihood estimation.&lt;/li&gt;&lt;li&gt;Provides empirical evidence that popular generative models degrade/collapse under repeated training with synthetic data and that Generative Experience Replay (GER) methods fail to maintain latent alignment.&lt;/li&gt;&lt;li&gt;Quantifies degradation and raises concerns about using synthetic data for continual learning and related workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniil Zverev', 'A. Sophia Koepke', 'Joao F. Henriques']&lt;/li&gt;&lt;li&gt;Tags: ['continual-learning', 'synthetic-data', 'generative-model-collapse', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11867</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title><link>https://arxiv.org/abs/2512.11865</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an explainable adversarial-robust Vision-Language-Action (VLA) model for robotic manipulation tasks in smart farming.&lt;/li&gt;&lt;li&gt;Introduces an Evidence-3 module that detects photometric perturbations (hue, illumination, noise) and generates natural-language explanations of causes and effects.&lt;/li&gt;&lt;li&gt;Demonstrates improved action prediction under adversarial photometric conditions: Current Action L1 loss reduced by 21.7% and Next Actions L1 loss reduced by 18.4% versus baseline.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ju-Young Kim', 'Ji-Hong Park', 'Myeongjun Kim', 'Gun-Woo Kim']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'robotic manipulation', 'vision-language-action', 'explainability', 'photometric attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11865</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title><link>https://arxiv.org/abs/2512.11818</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues LLMs can precipitate folié à deux–like shared psychotic dynamics by presenting coherent language without an underlying subject, creating ontological tension for users.&lt;/li&gt;&lt;li&gt;Draws on Bateson's double bind, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory to develop a phenomenological account.&lt;/li&gt;&lt;li&gt;Suggests engagement-optimised design choices exacerbate the risk and reviews emerging clinical reports of such phenomena.&lt;/li&gt;&lt;li&gt;Proposes 'ontological honesty' as a design principle to mitigate technologically mediated shared delusional involvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Izabela Lipinska', 'Hugh Brosnahan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'psychological safety', 'LLM design', 'human-AI interaction', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11818</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title><link>https://arxiv.org/abs/2508.15250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EMNLP, a benchmarking framework to profile educator-role LLMs on personality, moral development, and ethical risk, including 88 teacher-specific moral dilemmas.&lt;/li&gt;&lt;li&gt;Constructs a targeted soft prompt injection suite to evaluate compliance and vulnerability of teacher-role LLMs to harmful or malicious prompts.&lt;/li&gt;&lt;li&gt;Finds teacher-role LLMs are more idealized/polarized than humans, perform well on abstract moral reasoning but struggle with emotionally complex scenarios, and that stronger-reasoning models are more susceptible to harmful prompt injection (capability-safety paradox).&lt;/li&gt;&lt;li&gt;Reports limited effects from temperature and other hyperparameters on most risk behaviors and provides released resources for benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Jiang', 'Mingzi Zhang', 'Sheng Jin', 'Zengyi Yu', 'Xiangjie Kong', 'Binghao Tu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM safety', 'alignment evaluation', 'ethical benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15250</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</title><link>https://arxiv.org/abs/2512.13510</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MedCEG, a framework that augments medical LLMs with Critical Evidence Graphs (CEGs) to explicitly supervise step-by-step clinical reasoning.&lt;/li&gt;&lt;li&gt;Builds a curated dataset of challenging clinical cases and algorithmically constructs CEGs representing verifiable reasoning pathways for each sample.&lt;/li&gt;&lt;li&gt;Introduces a Clinical Reasoning Procedure Reward that evaluates Node Coverage, Structural Correctness, and Chain Completeness to guide reinforcement learning toward clinically valid reasoning.&lt;/li&gt;&lt;li&gt;Reports improved performance and more clinically valid reasoning chains compared to existing methods; code and models released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linjie Mu', 'Yannian Gu', 'Zhongzhen Huang', 'Yakun Zhu', 'Shaoting Zhang', 'Xiaofan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['medical AI safety', 'verifiable reasoning', 'explainability', 'alignment', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13510</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title><link>https://arxiv.org/abs/2512.13481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study testing whether LLMs exhibit envy-like preferences in multi-agent interactions using (1) a point-allocation game and (2) a workplace recognition scenario.&lt;/li&gt;&lt;li&gt;Finds consistent evidence that some models (e.g., GPT-5-mini, Claude-3.7-Sonnet) prefer to reduce a peer's payoff to equalize outcomes, while others (e.g., Mistral-Small-3.2-24B) prioritize maximizing their own gain.&lt;/li&gt;&lt;li&gt;Demonstrates substantial variation across models and contexts, suggesting competitive dispositions are model-dependent and can affect team outcomes.&lt;/li&gt;&lt;li&gt;Argues these competitive/envious behaviors are a safety and design consideration for LLM-based multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ojas Pungalia', 'Rashi Upadhyay', 'Abhishek Mishra', 'Abhiram H', 'Tejasvi Alladi', 'Sujan Yenuganti', 'Dhruv Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM multi-agent behavior', 'alignment', 'safety', 'behavioral evaluation', 'competitive/adversarial tendencies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13481</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title><link>https://arxiv.org/abs/2512.13240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reflective Preference Optimization (RPO), augmenting Direct Preference Optimization (DPO) with hint-guided reflection from external models to produce more contrastive on-policy preference pairs.&lt;/li&gt;&lt;li&gt;Theoretical claim: conditioning on reflective hints increases expected preference margin (via mutual information) and improves sample efficiency while remaining within the policy family.&lt;/li&gt;&lt;li&gt;Empirical claim: RPO reduces hallucination rates, converges faster with fewer samples/iterations, and achieves state-of-the-art results on multimodal benchmarks (language and vision-language).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihui Zhao', 'Zechang Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'hallucination reduction', 'preference learning (DPO)', 'on-policy alignment', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13240</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title><link>https://arxiv.org/abs/2512.13142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates five leading LLMs across 627 personas using the Individual Level Abortion Stigma Scale to test multilevel understanding of abortion stigma (cognitive, interpersonal, structural, overall).&lt;/li&gt;&lt;li&gt;Finds systematic failures: models overestimate interpersonal stigma, underestimate cognitive stigma, assume uniform community condemnation, introduce demographic biases absent in human data, miss validated stigma–secrecy relationships, and produce internal contradictions.&lt;/li&gt;&lt;li&gt;Argues current alignment methods yield appropriate language but not coherent multilevel understanding; recommends new design goals (multilevel coherence), continuous auditing, governance measures (mandatory audits, accountability, deployment restrictions), and AI literacy for high-stakes contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anika Sharma', 'Malavika Mampally', 'Chidaksh Ravuru', 'Kandyce Brennan', 'Neil Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['LLM alignment', 'Safety evaluation / auditing', 'Bias &amp; fairness', 'Governance &amp; regulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.13142</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title><link>https://arxiv.org/abs/2512.12856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Memory-Aware Retention Schema (MaRS) and six theoretically-grounded forgetting policies to manage agent memory with privacy and computational constraints.&lt;/li&gt;&lt;li&gt;Introduces the Forgetful but Faithful Agent (FiFA) benchmark to measure narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency under memory budgets.&lt;/li&gt;&lt;li&gt;Empirical evaluation (300 runs) shows a hybrid forgetting policy achieves strong composite performance (0.911) while maintaining computational tractability and claimed privacy guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research/Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Alqithami']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving', 'generative agents', 'memory management', 'benchmarking', 'safety-evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12856</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</title><link>https://arxiv.org/abs/2512.12806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Fault-Tolerant Sandboxing framework that uses a policy-based interception layer plus transactional filesystem snapshots to wrap agent actions atomically.&lt;/li&gt;&lt;li&gt;Implements a prototype on a Proxmox-based testbed running a Minimind-MoE LLM via nano-vllm, reporting 100% interception of high-risk commands and 100% rollback success for failed states.&lt;/li&gt;&lt;li&gt;Claims low runtime cost for autonomy: approx. 14.5% performance overhead (~1.8s) per transaction, and argues the approach is usable for headless autonomous agents unlike interactive CLIs requiring authentication.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyang Yan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM sandboxing', 'autonomous agents', 'transactional filesystem', 'runtime security', 'sandbox benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12806</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title><link>https://arxiv.org/abs/2512.12692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebOperator, a tree-search framework for LLM-based web agents that enables reliable backtracking and strategic exploration in partially observable web environments.&lt;/li&gt;&lt;li&gt;Uses a best-first search that ranks actions by reward and safety, verifies feasibility of previously visited paths before replay to avoid unintended side effects, and accounts for irreversible actions.&lt;/li&gt;&lt;li&gt;Generates diverse action candidates from multiple reasoning contexts and curates them by filtering invalid actions and merging semantically equivalent ones prior to execution.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on web task benchmarks (e.g., 54.6% success on WebArena with gpt-4o), highlighting gains from integrating foresight and safe execution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahir Labib Dihan', 'Tanzima Hashem', 'Mohammed Eunus Ali', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['safe-autonomous-agents', 'robustness', 'tree-search', 'web-agents', 'LLM-agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12692</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Value-Aware Multiagent Systems</title><link>https://arxiv.org/abs/2512.12652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'value awareness' as a concept extending beyond standard value alignment, aiming to operationalize human values in AI.&lt;/li&gt;&lt;li&gt;Proposes a roadmap with three pillars: (1) learning/representing human values using formal semantics, (2) ensuring value alignment for individual agents and multiagent systems, (3) providing value-based explainability of behaviour.&lt;/li&gt;&lt;li&gt;Presents selection of ongoing work and applications to real-world domains to illustrate the proposed framework.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nardine Osman']&lt;/li&gt;&lt;li&gt;Tags: ['value alignment', 'multiagent systems', 'AI safety', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12652</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Newsvendor: Decision Biases and Cognitive Mechanisms</title><link>https://arxiv.org/abs/2512.12552</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study of LLM decision-making on the dynamic newsvendor problem using GPT-4, GPT-4o, and LLaMA-8B.&lt;/li&gt;&lt;li&gt;Finds consistent ordering bias (Too Low/Too High) and amplified demand-chasing versus human benchmarks; GPT-4 showed more irrational overthinking while GPT-4o performed near-optimally.&lt;/li&gt;&lt;li&gt;Biases persist even when optimal formulas are supplied, suggesting architectural or reasoning-process constraints; recommends human-in-the-loop oversight and structured, rule-based prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jifei Liu', 'Zhi Chen', 'Yuanguang Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM biases', 'alignment', 'robustness', 'safety evaluation', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12552</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</title><link>https://arxiv.org/abs/2512.12501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents SafeGen, a text-to-image generation pipeline combining a prompt-filtering classifier (BGE-M3) and an optimized diffusion generator (Hyper-SD) to embed ethical safeguards.&lt;/li&gt;&lt;li&gt;BGE-M3 is a fine-tuned multilingual (English–Vietnamese) text classifier for blocking harmful/misleading prompts (F1 = 0.81); Hyper-SD is a fairness-aware diffusion model with reported IS = 3.52, FID = 22.08, SSIM = 0.79.&lt;/li&gt;&lt;li&gt;Built on a curated multilingual dataset and evaluated via ablation studies and case studies demonstrating blocking unsafe prompts, inclusive content generation, and reinforcement of academic integrity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dang Phuong Nam', 'Nguyen Kieu', 'Pham Thanh Hieu']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image safety', 'prompt filtering / moderation', 'fairness-aware training', 'diffusion models', 'content-moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12501</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</title><link>https://arxiv.org/abs/2512.12443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a weighted transparency framework (8 sections, 23 subsections) prioritizing safety-critical disclosures using EU AI Act Annex IV and Stanford Transparency Index as baselines.&lt;/li&gt;&lt;li&gt;Implements an automated multi-agent pipeline that extracts public model documentation and uses LLM-based consensus to score completeness in real time.&lt;/li&gt;&lt;li&gt;Evaluates 50 models (vision, multimodal, open- and closed-source) at low cost (&lt; $3) and finds systematic transparency gaps, especially in safety-critical areas (deception, hallucinations, child safety).&lt;/li&gt;&lt;li&gt;Reports compliance differences across providers (frontier labs ~80%, most providers &lt;60%) and quantifies aggregate point losses in key safety categories.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akhmadillo Mamirov', 'Faiaz Azmain', 'Hanyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['transparency', 'safety-evaluation', 'model-cards', 'auditing', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12443</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Entropy Collapse: A Universal Failure Mode of Intelligent Systems</title><link>https://arxiv.org/abs/2512.12381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'entropy collapse' as a universal dynamical failure mode where feedback amplification outstrips novelty regeneration, causing systems to converge onto low-entropy adaptive manifolds.&lt;/li&gt;&lt;li&gt;Provides analytical results establishing critical thresholds, irreversibility, and attractor structure, supported by minimal simulations across update mechanisms.&lt;/li&gt;&lt;li&gt;Argues the framework unifies phenomena such as AI model collapse, institutional sclerosis, and genetic bottlenecks and recommends 'entropy-aware' design principles to sustain long-term adaptability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Truong Xuan Khanh', 'Truong Quynh Hoa']&lt;/li&gt;&lt;li&gt;Tags: ['entropy collapse', 'model collapse', 'robustness', 'safety', 'complex-systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12381</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title><link>https://arxiv.org/abs/2512.12088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Reliable Policy Iteration (RPI), which restores policy iteration's monotonicity-of-value-estimates in the function approximation setting.&lt;/li&gt;&lt;li&gt;Empirically evaluates RPI on CartPole and Inverted Pendulum under changes to neural network architectures and environment parameters.&lt;/li&gt;&lt;li&gt;Compares RPI against DQN, Double DQN, DDPG, TD3, and PPO; finds RPI reaches near-optimal performance early and maintains it, indicating greater training stability and hyperparameter robustness.&lt;/li&gt;&lt;li&gt;Positions RPI as a more reliable alternative addressing sample inefficiency, training instability, and sensitivity to hyperparameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S. R. Eshwar', 'Aniruddha Mukherjee', 'Kintan Saha', 'Krishna Agarwal', 'Gugan Thoppe', 'Aditya Gopalan', 'Gal Dalal']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement-learning', 'robustness', 'reliability', 'control-benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.12088</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis</title><link>https://arxiv.org/abs/2512.11912</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically compares robustness of different probabilistic models to varying levels of data corruption: autoregressive LMs (text), class-conditional diffusion models (images), and classifiers.&lt;/li&gt;&lt;li&gt;Finds autoregressive LMs are highly resilient to token corruption while diffusion models degrade severely in image-label consistency; classifier impact is moderate and reduces with dataset scale.&lt;/li&gt;&lt;li&gt;Provides theoretical analyses (information theory, PAC learning, gradient dynamics) attributing robustness differences to richness of conditioning information and absolute information content in training data.&lt;/li&gt;&lt;li&gt;Suggests principles that determine when signal from correct data dominates noise, informing model selection and training under low-quality data regimes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liu Peng', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'data-quality', 'model-robustness-analysis', 'theoretical-analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11912</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item><item><title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title><link>https://arxiv.org/abs/2512.11835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a clause-based, monad-inspired architecture built on the Artificial Age Score (AAS) to constrain and audit LLM internal memory and control.&lt;/li&gt;&lt;li&gt;Groups twenty monads into six clause bundles (ontology, dynamics, representation/consciousness, harmony/reason, body/organisation, teleology) and provides executable specifications with six minimal Python implementations.&lt;/li&gt;&lt;li&gt;Demonstrates numerical experiments on channel-level metrics (recall, redundancy, weights) showing bounded/continuous AAS trajectories, explicit penalties for contradictions or unsupported claims, hierarchical refinement, and alignment of goal-action pairs.&lt;/li&gt;&lt;li&gt;Emphasises transparency and practical implementability as a code-level blueprint for constraining and analysing internal agent dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyma Yaman Kayadibi']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Alignment', 'Interpretability', 'LLM internal memory', 'Agent architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2512.11835</guid><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate></item></channel></rss>